<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201643</article-id><article-id pub-id-type="doi">10.1101/2024.11.29.626019</article-id><article-id pub-id-type="archive">PPR948587</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Moving beyond the motor cortex: a brain-wide evaluation of target locations for intracranial speech neuroprostheses</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Verwoert</surname><given-names>Maxime</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Ottenhoff</surname><given-names>Maarten C.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Tousseyn</surname><given-names>Simon</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>van Dijk</surname><given-names>Johannes P.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Kubben</surname><given-names>Pieter L.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Herff</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Neurosurgery, Mental Health and Neuroscience Research Institute, Maastricht, The Netherlands</aff><aff id="A2"><label>2</label>Academic Center for Epileptology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03bbe8e53</institution-id><institution>Kempenhaeghe</institution></institution-wrap>/<institution-wrap><institution-id institution-id-type="ror">https://ror.org/02d9ce178</institution-id><institution>Maastricht University Medical Center</institution></institution-wrap>, <city>Heeze</city>, <country country="NL">The Netherlands</country></aff><aff id="A3"><label>3</label>Department of Orthodontics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/032000t02</institution-id><institution>Ulm University</institution></institution-wrap>, <city>Ulm</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>Department of Electrical Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02c2kyt77</institution-id><institution>Eindhoven University of Technology</institution></institution-wrap>, <city>Eindhoven</city>, <country country="NL">The Netherlands</country></aff><aff id="A5"><label>5</label>Academic Center for Epileptology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03bbe8e53</institution-id><institution>Kempenhaeghe</institution></institution-wrap>/<institution-wrap><institution-id institution-id-type="ror">https://ror.org/02d9ce178</institution-id><institution>Maastricht University Medical Center</institution></institution-wrap>, <city>Maastricht</city>, <country country="NL">The Netherlands</country></aff><author-notes><corresp id="CR1"><label>*</label>corresponding author(s): Christian Herff (<email>c.herff@maastrichuniversity.nl</email>), Maxime Verwoert (<email>m.verwoert@maastrichtuniversity.nl</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nd/4.0/">CC BY-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Speech is the fastest and most natural form of communication, which can be impaired in certain disorders. Speech brain-computer interfaces (BCIs) offer a solution by decoding brain activity into speech. Current neuroprosthetic devices focus on the motor cortex, which might not be usable in all patient populations. Fortunately, many other brain regions have been associated with the speech production process. Here, we investigate which regions are potential (alternative) targets for a speech BCI across a brain-wide distribution within a single study. The distribution includes sulci and subcortical areas, sampled with both a high temporal and a high spatial resolution. Thirty participants were recorded with intracranial electroencephalography during speech production, resulting in 3249 recorded contacts across the brain. We trained machine learning models to continuously predict speech from a brain-wide global to a single-channel local scale. Within each scale we examined a variation of selected electrode contacts based on anatomical features within participants. We found significant speech detection in both gray and white matter tissue, no significant difference between gyri and sulci at any of the analysis scales and limited contribution from subcortical areas. The best potential targets in terms of decoding accuracy and consistency are located within the depth of and surrounding the lateral fissure bilaterally, such as the (sub)central sulcus, transverse temporal gyrus (Heschls’ gyrus), the supramarginal cortex and parts of the insula. These results highlight the potential benefits of extending beyond the motor cortex and reaching the sulcal depth for speech neuroprostheses.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Decoding neural signals to restore natural and efficient communication for individuals with speech impairments is a key objective of speech brain-computer interface (BCI) research<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Speech production is a highly complex process involving an extensive and interconnected network of brain regions<sup><xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref></sup>. Yet, the current state-of-the-art speech BCIs are primarily focused on decoding signals from the sensorimotor cortex<sup><xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. While this region has shown promise in enabling communication for patients with dysarthria or anarthria due to amyotrophic lateral sclerosis (ALS) or a brain-stem stroke, it may not cover the full intent of the user<sup><xref ref-type="bibr" rid="R10">10</xref></sup> nor translate to individuals with different diseases or disease progressions. This is especially the case for individuals with damage to the motor cortex, which may happen due to a stroke or degeneration of cortical motor neurons in ALS<sup><xref ref-type="bibr" rid="R11">11</xref></sup>.</p><p id="P3">Given the extensive network of brain areas contributing to produce speech, it is essential to consider alternative regions across the entire brain. Existing non-invasive techniques that can capture signals across the brain, such as functional magnetic resonance imaging (fMRI), scalp electro-encephalography (EEG), and magneto-enecephalography (MEG), lack the simultaneous temporal and spatial resolution necessary to capture the rapid and complex dynamics of natural speech production for a continuous BCI. The intracranial technologies electro-corticography (ECoG) and micro-electrode arrays (MEAs) provide high temporal-spatial fidelity and have demonstrated success in speech BCIs<sup><xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R9">9</xref></sup>. However, both cover only the brain’s outer surface in typically a few specific cortical regions (i.e., the sensorimotor cortex). While research with these technologies provide crucial insights, these tools cannot access deeper areas of the brain—such as the sulci, inter-hemispheric regions, and subcortical nuclei.</p><p id="P4">Stereo-electroencephalography (sEEG), in contrast, offers an opportunity to probe these deeper areas of the brain. Research utilizing sEEG has revealed involvement of subcortical structures in speech production<sup><xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref></sup>, as well as cortical areas nestled within the sulci that are beyond the reach of surface-based recording technologies<sup><xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref></sup>. An added advantage of sEEG is the typical broad placement of electrodes across multiple regions, often bilaterally, within a single individual<sup><xref ref-type="bibr" rid="R16">16</xref></sup>, enabling a more diverse survey of brain regions implicated in speech. Although sEEG sampling can be spatially sparse, by aggregating data across multiple participants with varying electrode placements, we can achieve a brain-wide investigation, as we demonstrate in the current study.</p><p id="P5">Different representations of speech (e.g., articulatory, acoustic or semantic components) can be decoded from varying locations<sup><xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref></sup>, which may be beneficial for individuals with diverse brain injuries affecting speech. For instance, in motor-based impairments like apraxia of speech, where motor planning is disrupted<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, BCIs may benefit from targeting areas associated with higher-level phonological or acoustic representations, such as the posterior superior temporal cortex<sup><xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref></sup>. In this study, we apply a continuous speech/silence decoder to 3249 widely distributed recording sites, covering gyri, sulci and deeper structures to investigate potential targets for speech neuroprostheses.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P6">We recorded data from 30 individuals with medication-resistant epilepsy in the current study (<xref ref-type="table" rid="T1">Table 1</xref>). The data from the first 10 participants (P01 to P10) have previously been published<sup><xref ref-type="bibr" rid="R23">23</xref></sup>. All participants joined the study voluntarily and signed written informed consent. The study has been approved by the Institutional Review Boards of both Maastricht Unviversity (METC 2018-0451) and Epilepsy Center Kempenhaeghe. Participants were native Dutch speakers and were implanted with sEEG electrodes (<xref ref-type="fig" rid="F1">Fig. 1A</xref>) to localize the epileptic onset zone. Electrode placement was solely determined based on clinical needs.</p></sec><sec id="S4"><title>Tasks</title><p id="P7">Participants performed one of three speech production tasks in which they spoke 100 Dutch words or sentences out loud. In the SWPD dataset<sup><xref ref-type="bibr" rid="R24">24</xref></sup> (P01 to P10), unique words (UW) were presented for two seconds. The next set of participants (P11 to P20) spoke 20 words, in random order, that were chosen to be relevant for patients in locked-in state<sup><xref ref-type="bibr" rid="R25">25</xref></sup>. The block of 20 words was repeated 5 times (RW). The last set of participants (P21 to P30) were presented with unique sentences (US). Since the reading speed varies between people, the first five participants of this set (P21 to P26) chose beforehand how much time they would like for reading each sentence. For the last five participants we modified the task design to allow the experimenter to press a key to end the speech trial once the participant finished reading the sentence. Across all tasks, each speech trial was followed by a 1-second rest period.</p></sec><sec id="S5"><title>Data acquisition</title><p id="P8">The neural data was recorded with one or two Micromed SD LTM amplifiers (Micromed S.p.A., Treviso, Italy) of 64 channels each, with a sampling rate of either 1024 Hz or 2048 Hz. The 2048 Hz signal was subsequently downsampled to 1024 Hz. The sEEG electrode shafts (Microdeep intracerebral electrodes; Dixi Medical, Beçanson, France) had a diameter of 0.8 mm, contact length of 2 mm and an inter-contact distance of 1.5 mm. The number of contacts on an electrode shaft could vary between 5 and 18. The number of implanted shafts varied between 5 and 19. The audio data was recorded with the microphone of the recording laptop (HP Probook) with a sampling rate of 48 kHz. LabStreamingLayer<sup><xref ref-type="bibr" rid="R26">26</xref></sup> and our in-house software T-Rex<sup><xref ref-type="bibr" rid="R27">27</xref></sup> was used to synchronize neural, audio and stimulus data.</p></sec><sec id="S6"><title>Electrode localization</title><p id="P9">The electrodes were localized for each participant with the img_pipe<sup><xref ref-type="bibr" rid="R28">28</xref></sup> Python package and Freesurfer. The pipeline included the co-registration between a pre-implantation T1-weighted anatomical magnetic resonance imaging (MRI) scan and a post-implantation computerized tomography (CT) scan, manual identification of two contacts from each electrode, a linear inter- and/or extrapolation of the contacts accounting for the inter-electrode distance, a manual inspection of each contacts location and a non-linear warping to an average brain (MNI152) for visualisations only. The native MRI scan was segmented using the Fischl atlas<sup><xref ref-type="bibr" rid="R29">29</xref></sup> to extract subcortical labels and parcellated using the Destrieux atlas<sup><xref ref-type="bibr" rid="R30">30</xref></sup> to extract cortical labels for each contact. There were 3807 implanted electrode contacts of which 3249 were recorded. From here on, we only focus on the recorded contacts. There were 140 unique labels, 1419 had the ‘Cerebral White Matter’ label (43.67%) and 318 the ‘Unknown’ label (9.79%). The ‘Unknown’ label was given to contacts outside of brain tissue. The remaining 46.54% of contacts were spread across a variety of locations, including cortical and subcortical regions.</p></sec><sec id="S7"><title>Re-referencing</title><p id="P10">The data is recorded with a white matter reference (WMR), a contact located in white matter that is hand-picked by the treating neurologist as a suitable reference contact. It is usually a channel with a low amplitude signal that does not show epileptic activity. However, the use of one single contact for a reference can bias the signal in other contacts, solely due to spatial differences. Moreover, the reference contact would need to be located on the implantation shaft itself for a simulation of the performance of a single shaft. Thus, we applied an electrode shaft re-reference (ESR) by subtracting the average signal from all contacts within the same shaft from each channel. This is similar to the commonly used common average reference, except that it is restricted to the same electrode shaft rather than all implanted contacts across the brain. It is noteworthy that, in most cases, there were more electrodes implanted than recorded due to a limited number of channels in the amplifiers. Thus, the neurologist had to select which contacts to record data from. Sometimes this lead to missing contacts within a shaft of electrodes.</p></sec><sec id="S8"><title>Signal processing</title><p id="P11">The neural signals were filtered to the broadband high-frequency activity (70-170 Hz) with an IIR bandpass filter (filter order 4) and the Hilbert envelope was extracted. Two IIR bandstop filters (filter order 4) were applied to attenuate the first two harmonics of the 50 Hz line noise. The filters were employed forward and backward to eliminate a potential phase-shift. The envelope was averaged in 50 ms windows with a 10 ms frameshift as the neural features. The audio signal was downsampled to 16 kHz and the Short-Term-Fourier-Transform was applied to extract audio features, also in 50 windows with a 10 ms frameshift. This procedure ensured an alignment between the labels and the neural features. An energy threshold was calculated with the maximum plus minimum value of the average energy across spectral bins, multiplied by a static value of 0.45. This threshold was applied to the audio features to extract ‘speech’ and ‘silence’ labels.</p></sec><sec id="S9"><title>Analysis scales</title><p id="P12">The data was analyzed at three different scales. In the ‘global’ scale, all recorded electrode contacts within an individual were included as features in the model. In the ‘shaft’ scale, the analysis was restricted to all contacts within a shaft. Finally, in the ‘local’ scale, each contact was analyzed individually (<xref ref-type="fig" rid="F1">Fig. 1B</xref>).</p></sec><sec id="S10"><title>Electrode selection categories</title><p id="P13">For each of the analysis scales, we additionally selectively included contacts located in specific tissue types or structures in separate analyses to determine the contribution of each. Due to a margin of error in electrode labeling, we utilized the proximal tissue density (PTD) for each contact to determine if it was located in ‘white matter’ or ‘gray matter’. The PTD score indicates the relative proportion of gray versus white matter in the 26 MRI voxels surrounding the center of the contact<sup><xref ref-type="bibr" rid="R31">31</xref></sup>, using the following formula: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>P</mml:mi><mml:mi>T</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi><mml:mi>G</mml:mi><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mi>W</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>G</mml:mi><mml:mi>M</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mi>W</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>nGM</italic> and <italic>nWM</italic> represent the number of gray matter and white matter voxels, respectively. We included subcortical areas and labeled these as gray matter. For the white matter analysis, we only included contacts with <italic>PTD</italic> &lt; 0. For the gray matter analysis, we only included contacts with <italic>PTD &gt;</italic> 0. The PTD could not be calculated for some contacts as they were surrounded entirely by non-brain tissue, these were excluded in both analyses.</p><p id="P14">Next, we used the labels from the atlases to further sort the contacts into the categories ‘cortical’, ‘subcortical’, ‘gyrus’, ‘sulcus’ and ‘gyrus-sulcus’. The ‘cortical’ category refers to those contacts labeled by the atlas as gray matter. However, these do not include the subcortical structures as they do in the general ‘gray matter’ analysis. In this case, the subcortical structures (e.g., the hippocampus) are labeled as ‘subcortical’. The ‘gyrus’ category refers to contacts located near the cortical surface (e.g., the precentral gyrus), whereas the ‘sulcus’ refers to contacts located deeper within a cortical fold (e.g., the central sulcus) which could not be reached with a standard electrode grid. The ‘gyrus-sulcus’ category refers to contacts located on the border between a gyrus and sulcus, which could not be clearly separated and therefore have this combined label in the atlas (e.g., the subcentral gyrus and sulcus). Note that some regions within the lateral fissure are gyral, even when they cannot be reached with electrode grids (e.g., the short insular gyrus).</p></sec><sec id="S11"><title>Speech detection</title><p id="P15">Neural features were enhanced with causal temporal information from non-overlapping windows up to 200 ms into the past. This means that for each channel, there were 4 additional neural features with a shifted time alignment to the labels. Only temporal windows from the past were used, as future windows could not be used in a real-time BCI. The data was split into a 10-fold cross-validation. Neural features were normalized to zero mean and unit variance using the mean and standard deviation of the training data. The same normalization was then applied to the testing data.</p><p id="P16">We employed a continuous binary classification approach to distinguish between speech and silence. This simple framework facilitates analysis across tasks and serves as a foundation for a basic BCI. The goal was not to optimize classification performance but rather to identify potential brain areas involved in speech production. For this, we utilized a Linear Discriminant Analysis (LDA) classifier. Given the class imbalances, with generally more silence than speech, we used balanced accuracy as the primary outcome metric to account for this disparity.</p><p id="P17">This analysis was computed for each of the electrode selection categories described in the previous section. Additionally, the single channel analysis was repeated for each individual temporal window from -500 ms up until +500 ms to investigate temporal dynamics.</p></sec><sec id="S12"><title>Significance threshold</title><p id="P18">We computed a distribution of chance level accuracy scores by randomly shuffling the speech/silence labels, calculating the balanced accuracy between the original and shuffled labels in 10 folds similar to the speech detection pipeline, and repeating this procedure 1,000 times for each participant. Next, we calculated the 99th percentile (<italic>α</italic> = 0.01) within each fold per participant and set the individual threshold at the maximum across folds. As the final significance threshold, we took the maximum score across participants. This procedure led to a strict threshold of 52.44%, well above the theoretical chance level of 50%.</p></sec><sec id="S13"><title>Regions-of-interest</title><p id="P19">Due to variations in sampling within and between regions as they are labeled by the atlases, we grouped results together in 9 larger regions-of-interest (ROIs) across the brain. A detailed list of the anatomical regions from the atlases included in each ROI is provided in the supplementary material (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>). Note that only the subcortical regions that were sampled were included in the list.</p></sec></sec><sec id="S14" sec-type="results"><title>Results</title><p id="P20">In this work, we recorded speech production sEEG data from 30 participants with electrode contacts covering a wide range of locations (<xref ref-type="fig" rid="F1">Fig. 1A</xref>) and structures/tissue types (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). We used a binary classification approach to decode the recorded neural activity into speech or silence labels (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Electrodes were analyzed at three levels (global, shaft and local), with additional analyses discriminating tissue types and brain structures (<xref ref-type="fig" rid="F1">Fig. 1B</xref>).</p><sec id="S15"><title>Global to local scale</title><p id="P21">Speech can be detected well above chance level for all 30 individuals (ranging between 60.13% and 93.79%) at the global scale using all recorded contacts (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). This holds true when only including contacts in gray matter as well. The accuracy of the results dropped slightly for certain individuals in the other categories and the performance goes down to below significance threshold for most individuals using only subcortical contacts. It is important to note that the lowest performing categories, subcortical and gyrus-sulcus, also had a much lower number of channels within individuals compared to the other categories (<xref ref-type="table" rid="T1">Table 1</xref>). Independent samples t-test were performed to compare certain electrode selection categories with one another. Gray matter only was not significantly different from white matter only (<italic>t</italic>(58) = 1.47, <italic>p</italic> = 0.15), nor was the sulcus significantly lower than the gyrus category (<italic>t</italic>(58) = 0.67, <italic>p</italic> = 0.51). However, the cortical category was higher than the subcortical one (<italic>t</italic>(51) = 9.20, <italic>p</italic> &lt; 0.001) and both the gyrus (<italic>t</italic>(51) = 3.25, <italic>p</italic> = 0.002) and the sulcus (<italic>t</italic>(51) = 2.49, <italic>p</italic> = 0.016) scored higher than the gyrus-sulcus category. The variation in performance between individuals could be related to the sheer amount of features included, as there was a significant correlation (Pearson’s <italic>r</italic>(58) = 0.43, <italic>p</italic> = 0.018) between the amount of features and accuracy (<xref ref-type="fig" rid="F2">Fig. 2B</xref>), and/or the specific anatomical regions that were sampled.</p><p id="P22">At the shaft scale, there are large variations between individual shaft results with the median accuracy being below the significance threshold in all of the categories (<xref ref-type="fig" rid="F2">Fig. 2C</xref>). However, the maximum scores (89, 89, 87, 88, 59, 88, 87 and 86%) are not much lower than in the global scale (94, 94, 89, 93, 61, 89, 88 and 88%). At this level, between shafts overall, there was a significant difference between the gray and white matter categories (<italic>t</italic>(602) = 2.25, <italic>p</italic> = 0.025) and between the cortical and subcortical ones (<italic>t</italic>(352) = 3.00, <italic>p</italic> = 0.003). There was no significant difference between the gyrus and sulcus (<italic>t</italic>(464) = 0.23, <italic>p</italic> = 0.82), nor either with the gyrus-sulcus category (gyrus: <italic>t</italic>(329) = 0.29, <italic>p</italic> = 0.77; sulcus: <italic>t</italic>(295) = 0.12, <italic>p</italic> = 0.91). At the shaft level, there was not a significant correlation (Pearson’s <italic>r</italic>(634) = 0.10, <italic>p</italic> = 0.07) between the size of the shaft and accuracy (<xref ref-type="fig" rid="F2">Fig. 2D</xref>), suggesting that the anatomical location of the shaft may be a more important factor than the sheer amount of features included.</p><p id="P23">In the local scale, the results are similar those at the shaft scale, with large variations between channels within categories and most channels below the significance threshold (<xref ref-type="fig" rid="F2">Fig. 2E</xref>). However, the maximum scores of single channels (88, 88, 87. 86, 59, 86, 86 and 79%) are still similar to those in the shaft and global scales. In these large groups, there are no statistically significant differences between gray matter and white matter (<italic>t</italic>(3170) = 1.95, <italic>p</italic> = 0.05), between the gyrus and sulcus (<italic>t</italic>(1105) = 0.28, <italic>p</italic> = 0.78), the gyrus versus gyrus-sulcus (<italic>t</italic>(790) = 1.22, <italic>p</italic> = 0.22), nor the sulcus versus gyrus-sulcus (<italic>t</italic>(747) = 1.00, <italic>p</italic> = 0.32). The subcortical category is significantly lower than the cortical category (<italic>t</italic>(1485) = 3.06, <italic>p</italic> = 0.002). There was a strong correlation (Pearson’s <italic>r</italic>(634) = 0.95, <italic>p</italic> &lt; 0.001) between the maximum accuracy of a single channel within a shaft and the accuracy of the complete shaft (<xref ref-type="fig" rid="F2">Fig. 2F</xref>), suggesting that one or a few channels within a shaft are driving most of the results.</p></sec><sec id="S16"><title>Anatomical contributions</title><p id="P24">We zoom into the local scale as particular anatomical regions may be more important than the generic categories presented above. Since the global and shaft scales cover multiple regions at once, we can only compare regions at the local channel level. In <xref ref-type="fig" rid="F3">Figure 3A</xref> we see the classification accuracy distribution across all significant channels. This figure indicates that the best decoding results are clustered within and around the lateral fissure, with a more focal clustering in the right hemisphere.</p><p id="P25">Out of the 137 sampled anatomical regions (excluding white matter and unknown labels), we further focus on the 23 regions that were sampled by at least 5 different participants and had at least 3 significant channels. The right anterior transverse temporal gyrus (Heschl’s gyrus) scored the highest in terms of accuracy (<xref ref-type="fig" rid="F3">Fig. 3B</xref>) and second in terms of consistency (the percentage of significant channels within that region, <xref ref-type="fig" rid="F3">Fig. 3C</xref>), the left Heschl’s gyrus was not sampled enough. The right inferior circular insula scored the second highest in accuracy and third in consistency, while only at the 9th and 14th place in the left hemisphere. The left central sulcus and subcentral gyrus-sulcus (GS) both scored high in accuracy and consistency, with over 60% significant channels in the subcentral GS. These regions, along with the precentral gyrus, were not sampled by enough participants in the right hemisphere. However, the left precentral gyrus was, and only had 2/12 significant contacts. When we look at the larger regions-of-interests (ROIs; <xref ref-type="fig" rid="F3">Fig. 3D</xref>), including regions not sampled enough individually, we see that generally the motor cortex was equally involved between the hemispheres.</p><p id="P26">We further see bilateral regions surrounding the ones described above, such as the superior and middle temporal cortex, the supramarginal cortex, the inferior frontal gyrus and other parts of the insula. The most consistent results overall come from the auditory, motor and supramarginal regions (<xref ref-type="fig" rid="F3">Fig. 3D</xref>). Note that parts of the superior temporal cortex are incorporated in the auditory cortex ROI, the remaining parts of the relative large superior temporal cortex ROI are therefore not very consistently involved. The insular ROI follows, as it was particularly the inferior part that scored high in accuracy and consistency. It is worth noting that, while we look at the relative number of significant channels, the channels were not sampled exactly uniformly within regions and between hemispheres, which limits the interpretability of comparisons.</p></sec><sec id="S17"><title>Temporal dynamics</title><p id="P27">We explored how the spatial distribution and number of significant channels varied between temporally misaligned features and tasks (<xref ref-type="fig" rid="F4">Fig. 4</xref>). The results were grouped in five larger time segments (<xref ref-type="fig" rid="F4">Fig. 4A-E</xref>), representing time windows well before alignment (-500 until -300 ms), just prior to alignment (-300 until -100 ms), surrounding alignment (-100 until +100 ms), just after alignment (+100 until +300 ms) and well after alignment (+300 until +500 ms). Neural features just prior to alignment resulted in the largest number of channels significantly detecting speech (<xref ref-type="fig" rid="F4">Fig. 4F</xref>), with a steeper drop-off after than before alignment. The variation between tasks reflects the variation in the length of the speech trials (short words versus long sentences). All three tasks engaged peri-sylvian, temporal and sensorimotor areas, while only the sentences (US) task engaged frontal areas.</p></sec></sec><sec id="S18" sec-type="discussion"><title>Discussion</title><p id="P28">Speech detection was performed at three scales of analysis and eight electrode selection categories. Speech was reliably detected for all thirty individuals using all available data within individuals (global scale), but the performance varied greatly between electrode shafts (shaft scale) and individual channels (local scale). While at a global scale the performance was correlated with the amount of included channels, this was not the case at the shaft scale, but instead correlated strongly with the accuracy of the best individual channel along the shaft. These results combined suggest that the location of individuals channels is important, while a combination of multiple distributed regions may be beneficial as well. Previous research has indeed showed that overtly and covertly produced speech can be reconstructed well from distributed regions in offline<sup><xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref></sup> and in real-time<sup><xref ref-type="bibr" rid="R33">33</xref></sup> decoding.</p><p id="P29">Overall, the gray matter category scored slightly higher than the white matter category at the shaft scale. At the global scale the difference was not significant likely due to difference in the number of data-points. Whilst lower than gray matter, the nonetheless high accuracy results from contacts located in white matter indicates that we may be able to decode valuable information directly from white matter tracts. This activity may reflect volume conduction from a combination of nearby and distant gray matter regions<sup><xref ref-type="bibr" rid="R31">31</xref></sup>. For example, direct electrical stimulation of the anterior arcuate fasciculus tract, superior to the insula, has been found to induce thoughts or alter the conscious awareness of thoughts, similar to stimulation of the posterior parietal cortex itself<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. Our results are in line with previous decoding studies with sEEG that have also found contributions from white matter contacts<sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref></sup>. However, in our work, these findings could, at least in part, have been due to spreading of information from cortical contacts within the same electrode shaft due to re-referencing. It is also a challenge to locate the exact white matter tracts, requiring additional anatomical analyses and/or diffusion-tensor imaging, and therefore beyond the scope of our current work.</p><p id="P30">There were clear differences between the cortical and subcortical categories in all analysis scales. While there were a few significant channels in subcortical areas, the accuracy did not reach nearly as high as in cortical and even white matter regions. Previous studies have found language-related functions in the hippocampus<sup><xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R38">38</xref></sup> and basal ganglia<sup><xref ref-type="bibr" rid="R39">39</xref></sup> and other subcortical areas through a large meta-analysis<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. Studies reveal specialized functions of these areas, such as semantic integration<sup><xref ref-type="bibr" rid="R37">37</xref></sup> and syntactic processing<sup><xref ref-type="bibr" rid="R39">39</xref></sup>, and therefore may be too subtle for general speech detection as in the current study. Alternatively, in electrophysiology, we may need to investigate these areas with lower frequencies<sup><xref ref-type="bibr" rid="R37">37</xref></sup> or even single cell recordings<sup><xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R41">41</xref></sup>.</p><p id="P31">Within the cortical channels, there was no statistically significant difference between the gyrus and sulcus categories at any of the analysis scales across the brain, despite general differences in cytoarchitecture and function<sup><xref ref-type="bibr" rid="R42">42</xref></sup>. Even within the local scale, only looking at significant contacts, we see both gyral and sulcus regions detecting speech. However, the gyri are more often mentioned in the speech decoding literature as sulci cannot be sampled with the commonly used electrocorticography grids. Note that, in this work, gyri do not necessarily mean they can be sampled with electrode grids on the surface of the brain, as they can be located within the lateral fissure. A previous sEEG study did analyze differences in speech detection accuracy across the level of depth along shafts and found generally higher accuracies at the surface level with overt and mouthed speech, but not with imagined speech<sup><xref ref-type="bibr" rid="R15">15</xref></sup>. These results suggest that the deeper regions may be just as important for an imaginary-based speech neuroprosthesis.</p><p id="P32">Next, we dived further into significant anatomical regions at the local scale. The overall spatial distribution across the brain corresponds with the core language network<sup><xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref></sup> and its bilaterality is consistent with previous sEEG speech production studies<sup><xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R22">22</xref></sup>. At the top positions in terms of both accuracy and consistency are Heschl’s gyrus (the auditory cortex), the inferior insular sulcus and the (sub)central sulcus. While named gyrus, Heschl’s gyrus is also nestled deeper within the lateral fissure. Interestingly, the precentral gyrus (primary motor cortex) was sampled plenty in the left hemisphere, but only showed 2/12 significant contacts. This is in stark contrast with the sulcal parts of the motor cortex, suggesting that reaching the sulcal depth may be beneficial for motor-based BCIs<sup><xref ref-type="bibr" rid="R43">43</xref></sup>.</p><p id="P33">We further looked at larger regions-of-interest to generate a better overview across regions, including the individual areas that were left out of <xref ref-type="fig" rid="F3">Figure 3B and C</xref> due to not being sampled by plenty of different participants or not containing plenty of significant contacts. Whilst sampling of the motor cortex was not ideal, the auditory areas of the superior temporal lobe showed the most consistent results, despite known suppression of regions within the auditory cortex during self-generated speech<sup><xref ref-type="bibr" rid="R44">44</xref></sup>. The supramarginal cortex may also be a good alternative for a speech neuroprosthesis, as was previously shown with a micro-electrode array even for imagined speech<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. There was limited contribution from the inferior frontal cortex, which may be due to the reading task not requiring spontaneous speech planning<sup><xref ref-type="bibr" rid="R46">46</xref></sup>. The reading could also explain the relatively stronger occipital than frontal involvement overall. However, when we split the results between the three tasks for temporal dynamics, we do note that only the most complex (sentences) task reached significance in the frontal areas, likely engaging more higher-order executive control<sup><xref ref-type="bibr" rid="R47">47</xref></sup> than single word reading.</p><p id="P34">As most of the significant regions are also known to be activated during speech or sound perception<sup><xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R48">48</xref></sup>, the question is whether they will remain activated without or with a limited amount of overt speech. Previous research has indeed found similar, albeit more attenuated, activation patterns with purely imaged speech, compared to overt or mouthed speech<sup><xref ref-type="bibr" rid="R15">15</xref></sup>. This even applies to auditory regions<sup><xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref></sup> as they may be involved in the internal representation of speech<sup><xref ref-type="bibr" rid="R49">49</xref></sup>. However, responses to perceived speech would be another problem for a practical speech BCI. This is even the case for a speech neuroprosthesis on the motor cortex, with the potential of leading to false positives through speech perception<sup><xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref></sup> or even through unintentional inner speech<sup><xref ref-type="bibr" rid="R52">52</xref></sup>. Ensuring executive control is an important ethical aspect in the development of speech neuroprostheses<sup><xref ref-type="bibr" rid="R53">53</xref></sup> and efforts must therefore be made to mitigate false positives.</p><p id="P35">In the sensorimotor cortex, differential spatial patterns between produced speech, perceived speech and rest can be used as an ‘intended’ speech detector<sup><xref ref-type="bibr" rid="R51">51</xref></sup>. A similar spatial decoder may also be possible in the superior temporal cortex, considering differential patterns found between produced and perceived speech here as well<sup><xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R44">44</xref></sup>, with a gradient of more anterior regions preferring self-generated and posterior regions preferring perceived speech<sup><xref ref-type="bibr" rid="R22">22</xref></sup>. The hippocampus may also be interesting as it is found to have a stronger coupling with auditory cortex during self-generated speech versus hearing one’s own pre-recorded speech and is thought to have a predictive role in speech production<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. However, we did not see hippocampal involvement in our current work. The posterior insula, similar to the area we see in this work, is also found to be more activated during self-generated than perceived speech<sup><xref ref-type="bibr" rid="R54">54</xref></sup>. However, the insula may be more related to the coordination of speech with autonomic functions and may therefore not be present during purely imagined speech<sup><xref ref-type="bibr" rid="R55">55</xref></sup>. In our earlier work, we have also shown that speech is better reconstructed from this region using neural features during speech articulation rather than before, unlike the other regions<sup><xref ref-type="bibr" rid="R17">17</xref></sup>. In our current work, while the largest amount of significant channels overall were based on neural features just prior to the speech features, we could not distinguish pre- and post-articulation signals due to the continuous nature of our analysis and different temporal dynamics between our speech tasks. Nonetheless, a speech detector from within or between such regions that can additionally differentiate between self-generated and perceived speech may be used to mitigate the false positive effect from speech perception, as well as as on/off switch that a participant can control to mitigate the potential effect from unintended inner thoughts<sup><xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref></sup>.</p></sec><sec id="S19" sec-type="conclusions"><title>Conclusion</title><p id="P36">This study provides a comprehensive overview of speech decoding throughout the brain, and underscores the potential of regions beyond the cortical surface for speech neuroprostheses. We showed that white matter contacts provided strong decoding accuracy. Sulci regions, which cannot be measured with ECoG or MEA, yielded decoding results on-par with gyral locations. Additional to the usually recorded left hemisphere, we found that locations on both hemispheres provided very high accuracies, with the most consistent results derived from auditory regions, followed by the motor and supramarginal cortices. Frontal areas provided significant decoding results only in the sentence task, indicating the importance of different types of speech in the training data. Overall, these findings indicate that there is large potential for speech neuroprostheses even when the motor cortex cannot be targeted.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS201643-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d29aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S20"><title>Acknowledgements</title><p>This publication is part of the project INTENSE (with project number 17619 of the research programme NWO Crossover Programme) which is (partly) financed by the Dutch Research Council (NWO). C.H. acknowledges funding by the Kavli Foundation.</p></ack><sec id="S21" sec-type="data-availability"><title>Data availability</title><p id="P37">The raw data is publicly available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17605/OSF.IO/AK3DP">https://doi.org/10.17605/OSF.IO/AK3DP</ext-link> and code can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/neuralinterfacinglab/SpeechTargets">https://github.com/neuralinterfacinglab/SpeechTargets</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P38"><bold>Author contributions statement</bold></p><p id="P39">C.H. and P.K. and M.V. designed the experiments, C.H, M.C.O. and M.V. collected the data, M.V. ran the analyses, M.V. wrote the manuscript. All authors reviewed the manuscript and declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva</surname><given-names>AB</given-names></name><name><surname>Littlejohn</surname><given-names>KT</given-names></name><name><surname>Liu</surname><given-names>JR</given-names></name><name><surname>Moses</surname><given-names>DA</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>The speech neuroprosthesis</article-title><source>Nat Rev Neurosci</source><year>2024</year><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC11540306</pub-id><pub-id pub-id-type="pmid">38745103</pub-id><pub-id pub-id-type="doi">10.1038/s41583-024-00819-9</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Indefrey</surname><given-names>P</given-names></name></person-group><article-title>The spatial and temporal signatures of word production components: a critical update</article-title><source>Front psychology</source><year>2011</year><volume>2</volume><fpage>255</fpage><pub-id pub-id-type="pmcid">PMC3191502</pub-id><pub-id pub-id-type="pmid">22016740</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00255</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertrich</surname><given-names>I</given-names></name><name><surname>Dietrich</surname><given-names>S</given-names></name><name><surname>Ackermann</surname><given-names>H</given-names></name></person-group><article-title>The margins of the language network in the brain</article-title><source>Front Commun</source><year>2020</year><volume>5</volume><elocation-id>519955</elocation-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moses</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Neuroprosthesis for decoding speech in a paralyzed person with anarthria</article-title><source>New Engl J Medicine</source><year>2021</year><volume>385</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="pmcid">PMC8972947</pub-id><pub-id pub-id-type="pmid">34260835</pub-id><pub-id pub-id-type="doi">10.1056/NEJMoa2027540</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzger</surname><given-names>SL</given-names></name><etal/></person-group><article-title>A high-performance neuroprosthesis for speech decoding and avatar control</article-title><source>Nature</source><year>2023</year><volume>620</volume><fpage>1037</fpage><lpage>1046</lpage><pub-id pub-id-type="pmcid">PMC10826467</pub-id><pub-id pub-id-type="pmid">37612505</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06443-4</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willett</surname><given-names>FR</given-names></name><etal/></person-group><article-title>A high-performance speech neuroprosthesis</article-title><source>Nature</source><year>2023</year><volume>620</volume><fpage>1031</fpage><lpage>1036</lpage><pub-id pub-id-type="pmcid">PMC10468393</pub-id><pub-id pub-id-type="pmid">37612500</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06377-x</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>S</given-names></name><etal/></person-group><article-title>Stable decoding from a speech bci enables control for an individual with als without recalibration for 3 months</article-title><source>Adv Sci</source><year>2023</year><volume>10</volume><elocation-id>2304853</elocation-id><pub-id pub-id-type="pmcid">PMC10724434</pub-id><pub-id pub-id-type="pmid">37875404</pub-id><pub-id pub-id-type="doi">10.1002/advs.202304853</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angrick</surname><given-names>M</given-names></name><etal/></person-group><article-title>Online speech synthesis using a chronically implanted brain–computer interface in an individual with als</article-title><source>Sci reports</source><year>2024</year><volume>14</volume><elocation-id>9617</elocation-id><pub-id pub-id-type="pmcid">PMC11053081</pub-id><pub-id pub-id-type="pmid">38671062</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-60277-2</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Card</surname><given-names>NS</given-names></name><etal/></person-group><article-title>An accurate and rapidly calibrating speech neuroprosthesis</article-title><source>New Engl J Medicine</source><year>2024</year><volume>391</volume><fpage>609</fpage><lpage>618</lpage><pub-id pub-id-type="pmcid">PMC11328962</pub-id><pub-id pub-id-type="pmid">39141853</pub-id><pub-id pub-id-type="doi">10.1056/NEJMoa2314132</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Makin</surname><given-names>TR</given-names></name><name><surname>McDougle</surname><given-names>SD</given-names></name></person-group><article-title>Going beyond primary motor cortex to improve brain–computer interfaces</article-title><source>Trends neurosciences</source><year>2022</year><volume>45</volume><fpage>176</fpage><lpage>183</lpage><pub-id pub-id-type="pmid">35078639</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ragagnin</surname><given-names>AM</given-names></name><name><surname>Shadfar</surname><given-names>S</given-names></name><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Jamali</surname><given-names>MS</given-names></name><name><surname>Atkin</surname><given-names>JD</given-names></name></person-group><article-title>Motor neuron susceptibility in als/ftd</article-title><source>Front neuroscience</source><year>2019</year><volume>13</volume><fpage>532</fpage><pub-id pub-id-type="pmcid">PMC6610326</pub-id><pub-id pub-id-type="pmid">31316328</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2019.00532</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamamé</surname><given-names>CM</given-names></name><name><surname>Alario</surname><given-names>F-X</given-names></name><name><surname>Llorens</surname><given-names>A</given-names></name><name><surname>Liégeois-Chauvel</surname><given-names>C</given-names></name><name><surname>Trébuchon-Da Fonseca</surname><given-names>A</given-names></name></person-group><article-title>High frequency gamma activity in the left hippocampus predicts visual object naming performance</article-title><source>Brain language</source><year>2014</year><volume>135</volume><fpage>104</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">25016093</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tankus</surname><given-names>A</given-names></name><etal/></person-group><article-title>A speech neuroprosthesis in the frontal lobe and hippocampus: Decoding high-frequency activity into phonemes</article-title><source>Neurosurgery</source><year>2024</year><pub-id pub-id-type="pmid">38934637</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Afif</surname><given-names>A</given-names></name><name><surname>Minotti</surname><given-names>L</given-names></name><name><surname>Kahane</surname><given-names>P</given-names></name><name><surname>Hoffmann</surname><given-names>D</given-names></name></person-group><article-title>Middle short gyrus of the insula implicated in speech production: intracerebral electric stimulation of patients with epilepsy</article-title><source>Epilepsia</source><year>2010</year><volume>51</volume><fpage>206</fpage><lpage>213</lpage><pub-id pub-id-type="pmid">19694793</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soroush</surname><given-names>PZ</given-names></name><etal/></person-group><article-title>The nested hierarchy of overt, mouthed, and imagined speech activity evident in intracranial recordings</article-title><source>NeuroImage</source><year>2023</year><volume>269</volume><elocation-id>119913</elocation-id><pub-id pub-id-type="pmid">36731812</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname><given-names>C</given-names></name><name><surname>Krusienski</surname><given-names>DJ</given-names></name><name><surname>Kubben</surname><given-names>P</given-names></name></person-group><article-title>The potential of stereotactic-eeg for brain-computer interfaces: current progress and future directions</article-title><source>Front neuroscience</source><year>2020</year><volume>14</volume><fpage>123</fpage><pub-id pub-id-type="pmcid">PMC7056827</pub-id><pub-id pub-id-type="pmid">32174810</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00123</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verwoert</surname><given-names>M</given-names></name><etal/></person-group><article-title>Whole-brain dynamics of articulatory, acoustic and semantic speech representations</article-title><source>bioRxiv</source><year>2024</year><fpage>2024</fpage><lpage>08</lpage></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>TM</given-names></name><etal/></person-group><article-title>Decoding articulatory and phonetic components of naturalistic continuous speech from the distributed language network</article-title><source>J Neural Eng</source><year>2023</year><volume>20</volume><elocation-id>046030</elocation-id><pub-id pub-id-type="pmid">37487487</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ogar</surname><given-names>J</given-names></name><name><surname>Slama</surname><given-names>H</given-names></name><name><surname>Dronkers</surname><given-names>N</given-names></name><name><surname>Amici</surname><given-names>S</given-names></name><name><surname>Luisa Gorno-Tempini</surname><given-names>M</given-names></name></person-group><article-title>Apraxia of speech: an overview</article-title><source>Neurocase</source><year>2005</year><volume>11</volume><fpage>427</fpage><lpage>432</lpage><pub-id pub-id-type="pmid">16393756</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><year>2014</year><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="pmcid">PMC4350233</pub-id><pub-id pub-id-type="pmid">24482117</pub-id><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>K</given-names></name><name><surname>Grayden</surname><given-names>DB</given-names></name><name><surname>Cook</surname><given-names>MJ</given-names></name><name><surname>Vogrin</surname><given-names>S</given-names></name><name><surname>Goodarzy</surname><given-names>F</given-names></name></person-group><source>Identification of discriminative features for decoding overt and imagined speech using stereotactic electroencephalography</source><conf-name>2021 9th International Winter Conference on Brain-Computer Interface (BCI)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2021</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourski</surname><given-names>KV</given-names></name><etal/></person-group><article-title>Electrophysiology of the human superior temporal sulcus during speech processing</article-title><source>Cereb Cortex</source><year>2021</year><volume>31</volume><fpage>1131</fpage><lpage>1148</lpage><pub-id pub-id-type="pmcid">PMC7786351</pub-id><pub-id pub-id-type="pmid">33063098</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhaa281</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herff</surname><given-names>C</given-names></name><name><surname>Verwoert</surname><given-names>M</given-names></name></person-group><article-title>Dataset of speech production in intracranial electroencephalography</article-title><source>Open Science Framework</source><year>2022</year><pub-id pub-id-type="pmcid">PMC9307753</pub-id><pub-id pub-id-type="pmid">35869138</pub-id><pub-id pub-id-type="doi">10.1038/s41597-022-01542-9</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verwoert</surname><given-names>M</given-names></name><etal/></person-group><article-title>Dataset of speech production in intracranial electroencephalography</article-title><source>Sci Data</source><year>2022</year><volume>9</volume><fpage>434</fpage><pub-id pub-id-type="doi">10.1038/s41597-022-01542-9</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Fully implanted brain–computer interface in a locked-in patient with als</article-title><source>New Engl J Medicine</source><year>2016</year><volume>375</volume><fpage>2060</fpage><lpage>2066</lpage><pub-id pub-id-type="pmcid">PMC5326682</pub-id><pub-id pub-id-type="pmid">27959736</pub-id><pub-id pub-id-type="doi">10.1056/NEJMoa1608085</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Kothe</surname><given-names>C</given-names></name></person-group><source>Lab streaming layer (lsl)</source><year>2014</year><volume>26</volume><comment>2015. <ext-link ext-link-type="uri" xlink:href="https://github.com/sccn/labstreaminglayer">https://github.com/sccn/labstreaminglayer</ext-link></comment></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amigó-Vega</surname><given-names>J</given-names></name><etal/></person-group><article-title>The easy and versatile neural recording platform (t-rex): Design and development study</article-title><source>JMIR Neurotechnology</source><year>2023</year><volume>2</volume><elocation-id>e47881</elocation-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Chang</surname><given-names>DL</given-names></name><name><surname>Lee</surname><given-names>MB</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Semi-automated anatomical labeling and inter-subject warping of high-density intracranial recording electrodes in electrocorticography</article-title><source>Front Neuroinformatics</source><year>2017</year><volume>11</volume><fpage>62</fpage><pub-id pub-id-type="pmcid">PMC5671481</pub-id><pub-id pub-id-type="pmid">29163118</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00062</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name><etal/></person-group><article-title>Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain</article-title><source>Neuron</source><year>2002</year><volume>33</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="pmid">11832223</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname><given-names>C</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>A</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>NeuroImage</source><year>2010</year><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC2937159</pub-id><pub-id pub-id-type="pmid">20547229</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>MR</given-names></name><etal/></person-group><article-title>Evaluation of cortical local field potential diffusion in stereotactic electro-encephalography recordings: A glimpse on white matter signal</article-title><source>NeuroImage</source><year>2017</year><volume>147</volume><fpage>219</fpage><lpage>232</lpage><pub-id pub-id-type="pmid">27554533</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>S</given-names></name><etal/></person-group><article-title>Decoding spectrotemporal features of overt and covert speech from the human cortex</article-title><source>Front neuroengineering</source><year>2014</year><volume>7</volume><fpage>14</fpage><pub-id pub-id-type="pmcid">PMC4034498</pub-id><pub-id pub-id-type="pmid">24904404</pub-id><pub-id pub-id-type="doi">10.3389/fneng.2014.00014</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angrick</surname><given-names>M</given-names></name><etal/></person-group><article-title>Real-time synthesis of imagined speech processes from minimally invasive recordings of neural activity</article-title><source>Commun biology</source><year>2021</year><volume>4</volume><elocation-id>1055</elocation-id><pub-id pub-id-type="pmcid">PMC8460739</pub-id><pub-id pub-id-type="pmid">34556793</pub-id><pub-id pub-id-type="doi">10.1038/s42003-021-02578-0</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koubeissi</surname><given-names>MZ</given-names></name><name><surname>Fernandez-Baca Vaca</surname><given-names>G</given-names></name><name><surname>Maciunas</surname><given-names>R</given-names></name><name><surname>Stephani</surname><given-names>C</given-names></name></person-group><article-title>A white matter tract mediating awareness of speech</article-title><source>Neurology</source><year>2016</year><volume>86</volume><fpage>177</fpage><lpage>179</lpage><pub-id pub-id-type="pmid">26643545</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>G</given-names></name><etal/></person-group><article-title>Detection of human white matter activation and evaluation of its function in movement decoding using stereo-electroencephalography (seeg)</article-title><source>J neural engineering</source><year>2021</year><volume>18</volume><elocation-id>0460c6</elocation-id><pub-id pub-id-type="pmid">34284361</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouton</surname><given-names>C</given-names></name><etal/></person-group><article-title>Decoding neural activity in sulcal and white matter areas of the brain to accurately predict individual finger movement and tactile stimuli of the human hand</article-title><source>Front Neurosci</source><year>2021</year><volume>15</volume><elocation-id>699631</elocation-id><pub-id pub-id-type="pmcid">PMC8415782</pub-id><pub-id pub-id-type="pmid">34483823</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2021.699631</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piai</surname><given-names>V</given-names></name><etal/></person-group><article-title>Direct brain recordings reveal hippocampal rhythm underpinnings of language processing</article-title><source>Proc Natl Acad Sci</source><year>2016</year><volume>113</volume><fpage>11366</fpage><lpage>11371</lpage><pub-id pub-id-type="pmcid">PMC5056038</pub-id><pub-id pub-id-type="pmid">27647880</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1603312113</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>V</given-names></name><name><surname>Waldorp</surname><given-names>L</given-names></name><name><surname>Christoffels</surname><given-names>I</given-names></name></person-group><article-title>Hippocampus plays a role in speech feedback processing</article-title><source>NeuroImage</source><year>2020</year><volume>223</volume><elocation-id>117319</elocation-id><pub-id pub-id-type="pmid">32882376</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thibault</surname><given-names>S</given-names></name><etal/></person-group><article-title>Tool use and language share syntactic processes and neural patterns in the basal ganglia</article-title><source>Science</source><year>2021</year><volume>374</volume><elocation-id>eabe0874</elocation-id><pub-id pub-id-type="pmid">34762470</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turker</surname><given-names>S</given-names></name><name><surname>Kuhnke</surname><given-names>P</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Caspers</surname><given-names>S</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name></person-group><article-title>Cortical, subcortical, and cerebellar contributions to language processing: A meta-analytic review of 403 neuroimaging experiments</article-title><source>Psychol Bull</source><year>2023</year><pub-id pub-id-type="pmid">37768610</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijksterhuis</surname><given-names>DE</given-names></name><etal/></person-group><article-title>Pronouns reactivate conceptual representations in human hippocampal neurons</article-title><source>Science</source><year>2024</year><volume>385</volume><fpage>1478</fpage><lpage>1484</lpage><pub-id pub-id-type="pmid">39325896</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Kendrick</surname><given-names>KM</given-names></name><name><surname>Liu</surname><given-names>T</given-names></name></person-group><article-title>Fundamental functional differences between gyri and sulci: implications for brain function, cognition, and behavior</article-title><source>Psychoradiology</source><year>2021</year><volume>1</volume><fpage>23</fpage><lpage>41</lpage><pub-id pub-id-type="pmcid">PMC10939337</pub-id><pub-id pub-id-type="pmid">38665307</pub-id><pub-id pub-id-type="doi">10.1093/psyrad/kkab002</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>MA</given-names></name><etal/></person-group><article-title>A motor association area in the depths of the central sulcus</article-title><source>Nat neuroscience</source><year>2023</year><volume>26</volume><fpage>1165</fpage><lpage>1169</lpage><pub-id pub-id-type="pmcid">PMC10322697</pub-id><pub-id pub-id-type="pmid">37202552</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01346-z</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname><given-names>A</given-names></name><etal/></person-group><article-title>Single-trial speech suppression of auditory cortex activity in humans</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>16643</fpage><lpage>16650</lpage><pub-id pub-id-type="pmcid">PMC3010242</pub-id><pub-id pub-id-type="pmid">21148003</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1809-10.2010</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandelt</surname><given-names>SK</given-names></name><etal/></person-group><article-title>Representation of internal speech by single neurons in human supramarginal gyrus</article-title><source>Nat human behaviour</source><year>2024</year><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC11199147</pub-id><pub-id pub-id-type="pmid">38740984</pub-id><pub-id pub-id-type="doi">10.1038/s41562-024-01867-y</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>Kovach</surname><given-names>CK</given-names></name><name><surname>Howard</surname><given-names>MA</given-names><suffix>III</suffix></name><name><surname>Greenlee</surname><given-names>JD</given-names></name><name><surname>Long</surname><given-names>MA</given-names></name></person-group><article-title>A speech planning network for interactive language use</article-title><source>Nature</source><year>2022</year><volume>602</volume><fpage>117</fpage><lpage>122</lpage><pub-id pub-id-type="pmcid">PMC9990513</pub-id><pub-id pub-id-type="pmid">34987226</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-04270-z</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourguignon</surname><given-names>NJ</given-names></name></person-group><article-title>A rostro-caudal axis for language in the frontal lobe: the role of executive control in speech production</article-title><source>Neurosci &amp; Biobehav Rev</source><year>2014</year><volume>47</volume><fpage>431</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">25305636</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berezutskaya</surname><given-names>J</given-names></name><name><surname>Freudenburg</surname><given-names>ZV</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><article-title>Neural tuning to low-level features of speech throughout the perisylvian cortex</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>7906</fpage><lpage>7920</lpage><pub-id pub-id-type="pmcid">PMC6596904</pub-id><pub-id pub-id-type="pmid">28716965</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0238-17.2017</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>AK</given-names></name><etal/></person-group><article-title>A special role for the right posterior superior temporal sulcus during speech production</article-title><source>Neuroimage</source><year>2019</year><volume>203</volume><elocation-id>116184</elocation-id><pub-id pub-id-type="pmcid">PMC6876272</pub-id><pub-id pub-id-type="pmid">31520744</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116184</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name></person-group><article-title>The dynamic and task-dependent representational transformation between the motor and sensory systems during speech production</article-title><source>Cogn Neurosci</source><year>2020</year><volume>11</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="pmid">32720845</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schippers</surname><given-names>A</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Freudenburg</surname><given-names>ZV</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><article-title>Don’t put words in my mouth: Speech perception can generate false positive activation of a speech bci</article-title><source>medRxiv</source><year>2024</year></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunz</surname><given-names>EM</given-names></name><etal/></person-group><article-title>Representation of verbal thought in motor cortex and implications for speech neuroprostheses</article-title><source>bioRxiv</source><year>2024</year><fpage>2024</fpage><lpage>10</lpage></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Stuijvenberg</surname><given-names>OC</given-names></name><name><surname>Samlal</surname><given-names>DP</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Broekman</surname><given-names>M</given-names></name><name><surname>Jongsma</surname><given-names>KR</given-names></name></person-group><article-title>The ethical significance of user-control in ai-driven speech-bcis: a narrative review</article-title><source>Front Hum Neurosci</source><year>2024</year><volume>18</volume><elocation-id>1420334</elocation-id><pub-id pub-id-type="pmcid">PMC11240287</pub-id><pub-id pub-id-type="pmid">39006157</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2024.1420334</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolnough</surname><given-names>O</given-names></name><name><surname>Forseth</surname><given-names>KJ</given-names></name><name><surname>Rollo</surname><given-names>PS</given-names></name><name><surname>Tandon</surname><given-names>N</given-names></name></person-group><article-title>Uncovering the functional anatomy of the human insula during speech</article-title><source>elife</source><year>2019</year><volume>8</volume><elocation-id>e53086</elocation-id><pub-id pub-id-type="pmcid">PMC6941893</pub-id><pub-id pub-id-type="pmid">31852580</pub-id><pub-id pub-id-type="doi">10.7554/eLife.53086</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackermann</surname><given-names>H</given-names></name><name><surname>Riecker</surname><given-names>A</given-names></name></person-group><article-title>The contribution (s) of the insula to speech production: a review of the clinical and functional imaging literature</article-title><source>Brain Struct Funct</source><year>2010</year><volume>214</volume><fpage>419</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">20512374</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Graphical overview of the methods.</title><p><bold>A)</bold> Locations of recorded electrode contacts in an averaged brain from a top view. Colors differentiate the 30 participants. <bold>B)</bold> Graphical illustration of the analysis scales and electrode selection categories within a coronal brain slice. The ‘all’ category is not displayed as it includes all contacts and no specific selection. <bold>C)</bold> Example of how the spectrogram of the recording speech audio was used to extract the ‘speech’ and ‘silence’ labels used for the decoders.</p></caption><graphic xlink:href="EMS201643-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Distribution of balanced accuracy results within electrode selection categories across the analysis scales and correlations.</title><p><bold>A)</bold> Global scale balanced accuracy results. <bold>B)</bold> Correlation between global accuracy and number of features. <bold>C)</bold> Shaft scale balanced accuracy results. <bold>D)</bold> Correlation between shaft accuracy and shaft size. <bold>E)</bold> Local scale balanced accuracy results. <bold>F)</bold> Correlation between maximum channel accuracy within a shaft and accuracy of the complete shaft. In A-C-E the individual dots represent the averaged balanced accuracy per individual, shaft or channel, respectively. The box plots show the median and inter-quartile range, whiskers show the maximum range excluding outliers. The dashed gray line represents the significance threshold in A-C-E and the best-fit linear trend, calculated using a least-squares linear regression, in B-D-F. *<italic>p</italic> &lt; 0.05, **<italic>p</italic> &lt; 0.01, ***<italic>p</italic> &lt; 0.001.</p></caption><graphic xlink:href="EMS201643-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Anatomical dissemination at the local scale.</title><p><bold>A)</bold> Spatial distribution of significant balanced accuracy scores across channels in an averaged brain. The colorbar ranges from the significance threshold (52%) to ≥ 80%. Smaller gray dots represent insignificant channels. <bold>B)</bold> Distribution of balanced accuracy scores within anatomical regions, sorted by average accuracies. The color of the bar indicates the hemisphere. <bold>C)</bold> Percentage of significant channels within anatomical regions, sorted by percentage. The text above the bar describes the amount of significant and total amount of channels within that region. The color of the bar indicates the hemisphere. <bold>D)</bold> Percentage of significant channels within larger regions-of-interest within each hemisphere, sorted by the average percentage between the two hemispheres. G = gyrus, S = sulcus, GS = gyrus-sulcus.</p></caption><graphic xlink:href="EMS201643-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Temporal dynamics.</title><p><bold>A-E)</bold> Significant channel locations with colors representing the three tasks, divided into five time segments. Smaller gray dots represent non-significant channels. <bold>F)</bold> Number of significant channels across time-frames from all (brain-wide) contacts. UW = unique words task, RW = repeated words task, US = unique sentences task.</p></caption><graphic xlink:href="EMS201643-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Demographic information, the performed task(s), the total number of channels and shafts that were recorded and the total number of channels available in the electrode selection categories for each participant. M = male; F = female; UW = unique words, RW = repeated words, US = unique sentences, GM = gray matter, WM = white matter, SC = subcortical, GS = gyrus-sulcus.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="middle">Participant</th><th align="center" valign="middle">Age</th><th align="center" valign="middle">Sex</th><th align="center" valign="middle">Task</th><th align="center" valign="middle">Channels</th><th align="center" valign="middle">Shafts</th><th align="center" valign="middle">GM</th><th align="center" valign="middle">WM</th><th align="center" valign="middle">Cortical</th><th align="center" valign="middle">SC</th><th align="center" valign="middle">Gyrus</th><th align="center" valign="middle">Sulcus</th><th align="center" valign="middle">GS</th></tr></thead><tbody><tr><td align="left" valign="middle">P01</td><td align="center" valign="middle">20</td><td align="center" valign="middle">F</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">12</td><td align="center" valign="middle">67</td><td align="center" valign="middle">53</td><td align="center" valign="middle">48</td><td align="center" valign="middle">1</td><td align="center" valign="middle">16</td><td align="center" valign="middle">24</td><td align="center" valign="middle">8</td></tr><tr><td align="left" valign="middle">P02</td><td align="center" valign="middle">43</td><td align="center" valign="middle">M</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">19</td><td align="center" valign="middle">24</td><td align="center" valign="middle">101</td><td align="center" valign="middle">17</td><td align="center" valign="middle">13</td><td align="center" valign="middle">6</td><td align="center" valign="middle">11</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P03</td><td align="center" valign="middle">24</td><td align="center" valign="middle">M</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">15</td><td align="center" valign="middle">74</td><td align="center" valign="middle">50</td><td align="center" valign="middle">53</td><td align="center" valign="middle">10</td><td align="center" valign="middle">30</td><td align="center" valign="middle">17</td><td align="center" valign="middle">6</td></tr><tr><td align="left" valign="middle">P04</td><td align="center" valign="middle">46</td><td align="center" valign="middle">F</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">115</td><td align="center" valign="middle">12</td><td align="center" valign="middle">72</td><td align="center" valign="middle">33</td><td align="center" valign="middle">56</td><td align="center" valign="middle">1</td><td align="center" valign="middle">25</td><td align="center" valign="middle">25</td><td align="center" valign="middle">6</td></tr><tr><td align="left" valign="middle">P05</td><td align="center" valign="middle">50</td><td align="center" valign="middle">F</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">60</td><td align="center" valign="middle">5</td><td align="center" valign="middle">34</td><td align="center" valign="middle">25</td><td align="center" valign="middle">17</td><td align="center" valign="middle">16</td><td align="center" valign="middle">8</td><td align="center" valign="middle">9</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P06</td><td align="center" valign="middle">16</td><td align="center" valign="middle">M</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">14</td><td align="center" valign="middle">90</td><td align="center" valign="middle">26</td><td align="center" valign="middle">82</td><td align="center" valign="middle">-</td><td align="center" valign="middle">39</td><td align="center" valign="middle">34</td><td align="center" valign="middle">9</td></tr><tr><td align="left" valign="middle">P07</td><td align="center" valign="middle">47</td><td align="center" valign="middle">M</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">11</td><td align="center" valign="middle">62</td><td align="center" valign="middle">62</td><td align="center" valign="middle">49</td><td align="center" valign="middle">10</td><td align="center" valign="middle">28</td><td align="center" valign="middle">16</td><td align="center" valign="middle">5</td></tr><tr><td align="left" valign="middle">P08</td><td align="center" valign="middle">22</td><td align="center" valign="middle">F</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">54</td><td align="center" valign="middle">5</td><td align="center" valign="middle">13</td><td align="center" valign="middle">41</td><td align="center" valign="middle">10</td><td align="center" valign="middle">-</td><td align="center" valign="middle">3</td><td align="center" valign="middle">7</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P09</td><td align="center" valign="middle">20</td><td align="center" valign="middle">F</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">117</td><td align="center" valign="middle">11</td><td align="center" valign="middle">72</td><td align="center" valign="middle">41</td><td align="center" valign="middle">57</td><td align="center" valign="middle">2</td><td align="center" valign="middle">29</td><td align="center" valign="middle">21</td><td align="center" valign="middle">7</td></tr><tr><td align="left" valign="middle">P10</td><td align="center" valign="middle">36</td><td align="center" valign="middle">M</td><td align="center" valign="middle">UW</td><td align="center" valign="middle">122</td><td align="center" valign="middle">13</td><td align="center" valign="middle">60</td><td align="center" valign="middle">46</td><td align="center" valign="middle">54</td><td align="center" valign="middle">6</td><td align="center" valign="middle">28</td><td align="center" valign="middle">22</td><td align="center" valign="middle">4</td></tr><tr><td align="left" valign="middle">P11</td><td align="center" valign="middle">36</td><td align="center" valign="middle">M</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">15</td><td align="center" valign="middle">79</td><td align="center" valign="middle">44</td><td align="center" valign="middle">62</td><td align="center" valign="middle">9</td><td align="center" valign="middle">31</td><td align="center" valign="middle">22</td><td align="center" valign="middle">9</td></tr><tr><td align="left" valign="middle">P12</td><td align="center" valign="middle">21</td><td align="center" valign="middle">M</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">57</td><td align="center" valign="middle">6</td><td align="center" valign="middle">21</td><td align="center" valign="middle">34</td><td align="center" valign="middle">19</td><td align="center" valign="middle">-</td><td align="center" valign="middle">8</td><td align="center" valign="middle">8</td><td align="center" valign="middle">3</td></tr><tr><td align="left" valign="middle">P13</td><td align="center" valign="middle">36</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">94</td><td align="center" valign="middle">10</td><td align="center" valign="middle">46</td><td align="center" valign="middle">42</td><td align="center" valign="middle">32</td><td align="center" valign="middle">10</td><td align="center" valign="middle">18</td><td align="center" valign="middle">14</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P14</td><td align="center" valign="middle">31</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">13</td><td align="center" valign="middle">72</td><td align="center" valign="middle">54</td><td align="center" valign="middle">51</td><td align="center" valign="middle">9</td><td align="center" valign="middle">23</td><td align="center" valign="middle">15</td><td align="center" valign="middle">13</td></tr><tr><td align="left" valign="middle">P15</td><td align="center" valign="middle">34</td><td align="center" valign="middle">M</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">115</td><td align="center" valign="middle">11</td><td align="center" valign="middle">70</td><td align="center" valign="middle">43</td><td align="center" valign="middle">65</td><td align="center" valign="middle">-</td><td align="center" valign="middle">20</td><td align="center" valign="middle">26</td><td align="center" valign="middle">19</td></tr><tr><td align="left" valign="middle">P16</td><td align="center" valign="middle">36</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">90</td><td align="center" valign="middle">9</td><td align="center" valign="middle">26</td><td align="center" valign="middle">63</td><td align="center" valign="middle">20</td><td align="center" valign="middle">7</td><td align="center" valign="middle">13</td><td align="center" valign="middle">7</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P17</td><td align="center" valign="middle">60</td><td align="center" valign="middle">M</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">65</td><td align="center" valign="middle">5</td><td align="center" valign="middle">35</td><td align="center" valign="middle">30</td><td align="center" valign="middle">15</td><td align="center" valign="middle">14</td><td align="center" valign="middle">3</td><td align="center" valign="middle">10</td><td align="center" valign="middle">2</td></tr><tr><td align="left" valign="middle">P18</td><td align="center" valign="middle">18</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">10</td><td align="center" valign="middle">66</td><td align="center" valign="middle">60</td><td align="center" valign="middle">53</td><td align="center" valign="middle">-</td><td align="center" valign="middle">17</td><td align="center" valign="middle">19</td><td align="center" valign="middle">17</td></tr><tr><td align="left" valign="middle">P19</td><td align="center" valign="middle">60</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">121</td><td align="center" valign="middle">9</td><td align="center" valign="middle">60</td><td align="center" valign="middle">58</td><td align="center" valign="middle">39</td><td align="center" valign="middle">10</td><td align="center" valign="middle">22</td><td align="center" valign="middle">11</td><td align="center" valign="middle">6</td></tr><tr><td align="left" valign="middle">P20</td><td align="center" valign="middle">25</td><td align="center" valign="middle">F</td><td align="center" valign="middle">RW</td><td align="center" valign="middle">127</td><td align="center" valign="middle">14</td><td align="center" valign="middle">82</td><td align="center" valign="middle">45</td><td align="center" valign="middle">71</td><td align="center" valign="middle">-</td><td align="center" valign="middle">21</td><td align="center" valign="middle">28</td><td align="center" valign="middle">22</td></tr><tr><td align="left" valign="middle">P21</td><td align="center" valign="middle">20</td><td align="center" valign="middle">F</td><td align="center" valign="middle">US</td><td align="center" valign="middle">107</td><td align="center" valign="middle">11</td><td align="center" valign="middle">68</td><td align="center" valign="middle">38</td><td align="center" valign="middle">44</td><td align="center" valign="middle">5</td><td align="center" valign="middle">24</td><td align="center" valign="middle">20</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P22</td><td align="center" valign="middle">59</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">111</td><td align="center" valign="middle">9</td><td align="center" valign="middle">44</td><td align="center" valign="middle">63</td><td align="center" valign="middle">34</td><td align="center" valign="middle">9</td><td align="center" valign="middle">17</td><td align="center" valign="middle">16</td><td align="center" valign="middle">1</td></tr><tr><td align="left" valign="middle">P23</td><td align="center" valign="middle">40</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">127</td><td align="center" valign="middle">12</td><td align="center" valign="middle">63</td><td align="center" valign="middle">61</td><td align="center" valign="middle">47</td><td align="center" valign="middle">5</td><td align="center" valign="middle">13</td><td align="center" valign="middle">33</td><td align="center" valign="middle">1</td></tr><tr><td align="left" valign="middle">P24</td><td align="center" valign="middle">37</td><td align="center" valign="middle">F</td><td align="center" valign="middle">US</td><td align="center" valign="middle">127</td><td align="center" valign="middle">14</td><td align="center" valign="middle">64</td><td align="center" valign="middle">62</td><td align="center" valign="middle">54</td><td align="center" valign="middle">3</td><td align="center" valign="middle">23</td><td align="center" valign="middle">26</td><td align="center" valign="middle">5</td></tr><tr><td align="left" valign="middle">P25</td><td align="center" valign="middle">60</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">115</td><td align="center" valign="middle">9</td><td align="center" valign="middle">58</td><td align="center" valign="middle">56</td><td align="center" valign="middle">56</td><td align="center" valign="middle">2</td><td align="center" valign="middle">21</td><td align="center" valign="middle">15</td><td align="center" valign="middle">20</td></tr><tr><td align="left" valign="middle">P26</td><td align="center" valign="middle">61</td><td align="center" valign="middle">F</td><td align="center" valign="middle">US</td><td align="center" valign="middle">87</td><td align="center" valign="middle">7</td><td align="center" valign="middle">38</td><td align="center" valign="middle">49</td><td align="center" valign="middle">30</td><td align="center" valign="middle">7</td><td align="center" valign="middle">7</td><td align="center" valign="middle">17</td><td align="center" valign="middle">6</td></tr><tr><td align="left" valign="middle">P27</td><td align="center" valign="middle">25</td><td align="center" valign="middle">F</td><td align="center" valign="middle">US</td><td align="center" valign="middle">127</td><td align="center" valign="middle">9</td><td align="center" valign="middle">87</td><td align="center" valign="middle">40</td><td align="center" valign="middle">80</td><td align="center" valign="middle">1</td><td align="center" valign="middle">37</td><td align="center" valign="middle">15</td><td align="center" valign="middle">28</td></tr><tr><td align="left" valign="middle">P28</td><td align="center" valign="middle">28</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">87</td><td align="center" valign="middle">8</td><td align="center" valign="middle">50</td><td align="center" valign="middle">36</td><td align="center" valign="middle">38</td><td align="center" valign="middle">-</td><td align="center" valign="middle">17</td><td align="center" valign="middle">9</td><td align="center" valign="middle">12</td></tr><tr><td align="left" valign="middle">P29</td><td align="center" valign="middle">24</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">81</td><td align="center" valign="middle">8</td><td align="center" valign="middle">34</td><td align="center" valign="middle">46</td><td align="center" valign="middle">25</td><td align="center" valign="middle">6</td><td align="center" valign="middle">8</td><td align="center" valign="middle">17</td><td align="center" valign="middle">-</td></tr><tr><td align="left" valign="middle">P30</td><td align="center" valign="middle">33</td><td align="center" valign="middle">M</td><td align="center" valign="middle">US</td><td align="center" valign="middle">127</td><td align="center" valign="middle">12</td><td align="center" valign="middle">66</td><td align="center" valign="middle">61</td><td align="center" valign="middle">46</td><td align="center" valign="middle">7</td><td align="center" valign="middle">20</td><td align="center" valign="middle">18</td><td align="center" valign="middle">8</td></tr></tbody></table></table-wrap></floats-group></article>