<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199207</article-id><article-id pub-id-type="doi">10.1101/2024.10.02.616238</article-id><article-id pub-id-type="archive">PPR919532</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cross-task implications: How hippocampal event boundary responses predict unrelated memory performance</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>van Dijk</surname><given-names>Daphne</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Collin</surname><given-names>Silvy H.P.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>1</label>Tilburg School of Humanities and Digital Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04b8v1s79</institution-id><institution>Tilburg University</institution></institution-wrap><city>Tilburg</city>, <country country="NL">Netherlands</country></aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label>Corresponding author(s). <email>s.h.p.collin@tilburguniversity.edu</email>;</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Hippocampal responses at event boundaries have been shown to predict memory performance for these events. However, are these hippocampal event boundary responses specific to memory for those particular events, or can they also have predictive power across various memory tasks? We used data from the Cam-CAN project (fMRI data from continuous movie viewing and memory results from an unrelated Famous Faces Task, N = 630) to determine whether hippocampal responses at event boundaries during the continuous movie viewing were indicative of memory performance in the unrelated Famous Faces task using various machine learning algorithms. The results showed that memory performance in the Famous Faces Task could be predicted based on participants’ hippocampal event boundary responses in another task, which suggests that the hippocampal event boundary responses are indicative for general memory performance. This might indicate importance of these hippocampal event boundary responses in terms of general information processing of the human brain.</p></abstract><kwd-group><kwd>event segmentation</kwd><kwd>hippocampus</kwd><kwd>machine learning</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">When people experience everyday activities, they parse this stream of activity into discrete, meaningful events. According to <xref ref-type="bibr" rid="R21">Kurby and Zacks (2008)</xref>, an ’event’ can be described as a segment of time at a certain location where you can indicate a clear beginning and end. The understanding and perception of events is supported by so-called ’event models’ that make predictions about what will happen next. When something happens that is not consistent with the prediction based on the current event model, it will be experienced as a prediction error. Subsequently, the current model gets an update based on the latest perceptual information. The time-points at which these updates are made are referred to as ’event boundaries’ (<xref ref-type="bibr" rid="R21">Kurby &amp; Zacks, 2008</xref>). When people are explicitly asked to mark event boundaries in, for example, a movie, it appears that people strongly agree on the location of these event boundaries (<xref ref-type="bibr" rid="R26">Newtson, 1973</xref>). This phenomenon was also explored in fMRI-studies. For instance, in <xref ref-type="bibr" rid="R40">Zacks et al., 2001</xref>, people were shown tapes of everyday events in the MRI-scanner. Participants were unaware of the segmentation task during this stage. When these same people were later explicitly asked to mark event boundaries in these same videos, there appeared to be a significant correlation between transient changes in brain activity and the explicitly self-labeled event boundaries. These results suggest that event segmentation is a natural and spontaneous aspect of human information processing (see also <xref ref-type="bibr" rid="R41">Zacks and Swallow, 2007</xref> and <xref ref-type="bibr" rid="R14">Geerligs et al., 2021</xref>). Furthermore, increased hippocampal activity is specific and sensitive in its response to subjective event boundaries. Moreover, this activity was larger at those boundaries for which they found high consensus across participants (<xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>). This means that strong, obvious event boundaries also trigger stronger responses in the hippocampus. In this study, they recorded activity peaks in the hippocampus and then analyzed the alignment between the event boundaries and these activity peaks. These event boundaries were identified by an independent group of participants. This analysis demonstrated that increased hippocampal activity was highly correlated with the identified event boundaries (<xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>). In summary, these studies have shown that brain activity seems to be modulated by event structures and that there is general agreement on those structures among people (<xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>; <xref ref-type="bibr" rid="R26">Newtson, 1973</xref>; <xref ref-type="bibr" rid="R40">Zacks et al., 2001</xref>). Moreover, it has become clear that event boundaries are reflected by peaks in hippocampal activity (<xref ref-type="bibr" rid="R4">Barnett et al., 2024</xref>; <xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>; <xref ref-type="bibr" rid="R9">Bilkey &amp; Jensen, 2021</xref>; <xref ref-type="bibr" rid="R10">Brunec et al., 2018</xref>; <xref ref-type="bibr" rid="R15">Griffiths &amp; Fuentemilla, 2020</xref>; <xref ref-type="bibr" rid="R32">Reagh et al., 2020</xref>; <xref ref-type="bibr" rid="R40">Zacks et al., 2001</xref>).</p><p id="P3">Numerous previous studies indicated that event segmentation of ongoing activity plays an important role in people’s ability to remember and understand things (<xref ref-type="bibr" rid="R1">Aitken &amp; Kok, 2022</xref>; <xref ref-type="bibr" rid="R3">Baldassano et al., 2017</xref>; <xref ref-type="bibr" rid="R5">Bein et al., 2020</xref>, <xref ref-type="bibr" rid="R7">2021</xref>, <xref ref-type="bibr" rid="R6">2023</xref>; <xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>; <xref ref-type="bibr" rid="R11">Buckley et al., 2022</xref>; <xref ref-type="bibr" rid="R14">Geerligs et al., 2021</xref>; <xref ref-type="bibr" rid="R17">Güler et al., 2024</xref>; <xref ref-type="bibr" rid="R20">Kalbe &amp; Schwabe, 2020</xref>; <xref ref-type="bibr" rid="R22">Kurby &amp; Zacks, 2018</xref>; <xref ref-type="bibr" rid="R27">Nolden et al., 2024</xref>; <xref ref-type="bibr" rid="R28">Pettijohn &amp; Radvansky, 2016</xref>; <xref ref-type="bibr" rid="R29">Pradhan &amp; Kumar, 2022</xref>; <xref ref-type="bibr" rid="R30">Radvansky &amp; Zacks, 2017</xref>; <xref ref-type="bibr" rid="R33">Sargent et al., 2013</xref>; <xref ref-type="bibr" rid="R35">Sinclair et al., 2021</xref>). <xref ref-type="bibr" rid="R3">Baldassano et al. (2017)</xref> discovered a relationship between event boundaries and hippocampal encoding in a movie-viewing experiment. Their research showed that the hippocampus was triggered at the end of an event, i.e., at event boundaries, to encode this new information about this event into memory. This implies that the segmentation of events contributes to how memories are organized in memory. The encoding seemed to be most powerful when the activity in the hippocampus was relatively low during an event, but considerably high at an event boundary (<xref ref-type="bibr" rid="R3">Baldassano et al., 2017</xref>). <xref ref-type="bibr" rid="R22">Kurby and Zacks (2018)</xref> also note that the event segmentation process is used to update human working memory and regulate encoding in people’s long-term memory. Worse event memory can therefore be explained by bad event segmentation ability. Event segmentation ability can be defined as the level at which someone agrees with a larger sample regarding the location of the event boundaries in ongoing activities (<xref ref-type="bibr" rid="R33">Sargent et al., 2013</xref>). According to <xref ref-type="bibr" rid="R22">Kurby and Zacks (2018)</xref>, if people cannot segment events properly, these events will not be properly encoded and this in turn has a negative effect on memory recall. <xref ref-type="bibr" rid="R33">Sargent et al. (2013)</xref> showed that the ability to segment a continuous experience can accurately predict memory related to this specific activity. In this study, people who were better at segmenting a movie were able to remember more actions from this movie afterwards. In conclusion, the ability to properly segment ongoing activity is crucial for memory, but all of these studies focus on event-specific memory of that particular task. It is less clear whether a reduced event segmentation ability, which would be reflected by less clear hippocampal peaks at event boundaries, is also indicative of people’s memory performance on a more general level.</p><p id="P4">Thus, it has been shown multiple times that there is a coincidence of activity peaks in the hippocampus and event boundaries, however, less is known about whether these hippocampal peaks at event boundaries can also be a reliable “more general” predictor across-tasks. The purpose of this study is therefore to extend the findings of previous studies by gaining more insight into the interaction between event segmentation, hippocampal activity, and the performance on a general and unrelated memory test. This will be accomplished by applying machine learning techniques (i.e., linear regression, support vector machine, multilayer perceptron) as well as intersubject correlation analyses (ISC, <xref ref-type="bibr" rid="R18">Hasson et al., 2004</xref>). Our goal is to determine whether hippocampal time courses on one cognitive task can be used to make reliable predictions for people’s memory performance across different tasks. If the hippocampal responses to event boundaries appear to be a reliable predictor of the performance on another unrelated task, it would suggest that a reduced event segmentation ability is not only indicative of someone’s event-specific memory, but also of someone’s memory performance in general.</p><p id="P5">Altogether, we hypothesize that participants that have hippocampal peaks that are better aligned with event boundaries have better overall information processing, and should outperform also in other information processing tasks. Thus, with this study we answer the following research question: <italic>To what extent are hippocampal responses to event boundaries in an ongoing activity indicative of across-task memory performance?</italic></p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><sec id="S3" sec-type="subjects"><label>2.1</label><title>Participants</title><p id="P6">Data used for this project was obtained from the Cam-CAN repository (available at <ext-link ext-link-type="uri" xlink:href="http://www.mrc-cbu.cam.ac.uk/datasets/camcan/">http://www.mrc-cbu.cam.ac.uk/datasets/camcan/</ext-link>) from the Cambridge Center for Aging and Neuroscience (University of Cambridge 2010; <xref ref-type="bibr" rid="R34">Shafto et al., 2014</xref>; <xref ref-type="bibr" rid="R37">Taylor et al., 2017</xref>). Cam-CAN uses cognitive, neuroimaging and epidemiological data to learn more about how elderly can best maintain cognitive capacities. The following material was used from this dataset for the current study: the fMRI data from the movie-viewing experiment (N = 649), and the behavioral data from the Famous Faces task (N = 660). In total there were 631 participants who took part in both movie-viewing as well as the Famous Faces task. One participant was excluded because the behavioral data for this participant was incomplete. Thus, the final dataset consisted of 630 participants. Among the participants were 315 men and 315 women with a mean age of 54.88 (SD = 18.31, range from 18.5 to 88.9 years old).</p></sec><sec id="S4"><label>2.2</label><title>Task</title><sec id="S5"><label>2.2.1</label><title>Movie-viewing task</title><p id="P7">Participants who took part in this movie-viewing task watched a condensed 8-minute version of Alfred Hitchcock’s “Bang! You’re Dead” in the fMRI-scanner. Even though the full 25-minute video was considerably shortened, the narrative of the video was preserved.</p></sec><sec id="S6"><label>2.2.2</label><title>Famous Faces task</title><p id="P8">The Famous Faces task has been selected to serve as the measure of general memory performance (i.e., unrelated to the movie-viewing task). This semantic memory test measures participants’ ability to recognize famous people from photos. Participants were shown photos of 30 celebrities and 10 random unknown people. Participants were asked if they recognized the person in the photo. If so, they were asked if they knew this person’s name, and if they could provide occupational information about this person.</p></sec></sec><sec id="S7"><label>2.3</label><title>Description of fMRI data used for this study</title><sec id="S8"><label>2.3.1</label><title>movie-viewing task</title><p id="P9">We used the pre-processed fMRI-data of the movie-viewing task for this study. Details about the fMRI data in terms of data collection and pre-processing can therefore be found in the paper written by <xref ref-type="bibr" rid="R37">Taylor et al. (2017)</xref>. In short, the fMRI data were collected using a 3T Siemens TIM Trio equipped with a 32-channel head coil. T1-weighted images as well as T2*-weighted echo planar images (EPIs) were obtained. A multi-echo sequence (whole brain coverage; TR = 2470 ms) was used. The acquired functional and structural images were pre-processed using SPM12 (see <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>) as implemented in the Automatic Analysis pipeline system described in the paper by <xref ref-type="bibr" rid="R13">Cusack et al. (2015)</xref>. In short, the obtained functional images were corrected for slice-timing differences and motion. An anatomical group template was created using the DARTEL procedure and then transformed into Montreal Neurological Institute (MNI) space. The EPI images were co-registered to the T1 image and normalized into MNI space.</p></sec></sec><sec id="S9"><label>2.4</label><title>Data analysis</title><sec id="S10"><label>2.4.1</label><title>Famous Faces task</title><p id="P10">We approached the classification in this study with binary classifiers (”good” vs ”bad” memory performance) on the Famous Faces task.</p><p id="P11"><xref ref-type="bibr" rid="R34">Shafto et al. (2014)</xref>, made a distinction between four memory components: (1) number of famous faces recognized, (2) number of faces for which occupational information was given, (3) number of faces whose full names were given, and (4) correct rejections (i.e., the number of unfamiliar faces that were correctly identified as unfamiliar). We used component 2 and 3 separately as memory scores, as well as a memory score for which we subtracted the percentage of false alarms (calculated based on memory component 4) from the percentage correct recognition (component 1). Additionally, the “final score” used is the sum of these three scores (descriptive statistics in <xref ref-type="table" rid="T1">table 1</xref>).</p><p id="P12">To separate participants in “good” and “bad” memory performance, we used the median. The distribution of the participants over the two classes per memory aspect is shown in <xref ref-type="fig" rid="F1">figure 1</xref>.</p></sec><sec id="S11"><label>2.4.2</label><title>Machine learning analyses</title><sec id="S12"><title>Algorithms and evaluation metrics</title><p id="P13">We used three algorithms in this study: logistic regression, support vector machine, and multilayer perceptron. The algorithms were implemented and defined using the scikit-learn library (version 0.23.2). Several kind of evaluation metrics were implemented to evaluate and compare the classification performance of the different machine learning models. The models were trained using accuracy, but other metrics are preferred for evaluating classification performance with unbalanced sample sizes (<xref ref-type="bibr" rid="R2">Arbabshirani et al., 2017</xref>). The get a thorough overview of model performance we also calculated precision, recall, F1-score and the Area Under the Receiver Operating Characteristic Curve (ROC AUC). Chance level is 50 percent (“good” vs “bad”) on all algorithms.</p></sec><sec id="S13"><title>Hippocampal timecourses</title><p id="P14">In this study, we only used the hippocampus (of the left and right hemisphere). Since no difference was expected between the left and right hemisphere, the average of these two sequences was used to represent people’s hippocampal time course. The obtained hippocampal time courses were normalized (z-scored) within subjects to enable better comparison across participants.</p></sec><sec id="S14"><title>Cross-validation</title><p id="P15">The z-scored mean array of the activity in the hippocampus of the left and right hemisphere was used as input in three different algorithms. For these three algorithms, it was examined whether they can be used to correctly predict four components of people’s performance on the Famous Faces task. The dataset was divided into 80 percent training data and 20 percent test data in a stratified fashion. The models were trained using k-fold cross-validation (k = 5) to make optimal use of the available data.</p></sec><sec id="S15"><title>Hyperparameter tuning</title><p id="P16">To find the optimal hyperparameters, a grid search was performed per algorithm during the training on the binarized final memory score. The selected hyperparameters based on these grid searches were then applied to all models for the individual memory components. As a result, the same hyperparameters were used for each memory component per algorithm. Thus, the hyperparameters only vary between algorithms and not across memory components. This makes it possible to interpret the classification performance on the various memory components more unambiguously. The hyperparameters that gave the best results can be found in <xref ref-type="table" rid="T2">Table 2</xref>.</p></sec><sec id="S16"><title>Comparing across machine learning models</title><p id="P17">Furthermore, to test whether there were significant differences between the algorithms in their classification performance, Cochran’s Q Tests were performed for each memory component using the accuracy scores achieved on the test dataset. Cochran’s Q test is a non-parametric statistical technique to compare the performance of multiple machine learning models (<xref ref-type="bibr" rid="R31">Raschka, 2020</xref>). It tests the null hypothesis that there is no significant difference between multiple classification models in their accuracy on the same test set.</p></sec></sec><sec id="S17"><label>2.4.3</label><title>Inter-subject correlation analysis</title><p id="P18">As an additional test, it was examined whether there was a significant difference between the level of event segmentation consistency between the good-memory group and the bad-memory group. This was established by performing an intersubject correlation (ISC) analysis on the hippocampal time courses (complete dataset). Intersubject correlation (ISC) analysis of functional brain imaging data offers great insight into how brain activity is correlated across different participants when they are exposed to the same ongoing activity (e.g., movie-watching). This means that ISC quantifies the consistency of neural responses to these kind of naturalistic stimuli among people (<xref ref-type="bibr" rid="R18">Hasson et al., 2004</xref>; <xref ref-type="bibr" rid="R25">Nastase et al., 2019</xref>). The ISCs can be established by computing the correlation coefficients between all possible pairs of participants. According to <xref ref-type="bibr" rid="R25">Nastase et al. (2019)</xref>, statistical inference for ISC analysis is complicated as each participant contributes to the calculation of the ISC of all other participants. For that reason, the assumption of independence is violated and standard parametric tests (e.g. T-tests) are not suitable for this type of analysis. Therefore, subject-wise permutation tests are used for comparing two groups that are expected to have different ISC values (<xref ref-type="bibr" rid="R12">Chen et al., 2016</xref>). Statistical significance is determined by testing against a null distribution resulting in a reliable permutation-based p-value corresponding to the two-sided test (<xref ref-type="bibr" rid="R12">Chen et al., 2016</xref>; <xref ref-type="bibr" rid="R25">Nastase et al., 2019</xref>). Here, the ISCs were computed using a pairwise approach. Next, a two-sample Monte Carlo approximate permutation test was performed on these ISCs using 1000 iterations. The labels belonging to the binarized final memory score were used as the group labels. In this way, differences were computed between the median ISC-score for within-group correlations while the between-group correlations were excluded. Monte Carlo resampling had to be applied because an exact test would result in an infinitely long list of possible permutations. After all, in a two-sample test the number of possible permutations equals the factorial of N (N = 630 in this case).</p></sec></sec><sec id="S18"><label>2.5</label><title>Software</title><p id="P19">For this study, all (pre)processing steps were executed by using Python (version 3.7.3) in Jupyter Notebooks (version 6.1.1). The MATLAB-files from the received Cam-CAN dataset were converted to Python format using the SciPy library (version 1.5.2). The algorithms were implemented and defined using the scikit-learn library (version 0.23.2). Furthermore, NumPy (version 1.19.1), pandas (version 1.1.1), glob2 (version 0.7) and Matplotlib (version 3.3.1) were used for data preprocessing, data visualization, and the implementation of the algorithms. The Cochran’s Q Tests were implemented using the Mlxtend library for Python (version 0.17.3). The intersubject correlation analysis was performed using the Brain Imaging Analysis Kit for Python (see <ext-link ext-link-type="uri" xlink:href="http://brainiak.org">http://brainiak.org</ext-link>). The implementation of this analysis kit is based on the ISC tutorial of <xref ref-type="bibr" rid="R24">Nastase (2019)</xref>.</p></sec></sec><sec id="S19" sec-type="results"><label>3</label><title>Results</title><sec id="S20"><label>3.1</label><title>Classification</title><p id="P20">We used the hippocampal activity at event boundaries to predict memory performance at the Famous Faces task (i.e., a memory task unrelated to the neural data) using linear regression, support vector machine and multilayer perceptron. For all three algorithms, we ran a separate model for each of the 4 determined memory scores (see <xref ref-type="table" rid="T3">Table 3</xref>).</p><p id="P21">All models scored above 50 percent chance level indicating that we could indeed predict memory performance of the Famous Faces task using hippocampal responses of an unrelated movie-viewing task. The SVM model seemed to perform best in most cases (between 0.60 and 0.64 for all 4 memory components). Also when investigating the recall scores for all 4 memory components, it was especially the SVM that managed to predict memory performance based on the hippocampal data (especially for the recognition performance with 0.73 and the final score with 0.66). However, when only precision scores were considered, the SVM and MLP appeared to perform very poorly. Thus, the LR model turned out to be the best classifier in terms of precision scores. No major differences were observed between the different memory components.</p></sec><sec id="S21"><label>3.2</label><title>Comparing across models</title><p id="P22">We used Cochran’s Q Tests for each memory component using the accuracy on the test dataset to determine if there were significant differences between the three algorithms in terms of classification accuracy. No significant difference was found between the classifiers for any memory component (see <xref ref-type="table" rid="T4">Table 4</xref>).</p></sec><sec id="S22"><label>3.3</label><title>Intersubject correlation (ISC) analysis of hippocampal time courses</title><p id="P23">The intersubject correlation analysis revealed a median correlation of 0.439 across all values. A two-sample Monte Carlo approximate permutation test was conducted to compare the ISCs in the bad-memory group with the good-memory group. The actual observed group difference in terms of the median ISC values turned out to be -0.088. The non-parametric test showed that there was a significant difference (p = 0.014) in the median ISC values for the bad-memory group (0.393) and the good-memory group (0.480). Thus, the time courses correlated significantly stronger in the good-memory group compared to the bad-memory group. This also means that there is more consistency in the time courses in the good-memory group than in the bad-memory group.</p></sec></sec><sec id="S23" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P24">The purpose of this current study was to find out whether hippocampal responses to event boundaries are indicative of general, across-task memory performance. This was examined by comparing three different algorithms in their ability to predict people’s performance on the Famous Faces task using hippocampal time courses of continuous movie-viewing. As an additional test, an intersubject correlation (ISC) analysis was carried out to gain insight into the group differences in the hippocampal time courses between people with good versus bad memory performance. All models used (LR, SVM, MLP) were able to predict performance on the unrelated (Famous Faces) task based on the hippocampal timecourses drawn from the movie viewing task.</p><sec id="S24"><label>4.1</label><title>Differences across machine learning models</title><p id="P25">We also thoroughly investigated possible differences across the three algorithms used. In general, it can be concluded that there were no significant differences across the three algorithms in their prediction ability in this study. When looking at accuracy, SVM seemed to perform slightly better than the other two models. However, completely different conclusions could be drawn by only looking at the precision score and the F1-score. The precision scores and F1-scores for the SVM models were in fact considerably low compared to the scores on the other evaluation metrics. The MLP models also suffered from this issue, but the LR models did not. It does make sense that this problem arises. After all, only healthy people took part in this study. This means that there were probably relatively few good representative examples in the dataset of people with really poor memory capacity. It has been decided to set the threshold at the median final score to minimize class imbalance, but the disadvantage of this approach is that some people labeled with ”bad” memory may in fact have quite normal / average memory performance. As a consequence, the hippocampal time course of these people will presumably not deviate much from many people in the good-memory group. This probably made it more difficult for algorithms to learn the relationship between the “bad” memory label and the hippocampal time courses. This issue may also explain why the SVM and MLP model show a large difference between the precision scores and the scores on the other evaluation metrics. After all, these models are nonlinear and probably created complex decision boundaries, with many “average” observations ending up in the bad-group because those were almost identical to the observations that truly belonged to the bad-group. The LR model uses a less complex decision boundary since it is a linear classifier. With the LR model, observations were classified almost at chance-level. The difficulty the model has in finding the right location for a linear decision boundary could possibly explain this. In this situation, some observations will incorrectly end up in the good-group and some incorrectly in the bad-group. While in a nonlinear model there will be a tendency to classify too many observations as “bad”, and this will result in poor precision scores. However, this explanation is speculative and must be further investigated in follow-up research.</p></sec><sec id="S25"><label>4.2</label><title>Predictive power of hippocampal timecourses</title><p id="P26">All these results together make it plausible that the hippocampal responses to event boundaries are related to general memory capacity. First of all, algorithms were capable of classifying memory performance using hippocampal time courses better than one would expect based on chance alone. Secondly, there appeared to be a significant difference in event segmentation consistency between the two groups. Thus, what someone’s hippocampal time course of ongoing activity looks like provides information about this person’s overall memory capacity. However, given the research approach in this study, it can never be established with certainty that it is actually only the event boundaries that are indicative of the memory performance. The indicative factors in the time course could involve other things as well. However, since <xref ref-type="bibr" rid="R8">Ben-Yakov and Henson (2018)</xref> indicated that activity peaks in this exact same dataset are specific to event boundaries, it is extremely likely that the event boundaries are indeed indicative factors. Despite the significant difference in hippocampal activity between the two groups, classification performance turned out to be only slightly better than the baseline. A weak classification rate like this is not uncommon in this type of research. According to <xref ref-type="bibr" rid="R2">Arbabshirani et al. (2017)</xref>, this can be explained by the fact that the time courses of the different groups usually overlap to a great extent. As a consequence, a significant group difference does not necessarily guarantee a strong classification performance.</p><p id="P27">What is also striking in these results is that the MLP model only yields the best results in a very few cases. In previous studies, the use of neural networks has proven to be very effective for neuroimaging classification (e.g., <xref ref-type="bibr" rid="R16">Güçlü and van Gerven, 2017</xref>; <xref ref-type="bibr" rid="R39">Wen et al., 2018</xref>). However, most deep learning studies have used more complex neural network architectures such as Convolutional Neural Networks (ConvNet / CNN) or Recurrent Neural Networks (RNN). The MLP is one of the most basic versions of a neural network. It is possible that the classification of complex fMRI data may require these type of more complex neural networks. In addition, neural networks perform especially well when a large amount of training data is available (<xref ref-type="bibr" rid="R36">Sun et al., 2017</xref>; <xref ref-type="bibr" rid="R38">Ulloa et al., 2018</xref>). It is therefore likely that the relatively weak results of the MLP model are (partly) caused by the relatively small size of the dataset.</p><p id="P28">In summary, this study suggests that hippocampal peak activity at event boundaries in ongoing activity relate to general across-task memory performance. This extends findings in earlier work of hippocampal peak activity being related to task-specific memory performance (<xref ref-type="bibr" rid="R1">Aitken &amp; Kok, 2022</xref>; <xref ref-type="bibr" rid="R3">Baldassano et al., 2017</xref>; <xref ref-type="bibr" rid="R4">Barnett et al., 2024</xref>; <xref ref-type="bibr" rid="R5">Bein et al., 2020</xref>, <xref ref-type="bibr" rid="R7">2021</xref>, <xref ref-type="bibr" rid="R6">2023</xref>; <xref ref-type="bibr" rid="R8">Ben-Yakov &amp; Henson, 2018</xref>; <xref ref-type="bibr" rid="R9">Bilkey &amp; Jensen, 2021</xref>; <xref ref-type="bibr" rid="R10">Brunec et al., 2018</xref>; <xref ref-type="bibr" rid="R11">Buckley et al., 2022</xref>; <xref ref-type="bibr" rid="R14">Geerligs et al., 2021</xref>; <xref ref-type="bibr" rid="R15">Griffiths &amp; Fuentemilla, 2020</xref>; <xref ref-type="bibr" rid="R17">Güler et al., 2024</xref>; <xref ref-type="bibr" rid="R20">Kalbe &amp; Schwabe, 2020</xref>; <xref ref-type="bibr" rid="R22">Kurby &amp; Zacks, 2018</xref>; <xref ref-type="bibr" rid="R27">Nolden et al., 2024</xref>; <xref ref-type="bibr" rid="R28">Pettijohn &amp; Radvansky, 2016</xref>; <xref ref-type="bibr" rid="R29">Pradhan &amp; Kumar, 2022</xref>; <xref ref-type="bibr" rid="R30">Radvansky &amp; Zacks, 2017</xref>; <xref ref-type="bibr" rid="R32">Reagh et al., 2020</xref>; <xref ref-type="bibr" rid="R33">Sargent et al., 2013</xref>; <xref ref-type="bibr" rid="R35">Sinclair et al., 2021</xref>). This might reflect that someone’s ability to detect event boundaries is reflective of a general benefit in information processing skills. However, additional research is needed to be able to make more solid statements about it.</p></sec><sec id="S26"><label>4.3</label><title>Future research</title><p id="P29">For future studies, it would be good to make use of larger, strictly demarcated groups. By also collecting data from people suffering from memory deficits, it is easier to investigate differences in the hippocampal time courses since this is expected to result in less overlap in the time courses. This will allow stronger conclusions to be drawn about the relationship between hippocampal activity and overall memory capacity. Moreover, the amount of data in this dataset is not sufficient to achieve great success with deep learning methods. More data could improve this, and it would also reduce the risk of overfitting. Furthermore, it could also be very interesting to experiment with more complex neural network architectures. A major problem here is that it is often difficult to acquire a large (labeled) medical imaging dataset due to the high costs (<xref ref-type="bibr" rid="R36">Sun et al., 2017</xref>). In follow-up research, it may therefore be interesting to explore ensemble learning strategies in which models are used with low precision performance but relatively higher recall performance. If these models are diverse enough, it most likely means that the false positives will be diverse as well. This then makes it possible to cancel those false positives out by averaging the models (<xref ref-type="bibr" rid="R23">Ma et al., 2021</xref>). <xref ref-type="bibr" rid="R23">Ma et al. (2021)</xref> showed that this approach can yield great results when dealing with small classes. It has also been shown in other research areas that combining different classification models, including deep learning methods, can yield very good results (e.g., <xref ref-type="bibr" rid="R19">Kahou et al., 2016</xref>; <xref ref-type="bibr" rid="R36">Sun et al., 2017</xref>). Lastly, for future research it could also be interesting not to use the entire time course as input. For example, it is possible to choose to only investigate brain activity at the event boundaries that have a high inter-subject agreement. This will make it possible to determine with more certainty that the responses to event boundaries are indicative of memory performance because only boundary-evoked responses are studied, meaning that there will be less random noise in the data.</p></sec></sec><sec id="S27" sec-type="conclusions"><label>5</label><title>Conclusion</title><p id="P30">The results of this study show that all proposed classifiers were capable of predicting memory performance based on hippocampal time courses better than one would expect based on chance alone. Accuracy scores were found to be around 60 percent. These are not exceptionally high scores, but this is expected to be partly caused by various limitations as discussed above. These results suggest that hippocampal responses to event boundaries are indeed indicative of general across-task memory performance. This might indicate predictive power of these hippocampal event boundary responses in terms of general information processing of the human brain.</p></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>Data collection and sharing for this project was provided by the Cambridge Centre for Ageing and Neuroscience (Cam-CAN). Cam-CAN funding was provided by the UK Biotechnology and Biological Sciences Research Council (grant number BB/H008217/1), together with support from the UK Medical Research Council and University of Cambridge, UK.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>F</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><article-title>Hippocampal representations switch from errors to pre-dictions during acquisition of predictive associations</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><issue>3294</issue><pub-id pub-id-type="pmcid">PMC9178037</pub-id><pub-id pub-id-type="pmid">35676285</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-31040-w</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arbabshirani</surname><given-names>MR</given-names></name><name><surname>Plis</surname><given-names>S</given-names></name><name><surname>Sui</surname><given-names>J</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><article-title>Single subject predic-tion of brain disorders in neuroimaging: Promises and pitfalls</article-title><source>Neuroimage</source><year>2017</year><volume>145</volume><fpage>137</fpage><lpage>165</lpage><pub-id pub-id-type="pmcid">PMC5031516</pub-id><pub-id pub-id-type="pmid">27012503</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.02.079</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Discovering event structure in continuous narrative perception and memory</article-title><source>Neuron</source><year>2017</year><volume>95</volume><issue>3</issue><fpage>709</fpage><lpage>721</lpage><pub-id pub-id-type="pmcid">PMC5558154</pub-id><pub-id pub-id-type="pmid">28772125</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.041</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>AJ</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Spargo</surname><given-names>J</given-names></name><name><surname>Yadav</surname><given-names>R</given-names></name><name><surname>Cohn-Sheehy</surname><given-names>BI</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><article-title>Hippocampal-cortical interactions during event boundaries support retention of complex narrative events</article-title><source>Neuron</source><year>2024</year><volume>112</volume><issue>2</issue><fpage>319</fpage><lpage>330</lpage><pub-id pub-id-type="pmid">37944517</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bein</surname><given-names>O</given-names></name><name><surname>Duncan</surname><given-names>K</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><article-title>Mnemonic prediction errors bias hip-pocampal states</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><issue>3451</issue><pub-id pub-id-type="pmcid">PMC7351776</pub-id><pub-id pub-id-type="pmid">32651370</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17287-1</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bein</surname><given-names>O</given-names></name><name><surname>Gasser</surname><given-names>C</given-names></name><name><surname>Amer</surname><given-names>T</given-names></name><name><surname>Maril</surname><given-names>A</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><article-title>Predictions transform memories: How expected versus unexpected events are integrated or separated in memory</article-title><source>Neuroscience Biobehavioral Reviews</source><year>2023</year><volume>153</volume><issue>105368</issue><pub-id pub-id-type="pmcid">PMC10591973</pub-id><pub-id pub-id-type="pmid">37619645</pub-id><pub-id pub-id-type="doi">10.1016/j.neubiorev.2023.105368</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bein</surname><given-names>O</given-names></name><name><surname>Plotkin</surname><given-names>NA</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><article-title>Mnemonic prediction errors promote detailed memories</article-title><source>Learn Mem</source><year>2021</year><volume>28</volume><fpage>422</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC8525423</pub-id><pub-id pub-id-type="pmid">34663695</pub-id><pub-id pub-id-type="doi">10.1101/lm.053410.121</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yakov</surname><given-names>A</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>The hippocampal film editor: Sensitivity and specificity to event boundaries in continuous experience</article-title><source>The Journal of Neuroscience</source><year>2018</year><volume>38</volume><issue>47</issue><fpage>10057</fpage><lpage>10068</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/38/47/10057">https://www.jneurosci.org/content/38/47/10057</ext-link></comment><pub-id pub-id-type="pmcid">PMC6246887</pub-id><pub-id pub-id-type="pmid">30301758</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0524-18.2018</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilkey</surname><given-names>DK</given-names></name><name><surname>Jensen</surname><given-names>C</given-names></name></person-group><article-title>Neural markers of event boundaries</article-title><source>Topics in cognitive science</source><year>2021</year><volume>13</volume><issue>1</issue><fpage>128</fpage><lpage>141</lpage><pub-id pub-id-type="pmid">31621199</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><article-title>Boundaries shape cognitive representations of spaces and events</article-title><source>Trends in cognitive sciences</source><year>2018</year><volume>22</volume><issue>7</issue><fpage>637</fpage><lpage>650</lpage><pub-id pub-id-type="pmid">29706557</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>MG</given-names></name><name><surname>Myles</surname><given-names>LA</given-names></name><name><surname>Easton</surname><given-names>A</given-names></name><name><surname>McGregor</surname><given-names>A</given-names></name></person-group><article-title>The spatial layout of doorways and environmental boundaries shape the content of event memories</article-title><source>Cognition</source><year>2022</year><volume>153</volume><issue>105091</issue><pub-id pub-id-type="pmid">35468358</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Shin</surname><given-names>Y-W</given-names></name><name><surname>Taylor</surname><given-names>PA</given-names></name><name><surname>Glen</surname><given-names>DR</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Israel</surname><given-names>RB</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>Untangling the relatedness among correlations, part i: Nonparametric approaches to inter-subject correlation analysis at the group level</article-title><source>NeuroImage</source><year>2016</year><volume>142</volume><fpage>248</fpage><lpage>259</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1053811916301331">https://www.sciencedirect.com/science/article/pii/S1053811916301331</ext-link></comment><pub-id pub-id-type="pmcid">PMC5114176</pub-id><pub-id pub-id-type="pmid">27195792</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.023</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Vicente-Grabovetsky</surname><given-names>A</given-names></name><name><surname>Mitchell</surname><given-names>DJ</given-names></name><name><surname>Wild</surname><given-names>CJ</given-names></name><name><surname>Auer</surname><given-names>T</given-names></name><name><surname>Linke</surname><given-names>AC</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name></person-group><article-title>Automatic analysis (aa): Efficient neuroimag-ing workflows and parallel processing using matlab and xml</article-title><source>Frontiers in neuroinformatics</source><year>2015</year><volume>8</volume><fpage>90</fpage><pub-id pub-id-type="pmcid">PMC4295539</pub-id><pub-id pub-id-type="pmid">25642185</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00090</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geerligs</surname><given-names>L</given-names></name><name><surname>van Gerven</surname><given-names>M</given-names></name><name><surname>Güçlü</surname><given-names>U</given-names></name></person-group><article-title>Detecting neural state transitions underlying event segmentation</article-title><source>NeuroImage</source><year>2021</year><volume>236</volume><issue>118085</issue><pub-id pub-id-type="pmid">33882350</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>BJ</given-names></name><name><surname>Fuentemilla</surname><given-names>L</given-names></name></person-group><article-title>Event conjunction: How the hippocampus integrates episodic memories across event boundaries</article-title><source>Hippocampus</source><year>2020</year><volume>30</volume><issue>2</issue><fpage>162</fpage><lpage>171</lpage><pub-id pub-id-type="pmid">31566860</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><article-title>Increasingly complex representations of natu-ral movies across the dorsal stream are shared between subjects</article-title><source>NeuroImage</source><year>2017</year><volume>145</volume><fpage>329</fpage><lpage>336</lpage><pub-id pub-id-type="pmid">26724778</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güler</surname><given-names>B</given-names></name><name><surname>Adıgüzel</surname><given-names>Z</given-names></name><name><surname>Uysal</surname><given-names>B</given-names></name><name><surname>Günseli</surname><given-names>E</given-names></name></person-group><article-title>Discrete memories of a continuous world: A working memory perspective on event segmentation</article-title><source>Current Research in Behavioral Sciences</source><year>2024</year><volume>6</volume><issue>100145</issue><pub-id pub-id-type="doi">10.1016/j.crbeha.2023.100145</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>Intersubject synchronization of cortical activity during natural vision</article-title><source>science</source><year>2004</year><volume>303</volume><issue>5664</issue><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="pmid">15016991</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahou</surname><given-names>S</given-names></name><name><surname>Ebrahimi</surname><given-names>X</given-names></name><name><surname>Bouthillier</surname><given-names>P</given-names></name><name><surname>Lamblin</surname><given-names>C</given-names></name><name><surname>Gulcehre</surname><given-names>V</given-names></name><name><surname>Michalski</surname><given-names>KK</given-names></name><name><surname>Jean</surname><given-names>Sébastien</given-names></name><name><surname>Dauphin</surname><given-names>Y</given-names></name><name><surname>Boulanger-Lewandowski</surname><given-names>N</given-names></name><etal/></person-group><article-title>Emonets: Multimodal deep learning approaches for emotion recognition in video</article-title><source>Journal on Multimodal User Interfaces</source><year>2016</year><volume>10</volume><issue>2</issue><fpage>99</fpage><lpage>111</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1503.01800</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalbe</surname><given-names>F</given-names></name><name><surname>Schwabe</surname><given-names>L</given-names></name></person-group><article-title>Beyond arousal: Prediction error related to aversive events promotes episodic memory formation</article-title><source>Psychology: Learning, Memory, and Cognition</source><year>2020</year><volume>46</volume><issue>2</issue><fpage>234</fpage><lpage>246</lpage><pub-id pub-id-type="pmid">31169402</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurby</surname><given-names>CA</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><article-title>Segmentation in the perception and memory of events</article-title><source>Trends in Cognitive Sciences</source><year>2008</year><volume>12</volume><issue>2</issue><fpage>72</fpage><lpage>79</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1364661307003312?via%3Dihub">https://www.sciencedirect.com/science/article/pii/S1364661307003312?via%3Dihub</ext-link></comment><pub-id pub-id-type="pmcid">PMC2263140</pub-id><pub-id pub-id-type="pmid">18178125</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2007.11.004</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurby</surname><given-names>CA</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><article-title>Preserved neural event segmentation in healthy older adults</article-title><source>Psychology and Aging</source><year>2018</year><volume>33</volume><issue>2</issue><fpage>232</fpage><lpage>245</lpage><pub-id pub-id-type="pmcid">PMC8577268</pub-id><pub-id pub-id-type="pmid">29446971</pub-id><pub-id pub-id-type="doi">10.1037/pag0000226</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Ong</surname><given-names>H</given-names></name><name><surname>Vora</surname><given-names>A</given-names></name><name><surname>Nguyen</surname><given-names>TD</given-names></name><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name></person-group><source>Ensembling low precision models for binary biomedical image segmentation</source><conf-name>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name><year>2021</year><fpage>325</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1109/WACV48630.2021.00037</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name></person-group><source>Intersubject correlation tutorial</source><year>2019</year><comment>[2020] <ext-link ext-link-type="uri" xlink:href="https://github.com/snastase/isc-tutorial">https://github.com/snastase/isc-tutorial</ext-link></comment></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Gazzola</surname><given-names>V</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Keysers</surname><given-names>C</given-names></name></person-group><article-title>Measuring shared responses across subjects using intersubject correlation</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2019</year><volume>14</volume><issue>6</issue><fpage>667</fpage><lpage>685</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/scan/article/14/6/667/5489905">https://academic.oup.com/scan/article/14/6/667/5489905</ext-link></comment><pub-id pub-id-type="pmcid">PMC6688448</pub-id><pub-id pub-id-type="pmid">31099394</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsz037</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newtson</surname><given-names>D</given-names></name></person-group><article-title>Attribution and the unit of perception of ongoing behavior</article-title><source>Journal of Personality and Social Psychology</source><year>1973</year><volume>28</volume><issue>1</issue><fpage>28</fpage><lpage>38</lpage><pub-id pub-id-type="doi">10.1037/h0035584</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolden</surname><given-names>S</given-names></name><name><surname>Turan</surname><given-names>G</given-names></name><name><surname>Güler</surname><given-names>B</given-names></name><name><surname>Günseli</surname><given-names>E</given-names></name></person-group><article-title>Prediction error and event segmentation in episodic memory</article-title><source>Neuroscience Biobehavioral Reviews</source><year>2024</year><volume>157</volume><issue>105533</issue><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0149763424000010">https://www.sciencedirect.com/science/article/pii/S0149763424000010</ext-link></comment><pub-id pub-id-type="pmid">38184184</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pettijohn</surname><given-names>KA</given-names></name><name><surname>Radvansky</surname><given-names>GA</given-names></name></person-group><article-title>Walking through doorways causes for-getting: Event structure or updating disruption?</article-title><source>Q J Exp Psychol</source><year>2016</year><volume>69</volume><issue>11</issue><fpage>2119</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">26556012</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pradhan</surname><given-names>R</given-names></name><name><surname>Kumar</surname><given-names>D</given-names></name></person-group><article-title>Event segmentation and event boundary advantage: Role of attention and postencoding processing</article-title><source>Journal of Experimental Psychology: General</source><year>2022</year><volume>151</volume><issue>7</issue><fpage>1542</fpage><lpage>1555</lpage><pub-id pub-id-type="pmid">34928683</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radvansky</surname><given-names>GA</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><article-title>Event boundaries in memory and cognition</article-title><source>Current Opinion in Behavioral Sciences</source><year>2017</year><volume>17</volume><fpage>133</fpage><lpage>140</lpage><pub-id pub-id-type="pmcid">PMC5734104</pub-id><pub-id pub-id-type="pmid">29270446</pub-id><pub-id pub-id-type="doi">10.1016/j.cobeha.2017.08.006</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raschka</surname><given-names>S</given-names></name></person-group><article-title>Model evaluation, model selection, and algorithm selection in machine learning</article-title><source>arxiv, 1811.12808</source><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1811.12808">https://arxiv.org/abs/1811.12808</ext-link></comment></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reagh</surname><given-names>ZM</given-names></name><name><surname>Delarazan</surname><given-names>AI</given-names></name><name><surname>Garber</surname><given-names>A</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name></person-group><article-title>Aging alters neural activity at event boundaries in the hippocampus and posterior medial network</article-title><source>Nature communications</source><year>2020</year><volume>11</volume><issue>1</issue><elocation-id>3980</elocation-id><pub-id pub-id-type="pmcid">PMC7414222</pub-id><pub-id pub-id-type="pmid">32769969</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17713-4</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sargent</surname><given-names>JQ</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Hambrick</surname><given-names>DZ</given-names></name><name><surname>Zacks</surname><given-names>RT</given-names></name><name><surname>Kurby</surname><given-names>CA</given-names></name><name><surname>Bailey</surname><given-names>HR</given-names></name><name><surname>Eisenberg</surname><given-names>ML</given-names></name><name><surname>Beck</surname><given-names>TM</given-names></name></person-group><article-title>Event segmentation ability uniquely predicts event memory</article-title><source>Cognition</source><year>2013</year><volume>129</volume><issue>2</issue><fpage>241</fpage><lpage>255</lpage><pub-id pub-id-type="pmcid">PMC3821069</pub-id><pub-id pub-id-type="pmid">23942350</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2013.07.002</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shafto</surname><given-names>MA</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Dixon</surname><given-names>M</given-names></name><name><surname>Taylor</surname><given-names>JR</given-names></name><name><surname>Rowe</surname><given-names>JB</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Dalgleish</surname><given-names>T</given-names></name><etal/></person-group><article-title>The cambridge centre for ageing and neuroscience (cam-can) study protocol: Across-sectional, lifespan, multidisciplinary examination of healthy cognitive ageing</article-title><source>BMC Neurology</source><year>2014</year><volume>14</volume><issue>1</issue><fpage>204</fpage><pub-id pub-id-type="pmcid">PMC4219118</pub-id><pub-id pub-id-type="pmid">25412575</pub-id><pub-id pub-id-type="doi">10.1186/s12883-014-0204-1</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinclair</surname><given-names>AH</given-names></name><name><surname>Manalili</surname><given-names>GM</given-names></name><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Barense</surname><given-names>MD</given-names></name></person-group><article-title>Prediction errors disrupt hippocampal representations and update episodic memories</article-title><source>Current Opinion in Behavioral Sciences</source><year>2021</year><volume>118</volume><issue>51</issue><elocation-id>e2117625118</elocation-id><pub-id pub-id-type="pmcid">PMC8713973</pub-id><pub-id pub-id-type="pmid">34911768</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2117625118</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Tseng</surname><given-names>T-LB</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Qian</surname><given-names>W</given-names></name></person-group><article-title>Enhancing deep convolutional neural network scheme for breast cancer diagnosis with unlabeled data</article-title><source>Computerized Medical Imaging and Graphics</source><year>2017</year><volume>57</volume><fpage>4</fpage><lpage>9</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0895611116300696">https://www.sciencedirect.com/science/article/abs/pii/S0895611116300696</ext-link></comment><pub-id pub-id-type="pmid">27475279</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>JR</given-names></name><name><surname>Williams</surname><given-names>N</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Auer</surname><given-names>T</given-names></name><name><surname>Shafto</surname><given-names>MA</given-names></name><name><surname>Dixon</surname><given-names>M</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Cam-Can</surname></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>The cambridge centre for ageing and neuroscience (cam-can) data repository: Structural and functional mri, meg, and cognitive data from a cross-sectional adult lifespan sample</article-title><source>NeuroImage</source><year>2017</year><volume>144</volume><fpage>262</fpage><lpage>269</lpage><pub-id pub-id-type="pmcid">PMC5182075</pub-id><pub-id pub-id-type="pmid">26375206</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.09.018</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulloa</surname><given-names>A</given-names></name><name><surname>Plis</surname><given-names>S</given-names></name><name><surname>Calhoun</surname><given-names>V</given-names></name></person-group><article-title>Improving classification rate of schizophrenia using a multimodal multi-layer perceptron model with structural and functional mr</article-title><source>arXiv, 1804.04591</source><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.04591">https://arxiv.org/abs/1804.04591</ext-link></comment></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>D</given-names></name><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Han</surname><given-names>W</given-names></name></person-group><article-title>Deep learning methods to process fmri data and their application in the diagnosis of cognitive impairment: A brief overview and our opinion</article-title><source>Frontiers in neuroinformatics</source><year>2018</year><volume>12</volume><fpage>23</fpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2018.00023/full">https://www.frontiersin.org/journals/neuroinformatics/articles/10.3389/fninf.2018.00023/full</ext-link></comment><pub-id pub-id-type="pmcid">PMC5932168</pub-id><pub-id pub-id-type="pmid">29755334</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2018.00023</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name><name><surname>Sheridan</surname><given-names>MA</given-names></name><name><surname>Donaldson</surname><given-names>DI</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Ollinger</surname><given-names>JM</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><article-title>Human brain activity timelocked to perceptual event boundaries</article-title><source>Nature Neuroscience</source><year>2001</year><volume>4</volume><issue>6</issue><fpage>651</fpage><lpage>655</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn0601651">https://www.nature.com/articles/nn0601651</ext-link></comment><pub-id pub-id-type="pmid">11369948</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacks</surname><given-names>JM</given-names></name><name><surname>Swallow</surname><given-names>KM</given-names></name></person-group><article-title>Event segmentation</article-title><source>Current directions in psychological science</source><year>2007</year><volume>16</volume><issue>2</issue><fpage>80</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC3314399</pub-id><pub-id pub-id-type="pmid">22468032</pub-id><pub-id pub-id-type="doi">10.1111/j.1467-8721.2007.00480.x</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>The distribution across ”good” and ”bad” memory performance for each of the 4 memory scores used in the study</title></caption><graphic xlink:href="EMS199207-f001"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Descriptive statistics per memory component.</title></caption><table frame="box" rules="all"><thead><tr><th valign="bottom" align="left" rowspan="2">component</th><th valign="bottom" align="left" colspan="5"/></tr><tr><th valign="bottom" align="left">min</th><th valign="bottom" align="left">max</th><th valign="bottom" align="left">mean</th><th valign="bottom" align="left">SD</th><th valign="bottom" align="left">median</th></tr></thead><tbody><tr><td valign="bottom" align="left" rowspan="2">true recognition</td><td valign="bottom" align="left" colspan="5"/></tr><tr><td valign="bottom" align="left">20</td><td valign="bottom" align="left">100</td><td valign="bottom" align="left">87.56</td><td valign="bottom" align="left">14.64</td><td valign="bottom" align="left">90</td></tr><tr><td valign="bottom" align="left" rowspan="2">naming</td><td valign="bottom" align="left" colspan="5"/></tr><tr><td valign="bottom" align="left">4</td><td valign="bottom" align="left">30</td><td valign="bottom" align="left">23.58</td><td valign="bottom" align="left">5.79</td><td valign="bottom" align="left">25</td></tr><tr><td valign="bottom" align="left" rowspan="2">occupation</td><td valign="bottom" align="left" colspan="5"/></tr><tr><td valign="bottom" align="left">6</td><td valign="bottom" align="left">30</td><td valign="bottom" align="left">26.86</td><td valign="bottom" align="left">4.20</td><td valign="bottom" align="left">28</td></tr><tr><td valign="bottom" align="left" rowspan="2">final score</td><td valign="bottom" align="left" colspan="5"/></tr><tr><td valign="bottom" align="left">39</td><td valign="bottom" align="left">160</td><td valign="bottom" align="left">138.00</td><td valign="bottom" align="left">21.79</td><td valign="bottom" align="left">144.67</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>The selected hyperparameters per algorithm based on performed grid searches. For the hyperparameters not mentioned, the default settings of the scikit-learn library (version 0.23.2) have been applied.</title></caption><table frame="box" rules="all"><thead><tr><th valign="bottom" align="left">Algorithm</th><th valign="bottom" align="left">hyperparameters</th></tr></thead><tbody><tr><td valign="bottom" align="left">LR</td><td valign="bottom" align="left">’C’: 1, ’penalty’: ’l1’, ’solver’: ’saga’</td></tr><tr><td valign="bottom" align="left">SVM</td><td valign="bottom" align="left">’C’: 1, ’cache_size’: 100, ’kernel’: ’rbf’</td></tr><tr><td valign="middle" align="left">MLP</td><td valign="bottom" align="left">’activation’: ’logistic’, ’hidden_layer_sizes’: 1000, ’solver’:<break/>’adam’, ’alpha’: ’0.001’, ’learning_rate’: ’adaptive’</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>The classification performance on the test dataset per algorithm for the different memory components. Test performance was evaluated based on accuracy, recall, precision, F1-score and the ROC AUC score.</title></caption><table frame="box" rules="all"><thead><tr><th valign="middle" align="left">component</th><th valign="top" align="left">models</th><th valign="top" align="left">accuracy</th><th valign="top" align="left">recall</th><th valign="top" align="left">precision</th><th valign="top" align="left">F1-score</th><th valign="top" align="left">ROC</th></tr></thead><tbody><tr><td valign="bottom" align="left">recognition</td><td valign="bottom" align="left">LR</td><td valign="bottom" align="left">0.57</td><td valign="bottom" align="left">0.46</td><td valign="bottom" align="left"><bold>0.37</bold></td><td valign="bottom" align="left"><bold>0.41</bold></td><td valign="bottom" align="left">0.57</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">SVM</td><td valign="bottom" align="left"><bold>0.64</bold></td><td valign="bottom" align="left"><bold>0.73</bold></td><td valign="bottom" align="left">0.16</td><td valign="bottom" align="left">0.26</td><td valign="bottom" align="left"><bold>0.62</bold></td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">MLP</td><td valign="bottom" align="left">0.61</td><td valign="bottom" align="left">0.60</td><td valign="bottom" align="left">0.12</td><td valign="bottom" align="left">0.20</td><td valign="bottom" align="left">0.61</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/></tr><tr><td valign="bottom" align="left">naming</td><td valign="bottom" align="left">LR</td><td valign="bottom" align="left">0.59</td><td valign="bottom" align="left">0.51</td><td valign="bottom" align="left"><bold>0.51</bold></td><td valign="bottom" align="left"><bold>0.51</bold></td><td valign="bottom" align="left">0.59</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">SVM</td><td valign="bottom" align="left"><bold>0.60</bold></td><td valign="bottom" align="left">0.54</td><td valign="bottom" align="left">0.38</td><td valign="bottom" align="left">0.44</td><td valign="bottom" align="left"><bold>0.60</bold></td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">MLP</td><td valign="bottom" align="left"><bold>0.60</bold></td><td valign="bottom" align="left"><bold>0.57</bold></td><td valign="bottom" align="left">0.23</td><td valign="bottom" align="left">0.32</td><td valign="bottom" align="left">0.57</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/></tr><tr><td valign="bottom" align="left">occupation</td><td valign="bottom" align="left">LR</td><td valign="bottom" align="left">0.62</td><td valign="bottom" align="left">0.50</td><td valign="bottom" align="left"><bold>0.46</bold></td><td valign="bottom" align="left"><bold>0.48</bold></td><td valign="bottom" align="left">0.58</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">SVM</td><td valign="bottom" align="left"><bold>0.63</bold></td><td valign="bottom" align="left"><bold>0.60</bold></td><td valign="bottom" align="left">0.12</td><td valign="bottom" align="left">0.21</td><td valign="bottom" align="left"><bold>0.62</bold></td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">MLP</td><td valign="bottom" align="left">0.56</td><td valign="bottom" align="left">0.40</td><td valign="bottom" align="left">0.29</td><td valign="bottom" align="left">0.34</td><td valign="bottom" align="left">0.58</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/><td valign="bottom" align="left"/></tr><tr><td valign="bottom" align="left">final score</td><td valign="bottom" align="left">LR</td><td valign="bottom" align="left">0.53</td><td valign="bottom" align="left">0.53</td><td valign="bottom" align="left"><bold>0.51</bold></td><td valign="bottom" align="left">0.52</td><td valign="bottom" align="left">0.51</td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">SVM</td><td valign="bottom" align="left"><bold>0.62</bold></td><td valign="bottom" align="left"><bold>0.66</bold></td><td valign="bottom" align="left">0.49</td><td valign="bottom" align="left"><bold>0.56</bold></td><td valign="bottom" align="left"><bold>0.66</bold></td></tr><tr><td valign="bottom" align="left"/><td valign="bottom" align="left">MLP</td><td valign="bottom" align="left">0.56</td><td valign="bottom" align="left">0.59</td><td valign="bottom" align="left">0.35</td><td valign="bottom" align="left">0.44</td><td valign="bottom" align="left">0.60</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="float"><label>Table 4</label><caption><title>Results of Cochran’s Q Tests: the Q-value and associated p-value per memory component.</title></caption><table frame="box" rules="all"><thead><tr><th valign="middle" align="left">component</th><th valign="top" align="left">Q-value</th><th valign="top" align="left">p-value</th></tr></thead><tbody><tr><td valign="bottom" align="left">recognition</td><td valign="bottom" align="left">2.450</td><td valign="bottom" align="left">0.295</td></tr><tr><td valign="bottom" align="left">naming</td><td valign="bottom" align="left">0.195</td><td valign="bottom" align="left">0.907</td></tr><tr><td valign="bottom" align="left">occupation</td><td valign="bottom" align="left">2.577</td><td valign="bottom" align="left">0.276</td></tr><tr><td valign="bottom" align="left">final score</td><td valign="bottom" align="left">4.217</td><td valign="bottom" align="left">0.121</td></tr></tbody></table></table-wrap></floats-group></article>