<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199332</article-id><article-id pub-id-type="doi">10.1101/2024.10.07.617103</article-id><article-id pub-id-type="archive">PPR921293</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Spatiotemporal characterisation of information coding and exchange in the multiple demand network</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Karimi-Rouzbahani</surname><given-names>Hamid</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Rich</surname><given-names>Anina N.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Woolgar</surname><given-names>Alexandra</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><aff id="A1"><label>1</label>Medical Research Council Cognition and Brain Sciences Unit, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Macquarie University Performance &amp; Expertise Research Centre and School of Psychological Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sf06y89</institution-id><institution>Macquarie University</institution></institution-wrap>, <country country="AU">Australia</country></aff><aff id="A3"><label>3</label>Queensland Brain Institute, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00rqy9422</institution-id><institution>University of Queensland</institution></institution-wrap>, <country country="AU">Australia</country></aff><aff id="A4"><label>4</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <country country="GB">UK</country></aff></contrib-group><author-notes><corresp id="CR1">Correspondence: <email>h.karimi-rouzbahani@uq.edu.au</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>07</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The multiple-demand network (MDN), a brain-wide system with nodes near sensory and higher-order cognitive regions, has been suggested to integrate and exchange task-related information across the brain, supporting cognitive task performance. However, the profile of information coding and the role of each node within this network in information exchange remain unclear. To address this, we combined fMRI and MEG data in a challenging stimulus-response mapping task. Using multivariate pattern analysis (MVPA), we decoded various forms of task information, including coarse and fine stimulus details, motor responses, and stimulus-response mapping rules, across the MDN and visual regions. Early in the task, visual regions responded to large physical differences in stimuli, while later on, fine stimulus information and rules were encoded across the MDN. To assess information exchange between regions, we developed Fusion-RCA, a novel connectivity analysis method based on fMRI-MEG fusion profiles. Our findings revealed significant transfer of fine stimulus information, rules, and responses, but little evidence for the transfer of coarse stimulus information. These results highlight distinct information encoding patterns within MDN nodes and suggest that the anterior cingulate cortex (ACC) plays a key role in distributing task-relevant information. This study offers new insights into the dynamic function of the MDN and introduces Fusion-RCA as a powerful tool for exploring brain-wide information transfer.</p></abstract><kwd-group><kwd>multiple-demand network</kwd><kwd>MEG</kwd><kwd>fMRI</kwd><kwd>decoding</kwd><kwd>fusion</kwd><kwd>connectivity</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Humans are able to engage in intelligent goal-directed behaviour across different scenarios. To achieve this, the brain must combine information from multiple systems, from the sensory input of multiple modalities, through to memory, attention and motor processes. These systems are distributed across the brain, and a key question in understanding intelligent behaviour is how such information is integrated and coordinated.</p><p id="P3">An excellent candidate for this high-level executive function was suggested to be a network of regions known as the MDN (<xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>), also called the frontoparietal control network (e.g., <xref ref-type="bibr" rid="R50">Marek and Dosenbach, 2018</xref>), or cognitive control network/networks (e.g., <xref ref-type="bibr" rid="R13">Cole and Schneider, 2007</xref>). These regions include areas in the intraparietal sulcus (IPS), anterior insula/frontal operculum (AI/FO), inferior frontal sulcus (IFS), and dorsal anterior cingulate cortex (ACC), and overlap considerably with the frontoparietal resting-state network (<xref ref-type="bibr" rid="R2">Assem et al., 2020</xref>). They are active in many cognitive tasks (<xref ref-type="bibr" rid="R25">Fedorenko et al., 2013</xref>; <xref ref-type="bibr" rid="R2">Assem et al., 2020</xref>) and are widely thought to have a key role in supporting intelligent flexible behaviour (<xref ref-type="bibr" rid="R41">Jung and Haier, 2007</xref>; Duncan et al., 2010; <xref ref-type="bibr" rid="R16">Colom et al., 2010</xref>). For example, the extent of damage to these regions (and not nearby language regions) linearly predicts deficits in fluid intelligence (<xref ref-type="bibr" rid="R70">Woolgar et al., 2010</xref>; <xref ref-type="bibr" rid="R67">Woolgar et al., 2018</xref>). Fluid intelligence refers to the ability to think abstractly and solve novel tasks/problems and is predictive of a wide range of cognitive abilities (<xref ref-type="bibr" rid="R4">Baltes et al., 1999</xref>).</p><p id="P4">In line with its putative role in supporting a wide range of goal-directed behaviours, the MDN shows remarkable flexibility in adaptively coding information across a wide range of tasks (<xref ref-type="bibr" rid="R74">Zheng et al., 2024</xref>). Univariate functional Magnetic Resonance Imaging (fMRI) studies showed activation in this network across cognitive tasks, including those involving memory, maths, conflict detection, visual discrimination and more (<xref ref-type="bibr" rid="R61">Stiers et al., 2010</xref>; <xref ref-type="bibr" rid="R25">Fedorenko et al., 2013</xref>; <xref ref-type="bibr" rid="R2">Assem et al., 2020</xref>), suggesting its involvement in cognitively challenging tasks regardless of specific content (<xref ref-type="bibr" rid="R23">Duncan and Owen, 2000</xref>; <xref ref-type="bibr" rid="R21">Dosenbach et al., 2006</xref>). Multivariate fMRI studies have demonstrated that the MDN differentiates a range of different types of information (including stimuli, rules and responses) across tasks (<xref ref-type="bibr" rid="R19">Crittenden et al., 2016</xref>; <xref ref-type="bibr" rid="R69">Woolgar et al., 2016</xref>; <xref ref-type="bibr" rid="R60">Shashidhara et al., 2024</xref>). Moreover, within tasks, MDN responses are strongly shaped by task relevance (<xref ref-type="bibr" rid="R74">Zheng et al., 2024</xref>) and difficulty (<xref ref-type="bibr" rid="R61">Stiers et al., 2010</xref>; <xref ref-type="bibr" rid="R68">Woolgar et al., 2011a</xref>; <xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>). For example, representations of identical stimuli were enhanced in the MDN when attention was directed to objects (<xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>) or object features (<xref ref-type="bibr" rid="R39">Jackson et al., 2017</xref>; <xref ref-type="bibr" rid="R64">Wisniewski et al., 2023</xref>; <xref ref-type="bibr" rid="R51">Moerel et al., 2024</xref>). Representations across the MDN were also enhanced for hard vs. easy stimulus (<xref ref-type="bibr" rid="R68">Woolgar et al., 2011a</xref>; <xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>) and rule (<xref ref-type="bibr" rid="R65">Woolgar et al., 2015a</xref>). By combining transcranial magnetic stimulation (TMS) and fMRI, it has further been demonstrated that the right dorsolateral prefrontal cortex node (dlPFC) of the MDN plays a causal role in the dominance of task-relevant information in the system specifically by upregulating task-relevant codes (<xref ref-type="bibr" rid="R38">Jackson et al., 2021</xref>).</p><p id="P5">There is also evidence that connectivity and information coding in the MDN can predict behavioural performance. For example, in an auditory detection task, pre-stimulus connectivity between auditory sensory areas and cingulo-opercular nodes of the MDN (i.e., ACC and AI/FO) correlated with the accuracy of behavioural responses (<xref ref-type="bibr" rid="R59">Sadaghiani et al., 2015</xref>). In a stimulus-response (SR) mapping study, where different rules determined the mapping between stimuli and response buttons, the information held by the MDN correlated with behavioural responses, such that when the response was incorrect, the MDN also represented incorrect information (<xref ref-type="bibr" rid="R66">Woolgar et al., 2019</xref>; see also <xref ref-type="bibr" rid="R56">Robinson et al. 2022</xref>). These results suggest an important role for the MDN in processing task-related information and supporting behaviour.</p><p id="P6">A key proposal has been that the MDN could underpin flexible goal-directed behaviour by functionally connecting distinct and distant cognitive systems of the brain (<xref ref-type="bibr" rid="R41">Jung and Haier, 2007</xref>; <xref ref-type="bibr" rid="R15">Cole et al., 2013</xref>; <xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>; <xref ref-type="bibr" rid="R74">Zheng et al., 2024</xref>). This requires functional connections between the areas involved. Indeed, in addition to co-activation and flexible encoding of information across tasks, there is also evidence from resting-state fMRI studies that, compared to non-MD areas, the nodes of the MDN are highly connected (<xref ref-type="bibr" rid="R14">Cole et al., 2010</xref>; <xref ref-type="bibr" rid="R54">Power et al., 2013</xref>; <xref ref-type="bibr" rid="R2">Assem et al., 2020</xref>). They follow a small-world network structure (<xref ref-type="bibr" rid="R8">Bullmore and Sporns, 2009</xref>), which can facilitate access to distant parts of the brain through relatively shorter pathways compared to a regular network where information needs to pass many nodes to move across the brain. Such connections provide a potential mechanism by which the MDN could bring together different sources of information in arbitrary ways (<xref ref-type="bibr" rid="R36">Higo et al., 2011</xref>; <xref ref-type="bibr" rid="R15">Cole et al., 2013</xref>; <xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>).</p><p id="P7">While the above studies have shown resting-state and functional connectivity between the nodes of the MDN, empirical evidence for information exchange across these nodes is missing. Specifically, it remains unclear whether co-activation and co-encoding of information across the nodes of the MDN reflect <italic>information exchange</italic> between those nodes. Alternatively, it could be that the nodes of the MDN become active simultaneously and encode similar information without necessarily exchanging information. Information coding and information exchange do not necessarily co-occur (<xref ref-type="bibr" rid="R1">Anzellotti and Coutanche, 2018</xref>). Therefore, it is unclear whether the encoded information held within nodes of the MDN moves across the brain. Moreover, the profile of information coding across the MDN and the nature of the transferred information remain unclear. To address this, we developed a new fusion-based representational connectivity analysis (Fusion-RCA) to evaluate the exchange of information between MDN nodes. We combined the logic of Granger causality with the spatiotemporal richness of fused MEG-fMRI data to quantify the information flow between the nodes of the MDN, and between these nodes and the visual system. We used this to examine the exchange of stimulus, rule and response information in a complex stimulus-response mapping task. We also separated the representation of stimulus information into coarse (i.e., the stimuli presented on the left vs. right side of the visual field) and fine (i.e., the stimuli presented inner vs. outer of the visual field) components, which we predicted may have differential representations in the MD system based on previous work showing higher MDN coding of stimulus information that is perceptually difficult to discriminate (Woolgar et al., 2011; <xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>).</p><p id="P8">We found that different nodes of the MDN encoded task-related information with subtly different dynamics and observed information transfer from sensory regions to MDN and its circulation within the MDN and back to early visual cortex (EVC). Fine stimulus information was exchanged more strongly than coarse stimulus information across the MDN. Rule information flowed bidirectionally between MDN and the visual area while response information flowed from ACC to posterior nodes of the MDN and EVC. We found a discriminable coding and connectivity profile in the cingulo-opercular (CO) and the frontoparietal (FP) nodes of the MDN, which aligns with previous distinctions (<xref ref-type="bibr" rid="R19">Crittenden et al., 2016</xref>). Our results provide insights into how information is exchanged within and beyond the MDN in support of flexible rule-based behaviour. We also introduce a new brain connectivity method for studying exchange of different types of information with high spatial and temporal resolution in humans.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P9">To characterise information coding and transfer across the MDN, we used a paradigm that allowed us to evaluate information about stimuli, rules and button-press responses (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). Participants were presented with one stimulus and rule cue on each trial after which they were supposed to press one of four buttons depending on a previously learned stimulus-response mapping (<xref ref-type="fig" rid="F1">Figure 1A</xref>). On each trial, the participant had to discriminate the location of the stimulus (one of four options, two on the left of fixation, two on the right) and apply the rule indicated by the cue colour (e.g., cue colour 1 or 2 indicated Rule 1 should be applied; colour 3 or 4 indicated Rule 2 should be applied; counterbalanced across participants; <xref ref-type="fig" rid="F1">Figure 1B</xref>). Participants performed the task with relatively high performance (accuracy: mean = 80.78%, sd = 9.29%; correct reaction time: mean = 1692 ms, sd = 284 ms), indicating that they had learned the task and could successfully implement the rules, despite the task complexity. Participants failed to respond on an average of 2.6% (sd = 3.04%) of trials.</p><sec id="S3"><title>Analysis 1: multivariate pattern analysis in space and time</title><p id="P10">As an initial step, we evaluated the coding of different types of information using multivariate pattern analysis (MVPA). For this, we used Region of Interest (ROI)-specific fMRI decoding (<xref ref-type="fig" rid="F2">Figure 2A</xref>) in early visual cortices, the lateral occipital complex (LOC), and the regions that form the MDN. Next, we performed time-resolved decoding on existing MEG data (<xref ref-type="fig" rid="F2">Figure 2B</xref>), locked to the stimulus onset (left panels) and locked to the response time (right panels).</p><p id="P11">We quantified the strength of representations about four orthogonal aspects of information using decoding (see <xref ref-type="sec" rid="S9">Methods</xref>, and <xref ref-type="fig" rid="F1">Figure 1A</xref>), namely coarse and fine stimulus information, rule information and response information. Specifically, for stimulus information, we looked at two possible levels of required precision in representations of stimulus location. To quantify <italic>coarse</italic> stimulus information, we decoded information about side of the visual hemifield where the stimulus was presented. To quantify fine stimulus information, we decoded information about the precise location of the stimulus within each visual hemifield. We decoded the coarse and fine stimulus information and compared them, to test whether these different aspects of the visual representation are encoded and transferred in a similar or different manner across the brain, testing the role of MDN in prioritising perceptually difficult vs. easy aspects of stimulus information (Woolgar et al., 2011; <xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>). For rule information, we decoded the information about the rule (Rule 1 or 2). For response information, we decoded trials where inner vs. outer fingers were pressed.</p><sec id="S4"><title>ROI-based fMRI decoding</title><p id="P12">In EVC, we observed strong representation of stimulus information (<xref ref-type="fig" rid="F2">Figure 2A</xref>). The classifier was able to distinguish both the coarse (red bars) and fine (green bars) stimulus information (coarse stimulus information BFs for EVC=1.4e92, LOC=1.6e9 and fine stimulus information BFs for EVC=4.6e20, LOC=1.1e3), and there was evidence that coding of coarse stimulus information was stronger than fine stimulus information (BF=1.7e6), reflecting the magnitude of the visual differences in coarse and fine stimulus comparisons. There was also evidence that the EVC represented rule information (BF=23) but there was insufficient evidence to determine whether rule was presented in the LOC (BF=0.48), or whether response information was represented in either ROI (BFs for EVC=3, LOC=1.4).</p><p id="P13">The MDN, on the other hand, exhibited a different pattern of information coding. There was evidence for coding of both coarse and fine stimulus information in all ROIs (coarse stimulus information BFs for IPS=1.1e7, IFS =75, ACC=12, and fine stimulus information BFs for IPS=6.6e8, AI/FO=7.8, IFS =46e6, ACC=6.6e3), except for the AI/FO (insufficient evidence for coarse; BF=0.92). However, the pattern for stimulus information was opposite to the EVC, with coding of fine stimulus information being numerically stronger than coding of coarse information in all regions, and statistical evidence for a difference in the IFS (BF=5). There was evidence that rules were represented across all the MDN (BFs for IPS=1.2e4, IFS =99, ACC=74) except for the AI/FO (insufficient evidence; BF=0.55). Only the IPS held information about response (BFs for IPS=130, AI/FO=0.31, IFS =1.4, ACC=0.55). The CO sub-network (AI/FO and ACC) numerically showed lower levels of information compared to the FP sub-network (IFS and IPS) of the MDN.</p></sec><sec id="S5"><title>Time-resolved MEG decoding</title><p id="P14">In MEG, we used data from a previous study using the same paradigm in an independent group of participants (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). As in that study, we used the stimulus-locked and response-locked analyses to examine different aspects of information processing: whereas stimulus-driven signals are likely to be strongest when locked to stimulus onset, rule application and decision making requires computation based on the cue and can take variable amounts of time on different trials, and so is likely to be more evident when we align the neural signals based on the time of response. Here we examine coding of a new aspect of the task (coarse stimulus information). We also re-ran the time-resolved decoding of fine stimulus, rule and response coding reported in (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). We reproduce the results here for ease of comparison to the coarse stimulus information and the fMRI decoding results.</p><p id="P15">Coarse and fine stimulus information were present (BFs&gt;3) when the signals were aligned to stimulus onset from 55ms and 65ms post-stimulus onset, respectively, both peaking at 130 ms (<xref ref-type="fig" rid="F2">Figure 2B</xref>, left). Both types of visual information were sustained until after 1000 ms with evidence (BFs&gt;3) for stronger coarse than fine stimulus information until 800 ms post stimulus onset. In contrast, when the signals were aligned to response, there was evidence (BFs&gt;3) for greater coding of fine than coarse stimulus information between 300 to 100 ms before the response (<xref ref-type="fig" rid="F2">Figure 2B</xref>, right).</p><p id="P16">For rule coding, we reproduce here the results reported in (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). The <italic>stimulus-aligned</italic> analysis showed only a few sparse time points with evidence (BFs&gt;3) for above-chance rule information after the stimulus onset (<xref ref-type="fig" rid="F2">Figure 2C</xref>, left, blue), but <italic>response-aligned</italic> analyses showed numerous consecutive time points with evidence (BFs&gt;3) before and around the time of response (from -500 to 500 ms post response time; <xref ref-type="fig" rid="F2">Figure 2C</xref>, right, blue).</p><p id="P17">We also reproduce here the results for response coding reported in (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). There was little response information in the <italic>stimulus-aligned</italic> analysis (<xref ref-type="fig" rid="F2">Figure 2C</xref>, left, grey) but when the data were <italic>response-aligned</italic>, there was evidence (BFs&gt;3) for response information around the time response starting from around -700ms and lasting until the end of the analysis window (<xref ref-type="fig" rid="F2">Figure 2C</xref>, right, grey).</p><p id="P18">Comparing the fMRI and MEG data qualitatively, we observe that the pattern of information coding in the EVC, with stronger coding of coarse relative to fine stimulus information, and relatively weak evidence for coding of rules and response, mirrors the data from earlier timepoints in the MEG. Conversely, data from several MD regions, reflecting stronger decoding of fine compared to coarse stimulus information, and task rules, more closely mirrors the MEG data from later timepoints. Next, we sought to formalise these observations and study the dynamics of information coding in each region using model-based MEG-fMRI fusion.</p></sec></sec><sec id="S6"><title>Analysis 2: spatiotemporal dynamics of information encoding (fMRI-MEG fusion)</title><p id="P19">We used model-based fusion of fMRI and MEG representations (Hebart et al., 2017, <xref ref-type="bibr" rid="R51">Moerel et al., 2024</xref>) to quantify when, and in which ROI, the structure of representational space in the two modalities correlated with each other and with a theoretical model that captured the coarse stimulus, fine stimulus, rule and response information (<xref ref-type="fig" rid="F3">Figure 3A</xref>). This allowed us to quantify, for each region, the timecourse with which different aspects of the task were represented. The commonality profiles in general followed the MEG decoding profiles with stimulus information more aligned to the stimulus onset (<xref ref-type="fig" rid="F3">Figure 3B</xref>, left) and the rule and response commonalities peaking before and around the response (<xref ref-type="fig" rid="F3">Figure 3B</xref>, right). Significant (p&lt;0.05; random permutation testing corrected for multiple comparisons) commonalities are shown by thickened lines. We used a permutation approach rather than Bayes factor analysis here as there was no way to <italic>a priori</italic> set the chance level of commonality.</p><p id="P20">In the <italic>stimulus-aligned</italic> analysis, there was numerically higher commonality for coarse than fine stimulus information in EVC, LOC, IPS and IFS. However, in the <italic>response-aligned</italic> analysis the commonality was numerically higher for fine than coarse stimulus information in all these regions around the time of response. Thus, the general pattern seen in the MEG data above was born out in this analysis again and could be ascribed to the responses of the EVC and MDN. However, the CO sub-network (AI/FO and ACC) showed a different pattern to the FP sub-network of the MDN. In the CO sub-network, there was little commonality overall, but what coding there was showed a more sustained pattern than the FP sub-network. In particular, as opposed to the FP sub-network, the AI/FO showed significant coding of only the fine stimulus information around the time of stimulus representation, and coding of only the coarse stimulus information around the time of the response, at around the time that the FP sub-network stopped coding the coarse information. The ACC showed a sustained representation of only the fine visual information both at after stimulus onset (<italic>stimulus-aligned</italic>) and around the time of the response (<italic>response-aligned</italic>). This may suggest a different role for the AI/FO and ACC relative to the more lateral and dorsal FP sub-network of the MDN in stimulus processing, as supported by previous studies (<xref ref-type="bibr" rid="R21">Dosenbach et al., 2006</xref>; <xref ref-type="bibr" rid="R19">Crittenden et al., 2016</xref>).</p><p id="P21">In the stimulus-aligned analysis, rule information first appeared later in time than the visual information (after ~700 ms) and was seen in all MDN but no visual areas (<xref ref-type="fig" rid="F3">Figure 3B</xref>, left). In the <italic>response-aligned</italic> analysis, all nodes of the MDN showed the rule information from ~-2500 ms before the response (ramping up and peaking at ~-500 ms before the response), the EVC showed rule information only appearing later, ~-1000 ms before the response (<xref ref-type="fig" rid="F3">Figure 3B</xref>, right). Therefore, the significant rule coding in the EVC in the fMRI data may have arisen at later timepoints, perhaps reflecting feedback.</p><p id="P22">Not surprisingly, the rising temporal profile of response information was more clearly observed in <italic>response-aligned</italic> analysis, which peaked around the time of response in all areas except AI/FO, which did not show response coding.</p><p id="P23">The commonality results suggest distinct temporal profiles of information encoding between the different types of information and brain regions. To quantify the comparison between different types of information, we calculated “time to first significant commonality”, “time to maximum commonality” and “time from maximum commonality” for each region and information aspect (see <xref ref-type="sec" rid="S9">Methods</xref>; <xref ref-type="fig" rid="F3">Figure 3C-E</xref>). These are only single data values as fusion was performed using group-averaged data across modalities. In <italic>stimulus-aligned</italic> data, the time to the first significant commonality was numerically slightly shorter for the fine (green bars) than coarse (red bars) stimulus information across all areas (except for AI/FO where coarse stimulus information was never significant so we do not have a data point; <xref ref-type="fig" rid="F3">Figure 3C</xref>). However, the times to maximum commonality for the two types of stimulus information were similar (for all regions except LOC where coarse information was first; <xref ref-type="fig" rid="F3">Figure 3D</xref>). In <italic>response-aligned</italic> data, commonalities also reached their peaks earlier for coarse relative to fine stimulus information, across all areas except AI/FO, which, as noted above showed a low but sustained representation of the fine stimulus throughout the analysis window (<xref ref-type="fig" rid="F3">Figure 3E</xref>).</p><p id="P24">Comparing between regions, relative to the stimulus onset, the time to the <italic>maximum</italic> commonality increased from posterior visual areas to anterior nodes of the MDN for the coarse (EVC = 645 ms; LOC = 785 ms; IPS = 785 ms; AI/FO = N/A; IFS = 795 ms; ACC = 880 ms) and fine (EVC = 570 ms; IPS = 570 ms; AI/FO = 725; IFS = 970 ms; ACC = 970 ms; LOC = 1330 ms) stimulus information. This suggests potential feed-forward flow of stimulus information from EVC to MDN (not including LOC, for which the time course suggests it might have been by-passed).</p><p id="P25">In <italic>response-aligned</italic> data, the time from maximum commonality (to response) for rule information suggests that, relative to response times, rule information was dominantly encoded by the posterior MDN areas followed by more anterior areas, and finally appeared in the EVC just prior to the response being actually made (LOC = N/A; IPS = -385 ms; AI/FO = -305; IFS = -305 ms; ACC = -285 ms; EVC = -130 ms).</p><p id="P26">The time from maximum commonality to response for response information did not show a clear pattern but was shortest in the IPS area (AI/FO = 1425 ms; IFS = 445 ms; EVC = 175 ms; LOC =175; ACC = 100 ms; IPS = 60 ms) suggesting that this area might be one of the critical ROIs in the MDN to support motor responses.</p></sec><sec id="S7"><title>Analysis 3: evaluating flow of information across the brain using Fusion-RCA</title><p id="P27">Finally, we tested for statistical relationships between the dynamic representational profiles of the different regions using an adaptation of Granger causality (<xref ref-type="bibr" rid="R5">Barnett and Seth, 2014</xref>), to quantify the potential exchange of information between them and characterise how information was circulated within and beyond the MDN. To do so, we applied Granger-causality analysis to the commonality indices from different ROIs to evaluate their statistical dependency in time, a new method that we call <italic>Fusion</italic>-RCA (<xref ref-type="fig" rid="F4">Figure 4A</xref>). This analysis goes beyond co-variation of activations or representational structures (<xref ref-type="bibr" rid="R30">Goddard et al., 2016</xref>; <xref ref-type="bibr" rid="R1">Anzellotti and Coutanche, 2018</xref>; <xref ref-type="bibr" rid="R47">Karimi-Rouzbahani et al., 2022</xref>) and considers the relationships between the commonality time courses obtained for each ROI in the fusion analysis. We evaluated the flow of information separately for incoming information to and outgoing information from any of the 6 target ROI using multivariate (multi-ROI) Granger causality (<xref ref-type="bibr" rid="R5">Barnett and Seth, 2014</xref>; see <xref ref-type="sec" rid="S9">Methods</xref>).</p><p id="P28">The results of the Fusion-RCA are shown in <xref ref-type="fig" rid="F4">Figure 4B</xref>. For interpretation, we focus on the significant results diagrammatically shown in <xref ref-type="fig" rid="F4">Figure 4C</xref>. For <italic>coarse</italic> stimulus information, we found no significant (p&lt;0.05; corrected for multiple comparisons; <xref ref-type="fig" rid="F4">Figure 4C</xref>) flows across any of the ROIs, regardless of either alignment to the stimulus onset or the response time. This suggests that while coarse stimulus information was strongly represented in visual areas, it was not significantly exchanged with and within the MDN.</p><p id="P29">In contrast, for fine stimulus information (<xref ref-type="fig" rid="F4">Figure 4C</xref>), we observed abundant flow between all ROIs, suggesting considerable exchange and circulation of information pertaining to this more challenging aspect of visual information. The <italic>stimulus-aligned</italic> analysis suggested a critical role for the EVC as a receiver of <italic>fine</italic>-grained visual information and AI/FO as the most active node of the MDN. We observed information coming from LOC, IFS, IPS and AI/FO to the EVC and output from this region to the IPS and AI/FO. AI/FO was the only MDN region exchanging information with all other MDN regions. In the <italic>response-aligned</italic> analysis, the ACC seemed to be the apex of processing, receiving fine stimulus information from EVC, IPS and IFS areas. In summary, all MDN areas took part in circulating the fine stimulus information. Upon stimulus onset, information from EVC flowed into the MDN via the IPS and AI/FO nodes and was circulated back to the EVC from all lateral MDN regions. Before the response, information was fed to the medial ACC region.</p><p id="P30">As expected, flow of <italic>rule</italic> information was most visible in the <italic>response-aligned</italic> data. Here we again observed a critical role of the ACC in receiving rule information (from EVC, and all MDN regions) and feeding rule information elsewhere in the system (in EVC, IPS, and AI/FO regions), while those other MDN regions did not appear to exchange rule information directly.</p><p id="P31">In the exchange of <italic>response</italic> information, ACC again played a central role, receiving input (stimulus-aligned) from LOC and AI/FO and driving response information (<italic>response-aligned</italic>) into almost all other regions. There was also exchange of response information from IPS to EVC, potentially accounting for the representation of response information in these regions at late timepoints (c.f., <xref ref-type="fig" rid="F3">Figure 3B</xref>, right).</p><p id="P32">Together, these results showed that not all types of information were exchanged as strongly across the MDN. Whereas there was no significant flow of (perceptually easy) coarse stimulus information, there was significant circulation of (perceptually hard) fine stimulus information, rule and response information between nodes of the MDN and visual areas. These support a role for the MDN in connecting distinct compartments of the brain depending on task demands (<xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>).</p><p id="P33">Moreover, we saw that sub-regions of the MDN play distinct roles in transferring different types of information within the same trial. The fusion-based connectivity analysis allowed us to characterise the direction and the content of the transferred information simultaneously which might be misinterpreted if we look at the fusion commonality traces alone. For example, while the temporal dynamics of the commonality traces appeared to emphasise potential feedback of rule information from the MDN to EVC, the connectivity analyses clarified that rule exchange with EVC was bidirectional and mainly involved the ACC MDN region. Finally, we saw a critical role for the ACC in exchanging different types of information including fine stimulus, rule and response information. We also observed that AI/FO also exchanged information with ACC for all these aspects of information suggesting their close functional cooperation and potential distinction from the FP sub-network of the MDN (<xref ref-type="bibr" rid="R19">Crittenden et al., 2016</xref>).</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P34">The multiple demand network has been suggested to be a potential candidate for connecting distant and functionally distinct areas of the brain to construct “integrated intelligence from distributed brain activity” (<xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>, p.838). However, the temporal dynamics of information coding and flow in the MDN and the role of distinct nodes of the MDN in information transfer have been challenging to investigate, especially in humans. Here we used MVPA in spatially (fMRI) and temporally (MEG) high-resolution neuroimaging modalities and showed that different parts of the MDN represent distinct types of information to varying degrees. We found that coding in the MDN emphasised the more fine-grained aspects of visual stimuli, despite this being a smaller physical visual signal and one that was coded less strongly than the large coarse grain stimulus distinctions in visual regions. Then we used the MEG data to show distinct temporal profiles for coarse- and fine-grained stimulus information. Together these results suggested time-varying involvement of MDN and other brain areas in information coding. To formalise this, we fused fMRI and MEG data and showed that coarse- and fine-grained stimulus information had distinct time courses of coding in all areas of the MDN. This provided evidence for the highly dynamic nature of information coding in the MDN depending on stimulus type and difficulty. Finally, we developed a novel fusion-based connectivity analysis which allowed us to trace information exchange within and beyond the MD system. This revealed network-wide engagement and circulation of the challenging aspects of stimulus processing (fine stimulus information), in the absence of information exchange about the large physical (coarse) differences. It also revealed the critical role of the EVC and the ACC in the analysis of fine stimulus information, with the ACC also critical for receiving and distributing rule and response information. This provides unprecedented resolution on the distinct roles of different MDN regions in the encoding and exchange of multiple types of task information.</p><p id="P35">Human fMRI studies have shown for two decades that the nodes of the MDN become active and encode task-relevant (and usually challenging) aspects of information across a variety of tasks (<xref ref-type="bibr" rid="R23">Duncan and Owen, 2000</xref>; <xref ref-type="bibr" rid="R22">Duncan, 2010</xref>). Due to the temporal resolution of fMRI, however, researchers have been unable to determine whether this apparently simultaneous activation pattern was due to the nodes of the MDN having similar temporal profiles (and therefore doing similar things), or a lack of temporal resolution. Here, we combined fMRI with MEG and found detectable differences between the temporal dynamics of information encoding across the nodes of the MDN for distinct aspects of information. This detailed picture of the dynamics of information processing across the MDN supported the possibility of the MDN’s role in brain-wide information exchange through information integration and propagation. Our findings are consistent with an fMRI study that provided evidence for the feeding of sensory-related activations into adjacent nodes of the MDN (<xref ref-type="bibr" rid="R3">Assem et al., 2021</xref>). However, their univariate activity-based analysis did not evaluate the information content potentially fed into the MDN, but together our results are in line with the hypothesis about the role of the MDN. Similarly, a recent study also found stimulus category information in parietal and frontal brain areas near the core MDN suggesting potential flow through the MDN, although they did not quantitatively evaluate connectivity (<xref ref-type="bibr" rid="R60">Shashidhara et al., 2024</xref>).</p><p id="P36">We observed that not all regions of the MDN encoded and transferred information similarly. Specifically, we observed numerically lower (c.f., <xref ref-type="fig" rid="F2">Figure 2A</xref>) and more sustained (c.f., <xref ref-type="fig" rid="F3">Figure 3B</xref>) information coding in the cingulo-opercular (i.e., AI/FO and ACC) than lateral frontoparietal (i.e., IPS and IFS) sub-networks of the MDN. We also observed that ACC played a central role in transferring information across the MDN and beyond while consistently exchanging information with AI/FO about different aspects of the task (c.f., <xref ref-type="fig" rid="F4">Figure 4C</xref>). These observations align with the distinctions made between the CO and FP sub-networks within the MDN, where studies have reported not only better encoding of task-relevant information in CO than FP but also more pronounced within-than cross-sub-network functional connectivity (<xref ref-type="bibr" rid="R19">Crittenden et al., 2016</xref>). The sustained and less variable encoding of information in the CO than FP sub-network observed here (c.f., <xref ref-type="fig" rid="F3">Figure 3B</xref>) aligns with previous studies which split the MDN into CO and FP sub-networks (<xref ref-type="bibr" rid="R21">Dosenbach et al., 2006</xref>) based on findings that FP activations changed on a trial-by-trial basis whereas the FP sub-network showed sustained activations over blocks of trials (<xref ref-type="bibr" rid="R63">Visscher et al., 2003</xref>). Among the MDN regions, we observed a central role for the ACC in information exchange with other brain areas. This may reflect its critical role in different aspects of human cognition and executive control (<xref ref-type="bibr" rid="R10">Carter et al., 1999</xref>). Specifically, the dominantly cognitive ACC region targeted in the present study has been shown to be involved in attention and motor control and especially activated in cognitively demanding tasks where stimulus and response selections were difficult such as in Stroop tasks (see <xref ref-type="bibr" rid="R9">Bush et al., 2000</xref> for a review).</p><p id="P37">We have previously shown a direct relationship between (stimulus and rule) difficulty and the level of information coding in the MDN (<xref ref-type="bibr" rid="R68">Woolgar et al., 2011a</xref>; <xref ref-type="bibr" rid="R65">Woolgar et al., 2015a</xref>; <xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>). The current results extend previous observations and highlight the role of the MDN in the encoding of the difficult aspects of stimuli without explicit cueing in the experimental design. Specifically, our previous studies compared distinct sets of stimuli (e.g., with vs. without noise (<xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>), or at different eccentricities (<xref ref-type="bibr" rid="R68">Woolgar et al., 2011a</xref>)) in the hard vs. easy conditions to show the preference of the MDN in coding of hard vs. easy stimulus conditions. As the two stimulus sets differed (in terms of noise or eccentricities), their comparison might have been affected by stimulus-related differences rather than difficulty only. The tasks also had a blocked design with each block containing either the easy or hard condition. Therefore, it might have been the case that the higher encoding in the MDN in hard vs. easy blocks reflected a general effort signal initiated by the cue at the beginning of each block to modulate the representations of information. In the current study, on the other hand, we used four stimuli in an event-related design and quantified the level of hard vs. easy aspect of the same stimuli in an orthogonal decoding fashion. Specifically, all four stimuli were included in the decoding whether decoding the coarse (left vs. right side) or fine (inner vs. outer) aspect of the stimulus. Critically, participants needed to process both aspects of the stimulus information on every trial for every single stimulus to successfully perform the task, but probably had more difficulty telling if a stimulus was inner vs. outer in the visual field (fine information) than if on the left vs. right side (coarse information). Therefore, the easy vs. hard aspect of stimulus discrimination existed implicitly on every trial rather than being cued. Our results showed that the MDN seem to automatically (without any explicit cues about the difficulty of the information and the effort exerted by the participant) provide an enhanced representation for the more fine-grained aspect of the visual input which was not as well represented in the visual system. This is consistent with the role of the MDN in processing difficult aspects of the task.</p><p id="P38">One novelty of the current work is the development of a fusion-based connectivity analysis, <italic>Fusion-RCA</italic>. This connectivity analysis provides several advantages over conventional connectivity analyses. First, it follows the recent shift from univariate to multivariate connectivity analyses (<xref ref-type="bibr" rid="R17">Coutanche and Thompson-Schill, 2013</xref>; <xref ref-type="bibr" rid="R26">Geerligs and Henson, 2016</xref>; <xref ref-type="bibr" rid="R30">Goddard et al., 2016</xref>; <xref ref-type="bibr" rid="R1">Anzellotti and Coutanche, 2018</xref>; <xref ref-type="bibr" rid="R6">Basti et al., 2020</xref>; Karimi-Rouzbahani et al., 2021a; <xref ref-type="bibr" rid="R47">Karimi-Rouzbahani et al., 2022</xref>). Specifically, conventional univariate connectivity analyses, which evaluate statistical dependency across individual sensors, are prone to missing statistical relationships/connectivity reflected in low-amplitude activity (<xref ref-type="bibr" rid="R1">Anzellotti and Coutanche, 2018</xref>), as well as high-dimensional patterns across multiple sensors (<xref ref-type="bibr" rid="R26">Geerligs and Henson, 2016</xref>; <xref ref-type="bibr" rid="R6">Basti et al., 2020</xref>). On the other hand, multivariate (<xref ref-type="bibr" rid="R55">Rahimi et al., 2022</xref>), also called multi-dimensional (<xref ref-type="bibr" rid="R6">Basti et al., 2020</xref>) and representational (<xref ref-type="bibr" rid="R47">Karimi-Rouzbahani et al., 2022</xref>) connectivity <xref ref-type="sec" rid="S9">methods</xref>, are less affected by absolute activity values and can detect high-dimensional (across many sensors/voxels/electrodes) statistical relationships. Second, while conventional univariate analyses evaluate connectivity by quantifying the statistical relationship between activations, Fusion-RCA quantifies information transfer and provides insights about the content of the transferred information. Third, it provides a higher spatial resolution than conventional (source-space) MEG (&gt;1.0 cm; <xref ref-type="bibr" rid="R34">Hauk et al., 2011</xref>) as it uses the voxel-level (here 3.0 mm<sup>3</sup>) fMRI representations. It also provides a higher temporal resolution than comparable connectivity analyses in fMRI such as informational connectivity (<xref ref-type="bibr" rid="R17">Coutanche and Thompson-Schill, 2013</xref>) or jack-knife-based representational connectivity (<xref ref-type="bibr" rid="R18">Coutanche et al., 2020</xref>) as it utilises the millisecond scale (here 5ms) time-resolved representations from MEG. This additional spatiotemporal resolution can be crucial for characterising highly dynamical systems such as the MDN whose nodes had shown very similar activations in previous fMRI studies (<xref ref-type="bibr" rid="R25">Fedorenko et al., 2013</xref>; <xref ref-type="bibr" rid="R24">Duncan et al., 2020</xref>). Without fusion analysis, the temporal dynamics of information encoding across multiple brain locations would require high spatiotemporal resolution data from multiple regions in animal models (<xref ref-type="bibr" rid="R53">Panichello and Buschman, 2021</xref>) or patients implanted for intracranial recordings (e.g., <xref ref-type="bibr" rid="R43">Karimi-Rouzbahani and McGonigal, 2024</xref>). Importantly, this fusion-based connectivity analysis is not limited to the current study but can be used to approach many questions in cognitive neuroscience where high spatiotemporal resolution is necessary.</p><p id="P39">An important point about representation- and/or information-based <italic>connectivity</italic> is its distinction from information <italic>representation/encoding</italic> within individual ROIs. Specifically, a given pair of ROIs might encode similar information (above chance) but not interact with each other or they might exchange a specific aspect of information but do not encode it significantly (<xref ref-type="bibr" rid="R1">Anzellotti and Coutanche, 2018</xref>). Therefore, inference about potential connectivity should not be based on decoding profiles or fusion commonalities <italic>per se</italic> (<xref ref-type="bibr" rid="R12">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R52">Mohsenzadeh et al., 2018</xref>). For example, we observed that although rule information was initially maximally encoded across the MDN followed by the EVC, connectivity analysis suggested bidirectional feed-forward and feedback flows of rule information around the time of response. While the feed-forward flow of rule information may be due to rule information needing to be extracted from the cues which are provided visually, the feedback of rule may also enhance the relevant visual information in sensory areas. So, this extraction process necessitates involvement of visual followed by higher-order cognitive areas. Fusion-RCA proposes that connectivity analysis can be built on the decoding profiles (in MEG or fMRI separately) or commonality indices which have been increasingly used in the field to provide a more detailed picture of the potential interaction of information between brain areas (<xref ref-type="bibr" rid="R12">Cichy et al., 2014</xref>; <xref ref-type="bibr" rid="R52">Mohsenzadeh et al., 2018</xref>; Hebart et al., 2017; <xref ref-type="bibr" rid="R51">Moerel et al., 2024</xref>).</p><p id="P40">While there are benefits in using fMRI-MEG fusion and Fusion-RCA for obtaining insights about the spatiotemporal dynamics of information encoding and connectivity, there are also limitations (Cichy et al., 2020). First, fusion is only useful when effects are present in both modalities; no additional effect will be generated through fusion. If effects appear in one imaging modality but not the other, they will be diminished or not detected. Second, as weak effects might be further weakened in fusion through imperfect correlation across modalities, fusion is more suited for medium to large effects that are clear in both modalities. This is particularly the case here, where the fMRI and MEG data come from separate groups of participants. Third, the signal-to-noise ratio of the information can also be an important factor when interpreting peaks of commonalities in fusion. For example, it is assumed that frontal MD areas receive visual signals later than visual areas. However, a large early peak of visual signals might cause a larger commonality index in that same MD area followed by a smaller peak which is caused by when visual signals reach the MD area. Therefore, peaks of commonality might sometimes be hard to interpret for extracting information transfer. Finally, while we did not delve into potential <italic>transformation of information</italic> across the brain in this work, all the significant connections involved some levels of information transformation across areas. In other words, if we had two fMRI ROIs with identical representations, our Fusion-RCA would not detect any difference in their commonality time courses and no causal relationship would have been found, because the correlation of two ROIs with identical representations with one MEG data would result in identical correlations – no inter-area difference. Therefore, it is important to note that all the detected transferred information have also been transformed from the source to destination. Therefore, while fusion and fusion-based analyses can provide additional insights into potential exchange of information in the brain, it is crucial to keep in mind these considerations when interpreting their results.</p><p id="P41">Because of previous challenges with temporal and spatial resolution in neural recording modalities, there have not yet been strong predictions about the specific involvement of different MDN nodes in the flow of information in an ordered manner. However, here we demonstrated that approaching the problem in a data-driven way allows for new insights. For example, we identified the apparently critical role of the ACC in information transfer across the MDN. Fusion-RCA method can also be used for future studies where the content of the transferred information is unknown. In the case of the MDN, for example, it would also be interesting to investigate how the patterns of information exchange change under different circumstances, such as during memory recall, or when directed selective attention is applied.</p><p id="P42">In conclusion, this study fused fMRI and MEG, and developed a novel fusion-based connectivity analysis, to provide new insights into the temporal dynamics of information encoding and transfer across and beyond the MDN. We showed that the MDN’s involvement varies across different aspects of the task including higher involvement and encoding of difficult vs. easy aspect of stimulus information. We also found subtle but detectable temporal differences in the time course of information encoding across the MDN for distinct information types, suggestive of a distinction between the frontoparietal (FP) and cingulo-opercular (CO) sub-networks of the MDN which played different roles in information exchange between the MDN and visual areas during the task. We found a central role for the ACC in information transfer in our experiment, presumably reflecting its multifunctional role in supporting different aspects of human cognition and behaviour. This work provides new insights about the potential role of the MDN as one of the main candidate mechanisms supporting complex task performance, and introduces Fusion-RCA, a novel connectivity method which can be used to address many questions about cognition.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><p id="P43">We collected fMRI data using the same paradigm for which we had previously collected MEG data (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>), with new participants. The paradigm is designed to investigate the encoding of stimulus, rule and response in a stimulus-response mapping task and has shown a strong link between behavioural performance and neural coding of information (<xref ref-type="bibr" rid="R66">Woolgar et al., 2019</xref>). Specifically, at around the time of response, the coding of stimulus representations in the MDN predicted whether participants would give a correct or incorrect response (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). We combined this MEG dataset with our new fMRI dataset to explore the role of the MDN in encoding and integrating task information for flexible behaviour.</p><sec id="S10" sec-type="subjects"><title>Participants</title><p id="P44">30 (14 female, 16 male; age: mean = 24.4 years, sd = 2.1) participants were recruited from Macquarie University volunteer panel for the fMRI study. No fMRI participant had participated in the previous MEG study (see <xref ref-type="bibr" rid="R56">Robinson et al. 2022</xref>, for full methodological details of the MEG study). Participants were right-handed and had normal or corrected to normal (through contact lenses) vision. Participants gave written informed consent for both a behavioural training of 1 hour prior to scanning (reimbursed AU$15) and the 2 hour fMRI experiment (including setup; reimbursed AU$40). The study was approved by the ethics committee of Macquarie University.</p></sec><sec id="S11"><title>Task design</title><p id="P45">We collected fMRI data while participants performed a visual stimulus-response mapping task (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>) designed to tease apart stimulus, cue, rule and response encoding in the brain. The task involved pressing of one of four buttons according to the position of a visual stimulus and one of two memorised stimulus-response mapping rules (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Each trial started with the presentation of a grey fixation square for 500 ms followed by the stimulus for another 500 ms after which the participants could provide a response (<xref ref-type="fig" rid="F1">Figure 1B</xref>). The response screen lasted for 4000 ms or until the response was made, whichever happened first.</p><p id="P46">In the initial behavioural training session, participants had to learn two set of mappings between stimuli and the buttons (rules), as indicated by the colour cue on the centre of the screen. Each rule was indicated by two cue colours (giving a total of 4 possible cue colours: two indicated that Rule 1 was to be applied, and the other two indicated that Rule 2 was to be applied) to allow us to see the effect of cue colour on rule processing, and to avoid the classifiers using cue colour to distinguish the categories. Specific cue colour-rule associations were randomised and counterbalanced across participants. Participants responded using the four fingers of their right hand.</p><p id="P47">Stimuli were presented and responses collected using MATLAB Psychtoolbox (Brainard, 1997). Our training session was identical to the one used in the MEG study (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). Briefly, the task started from easier versions, in which stimuli were presented in non-overlapping positions in earlier blocks, to the final version, with overlapping stimuli, like the version used in the MRI scanner. Rules were trained one by one in the first two blocks, each block containing 32 trials. Participants needed to reach 60% accuracy to continue to the subsequent block, otherwise the block would repeat until they reached the threshold or they were excluded and replaced if they did not reach it after 5 consecutive blocks. Participants did an average of 8.61 (sd = 2.46) training blocks to finish the training session successfully.</p></sec><sec id="S12"><title>Apparatus</title><p id="P48">FMRI scans were acquired using a Siemens 3 T Verio scanner with 32-channel head coil, at the Macquarie Medical Imaging centre, Macquarie University Hospital, Sydney, Australia. We used a high resolution interleaved ascending T2*-weighted echo planar imaging (EPI) acquisition sequence with the following parameters: repetition time (TR), 2000 ms; echo time (TE), 30 ms; 36 slices of 3.0 mm slice thickness with a 0.366 mm interslice gap; in-plane resolution, 3.0 × 3.0 mm; field of view, 126 mm. We also acquired T1-weighted MPRAGE structural images for all participants (non-selective inversion recovery, resolution 1.0 × 1.0 × 1.0 mm). The session began with the collection of a T1 image which lasted for about 5 minutes followed by EPI acquisition for the blocks of experimental trials.</p><p id="P49">Each block started with the presentation of a rule screen which showed the four cue conditions, 2 per rule, for 10 seconds, followed by the experimental trials. We acquired 6 scanning runs, each consisting of two blocks. Each run lasted for ~ 8 minutes after which the data were saved and scanning restarted. Within each block, there were 80 trials presented in pseudorandom order such that all stimulus configurations (4 cue colours * 4 stimuli) appeared five times with equal probability within each block. Stimuli were presented on an LCD screen at the back of the scanner, and participants could see the LCD through a head-coil mounted mirror.</p></sec><sec id="S13"><title>Pre-processing</title><p id="P50">We pre-processed the fMRI data using SPM 12 (Wellcome Department of Imaging Neuroscience, <ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/">www.fil.ion.ucl.ac.uk/spm</ext-link>) in Matlab 2018a. DICOM data were converted to NIFTII format. Functional images were slice-time corrected and realigned to the first functional scan in the run. They were then smoothed slightly (4 mm FWHM Gaussian kernel) to increase the signal-to-noise ratio as in previous work (<xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>; <xref ref-type="bibr" rid="R39">Jackson et al., 2017</xref>). Structural images were co-registered to the mean functional images and normalised to the MNI152 space (McConnell Brain Imaging Centre, Montreal Neurological Institute) to derive normalisation parameters that we later used to define Regions of Interest for individual participants.</p><p id="P51">The MEG pre-processing procedure is explained in the MEG study in detail (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>). Briefly, the signals were band-passed (0.03-200Hz), notch-filtered (50Hz) and down-sampled to 200 Hz with no additional pre-processing steps. Data were cut into <italic>stimulus-aligned</italic> epochs from -200 before to 2500 ms after stimulus onset and <italic>response-aligned</italic> epochs from -2500 to 500 ms after response. Signals are analysed in the 160-channel sensor space.</p></sec><sec id="S14"><title>Regions of Interest (ROIs)</title><p id="P52">Template space MDN ROIs were taken from our previous work (<xref ref-type="bibr" rid="R72">Woolgar et al., 2015b</xref>) where they were defined based on their activity in a wide range of cognitive tasks (<xref ref-type="bibr" rid="R22">Duncan, 2010</xref>). These consist of left and right inferior frontal sulcus (IFS; centre of mass +/−38 26 24, volume 17000 mm<sup>3</sup>); left and right anterior insula/frontal operculum (AI/FO; +/−35 19 3, 3000 mm<sup>3</sup>); left and right intraparietal sulcus (IPS; +/−35 −58 41, 7000 mm<sup>3</sup>) and the dorsal anterior cingulate area (ACC; +/−0 23 39, 21000 mm<sup>3</sup>).</p><p id="P53">The visual areas were obtained from the Brodmann template provided with MRIcro (<xref ref-type="bibr" rid="R57">Rorden and Brett, 2000</xref>): Brodmann area 17/18 (EVC; −13 −81 3, 16 −79 3, 54000 mm<sup>3</sup>). The lateral occipital complex (LOC; +/− 40 − 70 – 9, 41800 mm<sup>3</sup>) was defined from a prior review of 8 imaging studies reporting greater activation for objects compared to scrambled shapes or textures (<xref ref-type="bibr" rid="R62">Thompson and Duncan, 2009</xref>). ROIs were defined based on a subjects-averaged localiser data from a previous study where we contrasted the areas which responded more strongly to pictures of natural/man-made objects than to scrambled versions of the same objects (<xref ref-type="bibr" rid="R39">Jackson et al., 2017</xref>). Our LOC ROIs fell very close to anatomical LOC coordinates from previous studies (<xref ref-type="bibr" rid="R32">Grill-Spector et al., 2000</xref>, <xref ref-type="bibr" rid="R31">Grill-Spector et al., 1999</xref>). All ROIs were defined and analysed in each hemisphere separately, but decoding results were averaged over hemispheres for inference.</p></sec><sec id="S15"><title>First-level model</title><p id="P54">To estimate the activation patterns (beta values) for each condition and each area we used a General Linear Model (GLM), convolving event regressors with the duration of the trial reaction time (<xref ref-type="bibr" rid="R33">Grinband et al., 2008</xref>) with the first-order hemodynamic response of SPM. For MVPA, we used a total of 8 regressors representing <italic>coarse</italic> stimulus information (the two stimuli on the left vs. the two on the right side of space), fine stimulus information (the two stimuli located in the inner vs. outer peripheral positions relative to the central fixation square), <italic>rule</italic> (rule 1 vs. rule 2) and <italic>response</italic> (inner vs. outer buttons). Each trial contributed to the estimation of 4 of these regressors. For RSA analysis, we ran 4 separate sets of GLMs on the data to maximise the power for the desired information to avoid subtle information (e.g., rule) being dominated by stronger information (e.g., coarse stimulus)<sup><xref ref-type="fn" rid="FN1">1</xref></sup> and to have enough regressors for RSA (see <xref ref-type="fig" rid="F3">Figure 3A</xref>). Specifically, to construct a 4×4 representational dissimilarity matrix (RDM) for coarse stimulus information, we included two regressors for the coarse stimulus information (left vs. right) and two regressors for the rule information (rule 1 vs. rule 2). To construct the 4×4 RDM for fine stimulus information, we included two regressors for the fine stimulus information (inner vs. outer) and two regressors for the rule information (rule 1 vs. rule 2). To construct the 4×4 RDM for rule information, we included four regressors for the rule information (all four cue colours). Finally, to construct the 4×4 RDM for response information, we included two regressors for the response information (inner vs. outer buttons) and two regressors for the cue colours information collapsing across rules to avoid their effect (one regressor for each of two colours from different rules). For the RSA GLMs, each trial contributed to the estimation of 2 regressors (e.g., coarse stimulus information and rule). In all GLMs, movement parameters, block means, and run means were also included as covariates of no interest, and trials were modelled as epochs lasting from stimulus onset until response (<xref ref-type="bibr" rid="R33">Grinband et al., 2008</xref>). Error trials were excluded from the analysis. We estimated the regressors for each of the 12 blocks of each participant obtaining 12 beta values which we used in MVPA.</p></sec><sec id="S16"><title>Analysis 1: Multivariate pattern analysis (MVPA)</title><p id="P55">We used a standard multivariate pattern decoding to quantify the level of different types of information consisting of coarse and fine stimulus, rule and response information, as defined above, in fMRI (<xref ref-type="fig" rid="F2">Figure 2A</xref>) and MEG (<xref ref-type="fig" rid="F2">Figure 2B</xref>).</p><p id="P56">In fMRI, we used a leave-one-block-out cross-validation approach for decoding using a linear SVM classifier as implemented in Matlab. This way, we trained the classifier on all-minus-one blocks and tested the classifier on the remaining one block and repeated the procedure until all blocks were used in testing once. The decoding accuracy for a single participant was calculated by averaging the decoding results across cross-validation runs. The number of voxels within each ROI determined the dimension of the data entered into the classifier. The classification was performed for each ROI in each hemisphere separately and then averaged across the two hemispheres and reported.</p><p id="P57">In MEG, we used a time-resolved decoding procedure (<xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>), where we repeated the decoding analysis on every time point along the trial, once using the data aligned to the stimulus onset time (<italic>stimulus-aligned</italic> analysis) and once with the data aligned to the response time of each trial (<italic>response-aligned</italic> analysis). The two different alignments provide insights about the temporal profile of information encoding affected by stimulus onset and those aligned to the behavioural outputs. We used a 10-fold cross-validation procedure, with 9 folds of the data used for training the classifier and the left-out fold used for testing it and repeating the procedure 10 times until all folds are used once in testing. The decoding accuracy for a single participant was calculated by averaging the decoding results across these cross-validation runs.</p></sec><sec id="S17"><title>Analysis 2: RSA-based fMRI-MEG fusion</title><p id="P58">We used RSA to provide a common platform for comparing the representation of information across ROIs in fMRI and across time in MEG. For each participant (different in fMRI and MEG), ROI in fMRI, or time point in MEG, we constructed four distinct neural representational dissimilarity matrices (RDMs) by decoding different combinations of stimuli, cues, rules and responses (<xref ref-type="fig" rid="F3">Figure 3A</xref>). These 4 RDMs were designed to capture coarse and fine stimulus, rule and response information. Each cell in the RDM reflects the decoding accuracy of the two conditions of the column and row of the matrix. For example, the decoding value at the intersection of column “Right stim. Rule 1” and row “Left stim. Rule 1” contains the decoding of right stimuli in rule 1 and left stimuli in rule 1, therefore reflecting information (dissimilarity) about stimulus side irrespective of other variables such as rule. We obtained fMRI neural RDMs for every ROI and then averaged them across hemispheres. We also obtained MEG neural RDMs for every time point across the trial, once for the <italic>stimulus-aligned</italic> data and once for <italic>response-aligned</italic> data. MEG RDMs were constructed using signals from the sensors over the whole brain.</p><p id="P59">We also constructed 4 theoretical model RDMs to quantify coarse and fine stimulus, rule and response information within neural RDMs explained above (<xref ref-type="fig" rid="F3">Figure 3A</xref>). For example, in the coarse <italic>stimulus information</italic> model RDM, the elements which corresponded to the decoding of right stimuli from right stimuli, or left stimuli from left stimuli representations (and not their cross conditions) were valued as 0.5, and the elements which corresponded to the cross-conditions between right and left stimuli were valued as 1. For RSA, fusion and connectivity analyses (explained below), we selected and reshaped the lower triangular elements of the RDMs (excluding the diagonal elements) into vector RDMs (RDVs).</p><p id="P60">To fuse fMRI and MEG representations, we first averaged the RDM across participants and used a model-based approach using the subject-average neural fMRI and MEG RDVs and the model RDVs (Hebart et al., 2017, <xref ref-type="bibr" rid="R51">Moerel et al., 2024</xref>). To that end, we used <italic>Spearman’s</italic> partial correlation analysis and obtained one commonality index for each ROI in fMRI at each time point of MEG and using each of the four models as defined in (1): <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mspace width="0.2em"/><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mi>M</mml:mi><mml:mi>R</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mi>o</mml:mi><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>E</mml:mi><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>ρ</italic><sub><italic>fMRl(Rol)MEG(t)</italic></sub> refers to the Spearman correlation between the neural fMRI RDV in region <italic>Rol</italic> and neural MEG RDV at time <italic>t, ρ</italic><sub><italic>fMRl(Rol)Model(m)</italic></sub> refers to the neural fMRI RDV in region <italic>Rol</italic> and model RDV <italic>m</italic>, and <italic>ρ</italic><sub><italic>MEG(t)Model(m)</italic></sub> refers to the neural MEG RDV at time <italic>t</italic> and model RDV <italic>m</italic>. Using this formula, in which the fraction term is simply the correlation between the fMRI and MEG RDVs where model RDV is partialled out, we obtain four commonality indices for the four models in each ROI at every 5 ms time point. <xref ref-type="disp-formula" rid="FD1">Equation 1</xref> results in the brown area indicated in the Venn diagram shown in <xref ref-type="fig" rid="F4">Figure 4A</xref>.</p></sec><sec id="S18"><title>Analysis 3: Fusion-based representational connectivity analysis (Fusion-RCA)</title><p id="P61">We developed the Fusion-RCA to investigate how representation/information is transferred within the MDN and between the MDN and visual areas. While several recent RCA approaches have been used for the evaluation of connectivity, they all used one brain imaging modality. Specifically, while the concept of RCA is not specific to the imaging modality and can potentially be applied to all multivariate recording modalities, all previous <xref ref-type="sec" rid="S9">methods</xref> used either fMRI (<xref ref-type="bibr" rid="R18">Coutanche et al., 2020</xref>) or E/MEG (<xref ref-type="bibr" rid="R30">Goddard et al., 2016</xref>; <xref ref-type="bibr" rid="R42">Karimi-Rouzbahani, 2018</xref>; <xref ref-type="bibr" rid="R45">Karimi-Rouzbahani et al., 2019</xref>; <xref ref-type="bibr" rid="R48">Kietzmann et al., 2019</xref>; Karimi-Rouzbahani et al., 2021a; <xref ref-type="bibr" rid="R44">Karimi-Rouzbahani et al., 2021b</xref>; <xref ref-type="bibr" rid="R29">Goddard et al., 2022</xref>; <xref ref-type="bibr" rid="R47">Karimi-Rouzbahani et al., 2022</xref>), which respectively lack the temporal and spatial resolution needed to track rapid and spatially fine-grained cognitive processes in the brain.</p><p id="P62">Here we utilise the spatiotemporal specificity of the commonality time courses, which we obtain through fMRI-MEG fusion, and the logic of Granger causality, to track information across the brain with more spatial and temporal resolution than is achievable from either fMRI or MEG alone. Specifically, to evaluate information flow, we evaluated Granger causal relationships between the time courses of commonality indices from different ROIs and for different types of information separately (<xref ref-type="fig" rid="F4">Figure 4A</xref>). We use the multivariate Granger causality (MVGC) toolbox (<xref ref-type="bibr" rid="R5">Barnett and Seth, 2014</xref>) which allows the evaluation of Granger causality between multiple areas simultaneously. Granger causality suggests that if signal (here commonality index) X at the present time point improves the prediction of future time point of signal Y beyond present time point of signal Y, then X is said to Granger cause Y. As an advantage over conventional implementations of Granger causality, which involve the estimation of regression parameters once for the full model and once for the reduced model, MVGC does not explicitly estimate the reduced model, eliminating one source of estimation error leading to improved statistical power. It is achieved through the calculation of multiple equivalent representations of vector autoregressive models (<xref ref-type="bibr" rid="R5">Barnett and Seth, 2014</xref>), which is the core of Granger causality prediction. Application of MVGC to the commonality indices results in matrices of connectivity (<xref ref-type="fig" rid="F4">Figure 4A</xref>), reflecting measured Granger causal relationship/connectivity between source and destination ROIs, and determining the direction of causality/information flow. Note that as the commonality indices used in our new connectivity analysis are already specific about the type of information they reflect, the resultant connectivity quantifies the potential flow of information, rather than activations as conventionally measured by connectivity measures. For a review of caveats and nuances of model-based and model-free RCA see <xref ref-type="bibr" rid="R47">Karimi-Rouzbahani et al. (2022)</xref>. After statistical analysis (below) we visualised the connectivity results into an Information Transfer Diagram in which significant Granger-causal influences are depicted with an arrow (<xref ref-type="fig" rid="F4">Figure 4C</xref>).</p></sec></sec><sec id="S19"><title>Statistical analyses</title><sec id="S20"><title>Bayes factor analysis</title><p id="P63">We used Bayes factor t-tests, as implemented by Bart Krekelberg<sup><xref ref-type="fn" rid="FN2">2</xref></sup> based on <xref ref-type="bibr" rid="R58">Rouder et al. (2012)</xref>, for comparing the levels of decoding across conditions and between a condition and chance decoding. We used standard rules of thumb for interpreting levels of evidence (<xref ref-type="bibr" rid="R49">Lee and Wagenmakers, 2005</xref>; <xref ref-type="bibr" rid="R20">Dienes, 2014</xref>): Bayes factors of &gt;3 and &lt;1/3 were interpreted as evidence for the alternative and null hypotheses, respectively. We interpreted Bayes factors between 3 and 1/3 as insufficient evidence either way.</p><p id="P64">To evaluate the evidence for the null and alternative hypotheses of at-chance and above-chance decoding in fMRI, respectively, we generated a null distribution containing 1000 decoding values obtained by randomising class labels 1000 times (random permutation). As we were also interested in evaluating the effect of perceptual difficulty on information coding, we also used Bayes factor analysis to evaluate the evidence for difference in decoding levels between coarse and fine stimulus information across participants in each ROI separately.</p><p id="P65">To evaluate the evidence for the null and alternative hypotheses of at-chance and above-chance decoding in MEG, respectively, we generated a null distribution containing 1000 decoding values obtained by randomising class labels 1000 times (random permutation) for every time point. As in fMRI, to evaluate the evidence for difference in decoding levels between coarse and fine stimulus information, we compared their decoding rates across participants at every time point. Accordingly, we performed the Bayes factor analysis for alternative (i.e., difference; H1) versus the null (i.e., no difference; H0) hypotheses.</p><p id="P66">The priors for all Bayes factor analyses were determined based on Jeffrey-Zellner-Siow priors (Jeffreys, 1998; <xref ref-type="bibr" rid="R73">Zellner and Siow, 1980</xref>) from the Cauchy distribution based on the effect size that is initially calculated in the algorithm using t-test (<xref ref-type="bibr" rid="R58">Rouder et al., 2012</xref>). The priors are data-driven and have been shown to be invariant with respect to linear transformations of measurement units (<xref ref-type="bibr" rid="R58">Rouder et al., 2012</xref>), which reduces the chance of being biased towards the null or alternative hypotheses. We did not perform correction for multiple comparisons when using Bayes factors as they are much more conservative than frequentist analysis (<xref ref-type="bibr" rid="R27">Gelman and Tuerlinckx, 2000</xref>; <xref ref-type="bibr" rid="R28">Gelman et al., 2012</xref>).</p></sec><sec id="S21"><title>Permutation testing for fusion</title><p id="P67">To evaluate the significance of commonality indices obtained using partial correlation, we tested the true partial correlations against a null distribution obtained by shuffling the class labels in fMRI data and regenerating the RDMs using the data with shuffled labels 1000 times. We compared the true commonality indices at every time point with the randomly generated commonality indices for the same time point and deemed it significant if it exceeded 95% of the random correlations (p &lt; 0.05) after correcting for multiple comparisons across time (using Matlab mafdr function which uses the direct approach of Storey, 2002, where the algorithm fixes the rejection region and then estimates its corresponding error rate resulting in increased accuracy and power). We used a permutation approach rather than Bayes factor analysis here as there was no chance level of commonality known <italic>a priori</italic>. For quantitative comparison of commonality indices, we extracted three parameters from the commonality time courses consisting of “time to first significant commonality” (time from stimulus onset to the first significant commonality), “time to maximum commonality” (time from stimulus onset to the maximum commonality) and “time from maximum commonality” (time from the maximum commonality to the response).</p></sec><sec id="S22"><title>Connectivity statistics</title><p id="P68">For statistical testing of connectivity results, we used the statistical test results produced by the MVGC toolbox. The significance of connectivity indices obtained from the MVGC is evaluated based on a theoretical (asymptotic) null distribution which is corrected for multiple comparisons based on the number of source and destination ROIs, number of time points within the trial, the number of trials and the order of the auto-regression model (<xref ref-type="bibr" rid="R5">Barnett and Seth, 2014</xref>). The method shuffles the time samples in the time series (commonality indices here) to generate null time series from the true time series and evaluates the significance of the true connectivity values against those obtained from the null time series.</p></sec></sec></body><back><ack id="S23"><title>Acknowledgements</title><p>The authors thank Dr Amanda Robinson for providing the MEG data and the experimental scripts and for helpful discussion of the experimental design. This work was supported by Australian Research Council (ARC) Discovery Project grant DP170101840 awarded to AW and ANR. HK-R was further supported by Newton International Fellowship (NIF\R1\192608) and follow-on funding (AL\231037) from the Royal Society. AW was supported by Medical Research Council (UK) intramural funding SUAG/093/G116768 and an ARC Future Fellowship FT170100105. ANR is supported by an ARC Future Fellowship FT230100119. The authors acknowledge the facilities and scientific and technical assistance of the National Imaging Facility, a National Collaborative Research Infrastructure Strategy (NCRIS) capability, at Macquarie Medical Imaging and the KIT-Macquarie Brain Research Laboratory, Macquarie University. For the purpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P69">Information which is represented more strongly can suppress weaker information when the former is partialled out from the latter in RSA, making it harder to detect the weaker information.</p></fn><fn id="FN2"><label>2</label><p id="P70"><ext-link ext-link-type="uri" xlink:href="https://klabhub.github.io/bayesFactor/">https://klabhub.github.io/bayesFactor/</ext-link></p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anzellotti</surname><given-names>S</given-names></name><name><surname>Coutanche</surname><given-names>MN</given-names></name></person-group><article-title>Beyond functional connectivity: investigating networks of multivariate representations</article-title><source>Trends in cognitive sciences</source><year>2018</year><volume>22</volume><issue>3</issue><fpage>258</fpage><lpage>269</lpage><pub-id pub-id-type="pmid">29305206</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>A domain-general cognitive core defined in multimodally parcellated human cortex</article-title><source>Cerebral Cortex</source><year>2020</year><volume>30</volume><issue>8</issue><fpage>4361</fpage><lpage>4380</lpage><pub-id pub-id-type="pmcid">PMC7325801</pub-id><pub-id pub-id-type="pmid">32244253</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhaa023</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Shashidhara</surname><given-names>S</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Precise topology of adjacent domain-general and sensory-biased regions in the human brain</article-title><source>BioRxiv</source><year>2021</year><pub-id pub-id-type="pmcid">PMC9201597</pub-id><pub-id pub-id-type="pmid">34628494</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab362</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baltes</surname><given-names>PB</given-names></name><name><surname>Staudinger</surname><given-names>UM</given-names></name><name><surname>Lindenberger</surname><given-names>U</given-names></name></person-group><article-title>Lifespan psychology: Theory and application to intellectual functioning</article-title><source>Annual review of psychology</source><year>1999</year><volume>50</volume><fpage>471</fpage><lpage>507</lpage><pub-id pub-id-type="pmid">15012462</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of neuroscience methods</source><year>2014</year><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Basti</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Marzetti</surname><given-names>L</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>Multi-dimensional connectivity: a conceptual and mathematical review</article-title><source>NeuroImage</source><year>2020</year><volume>221</volume><elocation-id>117179</elocation-id><pub-id pub-id-type="pmid">32682988</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name><name><surname>Vision</surname><given-names>S</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bullmore</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Complex brain networks: graph theoretical analysis of structural and functional systems</article-title><source>Nature reviews neuroscience</source><year>2009</year><volume>10</volume><issue>3</issue><fpage>186</fpage><lpage>198</lpage><pub-id pub-id-type="pmid">19190637</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bush</surname><given-names>G</given-names></name><name><surname>Luu</surname><given-names>P</given-names></name><name><surname>Posner</surname><given-names>MI</given-names></name></person-group><article-title>Cognitive and emotional influences in anterior cingulate cortex</article-title><source>Trends in cognitive sciences</source><year>2000</year><volume>4</volume><issue>6</issue><fpage>215</fpage><lpage>222</lpage><pub-id pub-id-type="pmid">10827444</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>CS</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>The contribution of the anterior cingulate cortex to executive processes in cognition</article-title><source>Reviews in the Neurosciences</source><year>1999</year><volume>10</volume><issue>1</issue><fpage>49</fpage><lpage>58</lpage><pub-id pub-id-type="pmid">10356991</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>AM/EEG-fMRI fusion primer: resolving human brain responses in space and time</article-title><source>Neuron</source><year>2020</year><volume>107</volume><issue>5</issue><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="pmcid">PMC7612024</pub-id><pub-id pub-id-type="pmid">32721379</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Schneider</surname><given-names>W</given-names></name></person-group><article-title>The cognitive control network: Integrated cortical regions with dissociable functions</article-title><source>Neuroimage</source><year>2007</year><volume>37</volume><issue>1</issue><fpage>343</fpage><lpage>360</lpage><pub-id pub-id-type="pmid">17553704</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Pathak</surname><given-names>S</given-names></name><name><surname>Schneider</surname><given-names>W</given-names></name></person-group><article-title>Identifying the brain’s most globally connected regions</article-title><source>Neuroimage</source><year>2010</year><volume>49</volume><issue>4</issue><fpage>3132</fpage><lpage>3148</lpage><pub-id pub-id-type="pmid">19909818</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Reynolds</surname><given-names>JR</given-names></name><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Repovs</surname><given-names>G</given-names></name><name><surname>Anticevic</surname><given-names>A</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title><source>Nature neuroscience</source><year>2013</year><volume>16</volume><issue>9</issue><fpage>1348</fpage><lpage>1355</lpage><pub-id pub-id-type="pmcid">PMC3758404</pub-id><pub-id pub-id-type="pmid">23892552</pub-id><pub-id pub-id-type="doi">10.1038/nn.3470</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colom</surname><given-names>R</given-names></name><name><surname>Martínez-Molina</surname><given-names>A</given-names></name><name><surname>Shih</surname><given-names>PC</given-names></name><name><surname>Santacreu</surname><given-names>J</given-names></name></person-group><article-title>Intelligence, working memory, and multitasking performance</article-title><source>Intelligence</source><year>2010</year><volume>38</volume><issue>6</issue><fpage>543</fpage><lpage>551</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><article-title>Informational connectivity: identifying synchronized discriminability of multi-voxel patterns across the brain</article-title><source>Frontiers in human neuroscience</source><year>2013</year><volume>7</volume><fpage>15</fpage><pub-id pub-id-type="pmcid">PMC3566529</pub-id><pub-id pub-id-type="pmid">23403700</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00015</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coutanche</surname><given-names>MN</given-names></name><name><surname>Akpan</surname><given-names>E</given-names></name><name><surname>Buckser</surname><given-names>RR</given-names></name></person-group><article-title>Representational connectivity analysis: identifying networks of shared changes in representational strength through jackknife resampling</article-title><source>bioRxiv</source><year>2020</year></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crittenden</surname><given-names>BM</given-names></name><name><surname>Mitchell</surname><given-names>DJ</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Task encoding across the multiple demand cortex is consistent with a frontoparietal and cingulo-opercular dual networks distinction</article-title><source>Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>23</issue><fpage>6147</fpage><lpage>6155</lpage><pub-id pub-id-type="pmcid">PMC4899522</pub-id><pub-id pub-id-type="pmid">27277793</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4590-15.2016</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dienes</surname><given-names>Z</given-names></name></person-group><article-title>Using Bayes to get the most out of non-significant results</article-title><source>Frontiers in psychology</source><year>2014</year><volume>5</volume><fpage>781</fpage><pub-id pub-id-type="pmcid">PMC4114196</pub-id><pub-id pub-id-type="pmid">25120503</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00781</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dosenbach</surname><given-names>NU</given-names></name><name><surname>Visscher</surname><given-names>KM</given-names></name><name><surname>Palmer</surname><given-names>ED</given-names></name><name><surname>Miezin</surname><given-names>FM</given-names></name><name><surname>Wenger</surname><given-names>KK</given-names></name><name><surname>Kang</surname><given-names>HC</given-names></name><etal/><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>A core system for the implementation of task sets</article-title><source>Neuron</source><year>2006</year><volume>50</volume><issue>5</issue><fpage>799</fpage><lpage>812</lpage><pub-id pub-id-type="pmcid">PMC3621133</pub-id><pub-id pub-id-type="pmid">16731517</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2006.04.031</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in cognitive sciences</source><year>2010</year><volume>14</volume><issue>4</issue><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name></person-group><article-title>Common regions of the human frontal lobe recruited by diverse cognitive demands</article-title><source>Trends in neurosciences</source><year>2000</year><volume>23</volume><issue>10</issue><fpage>475</fpage><lpage>483</lpage><pub-id pub-id-type="pmid">11006464</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Shashidhara</surname><given-names>S</given-names></name></person-group><article-title>Integrated intelligence from distributed brain activity</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>10</issue><fpage>838</fpage><lpage>852</lpage><pub-id pub-id-type="pmcid">PMC7116395</pub-id><pub-id pub-id-type="pmid">32771330</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.06.012</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Broad domain generality in focal regions of frontal and parietal cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2013</year><volume>110</volume><issue>41</issue><fpage>16616</fpage><lpage>16621</lpage><pub-id pub-id-type="pmcid">PMC3799302</pub-id><pub-id pub-id-type="pmid">24062451</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1315235110</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geerligs</surname><given-names>L</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>Functional connectivity and structural covariance between regions of interest can be measured more accurately using multivariate distance correlation</article-title><source>NeuroImage</source><year>2016</year><volume>135</volume><fpage>16</fpage><lpage>31</lpage><pub-id pub-id-type="pmcid">PMC4922835</pub-id><pub-id pub-id-type="pmid">27114055</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.04.047</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Tuerlinckx</surname><given-names>F</given-names></name></person-group><article-title>Type S error rates for classical and Bayesian single and multiple comparison procedures</article-title><source>Computational statistics</source><year>2000</year><volume>15</volume><issue>3</issue><fpage>373</fpage><lpage>390</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hill</surname><given-names>J</given-names></name><name><surname>Yajima</surname><given-names>M</given-names></name></person-group><article-title>Why we (usually) don’t have to worry about multiple comparisons</article-title><source>Journal of research on educational effectiveness</source><year>2012</year><volume>5</volume><issue>2</issue><fpage>189</fpage><lpage>211</lpage></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname><given-names>E</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Spatial and Feature-selective Attention Have Distinct, Interacting Effects on Population-level Tuning</article-title><source>Journal of cognitive neuroscience</source><year>2022</year><volume>34</volume><issue>2</issue><fpage>290</fpage><lpage>312</lpage><pub-id pub-id-type="pmcid">PMC7613071</pub-id><pub-id pub-id-type="pmid">34813647</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_01796</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goddard</surname><given-names>E</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name><name><surname>Dermody</surname><given-names>N</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Representational dynamics of object recognition: Feedforward and feedback information flows</article-title><source>Neuroimage</source><year>2016</year><volume>128</volume><fpage>385</fpage><lpage>397</lpage><pub-id pub-id-type="pmid">26806290</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Itzchak</surname><given-names>Y</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>Differential processing of objects under various viewing conditions in the human lateral occipital complex</article-title><source>Neuron</source><year>1999</year><volume>24</volume><issue>1</issue><fpage>187</fpage><lpage>203</lpage><pub-id pub-id-type="pmid">10677037</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Hendler</surname><given-names>T</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>The dynamics of object-selective activation correlate with recognition performance in humans</article-title><source>Nature neuroscience</source><year>2000</year><volume>3</volume><issue>8</issue><fpage>837</fpage><lpage>843</lpage><pub-id pub-id-type="pmid">10903579</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grinband</surname><given-names>J</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Lindquist</surname><given-names>M</given-names></name><name><surname>Ferrera</surname><given-names>VP</given-names></name><name><surname>Hirsch</surname><given-names>J</given-names></name></person-group><article-title>Detection of time-varying signals in event-related fMRI designs</article-title><source>Neuroimage</source><year>2008</year><volume>43</volume><issue>3</issue><fpage>509</fpage><lpage>520</lpage><pub-id pub-id-type="pmcid">PMC2654219</pub-id><pub-id pub-id-type="pmid">18775784</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.07.065</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Wakeman</surname><given-names>DG</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name></person-group><article-title>Comparison of noise-normalized minimum norm estimates for MEG analysis using multiple resolution metrics</article-title><source>Neuroimage</source><year>2011</year><volume>54</volume><issue>3</issue><fpage>1966</fpage><lpage>1974</lpage><pub-id pub-id-type="pmcid">PMC3018574</pub-id><pub-id pub-id-type="pmid">20884360</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.09.053</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational dynamics of task and object processing in humans</article-title><source>Elife</source><year>2018</year><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higo</surname><given-names>T</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name><name><surname>Buch</surname><given-names>ER</given-names></name><name><surname>Rushworth</surname><given-names>MF</given-names></name></person-group><article-title>Distributed and causal influence of frontal operculum in task control</article-title><source>Proceedings of the National Academy of Sciences</source><year>2011</year><volume>108</volume><issue>10</issue><fpage>4230</fpage><lpage>4235</lpage><pub-id pub-id-type="pmcid">PMC3054014</pub-id><pub-id pub-id-type="pmid">21368109</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1013361108</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>JB</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Adaptive coding in the human brain: Distinct object features are encoded by overlapping voxels in frontoparietal cortex</article-title><source>Cortex</source><year>2018</year><volume>108</volume><fpage>25</fpage><lpage>34</lpage><pub-id pub-id-type="pmcid">PMC6629547</pub-id><pub-id pub-id-type="pmid">30121000</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2018.07.006</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>JB</given-names></name><name><surname>Feredoes</surname><given-names>E</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name><name><surname>Lindner</surname><given-names>M</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Concurrent neuroimaging and neurostimulation reveals a causal role for dlPFC in coding of task-relevant information</article-title><source>Communications biology</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmcid">PMC8128861</pub-id><pub-id pub-id-type="pmid">34002006</pub-id><pub-id pub-id-type="doi">10.1038/s42003-021-02109-x</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jackson</surname><given-names>J</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Feature-selective attention in frontoparietal cortex: multivoxel codes adjust to prioritize task-relevant information</article-title><source>Journal of cognitive neuroscience</source><year>2017</year><volume>29</volume><issue>2</issue><fpage>310</fpage><lpage>321</lpage><pub-id pub-id-type="pmid">27626230</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jeffreys</surname><given-names>H</given-names></name></person-group><source>Theory of probability</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>New York</publisher-loc><year>1961</year><edition>3rd</edition></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>RE</given-names></name><name><surname>Haier</surname><given-names>RJ</given-names></name></person-group><article-title>The Parieto-Frontal Integration Theory (P-FIT) of intelligence: converging neuroimaging evidence</article-title><source>Behavioral and brain sciences</source><year>2007</year><volume>30</volume><issue>2</issue><fpage>135</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">17655784</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name></person-group><article-title>Three-stage processing of category and variation information by entangled interactive mechanisms of peri-occipital and peri-frontal cortices</article-title><source>Scientific reports</source><year>2018</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>22</lpage><pub-id pub-id-type="pmcid">PMC6093927</pub-id><pub-id pub-id-type="pmid">30111859</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-30601-8</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name><name><surname>McGonigal</surname><given-names>A</given-names></name></person-group><article-title>Generalisability of epileptiform patterns across time and patients</article-title><source>Scientific Reports</source><year>2024</year><volume>14</volume><issue>1</issue><elocation-id>6293</elocation-id><pub-id pub-id-type="pmcid">PMC10942983</pub-id><pub-id pub-id-type="pmid">38491096</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-56990-7</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name><name><surname>Ramezani</surname><given-names>F</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Rich</surname><given-names>A</given-names></name><name><surname>Ghodrati</surname><given-names>M</given-names></name></person-group><article-title>Perceptual difficulty modulates the direction of information flow in familiar face recognition</article-title><source>NeuroImage</source><year>2021b</year><volume>233</volume><elocation-id>117896</elocation-id><pub-id pub-id-type="pmcid">PMC7614447</pub-id><pub-id pub-id-type="pmid">33667671</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117896</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name><name><surname>Vahab</surname><given-names>E</given-names></name><name><surname>Ebrahimpour</surname><given-names>R</given-names></name><name><surname>Menhaj</surname><given-names>MB</given-names></name></person-group><article-title>Spatiotemporal analysis of category and target-related information processing in the brain during object detection</article-title><source>Behavioural brain research</source><year>2019</year><volume>362</volume><fpage>224</fpage><lpage>239</lpage><pub-id pub-id-type="pmid">30654124</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name></person-group><article-title>Neural signatures of vigilance decrements predict behavioural errors before they occur</article-title><source>ELife</source><year>2021b</year><volume>10</volume><elocation-id>e60563</elocation-id><pub-id pub-id-type="pmcid">PMC8060034</pub-id><pub-id pub-id-type="pmid">33830017</pub-id><pub-id pub-id-type="doi">10.7554/eLife.60563</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi-Rouzbahani</surname><given-names>H</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name></person-group><article-title>Caveats and nuances of model-based and model-free representational connectivity analysis</article-title><source>Frontiers in Neuroscience</source><year>2022</year><volume>16</volume><pub-id pub-id-type="pmcid">PMC8960982</pub-id><pub-id pub-id-type="pmid">35360178</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.755988</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LK</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><issue>43</issue><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name></person-group><article-title>Bayesian statistical inference in psychology: comment on Trafimow (2003)</article-title><source>Psychological Review</source><year>2005</year><volume>112</volume><fpage>662</fpage><lpage>668</lpage><pub-id pub-id-type="pmid">16060758</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marek</surname><given-names>S</given-names></name><name><surname>Dosenbach</surname><given-names>NU</given-names></name></person-group><article-title>The frontoparietal network: function, electrophysiology, and importance of individual precision mapping</article-title><source>Dialogues in clinical neuroscience</source><year>2018</year><volume>20</volume><issue>2</issue><fpage>133</fpage><lpage>140</lpage><pub-id pub-id-type="pmcid">PMC6136121</pub-id><pub-id pub-id-type="pmid">30250390</pub-id><pub-id pub-id-type="doi">10.31887/DCNS.2018.20.2/smarek</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moerel</surname><given-names>D</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Selective attention and decision-making have separable neural bases in space and time</article-title><source>Journal of Neuroscience</source><year>2024</year><pub-id pub-id-type="pmcid">PMC11411586</pub-id><pub-id pub-id-type="pmid">39107058</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0224-24.2024</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenzadeh</surname><given-names>Y</given-names></name><name><surname>Qin</surname><given-names>S</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name></person-group><article-title>Ultra-Rapid serial visual presentation reveals dynamics of feedforward and feedback processes in the ventral visual pathway</article-title><source>Elife</source><year>2018</year><volume>7</volume><elocation-id>e36329</elocation-id><pub-id pub-id-type="pmcid">PMC6029845</pub-id><pub-id pub-id-type="pmid">29927384</pub-id><pub-id pub-id-type="doi">10.7554/eLife.36329</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panichello</surname><given-names>MF</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><article-title>Shared mechanisms underlie the control of working memory and attention</article-title><source>Nature</source><year>2021</year><volume>592</volume><issue>7855</issue><fpage>601</fpage><lpage>605</lpage><pub-id pub-id-type="pmcid">PMC8223505</pub-id><pub-id pub-id-type="pmid">33790467</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03390-w</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Lessov-Schlaggar</surname><given-names>CN</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Evidence for hubs in human functional brain networks</article-title><source>Neuron</source><year>2013</year><volume>79</volume><issue>4</issue><fpage>798</fpage><lpage>813</lpage><pub-id pub-id-type="pmcid">PMC3838673</pub-id><pub-id pub-id-type="pmid">23972601</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.035</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahimi</surname><given-names>S</given-names></name><name><surname>Jackson</surname><given-names>RL</given-names></name><name><surname>Farahibozorg</surname><given-names>SR</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name></person-group><article-title>Time Lagged Multidimensional Pattern Connectivity (TL MDPC): An EEG/MEG Pattern Transformation Based Functional Connectivity Metric</article-title><source>bioRxiv</source><year>2022</year><pub-id pub-id-type="pmcid">PMC10030313</pub-id><pub-id pub-id-type="pmid">36813063</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2023.119958</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Linking the brain with behavior: the neural dynamics of success and failure in goal-directed behavior</article-title><source>Journal of cognitive neuroscience</source><year>2022</year><volume>34</volume><issue>4</issue><fpage>639</fpage><lpage>654</lpage><pub-id pub-id-type="pmid">35061019</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rorden</surname><given-names>C</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name></person-group><article-title>Stereotaxic display of brain lesions</article-title><source>Behavioural neurology</source><year>2000</year><volume>12</volume><issue>4</issue><fpage>191</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">11568431</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Morey</surname><given-names>RD</given-names></name><name><surname>Speckman</surname><given-names>PL</given-names></name><name><surname>Province</surname><given-names>JM</given-names></name></person-group><article-title>Default Bayes factors for ANOVA designs</article-title><source>Journal of Mathematical Psychology</source><year>2012</year><volume>56</volume><issue>5</issue><fpage>356</fpage><lpage>374</lpage></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadaghiani</surname><given-names>S</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>D’Esposito</surname><given-names>M</given-names></name></person-group><article-title>Ongoing dynamics in large-scale functional connectivity predict perception</article-title><source>Proceedings of the National Academy of Sciences</source><year>2015</year><volume>112</volume><issue>27</issue><fpage>8463</fpage><lpage>8468</lpage><pub-id pub-id-type="pmcid">PMC4500238</pub-id><pub-id pub-id-type="pmid">26106164</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1420687112</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shashidhara</surname><given-names>S</given-names></name><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Task and stimulus coding in the multiple-demand network</article-title><source>Cerebral Cortex</source><year>2024</year><volume>34</volume><issue>7</issue><pub-id pub-id-type="pmcid">PMC11246790</pub-id><pub-id pub-id-type="pmid">39004756</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhae278</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stiers</surname><given-names>P</given-names></name><name><surname>Mennes</surname><given-names>M</given-names></name><name><surname>Sunaert</surname><given-names>S</given-names></name></person-group><article-title>Distributed task coding throughout the multiple demand network of the human frontal–insular cortex</article-title><source>Neuroimage</source><year>2010</year><volume>52</volume><issue>1</issue><fpage>252</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">20362676</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Attentional modulation of stimulus representation in human fronto-parietal cortex</article-title><source>Neuroimage</source><year>2009</year><volume>48</volume><issue>2</issue><fpage>436</fpage><lpage>448</lpage><pub-id pub-id-type="pmid">19577650</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visscher</surname><given-names>KM</given-names></name><name><surname>Miezin</surname><given-names>FM</given-names></name><name><surname>Kelly</surname><given-names>JE</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Donaldson</surname><given-names>DI</given-names></name><name><surname>McAvoy</surname><given-names>MP</given-names></name><etal/><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Mixed blocked/event-related designs separate transient and sustained activity in fMRI</article-title><source>Neuroimage</source><year>2003</year><volume>19</volume><issue>4</issue><fpage>1694</fpage><lpage>1708</lpage><pub-id pub-id-type="pmid">12948724</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wisniewski</surname><given-names>D</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Formica</surname><given-names>S</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>Adaptive coding of stimulus information in human frontoparietal cortex during visual classification</article-title><source>NeuroImage</source><year>2023</year><volume>274</volume><elocation-id>120150</elocation-id><pub-id pub-id-type="pmid">37191656</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Afshar</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name></person-group><article-title>Flexible coding of task rules in frontoparietal cortex: an adaptive system for flexible cognitive control</article-title><source>Journal of cognitive neuroscience</source><year>2015a</year><volume>27</volume><issue>10</issue><fpage>1895</fpage><lpage>1911</lpage><pub-id pub-id-type="pmid">26058604</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Dermody</surname><given-names>N</given-names></name><name><surname>Afshar</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name></person-group><article-title>Meaningful patterns of information in the brain revealed through analysis of errors</article-title><source>bioRxiv</source><year>2019</year><elocation-id>673681</elocation-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Manes</surname><given-names>F</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><article-title>The multiple-demand system but not the language system supports fluid intelligence</article-title><source>Nature human behaviour</source><year>2018</year><volume>2</volume><issue>3</issue><fpage>200</fpage><pub-id pub-id-type="pmcid">PMC6795543</pub-id><pub-id pub-id-type="pmid">31620646</pub-id><pub-id pub-id-type="doi">10.1038/s41562-017-0282-3</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Hampshire</surname><given-names>A</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Adaptive coding of task-relevant information in human frontoparietal cortex</article-title><source>Journal of Neuroscience</source><year>2011a</year><volume>31</volume><issue>41</issue><fpage>14592</fpage><lpage>14599</lpage><pub-id pub-id-type="pmcid">PMC6703398</pub-id><pub-id pub-id-type="pmid">21994375</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2616-11.2011</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>J</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Coding of visual, auditory, rule, and response information in the brain: 10 years of multivoxel pattern analysis</article-title><source>Journal of cognitive neuroscience</source><year>2016</year><volume>28</volume><issue>10</issue><fpage>1433</fpage><lpage>1454</lpage><pub-id pub-id-type="pmid">27315269</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Parr</surname><given-names>A</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Nimmo-Smith</surname><given-names>I</given-names></name><name><surname>Torralva</surname><given-names>T</given-names></name><etal/><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Fluid intelligence loss linked to restricted regions of damage within frontal and parietal cortex</article-title><source>Proceedings of the national academy of sciences</source><year>2010</year><volume>107</volume><issue>33</issue><fpage>14899</fpage><lpage>14902</lpage><pub-id pub-id-type="pmcid">PMC2930407</pub-id><pub-id pub-id-type="pmid">20679241</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1007928107</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Thompson</surname><given-names>R</given-names></name><name><surname>Bor</surname><given-names>D</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Multi-voxel coding of stimuli, rules, and responses in human frontoparietal cortex</article-title><source>Neuroimage</source><year>2011b</year><volume>56</volume><issue>2</issue><fpage>744</fpage><lpage>752</lpage><pub-id pub-id-type="pmid">20406690</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolgar</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Rich</surname><given-names>AN</given-names></name></person-group><article-title>Attention enhances multi-voxel representation of novel objects in frontal, parietal and visual cortices</article-title><source>Neuroimage</source><year>2015b</year><volume>109</volume><fpage>429</fpage><lpage>437</lpage><pub-id pub-id-type="pmid">25583612</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zellner</surname><given-names>A</given-names></name><name><surname>Siow</surname><given-names>A</given-names></name></person-group><article-title>Posterior odds ratios for selected regression hypotheses</article-title><source>Trabajos de estadística y de investigación operativa</source><year>1980</year><volume>31</volume><issue>1</issue><fpage>585</fpage><lpage>603</lpage></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>R</given-names></name><name><surname>Woolgar</surname><given-names>A</given-names></name></person-group><article-title>Radical flexibility of neural representations in service of flexible behaviour</article-title><year>2024</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental paradigm and the definition of different types of information (figure reproduced from <xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>).</title><p>(A) Experiment was a stimulus-response mapping task, where participants had to press the button that corresponded to each stimulus. Participants were trained to perform two distinct stimulus-response mappings (i.e., rules) which were indicated by the colour of fixation square (two per rule). We defined the coarse stimulus information as the distinction between the stimuli presented on the left vs. right side and the fine stimulus information as the distinction between the stimuli presented in the inner vs. outer positions in the visual field. Rule information refers to the distinction between the two rules and response information refers to the distinction between inner vs. outer finger press responses. (B) Participants were cued about which rule to apply on a trial-by-trial basis. Each trial started with a fixation cross, followed by a stimulus-rule display and then the response screen.</p></caption><graphic xlink:href="EMS199332-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Decoding of different types of information from fMRI ROIs and MEG time points.</title><p>(A) Average fMRI decoding (error bars = 95% confidence interval across participants) for every ROI. The theoretical chance-level decoding is 50%. Bottom panel: Bayesian evidence for above-chance decoding for each condition (relative to chance) and difference between coarse and fine stimulus information decoding (yellow), BF: Filled coloured circles show evidence (BF&gt;3) for the alternative hypothesis, and filled black circles reflect evidence for the null hypothesis (BF&lt;1/3). Empty circles indicate insufficient evidence (1/3&lt;BF&lt;3). BF were calculated using Bayesian t-tests. (B) Stimulus- and response-aligned decoding of stimulus information across time in MEG data. Thick lines show the average across participants (shading 95% confidence intervals), double-thickened lines show time points with evidence (BF&gt;3) for above-chance decoding. BFs are shown every 10 ms (rather than 5 ms) for clearer illustration. Horizontal dotted line refers to theoretical chance-level decoding (50%). Vertical dotted lines indicate critical times in the trial. Bottom panels should be read as in (A). (C) Same as (B) for rule and response information. Fine stimulus information, rule and responses were reproduced from <xref ref-type="bibr" rid="R56">Robinson et al., 2022</xref>. Note the different scales of the BF panels.</p></caption><graphic xlink:href="EMS199332-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Commonalities between fMRI, MEG and models.</title><p>(A) Four distinct neural Representational Dissimilarity Matrices (RDMs) were constructed for each fMRI ROI and each MEG time point by different arrangements of stimuli, rules, cues and responses, which allowed us to evaluate the commonality between fMRI and MEG representations and target models (here we are showing the models rather than neural RDMs). Note that the rows and columns of these matrices are different for each task feature (see <xref ref-type="sec" rid="S9">methods</xref>). Model RDMs had identical arrangements of conditions to the neural RDMs, but only included binary values (1 for high and 0.5 for low dissimilarity). (B) Commonality time courses aligned to the stimulus-onset (left column) and response (right column) times. Thickened lines indicate significant commonalities (p&lt;0.05; one-tailed random permutation testing; corrected for multiple comparisons across time). Note that only positive commonalities are interpretable as they reflect the correlation between modalities. Horizontal dotted line indicates no commonality and negative commonalities are not meaningful. Vertical dotted lines indicate critical times in the trial. (C-E) Quantitative commonality metrics: Time from the stimulus onset to the first significant (C) and to maximum (D) commonalities, and the time from the maximum commonality to the response (E), extracted from the commonality time courses for each information type. Bars are not shown where commonality traces did not reach significance.</p></caption><graphic xlink:href="EMS199332-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Fusion-based analysis of connectivity.</title><p>(A) RDMs from every fMRI Region of Interest (ROI), every MEG time point, and every model were entered into the commonality index equation (equation (1), see <xref ref-type="sec" rid="S9">Methods</xref>), and then the resultant commonality time series were fed to the multivariate Granger causality (MVGC) toolbox to determine potential flow of information between all six ROIs in both directions. (B) <italic>p</italic>-values of connectivity between every pair of ROIs in <italic>stimulus</italic> (upper) and <italic>response</italic> (lower)-<italic>aligned</italic> analyses. <italic>p</italic>-values were obtained using asymptotic null distribution corrected for multiple comparisons for the number of time samples and ROIs. Significant <italic>p</italic>-values (indicative of information exchange; &lt;0.05) are shown by red boxes. (C) Information transfer diagrams, visualising the results in (B). Arrows show the existence and the direction of information flow across ROIs in the <italic>stimulus</italic>-(orange) and <italic>response</italic> (green)-<italic>aligned</italic> analysis. Unidirectional connections are shown using dashed lines and bidirectional connections are shown using solid lines.</p></caption><graphic xlink:href="EMS199332-f004"/></fig></floats-group></article>