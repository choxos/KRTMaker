<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS201584</article-id><article-id pub-id-type="doi">10.1101/2024.11.27.625704</article-id><article-id pub-id-type="archive">PPR947747</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-stream predictions in human auditory cortex during natural music listening</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Robert</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">*</xref><xref ref-type="corresp" rid="CR1">°</xref></contrib><contrib contrib-type="author"><name><surname>Van Cang</surname><given-names>Mathieu Pham</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Mercier</surname><given-names>Manuel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Trébuchon</surname><given-names>Agnès</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Bartolomei</surname><given-names>Fabrice</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Arnal</surname><given-names>Luc H.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN2">†</xref></contrib><contrib contrib-type="author"><name><surname>Morillon</surname><given-names>Benjamin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN2">†</xref><xref ref-type="corresp" rid="CR1">°</xref></contrib><contrib contrib-type="author"><name><surname>Doelling</surname><given-names>Keith</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN2">†</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/035xkbk20</institution-id><institution>Aix-Marseille Université</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>INSERM</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/019kqby73</institution-id><institution>INS, Institut de Neurosciences des Systèmes</institution></institution-wrap>, <city>Marseille</city>, <country country="FR">France</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0495fxg12</institution-id><institution>Institut Pasteur</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f82e368</institution-id><institution>Université Paris Cité</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Inserm</institution></institution-wrap><postal-code>UA06</postal-code>, Institut de l’Audition, <city>Paris</city>, <country country="FR">France</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02en5vm52</institution-id><institution>Sorbonne Université</institution></institution-wrap>, Collège Doctoral, <postal-code>F-75005</postal-code><city>Paris</city>, <country country="FR">France</country></aff><aff id="A4"><label>4</label>APHM, Timone Hospital, Epileptology and Cerebral Rhythmology Department, Marseille, France</aff><author-notes><corresp id="CR1"><label>°</label>Corresponding authors</corresp><fn id="FN1"><label>*</label><p id="P1">First authors</p></fn><fn id="FN2"><label>†</label><p id="P2">Senior authors</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>01</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>29</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3">Real-world perception involves the prediction and integration of multiple dynamic objects and features in parallel, yet most research focuses on single-stream sequences. We present PolyRNN, a recurrent neural network designed to model predictions across multiple, simultaneous information streams, using polyphonic music as a case study. We recorded neurophysiological activity non invasively (MEG) and within the human cortex (intracranial EEG) while participants listened to real piano music. Musical expectations are encoded in P2- and P3-like components in auditory regions. Compared to a state-of-the-art generative music model, we demonstrate that parallelization better reflects the brain’s processing of simultaneous sequences compared to serialization. Overall, our approach enables the study of predictive processing in ecologically valid polyphonic music and provides a general framework for modeling predictions in simultaneous streams.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P4">Anticipation plays a crucial role in the perception of dynamic stimuli. For instance, tracking a moving object involves predicting its future position and adjusting eye fixations accordingly (<xref ref-type="bibr" rid="R27">Hayhoe et al., 2012</xref>), with the visual cortex responding strongly to errors in this prediction (<xref ref-type="bibr" rid="R70">Stefanics et al., 2014</xref>; <xref ref-type="bibr" rid="R67">Schellekens et al., 2016</xref>). To optimize anticipation, several streams of information can be integrated together, either across different sensory modalities (<xref ref-type="bibr" rid="R71">Talsma, 2015</xref>) — for example, visual cues aiding speech comprehension (<xref ref-type="bibr" rid="R45">Munhall et al., 1996</xref>; <xref ref-type="bibr" rid="R76">Van Wassenhove et al., 2005</xref>, <xref ref-type="bibr" rid="R3">Arnal et al., 2009</xref>) — or within a single modality, such as when prosodic (<xref ref-type="bibr" rid="R16">Frazier et al., 2006</xref>), structural (<xref ref-type="bibr" rid="R83">Weissbart and Martin, 2024</xref>) or contextual information (<xref ref-type="bibr" rid="R14">Donhauser &amp; Baillet, 2020</xref>; <xref ref-type="bibr" rid="R69">Slaats et al., 2023</xref>; <xref ref-type="bibr" rid="R23">Gwilliams et al., 2024</xref>) shapes speech processing.</p><p id="P5">Previous research has identified several architectures for sequence processing in humans, from transition probabilities to hierarchical tree structures (<xref ref-type="bibr" rid="R11">Dehaene et al., 2015</xref>). While this line of research effectively investigates single-stream sequences (<xref ref-type="bibr" rid="R43">Meyniel et al., 2016</xref>; <xref ref-type="bibr" rid="R40">Maheu et al., 2019</xref>; <xref ref-type="bibr" rid="R35">Lakretz et al., 2020</xref>; <xref ref-type="bibr" rid="R36">2021</xref>; <xref ref-type="bibr" rid="R57">Planton &amp; Dehaene, 2021</xref>; <xref ref-type="bibr" rid="R86">Xie et al., 2022</xref>; <xref ref-type="bibr" rid="R12">Desbordes et al., 2023</xref>; <xref ref-type="bibr" rid="R1">Roumi et al., 2023</xref>), it does not address scenarios with multiple, parallel information streams, in which several events may occur simultaneously and the number of streams may vary over time. This limitation is also present in existing models of perceptual predictive coding, which are mainly studied in speech or music perception (<xref ref-type="bibr" rid="R14">Donhauser &amp; Baillet, 2020</xref>; <xref ref-type="bibr" rid="R34">Koskinen et al., 2020</xref>; <xref ref-type="bibr" rid="R17">Gillis et al., 2021</xref>; <xref ref-type="bibr" rid="R68">Schmitt et al., 2021</xref>; <xref ref-type="bibr" rid="R28">Heilbron et al., 2022</xref>; <xref ref-type="bibr" rid="R6">Caucheteux et al, 2023</xref>; <xref ref-type="bibr" rid="R72">Temperley, 2008</xref>; <xref ref-type="bibr" rid="R53">Pearce, 2005</xref>; <xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Kern et al., 2022</xref>; <xref ref-type="bibr" rid="R64">Sankaran et al., 2024</xref>). These models are typically designed to process sequences of single events and predict the next item in a sequence, lacking the capacity to integrate multiple parallel streams of information.</p><p id="P6">To overcome this limitation, we propose designing models for multi-item prediction (also known as multi-label classification; <xref ref-type="bibr" rid="R74">Tsoumakas et al., 2008</xref>; <xref ref-type="bibr" rid="R61">Read et al., 2017</xref>; <xref ref-type="bibr" rid="R88">Zhang et al., 2020</xref>). This approach enables the prediction of multiple events at each step of a sequence without predefining a specific number of streams. Unlike previous methods, this involves estimating an occurrence probability for each possible event (rather than a probability distribution for a single upcoming event) and aligning sequence steps with an external time reference (e.g., a linear time axis).</p><p id="P7">We modeled predictions in polyphonic music (i.e., pieces in which multiple notes can occur simultaneously) as it is a typical case of multi-stream and hierarchical sequences. Nearly all popular forms of music feature multiple streams of notes, whether across (e.g., a rock band) or within instruments (e.g. chords in solo piano). Even in non-musicians, these streams are easily integrated to give rise to rhythmic or harmonic percepts, leading predictions of upcoming notes (<xref ref-type="bibr" rid="R51">Palmer &amp; Krumhansl, 1990</xref>; <xref ref-type="bibr" rid="R24">Hannon et al., 2004</xref>; <xref ref-type="bibr" rid="R80">Vuust &amp; Witek, 2014</xref>; <xref ref-type="bibr" rid="R4">Bharucha &amp; Stoeckig, 1986</xref>; <xref ref-type="bibr" rid="R32">Koelsch et al., 2000</xref>; <xref ref-type="bibr" rid="R39">Maess et al., 2001</xref>; Koelsch, 2009; <xref ref-type="bibr" rid="R33">2019</xref>).</p><p id="P8">We trained a recurrent neural network, PolyRNN, on a large dataset of real piano performances (see Method) to perform a multi-item prediction task with a time-resolved multidimensional representation of the score (“piano-roll”; <xref rid="F1" ref-type="fig">Figure 1A</xref>; see <xref ref-type="supplementary-material" rid="SD1">Video S1</xref>). The model outputs three key metrics for each timestep: (i) <italic>surprise</italic> (prediction error), reflecting the difference between predicted and actual note occurrences; (ii) <italic>uncertainty</italic> (entropy), reflecting the uncertainty among multiple possibilities; and (iii) <italic>predicted density</italic>, indicating the likelihood of note events occurring (<xref rid="F1" ref-type="fig">Figure 1B</xref>).</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P9">To validate PolyRNN, we first compared its performance with IDyOM (Information Dynamics of Music; <xref ref-type="bibr" rid="R53">Pearce 2005</xref>; <xref ref-type="bibr" rid="R54">2018</xref>), a well-established model based on variable-order Markov-chains and trained on a corpus of Western melodies. As IDyOM can only process single stream sequences, we tested both models using an existing open-source set of electroencephalography (EEG) data acquired during monophonic music listening (<xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>). We predicted the amplitude of the neural signal in response to individual notes (<xref rid="F1" ref-type="fig">Figure 1C-D</xref>) using basic acoustic features – temporal envelope and derivative – only (A), or using a combination of acoustic and IDyOM’s music features (AM). The difference between these models (ΔR<sup>2</sup> = AM - A) shows that the music features extracted from IDyOM predict a P2-like neural response over central electrodes around 200 ms (unilateral t-test vs. zero: t(19) = 4.4, p &lt; 0.001; <xref rid="F2" ref-type="fig">Figure 2A</xref>; see DiLiberto et al., 2020). PolyRNN better predicts this component of neural response to music notes (unilateral t-test vs. zero: t(19) = 6.3, p &lt; 0.001; PolyRNN vs. IDyOM bilateral paired t-test: t(19) = 5.2, p &lt; 0.001; <xref rid="F2" ref-type="fig">Figure 2B</xref>). To ensure that this effect is not driven by more complex features, such as distributions of note pitch and timing, we replicated these analyses with additional regressors (see Method). While IDyOM’s explained variance can be accounted for by these low-order contextual features (unilateral t-test vs. zero: t(19) = -4.5, p = 1), PolyRNN captures additional higher-order brain activity (unilateral t-test vs. zero: t(19) = 3.2, p = 0.002; PolyRNN vs. IDyOM bilateral paired t-test: t(19) = 4.52, p &lt; 0.001; see <xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref> for additional analyses).</p><p id="P10">Next, we investigated the encoding of polyphonic (multi-stream) expectations estimated by PolyRNN in the human brain. We recorded neural activity from 27 healthy participants in magnetoencephalography (MEG) and 10 patients in stereotactic EEG (sEEG) as they listened to recordings of ecological classical piano pieces. We predicted the amplitude of the neural signal in response to individual or simultaneous notes (<xref rid="F1" ref-type="fig">Figure 1C</xref>) with acoustic only (A; see <xref ref-type="supplementary-material" rid="SD1">Figure S2, S3</xref>) or acoustic and PolyRNN’s model features (AM; see Method).</p><p id="P11">In sEEG, PolyRNN predicts (ΔR<sup>2</sup> = AM - A) a P2-like neural response (61 significant temporal clusters encompassing 8/10 patients; cluster-based permutation test, ΔR<sup>2</sup>-threshold = 0.005, p &lt; 0.05; <xref rid="F3" ref-type="fig">Figure 3C</xref>), mostly in primary and associative auditory regions (<xref rid="F3" ref-type="fig">Figure 3A, 3C</xref>). In MEG, PolyRNN features are encoded in temporal sensors around 200 ms after note onset (12 significant spatiotemporal cluster; cluster-based permutation test, t-threshold = 2.63, p &lt; 0.05; <xref rid="F3" ref-type="fig">Figure 3B, 3F</xref>). No difference was observed between left and right hemisphere clusters (t-threshold = 2.06, p &gt; 0.9, two-tailed; <xref ref-type="supplementary-material" rid="SD1">Figure S4</xref>). To confirm these effects with an independent channel selection, we measured the improvement in predicted variance (ΔR<sup>2</sup> = AM - A) on the channels with the highest auditory response per participant (see Method). PolyRNN significantly improves the explained variance in those channels in both sEEG (in 10/10 patients; paired t-test, t(9) = 2.91, p = 0.017, two-tailed; <xref rid="F3" ref-type="fig">Figure 3D</xref>) and MEG (in 22/27 participants; paired t-test, t(26) = 3.44, p &lt; 0.002, two-tailed; <xref rid="F3" ref-type="fig">Figure 3G</xref>).</p><p id="P12">Finally, we used variance partitioning (<xref rid="F3" ref-type="fig">Figure 3E, 3H</xref>) to disentangle the contribution of the different PolyRNN music features, namely surprise, uncertainty and predicted density (see Method). In both MEG and sEEG, most of the effect is driven by the surprise of the current note. The encoding of predicted density arises earlier than surprise, while the encoding of uncertainty remains low overall. Lastly, in MEG, the surprise of the current note is associated with a P3-like neural response, which extends into the processing of the next note (n+1; &gt;350 ms; <xref rid="F3" ref-type="fig">Figure 3H</xref>).</p><p id="P13">With the advent of deep learning, many models have been created to generate music, including polyphonic music (<xref ref-type="bibr" rid="R9">Civit et al., 2022</xref>). These models can be derived to estimate the probability of upcoming notes, similarly to PolyRNN. However, while PolyRNN processes multiple notes in parallel, most current generative models deal with multi-stream sequences with serialization, handling each feature and note one at a time (i.e. simultaneous events are represented asynchronously; see Fradet et al., 2023). To compare these two approaches, we retrieved Performance RNN (PerfRNN), a generative music model from the Google Magenta Project (<xref ref-type="bibr" rid="R49">Oore et al., 2020</xref>; <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>).</p><p id="P14">In sEEG, PolyRNN outperforms PerfRNN in predicting the P2-like neural response, principally along the temporal lobe (<xref rid="F4" ref-type="fig">Figure 4A-B</xref>). On the channels with the highest auditory response (see Method), PolyRNN better predicts the brain’s response to music than PerfRNN (t(199) = 7.89, p &lt; 0.001; <xref rid="F4" ref-type="fig">Figure 4C</xref>). In MEG, we obtained similar results, with the highest difference between models around 200ms after note onset in temporal and frontal sensors (4 significant spatiotemporal clusters; cluster-based permutation test, t-threshold = 2.53, p &lt; 0.05, two-tailed; <xref rid="F4" ref-type="fig">Figure 4D-E</xref>).</p></sec><sec id="S3" sec-type="discussion"><title>Discussion</title><p id="P15">We have developed and tested a novel recurrent neural network, PolyRNN, to track multi-stream predictions during music listening. While previous work has highlighted the central role of predictive mechanisms in music perception, it has been limited to isolated predictions in artificial setups or monophonic music (<xref ref-type="bibr" rid="R32">Koelsch et al., 2000</xref>; <xref ref-type="bibr" rid="R54">Pearce, 2018</xref>; <xref ref-type="bibr" rid="R20">Gold et al., 2019</xref>; <xref ref-type="bibr" rid="R8">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="R47">Omigie et al., 2019</xref>; Quiroga-Martinez, 2020; <xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>, <xref ref-type="bibr" rid="R31">Kern et al., 2022</xref>, <xref ref-type="bibr" rid="R64">Sankaran et al., 2024</xref>). PolyRNN enables the use of real, polyphonic music, and brings the ecological validity that is crucial to study the perceptual and emotional mechanisms involved in real-life music listening (<xref ref-type="bibr" rid="R73">Tervaniemi, 2023</xref>).</p><p id="P16">PolyRNN successfully predicts neural activity in primary and associative auditory regions around 200 ms after note onset. This effect is primarily driven by the encoding of surprise (prediction error) and is reminiscent of the event-related components such as the mismatch negativity and early-right anterior negativity found in seminal studies on musical expectations (see Koelsch, 2009; for a review) and recent studies using monophonic stimuli (<xref ref-type="bibr" rid="R47">Omigie et al., 2019</xref>; <xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Kern et al., 2022</xref>; Shankaran et al., 2024). In addition, we show two other effects. First, surprise is also later encoded, peaking at 300 ms in sEEG and closer to 400 ms in MEG. These timings are highly overlapping with encoding of surprise for the next note in the piece, suggesting that neural activity related to processing of the next note is modified by the surprise context of the preceding note. Second, the predicted density is also encoded in the neural data, but at a much earlier time-point. This suggests that predictions initially concern the mere occurrence of an event, regardless of its content, with pitch-related predictions emerging at a later stage. The model does not predict beyond the next sample, so the notion of “temporal prediction” as studied in music or cognitive science is not directly applicable (<xref ref-type="bibr" rid="R63">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="R87">Zalta et al., 2024</xref>). However, predicted density likely represents the closest approximation in our model to assessing the likelihood of an event occurring at a given sample. Future work may focus on developing models capable of making predictions further into the future.</p><p id="P17">Our analysis shows that PolyRNN advances over the well-established IDyOM model in capturing neural responses, even for monophonic music (<xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>). While the encoding of IDyOM’s predictions could be explained by low-order contextual regressors, PolyRNN retained predictive value beyond this, suggesting that it captures higher-order relationships in music. This is achieved through an LSTM layer, without the need for musicological rules to build these predictions. It remains unclear however, what aspect of PolyRNN drives this neural prediction improvement, as both the training dataset (polyphonic music for PolyRNN, monophonic music for IDyOM), and the architecture (LSTM for PolyRNN, Markov-chains for IDyOM) have changed. From a training perspective, it could be that the listener’s expectations are fundamentally shaped by their experience with polyphonic music, and that the lack of polyphony in simple melodies generates a surprise on its own that is well captured by the model. Furthermore, the model’s ability to process polyphony allows for use of a training dataset that better reflects participant experience. On the other hand, it is possible that shifting to an LSTM RNN architecture enables better capture of higher-order relationships between notes that humans naturally perceive, even in single-stream melodies. This would particularly support the growing trend in cognitive neurosciences to adopt RNN and Transformer models, which offer a more accurate representation of the neural encoding of perceptual statistical properties (<xref ref-type="bibr" rid="R14">Donhauser &amp; Baillet, 2020</xref>; <xref ref-type="bibr" rid="R65">Saxe et al., 2021</xref>; <xref ref-type="bibr" rid="R5">Caucheteux &amp; King, 2022</xref>; <xref ref-type="bibr" rid="R30">Kanwisher et al., 2023</xref>; <xref ref-type="bibr" rid="R2">Arana et al., 2024</xref>).</p><p id="P18">Another critical feature of PolyRNN is that it processes multiple streams in parallel. Indeed, while the generative music model Performance RNN (PerfRNN) is built with the same LSTM architecture and trained on the same data, it performs poorly in predicting neural responses to multi-stream naturalistic music. The primary difference between these models is that PerfRNN processes simultaneous events in a serialized format, disrupting temporal organization of the stimulus. Our results show that the brain handles multiple streams of information in parallel rather than in sequence. This highlights the critical importance of temporal relationships between elements in neural processing. It also serves as an important reminder for the field that the structure of inputs and outputs to these models plays a crucial role when comparing them with human electrophysiology. Furthermore, it suggests a potential method for discriminating between competing cognitive theories for how information is represented, predicted, and/or integrated in natural complex stimuli. In addition to manipulating the task (objective function), input (training data), architecture or training algorithm (fitting procedure; <xref ref-type="bibr" rid="R30">Kanwisher et al., 2023</xref>), manipulating input and output formats can also help us understand why the brain works the way it does, and which design constraints may have shaped it.</p><p id="P19">While this research has focused on music as a case study, the processing of multiple objects and features in parallel is a key hallmark of brain activity in natural environments. We expect that the findings and model laid out here can apply well to these other scenarios. Future work can apply these findings to multiple features in speech perception, such as the effects of prosody and contextual lexical information on word comprehension (<xref ref-type="bibr" rid="R16">Frazier et al., 2006</xref>; <xref ref-type="bibr" rid="R23">Gwilliams et al., 2024</xref>); in vision, as in the simultaneous identification of objects in natural search (<xref ref-type="bibr" rid="R38">MacEvoy &amp; Epstein, 2009</xref>; <xref ref-type="bibr" rid="R85">Wolfe, 2020</xref>); or in cross modal integration of information (<xref ref-type="bibr" rid="R71">Talsma, 2015</xref>). While numerous studies have highlighted that sensory inputs in one modality can lead to predictions in another (<xref ref-type="bibr" rid="R76">Van Wassenhove et al., 2005</xref>; <xref ref-type="bibr" rid="R75">Van der Burg et al., 2008</xref>; <xref ref-type="bibr" rid="R3">Arnal et al., 2009</xref>; <xref ref-type="bibr" rid="R79">Vroomen &amp; Stekelenburg, 2010</xref>; <xref ref-type="bibr" rid="R71">Talsma, 2015</xref>), there is, to our knowledge, no model that accounts for their integration in continuous, ecological settings, where inputs from each modality unfold in parallel and with their own temporal dynamics. An interesting direction for future research would be to isolate the information emerging from multi-stream integration (beyond the sum of the parts) by processing each stream either separately or together through one model. Such a procedure could reveal cross-modal representations in a data-driven manner, providing a new tool for studying their implementation in the brain.</p></sec><sec id="S4" sec-type="materials | methods"><title>Materials and Method</title><sec id="S5" sec-type="subjects"><label>1</label><title>Participants</title><sec id="S6"><label>1.1</label><title>EEG</title><p id="P20">The open source EEG dataset from <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref> was acquired on 20 healthy adults (10 females, mean age = 29 years). Half of them were musically trained, and the others had no musical background. They had no history of hearing impairment or neurological disorder.</p></sec><sec id="S7"><label>1.2</label><title>MEG</title><p id="P21">MEG signals were acquired on 27 normal-hearing participants (19 females, age = 28.3 ± 7.4 years) at the Institut du Cerveau, Paris, France. Musical experience was self-reported via the Goldsmiths Musical Sophistication Index (<xref ref-type="bibr" rid="R44">Müllensiefen et al., 2014</xref>). The General Sophistication value spanned from 26 to 102 (mean = 70, std = 18), showing a high diversity in musical experience across participants. This study was approved by the Comité de Protection des Personnes Tours OUEST 1 on 10/09/2020 (project identification number 2020T317 RIPH3 HPS).</p></sec><sec id="S8"><label>1.3</label><title>sEEG</title><p id="P22">10 patients (3 females, age = 28.1 ± 5.4 years) with pharmacoresistant epilepsy took part in the study. They were implanted with depth electrodes for clinical purposes at the Hôpital de la Timone (Marseille). Neural recordings were performed between 3 to 10 days after the implantation procedure. No sedation or analgesics drugs were used, and patients received their usual antiepileptic treatment. Recordings were always acquired more than 4 hours after the last seizure. Patients were included in the study if their implantation map covered at least partially the Heschl’s gyrus (left or right). Neuropsychological assessments carried out before SEEG recordings indicated that all patients met the criteria for normal hearing. None of them had their epileptogenic zone including the auditory areas as identified by experienced epileptologists. 3 patients have had musical training (&gt; 5 years of instrumental practice), one being a professional musician. Informed consent was obtained from all patients. The study was approved by the Assistance Publique – Hôpitaux de Marseille (health data access portal registration number PADS E2YSEB). Recordings, interpretation and analysis of SEEG were performed following the French guidelines on stereoelectroencephalography (<xref ref-type="bibr" rid="R29">Isnard et al., 2018</xref>) and recommendations on SEEG analysis (<xref ref-type="bibr" rid="R42">Mercier et al., 2022</xref>).</p></sec></sec><sec id="S9"><label>2</label><title>Stimuli and procedure</title><sec id="S10"><label>2.1</label><title>EEG</title><p id="P23">EEG data from <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref> were acquired with ten excerpts of Bach pieces for solo instrument, with a duration of approximately 150s each. The audio stimuli were synthesized with piano sounds from the digital score. All stimuli were played 3 times throughout the experiment, for a total of 30 trials. Participants were asked to rate their familiarity to the piece at the end of each trial on a Likert scale. Behavioral data are not used here.</p></sec><sec id="S11"><label>2.2</label><title>MEG and sEEG</title><p id="P24">Stimuli for the two experiments were chosen from the MAESTRO database (Hawthorne et al, 2019), containing midi recordings of classical piano pieces (from baroque to modern eras) performed by skilled pianists. The midi files of the entire database were analyzed in 8 second clips with step size of 1 second. Within each 8-second window, the instantaneous note rate was assessed by identifying the timing of each note onset and taking the inverse of the difference between neighboring notes, discarding intervals less than 70 ms as part of chords.</p><p id="P25">After this, several features were recorded for each clip: Stability (inverse of the standard deviation of temporal intervals), Peak Frequency (most common note rate within bins of .5 Hz), Mean Frequency (mean note rate across frequencies), Onset Start (the time of the first note within the 8 second window) and Note Density (average number of concurrent notes played at each given point in time). From this information we selected the “best” clips as those with the highest Stability measurement within each Peak Frequency bin of 2.5 - 7.5 in 1 Hz steps. The 7.5 Hz condition was used only in the sEEG version of the experiment. Only clips with an average note density between 2 and 2.5 were considered. We avoided picking clips from the same songs by ensuring the levenshtein ratio between the titles and composers of each entry were not greater than .9. Selected clips were manually inspected to remove duplicates across multiple performances which could yield the same piece due to different naming practices. The 18 most stable clips were chosen to be included in the experiment.</p><p id="P26">Remaining clips were saved as individual midi files. They were then converted to sound files using the python package Pretty Midi (<xref ref-type="bibr" rid="R60">Raffel and Ellis, 2014</xref>) fluidsynth function. Midi files were snapped to note onset start to ensure no extra silence at the beginning of sound files). The soundfont used to generate the audio was a recording of a Mason Hamlin piano (MasonHamlin-A-v5.2.sf2) recorded and published online for free use at Soundfonts4u (<ext-link ext-link-type="uri" xlink:href="https://sites.google.com/site/soundfonts4u">https://sites.google.com/site/soundfonts4u</ext-link>). For a separate planned analysis, we were interested in altering the onset shapes of the notes and therefore used the original and several altered versions of the soundfont to smooth the piano’s acoustic edge. To do so, we used the polyphone soundfont editor to manipulate the attack in 3 settings: 0.001 seconds (original setting), 0.100 seconds (smooth setting), 0.250 seconds (smoothest setting), smoothing the onset shape of the piano sound. The onset manipulation was applied to specific clips separately so that participants would not hear the same clip with different onset shapes. The application was done in a balanced way across clips so that the mean stability rank for each onset shape was controlled. The sEEG data used all three onset shapes, while the MEG only used two (original and smoothest settings). The effect of onset shapes was not analyzed in the present study. Trial order was set and frozen across all participants. In the sEEG experiments, each clip was repeated twice while in the MEG version, clips were repeated 3 times. Trial order was balanced so that repetition numbers were as balanced as possible in terms of position within the experiment. Overall, the MEG and sEEG experiments contained 240 and 180 trials respectively.</p><p id="P27">Participants listened to 2 blocks of 90 stimuli in sEEG or 8 blocks of 30 stimuli in MEG. They completed an old/new task, where they had to indicate whether they already heard the same excerpt during the experiment. Their response was given with a button press at the end of each trial. Behavioral responses were not analyzed in the present study.</p></sec></sec><sec id="S12"><label>3</label><title>Acquisition and preprocessing</title><sec id="S13"><label>3.1</label><title>EEG</title><p id="P28">EEG data from <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref> were acquired with a 64-channels BioSemi Active Two System. To reproduce the preprocessing of the original paper, the signal was bandpass filtered (1 - 8 Hz) and downsampled (64 Hz) with MNE (v1.6) in a python environment. We re-epoched the data at the note level, keeping signals from -1 to 1 seconds relative to each note onset. Epochs based on notes occurring in the first 5 seconds of a trial were discarded to remove the initial auditory burst. Similarly, epochs centered on notes occurring in the last second of a trial were removed to avoid post-stimulus activity rebound. Overall, the data kept for analysis contained 18555 epochs per participant.</p></sec><sec id="S14"><label>3.2</label><title>MEG</title><p id="P29">MEG data was recorded with an Elekta Neuromag TRIUX system with a sampling frequency of 1000 Hz and a low-pass filter at 330 Hz. All the following preprocessing steps were performed in MNE-Python (<xref ref-type="bibr" rid="R21">Gramfort et al., 2013</xref>). Bad channels and signal portions were removed through automatic detection as well as visual inspection. We then used signal-space separation and Maxwell filtering to reduce external artifacts, compensate for head movement, and to reconstruct bad channels. A Notch filter at 50 Hz was applied to reduce electrical noise. Eye movements and cardiac electrical activity were filtered through Independent Component Analysis (ICA), using electrooculogram and electrocardiogram electrodes to automatically detect the corresponding components to discard. The 102 magnetometer channels were then selected, downsampled at 100 Hz and filtered between 0.5 and 12 Hz (zero-phase finite impulse response filter).</p><p id="P30">We re-epoched the data at the note level, keeping signals from -1 to 1 seconds relative to each note onset. Epochs based on notes occurring in the first 2 seconds of a trial were discarded to remove the initial auditory burst. Similarly, epochs centered on notes occurring in the last second of a trial were removed to avoid post-stimulus activity rebound. Finally, epochs based on notes played almost simultaneously (time interval &lt; 50 ms) were removed, with only the first epoch of such a group being kept. Overall, the data kept for analysis contained a maximum of 12766 epochs per participant when no trials were removed during artifact rejection.</p></sec><sec id="S15"><label>3.3</label><title>sEEG</title><p id="P31">The sEEG signal was recorded using depth electrodes shafts of 0.8 mm diameter containing 10 to 15 electrode contacts (Dixi Medical or Alcis, Besançon, France). The contacts were 2 mm long and were spaced from each other by 1.5 mm. The locations of the electrode implantations were determined solely on clinical grounds. Patients were included in the study if their implantation map covered at least partially the Heschl’s gyrus (left or right). The cohort consists of 9 bilateral implantations and 1 unilateral implantation (right), yielding a total of 150 electrodes and 1690 contacts. Data were recorded using a 256-channels Natus amplifier (Deltamed system), sampled at 512 Hz and high-pass filtered at 0.16 Hz. A monopolar reference montage setup was used for the recording. The recording reference and ground were chosen by the clinical staff as two consecutive sEEG contacts on the same shaft both located in the white matter and/or at distance from any epileptic activity.</p><p id="P32">The precise localization of the channels was retrieved with a procedure similar to the one used in the iELVis toolbox (<xref ref-type="bibr" rid="R22">Groppe et al., 2017</xref>). First, we manually identified the location of each channel centroid on the post-implant CT scan using the Gardel software (Medina <xref ref-type="bibr" rid="R78">Villalon et al., 2018</xref>). Second, we performed volumetric segmentation and cortical reconstruction on the pre-implant MRI with the Freesurfer image analysis suite (documented and freely available for download online <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>). Third, the post-implant CT scan was coregistered to the pre-implant MRI via a rigid affine transformation and the pre-implant MRI was registered to the MNI template (MNI 152 Linear), via a linear and a non linear transformation from SPM12 methods (<xref ref-type="bibr" rid="R56">Penny et al., 2011</xref>), through the FieldTrip toolbox (<xref ref-type="bibr" rid="R50">Oostenveld et al., 2011</xref>). Based on the brain segmentation performed using SPM12 methods through the Fieldtrip toolbox, channels located outside of the brain were removed from the data (2.25 %).</p><p id="P33">The processing of continuous sEEG data was performed with MNE (v1.6) in a python environment. The signal was notch filtered at 50Hz and harmonics up to 250Hz to remove power line artifacts, bandpass filtered between 0.5 and 12 Hz and downsampled to 100Hz.</p><p id="P34">Channels and epochs with artifacts and epileptic spikes were discarded with a semi-automatic procedure (<xref ref-type="bibr" rid="R42">Mercier et al., 2022</xref>). sEEG data were epoched between -0.5 to 8.5 seconds relative to stimulus onset, and the peak of the absolute value of the signal was taken for each epoch of each channel. For channel rejection, peak values were first averaged across epochs. Then, the mean and the standard deviations of peak values across channels were computed, and a threshold was defined as N standard deviations above the mean. The factor N was set based on visual inspection of the data (in a range from 4 to 10), and all channels with a peak value exceeding the threshold were removed from subsequent analyses. For epoch rejection, the same procedure was applied with peak values averaged across channels. This whole procedure was conducted for each patient independently, removing a total of 3 channels and 1 epochs. Additionally, a technical issue made the triggers unusable for half of the data of 1 patient. These data have been removed from all analyses.</p><p id="P35">The sEEG data was then re-epoched at the note level, following the procedure used in MEG, yielding 10795 events when no trials were rejected.</p></sec></sec><sec id="S16"><label>4</label><title>Models of musical expectancies</title><sec id="S17"><label>4.1</label><title>IDyOM</title><p id="P36">The Information Dynamics Of Music (IDyOM; <xref ref-type="bibr" rid="R53">Pearce, 2005</xref>) model was designed to estimate the conditional probability of the pitch and onset time of upcoming notes in a musical sequence. IDyOM features a long-term memory (trained on a corpus) and a short-term memory (capturing the regularities in the current piece). IDyOM is limited to the processing of sequences of single events, and has mainly been used with monophonic music (<xref ref-type="bibr" rid="R48">Omigie et al., 2012</xref>; <xref ref-type="bibr" rid="R47">2019</xref>; <xref ref-type="bibr" rid="R20">Gold et al., 2019</xref>; <xref ref-type="bibr" rid="R59">Quiroga-Martinez et al., 2019</xref>; <xref ref-type="bibr" rid="R58">2020</xref>; <xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="R31">Kern et al., 2022</xref>) or chord sequences (<xref ref-type="bibr" rid="R8">Cheung et al., 2019</xref>; <xref ref-type="bibr" rid="R7">2024</xref>), and cannot process polyphonic pieces. Similarly to <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref>, we used a version of IDyOM trained on a large corpus of Western tonal music, including Canadian folk songs (<xref ref-type="bibr" rid="R10">Creighton, 1966</xref>), German folk songs from the Essen folk song collection (<xref ref-type="bibr" rid="R66">Schaffrath, 1992</xref>), and chorale melodies harmonized by Bach (<xref ref-type="bibr" rid="R62">Riemenschneider, 1941</xref>) as in other applications of IDyOM (e.g., <xref ref-type="bibr" rid="R53">Pearce, 2005</xref>; <xref ref-type="bibr" rid="R55">Pearce and Wiggins, 2006</xref>; <xref ref-type="bibr" rid="R15">Egermann et al., 2013</xref>; <xref ref-type="bibr" rid="R25">Hansen and Pearce, 2014</xref>; <xref ref-type="bibr" rid="R20">Gold et al, 2019</xref>). This corpus did not contain the stimuli used by Di Liberto and colleagues. For each note of each stimulus, we retrieved probability distributions of pitch and timing separately, and computed the <italic>surprise</italic> (<xref rid="FD1" ref-type="disp-formula">Equation 1</xref>) and <italic>uncertainty</italic> (<xref rid="FD2" ref-type="disp-formula">Equation 2</xref>).</p><disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><p id="P37">where <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is the conditional probability of the pitch or timing of the <italic>n<sup>th</sup></italic> event of the sequence.</p><disp-formula id="FD2"><label>(2)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>∣</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>*</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>∣</mml:mo><mml:msubsup><mml:mi>e</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>…</mml:mo><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></sec><sec id="S18"><label>4.2</label><title>Performance RNN</title><p id="P38">Performance RNN (PerfRNN) is a recurrent neural network model designed to generate piano music. It is based on a musical representation analogous to digital scores (MIDI format), with discrete events of different types: note onsets (pitch), note offsets (pitch), and time intervals between events. In every processing step, PerfRNN generates a probability distribution over all possible event types and values and pulls one specific event out before starting the process again. Polyphony is thus handled as a serialized process, with simultaneous notes generated one after the other with no time interval in between. The model can also be applied to existing music, generating probability distributions for every event of our stimuli. We used an available version of PerfRNN (<ext-link ext-link-type="uri" xlink:href="http://download.magenta.tensorflow.org/models/performance.mag">http://download.magenta.tensorflow.org/models/performance.mag</ext-link>) that was pre-trained on real performances from the International Piano-e-Competition dataset (renamed MAESTRO at time of publication; <ext-link ext-link-type="uri" xlink:href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</ext-link>). It is a recurrent neural network with 3 long short-term memory (LSTM) layers (see <xref ref-type="bibr" rid="R49">Oore et al., 2020</xref> for details). For each note of each stimulus, we retrieved probability distributions of note onset and time interval events separately, and computed the <italic>surprise</italic> (<xref rid="FD1" ref-type="disp-formula">Equation 1</xref>) and <italic>uncertainty</italic> (<xref rid="FD2" ref-type="disp-formula">Equation 2</xref>). When multiple notes start at the same timing, they were all assigned the mean surprise and uncertainty value across them, and duplicate events were removed later in the analysis pipeline (see Section <xref ref-type="sec" rid="S12">Acquisition and preprocessing</xref>).</p></sec><sec id="S19"><label>4.3</label><title>PolyRNN</title><p id="P39">We designed a recurrent neural network, PolyRNN, to process time-resolved, polyphonic musical sequences (<ext-link ext-link-type="uri" xlink:href="https://github.com/pl-robert/musecog">https://github.com/pl-robert/musecog</ext-link>). The model takes as input a sequence of timesteps, with a vector representing the onset, sustain or absence of each possible note at each timestep (a “piano-roll”). PolyRNN is trained to perform a multi-item prediction task, where a probability of occurrence is estimated for each note of the next timestep (e.g. the model could output a probability of 1 for several notes, or 0 for all notes). This choice of representation implies that the model doesn’t make any prediction about the velocity and the duration of upcoming notes, making it unable to generate music. Furthermore, this representation does not feature an explicit stream segregation. Even if the model learns to separate different streams (e.g. a melody line and a bass line), it will not appear explicitly in its output.</p><p id="P40">PolyRNN was trained on the International Piano-e-Competition dataset (renamed MAESTRO at time of publication; <ext-link ext-link-type="uri" xlink:href="https://magenta.tensorflow.org/datasets/maestro">https://magenta.tensorflow.org/datasets/maestro</ext-link>), containing approximately 150 hours of classical piano music (from XVII<sup>th</sup> to XX<sup>th</sup> centuries) performed by professional pianists and recorded in the MIDI format. MIDI files were converted to piano rolls (2D arrays with dimensions n_pitch and n_timesteps) with a frequency sampling of 20 Hz. All velocity information was removed, the onset of a note was coded as a 2, a sustained note as a 1, and an absence of note as a 0. To avoid any bias toward the most common scales in classical music, the pitch of all notes of a given piece were randomly raised or lowered by +/- 6 semitones during training. The dataset was randomly divided in training (80%) and validation (20%) sets.</p><p id="P41">PolyRNN was built with the pytorch python package (<xref ref-type="bibr" rid="R52">Paszke et al., 2019</xref>). The model contains a single Long-Short Term Memory (LSTM) layer (input size = 88, hidden size = 88, dropout = 0.05) connected to a linear layer (output size = 88) with a sigmoid activation function. PolyRNN was trained with batch learning (batch size = 15, batch length = 60 sec), truncated back-propagation through time (TBPTT, window = 5 sec) and an Adam optimizer (learning rate = 10<sup>-5</sup>). The model’s performance was measured with binary cross entropy (log base 2). The training was manually stopped at batch number 100K, after training and validation losses had stagnated for the last 20K batches (<xref ref-type="supplementary-material" rid="SD1">Fig. S6</xref>). The final training loss and validation loss averaged across the last 1K batches were 3.41 and 3.49 bits/timestep respectively. Other versions of the model have been trained with LSTM layers ranging from 1 to 5, LSTM hidden size from 88 to 512, batch size from 4 to 32, TBPTT from 2 to 10 seconds and learning rates from 10<sup>-5</sup> to 10<sup>-2</sup>, but didn’t reach a lower validation loss.</p><p id="P42">Qualitatively, visual inspection of the output of PolyRNN suggests that its prediction dynamically adapts to the note rate, the pitch range and the tonality of the preceding musical context (<xref ref-type="supplementary-material" rid="SD1">Video S1</xref>). It further makes timing predictions when the input keeps a steady rhythm, and predicts simple pitch trajectories like ascending or descending scales, arpeggios, and trills. Consistent with the known limitations of recurrent neural networks (<xref ref-type="bibr" rid="R84">Williams &amp; Peng, 1990</xref>), the model seems to forget the events that happened past the TBPTT window, and is unable to catch precise patterns (specific themes or broken chords) or long-term structure (e.g., second presentation of a theme). Overall, PolyRNN seems to perform the non-trivial task of dynamically retrieving multiple relevant musical features (note rate, pitch range, tonality, musical context, pitch trajectories…) and represents a plausible implementation of predictive processing derived from statistical learning.</p><p id="P43">The stimuli (see Section Stimuli) were then processed with PolyRNN, and three features of interest were derived from its predictions. First, the <italic>surprise</italic> value S<sub>t</sub> of a note was computed as the binary cross entropy (same as the loss function used during training) of the timestep where the note appears (<xref rid="FD3" ref-type="disp-formula">Equation 3</xref>). When multiple notes appear at the same timestep, they were all given the same surprise value, and duplicate events were removed later in the analysis pipeline (see Section <xref ref-type="sec" rid="S12">Acquisition and preprocessing</xref>). Second, the <italic>uncertainty</italic> value of a note H<sub>t</sub> was obtained by normalizing the model’s output of the timestep where the note appears and computing its entropy (<xref rid="FD4" ref-type="disp-formula">Equation 4</xref>). This feature was chosen to allow for a direct comparison with the uncertainty derived from IDyOM in previous studies (<xref ref-type="bibr" rid="R8">Cheung et al., 2019</xref>; Gold et al., 2020; <xref ref-type="bibr" rid="R13">Di Liberto et al., 2020</xref>). However, its application in this modeling approach implies that it will be affected by the number of simultaneous notes happening in the current context (i.e. predicting 10 different notes will lead to a higher uncertainty than predicting only 1 note, even if it is a correct prediction). Finally, the <italic>predicted density</italic> D<sub>t</sub> value of a note was computed as the sum of all the probabilities at the timestep where the note appears (<xref rid="FD5" ref-type="disp-formula">Equation 5</xref>). In contrast with other models, PolyRNN is predicting the occurrence of each note (with their own probability between 0 and 1) at each timestep. Thus, the predicted density is expected to vary depending on the beat (i.e. high on the beats, low between the beats) and on the number of predicted notes.</p><disp-formula id="FD3"><label>(3)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><p id="P44">where <inline-formula><mml:math id="M5"><mml:msup><mml:mover><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup></mml:math></inline-formula> is the vector of occurrence probability estimated by PolyRNN at a time step <italic>t</italic>, and <italic>Y<sup>t</sup></italic> is the groundtruth with a note onset coded as 1 and 0 otherwise.</p><disp-formula id="FD4"><label>(4)</label><mml:math id="M6"><mml:msub><mml:mi>H</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mtext>Y</mml:mtext><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo>*</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="italic">log</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mtext>Y</mml:mtext><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mtext> where </mml:mtext><mml:msup><mml:mover><mml:mtext>Y</mml:mtext><mml:mo>−</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mover><mml:mtext>Y</mml:mtext><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mi>t</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mrow><mml:mover><mml:mtext>Y</mml:mtext><mml:mo>−</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:math></disp-formula><disp-formula id="FD5"><label>(5)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mi>∑</mml:mi><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mover accent="true"><mml:mtext>Y</mml:mtext><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></sec></sec><sec id="S20"><label>5</label><title>Control features</title><p id="P45">A set of control features was selected based on note properties (from original MIDI files) and acoustic features (derived from audio stimuli). Note properties contained the absolute time (since trial start), time interval (since last note), note rate (constant within a trial), absolute pitch (in log scale), and pitch interval (since last note). When multiple notes were played simultaneously, only the pitch of the highest note was kept.</p><p id="P46">In EEG analyses, acoustic features only contained the broadband amplitude envelope (Hilbert transform) and its half-way rectified derivative. These features were included in the available dataset but not the audio files of the stimuli, preventing us from computing other acoustic descriptors.</p><p id="P47">In MEG and sEEG analyses, acoustic features contained several time-varying descriptors of auditory sensations (<xref ref-type="bibr" rid="R18">Giordano et al., 2023</xref>), such as loudness, periodicity, timbre brightness and roughness (temporal resolution = 1 ms). Time-varying loudness and brightness were derived from the instantaneous specific loudness of the input signal (<xref ref-type="bibr" rid="R19">Glasberg &amp; Moore, 2002</xref>), as estimated in the Genesis Loudness Toolbox in MATLAB. Instantaneous specific loudness measures the time-varying contribution to the overall instantaneous loudness (temporal resolution = 1 ms) in separate frequency bands (N frequency bands = 153, equally spaced on an ERB-rate scale between 20 and 15,000 Hz). For each temporal frame, loudness (measured on a sone scale) was then defined as the sum of the instantaneous specific loudness across frequency bands, whereas timbre brightness was defined as the spectral centroid, that is, as the specific loudness weighted average ERB-rate frequency (<xref ref-type="bibr" rid="R41">McAdams et al., 1995</xref>). Time-varying periodicity (ratio of periodic to aperiodic power, in dB) were estimated using the Yin model (Cheveigné &amp; Kawahara, 2002). Time-varying roughness (<xref ref-type="bibr" rid="R77">Vencovský, 2016</xref>) was estimated using the model implemented in the MIRtoolbox v.1.7.2 in MATLAB. Spectral flux (rate of timbral change) was added as a fifth acoustic feature, as it has been shown to be strongly encoded in the brain’s response to music (<xref ref-type="bibr" rid="R82">Weineck et al., 2022</xref>). Spectral flux was estimated with the Surfboard python package v.0.2.0 (window size = 0.04 s, hop size = 0.01 s). The derivatives of each acoustic feature were also added to the set of control features, except in the case of loudness, where the half-way rectified derivative was used. Finally, all acoustic features were averaged in a 150 ms time-window starting from note onsets, to obtain a single value per epoch.</p></sec><sec id="S21"><label>6</label><title>Encoding analysis of PolyRNN music features</title><p id="P48">In all datasets, a multiple linear ridge regression was used to model the amplitude of the brain response to individual notes (or groups of simultaneous notes). For each channel and each time-point (from -1 to 1 seconds; see Section <xref ref-type="sec" rid="S12">Acquisition and preprocessing</xref>) of the response separately, the linear model was fitted and tested following a 5-fold cross-validation scheme, yielding a measure of predicted variance (cv-R<sup>2</sup>). This was repeated with several ridge parameters (alphas ranging from 10<sup>2</sup> to 10<sup>7</sup>), and the best cv-R<sup>2</sup> were kept. This flat cross-validation setup was preferred over a nested cross-validation scheme, to reduce the computational cost (see below, permutation tests) and because of similar overall quality between both procedures when only a few hyperparameters (here n=1) have to be optimized (<xref ref-type="bibr" rid="R81">Wainer &amp; Cawley, 2021</xref>). Importantly, the same procedure was applied for the different linear models of interest and the permuted analyses (see below).</p><p id="P49">To assess the specific encoding of musical expectancies, we focused on the variance uniquely explained by music models features. It was retrieved by computing the cv-R<sup>2</sup> from all acoustic control features (model A), or from the full set of control and music model features together (model AM), and taking the difference (ΔR<sup>2</sup> = cv-R<sup>2</sup><sub>AM</sub> - cv-R<sup>2</sup><sub>A</sub>).</p><p id="P50">In our ecological stimuli, the time interval between consecutive notes is short (mean time interval = 197 ± 77ms) and is likely to induce overlapping neural responses. Since the music models features are not independent across consecutive events (positive autocorrelation; <xref ref-type="supplementary-material" rid="SD1">Fig. S7</xref>), the values associated with the current note <italic>n</italic> may partly predict the neural response to the following note <italic>n + 1</italic>. To allow the current analysis to disentangle late components in response to <italic>n</italic> from early components to <italic>n + 1</italic>, we computed the regression with the features associated with both notes (<italic>n</italic> and <italic>n + 1</italic>), and we separated the contribution of each with variance partitioning (later in the analysis pipeline; see below). In all datasets, the final linear model A thus contains all acoustic control features of notes n and <italic>n + 1</italic>. The final AM<sub>PolyRNN</sub> model contains the same features as A, with the addition of <italic>Surprise</italic>, <italic>Uncertainty</italic>, <italic>Predicted density</italic> of notes <italic>n and n + 1</italic>.</p><p id="P51">In MEG and sEEG, statistical significance was assessed with cluster level permutation tests. The regression model AM<sub>PolyRNN</sub> was estimated 1000 times with random permutations of the music model features’ values across epochs. Chance-level ΔR<sup>2</sup> values were computed by taking the difference in cv-R<sup>2</sup> between model A and the permuted versions of AM<sub>PolyRNN</sub>.</p><p id="P52">In MEG, group-level statistics were obtained with a one-tailed t-test (ΔR<sup>2</sup> &gt; 0) on both original and permuted results. Spatio-temporal clusters were defined with MNE’s _find_clusters function, in which channels and time points with a t-statistic that exceeds an arbitrary threshold (t-threshold = 2.63) are clustered based on adjacency in time (consecutive time points) and space (adjacent channels on the MEG topography using MNE’s find_ch_adjacency function). The null-distribution was obtained by taking the cluster of highest magnitude (mean t-statistic) in each permutation. In the original data, a cluster is significant when its magnitude exceeds at least 95% of the cluster magnitudes of the null distribution.</p><p id="P53">In MEG, all the channels belonging to at least one significant spatio-temporal cluster were separated into two spatial groups (left and right). A sign-based permutation temporal cluster test (permutation_cluster_1samp_test in MNE) on the spatial averages was used to test for lateralization effects (10000 permutations, t-threshold = 2.06, two-tailed).</p><p id="P54">In sEEG, the heterogeneity of channel locations prevents us from comparing channels across participants and computing group-level statistics. Hence, clusters were based on ΔR<sup>2</sup> values (ΔR<sup>2</sup> threshold = 0.005) and temporal adjacency between data points. The null-distribution and statistical threshold were obtained in a similar way as with MEG data.</p><p id="P55">To complement these analyses with an independent channel selection, we define the auditory channels as the 20 channels with the highest M100 amplitude for each participant in MEG, and the 20 channels with highest magnitude (absolute value of amplitude) at 100ms after note onset per participant in sEEG. We computed the average of peak R<sup>2</sup> values (within [-0.5, 1] s) across these channels, and compared the linear models A and AM<sub>PolyRNN</sub> with two-tailed paired t-tests.</p><p id="P56">Finally, we performed a variance partitioning analysis to evaluate the contribution of each music model feature in significant clusters. In AM<sub>PolyRNN</sub>, the unique variance of a given feature was obtained by taking the difference in cv-R<sup>2</sup> values between full AM<sub>PolyRNN</sub> and AM<sub>PolyRNN</sub> without that feature.The shared variance (between at least 2 variables) was defined as the cv-R<sup>2</sup> of the full AM<sub>PolyRNN</sub> minus the sum of all unique variances.</p></sec><sec id="S22"><label>7</label><title>Model comparison</title><sec id="S23"><label>7.1</label><title>PolyRNN vs IDyOM</title><p id="P57">In EEG, the encoding of IDyOM’s expectations were estimated with a linear model AM<sub>Idyom</sub> (five-fold cross-validated ridge regression) containing IDyOM’s <italic>Pitch-Surprise</italic>, <italic>Pitch-Uncertainty</italic>, <italic>Timing-Surprise</italic>, <italic>Timing-Uncertainty</italic> of notes n and n + 1. The acoustic control model A contained the sound envelope, its derivative, and all note properties (note rate, absolute time, time interval, absolute pitch, pitch interval) as regressors. To allow for a direct comparison with the results of <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref>, we also estimated a model A<sub>env</sub> with the envelope and its derivative only, as well as full models A<sub>env</sub>M<sub>PolyRNN</sub> and A<sub>env</sub>M<sub>Idyom</sub> without any shifted feature (i.e., of note n + 1).</p><p id="P58">We first retrieved the spatio-temporal extent of the main effect obtained with A<sub>env</sub>M<sub>Idyom</sub> - A<sub>env</sub>. Using an arbitrary threshold (ΔR<sup>2</sup> &gt; 0.00035), we obtained a mask covering central electrodes and timelags ranging from 156 to 220 ms. For each linear model applied on EEG data, we took the average of ΔR<sup>2</sup> values within this mask, and compared them with 0 with one-tailed t-tests and Bonferroni correction. To compare the music models, we contrasted PolyRNN and IDyOM with either the envelope-only control (A<sub>env</sub>M<sub>PolyRNN</sub> - A<sub>env</sub> versus A<sub>env</sub>M<sub>Idyom</sub> - A<sub>env</sub>) or the full acoustic control (AM<sub>PolyRNN</sub> - A versus AM<sub>Idyom</sub> - A) with two-tailed paired t-tests and Bonferroni correction.</p></sec><sec id="S24"><label>7.2</label><title>PolyRNN vs PerfRNN</title><p id="P59">As with PolyRNN, a multiple linear ridge regression was used to model the amplitude of the brain response to individual notes in both sEEG and MEG. The AM<sub>PerfRNN</sub> model contains the same features as A (see Section <xref ref-type="sec" rid="S20">Control features</xref>), plus <italic>Pitch-Surprise</italic>, <italic>Pitch-Uncertainty</italic>, <italic>Timing-Surprise</italic>, <italic>Timing-Uncertainty</italic> of notes n and n + 1. PolyRNN and PerfRNN were explicitly compared by taking the difference between cv-R<sup>2</sup><sub>PolyRNN</sub> and cv-R<sup>2</sup><sub>PerfRNN</sub> for each timepoint of each channel.</p><p id="P60">In MEG, statistical significance was assessed with sign-based permutation tests at the spatio-temporal cluster level as implemented in MNE’s spatio_temporal_cluster_1samp_test function (10000 permutations, T threshold = 2.53, two-tailed).</p><p id="P61">In sEEG, the difference between the peak-R<sup>2</sup><sub>PolyRNN</sub> and the peak-R<sup>2</sup><sub>PerfRNN</sub> was computed for each channel. Statistical significance was assessed with a two-tailed t-test across the auditory channels, defined as the 20 channels with highest magnitude (absolute value of amplitude) at 100ms after note onset per participant in sEEG (i.e. 200 channels).</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary materials</label><media xlink:href="EMS201584-supplement-Supplementary_materials.pdf" mimetype="application" mime-subtype="pdf" id="d113aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S25"><title>Acknowledgments</title><p>We thank Bruno Giordano for his support with data analysis. This research is co-funded by the European Union (ERC, SPEEDY, ERC-CoG-101043344), supported by Fondation Pour l’Audition (FPA RD-2022-09; FPA RD-2020-10), the Fondation Fyssen, grants from France 2030 (ANR-16-CONV-0002) and the Excellence Initiative of Aix-Marseille University (A*MIDEX).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Brain-imaging evidence for compression of binary sound sequences in human memory</article-title><source>Elife</source><year>2023</year><volume>12</volume><elocation-id>e84376</elocation-id><pub-id pub-id-type="pmcid">PMC10619979</pub-id><pub-id pub-id-type="pmid">37910588</pub-id><pub-id pub-id-type="doi">10.7554/eLife.84376</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arana</surname><given-names>S</given-names></name><name><surname>Pesnot Lerousseau</surname><given-names>J</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><article-title>Deep learning models to study sentence comprehension in the human brain</article-title><source>Language, Cognition and Neuroscience</source><year>2024</year><volume>39</volume><issue>8</issue><fpage>972</fpage><lpage>990</lpage></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Kell</surname><given-names>CA</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Dual neural routing of visual facilitation in speech processing</article-title><source>Journal of Neuroscience</source><year>2009</year><volume>29</volume><issue>43</issue><fpage>13445</fpage><lpage>13453</lpage><pub-id pub-id-type="pmcid">PMC6665008</pub-id><pub-id pub-id-type="pmid">19864557</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3194-09.2009</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharucha</surname><given-names>JJ</given-names></name><name><surname>Stoeckig</surname><given-names>K</given-names></name></person-group><article-title>Reaction time and musical expectancy: Priming of chords</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1986</year><volume>12</volume><issue>4</issue><fpage>403</fpage><pub-id pub-id-type="pmid">2946797</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Brains and algorithms partially converge in natural language processing</article-title><source>Communications biology</source><year>2022</year><volume>5</volume><issue>1</issue><fpage>134</fpage><pub-id pub-id-type="pmcid">PMC8850612</pub-id><pub-id pub-id-type="pmid">35173264</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title><source>Nature Human Behaviour</source><year>2023</year><volume>7</volume><issue>3</issue><fpage>430</fpage><lpage>441</lpage><pub-id pub-id-type="pmcid">PMC10038805</pub-id><pub-id pub-id-type="pmid">36864133</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>VK</given-names></name><name><surname>Harrison</surname><given-names>PM</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name></person-group><article-title>Cognitive and sensory expectations independently shape musical expectancy and pleasure</article-title><source>Philosophical Transactions of the Royal Society B</source><year>2024</year><volume>379</volume><issue>1895</issue><elocation-id>20220420</elocation-id><pub-id pub-id-type="pmcid">PMC10725761</pub-id><pub-id pub-id-type="pmid">38104601</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2022.0420</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>VK</given-names></name><name><surname>Harrison</surname><given-names>PM</given-names></name><name><surname>Meyer</surname><given-names>L</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name></person-group><article-title>Uncertainty and surprise jointly predict musical pleasure and amygdala, hippocampus, and auditory cortex activity</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><issue>23</issue><fpage>4084</fpage><lpage>4092</lpage><pub-id pub-id-type="pmid">31708393</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Civit</surname><given-names>M</given-names></name><name><surname>Civit-Masot</surname><given-names>J</given-names></name><name><surname>Cuadrado</surname><given-names>F</given-names></name><name><surname>Escalona</surname><given-names>MJ</given-names></name></person-group><article-title>A systematic review of artificial intelligence-based music generation: Scope, applications, and future trends</article-title><source>Expert Systems with Applications</source><year>2022</year><volume>209</volume><elocation-id>118190</elocation-id></element-citation></ref><ref id="R10"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Creighton</surname><given-names>H</given-names></name></person-group><source>Songs and ballads from Nova Scotia</source><year>1966</year><comment>(No Title)</comment></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name></person-group><article-title>The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees</article-title><source>Neuron</source><year>2015</year><volume>88</volume><issue>1</issue><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desbordes</surname><given-names>T</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Chanoine</surname><given-names>V</given-names></name><name><surname>Oquab</surname><given-names>M</given-names></name><name><surname>Badier</surname><given-names>J-M</given-names></name><name><surname>Trébuchon</surname><given-names>A</given-names></name><name><surname>Carron</surname><given-names>R</given-names></name><name><surname>Bénar</surname><given-names>C-G</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Dimensionality and ramping: Signatures of sentence integration in the dynamics of brains and deep language models</article-title><source>Journal of Neuroscience</source><year>2023</year><volume>43</volume><issue>29</issue><fpage>5350</fpage><lpage>5364</lpage><pub-id pub-id-type="pmcid">PMC10359032</pub-id><pub-id pub-id-type="pmid">37217308</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1163-22.2023</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>De Cheveigné</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>Elife</source><year>2020</year><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="pmcid">PMC7053998</pub-id><pub-id pub-id-type="pmid">32122465</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Two distinct neural timescales for predictive speech processing</article-title><source>Neuron</source><year>2020</year><volume>105</volume><issue>2</issue><fpage>385</fpage><lpage>393</lpage><pub-id pub-id-type="pmcid">PMC6981026</pub-id><pub-id pub-id-type="pmid">31806493</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egermann</surname><given-names>H</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name><name><surname>McAdams</surname><given-names>S</given-names></name></person-group><article-title>Probabilistic models of expectation violation predict psychophysiological emotional responses to live concert music</article-title><source>Cognitive, Affective &amp; Behavioral Neuroscience</source><year>2013</year><volume>13</volume><fpage>533</fpage><lpage>553</lpage><pub-id pub-id-type="pmid">23605956</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>L</given-names></name><name><surname>Carlson</surname><given-names>K</given-names></name><name><surname>Clifton</surname><given-names>C</given-names></name></person-group><article-title>Prosodic phrasing is central to language comprehension</article-title><source>Trends in Cognitive Sciences</source><year>2006</year><volume>10</volume><issue>6</issue><fpage>244</fpage><lpage>249</lpage><pub-id pub-id-type="pmid">16651019</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillis</surname><given-names>M</given-names></name><name><surname>Vanthornhout</surname><given-names>J</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name></person-group><article-title>Neural markers of speech comprehension: Measuring EEG tracking of linguistic speech representations, controlling the speech acoustics</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>50</issue><fpage>10316</fpage><lpage>10329</lpage><pub-id pub-id-type="pmcid">PMC8672699</pub-id><pub-id pub-id-type="pmid">34732519</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0812-21.2021</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Esposito</surname><given-names>M</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><issue>4</issue><fpage>664</fpage><lpage>672</lpage><pub-id pub-id-type="pmcid">PMC10076214</pub-id><pub-id pub-id-type="pmid">36928634</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasberg</surname><given-names>BR</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><article-title>A model of loudness applicable to time-varying sounds</article-title><source>Journal of the Audio Engineering Society</source><year>2002</year><volume>50</volume><issue>5</issue><fpage>331</fpage><lpage>342</lpage></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>BP</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Mas-Herrero</surname><given-names>E</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Predictability and uncertainty in the pleasure of music: A reward for learning?</article-title><source>Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>47</issue><fpage>9397</fpage><lpage>9409</lpage><pub-id pub-id-type="pmcid">PMC6867811</pub-id><pub-id pub-id-type="pmid">31636112</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0428-19.2019</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Luessi</surname><given-names>M</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Strohmeier</surname><given-names>D</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Goj</surname><given-names>R</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Brooks</surname><given-names>T</given-names></name><name><surname>Parkkonen</surname><given-names>L</given-names></name></person-group><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroinformatics</source><year>2013</year><volume>7</volume><fpage>267</fpage><pub-id pub-id-type="pmcid">PMC3872725</pub-id><pub-id pub-id-type="pmid">24431986</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groppe</surname><given-names>DM</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Dykstra</surname><given-names>AR</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Mégevand</surname><given-names>P</given-names></name><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Lado</surname><given-names>FA</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><article-title>iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</article-title><source>Journal of Neuroscience Methods</source><year>2017</year><volume>281</volume><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="pmid">28192130</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Top-down information shapes lexical processing when listening to continuous speech</article-title><source>Language, Cognition and Neuroscience</source><year>2024</year><volume>39</volume><issue>8</issue><fpage>1045</fpage><lpage>1058</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannon</surname><given-names>EE</given-names></name><name><surname>Snyder</surname><given-names>JS</given-names></name><name><surname>Eerola</surname><given-names>T</given-names></name><name><surname>Krumhansl</surname><given-names>CL</given-names></name></person-group><article-title>The role of melodic and temporal cues in perceiving musical meter</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2004</year><volume>30</volume><issue>5</issue><fpage>956</fpage><pub-id pub-id-type="pmid">15462633</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>NC</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><article-title>Predictive uncertainty in auditory sequence processing</article-title><source>Frontiers in Psychology</source><year>2014</year><volume>5</volume><fpage>1052</fpage><pub-id pub-id-type="pmcid">PMC4171990</pub-id><pub-id pub-id-type="pmid">25295018</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01052</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawthorne</surname><given-names>C</given-names></name><name><surname>Stasyuk</surname><given-names>A</given-names></name><name><surname>Roberts</surname><given-names>A</given-names></name><name><surname>Simon</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>C-ZA</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Elsen</surname><given-names>E</given-names></name><name><surname>Engel</surname><given-names>J</given-names></name><name><surname>Eck</surname><given-names>D</given-names></name></person-group><article-title>Enabling factorized piano music modeling and generation with the MAESTRO dataset</article-title><source>arXiv Preprint arXiv</source><year>2018</year><elocation-id>1810.12247</elocation-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>MM</given-names></name><name><surname>McKinney</surname><given-names>T</given-names></name><name><surname>Chajka</surname><given-names>K</given-names></name><name><surname>Pelz</surname><given-names>JB</given-names></name></person-group><article-title>Predictive eye movements in natural vision</article-title><source>Experimental Brain Research</source><year>2012</year><volume>217</volume><fpage>125</fpage><lpage>136</lpage><pub-id pub-id-type="pmcid">PMC3328199</pub-id><pub-id pub-id-type="pmid">22183755</pub-id><pub-id pub-id-type="doi">10.1007/s00221-011-2979-2</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Armeni</surname><given-names>K</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>De Lange</surname><given-names>FP</given-names></name></person-group><article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>32</issue><elocation-id>e2201968119</elocation-id><pub-id pub-id-type="pmcid">PMC9371745</pub-id><pub-id pub-id-type="pmid">35921434</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isnard</surname><given-names>J</given-names></name><name><surname>Taussig</surname><given-names>D</given-names></name><name><surname>Bartolomei</surname><given-names>F</given-names></name><name><surname>Bourdillon</surname><given-names>P</given-names></name><name><surname>Catenoix</surname><given-names>H</given-names></name><name><surname>Chassoux</surname><given-names>F</given-names></name><name><surname>Chipaux</surname><given-names>M</given-names></name><name><surname>Clémenceau</surname><given-names>S</given-names></name><name><surname>Colnat-Coulbois</surname><given-names>S</given-names></name><name><surname>Denuelle</surname><given-names>M</given-names></name></person-group><article-title>French guidelines on stereoelectroencephalography (SEEG)</article-title><source>Neurophysiologie Clinique</source><year>2018</year><volume>48</volume><issue>1</issue><fpage>5</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">29277357</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Khosla</surname><given-names>M</given-names></name><name><surname>Dobs</surname><given-names>K</given-names></name></person-group><article-title>Using artificial neural networks to ask ‘why’ questions of minds and brains</article-title><source>Trends in Neurosciences</source><year>2023</year><volume>46</volume><issue>3</issue><fpage>240</fpage><lpage>254</lpage><pub-id pub-id-type="pmid">36658072</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kern</surname><given-names>P</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><article-title>Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience</article-title><source>Elife</source><year>2022</year><volume>11</volume><elocation-id>e80935</elocation-id><pub-id pub-id-type="pmcid">PMC9836393</pub-id><pub-id pub-id-type="pmid">36562532</pub-id><pub-id pub-id-type="doi">10.7554/eLife.80935</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Gunter</surname><given-names>T</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Schröger</surname><given-names>E</given-names></name></person-group><article-title>Brain indices of music processing:“nonmusicians” are musical</article-title><source>Journal of Cognitive Neuroscience</source><year>2000</year><volume>12</volume><issue>3</issue><fpage>520</fpage><lpage>541</lpage><pub-id pub-id-type="pmid">10931776</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>Predictive processes and the peculiar case of music</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><issue>1</issue><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="pmid">30471869</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koskinen</surname><given-names>M</given-names></name><name><surname>Kurimo</surname><given-names>M</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name></person-group><article-title>Brain activity reflects the predictability of word sequences in listened continuous speech</article-title><source>NeuroImage</source><year>2020</year><volume>219</volume><elocation-id>116936</elocation-id><pub-id pub-id-type="pmid">32474080</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>What limits our capacity to process nested long-range dependencies in sentence comprehension?</article-title><source>Entropy</source><year>2020</year><volume>22</volume><issue>4</issue><fpage>446</fpage><pub-id pub-id-type="pmcid">PMC7516924</pub-id><pub-id pub-id-type="pmid">33286220</pub-id><pub-id pub-id-type="doi">10.3390/e22040446</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Hupkes</surname><given-names>D</given-names></name><name><surname>Vergallito</surname><given-names>A</given-names></name><name><surname>Marelli</surname><given-names>M</given-names></name><name><surname>Baroni</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Mechanisms for handling nested dependencies in neural-network language models and humans</article-title><source>Cognition</source><year>2021</year><volume>213</volume><elocation-id>104699</elocation-id><pub-id pub-id-type="pmid">33941375</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name><name><surname>Kilner</surname><given-names>J</given-names></name><name><surname>Barnes</surname><given-names>G</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name></person-group><article-title>EEG and MEG data analysis in SPM8</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><issue>1</issue><elocation-id>852961</elocation-id><pub-id pub-id-type="pmcid">PMC3061292</pub-id><pub-id pub-id-type="pmid">21437221</pub-id><pub-id pub-id-type="doi">10.1155/2011/852961</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacEvoy</surname><given-names>SP</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><article-title>Decoding the representation of multiple simultaneous objects in human occipitotemporal cortex</article-title><source>Current Biology</source><year>2009</year><volume>19</volume><issue>11</issue><fpage>943</fpage><lpage>947</lpage><pub-id pub-id-type="pmcid">PMC2875119</pub-id><pub-id pub-id-type="pmid">19446454</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2009.04.020</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maess</surname><given-names>B</given-names></name><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Gunter</surname><given-names>TC</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>Musical syntax is processed in Broca’s area: An MEG study</article-title><source>Nature Neuroscience</source><year>2001</year><volume>4</volume><issue>5</issue><fpage>540</fpage><lpage>545</lpage><pub-id pub-id-type="pmid">11319564</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maheu</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name></person-group><article-title>Brain signatures of a multiscale process of sequence learning in humans</article-title><source>Elife</source><year>2019</year><volume>8</volume><elocation-id>e41541</elocation-id><pub-id pub-id-type="pmcid">PMC6361584</pub-id><pub-id pub-id-type="pmid">30714904</pub-id><pub-id pub-id-type="doi">10.7554/eLife.41541</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McAdams</surname><given-names>S</given-names></name><name><surname>Winsberg</surname><given-names>S</given-names></name><name><surname>Donnadieu</surname><given-names>S</given-names></name><name><surname>De Soete</surname><given-names>G</given-names></name><name><surname>Krimphoff</surname><given-names>J</given-names></name></person-group><article-title>Perceptual scaling of synthesized musical timbres: Common dimensions, specificities, and latent subject classes</article-title><source>Psychological Research</source><year>1995</year><volume>58</volume><fpage>177</fpage><lpage>192</lpage><pub-id pub-id-type="pmid">8570786</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mercier</surname><given-names>MR</given-names></name><name><surname>Dubarry</surname><given-names>A-S</given-names></name><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Avanzini</surname><given-names>P</given-names></name><name><surname>Axmacher</surname><given-names>N</given-names></name><name><surname>Cellier</surname><given-names>D</given-names></name><name><surname>Del Vecchio</surname><given-names>M</given-names></name><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><article-title>Advances in human intracranial electroencephalography research, guidelines and good practices</article-title><source>Neuroimage</source><year>2022</year><volume>260</volume><elocation-id>119438</elocation-id><pub-id pub-id-type="pmcid">PMC10190110</pub-id><pub-id pub-id-type="pmid">35792291</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119438</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Maheu</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Human inferences about sequences: A minimal transition probability model</article-title><source>PLoS Computational Biology</source><year>2016</year><volume>12</volume><issue>12</issue><elocation-id>e1005260</elocation-id><pub-id pub-id-type="pmcid">PMC5193331</pub-id><pub-id pub-id-type="pmid">28030543</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005260</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Müllensiefen</surname><given-names>D</given-names></name><name><surname>Gingras</surname><given-names>B</given-names></name><name><surname>Musil</surname><given-names>J</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name></person-group><article-title>The musicality of non-musicians: An index for assessing musical sophistication in the general population</article-title><source>PloS one</source><year>2014</year><volume>9</volume><issue>2</issue><elocation-id>e89642</elocation-id><pub-id pub-id-type="pmcid">PMC3935919</pub-id><pub-id pub-id-type="pmid">24586929</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089642</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munhall</surname><given-names>KG</given-names></name><name><surname>Gribble</surname><given-names>P</given-names></name><name><surname>Sacco</surname><given-names>L</given-names></name><name><surname>Ward</surname><given-names>M</given-names></name></person-group><article-title>Temporal constraints on the McGurk effect</article-title><source>Perception &amp; Psychophysics</source><year>1996</year><volume>58</volume><fpage>351</fpage><lpage>362</lpage><pub-id pub-id-type="pmid">8935896</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nathan</surname><given-names>F</given-names></name><name><surname>Jean-Pierre</surname><given-names>B</given-names></name><name><surname>Fabien</surname><given-names>C</given-names></name><name><surname>El</surname><given-names>F-SA</given-names></name><name><surname>Nicolas</surname><given-names>G</given-names></name></person-group><source>MidiTok: A Python package for MIDI file tokenization</source><conf-name>Extended Abstracts for the Late-Breaking Demo Session of the 22nd Int. Society for Music Information Retrieval Conf., Online</conf-name><year>2021</year></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Pearce</surname><given-names>M</given-names></name><name><surname>Lehongre</surname><given-names>K</given-names></name><name><surname>Hasboun</surname><given-names>D</given-names></name><name><surname>Navarro</surname><given-names>V</given-names></name><name><surname>Adam</surname><given-names>C</given-names></name><name><surname>Samson</surname><given-names>S</given-names></name></person-group><article-title>Intracranial recordings and computational modeling of music reveal the time course of prediction error signaling in frontal and temporal cortices</article-title><source>Journal of Cognitive Neuroscience</source><year>2019</year><volume>31</volume><issue>6</issue><fpage>855</fpage><lpage>873</lpage><pub-id pub-id-type="pmid">30883293</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Stewart</surname><given-names>L</given-names></name></person-group><article-title>Tracking of pitch probabilities in congenital amusia</article-title><source>Neuropsychologia</source><year>2012</year><volume>50</volume><issue>7</issue><fpage>1483</fpage><lpage>1493</lpage><pub-id pub-id-type="pmid">22414591</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oore</surname><given-names>S</given-names></name><name><surname>Simon</surname><given-names>I</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Eck</surname><given-names>D</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group><article-title>This time with feeling: Learning expressive musical performance</article-title><source>Neural Computing and Applications</source><year>2020</year><volume>32</volume><fpage>955</fpage><lpage>967</lpage></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><issue>1</issue><elocation-id>156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>C</given-names></name><name><surname>Krumhansl</surname><given-names>CL</given-names></name></person-group><article-title>Mental representations for musical meter</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1990</year><volume>16</volume><issue>4</issue><fpage>728</fpage><pub-id pub-id-type="pmid">2148588</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R53"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><source>The construction and evaluation of statistical models of melodic structure in music perception and composition</source><year>2005</year></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><article-title>Statistical learning and probabilistic prediction in music cognition: Mechanisms of stylistic enculturation</article-title><source>Annals of the New York Academy of Sciences</source><year>2018</year><volume>1423</volume><issue>1</issue><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="pmcid">PMC6849749</pub-id><pub-id pub-id-type="pmid">29749625</pub-id><pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name></person-group><article-title>Expectation in melody: The influence of context and learning</article-title><source>Music Perception</source><year>2006</year><volume>23</volume><issue>5</issue><fpage>377</fpage><lpage>405</lpage></element-citation></ref><ref id="R56"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Penny</surname><given-names>WD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Ashburner</surname><given-names>JT</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><source>Statistical parametric mapping: the analysis of functional brain images</source><publisher-name>Elsevier</publisher-name><year>2011</year></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Cerebral representation of sequence patterns across multiple presentation formats</article-title><source>Cortex</source><year>2021</year><volume>145</volume><fpage>13</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">34673292</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Hansen</surname><given-names>NC</given-names></name><name><surname>Højlund</surname><given-names>A</given-names></name><name><surname>Pearce</surname><given-names>M</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name></person-group><article-title>Decomposing neural responses to melodic surprise in musicians and non-musicians: Evidence for a hierarchy of predictions in the auditory system</article-title><source>NeuroImage</source><year>2020</year><volume>215</volume><elocation-id>116816</elocation-id><pub-id pub-id-type="pmid">32276064</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Hansen</surname><given-names>NC</given-names></name><name><surname>Højlund</surname><given-names>A</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name></person-group><article-title>Reduced prediction error responses in high-as compared to low-uncertainty musical contexts</article-title><source>Cortex</source><year>2019</year><volume>120</volume><fpage>181</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">31323458</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Ellis</surname><given-names>DP</given-names></name></person-group><source>Intuitive analysis, creation and manipulation of MIDI data with pretty_midi</source><year>2014</year><fpage>84</fpage><lpage>93</lpage></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Read</surname><given-names>J</given-names></name><name><surname>Martino</surname><given-names>L</given-names></name><name><surname>Hollmén</surname><given-names>J</given-names></name></person-group><article-title>Multi-label methods for prediction with sequential data</article-title><source>Pattern Recognition</source><year>2017</year><volume>63</volume><fpage>45</fpage><lpage>55</lpage></element-citation></ref><ref id="R62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Riemenschneider</surname><given-names>A</given-names></name></person-group><source>371 Harmonized Chorales and 69 Chorale Melodies with figured bass</source><publisher-name>G. Schirmer</publisher-name><year>1941</year></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname><given-names>JM</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name></person-group><article-title>Proactive sensing of periodic and aperiodic auditory patterns</article-title><source>Trends in cognitive sciences</source><year>2018</year><volume>22</volume><issue>10</issue><fpage>870</fpage><lpage>882</lpage><pub-id pub-id-type="pmid">30266147</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sankaran</surname><given-names>N</given-names></name><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Theunissen</surname><given-names>F</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>Encoding of melody in the human auditory cortex</article-title><source>Science Advances</source><year>2024</year><volume>10</volume><issue>7</issue><elocation-id>eadk0010</elocation-id><pub-id pub-id-type="pmcid">PMC10871532</pub-id><pub-id pub-id-type="pmid">38363839</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adk0010</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>If deep learning is the answer, what is the question?</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><issue>1</issue><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">33199854</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaffrath</surname><given-names>H</given-names></name></person-group><article-title>The ESAC databases and MAPPET software</article-title><source>Computing in Musicology</source><year>1992</year><volume>8</volume><issue>66</issue><fpage>1658</fpage></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schellekens</surname><given-names>W</given-names></name><name><surname>Van Wezel</surname><given-names>RJ</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name><name><surname>Raemaekers</surname><given-names>M</given-names></name></person-group><article-title>Predictive coding for motion stimuli in human early visual cortex</article-title><source>Brain Structure and Function</source><year>2016</year><volume>221</volume><fpage>879</fpage><lpage>890</lpage><pub-id pub-id-type="pmid">25445839</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitt</surname><given-names>L-M</given-names></name><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Tune</surname><given-names>S</given-names></name><name><surname>Rysop</surname><given-names>AU</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Predicting speech from a cortical hierarchy of event-based time scales</article-title><source>Science Advances</source><year>2021</year><volume>7</volume><issue>49</issue><elocation-id>eabi6070</elocation-id><pub-id pub-id-type="pmcid">PMC8641937</pub-id><pub-id pub-id-type="pmid">34860554</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abi6070</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Slaats</surname><given-names>S</given-names></name><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Meyer</surname><given-names>AS</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><article-title>Delta-band neural responses to individual words are modulated by sentence processing</article-title><source>Journal of Neuroscience</source><year>2023</year><volume>43</volume><issue>26</issue><fpage>4867</fpage><lpage>4883</lpage><pub-id pub-id-type="pmcid">PMC10312058</pub-id><pub-id pub-id-type="pmid">37221093</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0964-22.2023</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanics</surname><given-names>G</given-names></name><name><surname>Kremláček</surname><given-names>J</given-names></name><name><surname>Czigler</surname><given-names>I</given-names></name></person-group><article-title>Visual mismatch negativity: A predictive coding view</article-title><source>Frontiers in Human Neuroscience</source><year>2014</year><volume>8</volume><fpage>666</fpage><pub-id pub-id-type="pmcid">PMC4165279</pub-id><pub-id pub-id-type="pmid">25278859</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00666</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name></person-group><article-title>Predictive coding and multisensory integration: An attentional account of the multisensory mind</article-title><source>Frontiers in Integrative Neuroscience</source><year>2015</year><volume>9</volume><fpage>19</fpage><pub-id pub-id-type="pmcid">PMC4374459</pub-id><pub-id pub-id-type="pmid">25859192</pub-id><pub-id pub-id-type="doi">10.3389/fnint.2015.00019</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temperley</surname><given-names>D</given-names></name></person-group><article-title>A probabilistic model of melody perception</article-title><source>Cognitive Science</source><year>2008</year><volume>32</volume><issue>2</issue><fpage>418</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">21635341</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tervaniemi</surname><given-names>M</given-names></name></person-group><article-title>The neuroscience of music–towards ecological validity</article-title><source>Trends in Neurosciences</source><year>2023</year><volume>46</volume><issue>5</issue><fpage>355</fpage><lpage>364</lpage><pub-id pub-id-type="pmid">37012175</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsoumakas</surname><given-names>G</given-names></name><name><surname>Katakis</surname><given-names>I</given-names></name></person-group><article-title>Multi-label classification: An overview</article-title><source>Data Warehousing and Mining: Concepts, Methodologies, Tools, and Applications</source><year>2008</year><fpage>64</fpage><lpage>74</lpage></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van der Burg</surname><given-names>E</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name><name><surname>Bronkhorst</surname><given-names>AW</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name></person-group><article-title>Pip and pop: nonspatial auditory signals improve spatial visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2008</year><volume>34</volume><issue>5</issue><fpage>1053</fpage><pub-id pub-id-type="pmid">18823194</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Wassenhove</surname><given-names>V</given-names></name><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Visual speech speeds up the neural processing of auditory speech</article-title><source>Proceedings of the National Academy of Sciences</source><year>2005</year><volume>102</volume><issue>4</issue><fpage>1181</fpage><lpage>1186</lpage><pub-id pub-id-type="pmcid">PMC545853</pub-id><pub-id pub-id-type="pmid">15647358</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0408949102</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vencovský</surname><given-names>V</given-names></name></person-group><article-title>Roughness prediction based on a model of cochlear hydrodynamics</article-title><source>Archives of Acoustics</source><year>2016</year><volume>41</volume><issue>2</issue><fpage>189</fpage><lpage>201</lpage></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villalon</surname><given-names>SM</given-names></name><name><surname>Paz</surname><given-names>R</given-names></name><name><surname>Roehri</surname><given-names>N</given-names></name><name><surname>Lagarde</surname><given-names>S</given-names></name><name><surname>Pizzo</surname><given-names>F</given-names></name><name><surname>Colombet</surname><given-names>B</given-names></name><name><surname>Bartolomei</surname><given-names>F</given-names></name><name><surname>Carron</surname><given-names>R</given-names></name><name><surname>Bénar</surname><given-names>C-G</given-names></name></person-group><article-title>EpiTools, A software suite for presurgical brain mapping in epilepsy: Intracerebral EEG</article-title><source>Journal of Neuroscience Methods</source><year>2018</year><volume>303</volume><fpage>7</fpage><lpage>15</lpage><pub-id pub-id-type="pmid">29605667</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Stekelenburg</surname><given-names>JJ</given-names></name></person-group><article-title>Visual anticipatory information modulates multisensory interactions of artificial audiovisual stimuli</article-title><source>Journal of cognitive neuroscience</source><year>2010</year><volume>22</volume><issue>7</issue><fpage>1583</fpage><lpage>1596</lpage><pub-id pub-id-type="pmid">19583474</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Witek</surname><given-names>MA</given-names></name></person-group><article-title>Rhythmic complexity and predictive coding: A novel approach to modeling rhythm and meter perception in music</article-title><source>Frontiers in Psychology</source><year>2014</year><volume>5</volume><fpage>1111</fpage><pub-id pub-id-type="pmcid">PMC4181238</pub-id><pub-id pub-id-type="pmid">25324813</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01111</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainer</surname><given-names>J</given-names></name><name><surname>Cawley</surname><given-names>G</given-names></name></person-group><article-title>Nested cross-validation when selecting classifiers is overzealous for most practical applications</article-title><source>Expert Systems with Applications</source><year>2021</year><volume>182</volume><elocation-id>115222</elocation-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weineck</surname><given-names>K</given-names></name><name><surname>Wen</surname><given-names>OX</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title><source>Elife</source><year>2022</year><volume>11</volume><elocation-id>e75515</elocation-id><pub-id pub-id-type="pmcid">PMC9467512</pub-id><pub-id pub-id-type="pmid">36094165</pub-id><pub-id pub-id-type="doi">10.7554/eLife.75515</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><article-title>The structure and statistics of language jointly shape cross-frequency neural dynamics during spoken language comprehension</article-title><source>Nature Communications</source><year>2024</year><volume>15</volume><issue>1</issue><elocation-id>8850</elocation-id><pub-id pub-id-type="pmcid">PMC11471778</pub-id><pub-id pub-id-type="pmid">39397036</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-53128-1</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>RJ</given-names></name><name><surname>Peng</surname><given-names>J</given-names></name></person-group><article-title>An efficient gradient-based algorithm for on-line training of recurrent network trajectories</article-title><source>Neural Computation</source><year>1990</year><volume>2</volume><issue>4</issue><fpage>490</fpage><lpage>501</lpage></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Visual search: How do we find what we are looking for?</article-title><source>Annual review of vision science</source><year>2020</year><volume>6</volume><issue>1</issue><fpage>539</fpage><lpage>562</lpage><pub-id pub-id-type="pmid">32320631</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Yang</surname><given-names>T</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Tang</surname><given-names>S</given-names></name><name><surname>Min</surname><given-names>B</given-names></name></person-group><article-title>Geometry of sequence working memory in macaque prefrontal cortex</article-title><source>Science</source><year>2022</year><volume>375</volume><issue>6581</issue><fpage>632</fpage><lpage>639</lpage><pub-id pub-id-type="pmid">35143322</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zalta</surname><given-names>A</given-names></name><name><surname>Large</surname><given-names>EW</given-names></name><name><surname>Schön</surname><given-names>D</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name></person-group><article-title>Neural dynamics of predictive timing and motor engagement in music listening</article-title><source>Science Advances</source><year>2024</year><volume>10</volume><issue>10</issue><elocation-id>eadi2525</elocation-id><pub-id pub-id-type="pmcid">PMC10917349</pub-id><pub-id pub-id-type="pmid">38446888</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adi2525</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Jha</surname><given-names>DK</given-names></name><name><surname>Laftchiev</surname><given-names>E</given-names></name><name><surname>Nikovski</surname><given-names>D</given-names></name></person-group><article-title>Multi-label prediction in time series data using deep neural networks</article-title><source>arXiv Preprint arXiv</source><elocation-id>2001.10098</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Paradigm and analysis pipeline.</title><p><bold>(A)</bold> Description of PolyRNN working principles. PolyRNN receives sequences of vectors coding for the start (2), the sustain (1) or the absence (0) of each possible note of the 88 notes of the piano keyboard, with one vector per timestep (50 ms). With this representation, multiple note onsets can be represented within a single timestep. The model outputs predictions about the next timestep with one probability of occurrence per note. <bold>(B)</bold> When a musical sequence is presented to PolyRNN, three metrics are computed at each timestep based on the predictions: Surprise, Uncertainty and Predicted density. <bold>(C) Top:</bold> Each piano note in the experiment is associated with a series of acoustic features (gray; up to k=29) and with the metrics derived from the models’ outputs (cyan). <bold>Bottom:</bold> Neural signals are epoched around note onsets. Epochs of an exemplar sEEG channel (right auditory cortex) are shown. <bold>(D)</bold> Five-fold cross-validated ridge regression to predict the neural response to individual notes. <bold>Top:</bold> Beta coefficients of a subset of acoustic and music model features, obtained on the exemplar sEEG channel from (C). <bold>Bottom:</bold> Variance explained by all (acoustic and model) features (AM) in the exemplar sEEG channel, acoustic features only (A), and uniquely explained by model features (ΔR<sup>2</sup> = AM - A).</p></caption><graphic xlink:href="EMS201584-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Comparison of PolyRNN and IDyOM models with monophonic music in EEG.</title><p><bold>(A)</bold> Replication of the results of <xref ref-type="bibr" rid="R13">Di Liberto and colleagues (2020)</xref>. Plotted lines represent the variance uniquely explained by IDyOM’s features (ΔR<sup>2</sup> = AM - A; see <xref rid="F1" ref-type="fig">Fig. 1</xref>) at the note-level (note onsets locked at t = 0s) for each EEG channel, averaged across participants. The sound envelope and its half-way rectified derivative were used as control acoustic features. All channels and timepoints with ΔR<sup>2</sup> &gt; 3.5*10<sup>-4</sup> are kept as a cluster of interest for subsequent models comparison. <bold>(B)</bold> Variance uniquely explained by IDyOM and PolyRNN features, averaged within the spatiotemporal mask. The analysis was conducted by controlling for the variance captured either by (left) the acoustic temporal envelope and its derivative alone (as in <xref ref-type="bibr" rid="R13">Di Liberto and colleagues, 2020</xref>), or (right) with additional descriptors of note pitch and timing (see Methods). *: p &lt; 0.05 for paired t-tests with Bonferroni correction. Error bars indicate SEM across participants (N = 20).</p></caption><graphic xlink:href="EMS201584-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Encoding of polyphonic expectations (PolyRNN) in sEEG and MEG.</title><p><bold>(A)</bold> Variance uniquely explained by PolyRNN in sEEG data (N = 10), plotted on whole brains and temporal regions. Peak ΔR<sup>2</sup> (within [-0.5, 1] s) are color coded in significant sEEG channels. Black dots indicate non-significant channels (N = 10 patients). <bold>(B)</bold> Topography of ΔR<sup>2</sup> values averaged across time ([-0.5, 1] s) and participants in MEG data (N = 27). Significant channels are indicated as white dots (p &lt; 0.05, cluster corrected). <bold>(C)</bold> Variance explained by PolyRNN and/or control acoustic features at the note-level (note onsets locked at t = 0s) in sEEG, averaged across participants and significant channels. Shaded areas are SEM across channels. Solid black ribbon indicate significant differences between control and full models in at least one spatio-temporal cluster (p &lt; 0.05, cluster corrected). Note frequency represents the frequency of occurrence of a note at each timepoint (p = 1 at t = 0s, right y-axis). <bold>(D)</bold> Individual R<sup>2</sup> peaks (within [-0.5, 1] s) averaged across auditory channels (see Method) in sEEG. Colored lines indicate individual performance increase uniquely provided by PolyRNN music features. Error bars indicate SEM across participants. <bold>(E)</bold> Cumulative plot of variance explained, partitioned by each PolyRNN music feature, in sEEG. Surprise (n+1) corresponds to the surprise of the next note. The shared variance represents all non-unique explained variances. The Y-axis is depicted from zero as negative cross-validated R2 is meaningless. <bold>(F-G-H)</bold> Same as <bold>(C-D-E)</bold> with MEG data. <bold>(F)</bold> Shaded areas are SEM across participants.</p></caption><graphic xlink:href="EMS201584-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Comparison between PolyRNN and PerfRNN.</title><p>(<bold>A</bold>) Spatial distribution of R<sup>2</sup> peak differences (within [-0.5, 1] s) between PolyRNN (cyan) and PerfRNN (red) in sEEG. Only channels with abs(ΔR<sup>2</sup>) &gt; 0.002 are coloured, for visual purpose (no statistical testing). (<bold>B</bold>) Variance uniquely explained by each model, compared with control acoustic features, averaged across participants and auditory channels in sEEG. Shaded areas show SEM across channels. (<bold>C</bold>) Histogram of R<sup>2</sup> peak differences between PolyRNN and PerfRNN models in sEEG, across auditory channels. * t-test against zero, p &lt; 0.001. (<bold>D</bold>) Same as (B) in MEG.The solid black ribbon indicates a significant difference between PolyRNN and PerfRNN (p &lt; 0.05, cluster corrected). (<bold>E</bold>) Topography of R<sup>2</sup> difference between PolyRNN and PerfRNN at the peak (within [-0.5, 1] s), in MEG. Significant channels are indicated as white dots (p &lt; 0.05, cluster corrected).</p></caption><graphic xlink:href="EMS201584-f004"/></fig></floats-group></article>