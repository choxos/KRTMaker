<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199536</article-id><article-id pub-id-type="doi">10.1101/2024.10.16.618646</article-id><article-id pub-id-type="archive">PPR927249</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A deep learning approach to detect and visualise sexual dimorphism in monomorphic species</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Silva</surname><given-names>Nicolas J.</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Ferreira</surname><given-names>André C.</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A3">c</xref><xref ref-type="aff" rid="A5">e</xref></contrib><contrib contrib-type="author"><name><surname>Silva</surname><given-names>Liliana R.</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A3">c</xref></contrib><contrib contrib-type="author"><name><surname>Perret</surname><given-names>Samuel</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Tieo</surname><given-names>Sonia</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Renoult</surname><given-names>Julien P.</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Covas</surname><given-names>Rita</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="aff" rid="A3">c</xref><xref ref-type="aff" rid="A4">d</xref></contrib><contrib contrib-type="author"><name><surname>Doutrelant</surname><given-names>Claire</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A4">d</xref></contrib></contrib-group><aff id="A1"><label>a</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008rywf59</institution-id><institution>CEFE</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/051escj72</institution-id><institution>Univ Montpellier</institution></institution-wrap>, CNRS, EPHE, IRD, <city>Montpellier</city>, <country country="FR">France</country></aff><aff id="A2"><label>b</label>CIBIO/InBIO, Centro de Investigação em Biodiversidade e Recursos Genéticos, Campus de Vairão, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/043pwc612</institution-id><institution>Universidade do Porto</institution></institution-wrap>, <postal-code>4485-66</postal-code>, <city>Vairão</city>, <country country="PT">Portugal</country></aff><aff id="A3"><label>c</label>BIOPOLIS Program in genomics, Biodiversity and Land Planning, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0476hs695</institution-id><institution>CIBIO</institution></institution-wrap>, <addr-line>Campus de Vairão</addr-line>, <postal-code>4485-661</postal-code>, <city>Vairão</city>, <country country="PT">Portugal</country></aff><aff id="A4"><label>d</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rgme147</institution-id><institution>FitzPatrick Institute of African Ornithology</institution></institution-wrap>, DST-NRF Centre of Excellence, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03p74gp79</institution-id><institution>University of Cape Town</institution></institution-wrap>, <city>Rondebosch</city><postal-code>7701</postal-code>, <country country="ZA">South Africa</country></aff><aff id="A5"><label>e</label>Department of Evolutionary Biology and Environmental Studies, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02crff812</institution-id><institution>University of Zurich</institution></institution-wrap>, <city>Zurich</city>, <country country="CH">Switzerland</country></aff><author-notes><corresp id="CR1"><bold>Corresponding author:</bold> Nicolas J. Silva, <email>silva.nicolas.j@gmail.com</email></corresp><fn id="FN1"><label>1</label><p id="P1">Rita Covas and Claire Doutrelant should be considered joint last authors.</p></fn><fn id="FN2" fn-type="con"><p id="P2">Author’s contribution</p><p id="P3">N.J.S., A.C.F., L.R.S., R.C. and C.D. conceived the study. N.J.S., A.C.F, L.R.S., S.P., R.C. and C.D. collected long-term data. N.J.S., A.C.F., L.R.S., S.P. pre-processed the data. N.J.S., with the input from A.C.F., L.R.S., S.T. and J.P.R., led the deep learning model training and the statistical analyses. N.J.S led the writing of the manuscript. R.C. and C.D. acquired the funding and led the long-term program that made this work possible. All authors contributed critically to the drafts and gave final approval for publication.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>20</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>18</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P4">Sex recognition is facilitated by dimorphism in some traits. However, humans often fail to find the traits that allow to distinguish between sexes in other species. Deep learning has the potential to surpass humans in identifying cryptic differences between sexes, but, so far, has rarely been used to assess sexual dimorphism. In this study, we evaluated (i) the ability of a fine-tuned classification neural network, EfficientNet, to find differences between sexes in a species that appears monomorphic to humans, the sociable weaver (Philetairus socius). We then assessed (ii) the benefits of Grad-CAM visualisation techniques to understand which parts of the individuals are used by the network to differentiate the sexes. We trained 10-folds cross-validation models on more than 4,500 pictures of the head from more than 1,300 individuals. Our results show that the network can predict sex of sociable weavers with an accuracy of 76%, which is considerably higher than humans’ performance, and that the model was similarly good at predicting females and males. When interpreting the probability of being classified to one sex, our results further reveal an effect of the interaction of sex with age on the confidence score of the models which shows that younger males are less masculine than older ones, and older females more masculine than younger ones. Finally, using Grad-CAM we found that the model mostly uses the beak region to predict the sex of individuals. Overall, this work shows that artificial intelligence has the potential to be a non-invasive sexing tool, surpassing human capabilities and aiding in pinpointing potential cryptic dimorphic body parts that have yet to be identified. In birds, half of the world’s species appear sexually monomorphic to humans, and re-evaluation of species dimorphism with this type of methods could deepen our understanding of the effect of selection on animal traits.</p></abstract><kwd-group><kwd>Computer vision</kwd><kwd>Dimorphism detection</kwd><kwd>EfficientNet</kwd><kwd>Grad-CAM</kwd><kwd>Monomorphism</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P5">Sexual dimorphism can be present in few or several species’ traits. It is the result of selection on those traits (in particularly sexual selection and predation), drift and phylogenetic trajectories (<xref ref-type="bibr" rid="R5">Badyaev &amp; Hill, 2003</xref>). Exploring sexual dimorphism in different species allows to determine how the evolutionary pressures may vary within and between the sexes. Sexual dimorphism is usually based on differences on size, shape or colouration (<xref ref-type="bibr" rid="R31">Hedrick &amp; Temeles, 1989</xref>; <xref ref-type="bibr" rid="R18">Corl et al., 2010</xref>; <xref ref-type="bibr" rid="R63">Warren et al., 2013</xref>; <xref ref-type="bibr" rid="R39">Kuntner &amp; Coddington, 2020</xref>; <xref ref-type="bibr" rid="R48">Mori et al., 2022</xref>), and is common in taxa such as birds (<xref ref-type="bibr" rid="R3">Andersson, 1982</xref>), fish (<xref ref-type="bibr" rid="R38">Kodric-Brown &amp; Brown, 1984</xref>), mammals (<xref ref-type="bibr" rid="R11">Caro, 2009</xref>) and invertebrates (<xref ref-type="bibr" rid="R2">Allen et al., 2011</xref>). Humans, usually assess these traits through direct observations of the visual aspect of the individuals (<xref ref-type="bibr" rid="R6">Badyaev &amp; Hill, 2000</xref>; <xref ref-type="bibr" rid="R5">2003</xref>). However, our ability to detect dimorphism is limited by the capacity of our visual system that may differ from the ones of animals, and often only the most conspicuous dimorphism can be reliably identified by human observers. This limits our ability to detect different types of dimorphism, and thus to study their adaptive value (<xref ref-type="bibr" rid="R60">Villafuerte &amp; Negro, 1998</xref>; <xref ref-type="bibr" rid="R33">Hung et al., 2017</xref>; <xref ref-type="bibr" rid="R50">Ó Marcaigh et al., 2021</xref>).</p><p id="P6">Different approaches have been developed to overcome the issue of detecting sexual dimorphism. For instance, <xref ref-type="bibr" rid="R24">Eaton (2005)</xref> used spectrophotometry measurements of reflectance and, considering birds’ colour discriminatory abilities between 300nm and 700nm, showed that on 139 presumed sexually monochromatic bird species from the Passeriforme order, more than 90% were dichromatic from an avian visual perspective. Similar measures were used in a jumping spider (<italic>Phintella vittata</italic>) revealing a UV-B sexual dimorphism in this species (<xref ref-type="bibr" rid="R42">Li et al., 2008</xref>). More recently, computer vision has been shown to be effective in measuring colouration. For instance, Support Vector Machine, a machine learning tool, was used to classify sex of zebrafish (<italic>Danio rerio</italic>) based on caudal fin colouration (<xref ref-type="bibr" rid="R32">Hosseini et al., 2019</xref>). Support Vector Machine requires human interference to pre-define which features to learn: i.e., to identify and measure the specific visual features that allow to distinguish and classify sexes. Knowing the sexually dimorphic trait before the analyses has the main drawbacks of potentially missing important parts and features of the animals that might be useful for sex classification by the animals, but are cryptic to humans.</p><p id="P7">Deep learning methods, and in particularly Convolutional Neural Networks (CNNs), do not require predefinition of the relevant features and can overcome this limitation by automatically determining the features that are optimal to perform a given classification task (<xref ref-type="bibr" rid="R35">Jordan &amp; Mitchell, 2015</xref>; <xref ref-type="bibr" rid="R4">Angermueller et al., 2016</xref>; <xref ref-type="bibr" rid="R16">Christin et al., 2019</xref>; <xref ref-type="bibr" rid="R15">Chollet, 2021</xref>). Solving category classification problems using CNNs is now increasingly used in the fields of ecology and evolution (e.g., <xref ref-type="bibr" rid="R16">Christin et al., 2019</xref>; <xref ref-type="bibr" rid="R25">Ferreira et al., 2020</xref>; <xref ref-type="bibr" rid="R29">Hansen et al., 2020</xref>; <xref ref-type="bibr" rid="R49">Norman et al., 2023</xref>). Specifically, CNNs have been previously used in species with known sexually dimorphic traits in order to increase processing speed and efficiency of sex recognition, but to date, mostly in humans (<xref ref-type="bibr" rid="R23">Dwivedi &amp; Singh, 2019</xref>). For instance, deep learning was used on dentomaxillofacial features extracted from radiographs to sex children at an early age, when sexual dimorphism is not well marked yet (<xref ref-type="bibr" rid="R27">Franco et al., 2022</xref>). Similarly, on human skulls, a deep learning network analysing computed tomography scans of human brains was able to classify women and men with an accuracy of 95% (<xref ref-type="bibr" rid="R9">Bewes et al., 2019</xref>). In animals, <xref ref-type="bibr" rid="R14">Chen et al. (2021)</xref> showed that deep learning can be used to classify females and males in 30 <italic>Pseudopoda</italic> spider species based on their genitals (<xref ref-type="bibr" rid="R14">Chen et al., 2021</xref>); and <xref ref-type="bibr" rid="R32">Hosseini et al. (2019)</xref> also used CNNs successfully to classify sex based on colour on zebrafish pictures. However, CNNs have been rarely used to find sexual dimorphism in traits that appear monomorphic to the human eye, but in the gray wolf (<italic>Canis lupus</italic>) deep learning revealed a previously unnoticed sexual dimorphism in cranial shape (<xref ref-type="bibr" rid="R47">MacLeod &amp; Horwitz, 2020</xref>). In <italic>Pseudopoda</italic> spider species, <xref ref-type="bibr" rid="R14">Chen et al. (2021)</xref> also used deep learning identify sex-specific patterns and colours of the back of individuals (that are sexually monomorphic for humans) and found sexual dimorphism in 94% of their dataset. Finaly, a recent study on giant pandas (<italic>Ailuropoda melanoleuca</italic>) faces succeeded to perform sex classification with an accuracy of 77% using CNNs (<xref ref-type="bibr" rid="R62">Wang et al., 2019</xref>).</p><p id="P8">A major drawback of using CNNs to classify the sex of animals, however, is that the features automatically built and used by the algorithm for performing classification are unknown (without additional analyses), or are difficult to represent in a way that is meaningful for humans (i.e., CNNs work as ‘black boxes’, see <xref ref-type="bibr" rid="R1">Alain &amp; Bengio, 2016</xref>; <xref ref-type="bibr" rid="R12">Castelvecchi, 2016</xref>; <xref ref-type="bibr" rid="R55">Shwartz-Ziv &amp; Tishby, 2017</xref>; <xref ref-type="bibr" rid="R40">Lei et al., 2018</xref>). In order to overcome these limitations, tools have been developed to understand what do CNNs learn. In computer vision, one of the most widely used tools is the Gradient-weighted Class Activation Mapping (Grad-CAM) (<xref ref-type="bibr" rid="R54">Selvaraju et al., 2017</xref>). Grad-CAM creates an activation thermal map for a specific class that is superimposed with the original image. Every channel of the picture (i.e., the different dimensions of an image pixels) is weighted with the class gradient of the channel. These Grad-CAMs colour the pictures to identify which parts of the images were the most relevant for classification, and thus the possibly most important features to discriminate sex. For instance, in giant pandas, Grad-CAMs qualitatively showed sex difference in the eyes and in the nose of the individuals (<xref ref-type="bibr" rid="R62">Wang et al., 2019</xref>).</p><p id="P9">Here, we developed a deep learning tool based on EfficientNet (<xref ref-type="bibr" rid="R57">Tan &amp; Le, 2019</xref>) and used a monomorphic bird species, the sociable weaver (<italic>Philetairus socius</italic>) (<xref ref-type="bibr" rid="R45">MacLean, 1973</xref>) to (i) explore whether species that appear sexually monomorphic to humans can be detected as sexually dimorphic by CNNs based on pictures. In addition, (ii) we explored the use of Grad-CAM visualisation techniques to pinpoint which parts of the individuals are used by the network to differentiate the sexes. In birds, it is common to observe delayed plumage maturation (<xref ref-type="bibr" rid="R30">Hawkins et al., 2012</xref>), making young males generally more look alike females. We therefore (iii) tested whether the accuracy of sex identification varied with age, expecting younger males to be accurately classified less often than older males.</p></sec><sec id="S2" sec-type="materials | methods"><label>2</label><title>Material and Methods</title><sec id="S3"><label>2.1</label><title>Species and data collection</title><p id="P10">We worked with a population of sociable weavers inhabiting Benfontein Nature Reserve, in South Africa (-28°818S, 24°816E, 1190m asl) and monitored since 1993 (<xref ref-type="bibr" rid="R19">Covas et al., 2002</xref>). Since 2008, birds in the study colonies are caught every year, usually at the end of winter (August-September). During, the capture events, a blood sample is taken and used for genetic sexing (<xref ref-type="bibr" rid="R20">Covas et al., 2006</xref>). Additionally, top view pictures of the head of the birds are taken following an established protocol (<xref ref-type="bibr" rid="R53">Rat et al., 2015</xref>).</p><p id="P11">For most of the birds, breeding monitoring allows us to know their exact age. However, individuals can move between colonies (especially females; <xref ref-type="bibr" rid="R20">Covas et al., 2006</xref>; <xref ref-type="bibr" rid="R59">van Dijk et al., 2015</xref>). For immigrant birds first caught as adults in the studied population, we consider them to have the average population dispersion age at capture. We computed this age by adding to the first date of capture the average minimum age of first dispersion measured in the population (690 days for males and 727 days for females to the date of first capture (<xref ref-type="bibr" rid="R25">Silva et al., in prep</xref>)).</p></sec><sec id="S4"><label>2.2</label><title>Ethical note</title><p id="P12">Birds were captured and handled using protocols approved by the Northern Cape Nature Conservation (permit FAUNA 1638/2015, 0825/2016, 0212/2017, 0684/2019 and 0059/2021) and the Ethics Committee of the University of Cape Town (permit 2014/V1/RC and 2018/V20/RC). The birds were captured using mist nests and were extracted directly after being caught. All efforts have been made to minimise handling time of the birds (i.e., task division of the processing steps of extracting the birds from the mist nests, ringing the birds, taking blood samples, taking pictures; release of the birds was closely monitored to detect any uncommon flight behaviour). These procedures have been performed during the non-breeding season to avoid disturbing birds with chicks at the nests. In total, 1,323 caught individuals were included in this study (see more details in the following subsection).</p></sec><sec id="S5"><label>2.3</label><title>Datasets</title><p id="P13">We obtained 4,595 images (2,246 for females and 2,349 for males) depicting 1,323 individuals (661 females and 662 males) photographed during six annual capture events (between August 2015 and Septembre 2021 - no captures in 2020). We selected only individuals with a fully developed adult plumage. At each capture event, one to four pictures per individual were collected ((mean±SD) females: 2.63±0.81; males: 2.46±0.88). We trained and used a Mask-RCNN model to extract the birds’ heads from full pictures (see <xref ref-type="supplementary-material" rid="SD1">supplementary 1</xref> for description of the model). All pictures were orientated with the throat of the individuals facing the bottom of the pictures, in a portrait style (<xref ref-type="fig" rid="F1">Fig. 1.A</xref>). On the 1,323 individuals in our dataset, 993 were assigned with their real age ((mean±SD) 773±560 days old) and 330 with an estimated age ((mean±SD) 1,372±924 days old).</p></sec><sec id="S6"><label>2.4</label><title>Model training</title><p id="P14">We used transfer learning from an EfficientNet B4 network (<xref ref-type="bibr" rid="R57">Tan &amp; Le, 2019</xref>) pre-trained on ImageNet database (<xref ref-type="bibr" rid="R21">Deng et al., 2009</xref>) composed of more than 14 million pictures to classify objects from images. This method allowed us to use the weights of an already trained model and fine-tune them for the task we desire. Since our objective is also image classification (i.e., classify images as males or females), this pre-trained model on ImageNet network is an appropriate choice to perform transfer learning. During the fine-tuning of the network, we implemented average pooling throughout the model’s backbone to prevent over-fitting during the training. We replaced the fully connected part at the end of the EfficientNet B4 network with a dropout layer (rate: 0.4), a dense layer with 1,024 neurons and a ReLU activation, a dense layer with 64 neurons and a ReLU activation, and a final dense layer with one neuron and a Sigmoid activation. The dropout layer was added to limit over-fitting of the model by randomly setting to zero some neurons of the network (<xref ref-type="bibr" rid="R56">Srivastava et al., 2014</xref>). We compiled the model with an Adam optimizer (<xref ref-type="bibr" rid="R36">Kingma &amp; Ba, 2014</xref>), a binary cross-entropy loss and a binary accuracy. All layers were set as trainable except for the BatchNormalisation layers. Due to the sigmoid last layer, the model outputs are numbers between zero and one, with [0; 0.5[ being scores of sex prediction to females and [0.5; 1] being scores of sex prediction to males. To better compare males and females, we reversed the females’ score (one minus the predicted score). Therefore, for all individuals, a final score between [0; 0.5[ signal an erroneous sex prediction and a score between [0.5; 1] a correct sex prediction.</p><p id="P15">The hyper-parameters of the training process were inferred by training a preliminary model using 90% of the individuals (595 males, 596 females, 4183 pictures) for training dataset and 10% for validation (66 males, 66 females, 412 pictures). This preliminary model helped us to define the following parameters for training. The pictures were re-sized to 400x400 pixels and passed through the network with a batch size of 16 pictures. The training process was performed on 13 epochs with a learning rate of 10<sup>-3</sup> from epoch one to three to quickly learn general patterns for sex classification, then learning rate was set to 10<sup>-4</sup> to help the model to learn more precise features.</p><p id="P16">We then performed 10-fold cross-validation (again 90% of the individuals as training dataset and 10% of the individuals as test dataset). At each fold, individuals from the test dataset were swapped with an equivalent number of individuals from the training dataset. This allowed each individual to be in a test set once during the training, and thus to get one prediction of sex per image. We performed data augmentation and applied to the pictures random horizontal flip with a probability of 0.5 and random contrast between [-0.2; 0.2] with a probability of 0.5. Additionally, we evaluated the impact on the training process of reducing by a quarter, half and three quarters the number of individuals on in the training dataset (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>).</p></sec><sec id="S7"><label>2.5</label><title>Model visualisation</title><p id="P17">We used Grad-CAM (<xref ref-type="bibr" rid="R54">Selvaraju et al., 2017</xref>) to identify the specific areas of the images that influenced the model’s decision in classifying individuals as male or female. We computed on the Grad-CAM of the last 2D convolution layer (called “top_activation”) to calculate the probability of being a male (because we used a sigmoid activation for the last layer). The different regions of the pictures were coloured from blue to red, respectively from lower to higher importance, depending on the role they had in the network’s decision (<xref ref-type="fig" rid="F1">Fig. 1.B</xref>). Analysis for this part was adapted from <xref ref-type="bibr" rid="R15">Chollet (2021)</xref>.</p></sec><sec id="S8"><label>2.6</label><title>Statistical analysis</title><p id="P18">All statistical analyses were conducted on R version 4.3.1.</p><sec id="S9"><label>2.6.1</label><title>Sex recognition</title><p id="P19">The average accuracy of the models on the test datasets was saved (N=10 models) at the end of each fold during the training process. These accuracies were compared to those reached by human observers. To evaluate human performance, seven of the authors (AUTHORS’ NAMES HIDDEN FOR THE REVIEWING PROCESS) were first given the same set of labelled pictures of 25 females and 25 males as an attempt to train themselves to distinguish males and females. Then, they manually and blindly classified each a 100 randomly selected images of unique individuals (50 males and 50 females). Then, we performed a first Wilcoxon test to compare the accuracies of the machine and humans.</p><p id="P20">We simulated a random classification of sex of the 100 birds classified by humans using a binomial distribution with a probability of 0.5. With Wilcoxon signed rank exact tests, we tested if the accuracy reached by the CNNs and the one reached by humans were different from a random classification.</p></sec><sec id="S10"><label>2.6.2</label><title>Visualisation of the features used for classification</title><p id="P21">We quantitatively measured which parts of the pictures the models are using to classify males and females. For each of the 10 training folds, we randomly sampled, from the test dataset, 10 pictures of males and 10 pictures of females that were rightly classified. We manually segmented these 200 pictures (<xref ref-type="fig" rid="F1">Fig. 1.D</xref>) to identify four parts of interest on the head of the birds: the cap, the beak, the bib and the eyes (<xref ref-type="fig" rid="F1">Fig. 1.E</xref>). We computed Grad-CAM images with a grey scale colouration (pure black indicates no activation and pure white indicates very high activation) (<xref ref-type="fig" rid="F1">Fig. 1.C</xref>). On these images, using GIMP version 2.10.36, for each region we measured, a) the proportion of activated pixels to estimate the surface proportion of each of the four regions used by the model for sex classification; and b) the average activation of the activated pixels from ]0; 1] (one indicates pure white). The first approach identifies the regions of the head that is most used by the model by considering its surface, while the second approach tells us about the importance of each activated region among the activated pixels in the pictures. Both measurements are useful to identify dimorphism because the first tells us how much of a region is used for classification and the second helps to pinpoint the intensity of a region. For instance, by relying on both measurements we could distinguish regions with a large proportion of pixels activated at low intensity from regions with only a small portion of its surface activated but with very high intensity. Because of the sigmoid last layer, we predict the probability for an individual to be a male. Therefore, high difference of activation between males and females indicates higher dimorphism on the trait, while if the activation for a trait is similar between males and females, then the traits isn’t very dimoprhic.</p><p id="P22">We performed multiple Student tests to determine if the proportion of activated pixels per regions and the average activation were different between sexes, and multiple paired Student tests to understand if these two measures were different between regions within the same individual. P-values were adjusted with the Benjamini-Hochberg procedure (<xref ref-type="bibr" rid="R8">Benjamini &amp; Hochberg, 1995</xref>).</p></sec><sec id="S11"><label>2.6.3</label><title>Sex and age difference on the classification performance</title><p id="P23">We tested if males and females differed in their likelihood of being correctly identified and if age also influenced the classification performance by running two mixed models. <list list-type="simple" id="L1"><list-item><label>(i)</label><p id="P24">We performed a generalised linear mixed model (GLMM) (Binomial error distribution with a link logit). As response variable, we binarised the average predicted score per individual, with 1 indicating rightly classified individuals and 0 wrongly classified ones. As fixed factors, we used sex, and we added as random factor the identity of the individuals, the cross-validation fold and the year of data collection. Several pictures per individuals were used and it is important to specify in the model they are not independent points. The pictures in a test dataset during the cross-validation folds were evaluated with the same model and therefore their predicted score is not independent. Last, different people between years were involved in the picture collection of the birds, so we controlled for intra- and inter-photographer variation.</p></list-item><list-item><label>(ii)</label><p id="P25">We fitted a GLMM (Beta error distribution with a link logit). The response variable, was the score predicted by the model (i.e., the confidence score given by the deep learning model). This score indicates how confident the model is about its classification decision. A score closer to 1 indicates that the sex classification is relatively robust. We used sex, age, and their interaction as fixed factors, to account for the expectation of males becoming more masculine with age, an effect not anticipated for females. As random factors, we included the identity of the individuals, the cross-validation fold and the year of data collection. 27 pictures (18 males and 2 females individuals, (mean±SD) 1,009±825 days old) were predicted with a confident score of exactly 1, but because Beta distribution only accepts continuous values in range ]0; 1[, we substracted a value of 10<sup>-10</sup> for the pictures to be included in the model. Age was scaled and mean-centered. Since age is either the true age or the estimated age for immigrants, the model was computed first with the individuals with the exact known age only (992 individuals, 3,445 pictures), and second with all the individuals (1,323 individuals, 4,595 pictures). The two models were similar, except for the effect of age in females, that tends to be negative in the model including all individuals and is negative in the model including only individuals with exact age (<xref ref-type="supplementary-material" rid="SD1">Table S2</xref>). We decided to be conservative and keep only the individuals with known exact age.</p></list-item></list>
</p><p id="P26">Mixed models were computed with lme4 package version 1.1‐34 (<xref ref-type="bibr" rid="R7">Bates et al., 2014</xref>) and glmmTMB package version 1.1.8 (<xref ref-type="bibr" rid="R10">Brooks et al., 2017</xref>) for the GLMM Beta. Plots of models were created with ggplot2 package version 3.4.2 (<xref ref-type="bibr" rid="R64">Wickham, 2014</xref>), ggeffects version 1.3.1 (<xref ref-type="bibr" rid="R43">Lüdecke, 2018</xref>) and sjPlot package version 2.8.15 (<xref ref-type="bibr" rid="R44">Lüdecke, 2023</xref>).</p></sec></sec></sec><sec id="S12" sec-type="results"><label>3</label><title>Results</title><sec id="S13"><label>3.1</label><title>Sex recognition</title><p id="P27">On average, we found that the models were able to classify males and females with an average accuracy of (mean±SD) 75.8%±2.4% at the picture level within the 10 folds (<xref ref-type="fig" rid="F1">Fig. 1</xref>), and very strong evidence (Wilcoxon test, V=0, P&lt;0.001) that these classifications were different from a random guess ((i.e., accuracy=0.5). Human classifications were also found to be different from a random guess (Wilcoxon test, V=0, P&lt;0.001) but they were less accurate (mean±SD) 58.0%±8.2% (N=7 humans; <xref ref-type="fig" rid="F1">Fig. 1</xref>) than the deep learning models’ performance (Wilcoxon test, W=70, P&lt;0.001; <xref ref-type="fig" rid="F1">Fig. 1</xref>). These results show that the deep learning method is able to classify the sex of sociable weavers with higher accuracy than humans.</p></sec><sec id="S14"><label>3.2</label><title>Visualisation of the features used for classification</title><p id="P28">Males presented both higher proportion of activated pixels and a higher intensity in the activated pixels than females for all of the four regions delimited in the head of the birds (beak, bib, cap and eyes: <xref ref-type="fig" rid="F3">Fig. 3A,B</xref>; <xref ref-type="supplementary-material" rid="SD1">Table S3</xref>). Among these four regions, both the highest proportion of activated pixels and the highest intensity in the activated pixels were observed in the beak for males, and in the bib for females (<xref ref-type="supplementary-material" rid="SD1">Table S4</xref>). This indicates that the four regions are used by the model to differentiate males from females, but that the model focuses more intensely on the beak of the individuals to perform its classification. The fact that the bib is also activated in females may indicate that the bib is a source of misclassification and that not many sex difference can be found on that trait.</p></sec><sec id="S15"><label>3.3</label><title>Sex and age difference on the classification performance</title><p id="P29">We found no evidence that sex has an effect on the probability to rightly classify a picture of an individual ((estimate±SE (95%CI)) -0.19±0.17 (-0.53 ; 0.14), z=-1.12, P=0.260; <xref ref-type="table" rid="T1">Table 1</xref>), indicating that no sex is better classified than the other one.</p><p id="P30">We found a strong evidence for an association between predicted score and age that differed between the sexes ((estimate±SE (95%CI)) 0.43±0.07 (0.30 ; 0.56), z=6.47, P&lt;0.001; <xref ref-type="supplementary-material" rid="SD1">Fig. S1; Table S2.B</xref>), with a positive link for males and a negative for females (<xref ref-type="fig" rid="F4">Fig. 4</xref>; <xref ref-type="supplementary-material" rid="SD1">Table S2.B</xref>). This indicates that, for males, younger individuals are less well classified than older ones, while for females it is the reverse.</p></sec></sec><sec id="S16" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P31">Our results show that a deep learning-based method surpasses human performance when classifying the sex in a bird species with limited sexual dimorphism, and that by using Grad-CAM we were able to pinpoint differences used by the algorithm to differentiate between males and females. Finally, by investigating the source of misclassifications of the models, we found that age influenced differently females and males when predicting sex using CNNs.</p><p id="P32">Species visually monomorphic for human eyes are common, and in these cases long and costly behavioural observations or genetic methods are necessary to determine the sex of individuals. For instance, in birds, half of the world species does not show clear differences between males and females for human eyes (<xref ref-type="bibr" rid="R28">Griffiths et al., 1998</xref>). If the CNNs identify features that are detectable for animals, this will have implications that will allow to better understand how sexual and natural selection may have shaped the evolution of these species’ morphological traits. Deep learning methods require large volumes of data to be trained and thus time and funding to collect these data. In addition, running deep learning models use large amounts of energy. However, once a model is trained it can be indefinitely re-used, improved and can often outperformed humans (e.g., <xref ref-type="bibr" rid="R22">Ditria et al., 2020</xref>; <xref ref-type="bibr" rid="R51">Ouyang et al., 2020</xref>; <xref ref-type="bibr" rid="R34">Jocher et al., 2022</xref>; this study).</p><p id="P33">Here, we reached only 76% accuracy, similarly to the one obtained on giant panda faces (<xref ref-type="bibr" rid="R62">Wang et al., 2016</xref>). However, we only used a small portion of the bird’s body and future work could potentially increase model’s performance by considering, for instance, photos from multiple body parts of the birds at the same time. In addition, increasing the training sample size and including more pictures of more individuals at different ages is also expected to improve model accuracy. Our results (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>) show that the efficiency of our trained network decreased when decreasing the training dataset sample size, indicating that we may not have reach the full potential of using CNNs for sex classification in our species. Here, we selected very standardised pictures to train and test our models, and further tests could be performed with pictures taken under more natural conditions (e.g., at bird feeders; see in <xref ref-type="bibr" rid="R25">Ferreira et al., 2020</xref>). Hence, we suggest that in some monomorphic species and with large enough training datasets, and photos from different parts of the body, sexual recognition of individuals using deep learning models could potentially achieve sufficient performance to replace genetic sexing, which, in the long run, could save time and research funds. Importantly, these methods have the potential to limit animal suffering, pictures can be taken without capturing individuals (e.g., <xref ref-type="bibr" rid="R25">Ferreira et al., 2020</xref>) and avoiding the capture and handling stress.</p><p id="P34">With the Grad-CAMs, we were able to find a sexual dimorphism on the beak of the individuals. No sexual differences based on the head of the individuals were ever determined based on morphological measurements or the field obervations, and the sexes appeared undistinguishable (<xref ref-type="bibr" rid="R45">Maclean, 1973</xref>; <xref ref-type="bibr" rid="R20">Covas &amp; Doutrelant, unpublished data</xref>). Sexual dimorphism based on the beak of individuals have been described in other weaver species (e.g., red-billed quelea (<italic>Quelea quelea</italic>) (<xref ref-type="bibr" rid="R61">Walsh et al., 2012</xref>); red-billed buffalo weaver (<italic>Bubalornis niger</italic>) (<xref ref-type="bibr" rid="R46">Maclean, 1993</xref>); african village weaverbird (<italic>Ploceus cucullatus</italic>) (<xref ref-type="bibr" rid="R17">Collias &amp; Collias, 1970</xref>); white-browed sparrow-weaver (<italic>Plocepasser mahali</italic>) (<xref ref-type="bibr" rid="R41">Leitner et al., 2009</xref>)). However, the dimorphism described for these species, based on bill length and/or colouration, is visible using measurements, which in not the case for the sociable weavers. Further investigation is now necessary to understand which features are used by the model used here to classify sex (e.g., shapes, colours, contrasts). The mechanisms used by the deep learning models to identify the critical features is usually treated as a ‘black box’ (e.g., <xref ref-type="bibr" rid="R1">Alain &amp; Bengio, 2016</xref>; <xref ref-type="bibr" rid="R12">Castelvecchi, 2016</xref>; <xref ref-type="bibr" rid="R55">Shwartz-Ziv &amp; Tishby, 2017</xref>; <xref ref-type="bibr" rid="R40">Lei et al., 2018</xref>). Grad-CAMs are one of the first steps to overcome this issue and understand potential cryptic differences between categories. In dimorphic species, many studies have investigated the potential adaptive value of conspicuous traits (e.g., colouration, size), linking them to fitness benefits (e.g., reproductive success, mating success). Our results highlight the potential to investigate similar questions on dimorphic traits which are cryptic to the human eye, but can be detected by a deep learning neural network, that could potentially be under the same evolutionary pressures as conspicuous traits.</p><p id="P35">When dealing with classification problems, particularly in the field of computer science, there is a push to maximise performance metrics (e.g., to obtain nearly 100% accuracy). However, in the biological sciences, inefficiency of the neural networks could also inform us about cryptic features of animals (or organs, cells, etc.) that might open the door to new, biologically relevant hypotheses. For instance, deep learning was used in mandrills (<italic>Mandrillus sphinx</italic>) to assess face similarity of individuals, revealing higher similarity among kin and allowing to explore spatial associations between similar-looking individuals (<xref ref-type="bibr" rid="R13">Charpentier et al., 2022</xref>). Here, we took advantage of the imperfection of our models regarding sex determination to reveal that young males are less masculine than older ones and that older females look more masculine than older ones, something that as been already observed in primates (<xref ref-type="bibr" rid="R37">Kloth et al., 2015</xref>; <xref ref-type="bibr" rid="R58">Tieo et al., 2023</xref>). In birds, however, only a lower masculinity of younger males has been regularly observed with notably plumage delayed maturation (<xref ref-type="bibr" rid="R30">Hawkins et al., 2012</xref>), while, to our knowledge, no quantitative description of female masculinisation with age has been reported in a non-primate species (although some descriptions exist, for example older females gang-gang cockatoos (<italic>Callocephalon fimbriatum</italic>) can acquire the orange colouration in the head that is typical of males; <xref ref-type="bibr" rid="R26">Forshaw &amp; Cooper, 2002</xref>). This is not surprisingly, particularly in apparently monomorphic species, where humans cannot directly obtain information about which regions of the individuals are attributed to a masculine or a feminine appearance, hindering attempts of investigating how they change with age. We note here that for very old female sociable weavers, the relationship between predicted score and age could arise from the male bias sample size in the training dataset (0 females and 21 males older than 3,000 days; <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). However, removing these old individuals from the dataset used for the mixed models did not change the observed relationship for younger individuals, although with a slight change in effect size (<xref ref-type="supplementary-material" rid="SD1">Table S5</xref>). This indicates that despite the unbalanced sex ratio of very old individuals, we can still be confident, for most individuals, of the relationship between predicted score and age in relation to sex that we found. Our approach and results indicate that in birds, similarly to primates (<xref ref-type="bibr" rid="R13">Charpentier et al., 2022</xref>; <xref ref-type="bibr" rid="R58">Tieo et al., 2023</xref>), deep learning could be used to study how sex and age (or other relevant traits) interact and affects the visual aspect of individuals. In addition, this also indicates that potential features expressing age may be present in the head of the individuals (e.g., age classification of giant pandas (<xref ref-type="bibr" rid="R52">Qi et al., (2022)</xref>). These results open the door to address the potential adaptative value of such interactions.</p></sec><sec id="S17" sec-type="conclusions"><label>5</label><title>Conclusion</title><p id="P36">Our study emphasises the potential for deep learning methods to surpass human abilities in detecting sexual dimorphism in species. It also helped us to identify the beak as a trait potentially indicating sex in our study species. We hope that this approach will stimulate interest among researchers to re-assess sexual monomorphism in their study species, and how it varies in relation to individual attributes. Sexual dimorphism is one of the main traits used in sexual selection studies and better quantification of species dimorphism could influence our understanding of the selective forces acting in each focal species, potentially uncovering new avenues for research.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary materials</label><media xlink:href="EMS199536-supplement-Supplementary_materials.pdf" mimetype="application" mime-subtype="pdf" id="d116aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S18"><title>Acknowledgments</title><p>We thank all the people that helped in field data collection along the years and in particular Annick Lucas and Margaux Rat. We also thank Franck Théron for the management of the long-term database. Access to Benfontein Nature Reserve was provided by De Beers Mining Corporation. This study was supported by funding from ANR to C.D. (France, grant 19-CE02-0014-01), ERC to R.C. (EU, Consolidator grant 866489) and by the DST-NRF Centre of Excellence at the Fitzpatrick Institute of African Ornithology University of Cape Town. N.J.S. was funded by the ANR 19-CE02-0014-01, L.R.S by ERC-Consolidator 866489, R.C. by FCT (CEECIND/03451/2018) and A.C.F by University of Zurich Forschungskredit postdoc grant (K-74312-01-01 University of Zurich), Swiss Federal Commission for Scholarships and by European Research Council (grant agreement no. 850859 awarded to Damien Farine). This project is part of the OSU OREME, and long-term Studies in Ecology and Evolution (SEE-Life) program of the CNRS.</p></ack><sec id="S19" sec-type="data-availability"><title>Data availability</title><p id="P37">Data are available in the Dryad repository : <ext-link ext-link-type="uri" xlink:href="https://datadryad.org/stash/share/hZ8_tpL2XtIzvEH3pDBT4HsF9EExA2sdVb22foe7oh8">https://datadryad.org/stash/share/hZ8_tpL2XtIzvEH3pDBT4HsF9EExA2sdVb22foe7oh8</ext-link></p></sec><fn-group><fn id="FN3" fn-type="conflict"><p id="P38">Conflict of interest</p><p id="P39">We declare no conflicts of interest.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alain</surname><given-names>G</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title>Understanding Intermediate Layers Using Linear Classifier Probes</article-title><source>ArXiv Preprint</source><year>2017</year><elocation-id>ArXiv:1610.01644</elocation-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>CE</given-names></name><name><surname>Zwaan</surname><given-names>BJ</given-names></name><name><surname>Brakefield</surname><given-names>PM</given-names></name></person-group><article-title>Evolution of sexual dimorphism in the lepidoptera</article-title><source>Annual Review of Entomology</source><year>2011</year><volume>56</volume><fpage>445</fpage><lpage>464</lpage><pub-id pub-id-type="pmid">20822452</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>M</given-names></name></person-group><article-title>Sexual selection, natural selection and quality advertisement</article-title><source>Biological Journal of the Linnean Society</source><year>1982</year><volume>17</volume><issue>4</issue><fpage>375</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8312.1982.tb02028.x</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angermueller</surname><given-names>C</given-names></name><name><surname>Pärnamaa</surname><given-names>T</given-names></name><name><surname>Parts</surname><given-names>L</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name></person-group><article-title>Deep learning for computational biology</article-title><source>Molecular Systems Biology</source><year>2016</year><volume>12</volume><issue>7</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmcid">PMC4965871</pub-id><pub-id pub-id-type="pmid">27474269</pub-id><pub-id pub-id-type="doi">10.15252/msb.20156651</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badyaev</surname><given-names>AV</given-names></name><name><surname>Hill</surname><given-names>GE</given-names></name></person-group><article-title>Avian Sexual Dichromatism in Relation to Phylogeny and Ecology</article-title><source>Annual Review of Ecology, Evolution, and Systematics</source><year>2003</year><volume>34</volume><fpage>27</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1146/annurev.ecolsys.34.011802.132441</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badyaev</surname><given-names>AV</given-names></name><name><surname>Hill</surname><given-names>GE</given-names></name></person-group><article-title>Evolution of sexual dichromatism: Contribution of carotenoid-versus melanin-based coloration</article-title><source>Biological Journal of the Linnean Society</source><year>2000</year><volume>69</volume><issue>2</issue><fpage>153</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1006/bijl.1999.0350</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>BM</given-names></name><name><surname>Walker</surname><given-names>SC</given-names></name></person-group><article-title>Fitting linear mixed-effects models using lme4</article-title><source>Journal of Statistical Software</source><year>2014</year><volume>67</volume><issue>1</issue><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society. Series B (Methodological)</source><year>1995</year><volume>57</volume><issue>1</issue><fpage>289</fpage><lpage>300</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bewes</surname><given-names>J</given-names></name><name><surname>Low</surname><given-names>A</given-names></name><name><surname>Morphett</surname><given-names>A</given-names></name><name><surname>Pate</surname><given-names>FD</given-names></name><name><surname>Henneberg</surname><given-names>M</given-names></name></person-group><article-title>Artificial intelligence for sex determination of skeletal remains: Application of a deep learning artificial neural network to human skulls</article-title><source>Journal of Forensic and Legal Medicine</source><year>2019</year><volume>62</volume><month>December</month><fpage>40</fpage><lpage>43</lpage><comment>2018</comment><pub-id pub-id-type="pmid">30639854</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname><given-names>ME</given-names></name><name><surname>Kristensen</surname><given-names>K</given-names></name><name><surname>van Benthem</surname><given-names>KJ</given-names></name><name><surname>Magnusson</surname><given-names>A</given-names></name><name><surname>Berg</surname><given-names>CW</given-names></name><name><surname>Nielsen</surname><given-names>A</given-names></name><name><surname>Skaug</surname><given-names>HJ</given-names></name><name><surname>Mächler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>BM</given-names></name></person-group><article-title>glmmTMB balances speed and flexibility among packages for zero-inflated generalized linear mixed modeling</article-title><source>R Journal</source><year>2017</year><volume>9</volume><issue>2</issue><fpage>378</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.32614/rj-2017-066</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caro</surname><given-names>T</given-names></name></person-group><article-title>Contrasting coloration in terrestrial mammals</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2009</year><volume>364</volume><issue>1516</issue><fpage>537</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC2674080</pub-id><pub-id pub-id-type="pmid">18990666</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2008.0221</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelvecchi</surname><given-names>D</given-names></name></person-group><article-title>Can we open the black box of AI?</article-title><source>Nature News</source><year>2016</year><volume>538</volume><issue>7623</issue><fpage>20</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">27708329</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charpentier</surname><given-names>MJE</given-names></name><name><surname>Poirotte</surname><given-names>C</given-names></name><name><surname>Roura-Torres</surname><given-names>B</given-names></name><name><surname>Amblard-Rambert</surname><given-names>P</given-names></name><name><surname>Willaume</surname><given-names>E</given-names></name><name><surname>Kappeler</surname><given-names>PM</given-names></name><name><surname>Rousset</surname><given-names>F</given-names></name><name><surname>Renoult</surname><given-names>JP</given-names></name></person-group><article-title>Mandrill mothers associate with infants who look like their own offspring using phenotype matching</article-title><source>ELife</source><year>2022</year><volume>11</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC9665846</pub-id><pub-id pub-id-type="pmid">36377479</pub-id><pub-id pub-id-type="doi">10.7554/eLife.79417</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>T</given-names></name></person-group><article-title>Research on Spider Sex Recognition from Images Based on Deep Learning</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>120985</fpage><lpage>120995</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2021.3109120</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year>2021</year><chapter-title>Deep Learning with Python</chapter-title><source>Angular and Machine Learning Pocket Primer</source><publisher-name>Simon and Schuster</publisher-name></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christin</surname><given-names>S</given-names></name><name><surname>Hervet</surname><given-names>É</given-names></name><name><surname>Lecomte</surname><given-names>N</given-names></name></person-group><article-title>Applications for deep learning in ecology</article-title><source>Methods in Ecology and Evolution</source><year>2019</year><volume>10</volume><issue>10</issue><fpage>1632</fpage><lpage>1644</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13256</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collias</surname><given-names>NE</given-names></name><name><surname>Collias</surname><given-names>EC</given-names></name></person-group><article-title>the Behaviour of the West African Village Weaverbird</article-title><source>Ibis</source><year>1970</year><volume>112</volume><issue>4</issue><fpage>457</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1111/j.1474-919X.1970.tb00818.x</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corl</surname><given-names>A</given-names></name><name><surname>Davis</surname><given-names>AR</given-names></name><name><surname>Kuchta</surname><given-names>SR</given-names></name><name><surname>Comendant</surname><given-names>T</given-names></name><name><surname>Sinervo</surname><given-names>B</given-names></name></person-group><article-title>Alternative mating strategies and the evolution of sexual size dimorphism in the side-blotched lizard, Uta stansburiana: A population-level comparative analysis</article-title><source>Evolution</source><year>2010</year><volume>64</volume><issue>1</issue><fpage>79</fpage><lpage>96</lpage><pub-id pub-id-type="pmid">19659598</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Covas</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>CR</given-names></name><name><surname>Anderson</surname><given-names>MD</given-names></name><name><surname>Brown</surname><given-names>MB</given-names></name></person-group><article-title>Stabilizing selection on body mass in the sociable weaver Philetairus socius</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2002</year><volume>269</volume><issue>1503</issue><fpage>1905</fpage><lpage>1909</lpage><pub-id pub-id-type="pmcid">PMC1691106</pub-id><pub-id pub-id-type="pmid">12350252</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2002.2106</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Covas</surname><given-names>R</given-names></name><name><surname>Dalecky</surname><given-names>A</given-names></name><name><surname>Caizergues</surname><given-names>A</given-names></name><name><surname>Doutrelant</surname><given-names>C</given-names></name></person-group><article-title>Kin associations and direct vs indirect fitness benefits in colonial cooperatively breeding sociable weavers Philetairus socius</article-title><source>Behavioral Ecology and Sociobiology</source><year>2006</year><volume>60</volume><fpage>323</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1007/s00265-006-0168-2</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>L-J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>K</given-names></name></person-group><article-title>ImageNet: Constructing a large-scale image database</article-title><source>Journal of Vision</source><year>2009</year><volume>9</volume><issue>8</issue><fpage>1037</fpage><pub-id pub-id-type="doi">10.1167/9.8.1037</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditria</surname><given-names>EM</given-names></name><name><surname>Lopez-Marcano</surname><given-names>S</given-names></name><name><surname>Sievers</surname><given-names>M</given-names></name><name><surname>Jinks</surname><given-names>EL</given-names></name><name><surname>Brown</surname><given-names>CJ</given-names></name><name><surname>Connolly</surname><given-names>RM</given-names></name></person-group><article-title>Automating the Analysis of Fish Abundance Using Object Detection: Optimizing Animal Ecology With Deep Learning</article-title><source>Frontiers in Marine Science</source><year>2020</year><month>June</month><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.3389/fmars.2020.00429</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dwivedi</surname><given-names>N</given-names></name><name><surname>Singh</surname><given-names>DK</given-names></name></person-group><source>Review of deep learning techniques for gender classification in images</source><conf-name>Harmony Search and Nature Inspired Optimization Algorithms: Theory and Applications, ICHSA 2018</conf-name><conf-sponsor>Springer</conf-sponsor><conf-loc>Singapore</conf-loc><year>2019</year><fpage>1089</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1007/978-981-13-0761-4_102</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eaton</surname><given-names>MD</given-names></name></person-group><article-title>Human vision fails to distinguish widespread sexual dichromatism among sexually “monochromatic” birds</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2005</year><volume>102</volume><issue>31</issue><fpage>10942</fpage><lpage>10946</lpage><pub-id pub-id-type="pmcid">PMC1182419</pub-id><pub-id pub-id-type="pmid">16033870</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0501891102</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferreira</surname><given-names>AC</given-names></name><name><surname>Silva</surname><given-names>LR</given-names></name><name><surname>Renna</surname><given-names>F</given-names></name><name><surname>Brandl</surname><given-names>HB</given-names></name><name><surname>Renoult</surname><given-names>JP</given-names></name><name><surname>Farine</surname><given-names>DR</given-names></name><name><surname>Covas</surname><given-names>R</given-names></name><name><surname>Doutrelant</surname><given-names>C</given-names></name></person-group><article-title>Deep learning-based methods for individual recognition in small birds</article-title><source>Methods in Ecology and Evolution</source><year>2020</year><volume>11</volume><issue>9</issue><fpage>1072</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.13436</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Forshaw</surname><given-names>JM</given-names></name><name><surname>Cooper</surname><given-names>WT</given-names></name></person-group><source>Australian Parrots</source><publisher-name>Lansdowne</publisher-name><publisher-loc>Melbourne</publisher-loc><year>2002</year></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franco</surname><given-names>A</given-names></name><name><surname>Porto</surname><given-names>L</given-names></name><name><surname>Heng</surname><given-names>D</given-names></name><name><surname>Murray</surname><given-names>J</given-names></name><name><surname>Lygate</surname><given-names>A</given-names></name><name><surname>Franco</surname><given-names>R</given-names></name><name><surname>Bueno</surname><given-names>J</given-names></name><name><surname>Sobania</surname><given-names>M</given-names></name><name><surname>Costa</surname><given-names>MM</given-names></name><name><surname>Paranhos</surname><given-names>LR</given-names></name><name><surname>Manica</surname><given-names>S</given-names></name><etal/></person-group><article-title>Diagnostic performance of convolutional neural networks for dental sexual dimorphism</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC9568558</pub-id><pub-id pub-id-type="pmid">36241670</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-21294-1</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffiths</surname><given-names>R</given-names></name><name><surname>Double</surname><given-names>CM</given-names></name><name><surname>Orr</surname><given-names>K</given-names></name><name><surname>Dawson</surname><given-names>JGR</given-names></name></person-group><article-title>A DNA test to sex most birds</article-title><source>Molecular Ecology</source><year>1998</year><volume>7</volume><fpage>1071</fpage><lpage>1075</lpage><pub-id pub-id-type="pmid">9711866</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>OLP</given-names></name><name><surname>Svenning</surname><given-names>JC</given-names></name><name><surname>Olsen</surname><given-names>K</given-names></name><name><surname>Dupont</surname><given-names>S</given-names></name><name><surname>Garner</surname><given-names>BH</given-names></name><name><surname>Iosifidis</surname><given-names>A</given-names></name><name><surname>Price</surname><given-names>BW</given-names></name><name><surname>Høye</surname><given-names>TT</given-names></name></person-group><article-title>Species-level image classification with convolutional neural network enables insect identification from habitus images</article-title><source>Ecology and Evolution</source><year>2020</year><volume>10</volume><issue>2</issue><fpage>737</fpage><lpage>747</lpage><pub-id pub-id-type="pmcid">PMC6988528</pub-id><pub-id pub-id-type="pmid">32015839</pub-id><pub-id pub-id-type="doi">10.1002/ece3.5921</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname><given-names>GL</given-names></name><name><surname>Hill</surname><given-names>GE</given-names></name><name><surname>Mercadante</surname><given-names>A</given-names></name></person-group><article-title>Delayed plumage maturation and delayed reproductive investment in birds</article-title><source>Biological Reviews</source><year>2012</year><volume>87</volume><issue>2</issue><fpage>257</fpage><lpage>274</lpage><pub-id pub-id-type="pmid">21790949</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedrick</surname><given-names>AV</given-names></name><name><surname>Temeles</surname><given-names>EJ</given-names></name></person-group><article-title>The evolution of sexual dimorphism in animals: Hypotheses and tests</article-title><source>Trends in Ecology and Evolution</source><year>1989</year><volume>4</volume><issue>5</issue><fpage>136</fpage><lpage>138</lpage><pub-id pub-id-type="pmid">21227335</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosseini</surname><given-names>S</given-names></name><name><surname>Simianer</surname><given-names>H</given-names></name><name><surname>Tetens</surname><given-names>J</given-names></name><name><surname>Brenig</surname><given-names>B</given-names></name><name><surname>Herzog</surname><given-names>S</given-names></name><name><surname>Sharifi</surname><given-names>AR</given-names></name></person-group><article-title>Efficient phenotypic sex classification of zebrafish using machine learning methods</article-title><source>Ecology and Evolution</source><year>2019</year><volume>9</volume><issue>23</issue><fpage>13332</fpage><lpage>13343</lpage><pub-id pub-id-type="pmcid">PMC6912926</pub-id><pub-id pub-id-type="pmid">31871648</pub-id><pub-id pub-id-type="doi">10.1002/ece3.5788</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>HY</given-names></name><name><surname>Yeung</surname><given-names>CKL</given-names></name><name><surname>Omland</surname><given-names>KE</given-names></name><name><surname>Te Yao</surname><given-names>C</given-names></name><name><surname>Yao</surname><given-names>CJ</given-names></name><name><surname>Li</surname><given-names>SH</given-names></name></person-group><article-title>Himalayan black bulbuls (Hypsipetes leucocephalus niggerimus) exhibit sexual dichromatism under ultraviolet light that is invisible to the human eye</article-title><source>Scientific Reports</source><year>2017</year><month>October</month><volume>7</volume><fpage>1</fpage><lpage>9</lpage><comment>2016</comment><pub-id pub-id-type="pmcid">PMC5382547</pub-id><pub-id pub-id-type="pmid">28382942</pub-id><pub-id pub-id-type="doi">10.1038/srep43707</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jocher</surname><given-names>G</given-names></name><name><surname>Chaurasia</surname><given-names>A</given-names></name><name><surname>Stoken</surname><given-names>A</given-names></name><name><surname>Borovec</surname><given-names>J</given-names></name><name><surname>Kwon</surname><given-names>Y</given-names></name><name><surname>Michael</surname><given-names>K</given-names></name><name><surname>Fang</surname><given-names>J</given-names></name><name><surname>Yifu</surname><given-names>Z</given-names></name><name><surname>Wong</surname><given-names>C</given-names></name><name><surname>Montes</surname><given-names>D</given-names></name></person-group><article-title>ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime Instance Segmentatio</article-title><source>Zenodo</source><year>2022</year><pub-id pub-id-type="doi">10.5281/zenodo.7347926</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><article-title>Machine learning: Trends, perspectives, and prospects</article-title><source>Science</source><year>2015</year><volume>349</volume><issue>6245</issue><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="pmid">26185243</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>JL</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>ArXiv Preprint</source><year>2015</year><elocation-id>ArXiv:1412.6980</elocation-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kloth</surname><given-names>N</given-names></name><name><surname>Damm</surname><given-names>M</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name><name><surname>Wiese</surname><given-names>H</given-names></name></person-group><article-title>Aging affects sex categorization of male and female faces in opposite ways</article-title><source>Acta Psychologica</source><year>2015</year><volume>158</volume><fpage>78</fpage><lpage>86</lpage><pub-id pub-id-type="pmid">25974392</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kodric-Brown</surname><given-names>A</given-names></name><name><surname>Brown</surname><given-names>JH</given-names></name></person-group><article-title>Truth in advertising: the kinds of traits favored by sexual selection</article-title><source>American Naturalist</source><year>1984</year><volume>124</volume><issue>3</issue><fpage>309</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1086/284275</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuntner</surname><given-names>M</given-names></name><name><surname>Coddington</surname><given-names>JA</given-names></name></person-group><article-title>Sexual size dimorphism: Evolution and perils of extreme phenotypes in spiders</article-title><source>Annual Review of Entomology</source><year>2020</year><volume>65</volume><fpage>57</fpage><lpage>80</lpage><pub-id pub-id-type="pmid">31573828</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>D</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name></person-group><article-title>Opening the black box of deep learning</article-title><source>ArXiv Preprint</source><year>2018</year><fpage>1</fpage><lpage>27</lpage><elocation-id>ArXiv:1805.08355</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1805.08355">http://arxiv.org/abs/1805.08355</ext-link></comment></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leitner</surname><given-names>S</given-names></name><name><surname>Mundy</surname><given-names>P</given-names></name><name><surname>Voigt</surname><given-names>C</given-names></name></person-group><article-title>Morphometrics of White-browed Sparrow-Weavers Plocepasser mahali in south-western Zimbabwe</article-title><source>Ostrich</source><year>2009</year><volume>80</volume><issue>2</issue><fpage>99</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.2989/OSTRICH.2009.80.2.6.833</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Lim</surname><given-names>MLM</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>F</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>D</given-names></name></person-group><article-title>Sexual dichromatism and male colour morph in ultraviolet-B reflectance in two populations of the jumping spider Phintella vittata (Araneae: Salticidae) from tropical China</article-title><source>Biological Journal of the Linnean Society</source><year>2008</year><volume>94</volume><issue>1</issue><fpage>7</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8312.2008.00968.x</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lüdecke</surname><given-names>D</given-names></name></person-group><article-title>ggeffects: Tidy Data Frames of Marginal Effects from Regression Models</article-title><source>Journal of Open Source Software</source><year>2018</year><volume>3</volume><issue>26</issue><fpage>772</fpage><pub-id pub-id-type="doi">10.21105/joss.00772</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Lüdecke</surname><given-names>DM</given-names></name></person-group><year>2023</year><source>Package ‘sjPlot’</source><issue>1</issue></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maclean</surname><given-names>GL</given-names></name></person-group><article-title>The sociable weaver, part 1: description, distribution, dispersion and populations</article-title><source>Ostrich</source><year>1973</year><volume>44</volume><issue>3–4</issue><fpage>176</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1080/00306525.1973.9639158</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Maclean</surname><given-names>GL</given-names></name></person-group><year>1993</year><source>Roberts birds of southern Africa</source><publisher-name>John Voelcker Bird Book Fund</publisher-name><publisher-loc>Cape Town</publisher-loc></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>N</given-names></name><name><surname>Kolska Horwitz</surname><given-names>L</given-names></name></person-group><article-title>Machine-learning strategies for testing patterns of morphological variation in small samples: Sexual dimorphism in gray wolf (Canis lupus) crania</article-title><source>BMC Biology</source><year>2020</year><volume>18</volume><issue>1</issue><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="pmcid">PMC7470621</pub-id><pub-id pub-id-type="pmid">32883273</pub-id><pub-id pub-id-type="doi">10.1186/s12915-020-00832-1</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mori</surname><given-names>E</given-names></name><name><surname>Mazza</surname><given-names>G</given-names></name><name><surname>Lovari</surname><given-names>S</given-names></name></person-group><chapter-title>Sexual Dimorphism</chapter-title><source>Encyclopedia of Animal Cognition and Behavior</source><year>2022</year><fpage>6389</fpage><lpage>6395</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-47829-6</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>DL</given-names></name><name><surname>Bischoff</surname><given-names>PH</given-names></name><name><surname>Wearn</surname><given-names>OR</given-names></name><name><surname>Ewers</surname><given-names>RM</given-names></name><name><surname>Rowcliffe</surname><given-names>JM</given-names></name><name><surname>Evans</surname><given-names>B</given-names></name><name><surname>Sethi</surname><given-names>S</given-names></name><name><surname>Chapman</surname><given-names>PM</given-names></name><name><surname>Freeman</surname><given-names>R</given-names></name></person-group><article-title>Can CNN-based species classification generalise across variation in habitat within a camera trap survey?</article-title><source>Methods in Ecology and Evolution</source><year>2023</year><volume>14</volume><issue>1</issue><fpage>242</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1111/2041-210X.14031</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ó Marcaigh</surname><given-names>F</given-names></name><name><surname>Kelly</surname><given-names>DJ</given-names></name><name><surname>Analuddin</surname><given-names>K</given-names></name><name><surname>Karya</surname><given-names>A</given-names></name><name><surname>Lawless</surname><given-names>N</given-names></name><name><surname>Marples</surname><given-names>NM</given-names></name></person-group><article-title>Cryptic sexual dimorphism reveals differing selection pressures on continental islands</article-title><source>Biotropica</source><year>2021</year><volume>53</volume><issue>1</issue><fpage>121</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1111/btp.12852</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>D</given-names></name><name><surname>He</surname><given-names>B</given-names></name><name><surname>Ghorbani</surname><given-names>A</given-names></name><name><surname>Yuan</surname><given-names>N</given-names></name><name><surname>Ebinger</surname><given-names>J</given-names></name><name><surname>Langlotz</surname><given-names>CP</given-names></name><name><surname>Heidenreich</surname><given-names>PA</given-names></name><name><surname>Harrington</surname><given-names>RA</given-names></name><name><surname>Liang</surname><given-names>DH</given-names></name><name><surname>Ashley</surname><given-names>EA</given-names></name><name><surname>Zou</surname><given-names>JY</given-names></name></person-group><article-title>Video-based AI for beat-to-beat assessment of cardiac function</article-title><source>Nature</source><year>2020</year><volume>580</volume><issue>7802</issue><fpage>252</fpage><lpage>256</lpage><pub-id pub-id-type="pmcid">PMC8979576</pub-id><pub-id pub-id-type="pmid">32269341</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2145-8</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>Y</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Hou</surname><given-names>R</given-names></name><name><surname>Zang</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>P</given-names></name><name><surname>He</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>P</given-names></name></person-group><article-title>Giant panda age recognition based on a facial image deep learning system</article-title><source>Ecology and Evolution</source><year>2022</year><volume>12</volume><issue>12</issue><elocation-id>e9507</elocation-id><pub-id pub-id-type="pmcid">PMC9719823</pub-id><pub-id pub-id-type="pmid">36479031</pub-id><pub-id pub-id-type="doi">10.1002/ece3.9507</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rat</surname><given-names>M</given-names></name><name><surname>van Dijk</surname><given-names>RE</given-names></name><name><surname>Covas</surname><given-names>R</given-names></name><name><surname>Doutrelant</surname><given-names>C</given-names></name></person-group><article-title>Dominance hierarchies and associated signalling in a cooperative passerine</article-title><source>Behavioral Ecology and Sociobiology</source><year>2015</year><volume>69</volume><fpage>437</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1007/s00265-014-1856-y</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Selvaraju</surname><given-names>RR</given-names></name><name><surname>Cogswell</surname><given-names>M</given-names></name><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Vedantam</surname><given-names>R</given-names></name><name><surname>Parikh</surname><given-names>D</given-names></name><name><surname>Batra</surname><given-names>D</given-names></name></person-group><source>Grad-cam: Visual explanations from deep networks via gradient-based localization</source><conf-name>Proceedings of the IEEE International Conference on Computer Vision</conf-name><year>2017</year><fpage>618</fpage><lpage>626</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1610.02391">http://arxiv.org/abs/1610.02391</ext-link></comment></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shwartz-Ziv</surname><given-names>R</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><article-title>Opening the Black Box of Deep Neural Networks via Information</article-title><source>ArXiv Preprint</source><year>2017</year><fpage>1</fpage><lpage>19</lpage><elocation-id>ArXiv:1703.00810</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1703.00810">http://arxiv.org/abs/1703.00810</ext-link></comment></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title><source>Journal of Machine Learning Research</source><year>2014</year><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></element-citation></ref><ref id="R57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M</given-names></name><name><surname>Le</surname><given-names>QV</given-names></name></person-group><source>EfficientNet: Rethinking model scaling for convolutional neural networks</source><conf-name>International Conference on Machine Learning</conf-name><year>2019</year><fpage>6105</fpage><lpage>6114</lpage></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tieo</surname><given-names>S</given-names></name><name><surname>Dezeure</surname><given-names>J</given-names></name><name><surname>Cryer</surname><given-names>A</given-names></name><name><surname>Lepou</surname><given-names>P</given-names></name><name><surname>Charpentier</surname><given-names>MJE</given-names></name><name><surname>Renoult</surname><given-names>JP</given-names></name></person-group><article-title>Social and sexual consequences of facial femininity in a non-human primate</article-title><source>IScience</source><year>2023</year><volume>26</volume><issue>10</issue><elocation-id>107901</elocation-id><pub-id pub-id-type="pmcid">PMC10520438</pub-id><pub-id pub-id-type="pmid">37766996</pub-id><pub-id pub-id-type="doi">10.1016/j.isci.2023.107901</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Dijk</surname><given-names>RE</given-names></name><name><surname>Covas</surname><given-names>R</given-names></name><name><surname>Doutrelant</surname><given-names>C</given-names></name><name><surname>Spottiswoode</surname><given-names>CN</given-names></name><name><surname>Hatchwell</surname><given-names>BJ</given-names></name></person-group><article-title>Fine-scale genetic structure reflects sex-specific dispersal strategies in a population of sociable weavers (Philetairus socius)</article-title><source>Molecular Ecology</source><year>2015</year><volume>24</volume><issue>16</issue><fpage>4296</fpage><lpage>4311</lpage><pub-id pub-id-type="pmid">26172866</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villafuerte</surname><given-names>R</given-names></name><name><surname>Negro</surname><given-names>JJ</given-names></name></person-group><article-title>Digital imaging for colour measurement in ecological research</article-title><source>Ecology Letters</source><year>1998</year><volume>1</volume><issue>3</issue><fpage>151</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1046/j.1461-0248.1998.00034.x</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>N</given-names></name><name><surname>Dale</surname><given-names>J</given-names></name><name><surname>McGraw</surname><given-names>KJ</given-names></name><name><surname>Pointer</surname><given-names>MA</given-names></name><name><surname>Mundy</surname><given-names>NI</given-names></name></person-group><source>Candidate genes for carotenoid coloration in vertebrates and their expression profiles in the carotenoid-containing plumage and bill of a wild bird</source><conf-name>Proceedings of the Royal Society B: Biological Sciences</conf-name><year>2011</year><volume>279</volume><issue>1726</issue><fpage>58</fpage><lpage>66</lpage><pub-id pub-id-type="pmcid">PMC3223654</pub-id><pub-id pub-id-type="pmid">21593031</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2011.0765</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>P</given-names></name><name><surname>Hou</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name></person-group><source>Learning deep features for giant panda gender classification using face images</source><conf-name>Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</conf-name><year>2019</year><fpage>279</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1109/ICCVW.2019.00037</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>IA</given-names></name><name><surname>Gotoh</surname><given-names>H</given-names></name><name><surname>Dworkin</surname><given-names>IM</given-names></name><name><surname>Emlen</surname><given-names>DJ</given-names></name><name><surname>Lavine</surname><given-names>LC</given-names></name></person-group><article-title>A general mechanism for conditional expression of exaggerated sexually-selected traits</article-title><source>BioEssays</source><year>2013</year><volume>35</volume><issue>10</issue><fpage>889</fpage><lpage>899</lpage><pub-id pub-id-type="pmid">23852854</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wickham</surname><given-names>MH</given-names></name></person-group><chapter-title>Package ‘ggplot2’</chapter-title><source>Create elegant data visualisations using the grammar of graphics. Version</source><year>2014</year><volume>2</volume><issue>1</issue><fpage>1</fpage><lpage>189</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://ggplot2.tidyverse.org/">https://ggplot2.tidyverse.org/</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Workflow of picture processing along the training and visualisation process.</title><p>(A) The picture is segmented from a bigger picture with a first neural network to keep only the head of the individuals with their bib facing down. (B) Blue to red scale Grad-CAM superimposed with transparency to the original picture. It allows us to visualise which regions of an image the model is using to perform its classification. (C) Grey scale Grad-CAM. (D) Grey scale Grad-CAM with the shape of the regions of interest of the head of the birds: the cap, the beak, the bib and the eyes. (E) Each region is exported on its own and analysed separately.</p></caption><graphic xlink:href="EMS199536-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Accuracy of the individual classification in males and females in function of the classification method used. The deep learning accuracies were obtained from the predictions on the test dataset at each of the 10-folds during the training. The human accuracies were obtained from blind visual classification by humans. The classification methods were found to be significantly different from each other and they were both found to be significantly different from random classification. Triangles represent raw data.</p></caption><graphic xlink:href="EMS199536-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Difference in (A) proportion of activated pixels and (B) mean activation of the activated pixels, in function of sex and region of the head. Females are in red and males in light blue. The beak of the individuals seems to be the main feature used by the models to perform its sex classification (more proportion of activated pixels and higher activation intensity in males than in females). Boxplots represent median and quantiles at 25% and 75%, and error bars represent mean and 95% confidence intervals.</p></caption><graphic xlink:href="EMS199536-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Score predicted by the deep learning model in function of age and sex. Females are represented in red with dashed line and squares, and males are represented in blue with solid line and triangles. Squares and triangles are the raw data used to fit the mixed model. Lines correspond to estimated means obtain with the mixed model and shaded area to the 95% confidence intervals. Older males are better predicted than younger males while the opposite is found for females. Female predicted scores were transform such that 1 corresponds to the best predictions for both sexes.</p></caption><graphic xlink:href="EMS199536-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Estimate of the GLMM parameters to test the effect of sex on the probability for the deep learning network to correctly classify sex from pictures of the individuals’ head. The model was performed on 4,593 pictures, with individuals with both the exact age and the estimated minimum age.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Fixed effect</th><th align="center" valign="top">Estimate</th><th align="center" valign="top">SE</th><th align="center" valign="top">95% confidence interval</th><th align="center" valign="top">z-value</th><th align="center" valign="top">p-value</th></tr></thead><tbody><tr><td align="left" valign="top">Intercept</td><td align="center" valign="top">2.28</td><td align="center" valign="top">0.15</td><td align="center" valign="top">1.99 ; 2.57</td><td align="center" valign="top">15.27</td><td align="center" valign="top">&lt;0.001</td></tr><tr style="border-bottom:solid thin"><td align="left" valign="top"><bold>Sex</bold> <italic>- Male</italic></td><td align="center" valign="top">-0.19</td><td align="center" valign="top">0.17</td><td align="center" valign="top">-0.53 ; 0.14</td><td align="center" valign="top">-1.12</td><td align="center" valign="top">0.261</td></tr><tr style="border-bottom:solid thin"><td align="left" valign="top"><bold>Random effect</bold></td><td align="center" valign="top"><bold>Variance</bold></td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Individual (N=1,323)</td><td align="center" valign="top">5.591</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Season (N=6)</td><td align="center" valign="top">0.000</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Cross-validation fold (N=10)</td><td align="center" valign="top">0.000</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Residuals</td><td align="center" valign="top">3.290</td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr></tbody></table></table-wrap></floats-group></article>