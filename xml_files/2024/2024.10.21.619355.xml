<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199608</article-id><article-id pub-id-type="doi">10.1101/2024.10.21.619355</article-id><article-id pub-id-type="archive">PPR928823</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Capturing the emergent dynamical structure in biophysical neural models</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Milinkovic</surname><given-names>Borjan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Barnett</surname><given-names>Lionel</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Carter</surname><given-names>Olivia</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Seth</surname><given-names>Anil K.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Andrillon</surname><given-names>Thomas</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib></contrib-group><aff id="A1"><label>1</label>Melbourne School of Psychological Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ej9dk98</institution-id><institution>University of Melbourne</institution></institution-wrap>, <city>Melbourne</city>, <state>VIC</state>, <country country="AU">Australia</country></aff><aff id="A2"><label>2</label>Sussex Centre for Consciousness Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ayhx656</institution-id><institution>University of Sussex</institution></institution-wrap>, <city>Brighton</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01sdtdd95</institution-id><institution>Canadian Institute for Advanced Research</institution></institution-wrap>, Program on Brain, Mind, and Consciousness, <city>Toronto</city>, <country country="CA">Canada</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/050gn5214</institution-id><institution>Paris Brain Institute (ICM)</institution></institution-wrap> / <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>INSERM</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02mh9a093</institution-id><institution>Hôpital de la Pitié-Salpêtriére</institution></institution-wrap>, <city>Paris</city>, <country country="FR">France</country></aff><aff id="A5"><label>5</label>Monash Centre for Consciousness &amp; Contemplative Studies, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02bfwt286</institution-id><institution>Monash University</institution></institution-wrap>, <city>Melbourne</city>, <state>VIC</state><postal-code>3800</postal-code>, <country country="AU">Australia</country></aff><author-notes><corresp id="CR1">
<label>*</label><email>bmilinkovic@student.unimelb.edu.au</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Complex neural systems can display structured emergent dynamics. Capturing this structure remains a significant scientific challenge. Using information theory, we apply <italic>Dynamical Independence</italic> (DI) to uncover the emergent dynamical structure in a minimal 5-node biophysical neural model, shaped by the interplay of two key aspects of brain organisation: integration and segregation. In our study, functional integration within the biophysical neural model is modulated by a global coupling parameter, while functional segregation is influenced by adding dynamical noise, which counteracts global coupling. DI defines a dimensionally-reduced <italic>macroscopic variable</italic> (e.g., a coarse-graining) as emergent to the extent that it behaves as an independent dynamical process, distinct from the micro-level dynamics. We measure dynamical dependence (a departure from dynamical independence) for macroscopic variables across spatial scales. Our results indicate that the degree of emergence of macroscopic variables is relatively minimised at balanced points of integration and segregation and maximised at the extremes. Additionally, our method identifies to which degree the macroscopic dynamics are localised across microlevel nodes, thereby elucidating the emergent dynamical structure through the relationship between microscopic and macroscopic processes. We find that deviation from a balanced point between integration and segregation results in a less localised, more distributed emergent dynamical structure as identified by DI. This finding suggests that a balance of functional integration and segregation is associated with lower levels of emergence (higher dynamical dependence), which may be crucial for sustaining coherent, localised emergent macroscopic dynamical structures. This work also provides a complete computational implementation for the identification of emergent neural dynamics that could be applied both in silico and in vivo.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The co-existence of functional integration and segregation between distinct brain regions has been argued to support perceptual and cognitive states [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R6">6</xref>]. Typically, for a system to be fully integrated, strong interactions between microlevel constituents must be reinforced and drive the dynamics; tending towards complete order through global coupling [<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>]. Conversely, for a system to be fully segregated, local fluctuations must dominate global influences induced by coupling, fracturing functional connectivity and segregating the microlevel constituents [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. Dynamical noise has previously been employed as a way to functionally segregate variables with known anatomical connectivity [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>], and is used in dynamical systems theory as a way of reducing the effect of the coupling parameter in state-variables [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R14">14</xref>]. Yet, at some balanced point(s) between complete functional integration and segregation, dynamics emerge as highly organised patterns of activity [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>]. A pressing issue in computational neuroscience remains the quantification and identification of these emergent dynamics.</p><p id="P3">Neural complexity measures, some of which peak at a balance between functional integration and segregation, have been successfully used as biomarkers of conscious states [<xref ref-type="bibr" rid="R17">17</xref>–<xref ref-type="bibr" rid="R23">23</xref>]. More generally, the co-existence of these two aspects of brain organisation has been associated with <italic>criticality</italic> in complex systems [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R24">24</xref>]. Related to criticality [<xref ref-type="bibr" rid="R25">25</xref>], empirical measures of <italic>integrated information</italic> also peak at a balanced point between functional integration and segregation in brain data [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R29">29</xref>]. This previous work highlights an important and unresolved challenge: to precisely characterise how the interplay between functional integration and segregation mediates the emergent macroscopic <italic>dynamical structure</italic> of the underlying brain network. Simply, to not only quantify when emergent dynamics occur, but also to be able to describe them. We address this challenge by using a minimal 5-node biophysical neural model and leveraging an information-theoretic measure—called <italic>Dynamical Independence</italic> (DI)—of emergence [<xref ref-type="bibr" rid="R30">30</xref>]. Specifically, we set out to (i) characterise how the modulation of functional integration and segregation impacts the emergent dynamics and (ii) identify the underlying macroscopic dynamical structure across spatial scales. We thus provide a complete computational implementation to capture emergent dynamical structures, which could be applied to both in vivo and in silico recordings.</p><p id="P4">In this paper, we examine the <italic>emergent dynamical structure</italic> by focusing on how macroscopic dynamics are distributed across specific microlevel nodes. Specifically, we quantify the extent to which each microlevel node contributes to the overall macroscopic process, thereby highlighting the <italic>localisation</italic> of these dynamics.</p><p id="P5">In general, capturing the organisation of brain activity at a macroscopic level has been approached through various methods, including absolute signal correlations [<xref ref-type="bibr" rid="R31">31</xref>], clustering analysis [<xref ref-type="bibr" rid="R32">32</xref>], small-worldness and graph theory [<xref ref-type="bibr" rid="R33">33</xref>], phase synchronisation [<xref ref-type="bibr" rid="R34">34</xref>], metastability [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>], and more recently, eigenmode decomposition techniques [<xref ref-type="bibr" rid="R37">37</xref>–<xref ref-type="bibr" rid="R39">39</xref>]. However, while these methods often capture functional structures, they do not explicitly reveal <italic>emergent</italic> macroscopic dynamical structure arising from microlevel interactions. Given that the brain’s activity fluctuates between a variable degree of the co-existence of integration and segregation, the information flow between regions may transition from predictable and structured (integrated) to unpredictable and stochastic (segregated) [<xref ref-type="bibr" rid="R40">40</xref>]. Information theory [<xref ref-type="bibr" rid="R41">41</xref>], which quantifies how one node in the network predicts the behaviour of another [<xref ref-type="bibr" rid="R42">42</xref>–<xref ref-type="bibr" rid="R45">45</xref>], emerges as a natural candidate for uncovering macroscopic dynamical structure from microlevel interactions [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>].</p><p id="P6">Recent information-theoretic approaches to emergence show considerable promise in revealing the capacity for higher-order interactions [<xref ref-type="bibr" rid="R48">48</xref>–<xref ref-type="bibr" rid="R51">51</xref>]. Though these approaches align with capturing the capacity for emergence in brain networks, they do not explicitly consider the relation between functional integration and segregation and its effect on the emergent dynamical structure. Further, with the exception of [<xref ref-type="bibr" rid="R51">51</xref>], when applied to larger systems, measures based on decomposing information are approximated from pairwise interactions between microlevel variables alone, and do not capture macroscopic organisation across the full array of possible spatial scales.</p><p id="P7">To understand the relation between emergent dynamical structure and functional integration and segregation in neural systems a metric is needed that simultaneously captures i) coarse-grained macroscopic patterns that represent the <italic>dynamical structure</italic> of the system’s interactions and ii) measures the degree to which the coarse-grainings are to be considered as <italic>emergent</italic> variables with respect to the microlevel constituents.</p><p id="P8">The problem can be framed as an optimisation task where macroscopic (coarse-grained) variables across spatial scales are identified to best describe the underlying stochastic dynamical process. This approach highlights how brain activity can manifest organisation at the macroscopic level, revealing patterns not evident from the microlevel perspective alone. This data-driven approach would be a significant advance over existing complexity-based approaches in neuroscience, offering deeper insights into how emergent structure arises in neural processes—from the ground, up.</p><p id="P9">Consequently, we apply DI to specifically identify emergent macroscopic variables in complex dynamical systems across all spatiotemporal scales. DI captures macroscopic coarse-grainings of the system’s dynamics whereby the self-prediction of the future state of the coarse-grained macroscopic variable is not significantly improved by knowledge of the historical past of the microlevel dynamics. DI therefore identifies macroscopic coarse-grainings of the dynamics <italic>generated by</italic> the interactions between the microlevel constituents but that appear to be independent of them. Because DI can be applied directly to continuous-valued random variables, it is ideally suited for revealing the emergent dynamical structure in neurophysiological data.</p><p id="P10">To apply Dynamical Independence (DI) to biophysical neural models, we start by simulating a brain network model with five nodes. Each node’s local dynamics are defined by the Stefanescu-Jirsa 3D (SJ3D) neural mass model, which simulates the neural population activity of brain regions capable of spike-burst behavior [<xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref>]. This model was specifically chosen given recent studies proposing spike-bursts as a neural mechanism for conscious processing [<xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>].</p><p id="P11">While this model is not a large-scale whole-brain model informed by MRI/DTI data, it is nevertheless a complex biophysical model that incorporates biologically realistic local dynamics, global coupling, dynamical noise, and temporal delays. The term <italic>biophysical neural model</italic> is preferred to emphasise that, while it is not a full representation of whole-brain dynamics, it remains grounded in biological realism by displaying local activity attributed to neural populations [<xref ref-type="bibr" rid="R53">53</xref>], and system-system wide activity governed by three parameters known to influence whole-brain activity; global coupling, dynamical noise, and temporal delay [<xref ref-type="bibr" rid="R56">56</xref>–<xref ref-type="bibr" rid="R58">58</xref>]. This study uses the model as a means to validate the approach in a controlled, albeit simplified, setting, acknowledging that it does not represent the full complexity of whole-brain dynamics but is still informed by the principles that govern such systems.</p><p id="P12">In the brain network model, functional integration is modulated by the global coupling parameter, influencing the strength of connections between regions and affecting local dynamics [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R59">59</xref>]. Conversely, functional segregation is influenced by dynamical noise, which reduces the signal-to-noise ratio, thereby limiting the impact of global coupling on local dynamics [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R56">56</xref>]. Biophysically, this injected noise can be interpreted as representing the influence of unobserved exogenous inputs to the network, which are not explicitly modeled but affect the system’s behaviour.</p><p id="P13">We then minimised the dynamical dependence (DD) of macroscopic variables across varying degrees of functional integration and segregation. Our results show that when the coexistence of integration and segregation are balanced, the DD of macroscopic variables is higher compared to other parameter regime conditions, indicating that macroscopic dynamics are less emergent and more dependent on the underlying micro-level dynamics. This seems counterintuitive because measures of neural and dynamical complexity typically peak when integration and segregation are balanced [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R26">26</xref>]. However, at these balanced points, the emergent dynamical structure—defined by the <italic>localisation</italic> of macroscopic variables—is maximised. In our context, localisation refers to how closely the emergent macrovariables align with specific axes of the microscopic state space—each axis corresponding to a neural source or node. A macrovariable is considered localised when it closely aligns with one or a few of these axes, meaning it primarily captures the dynamics of specific microlevel components rather than being a distributed combination of many nodes. This alignment indicates that specific microlevel nodes contribute predominantly to the macroscopic dynamics. Conversely, deviating from these balanced points leads to a loss of localisation, where the contributions of microlevel nodes to the macroscopic variables become more distributed. These findings suggest that when functional integration and segregation are finely balanced, the macroscopic structure exhibits lower degrees of emergence in terms of dynamical dependence but simultaneously achieves maximal organisation into distinct localised contributions from the micro-level nodes.</p></sec><sec id="S2"><title>Models and methods</title><sec id="S3"><title>Theory</title><sec id="S4"><title>Coarse-graining and macroscopic variables</title><p id="P14">Consider a discrete-time multivariate stochastic process <italic>X</italic> taking values <italic>X</italic><sub><italic>t</italic></sub> in a state space <italic>𝒳</italic>, which we will refer to as the microlevel or microscopic scale of the system, where <italic>t</italic> ∈ ℤ is the discrete time index. (We generally set specific state values in lower case, and random variables in upper case. When referring to a stochastic process as a whole, we write just <italic>X</italic>.) Generally, <italic>𝒳</italic> will have some additional mathematical structure, e.g., topological, metric, differentiable, linear, etc. Later we shall specialise to the case where <italic>𝒳</italic> = ℝ<sup><italic>N</italic></sup>, i.e., <italic>N</italic>-dimensional Euclidean space with the usual metric and linear vector space structure. In that case, we use standard vector-matrix notation and set vector quantities in bold type; so the microscopic system becomes a multivariate vector process <bold><italic>X</italic></bold> defined by <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>X</mml:mi><mml:mi>t</mml:mi><mml:mi>N</mml:mi></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> with superscripts indexing components.</p><p id="P15">Now consider a surjective, structure-preserving mapping <italic>M</italic> : <italic>𝒳</italic> → <italic>𝒴</italic> from the microscopic state-space <italic>𝒳</italic> to a <italic>macroscopic</italic> state space <italic>𝒴</italic>. Surjectivity means that for every <italic>y</italic> ∈ <italic>𝒴</italic>, there exists at least one element <italic>x</italic> ∈ <italic>𝒳</italic> such that <italic>y</italic> = <italic>M</italic> (<italic>x</italic>). We refer to such a mapping as a <italic>coarse-graining</italic>. A coarse-graining effects a partitioning of the microscopic state space as a disjoint union: <italic>𝒳</italic> = ∪<sub><italic>y</italic>∈<italic>𝒴</italic></sub> <italic>M</italic><sup>−1</sup>(<italic>y</italic>). However, distinct coarse-grainings <italic>across scales</italic> do not necessarily partition the dimensions of the microscopic state space into non-overlapping sets. Meaning that the dimensions <italic>X</italic><sup><italic>n</italic></sup> ∈ <italic>𝒳</italic> could belong to multiple coarse-grainings. This also results in the possibility that coarse-grainings can be nested.</p><p id="P16">The macroscopic state space <italic>𝒴</italic> will typically be of smaller cardinality or dimension—which we refer to as the <italic>scale</italic> of the coarse-graining—so coarse-grainings may be considered as dimensional reductions. The coarse-graining <italic>M</italic> naturally maps the microscopic process <italic>X</italic> to the <italic>macroscopic</italic> process (or macroscopic variable) <italic>Y</italic> defined by <italic>Y</italic><sub><italic>t</italic></sub> = <italic>M</italic> (<italic>X</italic><sub><italic>t</italic></sub>); this will in general entail a loss of information. In the Euclidean case <italic>𝒳</italic> = ℝ<sup><italic>N</italic></sup>, <italic>𝒴</italic> = ℝ<sup><italic>n</italic></sup> with 0 <italic>&lt; n &lt; N</italic>, in order to preserve the vector space structure we consider only <italic>linear</italic> coarse-grainings, so that <italic>M</italic> becomes an <italic>n × N</italic> full-rank<sup><xref ref-type="fn" rid="FN1">1</xref></sup> matrix operator, and we write macrovariables as <bold><italic>Y</italic></bold> <sub><italic>t</italic></sub> = <italic>M</italic> <bold><italic>X</italic></bold><sub><italic>t</italic></sub>.</p></sec><sec id="S5"><title>The DI framework</title><p id="P17">DI is a data-driven information-theoretic principle aimed at the identification of emergent coarse-grained macroscopic variables from discrete-valued or continuous-valued time-series data [<xref ref-type="bibr" rid="R30">30</xref>]. Intuitively, a macroscopic variable <italic>Y</italic> given by <italic>Y</italic><sub><italic>t</italic></sub> = <italic>M</italic> (<italic>X</italic><sub><italic>t</italic></sub>) is dynamically independent if—notwithstanding its deterministic dependence on the microscopic process—it behaves like a dynamical process in its own right, following its own dynamical laws, distinct from the laws governing the dynamics of the microlevel variable <italic>X</italic>. Note that in general an arbitrary macroscopic variable will <italic>not</italic> have the property of dynamical independence from the micro-level base.</p><p id="P18">DI is defined in a <italic>predictive</italic> sense: <italic>Y</italic> is dynamically independent of <italic>X</italic> if knowledge of the history of <italic>X</italic> does not enhance prediction of <italic>Y</italic> beyond the extent to which <italic>Y</italic> already self-predicts. Discovering the macroscopic variables, <italic>Y</italic>, may be framed as an optimisation problem; namely to minimise,across all coarse-grainings, the objective function of <italic>dynamical dependence</italic> (DD), an information-theoretic measure of <italic>departure</italic> from dynamical independence for macroscopic variables:</p><p id="P19"><bold>Definition 1. Dynamical Dependence</bold> is defined as the <italic>transfer entropy</italic> [<xref ref-type="bibr" rid="R60">60</xref>, <xref ref-type="bibr" rid="R61">61</xref>]—a <italic>nonparametric measure of information flow—from the historical past of the microscopic process X</italic> to the present state of the macroscopic process, <italic>Y</italic> at time <italic>t</italic>:<sup><xref ref-type="fn" rid="FN2">2</xref></sup> <disp-formula id="FD1"><label>(1a)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2"><label>(1b)</label><mml:math id="M3"><mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>unpredictability</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>of</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>Y</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>given</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>its</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>own</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>history</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:munder><mml:mo>−</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">︸</mml:mo></mml:munder><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mtext>unpredictability</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>of</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>Y</mml:mtext></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mtext>given</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>its</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>own</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>X's</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>history</mml:mtext></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:munder></mml:mrow></mml:math></disp-formula></p><p id="P20">Here, we impose a supervenience relation, which asserts that: <disp-formula id="FD3"><label>(2)</label><mml:math id="M4"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p id="P21">This indicates that there is no additional information in the macroscopic process beyond what is already captured in the history of the microscopic process.</p><p id="P22">Consequently, within the DI framework, there is no room for additional information on the macroscopic process in the DI framework. Note that a coarse-grained macroscopic variable <italic>Y</italic><sub><italic>t</italic></sub> = <italic>M</italic> (<italic>X</italic><sub><italic>t</italic></sub>) trivially satisfies (<xref ref-type="disp-formula" rid="FD3">2</xref>).</p><p id="P23">If the process <italic>X</italic> (and hence also <italic>Y</italic>) is <italic>stationary</italic> —i.e., its statistical structure does not change over time—then the DD is not time-dependent, and we drop the subscript <italic>t</italic>. We assume stationarity for all processes from here on. In practice, to compute dynamical dependence from neurophysiological data such as that simulated by biologically-plausible brain network models (see next Section), we employ an approximation method based on linear state-space modelling (see <xref ref-type="supplementary-material" rid="SD1">Section S2 Appendix: Linear state-space modelling</xref> for details).</p><p id="P24">Thus a macrovariable is dynamically independent if and only if (iff) its dynamical dependence on the microlevel vanishes identically; i.e., its capacity to predict its future based only on its own history is not enhanced by knowledge of the history of the microscopic process. We define this with a transfer-entropic identity:</p><p id="P25"><bold>Definition 2</bold>. A macrovariable <italic>Y</italic> given by the coarse-graining <italic>Y</italic><sub><italic>t</italic></sub> = <italic>M</italic> (<italic>X</italic><sub><italic>t</italic></sub>) is <bold>dynamically independent</bold> of the microscopic process <italic>X</italic> iff <disp-formula id="FD4"><label>(3)</label><mml:math id="M5"><mml:mrow><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>→</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p id="P26">This formalises the intuition of an emergent macrovariable as one which behaves as a dynamical process in its own right, independently of the micro-level dynamics. In the language of Bertschinger <italic>et al</italic>. [<xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>], <italic>the macroscopic variable is informationally (or dynamically) closed</italic> with respect to the microscopic dynamics. Dynamical independence encapsulates the specific notion of emergence discussed in this article, and dynamical dependence stands as a quantitative information-theoretic measure of the degree of (non-)emergence.</p><p id="P27">In general, given a microscopic process <italic>X</italic>, there may well be <italic>no</italic> perfectly dynamically-independent macroscopic variables (i.e., variables for which <xref ref-type="disp-formula" rid="FD4">Eq 3</xref> holds identically) at some, or indeed at any, spatial scale. Even if there were, in an empirical scenario with finite data, due to the statistical nature of DI, it is in principle impossible to establish that a macroscopic variable at some given scale is in fact perfectly dynamically-independent. Instead, we search for macrovariables with <italic>minimal</italic> DD—i.e., macroscopic variables with a significant degree of emergence. In practice, at any given scale, we attempt to identify macroscopic variables that minimise the DD over the space of all possible structure-preserving coarse-grainings at that scale. Importantly, in an empirical scenario, <italic>minimal</italic> DD may be quantified in a principled manner: namely, that at some predefined significance level, and accounting appropriately for multiple hypotheses, we cannot reject the null hypothesis that an optimised DD value is zero.</p><p id="P28">For non-trivial problems like the biophysical neural model considered in this study, optimisation is generally analytically intractable, so we are bound to deploy numerical methods. DD minimisation—which will generally take the form of multiple optimising runs with random initial configurations—will, furthermore, be limited by time and computational resources, so will not be sure to guarantee that globally minimal-DD macrovariables are identified. Indeed, it turns out that numerical optimisation runs are in practice likely to terminate at local suboptima of the DD objective function. We therefore adopt the following pragmatic criterion for the discovery of emergent macroscopic variables:</p><p id="P29"><bold>Definition 3. Emergent</bold> <italic>n</italic><bold>-macros</bold> <italic>are macroscopic variables with minimal dynamical dependence over all optimisation runs at scale n</italic>.</p><p id="P30">We consider the emergent <italic>n</italic>-macros across all candidate <italic>spatial</italic> scales and we define the <italic>localisation perspective</italic> of emergent dynamical structure as<sup><xref ref-type="fn" rid="FN3">3</xref></sup>.</p><p id="P31"><bold>Definition 4. Emergent dynamical structure</bold> <italic>is defined by the set of all emergent n-macros which together reveal the emergent dynamical structure across spatial scales of the whole system</italic>.</p><p id="P32">This dynamical structure is revealed by identifying the degree to which the microlevel node contribution to emergent <italic>n</italic>-macros is distributed. This definition allows us to derive the <italic>localisation</italic> of <italic>n</italic>-macros in the original biophysical neural model.</p><p id="P33">In the context of our study, emergent macrovariables are defined as linear subspaces of the microscopic state space, where each dimension corresponds to a neural source or node in the network. The term localisation refers to the degree to which these macrovariables are aligned with specific axes of the microscopic space. Specifically, a macrovariable is considered <italic>localised</italic> when it aligns closely with one or a few of the original coordinate axes—meaning it has smaller subspace angles with these axes. This alignment indicates that the macrovariable primarily captures the dynamics of specific microlevel components, rather than being a distributed combination of many nodes. This concept of localisation is crucial for understanding how emergent dynamical structure forms within the network.</p><p id="P34">DI analysis applied to neurophysiological time-series will not necessarily reveal distinct emergent dynamical structure at the scale specified by the localisation of <italic>n</italic>-macros. Indeed, the present study develops a method able to characterise how functional integration and segregation relate to emergent dynamical structure across all spatial scales.</p></sec><sec id="S6"><title>Dynamical dependence minimisation in linear systems</title><p id="P35">We operationalise DI and the identification of emergent <italic>n</italic>-macros in simulated neurophysiological data using a linear approximation (see <xref ref-type="supplementary-material" rid="SD1">S1 Appendix: Granger causality, and S2 Appendix: Linear state-space modelling</xref> as to the appropriateness of the linear approximation). The microscopic state space is thus the Euclidean space <italic>𝒳</italic> = ℝ<sup><italic>N</italic></sup>, and coarse-grainings at spatial scale <italic>n</italic> are full-rank linear mappings (<italic>n × N</italic> matrices) <italic>M</italic> : ℝ<sup><italic>N</italic></sup> → ℝ<sup><italic>n</italic></sup>.</p><p id="P36">For the coarse-grained macrovariable <bold><italic>Y</italic></bold> <sub><italic>t</italic></sub> = <italic>M</italic> <bold><italic>X</italic></bold><sub><italic>t</italic></sub>, the dynamical dependence (<xref ref-type="disp-formula" rid="FD1">Eq 1a</xref>) is invariant under nonsingular (invertible) linear transformations of the macroscopic state space <italic>𝒴</italic> = ℝ<sup><italic>n</italic></sup>. Thus we may consider two coarse-grainings <italic>M, M′</italic> to specify the <italic>same macrovariable</italic> if they are related by <italic>M′</italic> = Φ<italic>M</italic> for some nonsingular linear transformation Φ : ℝ<sup><italic>n</italic></sup> → ℝ<sup><italic>n</italic></sup>. The space of all possible linear coarse-grainings at scale <italic>n &lt; N</italic> —the space over which DD will be minimised—may consequently be identified with the set of <italic>n</italic>-dimensional subspaces of ℝ<sup><italic>N</italic></sup> . This defines a mathematical object known as a <italic>Grassmannian manifold</italic> [<xref ref-type="bibr" rid="R64">64</xref>, <xref ref-type="bibr" rid="R65">65</xref>], <italic>written 𝒢</italic><sub><italic>N</italic></sub> (<italic>n</italic>). A coarse-graining <italic>M</italic> may be visualised as an <italic>n</italic>-dimensional subspace in the original microscopic state space ℝ<sup><italic>N</italic></sup>, on which the dynamics of the macroscopic variable <bold><italic>Y</italic></bold> <sub><italic>t</italic></sub> = <italic>M</italic> <bold><italic>X</italic></bold><sub><italic>t</italic></sub> play out. Intuitively, the coarse-graining map <italic>M</italic> projects the <italic>N</italic>-dimensional microscopic dynamics <bold><italic>X</italic></bold> down onto the macrovariable <bold><italic>Y</italic></bold>, which resides on the associated <italic>n</italic>-dimensional subspace [<xref ref-type="bibr" rid="R30">30</xref>].</p><p id="P37">Grassmannian manifolds are a type of homogeneous Riemannian manifold; they are non-Euclidean spaces with a distinctive metric geometry and symmetries, on which we can do calculus. Given a microscopic process <bold><italic>X</italic></bold><sub><italic>t</italic></sub>, then, we can minimise the DD <italic>T</italic> (<bold><italic>X</italic></bold> → <italic>M</italic> <bold><italic>X</italic></bold>)—considered now as the objective function on <italic>𝒢</italic><sub><italic>N</italic></sub> (<italic>n</italic>) parametrised by the matrix <italic>M</italic><sup><xref ref-type="fn" rid="FN4">4</xref></sup>—using standard methods like gradient descent [<xref ref-type="bibr" rid="R64">64</xref>, <xref ref-type="bibr" rid="R67">67</xref>] on the Grassmannian.</p><p id="P38">For the class of linear state-space models for the microscopic dynamics, moreover, both the DD and its gradient may be calculated explicitly; see <xref ref-type="supplementary-material" rid="SD1">S3 Appendix: Minimisation of dynamical dependence by gradient descent</xref>. In this study, we used gradient descent with an adaptive step-size [<xref ref-type="bibr" rid="R68">68</xref>] to minimise DD. Typically, minimisation is initiated at a uniform random <italic>M</italic> for a specific scale <italic>n</italic>, and allowed to run until it converges to a specified tolerance. However, as the optimisation landscape described by the DD is quite deceptive, with multiple local suboptima (minima), we repeat randomised optimisation runs many times to obtain acceptable minima for the specified scale.</p></sec><sec id="S7"><title>Characterisation the composition of linear macroscopic variables</title><p id="P39">A putatively emergent macroscopic variable is perhaps best thought of as a dimensionally-reduced subsystem of the global microscopic dynamics<sup><xref ref-type="fn" rid="FN5">5</xref></sup>. As explained above, in the linear case a macrovariable may be considered a projection of the microscopic process onto a subspace in the original microscopic Euclidean state space. In the context of modelling multi-region neurophysiological time-series data, we may get a sense of the extent to which different nodes (corresponding to neural signals in specific anatomical brain regions) in the brain network participate in, or contribute to, a macroscopic subsystem that we term an <italic>n</italic>-macro. We do this by invoking geometric intuition: consider, for example, a 2-dimensional plane in a 3-dimensional Euclidean space. The plane is uniquely identified if we know the <italic>angles</italic> between it and each of the three <italic>x, y, z</italic> coordinate axes in the 3-dimensional space (<xref ref-type="supplementary-material" rid="SD1">Fig 8(d)</xref>).</p><p id="P40">In the multi-channel recording scenario, microscopic coordinate axes correspond to recording channels (<xref ref-type="supplementary-material" rid="SD1">Fig 8(a-b)</xref>). Consider again the 3D example: if, say, the angle between axis <italic>y</italic> (corresponding to channel <italic>y</italic>) and the 2D plane associated with a given macroscopic variable at scale <italic>n</italic> = 2 is close to the maximum <italic>π</italic>/2, this tells us that neural activity in region <italic>y</italic> is <italic>projected away</italic> by the corresponding coarse-graining; region <italic>y</italic> does not participate strongly in the macroscopic process. As the angle approaches zero, participation is maximised.</p><p id="P41">With some caveats<sup><xref ref-type="fn" rid="FN6">6</xref></sup>, this generalises to arbitrary dimensions: to quantify participation of brain regions in a macroscopic subsystem we calculate the angles between each of the region axes and the macroscopic subspace; then <italic>small</italic> angles correspond to <italic>high</italic> contribution, and vice versa (see <xref ref-type="supplementary-material" rid="SD1">S5 Appendix: Principal angles &amp; single-node contribution to macroscopic dynamics</xref> for details).</p><p id="P42">Subspace angles have another important use, as a metric to measure the similarity, or co-planarity between macrovariables. For example, when optimising dynamical dependence for (nearly-)DI <italic>n</italic>-macros, two gradient-descent runs may yield coarse-grainings <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub>, respectively, which we suspect may be identical, or nearly so. Measuring the angle between <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> can help us decide how similar they are: angles close to zero indicate high similarity. In the case where we have subspaces <italic>M</italic><sub>1</sub> and <italic>M</italic><sub>2</sub> of <italic>different</italic> dimensions <italic>n</italic><sub>1</sub> <italic>&lt; n</italic><sub>2</sub>, a subspace angle near zero indicates that <italic>M</italic><sub>1</sub> is nested in <italic>M</italic><sub>2</sub>; that is, the <italic>M</italic><sub>1</sub> dynamics may be viewed as a self-contained subsystem of the <italic>M</italic><sub>2</sub> dynamics. Within the current study we only consider subspace angles from microlevel constituents to <italic>n</italic>-macros and leave the comparison across higher-order scales for future research.</p></sec></sec><sec id="S8"><title>Brain network model</title><p id="P43">Using The Virtual Brain (TVB) [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R69">69</xref>], we simulate biophysical neural models by constructing a 5-node network. Each node’s local dynamics are governed by the Stefanescu-Jirsa 3D (SJ3D) neural mass model (NMM), a reduced model capturing the mean field activity of 150 excitatory and 50 inhibitory Hindmarsh-Rose neurons [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref>]. Spike-burst neurons are thought to be implicated as critical neural mechanisms underlying conscious processing [<xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>]. The SJ3D model, governed by six coupled differential equations (considered state variables), is detailed in <xref ref-type="supplementary-material" rid="SD1">S4 Appendix: Global brain network dynamics</xref>. The model parameters used have been optimised to fit resting-state EEG data, following [<xref ref-type="bibr" rid="R69">69</xref>].</p><p id="P44">We choose our microscopic level as represented by the SJ3D NMMs, and the macroscopic scales defined across higher-order spatial scales <italic>n</italic> = 2 and <italic>n</italic> = 3.</p><sec id="S9"><title>Local dynamics</title><p id="P45">The SJ3D neural mass model consists of an excitatory and an inhibitory neural mass, which are interconnected through the fast excitatory variable <italic>ξ</italic> and the fast inhibitory variable <italic>α</italic> (detailed in <xref ref-type="supplementary-material" rid="SD1">Table 2</xref>). These variables, along with the connectivity between the excitatory masses, define the dynamics at each node. The SJ3D model can generate various ensemble dynamics, including excitable regimes, oscillations, and transient spike-bursts [<xref ref-type="bibr" rid="R53">53</xref>]. The illustration below shows the construction of the NMM.</p><p id="P46">A key advantage of the SJ3D worth noting is its multi-modal construction. Modes represent different dynamical regimes in which the local dynamics can exhibit and represent distinct population dynamics which give rise to rich heterogeneous activity exhibited locally [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R69">69</xref>, <xref ref-type="bibr" rid="R70">70</xref>].</p></sec><sec id="S10"><title>Global dynamics</title><p id="P47">Our simulations are based on a well-established evolution equation governing the dynamics of brain network models, adapted and modified from The Virtual Brain (TVB) for this study (details in Eq 27 in <xref ref-type="supplementary-material" rid="SD1">S4 Appendix: Global brain network dynamics</xref>). The structural connectivity, which defines the anatomical backbone of the network model, is represented by a weight matrix and a tract-length matrix. In the context of a whole-brain model (WBM), the weights matrix quantifies the strength of pairwise anatomical connections between brain regions, while the tract-length matrix measures the axonal fibre lengths between these regions.</p><p id="P48">The 5-node biophysical neural model used here is derived from an empirically-informed TVB structural connectome, which incorporates a biologically realistic tract-length matrix. This matrix is created through homotopical morphing, a computational technique that optimizes and aligns primate tracer imaging data with the human anatomical connectome, based on a Desikan-Killiany parcellation atlas [<xref ref-type="bibr" rid="R71">71</xref>]. To evaluate the plausibility of the emergent dynamical structure, constrained by network connectivity, we also perform the same analysis on an uncoupled structural connectome as a control condition. Throughout the experiments, the network connectivity was kept constant across both the coupled and uncoupled regimes, serving as a ground-truth model that constrains the dynamics revealed by varying degrees of functional integration and segregation (see <xref ref-type="fig" rid="F2">Fig 2</xref>).</p><p id="P49">To reiterate, two global parameters known to influence macroscopic brain dynamics [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R57">57</xref>, <xref ref-type="bibr" rid="R72">72</xref>] were manipulated: global coupling and dynamical noise. Functional integration is defined by the global coupling factor (<italic>G</italic> in Eq 27), which scales the influence of incoming activity from other nodes in the network. Conversely, dynamical noise, represented by the <italic>v</italic><sub><italic>i</italic></sub>(<italic>t</italic>) term in Eq 27, reflects functional segregation by reducing the signal-to-noise ratio, thereby dampening the impact of external nodes on local dynamics.</p></sec></sec><sec id="S11"><title>Simulation protocol</title><p id="P50">A parameter sweep was performed on the values of global coupling (<italic>G</italic>) and dynamical noise (<italic>η</italic>) between 0.01 to 0.31 and 0.001 to 0.1 in 20 logarithmic steps, respectively (see <xref ref-type="supplementary-material" rid="SD1">Table 2 in S3 Appendix: Minimisation of dynamical dependence by gradient descent</xref>. To obtain the time-series data, we consider the <italic>ξ</italic> state variable at each node, representing the excitatory activity. The activity for each <italic>ξ</italic> was summed over the 3 dynamic modes [<xref ref-type="bibr" rid="R70">70</xref>] and then z-scored. The resulting time series represents the LFP-like excitatory activity of the respective brain region nodes [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R53">53</xref>, <xref ref-type="bibr" rid="R73">73</xref>]. For numerical stability a Heun stochastic integration scheme was used with a step size of <italic>dt</italic> = 2<sup>−6</sup>. This parameter can vary depending on the NMM used and the global parameter regime explored. For the present analysis it was identified that <italic>dt</italic> = 2<sup>−6</sup> was consistently stable throughout the parameter sweep. Simulations were run for 5000 ms with the first 500 ms excluded to discount initial transients. Simulations were sampled at 256Hz to retain consistency with regularly deployed neurophysiological data acquisition methods such as electroencephalography (EEG) and intracranial-EEG (iEEG).</p><p id="P51">The resulting simulated neural time series for each of the 400 (20 x 20) simulations is subject to DI analysis using a linear approximations to estimate causal graphs and capture the emergent dynamical structure (see <xref ref-type="supplementary-material" rid="SD1">S1 Appendix: Granger causality</xref> for details).</p></sec></sec><sec id="S12" sec-type="results"><title>Results</title><p id="P52">To validate the capacity of our methodology to identify emergent macroscopic dynamics, we applied our DI analysis pipeline across 400 simulations with varying degrees of functional integration and segregation, using the network models defined in <xref ref-type="fig" rid="F2">Fig 2</xref>. As mentioned, we leverage an uncoupled network model as a control condition, which is defined by the connectivity illustrated in <xref ref-type="fig" rid="F2">Fig 2</xref>. For each simulation, we ran optimisations for a 2-macro and 3-macro in our 5-node biophysical network model. We selected a 5-node brain network to offer the simplest model in which we can vary the values of functional integration and segregation, while keeping the analysis and results as transparent as possible.</p><p id="P53"><xref ref-type="fig" rid="F3">Fig 3</xref> illustrates the minimal DD values of each emergent <italic>n</italic>-macro across simulations defined by varying global coupling and dynamical noise parameter values. Each matrix represents the entire bivariate parameter space, with dynamical noise varying along the <italic>x</italic>-axis and global coupling along the <italic>y</italic>-axis. The top row displays the DD values for the 2-macro and 3-macro in the coupled network, while the bottom row shows the corresponding results for the uncoupled network.</p><sec id="S13"><title>Dynamical dependence peaks in a parameter regime balancing functional integration and segregation</title><p id="P54"><xref ref-type="fig" rid="F3">Fig 3</xref> illustrates that in the coupled network, there exists a parameter regime where the DD of emergent 2-macros and 3-macros is <italic>higher</italic>, indicating <italic>lower</italic> emergence of macroscopic dynamics—a pattern absent in the uncoupled network. Notably, this regime exhibits an consistent relationship where increases in both functional integration (global coupling) and functional segregation (dynamical noise) coincide. This balance results in the emergent dynamical structure being <italic>maximised</italic> through the localisation of contributions from specific microlevel nodes to the emergent <italic>n</italic>-macros, as dynamical noise increases proportionally to global coupling (see <xref ref-type="fig" rid="F4">Fig 4</xref> and <xref ref-type="fig" rid="F5">5</xref>).</p><p id="P55">This pattern suggests that the observed regularity across simulations is not merely a result of individual parameter values but rather emerges from the <italic>interaction</italic> between global coupling and dynamical noise. Their combined influence shapes the system’s dynamical structure, leading to higher DD (lower emergence) and maximised localisation of contributions at this balanced parameter regime. This finding underscores the importance of considering the interplay between functional integration and segregation when assessing both the emergence and the organisational structure of complex macroscopic dynamics.</p><sec id="S14"><title>Identifying structure: Spatial localisation of single-node contributions in emergence <italic>n</italic>-macros across functional integration and segregation</title><p id="P56">We next performed a single-node analysis to examine the contributions of individual nodes to the emergent 2-macros across simulations. As shown in <xref ref-type="fig" rid="F4">Fig 4</xref> nodes 4 and 5 exhibit high contributions to the emergent 2-macro, while nodes 1, 2, and 3 show negligible contributions, particularly at the parameter regime where functional integration and segregation are balanced. This distinct organisation of node contributions towards an emergent 2-macro which is localised across 2 microlevel nodes is closely associated with the parameter regime characterised by higher DD (lower emergence) values (<xref ref-type="fig" rid="F3">Fig 3</xref>). This indicates that at the balance point, the emergent dynamical structure is maximised through the localisation of contributions from specific nodes.</p><p id="P57">The degree of single-node contribution is represented as follows: a value of 0 (lighter colour) indicates that the node is not implicated in the emergent 2-macro, and a value of 1 (darker colour) indicates that the node is fully implicated. For a detailed explanation see <xref ref-type="supplementary-material" rid="SD1">S6 Appendix: Worked example</xref>.</p><p id="P58">Further analysis, as illustrated in <xref ref-type="fig" rid="F4">Fig 4</xref>, reveals that in simulations where the parameter regime is dominated by either excessive functional integration or segregation, all nodes show varied degrees of contribution to the emergent 2-macro. This variability indicates a breakdown in the localisation of single-node contributions, which underpins the dynamical structure of the emergent <italic>n</italic>-macros. Notably the dynamical structure of the <italic>n</italic>-macros appears <italic>randomly</italic> distributive across the entire network.</p><p id="P59">By examining single-node contributions across all parameter regimes, we assess the degree of localisation within each <italic>n</italic>-macro, providing insight into the integrity of the emergent dynamical structure. Importantly, distinct localisation is most apparent in parameter regimes associated with higher dynamical dependence (lower emergence)—that is, at a balanced point of the co-existence of functional integration and segregation. In contrast, in regimes with extreme dynamical noise or global coupling, this localisation diminishes, leading to a more distributed contribution of nodes across the entire network and a subsequent weakening of the dynamical structure.</p><p id="P60">Similarly, <xref ref-type="fig" rid="F5">Fig 5</xref> shows that single-node contributions to an emergent 3-macro exhibit a distinct localised pattern at balanced points between functional integration and segregation. In this regime, nodes 1, 2, and 3 contribute significantly to the emergent 3-macro, while nodes 4 and 5 show minimal to no contribution, as reflected by the lighter colours in their respective matrices. However, when the parameter regime shifts towards extreme functional integration or segregation, this localisation of contributions weakens. The result is a more distributed contribution across all nodes, leading to a diminished and less coherent dynamical structure.</p><p id="P61">Finally, given that the uncoupled network serves as a control, we should expect the minimal DD-valued <italic>n</italic>-macros in such a system to reflect subspaces that span some combination of the original coordinate axes. That is, they are localised or distributed across some set of nodes. In fact, without the need for simulation, we can theoretically predict that for a macro dimension <italic>n</italic>, any subspace formed by linear combinations of <italic>n</italic> of the <italic>N</italic> original coordinate axes (microscopic state space) should exhibit close-to-zero DD values. Indeed, this expectation is supported by the results obtained in the lower graphs in <xref ref-type="fig" rid="F3">Fig 3</xref>, where the 2- and 3-dimensional emergent macros identified by the optimisation process in the uncoupled system correspond to these close-to-zero DD subspaces.</p><p id="P62">Further, <xref ref-type="fig" rid="F6">Fig 6</xref> illustrates that in the uncoupled network, the contributions of microscopic nodes to the emergent 3-macros<sup><xref ref-type="fn" rid="FN7">7</xref></sup> vary randomly across the entire parameter space, resulting in the absence of distinct localisation of the dynamical structure of emergent macros, even when functional integration and segregation are balanced. These findings suggest that the anatomical connectivity between nodes in the brain network is crucial in determining the localisation of dynamics and the contribution of nodes to emergent dynamical structure at the macroscopic scale. In the absence of coupling, the emergent dynamical structure is <italic>not maximised</italic>, and the DD values are close to zero, indicating higher emergence (lower DD) but without the distinct localisation that characterises the maximised emergent dynamical structure in the coupled network.</p></sec><sec id="S15"><title>Statistical significance of single-node contributions to emergent <italic>n</italic>-macros</title><p id="P63">Thus far, we have explored how and when macroscopic dynamical structure emerges through the localisation of single-node contributions to emergent <italic>n</italic>-macros. To assess whether these single-node contributions to emergent 2-macros and 3-macros across the parameter space are statistically significant compared to the uncoupled brain network (control condition), we performed a Wilcoxon rank-sum test. Specifically, this analysis compares the distribution of node weights across all simulations for the coupled and uncoupled regimes (<xref ref-type="fig" rid="F4">Fig 4</xref>, <xref ref-type="fig" rid="F5">5</xref>, and <xref ref-type="fig" rid="F6">6</xref>) . The statistical comparison is conducted across the entire parameter space explored, not just within the parameter regime where we qualitatively observe distinct localisation of the emergent macroscopic dynamical structure. By performing the statistical comparison across the full parameter space, we ensure a robust and comprehensive assessment of the significance of single-node contributions to emergent macroscopic dynamical structures, thereby avoiding potential biases that could arise from selectively focusing on regions where qualitative observations suggest distinct localisation.</p><p id="P64">Consulting <xref ref-type="fig" rid="F7">Fig 7</xref>, the Wilcoxon rank-sum test reveals a significant difference in the contribution of node 4 to an emergent 2-macro in the coupled brain network compared to the uncoupled brain network used as a control condition (<italic>Z</italic> = 2.08, <italic>p &lt;</italic> 0.05). No significant contribution was observed from any other nodes. Interestingly, despite the qualitative illustration in <xref ref-type="fig" rid="F4">Fig 4</xref> showing distinctly higher contributions from node 5 to the emergent 2-macro, this node did not exhibit statistical significance when compared to the control <xref ref-type="fig" rid="F6">Fig 6</xref>. This discrepancy might be due to the statistical analysis being conducted across the entire parameter space, rather than being confined to the regime where distinct localisation is observed. However, it is curious that node 4 still shows significance under the same conditions, suggesting that the lack of significance for node 5 may not be fully explained by this alone.</p><p id="P65">Similarly, a Wilcoxon rank-sum test reveals a significant difference in the contributions of nodes 1 (<italic>Z</italic> = 7.69, <italic>p &lt;</italic> 0.0001), 2 (<italic>Z</italic> = 4.13, <italic>p &lt;</italic> 0.0001), and 3 (<italic>Z</italic> = 4.98, <italic>p &lt;</italic> 0.0001) to an emergent 3-macro in the coupled network compared to the uncoupled network. In contrast, nodes 4 and 5 do not show significant contributions.</p></sec></sec></sec><sec id="S16" sec-type="discussion"><title>Discussion</title><p id="P66">Overall, our results reveal that when the co-existence functional integration and segregation are finely balanced, the dynamical dependence (DD) of macroscopic variables is <italic>higher</italic> compared to other parameter regimes. This indicates that macroscopic dynamics are less emergent and more dependent on the underlying micro-level dynamics at these points. However, the emergent dynamical structure—defined by the localisation of contributions to macroscopic variables—is <italic>maximised</italic> under these conditions, with specific micro-level nodes distinctly contributing to the macroscopic dynamics. Conversely, deviating from these balanced points leads to a <italic>lower</italic> DD (indicating more emergent macroscopic dynamics) but results in a loss of localisation, where the contributions of micro-level nodes become more distributed.</p><p id="P67">We developed and outlined a complete computational method for identifying emergent dynamical structure in neural models. By modulating global coupling—pushing the system toward functional integration—and dynamical noise—pushing the system toward functional segregation—we examined how their balanced coexistence influences both the dynamical dependence and the localisation of contributions to macroscopic variables in a biophysical neural model. This approach allowed us to uncover the nuanced relationship between minimised emergence (in terms of higher DD) and maximised emergent dynamical structure (in terms of localisation) at balanced integration and segregation.</p><p id="P68">First, in a worked example, we illustrated that even in simple toy systems, achieving absolute dynamical independence is nearly impossible, as predicted by theoretical claims [<xref ref-type="bibr" rid="R30">30</xref>], and that the weighting of each node’s contribution rarely equals zero It is important to note, however, that these node contributions primarily reflect the localisation of the <italic>n</italic>-macros within the original system’s state space, rather than directly indicating their emergent characteristics on the higher-order scales themselves. For instance, in a more general scenario where the network connectivity is arbitrarily rotated in a high-dimensional space, the emergent <italic>n</italic>-macros would remain unchanged (due to the transformation-invariance of the DI framework), but the node contributions could appear more dispersed and potentially random. This highlights the distinction between node contributions and the emergent properties on the higher-order scales. While this limitation means that node contributions alone may not fully capture the <italic>higher-order</italic> structure, they remain valuable for revealing how macroscopic dynamical are spatially localised within the network: an aspect of the <italic>emergent dynamical structure</italic> of the neural model. This localisation insight is crucial for understanding the aggregation of local interactions into global dynamics, complementing other methods that may better capture higher-order interactions. As mentioned a potential utility in capturing the structure of the higher-order interactions can be through similarity matrices (see <xref ref-type="supplementary-material" rid="SD1">S6 Appendix: Worked example</xref>, for a worked example on a simple system). By integrating single-node analysis with other methods, we can achieve a more comprehensive understanding of the emergent dynamical structure in empirical data.</p><p id="P69">Next we constructed neural models that were intentionally designed with modular structures to assess how functional integration and segregation effect the emergent dynamical structure even with ground truth modularity. While this setup aids in illustrating the concept, it may not fully represent real-world neural data where <italic>n</italic>-macros do not necessarily correspond to such straightforward structure via analysis of their spatial localisation.</p><p id="P70">This is important, because in our approach coarse-graining within the DI framework is not formalised as a partitioning function that simplifies a complex high-dimensional network into a more manageable structure, as seen in other formal approaches to emergence [<xref ref-type="bibr" rid="R74">74</xref>]. Instead, it serves as an information-theoretic dimensionality reduction technique that captures lower-dimensional descriptions of whole-system dynamics across spatial scales. Because it does not rely on strict partitioning, this approach is well-suited to address the graded nature of contributions from individual nodes within the network. By avoiding the rigid boundaries imposed by partitioning, our method allows for a nuanced analysis of how these contributions vary, enabling us to assess whether the dynamical structure is more localised—where a few nodes dominate—or more distributive—where contributions are spread across many nodes—as the parameter values change. This flexibility is crucial in capturing the complex interplay between functional integration and segregation, and in understanding how local interactions aggregate into global dynamics. Importantly, this method also differs from other dimensionality reduction techniques, such as PCA, t-SNE, and UMAP [<xref ref-type="bibr" rid="R75">75</xref>], as it focuses on capturing lower-dimensional descriptions of dynamics derived from the interactions of microlevel constituents, rather than merely accounting for variance or relying on algorithmic clustering. Following this, we now discuss some key results.</p><sec id="S17"><title>Localisation of emergent <italic>n</italic>-macros: defining the emergent dynamical structure</title><p id="P71">Our results demonstrate that when the co-existence of functional integration and segregation are finely balanced, the emergent macroscopic dynamical structure is <italic>maximised</italic> through the localisation of single-node contributions to the emergent <italic>n</italic>-macros. Specifically, at these balanced points, the whole-system dynamics exhibit distinct patterns of single-node contributions to the emergent <italic>n</italic>-macros, rather than random or evenly distributed patterns. This maximised localisation defines the emergent dynamical structure at the balanced points of integration and segregation.</p><p id="P72">This distinct structure is evident from <xref ref-type="fig" rid="F4">Fig 4</xref> and <xref ref-type="fig" rid="F5">Fig 5</xref>, where nodes 1, 2, and 3 show negligible contributions to the emergent 2-macro, while nodes 4 and 5 contribute significantly to the emergent 2-macro at the balanced points. Conversely, for the emergent 3-macro, nodes 4 and 5 show negligible contributions, while nodes 1, 2, and 3 contribute significantly. Statistically, we do not have a definitive explanation for why node 5 does not show significant contributions in certain cases while node 4 does (see <xref ref-type="fig" rid="F7">Fig 7</xref>). We suspect that there might be a hidden bias in the optimisation procedure used for the DI analysis, particularly in the case of the uncoupled network. In theory, the optimisation should identify 2-macros that are combinations of pairs of coordinate axes uniformly across all pairs. However, it is possible that the procedure is biased towards certain pairs more than others. This issue requires further investigation into the optimisation procedure across the coupled and uncoupled networks to fully understand the underlying reasons.Moreover, when moving away from the balanced points—either by increasing functional integration or segregation—the lack of distinct, localised single-node contributions to emergent <italic>n</italic>-macros leads to varied, distributed contributions from all micro-level nodes. This distributed nature induces a loss of distinct dynamical structure over the micro-level nodes that is otherwise observed at the balanced points. Interestingly, this loss of distinct structure is accompanied by relatively <italic>lower</italic> values of dynamical dependence (DD) for emergent <italic>n</italic>-macros, suggesting <italic>higher</italic> emergence of macroscopic dynamics.</p><p id="P73">While it might initially seem counterintuitive that a distinct, localised emergent dynamical structure occurs alongside <italic>lower emergence</italic> of macroscopic dynamics (as indicated by higher DD values), this observation remains consistent with the DI framework. At the balanced points, the DD is higher, indicating that the macroscopic dynamics are less emergent and more dependent on the underlying micro-level dynamics. The distinct structure does not necessarily imply that the emergent <italic>n</italic>-macros should possess a greater degree of dynamical (informational) closure. Crucially, the dynamical closure of the emergent <italic>n</italic>-macros is determined in relation to the microscopic processes alone, and not in comparison to other <italic>n</italic>-macros discovered across spatial scales.</p><p id="P74">Furthermore, lower dynamical dependence (higher emergence) does not necessarily mean that the emergent <italic>n</italic>-macro predicts itself well, i.e., that it is self-determining or autonomous (see [<xref ref-type="bibr" rid="R30">30</xref>]). Rather, it indicates that the macroscopic dynamics are more independent from the microscopic base. Through the same optimisation procedure, one could, in principle, reveal an entirely Gaussian, white-noise process as an <italic>n</italic>-macro that is independent of the micro-level constituents. Consequently, these ‘noisy’ macroscopic variables might not be dynamically relevant for the whole-system dynamical structure.</p><p id="P75">However, while such a macroscopic variable might initially seem of limited interest, identifying these white-noise macros could be useful depending on the empirical question. They can potentially be factored out, allowing researchers to focus on the core, non-trivial dynamical structures that are more informative. From the results obtained, we suggest that the lower dynamical dependence (higher emergence) values observed when increasing dynamical noise (mediating functional segregation) could be driven by the emergence of such noise-dominated macros.</p><p id="P76">Consequently, we speculate that emergent macroscopic variables accompanied by relatively higher dynamical dependence (lower emergence) could be expected at the balanced points. This suggests that for the emergent <italic>n</italic>-macros to hold any dynamical relevance within the whole-system dynamics, some degree of dependence between the macroscopic process and the microscopic process may be necessary. However, the actual relevance of the emergent <italic>n</italic>-macros identified by DI analysis is ultimately an empirical question. It is crucial to carefully assess the functionality and significance of the <italic>n</italic>-macros, recognising that not all identified macros may contribute meaningfully to the overall system dynamics.</p><p id="P77">Lastly, <xref ref-type="fig" rid="F6">Fig 6</xref> indicates that the parameter regime associated with the emergence of macroscopic dynamical structure is absent in an uncoupled network. This finding underscores the importance of interactions between micro-level constituents in driving the emergent macroscopic patterns of activity in neural models. Additionally, we demonstrated that statistical testing can effectively discern single-node contributions to emergent <italic>n</italic>-macros across different conditions, providing a rigorous method for determining <italic>n</italic>-macro localisability in the original state space. In this study, we focused on the contribution levels of each node to the dynamics of the emergent <italic>n</italic>-macros, comparing these contributions with those in the uncoupled network using a Wilcoxon rank-sum test.</p></sec><sec id="S18"><title>Beyond indices: Uncovering the dynamical structure of complexity and emergence</title><p id="P78">Our approach could challenge existing intuitions about organisational complexity in both computational [<xref ref-type="bibr" rid="R76">76</xref>, <xref ref-type="bibr" rid="R77">77</xref>] and biological [<xref ref-type="bibr" rid="R78">78</xref>, <xref ref-type="bibr" rid="R79">79</xref>] systems. Furthermore, it explores the association between organisational structure and the optimal balance between integration and segregation [<xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R80">80</xref>, <xref ref-type="bibr" rid="R81">81</xref>]. Our methodology uniquely provides a robust operational approach to identify the dominant dynamical structures underlying global brain states, which are often indexed by measures of criticality [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R82">82</xref>] and neural complexity [<xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R83">83</xref>–<xref ref-type="bibr" rid="R86">86</xref>].</p><p id="P79">An attempt to clarify the relationship between the integration-segregation balance and criticality has been attempted [<xref ref-type="bibr" rid="R25">25</xref>]. In general, criticality is commonly associated with systems at a phase transition, often characterised by power-law dynamics [<xref ref-type="bibr" rid="R87">87</xref>]. Theoretically, criticality refers to the point at which a system transitions between different phases, typically marked by scale-free properties [<xref ref-type="bibr" rid="R6">6</xref>]. However, the term”criticality” can be somewhat ambiguous, as it is empirically measured in various ways [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R82">82</xref>], often alongside measures of complexity or signal diversity [<xref ref-type="bibr" rid="R88">88</xref>].</p><p id="P80">In this context, both measures of criticality and complexity serve as indices that refer to underlying organisational structure within the system. For instance, the power-law structure, such as the 1/f curve, indicates that the system exhibits scale-free behaviour, where smaller clusters of activity are more common than larger ones. This reflects a form of organisational complexity that is indexed by the criticality measure. Similarly, measures of neural complexity aim to index the degree of organisational structure within brain dynamics.</p><p id="P81">While remaining agnostic to specific measures of criticality or complexity, our work seeks to vary the two parameters that are believed to influence both, with the aim of providing a complementary method that goes beyond empirical indices to identify the underlying dynamical structure and its level of emergence. An exciting avenue for future research will be to compare our method with other neural complexity measures—which actually peak during a balanced point of integration and segregation—to explore how these measures deviate from each other. Characterising the emergent dynamical structure of global brain states in relation to these indices, using both observational and perturbed datasets, could offer significant advancements in our understanding of complexity in neuroscience.</p><p id="P82">Building on existing studies that explore the macrostates of brain activity [<xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R89">89</xref>, <xref ref-type="bibr" rid="R90">90</xref>], as well as those quantifying synergy between brain region pairs [<xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R91">91</xref>], our approach offers a complementary perspective by uncovering the macroscopic dynamical structure derived from microscopic interactions and quantifying its dependence on these interactions. Unlike other views of emergence, such as synergy, which captures a specific type of higher-order interactions between brain regions, DI provides a framework for understanding dynamical closure at the macroscopic level. Thereby identifying lower-dimensional subspaces that might serve as independent communication subspaces for regions. This approach not only reveals emergent macroscopic variables across spatial scales but also highlights the system-wide organisation that arises from the underlying dynamics. While existing methods applicable to whole-brain modelling might effectively capture macroscopic activity, they often lack a concept of emergence. Integrating these tools to form a comprehensive understanding of emergence in the brain represents an intriguing direction for future research. Further research will need to expand the application of our framework to larger artificial networks or neurophysiological data.</p></sec><sec id="S19"><title>Bridging emergence, coarse-graining, dynamical closure, and dimensionality reduction</title><p id="P83">This work contributes to the broader effort to (i) quantify and detect coarse-grained macroscopic dynamics, and (ii) provide a precise methodology for further application to large-scale neurophysiological data. In particular, we consider the relation of our approach to effective information-based measures of causal emergence [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R74">74</xref>], informational closure [<xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R92">92</xref>], and common dimensionality reduction techniques [<xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R75">75</xref>]</p><p id="P84">First, we examine the distinction between coarse-graining in the DI framework and coarse-graining within the context of effective information-based causal emergence [<xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R74">74</xref>, <xref ref-type="bibr" rid="R93">93</xref>, <xref ref-type="bibr" rid="R94">94</xref>]. In causal emergence, coarse-graining involves recasting a complex network into non-overlapping partitions using a hard-partitioning function and evaluating the effective information of the resulting higher-order network. Effective information is measured by balancing the average degeneracy and determinism within the network. A partitioned graph that exhibits higher effective information than the original is considered causally emergent. This approach can be applied even without interventionist methods of causality [<xref ref-type="bibr" rid="R95">95</xref>, <xref ref-type="bibr" rid="R96">96</xref>], extending across various bidirectional networks [<xref ref-type="bibr" rid="R74">74</xref>, <xref ref-type="bibr" rid="R97">97</xref>].</p><p id="P85">In contrast, coarse-graining within the DI framework focuses on the degree of informational or causal closure of a macroscopic variable, aligning more closely with the principles of statistical mechanics. Instead of partitioning the network into sub-networks, this approach involves partitioning the microscopic <italic>state-space</italic> (which partitions the dynamics, not the nodes) into subsets <italic>f</italic><sup>−1</sup>(<italic>Y</italic>), where <italic>Y</italic> represents the macroscopic states. This method captures how individual microlevel nodes contribute to these macroscopic variables, which is much closer aligned to statistical mechanics descriptions macroscopic (ensemble) properties emerging from microscopic interactions [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R98">98</xref>]. DI serves as a dimensionality-reduction technique that reveals low-dimensional dynamics as self-contained systems, without transforming the network into hierarchical structures. Given its ability to capture the distributive nature of macroscopic variables and their degree of dependence on the microlevel, DI aligns more closely with a heterarchical perspective of dynamical structure [<xref ref-type="bibr" rid="R99">99</xref>].</p><p id="P86">In support of a dynamical closure perspective on the emergence of macroscopic dynamics in the brain, it is important to recognise that a core principle of brain organisation is its function as a highly distributed information system [<xref ref-type="bibr" rid="R100">100</xref>–<xref ref-type="bibr" rid="R102">102</xref>], where local (microlevel) functional units integrate to generate macrolevel dynamics. These dynamics are not fixed but fluctuate over time, involving the transient recruitment of various microlevel regions [<xref ref-type="bibr" rid="R102">102</xref>]. This suggests that the brain’s global dynamics are driven by overlapping compositions of regions, without clear distinction between the microlevel parts and their relation to the whole. Regions within the brain are dynamically organised in response to functional and computational demands, reflecting the adaptive and non-static nature of brain dynamics across spatial scales. Although we worked on a 5-node simulated network in which these challenges are not present, our approach can adapt to the challenges of larger or real brain networks because it emphasises dynamical closure, focuses on the emergent dynamical structure, and examines to what degree microlevel nodes contribute to macroscopic dynamics—allowing for us to assess the degree of localised or distributive activity.</p><p id="P87">DI distinguishes itself from other formal approaches to informational closure, such as those by [<xref ref-type="bibr" rid="R103">103</xref>] and [<xref ref-type="bibr" rid="R63">63</xref>], by extending the concept beyond absolute informational closure (perfect dynamical independence) and accommodating non-Markovian dynamics. This allows DI to capture the nuanced interdependence between macroscopic and microscopic processes.</p><p id="P88">While [<xref ref-type="bibr" rid="R92">92</xref>] explore informational closure as a framework for conscious processing, our approach remains neutral on such interpretations. Additionally, unlike Chang and colleagues’ consideration of direct information flow between macroscopic and environmental variables, our framework imposes a supervenience condition (See <xref ref-type="disp-formula" rid="FD3">Eq 2</xref>), ensuring that no new information emerges at the macroscopic level beyond what is determined by the microscopic processes. This means that the microscopic fully dictates the information shared with the macroscopic dynamics.</p><p id="P89">Finally, although DI results in dimensionality reduction, it offers distinct advantages over commonly used techniques such as Principal Components Analysis (PCA) and t-distributed Stochastic Neighbour Embedding (t-SNE). PCA identifies components of maximum variance, and t-SNE preserves local relationships between data points [<xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R75">75</xref>, <xref ref-type="bibr" rid="R104">104</xref>]. However, neither method explicitly accounts for the time-dependence of neural activity to capture low-dimensional dynamics. In contrast, DI operates as an information-theoretic dimensionality reduction technique, directly mediated by microlevel interactions by the temporal structure of the time series.</p></sec><sec id="S20"><title>Limitations and open questions</title><p id="P90">DI, in it’s current framework, is based on Granger causality. Although Granger causality is well-defined for non-stationary processes, it is notoriously difficult to estimate in these cases, and as such is not considered here. Thus we assume the wide-sense stationarity of neurophysiological data, and might not be able to capture physiologically-relevant non-stationarities in the time-series strongly correlated with specific brain states. An example can be illustrated by brief neural oscillations (e.g., sleep spindles), or waves (e.g., K-complexes, or epileptic activity) as well as transient responses to external or internal stimuli. However, linear approximations have been argued to optimally capture macroscopic neural dynamics [<xref ref-type="bibr" rid="R105">105</xref>], particularly in resting-state activity.</p><p id="P91">Furthermore, fitting a VAR or SS model for G-causality estimation assumes a linear model rather than a linear process, which is a subtle but important distinction. While the model assumes linearity in its structure, this does not necessarily mean that the underlying time series must be linear. The presence of non-linearities in the data does not inherently imply that the fitted model will be unstable (see [<xref ref-type="bibr" rid="R44">44</xref>] for an in-depth discussion); rather, it suggests that the model may not fully capture the effects of those non-linearities. In fact, if the SS or VAR model is stable, then Granger causality inference and DI analysis remain well-defined.</p><p id="P92">A more precise understanding is that the model, if stable, induces a linear representation of the underlying data. The key issue then becomes whether this induced linear model can sufficiently account for the non-linearities present in the time series. Wold’s decomposition theorem [<xref ref-type="bibr" rid="R106">106</xref>] guarantees that any stationary process can be decomposed into a linear model, although this model may require an infinite order, making it potentially non-parsimonious for time series generated by a nonlinear process [<xref ref-type="bibr" rid="R107">107</xref>]. Thus, the impact of nonlinearity on G-causality estimation is nuanced and complex. The critical question remains whether the linear model, when applied to nonlinear data, provides a sufficiently accurate representation of the underlying dynamics to make valid inferences. Though preliminary research suggests the advantages of linear models [<xref ref-type="bibr" rid="R105">105</xref>], this remains an interesting and open area for future research.</p><p id="P93">One limitation of our study is that the neural networks we employed are not functionally specialised—they are not designed to perform specific tasks or processes. In biological brain networks, functional specialisation is a fundamental characteristic that influences how integration and segregation manifest in neural dynamics. Therefore, it is possible that functionally specialised systems might exhibit different patterns of dynamical dependence and emergent dynamical structures. To address this, future research could explore simulations of networks with functional specialisation, perhaps by incorporating task-specific modules or connectivity patterns. Testing our methods in such simulated environments would help determine whether the observed results generalise to systems that more closely resemble the functional organisation of the brain.</p><p id="P94">While our study employs a relatively small 5-node neural model, which allows for detailed exploration of emergent <italic>n</italic>-macros, we acknowledge the importance of applying these methods to larger, functionally specialised systems. In ongoing work, we have begun to extend our techniques to whole-brain EEG data, demonstrating that our approach can be scaled up to analyse complex neural dynamics at the whole-brain level. This progression not only addresses the computational challenges associated with larger models but also brings us closer to understanding emergent dynamical structures in more realistic neural systems.</p><p id="P95">Ultimately, the dynamical relevance and implications of emergent <italic>n</italic>-macros for the systems under study remain an open empirical question. It is plausible to consider that emergent <italic>n</italic>-macros with many equally implicated contributions from microlevel nodes could function as low-dimensional communication subspaces through which higher-order interactions are mediated [<xref ref-type="bibr" rid="R108">108</xref>]. These <italic>n</italic>-macros, by capturing distributive contributions across the network, may represent the channels through which complex, coordinated dynamics occur at a macroscopic level. With DI we can establish the degree of localisability or distribution of these communication subspaces (<italic>n</italic>-macros). This concept of communication subspaces presents a compelling direction for future research.</p></sec></sec><sec id="S21" sec-type="conclusions"><title>Conclusion</title><p id="P96">This work provides methodological, empirical and theoretical contributions to the exploration of emergent dynamics in complex neural systems. From a methodological and empirical perspective, we demonstrate that the balance between functional integration and segregation significantly influences the emergent dynamical structure across macroscopic spatial scales in biophysical neural models. Our results revealed that a distinct dynamical structure is identified by the spatial localisation of <italic>n</italic>-macros at a balanced point between integration and segregation, where specific microlevel nodes predominantly contribute to specific emergent <italic>n</italic>-macros. In contrast, this organised structure becomes less localised and more distributed in parameter regimes marked by either excessive integration or segregation. These results illustrate the heuristic power our approach may have when applied to biological systems, in identifying the brain structures participating in emergent dynamics.</p><p id="P97">From a theoretical perspective, this work contributes to the broader agenda of moving beyond indexical measures of complexity and emergence to identifying the underlying structure of dynamics that underpin quantities like these. Progress in identifying dominant emergent macroscopic patterns has implications for defining stable global brain states and understanding how the brain organises itself to meet the computational demands of whole-brain function. The findings here represent a promising step towards leveraging DI for empirical investigations into dynamical properties that go beyond traditional complexity indices, offering a more qualitative perspective of brain organisation.</p></sec><sec id="S24"><title>Glossary</title><glossary><def-list><def-item><term>Emergence</term><def><p id="P98">The phenomenon where higher-level patterns or behaviours arise from interactions of lower level constituent units</p></def></def-item><def-item><term>Emergent Dynamical Structure</term><def><p id="P99">Macroscopic <italic>patterns</italic> of activity across spatial scales that emerge from the interactions between microscopic variables in a system</p></def></def-item><def-item><term>Integration</term><def><p id="P100">The tendency of different parts of a system to work together to form a cohesive whole, often through coupling between units</p></def></def-item><def-item><term>Segregation</term><def><p id="P101">The tendency of parts of a system to behave independently, reducing the interactions and coupling between them</p></def></def-item><def-item><term>Surjective</term><def><p id="P102">A function that maps every element in the target set (output) to at least one element in the domain (input)</p></def></def-item><def-item><term>Stochastic Process</term><def><p id="P103">A process that involves randomness, where the next state is not fully determined by a set of previous states in it’s past</p></def></def-item><def-item><term>Coarse-Graining</term><def><p id="P104">A technique that reduces the dimensionality of a system by grouping together similar states or variables into a single, larger-scale description</p></def></def-item><def-item><term>Macroscopic Variable</term><def><p id="P105">A higher-level, large-scale variable that describes the collective behaviour of many microscopic variables</p></def></def-item><def-item><term>Microscopic Variable</term><def><p id="P106">A small-scale variable that represents the state or behavior of a component in a system</p></def></def-item><def-item><term>Dynamical Independence (DI)</term><def><p id="P107">A measure of how independent the dynamics of a macroscopic variable is from the underlying microscopic variables</p></def></def-item><def-item><term>State-Space</term><def><p id="P108">The set of all possible states that a system can occupy</p></def></def-item><def-item><term>Partition</term><def><p id="P109">A division of the state-space into distinct, non-overlapping subsets</p></def></def-item><def-item><term>Parameter Sweep</term><def><p id="P110">The process of systematically varying parameters in a model to observe their effects on the system’s behaviour</p></def></def-item><def-item><term>Distributed / Local</term><def><p id="P111">Refers to whether a macroscopic process is spread across many variables (distributed) or confined to a few variables (local)</p></def></def-item><def-item><term>Neural Mass Model (NMM)</term><def><p id="P112">A simplified mathematical model that represents the collective dynamics of neural populations, often used to describe brain activity</p></def></def-item><def-item><term>Biophysical Model</term><def><p id="P113">A computational model that incorporates biological realistic details, capturing the physical and biological mechanisms underlying a system, such as neural dynamics</p></def></def-item><def-item><term>Evolution Equation</term><def><p id="P114">A mathematical equation that describes how a system’s state changes over time, often used in neural models to represent dynamic processes</p></def></def-item><def-item><term>Higher-order Scales</term><def><p id="P115">Larger-scale, macroscopic descriptions—across spatial scale—of a system that capture the emergent dynamics arising from interactions at the microscopic or lower-level scales</p></def></def-item></def-list></glossary></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supporting information</label><media xlink:href="EMS199608-supplement-Supporting_information.pdf" mimetype="application" mime-subtype="pdf" id="d26aAcGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments</title><p>We would like to thank the Melbourne School of Psychological Sciences at the University of Melbourne, the Sussex Centre for Consciousness Science at the University of Sussex, and the Paris Brain Institute (ICM) for providing the necessary resources and institutional support. We also extend our gratitude to lab members and colleagues for their insightful discussions and assistance over the drafting of the manuscript.</p><p>This research was supported through a National Health and Medical Research Council (NHMRC) grant #APP1183280 and internal University of Melbourne research funding of Prof. Olivia Carter. The research was also supported, in part, by the European Research Council (ERC) Advanced Investigator Grant 10109254 to A.K. Seth, which also supports L. Barnett. We also acknowledge the Australian Government Research Training Program Scholarship, which made this work possible. Additionally, we appreciate the use of The Virtual Brain (TVB) software, which was crucial to the biophysical neural model simulations and analyses in this study.</p><p>Finally, we thank Adam Barrett, Marcello Massimini, Fernando Rosas, Pedro Mediano, and Aniko Kusztor for invaluable discussions, encouragement, and ongoing support throughout the research process.</p></ack><sec id="S23" sec-type="data-availability"><title>Code Availability</title><p id="P116">The code used in this study integrates multiple toolboxes and custom scripts. The core method for Dynamical Independence (DI) analysis is implemented through the <bold>SSDI toolbox</bold> developed by Lionel Barnett, which is available at: <bold><ext-link ext-link-type="uri" xlink:href="https://github.com/lcbarnett/ssdi">https://github.com/lcbarnett/ssdi</ext-link></bold>.</p><p id="P117">This toolbox depends on the <bold>MVGC2 toolbox</bold>, also developed by Lionel Barnett, for multivariate Granger causality analysis, available at: <bold><ext-link ext-link-type="uri" xlink:href="https://github.com/lcbarnett/MVGC2">https://github.com/lcbarnett/MVGC2</ext-link></bold>.</p><p id="P118">Additionally, the simulations of biophysical neural models were adapted from <bold>The Virtual Brain (TVB)</bold> software, which is available at: <bold><ext-link ext-link-type="uri" xlink:href="https://www.thevirtualbrain.org/">https://www.thevirtualbrain.org/</ext-link></bold>.</p><p id="P119">A custom package specifically developed for this project is available on the first author’s GitHub under the repository <bold>TVBEmergence</bold>: <bold><ext-link ext-link-type="uri" xlink:href="https://github.com/bmilinkovic/TVBEmergence/">https://github.com/bmilinkovic/TVBEmergence/</ext-link></bold>.</p><p id="P120">This package includes all the scripts necessary to reproduce the results, including instructions for environment setup and dependencies.</p><p id="P121">For further inquiries or assistance with the code, please contact the corresponding author.</p></sec><fn-group><fn id="FN1"><label>1</label><p id="P122">The linear mapping is surjective iff <italic>M</italic> has full rank <italic>n</italic>.</p></fn><fn id="FN2"><label>2</label><p id="P123">we use the notation <italic>t</italic> − <italic>τ</italic>, where <italic>τ</italic> is not a single value but represents a set of lagged time points, <italic>τ</italic> = 1, 2, 3, …, <italic>n</italic>, referring to the history of the process. In the case of an infinite history <italic>τ</italic> = 1, 2, 3, …, this set would extend indefinitely.</p></fn><fn id="FN3"><label>3</label><p id="P124">We note, for example, that dynamical independence is <italic>transitive</italic> [<xref ref-type="bibr" rid="R30">30</xref>], and that emergent macrovariables may thus potentially (but not necessarily) be nested, leading to the possibility of heterarchical emergence structures.</p></fn><fn id="FN4"><label>4</label><p id="P125">The subspace associated with a coarse-graining is spanned by the <italic>n</italic> basis vectors defined by the columns of the <italic>N × n</italic> transposed coarse-graining matrix <italic>M</italic><sup>⊤</sup>. Exploiting the invariance of DD under transformations of the macroscopic space ℝ<sup><italic>n</italic></sup>, it turns out to be convenient to parametrise the Grassmannian by <italic>orthogonal</italic> matrices <italic>M</italic> ; i.e., matrices for which <italic>MM</italic><sup>⊤</sup> = <italic>I</italic>. These matrices constitute the <italic>Stiefel manifold</italic> of orthonormal bases, written <italic>𝒱</italic><sub><italic>N</italic></sub> (<italic>n</italic>)—also a homogeneous Riemannian manifold—and we consequently refer to the parametrisation of the Grassmannian <italic>𝒢</italic><sub><italic>N</italic></sub> (<italic>n</italic>) by orthogonal <italic>n × N</italic> matrices as the Stiefel parametrisation. We note that for <italic>n &gt;</italic> 1 this parametrisation is not one-to-one; <italic>𝒱</italic><sub><italic>N</italic></sub> (<italic>n</italic>) has dimension <inline-formula><mml:math id="M6"><mml:mrow><mml:mi>n</mml:mi><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, which for <italic>n &gt;</italic> 1 is larger than the dimension <italic>n</italic>(<italic>N</italic> − <italic>n</italic>) of <italic>𝒢</italic><sub><italic>N</italic></sub> (<italic>n</italic>). In practice, optimising over the Grassmannian and Stiefel manifolds produce similar results [<xref ref-type="bibr" rid="R66">66</xref>], and the latter is simpler to implement.</p></fn><fn id="FN5"><label>5</label><p id="P126">Computational neuroscience has a tendency to present functional structure in terms of networks. We stress that in DI, macroscopic variables are, in general, <italic>not</italic> networks in any meaningful sense.</p></fn><fn id="FN6"><label>6</label><p id="P127">In higher-dimensional systems, the axis angles do not uniquely identify a single subspace: the higher dimensionality affords more degrees of freedom for subspaces. Nonetheless, axis angles are informative about region contribution.</p></fn><fn id="FN7"><label>7</label><p id="P128">the analysis for the emergent 2-macros shows exactly similar varied pattern, excluded here for brevity</p></fn><fn fn-type="con" id="FN8"><p id="P129"><bold>Author Contributions</bold>
<list list-type="bullet" id="L1"><list-item><p id="P130"><bold>Conceptualisation</bold>: Borjan Milinkovic, Thomas Andrillon, Lionel Barnett, Anil K. Seth, Olivia Carter.</p></list-item><list-item><p id="P131"><bold>Formal analysis</bold>: Borjan Milinkovic.</p></list-item><list-item><p id="P132"><bold>Funding acquisition</bold>: Olivia Carter, Anil K. Seth.</p></list-item><list-item><p id="P133"><bold>Investigation</bold>: Borjan Milinkovic.</p></list-item><list-item><p id="P134"><bold>Methodology</bold>: Borjan Milinkovic, Lionel Barnett.</p></list-item><list-item><p id="P135"><bold>Supervision</bold>: Olivia Carter, Thomas Andrillon, Lionel Barnett, Anil K. Seth.</p></list-item><list-item><p id="P136"><bold>Writing – original draft</bold>: Borjan Milinkovic.</p></list-item><list-item><p id="P137"><bold>Writing – review &amp; editing</bold>: Borjan Milinkovic, Thomas Andrillon, Lionel Barnett, Anil K. Seth, Olivia Carter.</p></list-item></list></p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><article-title>A measure for brain complexity: relating functional segregation and integration in the nervous system</article-title><source>Proceedings of the National Academy of Sciences</source><year>1994</year><volume>91</volume><issue>11</issue><fpage>5033</fpage><lpage>5037</lpage><pub-id pub-id-type="pmcid">PMC43925</pub-id><pub-id pub-id-type="pmid">8197179</pub-id><pub-id pub-id-type="doi">10.1073/pnas.91.11.5033</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Robinson</surname><given-names>PA</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>The dynamic brain: From spiking neurons to neural masses and cortical fields</article-title><source>PLoS Computational Biology</source><year>2008</year><volume>4</volume><issue>8</issue><pub-id pub-id-type="pmcid">PMC2519166</pub-id><pub-id pub-id-type="pmid">18769680</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000092</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><article-title>Perception and self-organized instability</article-title><source>Frontiers in computational neuroscience</source><year>2012</year><volume>6</volume><fpage>44</fpage><pub-id pub-id-type="pmcid">PMC3390798</pub-id><pub-id pub-id-type="pmid">22783185</pub-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00044</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Ritter</surname><given-names>P</given-names></name></person-group><article-title>The dynamics of resting fluctuations in the brain: metastability and its dynamical cortical core</article-title><source>Scientific reports</source><year>2017</year><volume>7</volume><issue>1</issue><fpage>3095</fpage><pub-id pub-id-type="pmcid">PMC5465179</pub-id><pub-id pub-id-type="pmid">28596608</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-03073-5</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanz Leon</surname><given-names>P</given-names></name><name><surname>Knock</surname><given-names>SA</given-names></name><name><surname>Woodman</surname><given-names>MM</given-names></name><name><surname>Domide</surname><given-names>L</given-names></name><name><surname>Mersmann</surname><given-names>J</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><etal/></person-group><article-title>The Virtual Brain: a simulator of primate brain network dynamics</article-title><source>Frontiers in Neuroinformatics</source><year>2013</year><month>MAY</month><volume>0</volume><fpage>10</fpage><pub-id pub-id-type="pmcid">PMC3678125</pub-id><pub-id pub-id-type="pmid">23781198</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00010</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cocchi</surname><given-names>L</given-names></name><name><surname>Gollo</surname><given-names>LL</given-names></name><name><surname>Zalesky</surname><given-names>A</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><article-title>Criticality in the brain: A synthesis of neurobiology, models and cognition</article-title><source>Progress in neurobiology</source><year>2017</year><volume>158</volume><fpage>132</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">28734836</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Network structure of cerebral cortex shapes functional connectivity on multiple time scales</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><issue>24</issue><fpage>10240</fpage><lpage>10245</lpage><pub-id pub-id-type="pmcid">PMC1891224</pub-id><pub-id pub-id-type="pmid">17548818</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0701519104</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>Robinson</surname><given-names>PA</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>The dynamic brain: from spiking neurons to neural masses and cortical fields</article-title><source>PLoS computational biology</source><year>2008</year><volume>4</volume><issue>8</issue><elocation-id>e1000092</elocation-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanz-Leon</surname><given-names>P</given-names></name><name><surname>Knock</surname><given-names>SA</given-names></name><name><surname>Spiegler</surname><given-names>A</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>Mathematical framework for large-scale brain network modeling in The Virtual Brain</article-title><source>NeuroImage</source><year>2015</year><volume>111</volume><fpage>385</fpage><lpage>430</lpage><pub-id pub-id-type="pmid">25592995</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ay</surname><given-names>N</given-names></name></person-group><article-title>Information geometry on complexity and stochastic interaction</article-title><source>Entropy</source><year>2015</year><volume>17</volume><issue>4</issue><fpage>2432</fpage><lpage>2458</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bar-Yam</surname><given-names>Y</given-names></name></person-group><source>Dynamics of complex systems</source><publisher-name>CRC Press</publisher-name><year>2019</year></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oizumi</surname><given-names>M</given-names></name><name><surname>Amari</surname><given-names>Si</given-names></name><name><surname>Yanagawa</surname><given-names>T</given-names></name><name><surname>Fujii</surname><given-names>N</given-names></name><name><surname>Tsuchiya</surname><given-names>N</given-names></name></person-group><article-title>Measuring integrated information from the decoding perspective</article-title><source>PLoS computational biology</source><year>2016</year><volume>12</volume><issue>1</issue><elocation-id>e1004654</elocation-id><pub-id pub-id-type="pmcid">PMC4721632</pub-id><pub-id pub-id-type="pmid">26796119</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004654</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leung</surname><given-names>A</given-names></name><name><surname>Cohen</surname><given-names>D</given-names></name><name><surname>Van Swinderen</surname><given-names>B</given-names></name><name><surname>Tsuchiya</surname><given-names>N</given-names></name></person-group><article-title>Integrated information structure collapses with anesthetic loss of conscious arousal in Drosophila melanogaster</article-title><source>PLoS Computational Biology</source><year>2021</year><volume>17</volume><issue>2</issue><elocation-id>e1008722</elocation-id><pub-id pub-id-type="pmcid">PMC7946294</pub-id><pub-id pub-id-type="pmid">33635858</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008722</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirner</surname><given-names>M</given-names></name><name><surname>Kong</surname><given-names>X</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Ritter</surname><given-names>P</given-names></name></person-group><article-title>Dynamic primitives of brain network interaction</article-title><source>NeuroImage</source><year>2022</year><volume>250</volume><elocation-id>118928</elocation-id><pub-id pub-id-type="pmid">35101596</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shalizi</surname><given-names>CR</given-names></name><name><surname>Crutchfield</surname><given-names>JP</given-names></name></person-group><article-title>Computational mechanics: Pattern and prediction, structure and simplicity</article-title><source>Journal of statistical physics</source><year>2001</year><volume>104</volume><fpage>817</fpage><lpage>879</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shalizi</surname><given-names>CR</given-names></name><name><surname>Moore</surname><given-names>C</given-names></name></person-group><article-title>What Is a Macrostate? Subjective Observations and Objective Dynamics</article-title><source>arxiv</source><year>2003</year><pub-id pub-id-type="doi">10.48550/arxiv.cond-mat/0303625</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><article-title>Consciousness and complexity</article-title><source>science</source><year>1998</year><volume>282</volume><issue>5395</issue><fpage>1846</fpage><lpage>1851</lpage><pub-id pub-id-type="pmid">9836628</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Izhikevich</surname><given-names>E</given-names></name><name><surname>Reeke</surname><given-names>GN</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name></person-group><article-title>Theories and measures of consciousness: an extended framework</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><issue>28</issue><fpage>10799</fpage><lpage>10804</lpage><pub-id pub-id-type="pmcid">PMC1487169</pub-id><pub-id pub-id-type="pmid">16818879</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0604347103</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name></person-group><article-title>Causal density and integrated information as measures of conscious level</article-title><source>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</source><year>2011</year><volume>369</volume><issue>1952</issue><fpage>3748</fpage><lpage>3767</lpage><pub-id pub-id-type="pmid">21893526</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arsiwalla</surname><given-names>XD</given-names></name><name><surname>Verschure</surname><given-names>P</given-names></name></person-group><article-title>Measuring the complexity of consciousness</article-title><source>Frontiers in neuroscience</source><year>2018</year><volume>12</volume><fpage>424</fpage><pub-id pub-id-type="pmcid">PMC6030381</pub-id><pub-id pub-id-type="pmid">29997472</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00424</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colombo</surname><given-names>MA</given-names></name><name><surname>Napolitani</surname><given-names>M</given-names></name><name><surname>Boly</surname><given-names>M</given-names></name><name><surname>Gosseries</surname><given-names>O</given-names></name><name><surname>Casarotto</surname><given-names>S</given-names></name><name><surname>Rosanova</surname><given-names>M</given-names></name><etal/></person-group><article-title>The spectral exponent of the resting EEG indexes the presence of consciousness during unresponsiveness induced by propofol, xenon, and ketamine</article-title><source>NeuroImage</source><year>2019</year><volume>189</volume><fpage>631</fpage><lpage>644</lpage><pub-id pub-id-type="pmid">30639334</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dürschmid</surname><given-names>S</given-names></name><name><surname>Reichert</surname><given-names>C</given-names></name><name><surname>Walter</surname><given-names>N</given-names></name><name><surname>Hinrichs</surname><given-names>H</given-names></name><name><surname>Heinze</surname><given-names>HJ</given-names></name><name><surname>Ohl</surname><given-names>FW</given-names></name><etal/></person-group><article-title>Self-regulated critical brain dynamics originate from high frequency-band activity in the MEG</article-title><source>Plos one</source><year>2020</year><volume>15</volume><issue>6</issue><elocation-id>e0233589</elocation-id><pub-id pub-id-type="pmcid">PMC7289413</pub-id><pub-id pub-id-type="pmid">32525940</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0233589</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarasso</surname><given-names>S</given-names></name><name><surname>Casali</surname><given-names>AG</given-names></name><name><surname>Casarotto</surname><given-names>S</given-names></name><name><surname>Rosanova</surname><given-names>M</given-names></name><name><surname>Sinigaglia</surname><given-names>C</given-names></name><name><surname>Massimini</surname><given-names>M</given-names></name></person-group><article-title>Consciousness and complexity: a consilience of evidence</article-title><source>Neuroscience of Consciousness</source><year>2021</year><volume>2021</volume><issue>2</issue><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC10941977</pub-id><pub-id pub-id-type="pmid">38496724</pub-id><pub-id pub-id-type="doi">10.1093/nc/niab023</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>U</given-names></name></person-group><article-title>Criticality as a determinant of integrated information Φ in human brain networks</article-title><source>Entropy</source><year>2019</year><volume>21</volume><issue>10</issue><fpage>981</fpage></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Farah</surname><given-names>JC</given-names></name><name><surname>Shanahan</surname><given-names>M</given-names></name></person-group><article-title>Integrated information and metastability in systems of coupled oscillators</article-title><source>arXiv</source><year>2016</year><elocation-id>arXiv:160608313</elocation-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balduzzi</surname><given-names>D</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><article-title>Integrated information in discrete dynamical systems: motivation and theoretical framework</article-title><source>PLoS computational biology</source><year>2008</year><volume>4</volume><issue>6</issue><elocation-id>e1000091</elocation-id><pub-id pub-id-type="pmcid">PMC2386970</pub-id><pub-id pub-id-type="pmid">18551165</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000091</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Practical measures of integrated information for time-series data</article-title><source>PLoS computational biology</source><year>2011</year><volume>7</volume><issue>1</issue><elocation-id>e1001052</elocation-id><pub-id pub-id-type="pmcid">PMC3024259</pub-id><pub-id pub-id-type="pmid">21283779</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001052</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name></person-group><article-title>Measuring integrated information: Comparison of candidate measures in theory and simulation</article-title><source>Entropy</source><year>2018</year><volume>21</volume><issue>1</issue><fpage>17</fpage><pub-id pub-id-type="pmcid">PMC7514120</pub-id><pub-id pub-id-type="pmid">33266733</pub-id><pub-id pub-id-type="doi">10.3390/e21010017</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Farah</surname><given-names>JC</given-names></name><name><surname>Shanahan</surname><given-names>M</given-names></name><name><surname>Bor</surname><given-names>D</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name></person-group><article-title>Integrated information as a common signature of dynamical and information-processing complexity</article-title><source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source><year>2022</year><volume>32</volume><issue>1</issue><elocation-id>013115</elocation-id><pub-id pub-id-type="pmcid">PMC7614772</pub-id><pub-id pub-id-type="pmid">35105139</pub-id><pub-id pub-id-type="doi">10.1063/5.0063384</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Dynamical independence: discovering emergent macroscopic processes in complex dynamical systems</article-title><source>Physical Review E</source><year>2023</year><volume>108</volume><issue>1</issue><elocation-id>014304</elocation-id><pub-id pub-id-type="pmid">37583178</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tagliazucchi</surname><given-names>E</given-names></name><name><surname>Von Wegner</surname><given-names>F</given-names></name><name><surname>Morzelewski</surname><given-names>A</given-names></name><name><surname>Brodbeck</surname><given-names>V</given-names></name><name><surname>Jahnke</surname><given-names>K</given-names></name><name><surname>Laufs</surname><given-names>H</given-names></name></person-group><article-title>Breakdown of long-range temporal dependence in default mode and attention networks during deep sleep</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2013</year><volume>110</volume><issue>38</issue><fpage>15419</fpage><lpage>15424</lpage><pub-id pub-id-type="pmcid">PMC3780893</pub-id><pub-id pub-id-type="pmid">24003146</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1312848110</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demertzi</surname><given-names>A</given-names></name><name><surname>Tagliazucchi</surname><given-names>E</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Barttfeld</surname><given-names>P</given-names></name><name><surname>Raimondo</surname><given-names>F</given-names></name><etal/></person-group><article-title>Human consciousness is supported by dynamic complex patterns of brain signal coordination</article-title><source>Science Advances</source><year>2019</year><volume>5</volume><issue>2</issue><pub-id pub-id-type="pmcid">PMC6365115</pub-id><pub-id pub-id-type="pmid">30775433</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aat7603</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Mediano</surname><given-names>PAM</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Allanson</surname><given-names>J</given-names></name><name><surname>Pickard</surname><given-names>JD</given-names></name><name><surname>Williams</surname><given-names>GB</given-names></name><etal/></person-group><article-title>Paths to Oblivion: Common Neural Mechanisms of Anaesthesia and Disorders of Consciousness</article-title><source>bioRxiv</source><year>2021</year><elocation-id>2021.02.14.431140</elocation-id><pub-id pub-id-type="doi">10.1101/2021.02.14.431140</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadin</surname><given-names>D</given-names></name><name><surname>Duclos</surname><given-names>C</given-names></name><name><surname>Mahdid</surname><given-names>Y</given-names></name><name><surname>Rokos</surname><given-names>A</given-names></name><name><surname>Badawy</surname><given-names>M</given-names></name><name><surname>Letourneau</surname><given-names>J</given-names></name><etal/></person-group><article-title>Brain network motif topography may predict emergence from disorders of consciousness: a case series</article-title><source>Neuroscience of Consciousness</source><year>2020</year><volume>2020</volume><issue>1</issue><elocation-id>niaa017</elocation-id><pub-id pub-id-type="pmcid">PMC7751128</pub-id><pub-id pub-id-type="pmid">33376599</pub-id><pub-id pub-id-type="doi">10.1093/nc/niaa017</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tognoli</surname><given-names>E</given-names></name><name><surname>Kelso</surname><given-names>JS</given-names></name></person-group><article-title>The metastable brain</article-title><source>Neuron</source><year>2014</year><volume>81</volume><issue>1</issue><fpage>35</fpage><lpage>48</lpage><pub-id pub-id-type="pmcid">PMC3997258</pub-id><pub-id pub-id-type="pmid">24411730</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.12.022</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shanahan</surname><given-names>M</given-names></name></person-group><article-title>Metastable chimera states in community-structured oscillator networks</article-title><source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source><year>2010</year><volume>20</volume><issue>1</issue><pub-id pub-id-type="pmid">20370263</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>JC</given-names></name><name><surname>Aquino</surname><given-names>KM</given-names></name><name><surname>Oldehinkel</surname><given-names>M</given-names></name><name><surname>Robinson</surname><given-names>PA</given-names></name><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><etal/></person-group><article-title>Geometric constraints on human brain function</article-title><source>Nature</source><year>2023</year><volume>618</volume><issue>7965</issue><fpage>566</fpage><lpage>574</lpage><pub-id pub-id-type="pmcid">PMC10266981</pub-id><pub-id pub-id-type="pmid">37258669</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06098-1</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabral</surname><given-names>J</given-names></name><name><surname>Fernandes</surname><given-names>FF</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name></person-group><article-title>Intrinsic macroscale oscillatory modes driving long range functional connectivity in female rat brains detected by ultrafast fMRI</article-title><source>Nature Communications</source><year>2023</year><volume>14</volume><issue>1</issue><fpage>375</fpage><pub-id pub-id-type="pmcid">PMC9902553</pub-id><pub-id pub-id-type="pmid">36746938</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-36025-x</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koussis</surname><given-names>NC</given-names></name><name><surname>Pang</surname><given-names>JC</given-names></name><name><surname>Jeganathan</surname><given-names>J</given-names></name><name><surname>Paton</surname><given-names>B</given-names></name><name><surname>Fornito</surname><given-names>A</given-names></name><name><surname>Robinson</surname><given-names>P</given-names></name><etal/></person-group><article-title>Generation of surrogate brain maps preserving spatial autocorrelation through random rotation of geometric eigenmodes</article-title><source>bioRxiv</source><year>2024</year><fpage>2024</fpage><lpage>02</lpage></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bar-Yam</surname><given-names>Y</given-names></name></person-group><article-title>A mathematical theory of strong emergence using multiscale variety</article-title><source>Complexity</source><year>2004</year><volume>9</volume><issue>6</issue><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1002/CPLX.20029/FORMAT/PDF</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>TM</given-names></name><name><surname>Thomas</surname><given-names>JA</given-names></name></person-group><source>Elements of Information Theory</source><series>(Wiley Series in Telecommunications and Signal Processing) (Hardcover)</series><year>2006</year></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Prokopenko</surname><given-names>M</given-names></name><name><surname>Zomaya</surname><given-names>AY</given-names></name></person-group><chapter-title>A framework for the local information dynamics of distributed computation in complex systems</chapter-title><source>Guided self-organization: inception</source><publisher-name>Springer</publisher-name><year>2014</year><fpage>115</fpage><lpage>158</lpage></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of neuroscience methods</source><year>2014</year><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Granger causality for state-space models</article-title><source>Physical Review E</source><year>2015</year><volume>91</volume><issue>4</issue><elocation-id>040101</elocation-id><pub-id pub-id-type="pmid">25974424</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bossomaier</surname><given-names>T</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Harré</surname><given-names>M</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Bossomaier</surname><given-names>T</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name><etal/></person-group><source>Transfer entropy</source><publisher-name>Springer</publisher-name><year>2016</year></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoel</surname><given-names>EP</given-names></name><name><surname>Albantakis</surname><given-names>L</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><article-title>Quantifying causal emergence shows that macro can beat micro</article-title><source>Proceedings of the National Academy of Sciences</source><year>2013</year><volume>110</volume><issue>49</issue><fpage>19790</fpage><lpage>19795</lpage><pub-id pub-id-type="pmcid">PMC3856819</pub-id><pub-id pub-id-type="pmid">24248356</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1314922110</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Jensen</surname><given-names>HJ</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Carhart-Harris</surname><given-names>RL</given-names></name><etal/></person-group><article-title>Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data</article-title><source>PLoS computational biology</source><year>2020</year><volume>16</volume><issue>12</issue><elocation-id>e1008289</elocation-id><pub-id pub-id-type="pmcid">PMC7833221</pub-id><pub-id pub-id-type="pmid">33347467</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008289</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name></person-group><article-title>Combining network topology and information theory to construct representative brain networks</article-title><source>Network Neuroscience</source><year>2021</year><volume>5</volume><issue>1</issue><fpage>96</fpage><lpage>124</lpage><pub-id pub-id-type="pmcid">PMC7935031</pub-id><pub-id pub-id-type="pmid">33688608</pub-id><pub-id pub-id-type="doi">10.1162/netn_a_00170</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Holland</surname><given-names>N</given-names></name><name><surname>Fryer</surname><given-names>TD</given-names></name><name><surname>O’Brien</surname><given-names>JT</given-names></name><etal/></person-group><article-title>A synergistic core for human brain evolution and cognition</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><issue>6</issue><fpage>771</fpage><lpage>782</lpage><pub-id pub-id-type="pmcid">PMC7614771</pub-id><pub-id pub-id-type="pmid">35618951</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01070-0</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Varley</surname><given-names>TF</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Stramaglia</surname><given-names>S</given-names></name><etal/></person-group><article-title>Disentangling high-order mechanisms and high-order behaviours in complex systems</article-title><source>Nature Physics</source><year>2022</year><volume>18</volume><issue>5</issue><fpage>476</fpage><lpage>477</lpage></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varley</surname><given-names>TF</given-names></name><name><surname>Pope</surname><given-names>M</given-names></name><name><surname>Faskowitz</surname><given-names>J</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Multivariate information theory uncovers synergistic subsystems of the human cerebral cortex</article-title><source>Communications biology</source><year>2023</year><volume>6</volume><issue>1</issue><fpage>451</fpage><pub-id pub-id-type="pmcid">PMC10125999</pub-id><pub-id pub-id-type="pmid">37095282</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-04843-w</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanescu</surname><given-names>R</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name></person-group><article-title>A low dimensional description of globally coupled heterogeneous neural networks of excitatory and inhibitory neurons</article-title><source>PLoS Comput Biol</source><year>2008</year><volume>4</volume><issue>11</issue><elocation-id>e1000219</elocation-id><pub-id pub-id-type="pmcid">PMC2574034</pub-id><pub-id pub-id-type="pmid">19008942</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000219</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanescu</surname><given-names>R</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name></person-group><article-title>Reduced representations of heterogeneous mixed neural networks with synaptic coupling</article-title><source>Physical review E, Statistical, nonlinear, and soft matter physics</source><year>2011</year><volume>83</volume><issue>2 Pt 2</issue><elocation-id>026204</elocation-id><pub-id pub-id-type="pmid">21405893</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Suzuki</surname><given-names>M</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><article-title>Cellular mechanisms of conscious processing</article-title><source>Trends in cognitive sciences</source><year>2020</year><volume>24</volume><issue>10</issue><fpage>814</fpage><lpage>825</lpage><pub-id pub-id-type="pmid">32855048</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munn</surname><given-names>BR</given-names></name><name><surname>Müller</surname><given-names>EJ</given-names></name><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Whyte</surname><given-names>CJ</given-names></name><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name><etal/></person-group><article-title>A thalamocortical substrate for integrated information via critical synchronous bursting</article-title><source>Proceedings of the National Academy of Sciences</source><year>2023</year><volume>120</volume><issue>46</issue><elocation-id>e2308670120</elocation-id><pub-id pub-id-type="pmcid">PMC10655573</pub-id><pub-id pub-id-type="pmid">37939085</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2308670120</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Rho</surname><given-names>Y</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>Noise during Rest Enables the Exploration of the Brain’s Dynamic Repertoire</article-title><source>PLOS Computational Biology</source><year>2008</year><volume>4</volume><issue>10</issue><elocation-id>e1000196</elocation-id><pub-id pub-id-type="pmcid">PMC2551736</pub-id><pub-id pub-id-type="pmid">18846206</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000196</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Network structure of cerebral cortex shapes functional connectivity on multiple time scales</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2007</year><volume>104</volume><issue>24</issue><fpage>10240</fpage><lpage>10245</lpage><pub-id pub-id-type="doi">10.1073/PNAS.0701519104/SUPPL_FILE/IMAGE1067.GIF</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name><name><surname>McIntosh</surname><given-names>A</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name></person-group><article-title>Key role of coupling, delay, and noise in resting brain fluctuations</article-title><source>Proc Natl Acad Sci U S A</source><year>2009</year><volume>106</volume><issue>25</issue><fpage>10302</fpage><lpage>10307</lpage><pub-id pub-id-type="pmcid">PMC2690605</pub-id><pub-id pub-id-type="pmid">19497858</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0901831106</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name></person-group><article-title>The human connectome: A structural description of the human brain</article-title><source>PLoS Computational Biology</source><year>2005</year><volume>1</volume><issue>4</issue><fpage>0245</fpage><lpage>0251</lpage><pub-id pub-id-type="pmcid">PMC1239902</pub-id><pub-id pub-id-type="pmid">16201007</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0010042</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>A</given-names></name><name><surname>Schreiber</surname><given-names>T</given-names></name></person-group><article-title>Information transfer in continuous processes</article-title><source>Physica D: Nonlinear Phenomena</source><year>2002</year><volume>166</volume><issue>1-2</issue><fpage>43</fpage><lpage>62</lpage></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bossomaier</surname><given-names>T</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Harré</surname><given-names>M</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name></person-group><article-title>An introduction to transfer entropy: Information flow in complex systems</article-title><year>2016</year><fpage>1</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-43222-9/COVER</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bertschinger</surname><given-names>N</given-names></name><name><surname>Olbrich</surname><given-names>E</given-names></name><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Jost</surname><given-names>J</given-names></name></person-group><chapter-title>Information and closure in systems theory</chapter-title><source>Explorations in the Complexity of Possible Life</source><conf-name>Proceedings of the 7th German Workshop of Artificial Life</conf-name><publisher-name>IOS Press</publisher-name><publisher-loc>Amsterdam, The Netherlands</publisher-loc><year>2006</year><fpage>9</fpage><lpage>21</lpage></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfante</surname><given-names>O</given-names></name><name><surname>Olbrich</surname><given-names>E</given-names></name><name><surname>Bertschinger</surname><given-names>N</given-names></name><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Jost</surname><given-names>J</given-names></name></person-group><article-title>Closure measures for coarse-graining of the tent map</article-title><source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source><year>2014</year><volume>24</volume><issue>1</issue><pub-id pub-id-type="pmid">24697398</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Absil</surname><given-names>PA</given-names></name><name><surname>Mahony</surname><given-names>R</given-names></name><name><surname>Sepulchre</surname><given-names>R</given-names></name></person-group><article-title>Riemannian Geometry of Grassmann Manifolds with a View on Algorithmic Computation</article-title><source>Acta Applicandae Mathematica</source><year>2004</year><volume>80</volume><issue>2</issue><fpage>199</fpage><lpage>220</lpage><comment>2004 80:2</comment><pub-id pub-id-type="doi">10.1023/B:ACAP.0000013855.14971.91</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bendokat</surname><given-names>T</given-names></name><name><surname>Zimmermann</surname><given-names>R</given-names></name><name><surname>Absil</surname><given-names>PA</given-names></name></person-group><article-title>A Grassmann manifold handbook: Basic geometry and computational aspects</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>arXiv:201113699</elocation-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><article-title>Linear dimensionality reduction: Survey, insights, and generalizations</article-title><source>The Journal of Machine Learning Research</source><year>2015</year><volume>16</volume><issue>1</issue><fpage>2859</fpage><lpage>2900</lpage></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelman</surname><given-names>A</given-names></name><name><surname>Tom</surname><given-names>T</given-names></name><name><surname>Arias</surname><given-names>TA</given-names></name><name><surname>Smith</surname><given-names>ST</given-names></name></person-group><article-title>The Geometry Of Algorithms Wwith Orthogonality Constraints</article-title><source>Society for Industrial and Applied Mathematics</source><year>1998</year><volume>20</volume><issue>2</issue><fpage>303</fpage><lpage>353</lpage></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathews</surname><given-names>VJ</given-names></name><name><surname>Xie</surname><given-names>Z</given-names></name></person-group><article-title>A stochastic gradient adaptive filter with gradient adaptive step size</article-title><source>IEEE transactions on Signal Processing</source><year>1993</year><volume>41</volume><issue>6</issue><fpage>2075</fpage><lpage>2087</lpage></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritter</surname><given-names>P</given-names></name><name><surname>Schirner</surname><given-names>M</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>The Virtual Brain Integrates Computational Modeling and Multimodal Neuroimaging</article-title><source>Brain Connectivity</source><year>2013</year><volume>3</volume><issue>2</issue><fpage>121</fpage><pub-id pub-id-type="pmcid">PMC3696923</pub-id><pub-id pub-id-type="pmid">23442172</pub-id><pub-id pub-id-type="doi">10.1089/brain.2012.0120</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assisi</surname><given-names>C</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name><name><surname>Kelso</surname><given-names>J</given-names></name></person-group><article-title>Synchrony and clustering in heterogeneous networks with global coupling and parameter dispersion</article-title><source>Physical Review Letters</source><year>2005</year><volume>94</volume><issue>1</issue><elocation-id>018106</elocation-id><pub-id pub-id-type="pmid">15698140</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>K</given-names></name><name><surname>Bezgin</surname><given-names>G</given-names></name><name><surname>Schirner</surname><given-names>M</given-names></name><name><surname>Ritter</surname><given-names>P</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><article-title>A macaque connectome for large-scale network simulations in TheVirtualBrain</article-title><source>Scientific Data</source><year>2019</year><volume>6</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><comment>2019 6:1</comment><pub-id pub-id-type="pmcid">PMC6637142</pub-id><pub-id pub-id-type="pmid">31316116</pub-id><pub-id pub-id-type="doi">10.1038/s41597-019-0129-z</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Rho</surname><given-names>Y</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Kötter</surname><given-names>R</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>Noise during Rest Enables the Exploration of the Brain’s Dynamic Repertoire</article-title><source>PLOS Computational Biology</source><year>2008</year><volume>4</volume><issue>10</issue><elocation-id>e1000196</elocation-id><pub-id pub-id-type="doi">10.1371/JOURNAL.PCBI.1000196</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falcon</surname><given-names>MI</given-names></name><name><surname>Riley</surname><given-names>JD</given-names></name><name><surname>Jirsa</surname><given-names>V</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name><name><surname>Shereen</surname><given-names>AD</given-names></name><name><surname>Elinor Chen</surname><given-names>S</given-names></name><etal/></person-group><article-title>The Virtual Brain: Modeling Biological Correlates of Recovery after Chronic Stroke</article-title><source>Frontiers in Neurology</source><year>2015</year><month>NOV</month><volume>6</volume><fpage>2</fpage><pub-id pub-id-type="pmcid">PMC4629463</pub-id><pub-id pub-id-type="pmid">26579071</pub-id><pub-id pub-id-type="doi">10.3389/fneur.2015.00228</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>B</given-names></name><name><surname>Hoel</surname><given-names>E</given-names></name></person-group><article-title>The emergence of informative higher scales in complex networks</article-title><source>Complexity</source><year>2020</year><volume>2020</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature neuroscience</source><year>2014</year><volume>17</volume><issue>11</issue><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="pmcid">PMC4433019</pub-id><pub-id pub-id-type="pmid">25151264</pub-id><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crutchfield</surname><given-names>JP</given-names></name></person-group><article-title>Between order and chaos</article-title><source>Nature Physics</source><year>2012</year><volume>8</volume><issue>1</issue><fpage>17</fpage><lpage>24</lpage></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shalizi</surname><given-names>CR</given-names></name><name><surname>Moore</surname><given-names>C</given-names></name></person-group><article-title>What is a macrostate? Subjective observations and objective dynamics</article-title><source>arXiv preprint</source><year>2003</year><elocation-id>cond-mat/0303625</elocation-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Moreno</surname><given-names>A</given-names></name><name><surname>Mossio</surname><given-names>M</given-names></name></person-group><source>Biological autonomy. A philo</source><year>2015</year></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montévil</surname><given-names>M</given-names></name><name><surname>Mossio</surname><given-names>M</given-names></name></person-group><article-title>Biological organisation as closure of constraints</article-title><source>Journal of theoretical biology</source><year>2015</year><volume>372</volume><fpage>179</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">25752259</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crutchfield</surname><given-names>JP</given-names></name></person-group><article-title>The calculi of emergence: computation, dynamics and induction</article-title><source>Physica D: Nonlinear Phenomena</source><year>1994</year><volume>75</volume><issue>1-3</issue><fpage>11</fpage><lpage>54</lpage></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Edelman</surname><given-names>GM</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name></person-group><article-title>Complexity and coherency: integrating information in the brain</article-title><source>Trends in cognitive sciences</source><year>1998</year><volume>2</volume><issue>12</issue><fpage>474</fpage><lpage>484</lpage><pub-id pub-id-type="pmid">21227298</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maschke</surname><given-names>C</given-names></name><name><surname>Duclos</surname><given-names>C</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>Jerbi</surname><given-names>K</given-names></name><name><surname>Blain-Moraes</surname><given-names>S</given-names></name></person-group><article-title>Aperiodic brain activity and response to anesthesia vary in disorders of consciousness</article-title><source>NeuroImage</source><year>2023</year><volume>275</volume><elocation-id>120154</elocation-id><pub-id pub-id-type="pmid">37209758</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balduzzi</surname><given-names>D</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><article-title>Integrated information in discrete dynamical systems: Motivation and theoretical framework</article-title><source>PLoS Computational Biology</source><year>2008</year><volume>4</volume><issue>6</issue><pub-id pub-id-type="doi">10.1371/JOURNAL.PCBI.1000091</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Rosas</surname><given-names>F</given-names></name><name><surname>Carhart-Harris</surname><given-names>RL</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name></person-group><article-title>Beyond integrated information: A taxonomy of information dynamics phenomena</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:190902297</elocation-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schartner</surname><given-names>M</given-names></name><name><surname>Seth</surname><given-names>A</given-names></name><name><surname>Noirhomme</surname><given-names>Q</given-names></name><name><surname>Boly</surname><given-names>M</given-names></name><name><surname>Bruno</surname><given-names>MA</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><etal/></person-group><article-title>Complexity of multi-dimensional spontaneous EEG decreases during propofol induced general anaesthesia</article-title><source>PLoS ONE</source><year>2015</year><volume>10</volume><issue>8</issue><pub-id pub-id-type="pmcid">PMC4529106</pub-id><pub-id pub-id-type="pmid">26252378</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0133532</pub-id></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schartner</surname><given-names>MM</given-names></name><name><surname>Pigorini</surname><given-names>A</given-names></name><name><surname>Gibbs</surname><given-names>SA</given-names></name><name><surname>Arnulfo</surname><given-names>G</given-names></name><name><surname>Sarasso</surname><given-names>S</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name><etal/></person-group><article-title>Global and local complexity of intracranial EEG decreases during NREM sleep</article-title><source>Neuroscience of Consciousness</source><year>2020</year><volume>2017</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC6007155</pub-id><pub-id pub-id-type="pmid">30042832</pub-id><pub-id pub-id-type="doi">10.1093/nc/niw022</pub-id></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timme</surname><given-names>NM</given-names></name><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>Bennett</surname><given-names>N</given-names></name><name><surname>Ripp</surname><given-names>M</given-names></name><name><surname>Lautzenhiser</surname><given-names>E</given-names></name><name><surname>Beggs</surname><given-names>JM</given-names></name></person-group><article-title>Criticality maximizes complexity in neural tissue</article-title><source>Frontiers in physiology</source><year>2016</year><volume>7</volume><fpage>425</fpage><pub-id pub-id-type="pmcid">PMC5037237</pub-id><pub-id pub-id-type="pmid">27729870</pub-id><pub-id pub-id-type="doi">10.3389/fphys.2016.00425</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Christensen</surname><given-names>K</given-names></name><name><surname>Moloney</surname><given-names>NR</given-names></name></person-group><source>Complexity and criticality</source><publisher-name>World Scientific Publishing Company</publisher-name><year>2005</year><volume>1</volume></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michel</surname><given-names>CM</given-names></name><name><surname>Koenig</surname><given-names>T</given-names></name></person-group><article-title>EEG microstates as a tool for studying the temporal dynamics of whole-brain neuronal networks: a review</article-title><source>Neuroimage</source><year>2018</year><volume>180</volume><fpage>577</fpage><lpage>593</lpage><pub-id pub-id-type="pmid">29196270</pub-id></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cabral</surname><given-names>J</given-names></name><name><surname>Castaldo</surname><given-names>F</given-names></name><name><surname>Vohryzek</surname><given-names>J</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Bick</surname><given-names>C</given-names></name><name><surname>Lambiotte</surname><given-names>R</given-names></name><etal/></person-group><article-title>Metastable oscillatory modes emerge from synchronization in the brain spacetime connectome</article-title><source>Communications Physics</source><year>2022</year><volume>5</volume><issue>1</issue><fpage>184</fpage><pub-id pub-id-type="pmcid">PMC7615562</pub-id><pub-id pub-id-type="pmid">38288392</pub-id><pub-id pub-id-type="doi">10.1038/s42005-022-00950-y</pub-id></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luppi</surname><given-names>AI</given-names></name><name><surname>Mediano</surname><given-names>PA</given-names></name><name><surname>Rosas</surname><given-names>FE</given-names></name><name><surname>Harrison</surname><given-names>DJ</given-names></name><name><surname>Carhart-Harris</surname><given-names>RL</given-names></name><name><surname>Bor</surname><given-names>D</given-names></name><etal/></person-group><article-title>What it is like to be a bit: an integrated information decomposition account of emergent mental phenomena</article-title><source>Neuroscience of consciousness</source><year>2021</year><volume>2021</volume><issue>2</issue><elocation-id>niab027</elocation-id><pub-id pub-id-type="pmcid">PMC8600547</pub-id><pub-id pub-id-type="pmid">34804593</pub-id><pub-id pub-id-type="doi">10.1093/nc/niab027</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>AY</given-names></name><name><surname>Biehl</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Kanai</surname><given-names>R</given-names></name></person-group><article-title>Information closure theory of consciousness</article-title><source>Frontiers in Psychology</source><year>2020</year><volume>11</volume><elocation-id>505035</elocation-id><pub-id pub-id-type="pmcid">PMC7374725</pub-id><pub-id pub-id-type="pmid">32760320</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2020.01504</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoel</surname><given-names>EP</given-names></name><name><surname>Albantakis</surname><given-names>L</given-names></name><name><surname>Marshall</surname><given-names>W</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><article-title>Can the macro beat the micro? Integrated information across spatiotemporal scales</article-title><source>Neuroscience of Consciousness</source><year>2016</year><volume>2016</volume><issue>1</issue><elocation-id>niw012</elocation-id><pub-id pub-id-type="pmcid">PMC6367968</pub-id><pub-id pub-id-type="pmid">30788150</pub-id><pub-id pub-id-type="doi">10.1093/nc/niw012</pub-id></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griebenow</surname><given-names>R</given-names></name><name><surname>Klein</surname><given-names>B</given-names></name><name><surname>Hoel</surname><given-names>E</given-names></name></person-group><article-title>Finding the right scale of a network: efficient identification of causal emergence through spectral clustering</article-title><source>arXiv preprint</source><year>2019</year><elocation-id>arXiv:190807565</elocation-id></element-citation></ref><ref id="R95"><label>95</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J</given-names></name></person-group><article-title>Causal inference in statistics: An overview</article-title><source>Statistics Surveys</source><year>2009</year></element-citation></ref><ref id="R96"><label>96</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J</given-names></name></person-group><source>Trygve Haavelmo and the Emergence of Causal Calculus</source><year>2015</year></element-citation></ref><ref id="R97"><label>97</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>B</given-names></name><name><surname>Hoel</surname><given-names>E</given-names></name><name><surname>Swain</surname><given-names>A</given-names></name><name><surname>Griebenow</surname><given-names>R</given-names></name><name><surname>Levin</surname><given-names>M</given-names></name></person-group><article-title>Evolution and emergence: higher order information structure in protein interactomes across the tree of life</article-title><source>Integrative Biology</source><year>2021</year><pub-id pub-id-type="pmid">34933345</pub-id></element-citation></ref><ref id="R98"><label>98</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>ET</given-names></name></person-group><article-title>Information theory and statistical mechanics</article-title><source>Physical review</source><year>1957</year><volume>106</volume><issue>4</issue><fpage>620</fpage></element-citation></ref><ref id="R99"><label>99</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>WS</given-names></name></person-group><article-title>A heterarchy of values determined by the topology of nervous nets</article-title><source>The bulletin of mathematical biophysics</source><year>1945</year><volume>7</volume><fpage>89</fpage><lpage>93</lpage><pub-id pub-id-type="pmid">21006853</pub-id></element-citation></ref><ref id="R100"><label>100</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Priesemann</surname><given-names>V</given-names></name></person-group><article-title>Bits from brains for biologically inspired computing</article-title><source>Frontiers Robotics AI</source><year>2015</year><volume>2</volume><month>MAR</month><pub-id pub-id-type="doi">10.3389/FROBT.2015.00005</pub-id></element-citation></ref><ref id="R101"><label>101</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>György Buzsáki</surname><given-names>M</given-names></name></person-group><source>The brain from inside out</source><publisher-name>Oxford University Press</publisher-name><year>2019</year></element-citation></ref><ref id="R102"><label>102</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessoa</surname><given-names>L</given-names></name><name><surname>Medina</surname><given-names>L</given-names></name><name><surname>Desfilis</surname><given-names>E</given-names></name></person-group><article-title>Refocusing neuroscience: moving away from mental categories and towards complex behaviours</article-title><source>Philosophical Transactions of the Royal Society B</source><year>2022</year><volume>377</volume><issue>1844</issue><pub-id pub-id-type="pmcid">PMC8710886</pub-id><pub-id pub-id-type="pmid">34957851</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0534</pub-id></element-citation></ref><ref id="R103"><label>103</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertschinger</surname><given-names>N</given-names></name><name><surname>Olbrich</surname><given-names>E</given-names></name><name><surname>Ay</surname><given-names>N</given-names></name><name><surname>Jost</surname><given-names>J</given-names></name></person-group><article-title>Autonomy: an information theoretic perspective</article-title><source>Biosystems</source><year>2007</year><pub-id pub-id-type="pmid">17897774</pub-id></element-citation></ref><ref id="R104"><label>104</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><article-title>Strong and weak principles of neural dimension reduction</article-title><source>arXiv preprint</source><year>2020</year><pub-id pub-id-type="arxiv">arXiv:201108088</pub-id></element-citation></ref><ref id="R105"><label>105</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozari</surname><given-names>E</given-names></name><name><surname>Stiso</surname><given-names>J</given-names></name><name><surname>Caciagli</surname><given-names>L</given-names></name><name><surname>Cornblath</surname><given-names>EJ</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Bertolero</surname><given-names>MA</given-names></name><etal/></person-group><article-title>Is the brain macroscopically linear? A system identification of resting state dynamics</article-title><source>bioRxiv</source><year>2020</year><elocation-id>2020.12.21.423856</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.21.423856</pub-id></element-citation></ref><ref id="R106"><label>106</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wold</surname><given-names>H</given-names></name></person-group><source>A study in the analysis of stationary time series</source><publisher-name>Almqvist &amp; Wiksell</publisher-name><year>1938</year></element-citation></ref><ref id="R107"><label>107</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hannan</surname><given-names>EJ</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name></person-group><source>The statistical theory of linear systems</source><publisher-name>SIAM</publisher-name><year>2012</year></element-citation></ref><ref id="R108"><label>108</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Semedo</surname><given-names>JD</given-names></name><name><surname>Zandvakili</surname><given-names>A</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Byron</surname><given-names>MY</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name></person-group><article-title>Cortical areas interact through a communication subspace</article-title><source>Neuron</source><year>2019</year><volume>102</volume><issue>1</issue><fpage>249</fpage><lpage>259</lpage><pub-id pub-id-type="pmcid">PMC6449210</pub-id><pub-id pub-id-type="pmid">30770252</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.01.026</pub-id></element-citation></ref><ref id="R109"><label>109</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kraskov</surname><given-names>A</given-names></name></person-group><source>Synchronization and interdependence measures and their applications to the electroencephalogram of epilepsy patients and clustering of data</source><publisher-name>NIC-Secretariat, Research Centre Jülich</publisher-name><year>2004</year></element-citation></ref><ref id="R110"><label>110</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lizier</surname><given-names>JT</given-names></name></person-group><article-title>JIDT: An information-theoretic toolkit for studying the dynamics of complex systems</article-title><source>Frontiers in Robotics and AI</source><year>2014</year><volume>1</volume><fpage>11</fpage></element-citation></ref><ref id="R111"><label>111</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wollstadt</surname><given-names>P</given-names></name><name><surname>Lizier</surname><given-names>JT</given-names></name><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Finn</surname><given-names>C</given-names></name><name><surname>Martinez-Zarzuela</surname><given-names>M</given-names></name><name><surname>Mediano</surname><given-names>P</given-names></name><etal/></person-group><article-title>IDTxl: The Information Dynamics Toolkit xl: a Python package for the efficient analysis of multivariate information dynamics in networks</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv: 180710459</elocation-id></element-citation></ref><ref id="R112"><label>112</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name><name><surname>Lindner</surname><given-names>M</given-names></name><name><surname>Pipa</surname><given-names>G</given-names></name></person-group><article-title>Transfer entropy—a model-free measure of effective connectivity for the neurosciences</article-title><source>Journal of computational neuroscience</source><year>2011</year><volume>30</volume><issue>1</issue><fpage>45</fpage><lpage>67</lpage><pub-id pub-id-type="pmcid">PMC3040354</pub-id><pub-id pub-id-type="pmid">20706781</pub-id><pub-id pub-id-type="doi">10.1007/s10827-010-0262-3</pub-id></element-citation></ref><ref id="R113"><label>113</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vicente</surname><given-names>R</given-names></name><name><surname>Wibral</surname><given-names>M</given-names></name></person-group><chapter-title>Efficient estimation of information transfer</chapter-title><source>Directed Information Measures in Neuroscience</source><publisher-name>Springer</publisher-name><year>2014</year><fpage>37</fpage><lpage>58</lpage></element-citation></ref><ref id="R114"><label>114</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borisovsky</surname><given-names>P</given-names></name><name><surname>Eremeev</surname><given-names>A</given-names></name></person-group><article-title>A Study on Performance of the (1+1)-Evolutionary Algorithm</article-title><year>2002</year><comment>undefined</comment></element-citation></ref><ref id="R115"><label>115</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Granger causality and transfer entropy Are equivalent for gaussian variables</article-title><source>Physical Review Letters</source><year>2009</year><volume>103</volume><issue>23</issue><pub-id pub-id-type="pmid">20366183</pub-id></element-citation></ref><ref id="R116"><label>116</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Bossomaier</surname><given-names>T</given-names></name></person-group><article-title>Transfer Entropy as a Log-likelihood Ratio</article-title><source>Physical Review Letters</source><year>2012</year><pub-id pub-id-type="pmid">23030125</pub-id></element-citation></ref><ref id="R117"><label>117</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hlavackova-Schindler</surname><given-names>K</given-names></name><name><surname>Hlaváčková-Schindler</surname><given-names>K</given-names></name></person-group><article-title>Equivalence of Granger Causality and Transfer Entropy: A Generalization Theory of Neural Networks View project Equivalence of Granger Causality and Transfer Entropy: A Generalization</article-title><source>Applied Mathematical Sciences</source><year>2011</year><volume>5</volume><issue>73</issue><fpage>3637</fpage><lpage>3648</lpage></element-citation></ref><ref id="R118"><label>118</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granger</surname><given-names>CWJ</given-names></name></person-group><article-title>Investigating Causal Relations by Econometric Models and Cross-spectral Methods</article-title><source>Econometrica</source><year>1969</year><volume>37</volume><issue>3</issue><fpage>424</fpage><pub-id pub-id-type="doi">10.2307/1912791</pub-id></element-citation></ref><ref id="R119"><label>119</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geweke</surname><given-names>J</given-names></name></person-group><article-title>Measurement of linear dependence and feedback between multiple time series</article-title><source>Journal of the American Statistical Association</source><year>1982</year><volume>77</volume><issue>378</issue><fpage>304</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1080/01621459.1982.10477803</pub-id></element-citation></ref><ref id="R120"><label>120</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Multivariate Granger causality and generalized variance</article-title><source>Physical Review E</source><year>2010</year><volume>81</volume><issue>4</issue><elocation-id>041907</elocation-id><pub-id pub-id-type="pmid">20481753</pub-id></element-citation></ref><ref id="R121"><label>121</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohsenipour</surname><given-names>AA</given-names></name></person-group><article-title>On the Distribution of Quadratic Expressions in Various Types of Random Vectors. The University of Western Ontario</article-title><source>Electronic Thesis and Dissertation Repository</source><year>2012</year><volume>955</volume></element-citation></ref><ref id="R122"><label>122</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutknecht</surname><given-names>AJ</given-names></name><name><surname>Barnett</surname><given-names>L</given-names></name></person-group><article-title>Sampling distribution for single-regression Granger causality estimators</article-title><year>2023</year><volume>4</volume></element-citation></ref><ref id="R123"><label>123</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><year>2014</year><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/J.JNEUMETH.2013.10.018</pub-id></element-citation></ref><ref id="R124"><label>124</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>GT</given-names></name></person-group><article-title>The factorization of matricial spectral densities</article-title><source>SIAM J Appl Math</source><year>1972</year><volume>23</volume><issue>4</issue><fpage>420</fpage><lpage>426</lpage></element-citation></ref><ref id="R125"><label>125</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhamala</surname><given-names>M</given-names></name><name><surname>Rangarajan</surname><given-names>G</given-names></name><name><surname>Ding</surname><given-names>M</given-names></name></person-group><article-title>Estimating Granger causality from Fourier and wavelet transforms of time series data</article-title><source>Phys Rev Lett</source><year>2008</year><volume>100</volume><elocation-id>018701</elocation-id><pub-id pub-id-type="pmid">18232831</pub-id></element-citation></ref><ref id="R126"><label>126</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rozanov</surname><given-names>YA</given-names></name></person-group><source>Stationary Random Processes</source><publisher-name>Holden-Day</publisher-name><publisher-loc>San Francisco</publisher-loc><year>1967</year></element-citation></ref><ref id="R127"><label>127</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname><given-names>PA</given-names></name><name><surname>Purdon</surname><given-names>PL</given-names></name></person-group><article-title>A study of problems encountered in Granger causality analysis from a neuroscience perspective</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><issue>34</issue><fpage>E7063</fpage><lpage>E7072</lpage><pub-id pub-id-type="pmcid">PMC5576801</pub-id><pub-id pub-id-type="pmid">28778996</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1704663114</pub-id></element-citation></ref><ref id="R128"><label>128</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Barrett</surname><given-names>AB</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Misunderstandings regarding the application of Granger causality in neuroscience</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2018</year><volume>115</volume><issue>29</issue><fpage>E6676</fpage><lpage>E6677</lpage><pub-id pub-id-type="pmcid">PMC6055162</pub-id><pub-id pub-id-type="pmid">29991604</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1714497115</pub-id></element-citation></ref><ref id="R129"><label>129</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solo</surname><given-names>V</given-names></name></person-group><article-title>State-Space Analysis of Granger-Geweke Causality Measures with Application to fMRI</article-title><source>Neural Computation</source><year>2016</year><volume>28</volume><issue>5</issue><fpage>914</fpage><lpage>949</lpage><pub-id pub-id-type="pmcid">PMC5572774</pub-id><pub-id pub-id-type="pmid">26942749</pub-id><pub-id pub-id-type="doi">10.1162/NECO_a_00828</pub-id></element-citation></ref><ref id="R130"><label>130</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name><name><surname>McIntosh</surname><given-names>AR</given-names></name></person-group><article-title>Resting brains never rest: computational insights into potential cognitive architectures</article-title><source>Trends in neurosciences</source><year>2013</year><volume>36</volume><issue>5</issue><fpage>268</fpage><lpage>274</lpage><pub-id pub-id-type="pmid">23561718</pub-id></element-citation></ref><ref id="R131"><label>131</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>YC</given-names></name></person-group><article-title>Differential geometry of Grassmann manifolds</article-title><source>Proceedings of the National Academy of Sciences</source><year>1967</year><volume>57</volume><issue>3</issue><fpage>589</fpage><lpage>594</lpage><pub-id pub-id-type="pmcid">PMC335549</pub-id><pub-id pub-id-type="pmid">16591504</pub-id><pub-id pub-id-type="doi">10.1073/pnas.57.3.589</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Author summary</title></caption><p>Understanding how complex neural systems give rise to emergent macroscopic patterns is a central challenge in neuroscience. Emergence, where macroscopic structures appear from underlying microscopic interactions, plays a crucial role in brain function, yet identifying the specific dynamics involved remains elusive. Traditionally, methods have quantified the extent of emergence but have struggled to pinpoint the emergent dynamical structure itself. In this study, we develop and apply a method, based on a quantity called Dynamical Independence (DI), which simultaneously captures the extent of emergence and reveals the underlying dynamical structure in neurophysiological data. Using a minimal 5-node biophysical neural model, we explore how a balance between functional integration and segregation—two key organisational principles in the brain—affects emergent macroscopic dynamics. Our results show that a finely balanced system produces highly localised, coherent macroscopic structures, while extreme deviations lead to more distributed, less localised dynamics. This work provides a computational framework for identifying emergent dynamical structure in both theoretical models and potentially in empirical brain data, advancing our understanding of the brain’s complex organisation across higher-order scales.</p></boxed-text><fig id="F1" position="float"><label>Fig 1</label><caption><title>Network architecture of a Stefanescu-Jirsa model.</title><p><italic>K</italic><sub>12</sub> is the inhibitory-to-excitatory mass connectivity, <italic>K</italic><sub>21</sub> is the excitatory-to-inhibitory connection, and <italic>K</italic><sub>11</sub> is the excitatory-to-excitatory connectivity. The left-most panel indicates that each neural mass model, consisting of both, excitatory and inhibitory neural masses represents a single node in a region-based brain network simulation. Going from the central panel to the right-most indicates the reduced model of the mean-field approximations representing the ensemble activity on the excitatory (pink) and inhibitory (green) neural masses, respectively.</p></caption><graphic xlink:href="EMS199608-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><title>Structural connectivities defining the 5-node brain network models implemented in this study.</title><p><italic>(a)</italic> the coupled connectivity defined by the weights and tract-lengths matrices, and <italic>(b)</italic> the uncoupled connectivity.</p></caption><graphic xlink:href="EMS199608-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><title>Minimal DD values of emergent <italic>n</italic>-macros in the 5-node network with local dynamics defined by a SJ3D NMM.</title><p>Each matrix presents a parameter sweep across global coupling (y-axis) and dynamical noise (x-axis). The top row shows DD values for 2-macros and 3-macros in the coupled brain network. The bottom row presents the corresponding DD values for the uncoupled brain network. Darker colours indicate higher DD values, while lighter colours reflect lower DD values.</p></caption><graphic xlink:href="EMS199608-f003"/></fig><fig id="F4" position="float"><label>Fig 4</label><caption><title>The single-node contribution to emergent 2-macros across simulations.</title><p>Global coupling varies along the <italic>y</italic>-axis, and dynamical noise varies along the <italic>x</italic>-axis. Here, and on the subsequent figures identifying node contribution values on both axes are plotted on a logarithmic scale. Higher contributions are indicated by darker colours. The degree of localisation of single-node contributions within the emergent 2-macro is most distinct at a balance point between functional integration and segregation.</p></caption><graphic xlink:href="EMS199608-f004"/></fig><fig id="F5" position="float"><label>Fig 5</label><caption><title>The single-node contribution to emergent 3-macros across simulations.</title><p>Global coupling varies along the <italic>y</italic>-axis, and dynamical noise varies along the <italic>x</italic>-axis. Higher contributions are indicated by darker colours. The localisation of single-node contributions within the emergent 3-macro is evident at balanced points between functional integration and segregation, with a loss of localisation in extreme regimes.</p></caption><graphic xlink:href="EMS199608-f005"/></fig><fig id="F6" position="float"><label>Fig 6</label><caption><title>The single-node contribution to emergent 3-macros across simulations.</title><p>Global coupling varies along the <italic>y</italic>-axis, and dynamical noise varies along the <italic>x</italic>-axis. Higher contributions are indicated by darker colours. The contributions of nodes to the emergent 3-macros vary randomly across the parameter space, indicating the absence of distinct localisation in the uncoupled network.</p></caption><graphic xlink:href="EMS199608-f006"/></fig><fig id="F7" position="float"><label>Fig 7</label><caption><title>Wilcoxon rank sum test for single-node contribution.</title><p>(<italic>left</italic>) Emergent 2-macro and (<italic>right</italic>) Emergent 3-macro, in the coupled network compared to the uncoupled network across a parameter sweep of global coupling and dynamical noise</p></caption><graphic xlink:href="EMS199608-f007"/></fig></floats-group></article>