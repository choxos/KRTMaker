<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200366</article-id><article-id pub-id-type="doi">10.1101/2024.11.19.624356</article-id><article-id pub-id-type="archive">PPR942623</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>The “Ocular Response Function” for encoding and decoding oculomotor related neural activity</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gehmacher</surname><given-names>Quirin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Schubert</surname><given-names>Juliane</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kaltenmaier</surname><given-names>Aaron</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Weisz</surname><given-names>Nathan</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Press</surname><given-names>Clare</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <postal-code>WC1H 0AP</postal-code>, <country country="GB">United Kingdom</country></aff><aff id="A2"><label>2</label>Department of Imaging Neuroscience, UCL Queen Square Institute of Neurology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <postal-code>WC1N 3AR</postal-code>, <country country="GB">United Kingdom</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gs8cd61</institution-id><institution>Paris-Lodron-University of Salzburg</institution></institution-wrap>, Department of Psychology, Centre for Cognitive Neuroscience, <city>Salzburg</city>, <country country="AT">Austria</country></aff><aff id="A4"><label>4</label>Neuroscience Institute, Christian Doppler University Hospital, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03z3mg085</institution-id><institution>Paracelsus Medical University</institution></institution-wrap>, <city>Salzburg</city>, <country country="AT">Austria</country></aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding Author: <email>q.gehmacher@ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Oculomotor activity provides critical insights into cognition and health, with growing evidence demonstrating its involvement in various cognitive functions such as attention, memory, and sensory processing. Furthermore, eye movements are emerging as significant indicators of psychopathologies and neurological disorders, including schizophrenia, dementia, depression, and tinnitus. Despite its crucial importance across domains, the role of oculomotion has been underexplored in neuroimaging studies - largely due to methodological challenges. These challenges often involve treating eye movements as artefacts to be removed from the neural signal. While useful for data cleaning, this approach risks discarding valuable information about oculomotor control. An alternative is to directly model these signals. Using recently established time-resolved regression methods, we apply and extend this approach to present a unified framework we term ‘Ocular Response Functions’ (ORFs). Using simultaneous magnetoencephalography (MEG) and eye-tracking during the resting-state, we derive ORFs that characterise the neural signatures of distinct oculomotor events, specifically saccades, blinks, and pupil dilation. We demonstrate how this framework can be used to model the relationship between ocular action and neural activity (encoding) and, conversely, to reconstruct ocular events from brain activity. We further validate this approach by applying resting-state derived ORFs to a passive listening task, showing they can reveal oculomotor contributions to task-related neural processing. By providing an accessible framework for examining the interplay between eye movements and neural processes, we offer a powerful tool for research that has potential applications in clinical neuroscience.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Oculomotor activity offers a unique opportunity for understanding key aspects of cognition and health <sup><xref ref-type="bibr" rid="R1">1</xref></sup>, with recent demonstrations that it plays a key role in a wide range of cognitive functions, including attention, memory, visual, and auditory processing <sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R15">15</xref></sup>. Relatedly, it is increasingly being recognised as an important indicator of psychopathology and neurological disorders, with links to conditions such as schizophrenia <sup><xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup>, dementia <sup><xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R26">26</xref></sup>, and depression <sup><xref ref-type="bibr" rid="R27">27</xref>–<xref ref-type="bibr" rid="R30">30</xref></sup>, and even phantom perceptions like tinnitus <sup><xref ref-type="bibr" rid="R31">31</xref>–<xref ref-type="bibr" rid="R33">33</xref></sup>.</p><p id="P3">Despite its importance, studying the contribution of oculomotor activity has proven difficult methodologically. While there is some history of studying eye movement-related potentials, primarily via EEG, eye movements are still predominantly treated as confounding variables that can contaminate the neural signal. This has typically led to their exclusion from the neural signal, either by excluding affected time periods or correcting for their influence via methods like Independent Component Analysis (ICA). This common practice risks discarding meaningful neural activity related to oculomotor control, such that understanding of its role in various functions has therefore been limited. A powerful alternative, therefore, is to directly model the relationship between oculomotor events and the neural signal. This approach builds on a rich history establishing that neural potentials time-locked to eye movements are not mere artifacts, but systematically support a host of visuomotor and cognitive functions (see <sup><xref ref-type="bibr" rid="R34">34</xref></sup>, for a review). Building on this notion, modern regression methods have made it possible to model the neural correlates of various oculomotor processes as they unfold during naturalistic tasks <sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>.</p><p id="P4">However, a key challenge in interpreting these models arises when eye movements are correlated with the task events themselves, rendering difficult the disentanglement of intrinsic oculomotor activity from stimulus processing. Our approach addresses this by first deriving a 'pure' mapping of the eye-brain relationship from task-free, stimulus-free resting-state data. By combining this stimulus-free modeling with the high precision of magnetoencephalography (MEG), our work offers the unique ability to create detailed spatiotemporal maps of these oculomotor processes. We use this integrated approach to derive a comprehensive set of ‘Ocular Response Functions’ (ORFs) - linking saccades, blinks, and pupil dilation to distinct neural signals. We then demonstrate how this framework provides novel insights, first by establishing the ORFs at rest, and then by using them to probe oculomotor contributions during a passive listening task, highlighting its potential for uncovering cognitive insights in experimental settings. By providing a robust framework for examining the interplay between eye movements and neural processes, our method opens new avenues for both research and clinical applications.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P5">In the following, we present the ‘Ocular Response Function’ (ORF) method and organise the results into three sections. First, we outline the general steps for estimating ORFs, providing a schematic overview of the rationale and methodology behind their calculation, and how they can be used to predict brain activity from ocular action (encoding) or ocular action from brain data (reconstruction). Second, we show that ORFs account for variance across distinct brain regions - each characterised by unique temporal profiles, both in data with and without oculomuscular artefact correction (ICA and no-ICA). By convolving resting-state data with the corresponding ORFs, we demonstrate that brain activity can be predicted above chance from ocular action and vice versa. Third, we extend the ORF method to a passive listening task, demonstrating the approach's generalisability and offering insights into how oculomotor processes contribute to neural activity during cognitive tasks.</p><sec id="S3"><title>The ‘Ocular Response Function’</title><p id="P6">To establish a time-continuous and spatially resolved relationship between ocular features and neural activity, we first projected the continuous resting-state MEG data into anatomical source space using linearly constrained minimum variance (LCMV) beamformers. Then, for each source-space voxel, we fitted a forward encoding model using a deconvolution algorithm based on boosting <sup><xref ref-type="bibr" rid="R37">37</xref></sup> (see <xref ref-type="fig" rid="F1">Fig. 1A</xref>) as implemented in the Eelbrain toolkit <sup><xref ref-type="bibr" rid="R38">38</xref></sup>. This model estimated the unique relationship between each of our three ocular features (pupil dilation, blinks, and saccades) and the neural activity at that specific brain location (see <xref ref-type="fig" rid="F1">Fig. 1B</xref>). Fitting these models to stimulus-free resting-state data allows for the estimation of 'pure' eye-brain response functions, or ORFs, in the absence of experimental confounds (see <xref ref-type="fig" rid="F1">Fig. 1C</xref>). This process yields a unique ORF for each eye feature at each voxel, reflecting the strength of the relationship at different time lags. For example, a value at +100 ms reflects the brain's response following an eye event, whereas a value at -100 ms reflects neural activity preceding that same event.</p><p id="P7">Once the ORFs are estimated on training data, their performance is evaluated on held-out test data using a cross-validation procedure. For reconstruction, the process is reversed by flipping the temporal axis of the ORFs. This computationally efficient backward reconstruction approach is instrumental in understanding how neural patterns can be used to infer ocular behaviour, thereby contributing to a bidirectional understanding of brain-eye interactions. Models were fitted to source-projected data that were derived from MEG recordings either with or without prior ICA-based artefact correction - providing the opportunity to further differentiate between contributions of oculomuscular and oculomotor neural activity to M/EEG recordings. To identify the most relevant response components across space and time, we applied principal component analysis (PCA) to the ORFs from all voxels. For evaluation and application to other experimental tasks, we first convolve each ocular feature with the corresponding source-projected ORF - in a process analogous to the hemodynamic response function (HRF) convolution used in fMRI analysis - to generate a time course of predicted brain activity for each voxel, based on the respective eye tracking data. We then correlate these predicted time courses with the actual source-projected brain recordings to assess encoding performance (see <xref ref-type="fig" rid="F2">Fig. 2A</xref>). Conversely, by convolving the source-projected brain activity with the time-flipped ORFs, we generate a predicted time course of each ocular feature, which is then correlated with the actual eye-tracking recordings to assess reconstruction performance (see <xref ref-type="fig" rid="F2">Fig. 2B</xref>).</p><p id="P8">To summarise the high-dimensional results and identify the dominant patterns of eye-brain coupling, we applied principal component analysis (PCA) to each subject's source-projected ORFs. The resulting spatial weight maps for the principal components were then averaged across all participants. For our quantitative analysis, we identified the location of maximum weight within these group-averaged maps. This common, pre-defined location was then used to extract the encoding effects from each individual participant's data. In the following, we present distinct ORF time courses for the first component, along with encoding effects at the locations of maximum weight. Additionally, decoding effects are demonstrated by predicting eye movement patterns using all source-projected ORFs, either collectively or weighted by the PCA matrices of the first component (results for all other component-based models can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Material, Supplementary Figures</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>).</p></sec><sec id="S4"><title>Ocular response functions: encoding and reconstruction during the resting state</title><p id="P9">First, we established ORFs by modeling resting-state brain activity from three ocular features: pupil dilation, blinks, and saccades. To visualise the dominant spatiotemporal patterns in these relationships, we used PCA, applying it to the source-space ORFs from data with and without oculomuscular artifact correction (ICA and no-ICA, respectively).</p><p id="P10">In brain data without ICA preprocessing, the first principal component (PC1) for each ocular feature was generally characterised by strong loadings over frontocentral or early sensory regions. After applying ICA, these loadings typically maintained the early sensory responses, alongside now clearer cerebellar contributions, revealing patterns that were likely previously obscured by artifacts from eyeball rotation and related muscle activity. Below, we present the temporal and spatial patterns for PC1 for both preprocessing strategies.</p><sec id="S5"><title>No-ICA Preprocessing</title><p id="P11">For pupil dilation, the first component (explained variance: <italic>μ</italic> = 31.7%, sd = 8.1%) was characterised by primary loadings over posterior medial regions, including the precuneus. The associated time course was dominated by a large peak at approximately -400 ms, suggesting that preparatory activity in these regions precedes changes in pupil diameter (encoding at the group-defined peak voxel of this component, <italic>r</italic> = 0.03, <italic>t</italic>(28) = 6.26, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.16; E <xref ref-type="fig" rid="F3">Fig. 3A</xref>). For eye blinks, PC1 (explained variance: <italic>μ</italic> = 61.1%, <italic>sd</italic> = 20.2%) showed maximal loadings in frontocentral regions, consistent with oculomuscular artifacts, and peaked near-instantaneously around 0 ms (encoding at the component's peak, <italic>r</italic> = 0.35, <italic>t</italic>(28) = 13.22, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.45; see <xref ref-type="fig" rid="F3">Fig. 3B</xref>). Finally, for saccades, PC1 (explained variance: <italic>μ</italic> = 42.6%, sd = 20.2%) had widespread loadings with a maximum in occipital visual areas, including the calcarine sulcus. Its time course showed a sharp peak at approximately +100 ms, indicating a strong visual response immediately following saccade execution (encoding at the peak voxel, <italic>r</italic> = 0.08, <italic>t</italic>(28) = 7.69, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.43; see <xref ref-type="fig" rid="F3">Fig. 3C</xref>).</p></sec><sec id="S6"><title>ICA Preprocessing</title><p id="P12">After ICA correction, the spatial topographies no longer reflected any frontocentral loadings, but retained the sensory sources which were accompanied now by clearer cerebellar loadings. For pupil dilation, PC1 (explained variance: <italic>μ</italic> = 31.9%, <italic>sd</italic> = 9.1) now maximally loaded over occipital visual areas. The time course showed a prominent peak around -300 ms, suggesting that activity in the visual cortex precedes upcoming changes in pupil size (encoding effect, <italic>r</italic> = 0.03, <italic>t</italic>(28) = 6.77, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.26; see <xref ref-type="fig" rid="F3">Fig. 3D</xref>). For eye blinks, PC1 (explained variance: <italic>μ</italic> = 45.1%, sd = 14.3%) was now localised to the cerebellum. The time course exhibited an initial near-instantaneous peak, but now also showed a more pronounced second activation around 150-200 ms, suggesting a complex, delayed response to blinks after muscular artifacts were suppressed (encoding effect: <italic>r</italic> = 0.07, <italic>t</italic>(28) = 9.45, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.76; see <xref ref-type="fig" rid="F3">Fig. 3E</xref>). For saccades, PC1 (explained variance: <italic>μ</italic> = 41.4%, sd = 20.3%) showed a similar pattern to the no-ICA data, with primary loadings in occipital visual areas and a response peak around +100 ms. The stability of this occipital component post-ICA strongly suggests it reflects a robust neural response to saccadic reafference, clearly separable from frontal muscle artifacts (encoding effect: <italic>r</italic> = 0.06, <italic>t</italic>(28) = 9.47, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.76; see <xref ref-type="fig" rid="F3">Fig. 3F</xref>).</p><p id="P13">The spatial and temporal patterns for the second and third principal components generally showed loadings over parietal and sensory regions. For detailed results and visualisations for these components, please refer to the <xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref> (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figures</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig. S1&amp;2</xref>).</p><p id="P14">Next, we assessed how well each ocular feature could be reconstructed from the resting-state neural data. These predictions were generated by convolving the source-projected brain data with the corresponding time-flipped ORFs. We tested this using either the full source-space data ('All') or data restricted to the patterns of the top three principal components ('PC1', 'PC2', 'PC3'), as well as their combination ('PCA'). Reconstruction performance was quantified by correlating the predicted and actual eye-tracking time series, using Pearson's <italic>r</italic> for continuous data (pupil dilation) and the Matthews Correlation Coefficient (<italic>MCC</italic>) for binary events (blinks and saccades). In the main text, we focus on the results from the 'All' source model, which represents the total decodable information, and the 'PC1' model, which reflects information contained in the single most dominant spatiotemporal component.</p><p id="P15">As detailed in <xref ref-type="fig" rid="F4">Figure 4</xref>, all three ocular features were reconstructed at levels significantly above chance in both the no-ICA and ICA-cleaned data. In all cases, performance was highest when using the full source model ('All') and, while reduced, remained highly significant when using only the first principal component ('PC1').</p><p id="P16">These results indicate that there is significant, widespread information across the brain to allow for the reconstruction of ocular events, even after accounting for oculomuscular artifacts. Furthermore, these analyses show that while a large portion of this information is captured by the single most dominant spatiotemporal component, other, more subtle components also contribute to the full decodable signal.</p></sec></sec><sec id="S7"><title>Generalisation to a Cognitive Task: Oculomotor Contributions to Auditory Processing</title><p id="P17">To test the generalisability of our framework, we next applied the ORFs derived from resting-state data as a ‘proof-of-principle’ to a separate passive listening paradigm. For computational efficiency and because this solely constituted a proof-of-principle, all analyses reported in this section were performed on sensor-level data. This analysis demonstrates how the ORF method can be used to uncover relationships between oculomotor systems and neural activity even in contexts like passive listening, where eye movements are not typically expected to influence cognitive processing. In this task, participants were again instructed to direct their gaze toward a black cross at the centre of a screen while they listened to tone sequences that alternated between predictable ('ordered') and unpredictable ('random') patterns.</p><p id="P18">To investigate the interplay between oculomotor systems and auditory processing, we focused specifically on saccades, motivated by recent findings linking them to excitability in the auditory cortex <sup><xref ref-type="bibr" rid="R39">39</xref></sup>. We convolved the saccade time series recorded during the listening task with each participant's resting-state derived saccade ORFs. This generated a ‘predicted’ time course of saccade-related brain activity. We then applied multivariate pattern analysis (MVPA) to both the actual and this predicted neural activity to uncover the potential contribution of oculomotor dynamics to the processing of auditory regularity (see <xref ref-type="fig" rid="F5">Fig. 5A</xref>).</p><p id="P19">The MVPA revealed that auditory regularity could be successfully decoded from both the actual and the predicted saccade-related neural data (see <xref ref-type="fig" rid="F5">Fig. 5B</xref>). For the actual MEG data, there was a significant difference between the classification accuracies for 'ordered' and ‘random’ conditions (<italic>p</italic> &lt; .001). This difference was driven by a positive cluster indicating above-chance decoding, which was observed between 100 and 190 ms post-tone onset (<italic>t</italic>(27) = 3.61, <italic>d</italic> = 0.68). Importantly, a significant difference between conditions was also found for the predicted saccade-related activity (<italic>p</italic> = .013). This effect was reflected by a single cluster spanning the entire tested time window from -100 to 400 ms (<italic>t</italic>(27) = 2.36, <italic>d</italic> = 0.45). This result demonstrates that while saccadic behaviour is not as tightly time-locked to individual tone onsets as the direct neural response, it is nonetheless systematically modulated by the structure of the auditory stream (interestingly, saccade rates indicate more frequent saccading during the ordered condition, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figures</xref> <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>).</p><p id="P20">To understand the neural basis of this relationship, we projected the MVPA classifier weights into source space for both models and compared their relative spatial distributions (see <xref ref-type="fig" rid="F5">Fig. 5C</xref>). For the actual MEG data, informative activity primarily originated from the right temporal area and the left precentral gyrus. In contrast, the classifier trained on the predicted saccade-related activity drew upon information from the bilateral precentral gyri and the left occipital cortex. A direct comparison highlighted these distinct patterns, with the actual MEG data showing stronger informative activity in the right temporal lobe, while the saccade-predicted data relied more heavily on regions in the right frontal/supplementary motor area and left visual cortex. Crucially, we observed a substantial overlap between the two patterns in the left frontal and precentral regions, as well as the right temporal cortex and the midbrain. This convergence suggests potential shared neural substrates, where saccade-related dynamics may contribute to the overall neural processing of auditory regularity.</p><p id="P21">Finally, we assessed how well the time series of each ocular feature could be reconstructed from neural activity recorded during the passive listening task. Note that this reconstruction analysis is distinct from the MVPA classification described above: here, the goal is to predict the continuous or binary time course of an ocular feature, rather than a discrete experimental label. Predictions were generated by convolving the task-related sensor data with the ORFs originally derived from resting-state data. Reconstruction accuracy was then evaluated by correlating the predicted time series with the actual eye-tracking recordings, using Pearson's <italic>r</italic> for pupil dilation and the Matthews Correlation Coefficient (<italic>MCC</italic>) for blinks and saccades.</p><p id="P22">As detailed in <xref ref-type="fig" rid="F6">Figure 6</xref>, all ocular features were successfully reconstructed at levels significantly above chance for both the 'Ordered' and 'Random' conditions. These results demonstrate that the ORFs derived from resting-state data generalise effectively to a different experimental context, successfully capturing oculomotor-related neural dynamics during a cognitive task.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P23">In this study, we introduced the ‘Ocular Response Function’ (ORF) framework to characterise the spatiotemporal relationship between neural activity and a comprehensive set of oculomotor events. By applying a deconvolution approach to resting-state MEG and eye-tracking data, we demonstrated that it is possible to derive robust, anatomically specific transfer functions for saccades, blinks, and pupil dilation. Our results show that these ORFs can successfully predict neural activity from eye movements and, conversely, reconstruct ocular events from brain activity. Furthermore, we demonstrated that these functions, derived from task-free data, can be generalised to a separate cognitive paradigm to reveal how oculomotor processes contribute to task-related neural activity.</p><p id="P24">Our approach builds upon a well-established tradition of regression-based analyses in neuroimaging <sup><xref ref-type="bibr" rid="R34">34</xref></sup>, and more specifically the Temporal Response Function (TRF) framework <sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>. TRFs are a system identification method typically used to characterise neural responses to complex, external sensory stimuli like natural speech. In contrast, our work uses this logic to derive transfer functions from internally-generated oculomotor signals, which we term ‘Ocular Response Functions’ (ORFs). This application also conceptually parallels the use of the hemodynamic response function (HRF) in fMRI analysis, where an impulse response is convolved with an event time series to predict neural activity. However, a key distinction and advantage of our approach is that while the HRF is often assumed to be a canonical, stereotyped function, the ORFs are empirically derived from the data itself. This allows for a more flexible characterisation of the neural response, revealing distinct transfer functions for different oculomotor events (e.g., blinks vs. saccades).</p><p id="P25">Indeed, these interpretable transfer functions revealed distinct spatiotemporal profiles for each ocular event, highlighting how the ORF framework can characterise the distributed neural processes underlying oculomotor control. For saccades, the dominant component consistently localised to the occipital visual cortex, peaking at +100 ms - such that, following saccade events, there was a peak in visual processing. This finding validates the method's ability to capture well-documented neural signatures of visual reafference <sup><xref ref-type="bibr" rid="R42">42</xref></sup>. For eye blinks, the picture was more multifaceted. After removing frontocentral muscle artifacts via ICA, the dominant ORF component was localised to the cerebellum, while a secondary component revealed activity in visual areas. This source-space dissociation extends EEG studies of the Blink-Related Potential (BRP, e.g., <sup><xref ref-type="bibr" rid="R43">43</xref></sup>), which have concluded that there is a visual, as well as muscular contribution, by demonstrating the particular contributions also of regions like the cerebellum. As well as providing better source information via MEG, they also provide a more flexible tool over this classic ERP (or ERF) approach, because they are derived via regression, rather than epoching, approaches. This means that they can describe complex relationships in continuously evolving structures where events (e.g., blinks) can occur in quick succession yet not contaminate overlapping epochs - because there are none.</p><p id="P26">Similarly, pupil dilation was associated with a distributed network of brain regions, including the visual cortex, cuneus, and temporal lobe. The finding of such widespread, pupil-linked cortical activity using such TRFs, is inline with the comprehensive mapping conducted by Pfeffer et al. <sup><xref ref-type="bibr" rid="R44">44</xref></sup>, who used a diverse suite of statistical tools - including cross-correlations and frequency-resolved analyses - to exhaustively characterise the complex links between pupil-linked arousal and cortical activity. Their work provided a foundational 'map', detailing the specific lags, frequency-dependencies, and even non-linearities of this relationship within a large resting-state dataset. Our ORF framework advances this important work by moving from a descriptive characterisation to a unified, predictive model. Rather than employing multiple methods to describe different facets of the relationship, the ORF approach distils these complex dynamics into a single, data-driven linear transfer function. This provides a reusable 'toolkit' rather than a static map.</p><p id="P27">The key advantage of this approach is its inherent generalisability; the resulting filter is not merely a description of one dataset but a robust model that can be applied out-of-context to predict oculomotor-related neural activity in new paradigms - a step we validated by applying our resting-state ORFs to the passive listening task. The ability of the ORF framework to resolve these anatomically and functionally distinct components provides a potentially powerful tool for future studies. It enables researchers to form and test specific hypotheses by precisely interrogating the role of different oculomotor events within targeted brain regions and time windows. Deriving this repertoire of distinct, source-localised transfer functions from task-free data provides the foundation for the framework's key utility: testing how these baseline mappings generalise to new contexts.</p><p id="P28">The application of our framework to the passive listening task served as a critical proof-of-principle for its generalisability. The MVPA results, which revealed a spatial overlap between classifier patterns from actual and saccade-predicted neural data, provide strong evidence for a shared neural substrate linking oculomotor control and auditory processing. This generalisability was further supported by the successful reconstruction of all three ocular features from the task data, confirming that the underlying ORF models are robust across different experimental contexts. Ultimately, these findings highlight the framework's utility for uncovering complex, cross-modal contributions of oculomotor systems to cognition. In our test case, frontal, precentral regions, temporal and midbrain regions that could have been believed to be directly related to tone presentation, may arguably instead be mediated by differential oculomotor responses to tone sequences.</p><p id="P29">While the framework successfully generalised to the listening task, the reconstruction accuracies - especially for pupil dilation and saccades - were modest. This is likely due to two main, deliberate trade-offs in our approach: First, our simple, linear model, chosen for its ability to produce interpretable, spatiotemporally specific transfer functions. While other powerful toolboxes exist for this type of analysis - such as the mTRF-Toolbox <sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup> for linear modeling, the unfold toolbox <sup><xref ref-type="bibr" rid="R42">42</xref></sup> for handling non-linearities with generalised additive models (GAMs) or deep learning approaches like cdr(NN) which can capture highly complex, non-linear, and non-stationary dynamics <sup><xref ref-type="bibr" rid="R43">43</xref>,<xref ref-type="bibr" rid="R44">44</xref></sup> - we deliberately chose a linear model with a sparsity-promoting boosting algorithm <sup><xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R38">38</xref></sup>. The choice between different modeling techniques within this family ultimately depends on the scientific goal. If the primary aim is to maximise predictive accuracy - for instance, to reconstruct ocular signals with the highest possible fidelity (e.g., <sup><xref ref-type="bibr" rid="R45">45</xref></sup>) - then more complex, non-linear models are likely the superior choice. Our goal here, in contrast, was to prioritise the estimation of interpretable linear transfer functions that form a solid baseline for future, more complex models. Consequently, this model is constrained in its predictive accuracy and does not account for known non-linearities (e.g., saccade amplitude; <sup><xref ref-type="bibr" rid="R42">42</xref></sup>) or the collinearity between different oculomotor events, such as saccades versus fixations, which have themselves been shown to evoke distinct neural responses <sup><xref ref-type="bibr" rid="R45">45</xref></sup>. Second, the ORFs were derived from task-free resting-state data. While this provides an arguably ‘purer’ baseline, the dynamics of cognitively-modulated features like pupil dilation and saccades may differ during an engaged task compared to the more stereotyped physiological profile of a blink. Future work can address these limitations by employing non-linear models to capture more complex dynamics. A promising avenue for research is to also compare the baseline ORFs derived here with those estimated during active tasks, such as free viewing or smooth pursuit, to formally investigate how these fundamental eye-brain mappings are modulated by different task demands and their associated oculomotor dynamics. Improving the predictive fidelity through such advanced modeling is a crucial next step, as it could enable novel research applications - from estimating pupil-linked arousal during sleep where direct measurement is impossible, to wider clinical applications.</p><p id="P30">The individual variability we observed in the ORFs could be particularly valuable here, complementing and extending recent work demonstrating that variability in neurophysiological activity can provide meaningful insights into brain function and personalised neural signatures <sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>. This may contribute to understanding not only in healthy individuals but also in those with psychopathology or neurological disorders. Importantly, the ORFs provide a quantitative metric that can be directly assessed in patient populations. When combined with (existing) resting state datasets, these additional deterministic metrics may provide valuable insights into functional differentiations between cohorts. For example, differences in pupil dilation ORFs between patients with dementia or Parkinson’s disease and healthy controls could offer fundamental insights into aberrant neural functioning and associated behavioural outcomes. The ORF framework may therefore provide a powerful basis for developing robust, individualised biomarkers for the early detection of disorders where oculomotor control is known to be affected.</p><p id="P31">In summary, we presented the Ocular Response Function (ORF) framework as a method to characterise and disentangle the distinct neural signatures of different oculomotor events. By deriving interpretable, source-localised transfer functions from task-free resting-state data, our approach provides a robust baseline for understanding the fundamental mappings between the brain and a repertoire of ocular features. We demonstrated the utility of this framework by showing that these baseline ORFs can be generalised to probe the contribution of oculomotor dynamics during a separate cognitive task. This work underscores the importance of modeling, rather than simply removing, ocular-related signals and provides an accessible framework to investigate their intricate interplay with neural processing in both cognitive and clinical neuroscience.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><p id="P32">This manuscript used data acquired by Schubert et al. <sup><xref ref-type="bibr" rid="R48">48</xref></sup>. In the following, we provide a detailed description of materials and methods that are required for the understanding of the current work.</p><sec id="S10" sec-type="subjects"><title>Participants</title><p id="P33">We used data from 29 participants (12 female, 17 male; mean age = 25.70, range = 19 - 43). For the passive listening task, the eye tracker failed to detect the pupil and corneal reflection for one participant, leading to a flat signal for the majority of one block. As a result, no valid data were recorded for that block and the participant was excluded from the analysis, leaving a total of 28 participants for the passive listening task (29 remain in the core reported analyses). All participants reported normal hearing and had normal, or corrected to normal, vision. They gave written, informed consent and reported that they had no previous neurological or psychiatric disorders. The experimental procedure was approved by the ethics committee of the University of Salzburg and was carried out in accordance with the declaration of Helsinki. All participants received either a reimbursement of 10 € per hour or course credits.</p></sec><sec id="S11"><title>Stimuli and Experimental Design</title><p id="P34">We assessed participants' head shapes before the start of the experiment using cardinal head points (nasion and pre-auricular points), digitised with a Polhemus Fastrak Digitiser (Polhemus), and around 300 points on the scalp. For all parts of the experiment, for both the resting-state period and the passive tone listening task, participants were instructed to direct their gaze toward a black cross at the centre of a screen. Each MEG session then started with a 5-minute resting-state recording. This standard resting-state paradigm proved ideal for our goal of establishing fundamental ORFs in the absence of complex visual inputs or cognitive demands, which could otherwise confound the eye-brain relationship. While this task minimises large, exploratory saccades, it provides a clean baseline and generates ample data on blinks, microsaccades, and pupil fluctuations for our models. During the 5-minute resting-state period, participants produced an average of 0.32 ± 0.24 Hz for blinks and 1.80 ± 1.26 Hz for saccades. Afterwards, the individual hearing threshold was determined using a pure tone of 1043 Hz. This was followed by 2 blocks of passive listening to tone sequences of varying entropy levels. Sequences consisted of four different pure tones (f1: 440 Hz, f2: 587 Hz, f3: 782 Hz, f4: 1043 Hz, each lasting 100 ms), each block consisted of 1500 tones presented with a temporally predictable rate of 3 Hz. Entropy levels (ordered / random) changed pseudorandomly every 500 trials within each block, thus resulting in a total of 1500 trials per entropy condition. While in an ‘ordered’ context forward transitions (i.e. f1→f2, f2→f3, f3→f4, f4→f1) had a high probability of 75%, repetitions (e.g., f1→f1, f2→f2,…) were rather unlikely with a probability of 25%. However, in a ‘random’ context all possible transitions (including forward transitions and repetitions) were equally likely with a probability of 25%. Oculomotor behavior during this passive listening task was largely comparable to the resting state, with average rates of 0.30 ± 0.22 Hz for blinks and 1.59 ± 0.87 Hz for saccades.</p></sec><sec id="S12"><title>Data Acquisition and Preprocessing</title><sec id="S13"><title>MEG</title><p id="P35">We collected MEG data using a whole head system (Elekta Neuromag Triux, Elekta Oy, Finland), placed within a standard passive magnetically shielded room (AK3b, Vacuumschmelze, Germany). The signal was recorded with 102 magnetometers and 204 orthogonally placed planar gradiometers at 102 different positions at a sampling frequency of 1 kHz (hardware filters: 0.1 - 330 Hz). In a first step, a signal space separation algorithm, implemented in the Maxfilter program (version 2.2.15) provided by the MEG manufacturer, was used to clean the data from external noise and realign data from different blocks to a common standard head position. Data preprocessing was performed using Matlab R2020b (The MathWorks, Natick, Massachusetts, USA) and the FieldTrip Toolbox (Oostenveld et al., 2011). All data were band-pass filtered between 0.5 Hz and 30 Hz (onepass-zerophase finite impulse response (FIR) filter, order 3330, hamming-windowed) and downsampled to 100 Hz. We further used two versions of preprocessed data that either used independent component analysis to reject eye movement (and heart-rate) related artefacts or not. We further refer to these data as either ICA or no-ICA respectively. For ICA, to identify eye movement and heart rate artefacts, 65 independent components were identified from filtered (onepass-zerophase lowpass FIR filter, order 132, hamming-windowed; onepass-zerophase highpass FIR filter, order 1650, hamming-windowed) continuous data of the resting-state recordings using runica-ICA. The goal of this ICA step was to remove non-neurogenic artifacts while preserving the underlying neural signals. Based on visual inspection of component time-courses and spatial topographies, we identified and removed components dominated by cardiac or by ocular activity, including that from the eyelids (blinks), eyeball rotation, and extraocular muscles. This process resulted in the removal of a median of 3 components per participant (min = 2, max = 4).</p></sec><sec id="S14"><title>Eye Tracking</title><p id="P36">We used a Trackpixx3 binocular tracking system (Vpixx Technologies, Canada) with a 50 mm lens. We synchronised MEG and eye-tracking data feeding the Trackpixx’s output data as three additional channels into the MEG system, representative of horizontal gaze, vertical gaze, and pupil dilation of participants' right eye. Recordings started after a 13-point calibration and validation procedure. Throughout the experiment participants were seated in the MEG at a distance of 82 cm from the screen, with their chin resting on a chinrest to reduce head movements. We used an algorithm based on pupillometry noise to detect blinks <sup><xref ref-type="bibr" rid="R49">49</xref></sup>. To account for potential residual blink artefacts when examining pupil dilation, we removed an additional 100 ms around blink edges. Blinks were then removed from pupil dilation data and linearly interpolated. For saccade detection, we used a velocity-based detection approach <sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup> using the Euclidean distance between successive gaze points in the horizontal and vertical planes as in <sup><xref ref-type="bibr" rid="R5">5</xref></sup> with a 7 ms smoothing window on the resulting velocity vector (using the built “smoothdata” function in Matlab), thus considering two-dimensional velocity profiles. Saccade on-/offsets (i.e. durations) were then identified where velocity exceeded and subsequently fell below the threshold of five times the median velocity. A minimum delay of 100 ms was used between saccade onsets to prevent multiple detections of the same saccade. We additionally excluded saccades from pupil dilation data and linearly interpolated the gaps. For further analysis, blinks and saccades were transformed into binary vectors of zeros (no blink/saccade) and ones (blink/saccade) retaining event durations. Afterwards, pupil dilation data were bandpass filtered between 0.01 - 4 Hz (onepass-zerophase finite impulse response (FIR) filter, order 165000, hamming-windowed) and median centred on the screen centre. Finally, the three eye tracking data channels (pupil dilation, blinks, and saccades) were appended to the MEG data and therefore downsampled to 100 Hz alongside the MEG data. Note that binary blink and saccade vectors were then restored by adding “1 s” at the nearest time points of the original sampling rate to avoid potential data loss of short activation patterns (i.e. onsets).</p></sec><sec id="S15"><title>MEG and Eye Tracking</title><p id="P37">Resting-state data were cut from 10 s to 290 s after recording onset. Data of the entropy modulation paradigm were cut from -1 s of the first tone to +1 s of the last tone of every 500 tone segment and corrected for a 16 ms delay between trigger and stimulus onset generated by sound travelling through the pneumatic headphones into the shielded MEG room.</p></sec></sec><sec id="S16"><title>The Ocular Response Function</title><sec id="S17"><title>Encoding Models</title><p id="P38">To establish spatially-resolved transfer functions from oculomotor action to neural representations, we first projected the continuous resting-state MEG data into anatomical source space (see <xref ref-type="sec" rid="S19">Source Projection</xref>). We then estimated a multivariate temporal response function (mTRF) for the time series of each source-space voxel using the Eelbrain toolkit <sup><xref ref-type="bibr" rid="R38">38</xref></sup>. A deconvolution algorithm based on boosting <sup><xref ref-type="bibr" rid="R37">37</xref></sup> was applied to the source-space data to estimate the optimal TRFs that predict neural activity (dependent variable) from three key ocular features (predictors): pupil dilation, blinks, and saccades. We deliberately chose boosting over ridge regression <sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup> as it imposes a sparse prior, encouraging TRFs with fewer non-zero elements. This sparsity helps in interpreting the core temporal components of the response. For model fitting, the predictor time series and the neural time series for each voxel were separately centered and standardised. The model was trained to find the optimal linear filter over time lags from -1 s to +1 s, using a 100 ms wide Hamming window basis (please refer to <sup><xref ref-type="bibr" rid="R38">38</xref></sup> for further details on the boosting algorithm and parameters). A 4-fold cross-validation approach was used to evaluate the model and avoid overfitting <sup><xref ref-type="bibr" rid="R50">50</xref></sup>. All predictors were included into the same encoding model. All three predictors were included in the same encoding model, and we used selective stopping to prevent overfitting on individual predictors while allowing others to continue training <sup><xref ref-type="bibr" rid="R38">38</xref></sup>. This process results in a unique ORF for each ocular feature at each source-space voxel.</p></sec><sec id="S18"><title>Reconstruction and Evaluation Metrics</title><p id="P39">For reconstruction, the estimated ORF filters are simply reversed in time to predict ocular features from brain activity. This time-reversal is a computationally efficient method, analogous to a cross-correlation, for assessing the bidirectional relationship using the same estimated kernel.</p><p id="P40">Model performance was evaluated on held-out test data from the cross-validation procedure. For reconstruction, since blinks and saccades are binary events, the continuous model predictions were converted into probabilities using a sigmoid function and then thresholded at 0.5. We then calculated the Matthews Correlation Coefficient (MCC) to assess performance for blinks and saccades <sup><xref ref-type="bibr" rid="R51">51</xref>,<xref ref-type="bibr" rid="R52">52</xref></sup> and Pearson’s correlation (r) for the continuous pupil dilation signal. The same correlation metrics were used to evaluate encoding performance.</p></sec></sec><sec id="S19"><title>Source projection</title><p id="P41">To perform our main analyses in anatomical source space, we projected the continuous MEG time series data using Linearly Constrained Minimum Variance (LCMV) spatial filters <sup><xref ref-type="bibr" rid="R53">53</xref></sup>. LCVM filters were computed by warping anatomical template images to the individual head shape and further brought into a common space by co-registering them based on the 3 anatomical landmarks (nasion, left, and right preauricular points) with a standard brain from the Montreal Neurological Institute (MNI, Montreal, Canada; <sup><xref ref-type="bibr" rid="R54">54</xref></sup>). Then, we computed a single-shell head model <sup><xref ref-type="bibr" rid="R55">55</xref></sup> for each participant. As a source model, a grid with 1 cm resolution and 2982 voxels based on an MNI template brain was morphed into the brain volume of each participant. This allowed group-level averaging and statistical analysis as all the grid points in the warped grid belong to the same brain region across subjects. Spatial filters were estimated by following a publicly available tutorial that was made available by FieldTrip and can be found on their homepage (<ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/workshop/paris2019/handson_sourceanalysis/">https://www.fieldtriptoolbox.org/workshop/paris2019/handson_sourceanalysis/</ext-link>). A key step in our procedure was to utilise separate empty-room recordings to compute a robust noise covariance matrix. The continuous resting-state MEG data were then prewhitened using this noise covariance to suppress environmental and sensor noise <sup><xref ref-type="bibr" rid="R56">56</xref></sup>. The dimensionality reduction parameter for this step (kappa) was determined automatically by detecting the first major "cliff" in the singular value spectrum of the noise covariance matrix.</p><p id="P42">Following prewhitening, a data covariance matrix was computed from the main experimental data. A forward model (leadfield) was then prepared using the subject-specific headmodel and source model grid. Finally, the LCMV spatial filters were computed with a regularisation parameter (lambda) set to 5%. We specified a fixed dipole orientation (fixedori = 'yes') and used a 'unit noise gain' normalisation (weightnorm = 'unitnoisegain') to ensure uniform noise projection across all source locations. This entire procedure was performed separately for data with and without prior ICA correction, yielding two sets of spatial filters for each participant. These filters were then used to project the continuous resting-state MEG data into source space for subsequent ORF analyses. Anatomical labels for source-space results were obtained using the AAL3 <sup><xref ref-type="bibr" rid="R57">57</xref></sup>. Wherever the coordinates of an effect in mni-space did not correspond to any labelled region, nearest neighbour interpolation was applied. This approach allowed for the assignment of unlabeled coordinates by identifying the closest labelled region.</p><sec id="S20"><title>Principal component analysis</title><p id="P43">To reduce the dimensionality of the data and identify the most relevant spatiotemporal components of the neural response, we applied principal component analysis (PCA). This was performed on the source-projected ORFs for each subject and ocular feature individually, transforming the high-dimensional voxel space into a smaller set of orthogonal components that capture the most variance in the eye-brain relationship.</p><p id="P44">The analysis pipeline was as follows: First, to mitigate potential edge artifacts from the deconvolution, we cropped the ORFs to a time window of -0.9 to +0.9 seconds. To focus the decomposition on response magnitude irrespective of direction, we then computed the absolute value of these cropped ORFs. PCA was then performed on this rectified (absolute-magnitude) data. Following the decomposition, a polarity alignment procedure was applied to ensure component consistency across subjects. Because the sign of a principal component is arbitrary, we standardised the polarity by projecting each component's spatial weights back onto the original magnitude data. If this projection resulted in a negative sum, the signs of both the spatial weights and the temporal component were inverted. This step ensures that the directionality of the main effect (e.g., a positive peak) is consistent across all participants before averaging or group-level analysis. For all subsequent analyses, we consistently selected the top three principal components that explained the most variance for each ocular feature and preprocessing condition (ICA vs. no-ICA).</p></sec></sec><sec id="S21"><title>Multivariate Pattern Analysis (MVPA)</title><p id="P45">To gain deeper insights into how oculomotor activity may influence neural activation patterns during experimental tasks, we applied sensor-level resting-state derived ORFs to data from a passive listening paradigm (see <xref ref-type="fig" rid="F5">Fig. 5A</xref>). Given recent findings linking neural excitability in the auditory cortex with saccades <sup><xref ref-type="bibr" rid="R39">39</xref></sup>, we focused on potential overlaps with saccade-related neural activity. For reasons of computational efficiency and because this constituted solely a proof-of-principle, this specific analysis was performed entirely on sensor-level data. We estimated sensor-space saccade ORFs from the resting-state recordings and then convolved them with the saccadic eye movement data from the passive listening task, generating a predicted MEG time series for all 306 sensors.</p><p id="P46">Both the ICA-cleaned MEG and predicted MEG data were epoched around tone onsets (-0.1 to 0.4 seconds), resulting in 3000 epochs (1500 for the ordered regularity condition and 1500 for the random condition). The data were subjected to time-resolved classification using the MVPA-Light package <sup><xref ref-type="bibr" rid="R58">58</xref></sup>. We employed the default 5-fold cross-validation, z-scoring as a preprocessing step, and linear discriminant analysis (LDA) to decode regularity conditions (i.e. ordered vs. random) from both MEG and predicted MEG data, as well as a shuffled version of those labels as a control. This process yielded one accuracy value per time point and one classifier weight per time point per sensor for each participant.</p><p id="P47">Classifier weights were corrected by their covariance structure <sup><xref ref-type="bibr" rid="R59">59</xref></sup>, projected into source space by multiplying them with the resting-state spatial filters, and baseline-corrected (relative change) from -0.05 - 0 s <sup><xref ref-type="bibr" rid="R60">60</xref></sup> relative to tone onset. We then averaged these weights from 0.10 - 0.19 s (sig. Cluster of the MEG decoding, see <xref ref-type="fig" rid="F5">Fig. 5B</xref>). To ensure comparability between true and predicted MEG results, whilst maintaining the relative importance of brain areas in sensitivity to changes in stimulus regularity, weights from both datasets were z-scored (i.e., standardised to a mean of 0 and standard deviation of 1). After this transformation, the negative of the minimum value across both datasets (i.e. global minimum) was added to retain zero values (z' = z + (min(z) * -1)).</p><p id="P48">To explore the shared contributions of neural and oculomotor processes to regularity processing, we calculated both the difference and overlap of the averaged, normalised classifier weights. The difference was computed by subtracting the predicted MEG weights (Weights<sub>PredictedMEG</sub>) from the MEG weights (Weights<sub>MEG</sub>). The overlap was computed according to the following formula: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mtext>Weights</mml:mtext></mml:mrow><mml:mrow><mml:mtext>MEG</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>Weights</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PredictedMEG</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>abs</mml:mtext><mml:mo>(</mml:mo><mml:mtext>Weights</mml:mtext></mml:mrow><mml:mrow><mml:mtext>MEG</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mtext>Weights</mml:mtext></mml:mrow><mml:mrow><mml:mtext>PredictedMEG</mml:mtext></mml:mrow></mml:msub><mml:mtext>)</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></disp-formula></p></sec><sec id="S22"><title>Statistical Analysis</title><p id="P49">For the statistical analysis of resting-state data, we employed one-sample Student’s t-tests to evaluate the significance of the encoding and reconstruction correlation coefficients (see <xref ref-type="sec" rid="S18">evaluation metrics</xref>). To obtain one encoding correlation coefficient for each participant, we first averaged the PCA weight matrices across participants and identified the voxel with the maximum component weight for each of the first three components. We then extracted the correlation coefficient from these peak locations for each participant. Subsequently, we report the r-values, t-values, p-values, and Cohen’s d effect sizes for each one-sample Student’s t-test. We intentionally chose to use one-sample t-tests over alternatives such as ANOVA or mixed-effects models, as we did not aim to assess whether one type of eye movement was more strongly represented in neural activity than another. Our primary interest was to demonstrate that ocular behaviour is consistently represented across various brain regions, allowing for reliable encoding and reconstruction effects, rather than exploring the extent of differences between them.</p><p id="P50">To statistically evaluate the MVPA analysis of the passive listening data, we used a cluster-based randomisation approach. This analysis was conducted across all time points within the classifier time window (-0.1 to 0.4 seconds). Specifically, we tested two one-sided contrasts (see <xref ref-type="fig" rid="F5">Fig. 5B</xref>), comparing classifier accuracy for true versus shuffled regularity condition labels (ordered vs. random). We computed the randomisation distribution of t-values across 10,000 permutations, using a cluster-forming alpha threshold of 0.05. The resulting clusters were compared against the original contrast at an alpha level of 0.05, Bonferroni corrected. For each significant cluster, we report the average t-values, p-values, and Cohen’s d (effect size) across the time points within the cluster. Decoded eye movements and the corresponding correlation coefficients were evaluated in the same manner as for the resting-state data.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS200366-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d2aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgements</title><p>Thanks to the whole research team. Special thanks to Manfred Seifter for his support in conducting the measurements, to Gareth Barnes for comments on the manuscript, and to the MEG group at the Functional Imaging Lab, UCL, for input on the idea. This work was supported by a Leverhulme Project Grant (RPG-2022-358) and European Research Council (ERC) consolidator grant (101001592) under the European Union’s Horizon 2020 research and innovation programme, both awarded to C.P.</p></ack><sec id="S24" sec-type="data-availability"><title>Data availability</title><p id="P51">Preprocessed Data required to reproduce the analyses supporting this work are publicly available in the corresponding author’s Open Science Framework repository (<ext-link ext-link-type="uri" xlink:href="https://osf.io/5bucm/">https://osf.io/5bucm/</ext-link>). Raw data (&gt; 250 GB) will be shared upon request.</p></sec><sec id="S25" sec-type="data-availability"><title>Code availability</title><p id="P52">Code to analyse preprocessed data and further reproduce results and figures from this manuscript is available in the corresponding author’s Open Science Framework repository (<ext-link ext-link-type="uri" xlink:href="https://osf.io/5bucm/">https://osf.io/5bucm/</ext-link>).</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P53">Author contributions</p><p id="P54">Q.G. designed the experiment, collected and analysed the data, generated the figures, and wrote the manuscript. J.S. designed the experiment, collected and analysed the data, and edited the manuscript. A.K. contributed to data analyses and edited the manuscript. N.W. collected the data, contributed to data analyses and edited the manuscript. C.P. acquired the funding, supervised the project, and edited the manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaikh</surname><given-names>AG</given-names></name><name><surname>Zee</surname><given-names>DS</given-names></name></person-group><article-title>Eye Movement Research in the Twenty-First Century—a Window to the Brain, Mind, and More</article-title><source>The Cerebellum</source><year>2018</year><volume>17</volume><fpage>252</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">29260439</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><article-title>Functional but not obligatory link between microsaccades and neural modulation by covert spatial attention</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>3503</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-31217-3</pub-id><pub-id pub-id-type="pmcid">PMC9205986</pub-id><pub-id pub-id-type="pmid">35715471</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>B</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><article-title>Microsaccades transiently lateralise EEG alpha activity</article-title><source>Prog Neurobiol</source><year>2023</year><volume>224</volume><elocation-id>102433</elocation-id><pub-id pub-id-type="doi">10.1016/j.pneurobio.2023.102433</pub-id><pub-id pub-id-type="pmcid">PMC10074474</pub-id><pub-id pub-id-type="pmid">36907349</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>E</given-names></name><name><surname>Fejer</surname><given-names>G</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name></person-group><article-title>No obligatory trade-off between the use of space and time for working memory</article-title><source>Commun Psychol</source><year>2023</year><volume>1</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s44271-023-00042-9</pub-id><pub-id pub-id-type="pmcid">PMC11041649</pub-id><pub-id pub-id-type="pmid">38665249</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Vries</surname><given-names>E</given-names></name><name><surname>Van Ede</surname><given-names>F</given-names></name></person-group><article-title>Microsaccades Track Location-Based Object Rehearsal in Visual Working Memory</article-title><source>eneuro</source><year>2024</year><volume>11</volume><elocation-id>ENEURO.0276-23.2023</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0276-23.2023</pub-id><pub-id pub-id-type="pmcid">PMC10849020</pub-id><pub-id pub-id-type="pmid">38176905</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gehmacher</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Eye movements track prioritized auditory features in selective attention to natural speech</article-title><source>Nat Commun</source><year>2024</year><volume>15</volume><elocation-id>3692</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-024-48126-2</pub-id><pub-id pub-id-type="pmcid">PMC11063150</pub-id><pub-id pub-id-type="pmid">38693186</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schubert</surname><given-names>J</given-names></name><name><surname>Gehmacher</surname><given-names>Q</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Hartmann</surname><given-names>T</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Prediction tendency, eye movements, and attention in a unified framework of neural speech tracking</article-title><source>eLife</source><year>2024</year><volume>13</volume></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popov</surname><given-names>T</given-names></name><name><surname>Gips</surname><given-names>B</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Brain areas associated with visual spatial attention display topographic organization during auditory spatial attention</article-title><source>Cereb Cortex</source><year>2022</year><elocation-id>bhac285</elocation-id><pub-id pub-id-type="doi">10.1093/cercor/bhac285</pub-id><pub-id pub-id-type="pmcid">PMC10068281</pub-id><pub-id pub-id-type="pmid">35972419</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><article-title>Control of goal-directed and stimulus-driven attention in the brain</article-title><source>Nat Rev Neurosci</source><year>2002</year><volume>3</volume><fpage>201</fpage><lpage>215</lpage><pub-id pub-id-type="pmid">11994752</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><etal/></person-group><article-title>A Common Network of Functional Areas for Attention and Eye Movements</article-title><source>Neuron</source><year>1998</year><volume>21</volume><fpage>761</fpage><lpage>773</lpage><pub-id pub-id-type="pmid">9808463</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astafiev</surname><given-names>SV</given-names></name><etal/></person-group><article-title>Functional Organization of Human Intraparietal and Frontal Cortex for Attending, Looking, and Pointing</article-title><source>J Neurosci</source><year>2003</year><volume>23</volume><fpage>4689</fpage><lpage>4699</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-11-04689.2003</pub-id><pub-id pub-id-type="pmcid">PMC6740811</pub-id><pub-id pub-id-type="pmid">12805308</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engbert</surname><given-names>R</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name></person-group><article-title>Microsaccades uncover the orientation of covert attention</article-title><source>Vision Res</source><year>2003</year><volume>43</volume><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="pmid">12676246</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafed</surname><given-names>ZM</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name></person-group><article-title>Microsaccades as an overt measure of covert attention shifts</article-title><source>Vision Res</source><year>2002</year><volume>42</volume><fpage>2533</fpage><lpage>2545</lpage><pub-id pub-id-type="pmid">12445847</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>J</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><article-title>Bidirectional brain-body interactions during natural story listening</article-title><source>Cell Rep</source><year>2024</year><volume>43</volume><elocation-id>114081</elocation-id><pub-id pub-id-type="pmid">38581682</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rawal</surname><given-names>A</given-names></name><name><surname>Rademaker</surname><given-names>RL</given-names></name></person-group><article-title>Visual exploration goes down with higher working memory load, while more saccades negatively impact recall</article-title><source>J Vis</source><year>2024</year><volume>24</volume><fpage>383</fpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>H</given-names></name><etal/></person-group><article-title>Eye Movement Abnormalities Can Distinguish First-Episode Schizophrenia, Chronic Schizophrenia, and Prodromal Patients From Healthy Controls</article-title><source>Schizophr Bull Open</source><year>2023</year><volume>4</volume><elocation-id>sgac076</elocation-id><pub-id pub-id-type="doi">10.1093/schizbullopen/sgac076</pub-id><pub-id pub-id-type="pmcid">PMC11207660</pub-id><pub-id pub-id-type="pmid">39145342</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morita</surname><given-names>K</given-names></name><name><surname>Miura</surname><given-names>K</given-names></name><name><surname>Kasai</surname><given-names>K</given-names></name><name><surname>Hashimoto</surname><given-names>R</given-names></name></person-group><article-title>Eye movement characteristics in schizophrenia: A recent update with clinical implications</article-title><source>Neuropsychopharmacol Rep</source><year>2020</year><volume>40</volume><fpage>2</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1002/npr2.12087</pub-id><pub-id pub-id-type="pmcid">PMC7292223</pub-id><pub-id pub-id-type="pmid">31774633</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broerse</surname><given-names>A</given-names></name><name><surname>Crawford</surname><given-names>TJ</given-names></name><name><surname>den Boer</surname><given-names>JA</given-names></name></person-group><article-title>Parsing cognition in schizophrenia using saccadic eye movements: a selective overview</article-title><source>Neuropsychologia</source><year>2001</year><volume>39</volume><fpage>742</fpage><lpage>756</lpage><pub-id pub-id-type="pmid">11311304</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>A</given-names></name><name><surname>Ueda</surname><given-names>K</given-names></name><name><surname>Hirano</surname><given-names>Y</given-names></name></person-group><article-title>Recent updates of eye movement abnormalities in patients with schizophrenia: A scoping review</article-title><source>Psychiatry Clin Neurosci</source><year>2021</year><volume>75</volume><fpage>82</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1111/pcn.13188</pub-id><pub-id pub-id-type="pmcid">PMC7986125</pub-id><pub-id pub-id-type="pmid">33314465</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obyedkov</surname><given-names>I</given-names></name><etal/></person-group><article-title>Saccadic eye movements in different dimensions of schizophrenia and in clinical high-risk state for psychosis</article-title><source>BMC Psychiatry</source><year>2019</year><volume>19</volume><fpage>110</fpage><pub-id pub-id-type="doi">10.1186/s12888-019-2093-8</pub-id><pub-id pub-id-type="pmcid">PMC6454611</pub-id><pub-id pub-id-type="pmid">30961571</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caldani</surname><given-names>S</given-names></name><etal/></person-group><article-title>Saccadic eye movements as markers of schizophrenia spectrum: Exploration in at-risk mental states</article-title><source>Schizophr Res</source><year>2017</year><volume>181</volume><fpage>30</fpage><lpage>37</lpage><pub-id pub-id-type="pmid">27639418</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Antonio</surname><given-names>F</given-names></name><etal/></person-group><article-title>Blink Rate Study in Patients with Alzheimer’s Disease, Mild Cognitive Impairment and Subjective Cognitive Decline</article-title><source>Curr Alzheimer Res</source><year>2021</year><volume>18</volume><fpage>1104</fpage><lpage>1110</lpage><pub-id pub-id-type="pmid">34961444</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ladas</surname><given-names>A</given-names></name><name><surname>Frantzidis</surname><given-names>C</given-names></name><name><surname>Bamidis</surname><given-names>P</given-names></name><name><surname>Vivas</surname><given-names>AB</given-names></name></person-group><article-title>Eye Blink Rate as a biological marker of Mild Cognitive Impairment</article-title><source>Int J Psychophysiol</source><year>2014</year><volume>93</volume><fpage>12</fpage><lpage>16</lpage><pub-id pub-id-type="pmid">23912068</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molitor</surname><given-names>RJ</given-names></name><name><surname>Ko</surname><given-names>PC</given-names></name><name><surname>Ally</surname><given-names>BA</given-names></name></person-group><article-title>Eye Movements in Alzheimer’s Disease</article-title><source>J Alzheimers Dis JAD</source><year>2015</year><volume>44</volume><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granholm</surname><given-names>EL</given-names></name><etal/></person-group><article-title>Pupillary Responses as a Biomarker of Early Risk for Alzheimer’s Disease</article-title><source>J Alzheimers Dis JAD</source><year>2017</year><volume>56</volume><fpage>1419</fpage><lpage>1428</lpage><pub-id pub-id-type="doi">10.3233/JAD-161078</pub-id><pub-id pub-id-type="pmcid">PMC5808562</pub-id><pub-id pub-id-type="pmid">28157098</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeeman</surname><given-names>M</given-names></name><etal/></person-group><article-title>Task-Evoked Pupillary Response as a Potential Biomarker of Dementia and Mild Cognitive Impairment: A Scoping Review</article-title><source>Am J Alzheimers Dis Dementias®</source><year>2023</year><volume>38</volume><elocation-id>15333175231160010</elocation-id><pub-id pub-id-type="doi">10.1177/15333175231160010</pub-id><pub-id pub-id-type="pmcid">PMC10580717</pub-id><pub-id pub-id-type="pmid">36896819</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brendler</surname><given-names>A</given-names></name><etal/></person-group><article-title>Assessing hypo-arousal during reward anticipation with pupillometry in patients with major depressive disorder: replication and correlations with anhedonia</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><fpage>344</fpage><pub-id pub-id-type="doi">10.1038/s41598-023-48792-0</pub-id><pub-id pub-id-type="pmcid">PMC10764729</pub-id><pub-id pub-id-type="pmid">38172509</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>M</given-names></name><etal/></person-group><article-title>Depression and Cognitive Impairment: Current Understanding of Its Neurobiology and Diagnosis</article-title><source>Neuropsychiatr Dis Treat</source><year>2022</year><volume>18</volume><fpage>2783</fpage><lpage>2794</lpage><pub-id pub-id-type="doi">10.2147/NDT.S383093</pub-id><pub-id pub-id-type="pmcid">PMC9719265</pub-id><pub-id pub-id-type="pmid">36471744</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneider</surname><given-names>M</given-names></name><etal/></person-group><article-title>Pupil Dilation during Reward Anticipation Is Correlated to Depressive Symptom Load in Patients with Major Depressive Disorder</article-title><source>Brain Sci</source><year>2020</year><volume>10</volume><fpage>906</fpage><pub-id pub-id-type="doi">10.3390/brainsci10120906</pub-id><pub-id pub-id-type="pmcid">PMC7760331</pub-id><pub-id pub-id-type="pmid">33255604</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carvalho</surname><given-names>N</given-names></name><etal/></person-group><article-title>Saccadic Eye Movements in Depressed Elderly Patients</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><elocation-id>e105355</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0105355</pub-id><pub-id pub-id-type="pmcid">PMC4133355</pub-id><pub-id pub-id-type="pmid">25122508</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittaker</surname><given-names>CK</given-names></name></person-group><article-title>Tinnitus and eye movement</article-title><source>Am J Otol</source><year>1982</year><volume>4</volume><fpage>188</fpage><pub-id pub-id-type="pmid">7149011</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lockwood</surname><given-names>AH</given-names></name><etal/></person-group><article-title>The functional anatomy of gaze-evoked tinnitus and sustained lateral gaze</article-title><source>Neurology</source><year>2001</year><volume>56</volume><fpage>472</fpage><lpage>480</lpage><pub-id pub-id-type="pmid">11222790</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>R</given-names></name><name><surname>Dambra</surname><given-names>C</given-names></name><name><surname>Lobarinas</surname><given-names>E</given-names></name><name><surname>Stocking</surname><given-names>C</given-names></name><name><surname>Salvi</surname><given-names>R</given-names></name></person-group><article-title>Head, Neck, and Eye Movements That Modulate Tinnitus</article-title><source>Semin Hear</source><year>2008</year><volume>29</volume><fpage>361</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1055/s-0028-1095895</pub-id><pub-id pub-id-type="pmcid">PMC2633109</pub-id><pub-id pub-id-type="pmid">19183705</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimigen</surname><given-names>O</given-names></name><name><surname>Ehinger</surname><given-names>BV</given-names></name></person-group><article-title>Regression-based analysis of combined EEG and eye-tracking data: Theory and applications</article-title><source>J Vis</source><year>2021</year><volume>21</volume><fpage>3</fpage><pub-id pub-id-type="doi">10.1167/jov.21.1.3</pub-id><pub-id pub-id-type="pmcid">PMC7804566</pub-id><pub-id pub-id-type="pmid">33410892</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stankov</surname><given-names>AD</given-names></name><etal/></person-group><article-title>During natural viewing, neural processing of visual targets continues throughout saccades</article-title><source>J Vis</source><year>2021</year><volume>21</volume><fpage>7</fpage><pub-id pub-id-type="doi">10.1167/jov.21.10.7</pub-id><pub-id pub-id-type="pmcid">PMC8431980</pub-id><pub-id pub-id-type="pmid">34491271</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nentwich</surname><given-names>M</given-names></name><etal/></person-group><article-title>Semantic novelty modulates neural responses to visual change across the human brain</article-title><source>Nat Commun</source><year>2023</year><volume>14</volume><elocation-id>2910</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-38576-5</pub-id><pub-id pub-id-type="pmcid">PMC10203305</pub-id><pub-id pub-id-type="pmid">37217478</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Estimating sparse spectro-temporal receptive fields with natural stimuli</article-title><source>Netw Bristol Engl</source><year>2007</year><volume>18</volume><fpage>191</fpage><lpage>212</lpage><pub-id pub-id-type="pmid">17852750</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><etal/></person-group><article-title>Eelbrain, a Python toolkit for time-continuous analysis with temporal response functions</article-title><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e85012</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.85012</pub-id><pub-id pub-id-type="pmcid">PMC10783870</pub-id><pub-id pub-id-type="pmid">38018501</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leszczynski</surname><given-names>M</given-names></name><etal/></person-group><article-title>Saccadic modulation of neural excitability in auditory areas of the neocortex</article-title><source>Curr Biol</source><year>2023</year><volume>33</volume><fpage>1185</fpage><lpage>1195</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="doi">10.1016/j.cub.2023.02.018</pub-id><pub-id pub-id-type="pmcid">PMC10424710</pub-id><pub-id pub-id-type="pmid">36863343</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>The multivariate temporal response function (mTRF) toolbox: a MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Front Hum Neurosci</source><year>2016</year><volume>10</volume><fpage>604</fpage><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id><pub-id pub-id-type="pmcid">PMC5127806</pub-id><pub-id pub-id-type="pmid">27965557</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Linear modeling of neurophysiological responses to speech and other continuous stimuli: methodological considerations for applied research</article-title><source>Front Neurosci</source><year>2021</year><volume>15</volume><pub-id pub-id-type="doi">10.3389/fnins.2021.705621</pub-id><pub-id pub-id-type="pmcid">PMC8648261</pub-id><pub-id pub-id-type="pmid">34880719</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yagi</surname><given-names>A</given-names></name></person-group><article-title>Lambda waves associated with offset of saccades: A subject with large lambda waves</article-title><source>Biol Psychol</source><year>1979</year><volume>8</volume><fpage>235</fpage><lpage>238</lpage><pub-id pub-id-type="pmid">497316</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>P</given-names></name><name><surname>Davies</surname><given-names>MB</given-names></name></person-group><article-title>Eyeblink-related potentials</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1988</year><volume>69</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">2448116</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeffer</surname><given-names>T</given-names></name><etal/></person-group><article-title>Coupling of pupil- and neuronal population dynamics reveals diverse influences of arousal on cortical processing</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e71890</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71890</pub-id><pub-id pub-id-type="pmcid">PMC8853659</pub-id><pub-id pub-id-type="pmid">35133276</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amme</surname><given-names>C</given-names></name><etal/></person-group><article-title>Saccade onset, not fixation onset, best explains early responses across the human visual cortex during naturalistic vision</article-title><year>2024</year><elocation-id>2024.10.25.620167</elocation-id><pub-id pub-id-type="doi">10.1101/2024.10.25.620167</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>da Silva Castanheira</surname><given-names>J</given-names></name><name><surname>Orozco Perez</surname><given-names>HD</given-names></name><name><surname>Misic</surname><given-names>B</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Brief segments of neurophysiological activity enable individual differentiation</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>5713</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-25895-8</pub-id><pub-id pub-id-type="pmcid">PMC8481307</pub-id><pub-id pub-id-type="pmid">34588439</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dubois</surname><given-names>J</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name></person-group><article-title>Building a Science of Individual Differences from fMRI</article-title><source>Trends Cogn Sci</source><year>2016</year><volume>20</volume><fpage>425</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.03.014</pub-id><pub-id pub-id-type="pmcid">PMC4886721</pub-id><pub-id pub-id-type="pmid">27138646</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schubert</surname><given-names>J</given-names></name><name><surname>Gehmacher</surname><given-names>Q</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Hartmann</surname><given-names>T</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Prediction tendency, eye movements, and attention in a unified framework of neural speech tracking</article-title><year>2024</year><elocation-id>2023.06.27.546746</elocation-id><pub-id pub-id-type="doi">10.1101/2023.06.27.546746</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hershman</surname><given-names>R</given-names></name><name><surname>Henik</surname><given-names>A</given-names></name><name><surname>Cohen</surname><given-names>N</given-names></name></person-group><article-title>A novel blink detection method based on pupillometry noise</article-title><source>Behav Res Methods</source><year>2018</year><volume>50</volume><fpage>107</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">29340968</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ying</surname><given-names>X</given-names></name></person-group><article-title>An Overview of Overfitting and its Solutions</article-title><source>J Phys Conf Ser</source><year>2019</year><volume>1168</volume><elocation-id>022022</elocation-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chicco</surname><given-names>D</given-names></name><name><surname>Jurman</surname><given-names>G</given-names></name></person-group><article-title>The Matthews correlation coefficient (MCC) should replace the ROC AUC as the standard metric for assessing binary classification</article-title><source>BioData Min</source><year>2023</year><volume>16</volume><fpage>4</fpage><pub-id pub-id-type="doi">10.1186/s13040-023-00322-4</pub-id><pub-id pub-id-type="pmcid">PMC9938573</pub-id><pub-id pub-id-type="pmid">36800973</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chicco</surname><given-names>D</given-names></name><name><surname>Jurman</surname><given-names>G</given-names></name></person-group><article-title>The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary classification evaluation</article-title><source>BMC Genomics</source><year>2020</year><volume>21</volume><fpage>6</fpage><pub-id pub-id-type="doi">10.1186/s12864-019-6413-7</pub-id><pub-id pub-id-type="pmcid">PMC6941312</pub-id><pub-id pub-id-type="pmid">31898477</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>BD</given-names></name><name><surname>Van Drongelen</surname><given-names>W</given-names></name><name><surname>Yuchtman</surname><given-names>M</given-names></name><name><surname>Suzuki</surname><given-names>A</given-names></name></person-group><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Trans Biomed Eng</source><year>1997</year><volume>44</volume><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattout</surname><given-names>J</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>Canonical Source Reconstruction for MEG</article-title><source>Comput Intell Neurosci</source><year>2007</year><volume>2007</volume><elocation-id>e67613</elocation-id><pub-id pub-id-type="doi">10.1155/2007/67613</pub-id><pub-id pub-id-type="pmcid">PMC2266807</pub-id><pub-id pub-id-type="pmid">18350131</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolte</surname><given-names>G</given-names></name></person-group><article-title>The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors</article-title><source>Phys Med Biol</source><year>2003</year><volume>48</volume><elocation-id>3637</elocation-id><pub-id pub-id-type="pmid">14680264</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westner</surname><given-names>BU</given-names></name><etal/></person-group><article-title>A unified view on beamformers for M/EEG source reconstruction</article-title><source>NeuroImage</source><year>2022</year><volume>246</volume><elocation-id>118789</elocation-id><pub-id pub-id-type="pmid">34890794</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Huang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-P</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name></person-group><article-title>Automated anatomical labelling atlas 3</article-title><source>NeuroImage</source><year>2020</year><volume>206</volume><elocation-id>116189</elocation-id><pub-id pub-id-type="pmid">31521825</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>MS</given-names></name></person-group><article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title><source>Front Neurosci</source><year>2020</year><volume>14</volume><pub-id pub-id-type="doi">10.3389/fnins.2020.00289</pub-id><pub-id pub-id-type="pmcid">PMC7287158</pub-id><pub-id pub-id-type="pmid">32581662</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname><given-names>S</given-names></name><etal/></person-group><article-title>On the interpretation of weight vectors of linear models in multivariate neuroimaging</article-title><source>NeuroImage</source><year>2014</year><volume>87</volume><fpage>96</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">24239590</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demarchi</surname><given-names>G</given-names></name><name><surname>Sanchez</surname><given-names>G</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Automatic and feature-specific prediction-related neural activity in the human auditory system</article-title><source>Nat Commun</source><year>2019</year><volume>10</volume><elocation-id>3440</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-11440-1</pub-id><pub-id pub-id-type="pmcid">PMC6672009</pub-id><pub-id pub-id-type="pmid">31371713</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>The Ocular Response Function.</title><p><bold>a)</bold> Eye (x) - response (y) relationships are estimated using deconvolution (Boosting), where neural activity (y<sub>train</sub>) is linked to an ocular action (x<sub>train</sub>, e.g. saccades) with time-shifted versions to capture the relationship across time with the neural signal. The TRF (filter kernel, h) represents how the neural signal is related to the eye event over time. This filter is then applied (using convolution) to new eye data (x<sub>test</sub>) to predict the neural response (y<sub>pred</sub>). Finally, the predicted response is compared to the actual neural activity (y<sub>test</sub>) to assess the model fit and evaluate how accurately the TRF captures the brain’s response dynamics. This TRF approach has been previously developed to establish the relationship between experimental events (x) and neural activity (y), but we replace the x vectors with those reflecting ocular action to generate Ocular Response Functions (ORFs). <bold>b)</bold> Specifically, ORFs leverage this approach by predicting resting-state brain activity from three key ocular features (pupil dilation, blinks, and saccades) using a forward encoding model. One ORF is generated per eye feature per source-space location (virtual sensor). To reconstruct (decode) eye movements from brain data, the ORF is simply flipped along the temporal axis. <bold>c)</bold> Models were fitted to data derived from resting-state MEG recordings that were either cleaned of oculomuscular and cardiac artefacts (ICA) or had these components retained (no-ICA). In the present study, the continuous MEG data was first projected into source space. The ORFs were then estimated directly on the time series of each source-space voxel, resulting in anatomically localised transfer functions. Finally, principal component analysis (PCA) was applied to the resulting source-space ORFs to summarise the data and identify the dominant spatiotemporal patterns of eye-brain coupling.</p></caption><graphic xlink:href="EMS200366-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>The approach for obtaining encoding and decoding results.</title><p><bold>a)</bold> Encoding evaluation. A predicted neural time course is generated by convolving ocular data with a derived ORF. This prediction is then correlated with the actual neural recordings to assess model fit at each brain location. <bold>b)</bold> Decoding evaluation. A predicted ocular time course is generated by convolving neural data with a time-flipped ORF. This prediction is then correlated with the actual ocular recordings. <bold>c &amp; d)</bold> Schematic of the PCA-based analysis and hypothesised results. The diagrams illustrate how PCA is used to identify dominant components from the source-space ORFs, allowing for a comparison between data without (c) and with (d) oculomuscular artifact correction. The illustration reflects the hypothesis that uncorrected data would be dominated by large-scale frontal artifacts originating from eye movements, while corrected data would reveal more distributed neural sources.</p></caption><graphic xlink:href="EMS200366-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Encoding results for the first principal component (PC1) of the Ocular Response Functions.</title><p>Results are shown for data without (no-ICA) and with (ICA) oculomuscular artifact correction. Without ICA, activity is dominated by frontocentral, posterior medial, and occipital activity. After ICA, topographies shift to retain occipital loadings and additionally reveal clearer neural sources in cerebellar regions. <bold>a-c)</bold> No-ICA Results: <bold>a)</bold> Pupil dilation: The component showed primary loadings over posterior medial regions (e.g., precuneus) and peaked at ~ -400 ms. <bold>b)</bold> Eye blinks: The component was localised to frontocentral regions, consistent with muscle artifact, and peaked around 0 ms. <bold>c)</bold> Saccades: The component loaded maximally on the occipital cortex (e.g., calcarine sulcus) with a peak at ~ +100 ms, reflecting a strong visual response. <bold>d-f)</bold> ICA-Cleaned Results: <bold>d)</bold> Pupil dilation: After artifact correction, the component's primary loading shifted to occipital visual areas, with a peak at ~ -300 ms. <bold>e)</bold> Eye blinks: The component was now localised to the cerebellum and exhibited a more complex response with a secondary peak around 150-200 ms. <bold>f)</bold> Saccades: The component remained localised to the occipital visual cortex with a peak at ~ +100 ms, confirming a robust, non-muscular neural source. <italic>General Notes</italic>: Shaded error bars represent 95% CI’s. Anatomical labels were obtained using the automated anatomical labelling atlas 3 (AAL3). Statistics were performed using one-sample Student’s t-tests. ‘n.s.’: not significant; ‘*<italic>’</italic>: <italic>p</italic> &lt; 0.05; <italic>‘</italic>**’: <italic>p</italic> &lt; 0.01; ‘***’: <italic>p</italic> &lt; 0.001. <italic>N</italic> = 29.</p></caption><graphic xlink:href="EMS200366-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Reconstruction performance for ocular features from resting-state data.</title><p>Reconstruction was performed using all source-space ORFs ('All') or only those weighted by the first principal component ('PC1'). <bold>a)</bold> No-ICA preprocessing: Using 'All' sources yielded significant decoding for all features (Pupil: <italic>r</italic> = 0.12, <italic>t</italic>(28) = 20.73, <italic>p</italic> &lt; .001, <italic>d</italic> = 3.85; Blinks: <italic>MCC</italic> = 0.18, <italic>t</italic>(28) = 15.91, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.95; Saccades: <italic>MCC</italic> = 0.13, <italic>t</italic>(28) = 21.37, <italic>p</italic> &lt; .001, <italic>d</italic> = 3.97). Using only 'PC1' resulted in lower but still highly significant performance (Pupil: <italic>r</italic> = 0.03, <italic>t</italic>(28) = 8.66, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.61; Blinks: <italic>MCC</italic> = 0.10, <italic>t</italic>(28) = 11.62, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.16; Saccades: <italic>MCC</italic> = 0.05, <italic>t</italic>(28) = 10.25, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.90). <bold>b)</bold> ICA preprocessing: After ICA, decoding with 'All' sources remained strong (Pupil: <italic>r</italic> = 0.09, <italic>t</italic>(28) = 13.01, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.42; Blinks: <italic>MCC</italic> = 0.15, <italic>t</italic>(28) = 14.92, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.77; Saccades: <italic>MCC</italic> = 0.10, <italic>t</italic>(28) = 15.05, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.79). Performance with 'PC1' was also significant (Pupil: <italic>r</italic> = 0.03, <italic>t</italic>(28) = 9.31, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.73; Blinks: <italic>MCC</italic> = 0.08, <italic>t</italic>(28) = 10.69, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.99; Saccades: <italic>MCC</italic> = 0.04, <italic>t</italic>(28) = 11.32, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.10). Statistics reflect one-sample Student’s t-tests against zero. Error bars reflect standard error of the mean. ‘n.s.’: not significant; ‘*<italic>’</italic>: <italic>p</italic> &lt; 0.05; <italic>‘</italic>**’: <italic>p</italic> &lt; 0.01; ‘***’: <italic>p</italic> &lt; 0.001. <italic>N</italic> = 29.</p></caption><graphic xlink:href="EMS200366-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Oculomotor contributions to auditory regularity processing.</title><p><bold>a)</bold> Task Schematic. Participants listened to tone sequences that alternated between ordered and random patterns, while instructed to direct their gaze to a central cross. Multivariate pattern analysis (MVPA) was used to decode the regularity condition from the neural data. <bold>b)</bold> MVPA Decoding Performance. Classification was performed on both actual MEG data and the ‘predicted’ saccade-related activity. Cluster-based permutation tests revealed a significant difference between conditions for both analyses (MEG: <italic>p</italic> &lt; .001; Saccade-predicted: <italic>p</italic> = .013). Shaded areas indicate the temporal extent of the clusters driving this significance: 100-190 ms for the MEG data, and the full -100 to 400 ms window for the saccade-predicted data. <bold>c)</bold> Source-Projected Classifier Weights. Topographies show brain regions informative for decoding regularity. For actual MEG data, informative patterns originated from the right temporal area and left precentral gyrus. For predicted saccade data, patterns were strongest in the bilateral precentral gyri and left occipital cortex. The overlap highlights shared neural substrates in left frontal and precentral regions, and the right temporal cortex and midbrain. General notes: Weights were averaged across the significant decoding window for MEG (100-190 ms) and thresholded at 75% for visualisation. ‘n.s.’: not significant; ‘*<italic>’</italic>: <italic>p</italic> &lt; 0.05; <italic>‘</italic>**’: <italic>p</italic> &lt; 0.01; ‘***’: <italic>p</italic> &lt; 0.001. <italic>N</italic> = 28.</p></caption><graphic xlink:href="EMS200366-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Ocular feature reconstruction from task-related neural activity.</title><p>Resting-state derived sensor ORFs were used to reconstruct the time series of each ocular feature from neural data recorded during the passive listening task. Performance is shown for the 'Ordered' and 'Random' auditory conditions. <bold>a)</bold> No-ICA preprocessing: Ordered: Reconstruction was significant for all features (Pupil: <italic>r</italic> = 0.07, <italic>t</italic>(27) = 11.24, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.12; Blinks: <italic>MCC</italic> = 0.44, <italic>t</italic>(27) = 12.47, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.36; Saccades: <italic>MCC</italic> = 0.05, <italic>t</italic>(27) = 4.59, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.87). Random: Reconstruction was also significant for all features (Pupil: <italic>r</italic> = 0.06, <italic>t</italic>(27) = 10.39, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.96; Blinks: <italic>MCC</italic> = 0.44, <italic>t</italic>(27) = 13.17, <italic>p</italic> &lt; .001, <italic>d</italic> = 2.49; Saccades: <italic>MCC</italic> = 0.06, <italic>t</italic>(27) = 4.59, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.87). <bold>b)</bold> ICA preprocessing: Ordered: After ICA, reconstruction remained significant (Pupil: <italic>r</italic> = 0.05, <italic>t</italic>(27) = 8.28, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.56; Blinks: <italic>MCC</italic> = 0.18, <italic>t</italic>(27) = 7.44, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.41; Saccades: <italic>MCC</italic> = 0.05, <italic>t</italic>(27) = 4.32, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.82). Random: Performance was also significant in the random condition (Pupil: <italic>r</italic> = 0.05, <italic>t</italic>(27) = 8.93, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.69; Blinks: <italic>MCC</italic> = 0.19, <italic>t</italic>(27) = 6.83, <italic>p</italic> &lt; .001, <italic>d</italic> = 1.29; Saccades: <italic>MCC</italic> = 0.05, <italic>t</italic>(27) = 4.27, <italic>p</italic> &lt; .001, <italic>d</italic> = 0.81). Statistics were performed using one-sample Student’s t-tests. Error bars reflect standard error of the mean. ‘n.s.’: not significant; ‘*<italic>’</italic>: <italic>p</italic> &lt; 0.05; <italic>‘</italic>**’: <italic>p</italic> &lt; 0.01; ‘***’: <italic>p</italic> &lt; 0.001. <italic>N</italic> = 28.</p></caption><graphic xlink:href="EMS200366-f006"/></fig></floats-group></article>