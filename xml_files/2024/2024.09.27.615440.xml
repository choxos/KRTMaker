<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199090</article-id><article-id pub-id-type="doi">10.1101/2024.09.27.615440</article-id><article-id pub-id-type="archive">PPR917475</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Fast hierarchical processing of orthographic and semantic parafoveal information during natural reading</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Lijuan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Frisson</surname><given-names>Steven</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">#</xref></contrib><contrib contrib-type="author"><name><surname>Pan</surname><given-names>Yali</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">#</xref></contrib><contrib contrib-type="author"><name><surname>Jensen</surname><given-names>Ole</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">#</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Centre for Human Brain Health, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>University of Birmingham</institution></institution-wrap>, Edgbaston, <postal-code>B15 2TT</postal-code>, <city>Birmingham</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Experimental Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <postal-code>OX2 6GG</postal-code>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0172mzb45</institution-id><institution>Oxford Centre for Human Brain Activity</institution></institution-wrap>, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0172mzb45</institution-id><institution>Wellcome Centre for Integrative Neuroimaging</institution></institution-wrap>, Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <postal-code>OX3 7JX</postal-code>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1"><label>*</label> co-corresponding: <email>o.jensen@bham.ac.uk</email>; <email>lxw203@student.bham.ac.uk</email></corresp><fn id="FN1"><label>#</label><p id="P1">co-senior authorship</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>01</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>28</day><month>09</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Readers extract orthographic and semantic information from parafoveal words before fixating on them. While this has to be achieved within an intersaccadic interval, the neuronal mechanisms supporting this fast parafoveal word processing within the language network remain unknown. We co-registered MEG and eye-tracking data in a natural reading paradigm to uncover the neuronal mechanisms supporting parafoveal processing. Representational similarity analysis (RSA) revealed that parafoveal orthographic neighbours (e.g., “writer” vs. “waiter”) showed higher representational similarity than non-neighbours (e.g., “writer” vs. “police”), emerging ~68 ms after fixation onset on the preceding word (e.g., “clever”) in the visual word form area. Similarly, parafoveal semantic neighbours (e.g., “writer” vs. “author”) exhibited increased representational similarity at ~137 ms in the left inferior frontal gyrus. Importantly, the degree of orthographic and semantic parafoveal processing predicted individual reading speed. Our findings suggest fast hierarchical processing of parafoveal words across distinct brain regions, which enhances reading efficiency.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Reading is a seemingly effortless process that allows us to absorb vast amounts of information quickly. To read efficiently, readers not only process currently fixated words in the fovea but also pre-process upcoming words in the parafovea<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Studies have shown that masking parafoveal words can severely impair reading speed<sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>. This suggests that parafoveal processing is essential for fluent reading, as it allows readers to extract information from the to-be-focused word, providing a head start and thus reducing processing time when the word is subsequently fixated<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref></sup>. In some cases, if a word has been sufficiently processed in the parafovea, the reader may even skip it altogether<sup><xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R10">10</xref></sup>. In reading research, parafoveal processing has been intensely investigated using eye-tracking<sup><xref ref-type="bibr" rid="R1">1</xref></sup> and event/fixation-related potentials (ERPs/FRPs)<sup><xref ref-type="bibr" rid="R11">11</xref></sup>, with foci on the types of information processed and the associated timing. However, these studies have predominantly focused on a single level of word information during parafoveal processing. The current study aims to investigate how different levels of information from the same word, specifically orthography and semantics, are organized temporally (i.e., time course) and spatially (i.e., brain regions) during parafoveal processing.</p><p id="P4">Eye-tracking studies have provided valuable insights into the types of information that can be extracted from parafoveal words using the boundary paradigm<sup><xref ref-type="bibr" rid="R12">12</xref></sup>. In this paradigm, an invisible boundary is placed just to the left of the <italic>target word</italic>. While the reader’s gaze remains to the left of this boundary, a <italic>preview word</italic> occupies the position of the target word. Once the eyes cross the boundary, the preview word is replaced by the target word. If the preview word shares certain linguistic features with the target word, such as orthography<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R18">18</xref></sup> (e.g., “sweet” and “sleet”), phonology<sup><xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R23">23</xref></sup> (e.g., “sweet” and “suite”), or semantics<sup><xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R28">28</xref></sup> (e.g., “sweet” and “sugar”), the reader’s fixation duration on the target word is reduced—a phenomenon known as the <italic>preview benefit</italic> (for a review see<sup><xref ref-type="bibr" rid="R1">1</xref></sup>). While the preview benefit can demonstrate the existence of parafoveal information extraction, it cannot reveal the exact timing of this extraction, i.e., the time points during parafoveal processing at which specific word information is extracted.</p><p id="P5">Electrophysiological studies have provided valuable insights into the time course of orthographic<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R32">32</xref></sup> and semantic parafoveal processing<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R33">33</xref>–<xref ref-type="bibr" rid="R42">42</xref></sup>. Most of these studies have used passive reading paradigms, such as the flanker paradigm, in which sentences are presented word-by-word at fixation and flanked by parafoveal words<sup><xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R39">39</xref></sup>, and ERPs were obtained. Other studies have employed more natural saccadic reading paradigms, co-registering eye-tracking and EEG to obtain FRPs<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>. Early ERP components, such as the P1 and N1, have been shown to reflect orthographic parafoveal processing, with their amplitudes modulated by the orthographic properties of parafoveal words<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup>. Semantic parafoveal processing, in contrast, has been indexed by the N400 component, with larger N400 amplitudes observed for parafoveal words that are semantically incongruent with the sentence context compared to congruent ones<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R41">41</xref></sup>. However, the parafoveal N400 effect typically occurs over 250 ms after fixation onset on the pretarget word—too late to influence saccade planning to the parafoveal target word<sup><xref ref-type="bibr" rid="R6">6</xref></sup>, as typical fixation durations during natural reading are ~200 ms. Consequently, the parafoveal N400 effect likely captures a later stage of parafoveal semantic processing. Earlier neural mechanisms reflecting the onset of parafoveal semantic processing may exist but have yet to be identified.</p><p id="P6">Recently, a newly developed technique, rapid invisible frequency tagging (RIFT), has shown potential for measuring the early onset of parafoveal processing during natural reading. This technique involves subliminally flickering the location of the parafoveal word at a high frequency, such as 60 Hz. Although readers do not perceive the visual flicker, tagging responses can still be detected in the brain and used as an indicator of the amount of attention allocated to the parafoveal word. Studies have shown that these tagging responses are modulated by the lexical<sup><xref ref-type="bibr" rid="R43">43</xref></sup> and semantic<sup><xref ref-type="bibr" rid="R42">42</xref></sup> information of the parafoveal word within 100 ms of fixating the preceding word. Despite providing early onset timing information, RIFT only identifies the attentional modulation effect of the parafoveal word rather than the information extraction itself. Furthermore, due to the limitation in the brain regions that respond to visual flickers, the parafoveal processing effect can only be observed in the visual cortex, which is well known to be outside of the typical language processing network<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup>. Therefore, a new technique that can directly measure the extracted parafoveal information with reliable spatial localization is needed.</p><p id="P7">Representational similarity analysis (RSA) combined with Magnetoencephalography (MEG)<sup><xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup> offers a unique opportunity to directly measure extracted parafoveal information with excellent temporal and spatial sensitivity. RSA is based on the assumption that items sharing similarities in specific aspects produce similarly distributed patterns of neural activity compared to dissimilar items<sup><xref ref-type="bibr" rid="R46">46</xref></sup>. As such, it can be used to measure multiple levels of representation by manipulating similarities across different aspects of items. When combined with electrophysiological recordings, RSA can reveal when specific types of information are represented in the brain<sup><xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R48">48</xref></sup>. Moreover, RSA can be applied in the source space of MEG data with a searchlight approach to identify the brain regions producing this representational similarity. Several studies have employed RSA with EEG/MEG data to investigate the time course of preactivation of semantic information during language comprehension<sup><xref ref-type="bibr" rid="R49">49</xref>–<xref ref-type="bibr" rid="R52">52</xref></sup>. These studies compared the similarity of neural activity patterns in the interval when the same words (within-pairs) versus different words (between-pairs) could be predicted in a passive reading paradigm. In the present study, we embraced a similar RSA approach in a natural reading paradigm.</p><p id="P8">Our study aims to address how different levels of information from the same word are processed before the word is fixated upon—a question previously unaddressed due to methodological limitations. Specifically, we investigate two core questions: (1) Can we identify specific representational activities associated with orthographic and semantic information of a parafoveal word before it is fixated upon? (2) If so, how are these two levels of parafoveal information organized temporally (e.g., serially or in parallel) and spatially (across brain regions)? To investigate these questions, we simultaneously recorded MEG and eye movements in a natural reading paradigm (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). We selected a set of critical words (e.g., “writer”), each paired with an orthographic neighbour (e.g., “waiter”) and a semantic neighbour (e.g., “author”). Each critical word and its two neighbours formed a triplet of target words and were embedded in different sentences. Each target triplet was paired with another triplet (e.g., “police/policy/guards”). Importantly, pre-target words were the same (e.g., “clever”) within each triplet and between its matched triplet, i.e., all six sentences shared identical pre-target words (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). The RSA analysis was focused on the pre-target fixation period—when target words are in the parafovea (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Any difference in representational similarity between orthographically similar target words (e.g., “writer” &amp; “waiter”) and unrelated words during this period would indicate parafoveal orthographic processing. Similarly, differences between semantically similar words (e.g., “writer” &amp; “author”) and unrelated words would indicate parafoveal semantic processing. Finally, we employed a searchlight method to identify the brain areas supporting orthographic and semantic parafoveal processing. In short, our design and methodology allow us, for the first time, to track whether and when orthographic and semantic information is extracted from the <italic>same</italic> upcoming word and which brain areas are involved in these processes.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Hierarchical parafoveal processing</title><p id="P9">Using RSA, we first compared the similarity of MEG activity patterns for orthographically similar target words (orthographic within-pairs) with those for dissimilar target words (between-pairs) during the pre-target fixation period (<xref ref-type="fig" rid="F2">Fig. 2</xref>). MEG data were segmented into epochs of -0.2 to 0.5 s, relative to the onset of the first fixation on pre-target words. For each participant, we computed pairwise correlations between the spatial activation patterns of all MEG sensors for orthographically similar target words (orthographic within-pairs, e.g., “writer” and “waiter” in <xref ref-type="fig" rid="F1">Fig. 1b</xref>) and averaged these correlations. We also calculated the average correlation for between-pairs (e.g., “writer” and “police”). This process was repeated at every millisecond within the interval, yielding time series for both orthographic within-pair correlations (<italic>R<sub>orth</sub>(t)</italic>) and between-pair correlations (<italic>R<sub>between</sub>(t)</italic>). Finally, we calculated the grand average of these time series across all participants.</p><p id="P10">We found that the similarity in brain activity was higher when the parafoveal target words were orthographically similar, compared to when they were dissimilar (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). This difference, reflecting parafoveal orthographic previewing, was significant in the 68–186 ms interval after the fixation onset on the pre-target words (shaded area in <xref ref-type="fig" rid="F3">Fig. 3a</xref>, cluster-based permutation test: <italic>p</italic> &lt;.001, 5000 iterations). This finding provides neuronal evidence for parafoveal orthographic processing emerging at ~68 ms after the fixation onset on the pre-target word.</p><p id="P11">We followed a similar procedure to investigate semantic parafoveal processing. We found that the similarity of MEG activity patterns was higher for semantically similar parafoveal target words compared to dissimilar words (<xref ref-type="fig" rid="F3">Fig. 3b</xref>); this difference was significant in the 137–247 ms after the fixation onset on the pre-target words (shaded area in <xref ref-type="fig" rid="F3">Fig. 3b</xref>, cluster-based permutation test: <italic>p</italic> &lt;.001, 5000 iterations). To confirm that the observed semantic effect was not produced by the contextual information preceding the target words, we evaluated the semantic similarity of contexts between the paired sentences preceding the target words using latent semantic analysis (LSA)<sup><xref ref-type="bibr" rid="R53">53</xref></sup> (See <xref ref-type="sec" rid="S8">Methods</xref>). The results showed no difference in the LSA score when comparing the contexts for within-pair and between-pair target words (<italic>t</italic>(119) = - 0.33, <italic>p</italic> =.741), ruling out the possibility that the effect is from the semantic similarity of sentence contexts. These findings provide neuronal evidence demonstrating that parafoveal words are processed at the semantic level at ~137 ms after the onset of fixation on the pre-target word.</p><p id="P12">Considering the above neuronal evidence, parafoveal processing is not limited to low-level orthographic information but extends to high-level semantic information. Our results also demonstrated temporally distinct stages in parafoveal processing: low-level orthographic information is available first, and higher-level semantic information is available soon after, and these happen within an intersaccadic interval.</p></sec><sec id="S4"><title>Parafoveal processing was related to reading speed</title><p id="P13">To investigate whether the extraction of parafoveal orthographic and semantic information affects reading proficiency, we computed the correlation between these neuronal effects and individual reading speed. We assessed the magnitude of orthographic and semantic previewing effects by averaging the differences in R values when significant differences in the RSA analysis emerged (orthography: 68–186 ms; semantics: 137–247 ms after the fixation onset of the pre-target word). The reading speed of each participant was measured as the number of words read per second, calculated by dividing the total number of words across all sentences by the participant’s total reading time. Our analysis revealed that both orthographic and semantic previewing effects significantly correlated with individual reading speed (orthography: <xref ref-type="fig" rid="F3">Fig. 3c</xref>, R = 0.42, <italic>p</italic> =.011; semantics: <xref ref-type="fig" rid="F3">Fig. 3d</xref>, R = 0.34, <italic>p</italic> =.044, Spearman correlation), suggesting that individuals who extracted more orthographic and semantic information from parafoveal words tended to be faster readers. The orthographic and semantic previewing effects were not correlated (R = 0.12, p =.488, Spearman correlation), indicating that these two types of parafoveal information extraction contribute independently to reading speed.</p></sec><sec id="S5"><title>Neuronal sources underlying parafoveal processing</title><p id="P14">To identify the neuronal sources underlying the observed orthographic and semantic parafoveal processing, we performed RSA with a searchlight method at both sensor and source levels. Sensor-level analysis was conducted separately for magnetometers and gradiometers, using searchlight patches of 20 sensors for magnetometers and 40 sensors for gradiometers (the gradiometers were arranged in pairs of two at each recording location in the Neuromag system). For the orthographic parafoveal processing, we compared the similarity of neural activity patterns for orthographic within- to between-pairs (Δ<italic>R<sub>orthography</sub></italic>) within each searchlight patch, during the interval when the orthographic previewing effect was robust (68−186 ms in <xref ref-type="fig" rid="F3">Fig. 3a</xref>). We found that sensors involved in orthographic parafoveal processing spanned occipital, temporal, parietal, and frontal regions, exhibiting a notable left-lateralized distribution (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). This was observed in large clusters identified using a cluster-randomization approach in both magnetometers (<italic>p</italic> &lt;.001) and gradiometers (<italic>p</italic> &lt;.001). For the source-level analysis, individual-subject source-reconstructed MEG signals were obtained using a brain surface-constrained minimum norm estimate approach and were converted to a common space (See <xref ref-type="sec" rid="S8">Methods</xref> for details). We then applied searchlight RSA to the source data, using a searchlight consisting of 2000 vertices across the cortical surface (out of 20,484 vertices). The vertices showing the maximal difference between orthographic-within- and between-pairs emerged in the left ventral occipitotemporal region (lvOT; <xref ref-type="fig" rid="F4">Fig. 4b</xref> left)—a region overlapping with the Visual Word-Form Area (VWFA)<sup><xref ref-type="bibr" rid="R54">54</xref></sup>.</p><p id="P15">A searchlight RSA approach was also used to identify the neuronal sources of semantic parafoveal processing in the 137−247 ms interval (in <xref ref-type="fig" rid="F3">Fig. 3b</xref>), mirroring the approach used for orthographic parafoveal processing. The topography of sensor-level data using gradiometers (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, left) showed that the greatest difference in representational similarity between semantic-within pairs and between-pairs (Δ<italic>R<sub>semantics</sub></italic>) was produced over frontal and left temporal regions (<italic>p</italic> =.04). The topography of sensor-level data using magnetometers (<xref ref-type="fig" rid="F4">Fig. 4c</xref>, right) showed that the greatest R-values difference was over frontal regions (<italic>p</italic> =.008). A source-level searchlight approach revealed that the neuronal generator of the parafoveal semantic effect was localized at the Left Inferior Frontal Gyrus (LIFG) (<xref ref-type="fig" rid="F4">Fig. 4d</xref> left)—a core region of the language network.</p><p id="P16">We thus propose a hierarchical model of parafoveal processing, wherein low-level orthographic processing, facilitated by the Visual Word Form Area (VWFA), precedes higher-level semantic processing, supported by the Left Inferior Frontal Gyrus (LIFG).</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P17">The present study aims to understand how low-level word information, such as orthography, and high-level word information, such as semantics, are extracted before a word is fixated during natural reading. By applying representational similarity analysis to co-registered MEG and eye-tracking data, we found neuronal evidence that, ~68 ms after fixation onset on a word (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), orthographic processing of the parafoveal word is initiated in the visual word form area (VWFA) (<xref ref-type="fig" rid="F4">Fig. 4b</xref>). Subsequently, semantic processing of the parafoveal word is initiated at ~137 ms (<xref ref-type="fig" rid="F3">Fig. 3b</xref>), supported by the left inferior frontal gyrus (LIFG) (<xref ref-type="fig" rid="F4">Fig. 4d</xref>). This hierarchical organization, both temporally and spatially, allows for efficient pre-processing of different levels of parafoveal information, facilitating faster reading (<xref ref-type="fig" rid="F3">Fig. 3c &amp; d</xref>).</p><p id="P18">We provide direct neuronal evidence for parafoveal processing occurring at both orthographic and semantic levels, demonstrating that such parafoveal pre-processing is indeed “deep”. Although previous eye-tracking and ERPs/FRPs studies have provided evidence for orthographic and semantic parafoveal processing, they have largely approached this question indirectly. Eye-tracking studies<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R28">28</xref></sup> have inferred the existence of parafoveal processing by showing that exposure to a word’s features in the parafovea leads to shorter fixation durations when the word is later fixated upon, i.e., already in the foveal region. Electrophysiological studies<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R41">41</xref></sup> have used contrast design to compare event/fixation-related potentials (ERPs/FRPs) for different types of parafoveal words, such as orthographic regular versus irregular or semantic congruent versus incongruent words. These differences in ERPs/FRPs were used to infer the extraction of orthographic or semantic information from parafoveal words. Here, we employed a multivariate approach to directly analyse the distributed brain activity associated with parafoveal information. We thus provide neural evidence that both orthographic and semantic information can be extracted from the parafoveal word, supporting the notion of parallel processing across multiple words during natural reading<sup><xref ref-type="bibr" rid="R55">55</xref></sup>. Our approach was inspired by other studies that used EEG/MEG to decode semantic representations associated with predictions in sentences presented word-byword using RSA<sup><xref ref-type="bibr" rid="R49">49</xref>–<xref ref-type="bibr" rid="R52">52</xref></sup>. The RSA methodology seems to overcome some limitations of other multivariate techniques based on the classification of semantic word content<sup><xref ref-type="bibr" rid="R56">56</xref></sup>.</p><p id="P19">It is important to note that we used the same pre-target word (e.g., “clever”) in each sentence sextet (<xref ref-type="fig" rid="F1">Fig. 1b</xref>), ensuring that differences in representational similarity between orthographic/semantic within-pairs and between-pairs during pre-target fixation intervals could only be attributed to previewing the parafoveal target words. Moreover, the observed previewing effects were not influenced by the predictability of parafoveal words, as all target words had low cloze probability values. Additionally, the semantic similarity of the context preceding the target words was controlled (for details, see Behavioural pre-test in <xref ref-type="sec" rid="S8">Methods</xref>). Therefore, the present findings cannot be explained by facilitatory effects of predictability or semantic similarity in context.</p><p id="P20">Our results demonstrated a temporal hierarchy of processing different levels of word information during parafoveal reading: the low-level orthographic information is available at ~68 ms (<xref ref-type="fig" rid="F3">Fig. 3a</xref>) and higher-level semantic information is available at ~137 ms (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). The timing of orthographic parafoveal processing aligns well with the timing of visual information reaching the visual (~50 ms) and the temporal cortices (~70 ms)<sup><xref ref-type="bibr" rid="R57">57</xref></sup>. However, there is a debate as to when semantic information is derived. Previous work investigating the neuronal substrate of semantic parafoveal processing has characterized the N400-type ERPs/FRPs in response to parafoveal words being semantically incongruent compared to congruent with sentence context<sup><xref ref-type="bibr" rid="R33">33</xref>–<xref ref-type="bibr" rid="R41">41</xref></sup>. How does the timing of the N400 (emerging at 250–300 m) relate to the ~137 ms onset of semantic parafoveal processing found in our current study? As we typically saccade every 200−300 ms during natural reading as we read 3–4 words per second, the parafoveal N400 emerges too late to impact saccade planning and early semantic processing. The N400- type studies might reflect the integration of the target word into sentence context beyond just deriving the semantics of a given word. The timing for orthographic and semantic parafoveal processing we found is aligned with the estimated timing constraints, it is sufficiently early to impact the next saccade goal (e.g., to skip)<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R58">58</xref></sup>. To the best of our knowledge, the current study is the first to compare the neuronal time course of parafoveal orthographic and semantic information extraction of the same word at the representational level during natural reading, revealing hierarchical parafoveal processing. It is worth noting that the same between-pair (e.g. “writer” and “police”) was used as a baseline for both orthographic and semantic within-pair, which allowed us to directly compare the time course of orthographic and semantic parafoveal processing.</p><p id="P21">The neural sources underlying orthographic and semantic parafoveal processing were found to follow a hierarchical organization. By applying a searchlight approach, we found RSA patterns in the left ventral occipitotemporal cortex (lvOT) associated with the processing of parafoveal orthographic information. The lvOT has been labelled the Visual Word-Form Area (VWFA)<sup><xref ref-type="bibr" rid="R54">54</xref></sup> as it plays a significant role in orthographic processing. Our findings extend prior insights into the lvOT by demonstrating that it is engaged not only in foveal but also in parafoveal orthographic processing. Our study also provided novel evidence that parafoveal semantic processing is supported by the left inferior frontal gyrus (LIFG)—a region classically associated with various aspects of semantic processing, including the retrieval of lexical-semantic knowledge<sup><xref ref-type="bibr" rid="R59">59</xref></sup>, semantic decision<sup><xref ref-type="bibr" rid="R60">60</xref>–<xref ref-type="bibr" rid="R62">62</xref></sup>, semantic integration/unification<sup><xref ref-type="bibr" rid="R63">63</xref>,<xref ref-type="bibr" rid="R64">64</xref></sup> and semantic short-term memory<sup><xref ref-type="bibr" rid="R65">65</xref>,<xref ref-type="bibr" rid="R66">66</xref></sup>. The LIFG has also been reported as a key region for the representation of semantic similarity<sup><xref ref-type="bibr" rid="R67">67</xref>,<xref ref-type="bibr" rid="R68">68</xref></sup> and the abstractness dimension of word meaning<sup><xref ref-type="bibr" rid="R69">69</xref></sup>. However, other brain areas known to be involved in semantic processing, e.g., the left inferior temporal gyrus and the anterior temporal lobe, did not show evidence of representing semantic similarity in our study. This may be explained by MEG being less sensitive to regions (e.g. the ATL) further away from the sensors. Therefore, the absence of evidence of other regions known to be involved in semantic processing should not be over-interpreted.</p><p id="P22">We have thus far elucidated the time course and spatial localization of orthographic and semantic representations extracted from parafoveal words. But what is the behavioural significance of these neural representations of parafoveal information? Our correlation results indicate that the ability to extract orthographic and semantic information from parafoveal words is positively correlated with individual reading speed. This supports the notion that parafoveal processing is necessary for fluent reading<sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>. Given this importance of parafoveal processing, interventions aimed at training individuals to read faster could involve strategies promoting the extraction of parafoveal information. This might also be of significance for interventions supporting individuals with reading disorders.</p></sec><sec id="S7" sec-type="conclusions"><title>Conclusions</title><p id="P23">In summary, we applied RSA to co-registered MEG and eye-tracking data to uncover the neuronal mechanisms associated with orthographic and semantic parafoveal processing during natural reading. We found that orthographic parafoveal processing emerges already at ~68 ms after the fixation onset on the pre-target word, followed by semantic parafoveal processing at ~137 ms. We further identified the VWFA and LIFG as the neural sources of orthographic and semantic parafoveal processing, respectively. This parafoveal previewing predicted individual reading speed, with faster readers relying more on parafoveal processing. Our results provide evidence for fast hierarchical parafoveal processing within an intersaccadic interval supported by the language network.</p></sec><sec id="S8" sec-type="methods"><title>Methods</title><sec id="S9" sec-type="subjects"><title>Participants</title><p id="P24">We recruited 39 native English speakers (24 females), aged 21 ± 2.3 (mean ± SD). All participants were right-handed, had normal or corrected-to-normal eyesight, and had no known neurological or reading disorders (e.g., dyslexia). 4 participants were excluded from the data analysis due to excessive head movement, poor eye tracking, or too many bad sensors during the recordings, leaving a total of 35 participants (22 females) for analysis. Participants provided written informed consent and were compensated £15 per hour for their participation. The study was approved by the University of Birmingham Ethics Committee (under the approved Programme ERN_18-0226P).</p></sec><sec id="S10"><title>Stimuli</title><p id="P25">The stimuli consisted of 360 plausible one-line sentences, each embedded with an unpredictable target word (the plausibility of the sentences and the unpredictability of the target words were confirmed by behavioural pre-tests, details provided below). The target words were always preceded and followed by at least 3 words within the sentences. The length of target words ranged from 3 to 8 letters, with an average length of 5.1 letters. We constructed sentences in 120 triplets. Within each sentence triplet, the target word could be a critical word (e.g., “writer”), its orthographic neighbour (e.g., “waiter”) differing by only one or two letters (116 instances with a one-letter difference, 4 with a two-letter difference), or its semantic neighbour (e.g., “author”) with a highly similar meaning. Each sentence triplet was paired with another triplet that had a similar structure of target words (e.g., “police/policy/guards”) (see <xref ref-type="fig" rid="F1">Fig. 1b</xref>). This pairing established orthographic within-pair (e.g., “writer–waiter”) and semantic within-pair relationships (e.g., “writer−author”), while also providing unrelated between-pair controls (e.g., “writer–police”). Within each sentence sextet (a group of six sentences formed by pairing two triplets), the target words had identical lengths. To ensure a same processing of pre-target words when previewing different target words in the parafovea, the pre-target words were identical (e.g., “clever”) within a sentence sextet. Please note that only sentences in a sextet shared the same pre-target words. These 6 sentences were presented separately during the experiment, with an average of 58 other sentences (minimum 35) appearing between them.</p></sec><sec id="S11"><title>Behavioural pre-test</title><sec id="S12"><title>Predictability test of the target words</title><p id="P26">We carried out a cloze norming task to assess the predictability of the target word given the prior context in each sentence. The task involved 25 participants (18 females), all of whom were native English speakers, aged 24.0 ± 4.3 years (mean ± SD), with no reading disorder. None of the participants participated in the MEG experiment. The data were collected via the online survey platform Qualtrics (<ext-link ext-link-type="uri" xlink:href="https://www.qualtrics.com">https://www.qualtrics.com</ext-link>). Participants were presented with the sentence frames up to the target word and were asked to predict the next word in the sentence. See below an example sentence frame for the target word “waiter”:</p><p id="P27">They gave the clever ____________________</p><p id="P28">If more than 25% of participants predicted the target word, then this target word was considered predictable. If more than 65% of participants predicted the same word, though not the target word used in the experiment, the sentence was considered highly constrained. One target word was judged to be highly predictable, and one sentence was highly constrained, so the two sentences were modified and retested with 24 different participants (14 males). In the final version of sentences, no target words were predictable (the average predictability for target words was 0.9% ± 3.0%), and no sentences were highly constrained (the average predictability for the most predicted non-target words was 20.3 ± 10.4 %).</p></sec><sec id="S13"><title>Plausibility test</title><p id="P29">A separate group of participants (24 in total, 14 females, aged 22.2 ± 1.8, mean ± SD) participated in the plausibility test. The data were also collected by Qualtrics. Participants were instructed to read sentences and indicate to what extent they thought the sentences were plausible or acceptable, using a 7-point rating scale (see below for examples). We also included 200 filler sentences, of which 100 were moderately plausible and 100 implausible, to occupy the full range of the plausibility scale. Most filler sentences were chosen from previous studies (Joseph et al., 2008; Matsuki et al., 2011; Patson &amp; Warren 2010). In the example below, the first sentence was from our experimental material, while the second and the third sentences were moderately plausible and implausible, respectively.</p><table-wrap id="T1" position="anchor" orientation="portrait"><table frame="box" rules="all"><thead><tr><th align="center" valign="middle"/><th align="center" valign="middle">Implausible Plausible</th></tr></thead><tbody><tr><td align="left" valign="middle">They gave the clever waiter a tip for his good service.</td><td align="left" valign="middle">1 2 3 4 5 6 7</td></tr><tr><td align="left" valign="middle">The man used a kettle to cook porridge yesterday evening.</td><td align="left" valign="middle">1 2 3 4 5 6 7</td></tr><tr><td align="left" valign="middle">Jeremy quenched his thirst with a glass of program.</td><td align="left" valign="middle">1 2 3 4 5 6 7</td></tr><tr><td align="left" valign="middle">…..</td><td align="left" valign="middle">1 2 3 4 5 6 7</td></tr></tbody></table></table-wrap><p id="P30">The average plausibility rating for the experimental sentences was 6.0 ± 0.5 (mean ± SD), which was significantly higher than both moderately plausible filler sentences (3.2 ± 0.8 mean ± SD; two-tailed paired t-test: t<sub>(23)</sub> = 25.92, p &lt;.001) and implausible filler sentences (1.8 ± 0.6 mean ± SD; two-tailed paired t-test: t<sub>(23)</sub> = 17.39, p &lt;.001).</p></sec><sec id="S14"><title>Semantic relatedness</title><p id="P31">In natural language, words with similar meanings tend to appear in similar contexts. To ensure that the representational similarity was driven solely by the similarity of the target words and not by their preceding contexts, we evaluated the context similarity by computing the Latent Semantic Analysis (LSA) values (<ext-link ext-link-type="uri" xlink:href="http://wordvec.colorado.edu/">http://wordvec.colorado.edu/</ext-link>) between the contexts of the target words. For example, to assess the context similarity of the target words “police” and “guards”, we computed the LSA value between the contexts “The man was found by the clever” and “The manager assigned two clever”.</p><p id="P32">The results showed no difference in the semantic similarity of the contexts between orthographic within-pairs and unrelated between-pairs (two-sided independent samples t-test: <italic>t</italic><sub>(238)</sub> = 0.74, <italic>p</italic> =.462). Similarly, there was no difference in the semantic similarity of the contexts for semantic within-pairs and unrelated between-pairs (twosided independent samples t-test: <italic>t</italic><sub>(238)</sub> = 0.95, <italic>p</italic> =.343). Thus, the representational similarity for orthographically or semantically similar parafoveal words could not be explained by the semantic similarity in the context.</p></sec></sec><sec id="S15" sec-type="methods"><title>Experimental procedure</title><p id="P33">The experiment took place in a dimly lit, magnetically shielded room, where participants were seated under the MEG gantry. The gantry was set at a 60° upright angle and covered the participants’ head entirely. A projection screen, positioned 145 cm from the participants’ eyes, was used to display 360 one-line sentences, which were programmed using Psychophysics Toolbox-3 (Kleiner et al., 2007). The sentences were shown in bold, black text (RGB: [0, 0, 0]), using size-32 Courier New font with equal spacing, on a neutral grey background (RGB: [128, 128, 128]). Each letter and space occupying 0.316 degrees of visual angle, and the sentences typically spanned between 12.64 and 25.60 visual degrees in width. Participants were instructed to silently read the sentences at their own pace while minimizing head and body movement. Their eye movements and brain activity were recorded simultaneously using an eye-tracker and MEG.</p><p id="P34">Each trial started with a fixation cross at the centre of a middle-grey screen, lasting for 1.2−1.6 s. Then a black square (0.4° wide) was presented at the vertical centre of the screen, 1.3 degrees of visual angle from the left edge. Sentence presentation was triggered when participants fixated on this square for at least 0.2s, with the sentence starting from the location of the ‘starting square’. During sentence presentation, a grey “ending square” (RGB: [64, 64, 64], 0.4° wide) was displayed below the centre of the screen, 2.6 degrees of visual angle from the left edge. After reading the sentence, participants fixated on the ending square for 0.1s to terminate the sentence presentation, followed by a 0.5-s blank middle-grey screen. To ensure careful reading, 25% of all sentences were followed by a simple yes-or-no comprehension question (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Participants scored 90% or better in response to the questions (mean accuracy 96.3%, SD = 2.6%). The experiment consisted of ten blocks, with each block containing 36 sentences and taking approximately 5 minutes to read. Following each block, participants were given a rest period of at least 30 seconds. In total, the experiment took about 1 hour.</p></sec><sec id="S16"><title>Data acquisition</title><sec id="S17"><title>MEG</title><p id="P35">MEG data were obtained using a 306-sensor TRIUX Elekta Neuromag system (Elekta, Finland), consisting of 204 orthogonal planar gradiometers and 102 magnetometers. Three bony fiducial points (nasion, left and right preauricular points) were digitized utilizing the Polhemus Fastrack electromagnetic digitizer system before the MEG recording. The digitization process was then extended to include four head-position indicator (HPI) coils, placed on the left and right mastoid bones and on the forehead, ensuring a minimum distance of 3 cm between the two forehead coils. Additionally, over 250 additional points, evenly distributed across the entire scalp, were digitized to assist in aligning the MEG head model with individual structural MRI images. The MEG data were online filtered between 0.1 and 330 Hz with anti-aliasing filters and sampled at 1000 Hz.</p></sec><sec id="S18"><title>Eye movements</title><p id="P36">During the MEG session, we used the EyeLink 1000 Plus eye-tracker (long-range mount, SR Research Ltd, Canada) to collect participants’ eye movement data. The eye tracker was placed on a wooden table between the participants and the projection screen. The distance from the centre of the participant’s eyes to the camera of the eye-tracker was 90 cm. The eye tracker’s centre was aligned with the middle of the projection screen, and its top edge reached the bottom of the screen. With a 1000 Hz sampling rate, the eye-tracker continuously recorded both horizontal and vertical eye positions, as well as pupil size from the left eye of each participant. To ensure accurate and reliable tracking of eye movements, each session started with a nine-point calibration and validation test, which aimed to achieve an accuracy level where the permitted tracking error was below 1 visual degree in both horizontal and vertical dimensions. To maintain accuracy throughout the session, we conducted a one-point drift check every three trials and before the start of each block. Additionally, any failure to trigger sentence presentation through gaze led to an immediate one-point drift-check. If the drift-check error was bigger than 2 degrees, a nine-point calibration and validation was performed.</p><p id="P37">We extracted fixation onset events from the EyeLink output file. Only the first fixation on each word was selected. Fixations that were too long (&gt;1000 ms) or too short (&lt;80 ms), as well as those occurring during regressions or re-reading, were excluded when epoching the MEG data.</p><p id="P38">We also recorded Electrooculography (EOG) data by placing one pair of electrodes approximately 2 cm away from the outer canthus of each eye for horizontal EOG recordings, and another pair above and below the right eye in line with the pupil for vertical EOG recordings.</p></sec><sec id="S19"><title>MRI</title><p id="P39">Following MEG data acquisition, participants were scheduled for a separate visit to have an MRI scan. The T1-weighted structural MRI image was acquired with a 3-Tesla Siemens Magnetom Prisma scanner (TR = 2000 ms, TE = 2.01 ms, TI = 880 ms, flip angle = 8°, FOV = 256×256×208 mm, isotropic voxel = 1 mm). For the 7 participants who dropped out of the MRI scan, we utilized the standard FreeSurfer (version 6) average subject named “fsaverage” to do source modelling.</p></sec></sec><sec id="S20"><title>MEG data analyses</title><sec id="S21"><title>Pre-processing</title><p id="P40">MEG data were analysed using MNE python<sup><xref ref-type="bibr" rid="R70">70</xref></sup> (version 1.3.0) and following the FLUX pipeline<sup><xref ref-type="bibr" rid="R71">71</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://www.neuosc.com/flux">https://www.neuosc.com/flux</ext-link>). First, we identified sensors with excessive artefacts using a semi-automatic detection algorithm (on average about 6 faulty sensors per participant). Signal-Space Separation (SSS) and Maxwell filtering<sup><xref ref-type="bibr" rid="R72">72</xref></sup> were then applied to reduce artifacts from environmental sources and sensor noise. Faulty sensors were repaired via SSS, ensuring that all 306 MEG sensors were ultimately used for each participant. The data were down-sampled to 200Hz and bandpass filtered at 1–40 Hz prior to performing Independent Component Analysis (ICA). The fast ICA algorithm<sup><xref ref-type="bibr" rid="R73">73</xref></sup> was then applied to decompose the data into 30 independent components. Components containing ocular (eye blinks and eye movements), and heartbeat artifacts were identified by manually viewing the time course and topographies of the ICA components. Additionally, components containing ocular artifacts were confirmed by detecting those most correlated with the EOG signals using Pearson correlation. These identified components were subsequently removed from the original raw data (which was not downsampled or filtered). Next, we segmented the MEG data into epochs which contained a time window of -200 ms to 500 ms relative to first fixation onsets on pre-target words. We then applied a 30Hz low-pass filter to eliminate high-frequency noise to the epoched data.</p></sec><sec id="S22"><title>Representational Similarity Time Course Analysis</title><p id="P41">To combine gradiometers and magnetometers in the RSA analysis, we first normalized the MEG signals using z-scores for each sensor. For each trial, at each time point (from -200 to 500 ms relative to the fixation onset on the pre-target word, e.g., “clever”), we extracted the MEG signals across all 306 sensors, creating a 1 × 306 vector. This vector captures the spatial pattern of neural activity at a specific time point <italic>t</italic> (see <xref ref-type="fig" rid="F2">Fig. 2</xref>). To also capture the temporal activation pattern, we applied a sliding time window approach<sup><xref ref-type="bibr" rid="R74">74</xref>,<xref ref-type="bibr" rid="R75">75</xref></sup> incorporating data from <italic>t</italic> - 32 ms to <italic>t</italic> + 32 ms. This resulted in a 306 × 65 matrix (sensors × time points) for each time point. The matrix for each time point was then flattened into a 1 × 19,890, vector representing the neural activity pattern surrounding time <italic>t. We</italic> calculated the representational similarity between pairs of trials by computing Pearson’s correlation between their corresponding vectors. This was done separately for three conditions: orthographic-within pairs, semantic-within pairs, and between-pairs. The number of pairs for each condition was similar: 85.6 ± 14, 84.5 ± 13.7, and 85.0 ± 13.9 (mean ± SD, across participants) for orthographic-within pairs, semantic-within pairs, and between-pairs. We then averaged the correlation values across all pairs within each condition at t to obtain <italic>R<sub>orth</sub>(t), R<sub>sema</sub>(t)</italic>, and <italic>R<sub>between</sub>(t)</italic>. This process was repeated for each time point within the -200–500 ms interval relative to the first fixation onset on the pre-target words, resulting in 3 time series of pairwise correlations: <italic>R<sub>orth</sub>, R<sub>sema</sub></italic>, and <italic>R<sub>between</sub></italic> for each participant. To visualize the data, we averaged these similarity values across all participants (n = 32) at each time point. This yielded a “grand-average” spatial similarity time series for each condition (See <xref ref-type="fig" rid="F3">Fig. 3a</xref> and &amp; 3b).</p></sec><sec id="S23"><title>Sensor-level Searchlight Representational Similarity Analysis</title><p id="P42">To investigate which sensors contributed to the observed parafoveal orthographic and semantic effects, we applied a searchlight approach<sup><xref ref-type="bibr" rid="R46">46</xref></sup> to conduct the representational similarity analysis (RSA) in the sensor space (<xref ref-type="fig" rid="F4">Fig. 4a &amp; 4b</xref>). This analysis was conducted separately for magnetometers and gradiometers. For each magnetometer, a searchlight patch was defined by including 20 neighbouring magnetometers, while for each gradiometer, the searchlight patch included 40 neighbouring gradiometers, as gradiometers are arranged in pairs. The analysis was conducted within specific time windows identified from the representational similarity time course analysis (see <xref ref-type="fig" rid="F3">Fig. 3</xref>): 68−186 ms for the parafoveal orthographic effect, and 137–247 ms for the parafoveal semantic effect, both aligned with the onset of the first fixation on the pretarget word. For each sensor, we extracted MEG data from each trial within the relevant time window, producing an M × N matrix where M represents the number of sensors in the searchlight patch (20 for magnetometers, 40 for gradiometers), and N represents the number of time points (118 for the orthographic effect and 111 for semantic effects, given the 1000 Hz sampling rate). This matrix was flattened into a 1 × (M × N) vector, representing the neural activity pattern within the searchlight patch during the respective time window. We then computed representational similarity for orthographic within-pairs by calculating Pearson correlations between corresponding pairwise vectors. This produced a set of R values, which were averaged to obtain the representational similarity for orthographic within-pairs (<italic>R<sub>orth</sub></italic>). A similar process was used for between-pairs to obtain the average R values for between-pairs (<italic>R<sub>between</sub></italic>). The difference between <italic>R<sub>orth</sub></italic> and <italic>R<sub>between</sub></italic> for each sensor was used to quantify its contribution to the parafoveal orthographic effect. This analysis was repeated for all sensors, generating a topographical map of each sensor’s contribution to the observed parafoveal orthographic effect (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). Similarly, we calculated the Pearson correlation for the semantic within-pairs (<italic>R<sub>sema</sub></italic>) and contrasted it with the <italic>R<sub>between</sub></italic> within the 137−247 ms time window for each sensor to generate the topographical map of each sensor’s contribution to the observed parafoveal semantic effect (<xref ref-type="fig" rid="F4">Fig. 4b</xref>).</p></sec><sec id="S24"><title>Source reconstruction</title><p id="P43">First, FreeSurfer<sup><xref ref-type="bibr" rid="R76">76</xref></sup> was used to automatically reconstruct the cortical surfaces from participants’ anatomical MRI images. Co-registration between the MRI and MEG coordinate frames was done using the three fiducial landmarks obtained from head shape digitization process. For 7 subjects who did not have individual anatomical images, a standard template (i.e., FreeSurfer average brain “fsaverage”) was warped to fit the participant’s head shape, estimated from the digitized points. A surface-based source space, consisting of 20,484 vertices evenly distributed across the cortical surface (10,242 per hemisphere), was generated for each participant. The head conductivity model was built using individual structural MRIs and was modelled as a single-layer boundary element model (BEM). The forward solution was then computed using the transformation matrix between the MEG “head” and MRI coordinate frames, the source space and the BEM model.</p><p id="P44">A noise-covariance matrix was estimated using the -1000–0 ms intervals relative to the fixation cross before sentence presentation. The forward solution, implemented as a lead-field matrix, along with the noise-covariance matrix enabled the estimation of the inverse solution for source localization. We computed the inverse solution minimum norm estimates using the dynamic statistical parametric mapping (dSPM) approach<sup><xref ref-type="bibr" rid="R77">77</xref></sup> for single epochs which contained a time window of -200 ms to 500 ms relative to first fixation onsets on pre-target words. The epochs were down-sampled to 100 Hz to reduce computation times. To perform group-level analysis, individual source estimates were morphed onto a common source space (i.e., “fsaverage”) before performing source-level RSA.</p><p id="P45">As for the searchlight RSA in the source space, we used the same approach as it was implemented in the sensor space, except that each searchlight patch comprised the 2000 closest vertices for a given vertex.</p></sec><sec id="S25"><title>Statistical analysis</title><p id="P46">For the representational similarity time course analysis, we employed a cluster-based permutation test<sup><xref ref-type="bibr" rid="R78">78</xref></sup> to assess whether and when significant differences in the within-pair (for both orthographic and semantic) and between-pair correlations became apparent. We first computed differences between within-pair and between-pair correlations focusing on specific time intervals of interest: 60-250 ms after fixating on the pre-target words, resulting in two contrast arrays: one for orthography (<italic>D<sub>orth</sub></italic>) and another for semantics (<italic>D<sub>sema</sub></italic>). To identify time points when these contrasts significantly deviated from the null hypothesis (i.e., no effect), we performed one-sample t-tests across participants at each time point. Clusters of adjacent time points with significant t-statistics (p &lt; 0.05, two-sided) were identified. Next, we performed permutation tests by randomly flipping the sign of differences (i.e., <italic>D<sub>orth</sub></italic> and <italic>D<sub>sema</sub></italic>), this process was repeated 5,000 times to construct a null distribution of the maximum cluster statistic (i.e., the sum of t-values within a cluster) under each permutation. The observed clusterlevel statistics were then compared against this null distribution. Clusters falling within the highest or lowest 2.5% of the null distribution indicated significant differences in the representational similarity between conditions (i.e., <italic>R<sub>orth</sub></italic> and <italic>R<sub>between</sub></italic> or <italic>R<sub>sema</sub></italic> and <italic>R<sub>between</sub></italic>).</p><p id="P47">This statistical analysis for the sensor-level searchlight RSA was performed in a similar manner to the RSA time course analysis, with the key difference being that it focused on the spatial patterns of the data rather than time points.</p></sec></sec></sec></body><back><ack id="S26"><title>Acknowledgements</title><p>This work was supported by the following grant to O.J.: Wellcome Trust Investigator Award in Science (grant number 207550) and by the NIHR Oxford Health Biomedical Research Centre (NIHR203316). Y.P. was supported by the Leverhulme Early Career Fellowship (ECF-294-2023-626). The views expressed are those of the author(s) and not necessarily those of the funders. The funders had no role in the preparation of the manuscript or the decision to publish.</p></ack><sec id="S27" sec-type="data-availability"><title>Data availability</title><p id="P48">The following data in the current study will be made available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/rnx89/">https://osf.io/rnx89/</ext-link>): the raw MEG data, the MEG epoch data after pre-processing, the raw EyeLink files, the Psychotoolbox data, and the head models after the coregistration of T1 images with the MEG data.</p><sec id="S28" sec-type="data-availability"><title>Code availability</title><p id="P49">The experiment presentation scripts (Psychtoolbox) and analysis codes for the paper will be made available on OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/rnx89/">https://osf.io/rnx89/</ext-link>).</p></sec></sec><fn-group><fn id="FN2" fn-type="con"><p id="P50"><bold>Author contributions</bold></p><p id="P51">L.W., S.F., Y.P., and O.J. devised and designed the study, L.W. made the sentences with assistance from S.F., L.W. collected and analysed the data with assistance from Y.P., O.J. and S.F., L.W., Y.P., O.J., and S.F. wrote the paper together.</p></fn><fn id="FN3" fn-type="conflict"><p id="P52"><bold>Competing interests</bold></p><p id="P53">The authors declare no competing financial interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Angele</surname><given-names>B</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Parafoveal processing in reading</article-title><source>Atten Percept Psychophys</source><year>2012</year><volume>74</volume><fpage>5</fpage><lpage>35</lpage><pub-id pub-id-type="pmid">22042596</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McConkie</surname><given-names>GW</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>The span of the effective stimulus during a fixation in reading</article-title><source>Perception &amp; Psychophysics</source><year>1975</year><volume>17</volume><fpage>578</fpage><lpage>586</lpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Bertera</surname><given-names>JH</given-names></name></person-group><article-title>Reading Without a Fovea</article-title><source>Science</source><year>1979</year><volume>206</volume><fpage>468</fpage><lpage>469</lpage><pub-id pub-id-type="pmid">504987</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Well</surname><given-names>AD</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Bertera</surname><given-names>JH</given-names></name></person-group><article-title>The availability of useful information to the right of fixation in reading</article-title><source>Perception &amp; Psychophysics</source><year>1982</year><volume>31</volume><fpage>537</fpage><lpage>550</lpage><pub-id pub-id-type="pmid">7122189</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanchard</surname><given-names>HE</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>The acquisition of parafoveal word information in reading</article-title><source>Perception &amp; Psychophysics</source><year>1989</year><volume>46</volume><fpage>85</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">2755766</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>ED</given-names></name><name><surname>Reingold</surname><given-names>EM</given-names></name></person-group><article-title>Neurophysiological constraints on the eye-mind link</article-title><source>Front Hum Neurosci</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3710954</pub-id><pub-id pub-id-type="pmid">23874281</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00361</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>White</surname><given-names>SJ</given-names></name><name><surname>Kambe</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>B</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><chapter-title>On the Processing of Meaning from Parafoveal Vision During Eye Fixations in Reading</chapter-title><source>The Mind’s Eye</source><publisher-name>Elsevier</publisher-name><year>2003</year><fpage>213</fpage><lpage>234</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichle</surname><given-names>E</given-names></name><name><surname>Drieghe</surname><given-names>D</given-names></name></person-group><article-title>Using E-Z Reader to Examine Word Skipping During Reading</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2012</year><volume>39</volume><pub-id pub-id-type="pmcid">PMC3832990</pub-id><pub-id pub-id-type="pmid">23206168</pub-id><pub-id pub-id-type="doi">10.1037/a0030910</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzsimmons</surname><given-names>G</given-names></name><name><surname>Drieghe</surname><given-names>D</given-names></name></person-group><article-title>How fast can predictability influence word skipping during reading?</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2013</year><volume>39</volume><fpage>1054</fpage><lpage>1063</lpage><pub-id pub-id-type="pmid">23244054</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>PC</given-names></name><name><surname>Plummer</surname><given-names>P</given-names></name><name><surname>Choi</surname><given-names>W</given-names></name></person-group><article-title>See before you jump: Full recognition of parafoveal words precedes skips during reading</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2013</year><volume>39</volume><fpage>633</fpage><lpage>641</lpage><pub-id pub-id-type="pmcid">PMC3633587</pub-id><pub-id pub-id-type="pmid">22686842</pub-id><pub-id pub-id-type="doi">10.1037/a0028881</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degno</surname><given-names>F</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><article-title>Eye Movements and Fixation-Related Potentials in Reading: A Review</article-title><source>Vision</source><year>2020</year><volume>4</volume><fpage>11</fpage><pub-id pub-id-type="pmcid">PMC7157570</pub-id><pub-id pub-id-type="pmid">32028566</pub-id><pub-id pub-id-type="doi">10.3390/vision4010011</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>The perceptual span and peripheral cues in reading</article-title><source>Cognitive Psychology</source><year>1975</year><volume>7</volume><fpage>65</fpage><lpage>81</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balota</surname><given-names>DA</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>The interaction of contextual constraints and parafoveal visual information in reading</article-title><source>Cogn Psychol</source><year>1985</year><volume>17</volume><fpage>364</fpage><lpage>390</lpage><pub-id pub-id-type="pmid">4053565</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>CC</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Previewing the neighborhood: The role of orthographic neighbors as parafoveal previews in reading</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2006</year><volume>32</volume><fpage>1072</fpage><lpage>1082</lpage><pub-id pub-id-type="pmid">16846298</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>RL</given-names></name><name><surname>Perea</surname><given-names>M</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Transposed-letter effects in reading: Evidence from eye movements and parafoveal preview</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2007</year><volume>33</volume><fpage>209</fpage><lpage>229</lpage><pub-id pub-id-type="pmid">17311489</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dare</surname><given-names>N</given-names></name><name><surname>Shillcock</surname><given-names>R</given-names></name></person-group><article-title>Serial and parallel processing in reading: Investigating the effects of parafoveal orthographic information on nonisolated word recognition</article-title><source>Quarterly Journal of Experimental Psychology</source><year>2013</year><volume>66</volume><fpage>487</fpage><lpage>504</lpage><pub-id pub-id-type="pmid">22950804</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><article-title>Parafoveal preview benefit in sentence reading: Independent effects of plausibility and orthographic relatedness</article-title><source>Psychon Bull Rev</source><year>2017</year><volume>24</volume><fpage>519</fpage><lpage>528</lpage><pub-id pub-id-type="pmid">27418260</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milledge</surname><given-names>SV</given-names></name><name><surname>Blythe</surname><given-names>HI</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name></person-group><article-title>Parafoveal pre-processing in children reading English: The importance of external letters</article-title><source>Psychon Bull Rev</source><year>2021</year><volume>28</volume><fpage>197</fpage><lpage>208</lpage><pub-id pub-id-type="pmcid">PMC7870634</pub-id><pub-id pub-id-type="pmid">32918232</pub-id><pub-id pub-id-type="doi">10.3758/s13423-020-01806-8</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollatsek</surname><given-names>A</given-names></name><name><surname>Lesch</surname><given-names>M</given-names></name><name><surname>Morris</surname><given-names>RK</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Phonological codes are used in integrating information across saccades in word identification and reading</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1992</year><volume>18</volume><fpage>148</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">1532185</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Dixon</surname><given-names>P</given-names></name><name><surname>Petersen</surname><given-names>A</given-names></name><name><surname>Twilley</surname><given-names>LC</given-names></name><name><surname>Ferreira</surname><given-names>F</given-names></name></person-group><article-title>Evidence for the use of phonological representations during transsaccadic word recognition</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1995</year><volume>21</volume><fpage>82</fpage><lpage>97</lpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miellet</surname><given-names>S</given-names></name><name><surname>Sparrow</surname><given-names>L</given-names></name></person-group><article-title>Phonological codes are assembled before word fixation: Evidence from boundary paradigm in sentence reading</article-title><source>Brain and Language</source><year>2004</year><volume>90</volume><fpage>299</fpage><lpage>310</lpage><pub-id pub-id-type="pmid">15172547</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chace</surname><given-names>KH</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Well</surname><given-names>AD</given-names></name></person-group><article-title>Eye Movements and Phonological Parafoveal Preview: Effects of Reading Skill</article-title><source>Canadian Journal of Experimental Psychology / Revue canadienne de psychologie expérimentale</source><year>2005</year><volume>59</volume><fpage>209</fpage><lpage>217</lpage><pub-id pub-id-type="pmid">16248500</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milledge</surname><given-names>SV</given-names></name><name><surname>Zang</surname><given-names>C</given-names></name><name><surname>Liversedge</surname><given-names>SP</given-names></name><name><surname>Blythe</surname><given-names>HI</given-names></name></person-group><article-title>Phonological parafoveal pre-processing in children reading English sentences</article-title><source>Cognition</source><year>2022</year><volume>225</volume><elocation-id>105141</elocation-id><pub-id pub-id-type="pmid">35489158</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Synonyms Provide Semantic Preview Benefit in English</article-title><source>J Mem Lang</source><year>2013</year><volume>69</volume><pub-id pub-id-type="pmcid">PMC3859233</pub-id><pub-id pub-id-type="pmid">24347813</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2013.09.002</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rayner</surname><given-names>K</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Semantic preview benefit in reading English: The effect of initial letter capitalization</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2014</year><volume>40</volume><fpage>1617</fpage><lpage>1628</lpage><pub-id pub-id-type="pmcid">PMC4147079</pub-id><pub-id pub-id-type="pmid">24820439</pub-id><pub-id pub-id-type="doi">10.1037/a0036763</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Reiderman</surname><given-names>M</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>The effect of contextual constraint on parafoveal processing in reading</article-title><source>Journal of Memory and Language</source><year>2015</year><volume>83</volume><fpage>118</fpage><lpage>139</lpage><pub-id pub-id-type="pmcid">PMC4525713</pub-id><pub-id pub-id-type="pmid">26257469</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2015.04.005</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Jia</surname><given-names>A</given-names></name></person-group><article-title>Semantic and plausibility preview benefit effects in English: Evidence from eye movements</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2016</year><volume>42</volume><fpage>1839</fpage><lpage>1866</lpage><pub-id pub-id-type="pmcid">PMC5085893</pub-id><pub-id pub-id-type="pmid">27123754</pub-id><pub-id pub-id-type="doi">10.1037/xlm0000281</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veldre</surname><given-names>A</given-names></name><name><surname>Andrews</surname><given-names>S</given-names></name></person-group><article-title>Semantic preview benefit in English: Individual differences in the extraction and use of parafoveal semantic information</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2016</year><volume>42</volume><fpage>837</fpage><lpage>854</lpage><pub-id pub-id-type="pmid">26595070</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baccino</surname><given-names>T</given-names></name><name><surname>Manunta</surname><given-names>Y</given-names></name></person-group><article-title>Eye-Fixation-Related Potentials: Insight into Parafoveal Processing</article-title><source>Journal of Psychophysiology</source><year>2005</year><volume>19</volume><fpage>204</fpage><lpage>215</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degno</surname><given-names>F</given-names></name><etal/></person-group><article-title>Parafoveal previews and lexical frequency in natural reading: Evidence from eye movements and fixation-related potentials</article-title><source>Journal of Experimental Psychology: General</source><year>2019</year><volume>148</volume><fpage>453</fpage><lpage>474</lpage><pub-id pub-id-type="pmcid">PMC6388670</pub-id><pub-id pub-id-type="pmid">30335444</pub-id><pub-id pub-id-type="doi">10.1037/xge0000494</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirault</surname><given-names>J</given-names></name><etal/></person-group><article-title>Parafoveal-on-foveal repetition effects in sentence reading: A coregistered eye-tracking and electroencephalogram study</article-title><source>Psychophysiology</source><year>2020</year><volume>57</volume><elocation-id>e13553</elocation-id><pub-id pub-id-type="pmcid">PMC7507185</pub-id><pub-id pub-id-type="pmid">32091627</pub-id><pub-id pub-id-type="doi">10.1111/psyp.13553</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Meade</surname><given-names>G</given-names></name><name><surname>Meeter</surname><given-names>M</given-names></name><name><surname>Holcomb</surname><given-names>P</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><article-title>An electrophysiological investigation of orthographic spatial integration in reading</article-title><source>Neuropsychologia</source><year>2019</year><volume>129</volume><fpage>276</fpage><lpage>283</lpage><pub-id pub-id-type="pmcid">PMC6582630</pub-id><pub-id pub-id-type="pmid">31002854</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2019.04.009</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Doñamayor</surname><given-names>N</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Münte</surname><given-names>T</given-names></name></person-group><article-title>Parafoveal N400 effect during sentence reading</article-title><source>Neuroscience Letters</source><year>2010</year><volume>479</volume><fpage>152</fpage><lpage>156</lpage><pub-id pub-id-type="pmcid">PMC4096702</pub-id><pub-id pub-id-type="pmid">20580772</pub-id><pub-id pub-id-type="doi">10.1016/j.neulet.2010.05.053</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>van der Meij</surname><given-names>M</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><article-title>An electrophysiological analysis of contextual and temporal constraints on parafoveal word processing</article-title><source>Psychophysiology</source><year>2013</year><volume>50</volume><fpage>48</fpage><lpage>59</lpage><pub-id pub-id-type="pmcid">PMC4096715</pub-id><pub-id pub-id-type="pmid">23153323</pub-id><pub-id pub-id-type="doi">10.1111/j.1469-8986.2012.01489.x</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Event-related brain potentials reveal age-related changes in parafoveal-foveal integration during sentence processing</article-title><source>Neuropsychologia</source><year>2017</year><volume>106</volume><fpage>358</fpage><lpage>370</lpage><pub-id pub-id-type="pmcid">PMC5720378</pub-id><pub-id pub-id-type="pmid">28987909</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.10.002</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Getting ahead of yourself: Parafoveal word expectancy modulates the N400 during sentence reading</article-title><source>Cogn Affect Behav Neurosci</source><year>2017</year><volume>17</volume><fpage>475</fpage><lpage>490</lpage><pub-id pub-id-type="pmcid">PMC5603229</pub-id><pub-id pub-id-type="pmid">28101830</pub-id><pub-id pub-id-type="doi">10.3758/s13415-016-0492-6</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payne</surname><given-names>BR</given-names></name><name><surname>Stites</surname><given-names>MC</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Event-related brain potentials reveal how multiple aspects of semantic processing unfold across parafoveal and foveal vision during sentence reading</article-title><source>Psychophysiology</source><year>2019</year><volume>56</volume><elocation-id>e13432</elocation-id><pub-id pub-id-type="pmcid">PMC6879358</pub-id><pub-id pub-id-type="pmid">31274200</pub-id><pub-id pub-id-type="doi">10.1111/psyp.13432</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Primativo</surname><given-names>S</given-names></name><name><surname>Rusich</surname><given-names>D</given-names></name><name><surname>Martelli</surname><given-names>M</given-names></name><name><surname>Arduino</surname><given-names>LS</given-names></name></person-group><article-title>The Timing of Semantic Processing in the Parafovea: Evidence from a Rapid Parallel Visual Presentation Study</article-title><source>Brain Sciences</source><year>2022</year><volume>12</volume><fpage>1535</fpage><pub-id pub-id-type="pmcid">PMC9688821</pub-id><pub-id pub-id-type="pmid">36421859</pub-id><pub-id pub-id-type="doi">10.3390/brainsci12111535</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schotter</surname><given-names>ER</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Estevez</surname><given-names>VM</given-names></name></person-group><article-title>Event-related potentials show that parafoveal vision is insufficient for semantic integration</article-title><source>Psychophysiology</source><year>2023</year><volume>60</volume><elocation-id>e14246</elocation-id><pub-id pub-id-type="pmid">36811523</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antúnez</surname><given-names>M</given-names></name><name><surname>Milligan</surname><given-names>S</given-names></name><name><surname>Hernández-Cabrera</surname><given-names>JA</given-names></name><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Schotter</surname><given-names>ER</given-names></name></person-group><article-title>Semantic parafoveal processing in natural reading: Insight from fixation-related potentials &amp; eye movements</article-title><source>Psychophysiology</source><year>2022</year><volume>59</volume><elocation-id>e13986</elocation-id><pub-id pub-id-type="pmid">34942021</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kretzschmar</surname><given-names>F</given-names></name><name><surname>Bornkessel-Schlesewsky</surname><given-names>I</given-names></name><name><surname>Schlesewsky</surname><given-names>M</given-names></name></person-group><article-title>Parafoveal versus foveal N400s dissociate spreading activation from contextual fit</article-title><source>Neuroreport</source><year>2009</year><volume>20</volume><fpage>1613</fpage><lpage>1618</lpage><pub-id pub-id-type="pmid">19884865</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Early parafoveal semantic integration in natural reading</article-title><source>eLife</source><year>2023</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC11226228</pub-id><pub-id pub-id-type="pmid">38968325</pub-id><pub-id pub-id-type="doi">10.7554/eLife.91327</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Neural evidence for lexical parafoveal processing</article-title><source>Nat Commun</source><year>2021</year><volume>12</volume><elocation-id>5234</elocation-id><pub-id pub-id-type="pmcid">PMC8413448</pub-id><pub-id pub-id-type="pmid">34475391</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25571-x</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>The Brain Basis of Language Processing: From Structure to Function</article-title><source>Physiological Reviews</source><year>2011</year><volume>91</volume><fpage>1357</fpage><lpage>1392</lpage><pub-id pub-id-type="pmid">22013214</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turker</surname><given-names>S</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name></person-group><article-title>Exploring the neurobiology of reading through non-invasive brain stimulation: A review</article-title><source>Cortex</source><year>2021</year><volume>141</volume><fpage>497</fpage><lpage>521</lpage><pub-id pub-id-type="pmid">34166905</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational similarity analysis − connecting the branches of systems neuroscience</article-title><source>Front Sys Neurosci</source><year>2008</year><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Multivariate pattern analysis of MEG and EEG: a comparison of representational structure in time and space</article-title><pub-id pub-id-type="pmid">28716718</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubbard</surname><given-names>RJ</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Representational pattern similarity of electrical brain activity reveals rapid and specific prediction during language comprehension</article-title><pub-id pub-id-type="pmcid">PMC8328210</pub-id><pub-id pub-id-type="pmid">33895819</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab087</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Kuperberg</surname><given-names>G</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>Specific lexico-semantic predictions are associated with unique spatial and temporal patterns of neural activity</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e39061</elocation-id><pub-id pub-id-type="pmcid">PMC6322859</pub-id><pub-id pub-id-type="pmid">30575521</pub-id><pub-id pub-id-type="doi">10.7554/eLife.39061</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Neural Evidence for the Prediction of Animacy Features during Language Comprehension: Evidence from MEG and EEG Representational Similarity Analysis</article-title><source>J Neurosci</source><year>2020</year><volume>40</volume><fpage>3278</fpage><lpage>3291</lpage><pub-id pub-id-type="pmcid">PMC7159896</pub-id><pub-id pub-id-type="pmid">32161141</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1733-19.2020</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Brothers</surname><given-names>T</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Kuperberg</surname><given-names>GR</given-names></name></person-group><article-title>Dissociating the preactivation of word meaning and form during sentence comprehension: Evidence from EEG representational similarity analysis</article-title><source>Psychon Bull Rev</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10985416</pub-id><pub-id pub-id-type="pmid">37783897</pub-id><pub-id pub-id-type="doi">10.3758/s13423-023-02385-0</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Landauer</surname><given-names>TK</given-names></name><name><surname>McNamara</surname><given-names>DS</given-names></name><name><surname>Dennis</surname><given-names>S</given-names></name><name><surname>Kintsch</surname><given-names>W</given-names></name></person-group><source>Handbook of Latent Semantic Analysis</source><publisher-name>Psychology Press</publisher-name><year>2013</year></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><article-title>The unique role of the visual word form area in reading</article-title><source>Trends in Cognitive Sciences</source><year>2011</year><volume>15</volume><fpage>254</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">21592844</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snell</surname><given-names>J</given-names></name><name><surname>Grainger</surname><given-names>J</given-names></name></person-group><article-title>Readers Are Parallel Processors</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><fpage>537</fpage><lpage>546</lpage><pub-id pub-id-type="pmid">31138515</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazaryan</surname><given-names>G</given-names></name><etal/></person-group><article-title>Trials and tribulations when attempting to decode semantic representations from MEG responses to written text</article-title><source>Language, Cognition and Neuroscience</source><year>2023</year><fpage>1</fpage><lpage>12</lpage></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How Does the Brain Solve Visual Object Recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Frisson</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>An oscillatory pipelining mechanism supporting previewing during visual exploration and reading</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><volume>25</volume><fpage>1033</fpage><lpage>1044</lpage><pub-id pub-id-type="pmcid">PMC7615059</pub-id><pub-id pub-id-type="pmid">34544653</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2021.08.008</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><article-title>Activation of Extrastriate and Frontal Cortical Areas by Visual Words and Word-Like Stimuli</article-title><source>Science</source><year>1990</year><volume>249</volume><fpage>1041</fpage><lpage>1044</lpage><pub-id pub-id-type="pmid">2396097</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demb</surname><given-names>JB</given-names></name><etal/></person-group><article-title>Semantic encoding and retrieval in the left inferior prefrontal cortex: a functional MRI study of task difficulty and process specificity</article-title><source>J Neurosci</source><year>1995</year><volume>15</volume><fpage>5870</fpage><lpage>5878</lpage><pub-id pub-id-type="pmcid">PMC6577672</pub-id><pub-id pub-id-type="pmid">7666172</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-09-05870.1995</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dapretto</surname><given-names>M</given-names></name><name><surname>Bookheimer</surname><given-names>SY</given-names></name></person-group><article-title>Form and content: dissociating syntax and semantics in sentence comprehension</article-title><source>Neuron</source><year>1999</year><volume>24</volume><fpage>427</fpage><lpage>432</lpage><pub-id pub-id-type="pmid">10571235</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>JT</given-names></name><name><surname>Matthews</surname><given-names>PM</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Semantic Processing in the Left Inferior Prefrontal Cortex: A Combined Functional Magnetic Resonance Imaging and Transcranial Magnetic Stimulation Study</article-title><source>Journal of Cognitive Neuroscience</source><year>2003</year><volume>15</volume><fpage>71</fpage><lpage>84</lpage><pub-id pub-id-type="pmid">12590844</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><article-title>On Broca, brain, and binding: a new framework</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><fpage>416</fpage><lpage>423</lpage><pub-id pub-id-type="pmid">16054419</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Hald</surname><given-names>L</given-names></name><name><surname>Bastiaansen</surname><given-names>M</given-names></name><name><surname>Petersson</surname><given-names>KM</given-names></name></person-group><article-title>Integration of Word Meaning and World Knowledge in Language Comprehension</article-title><source>Science</source><year>2004</year><volume>304</volume><fpage>438</fpage><lpage>441</lpage><pub-id pub-id-type="pmid">15031438</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabrieli</surname><given-names>JDE</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Desmond</surname><given-names>JE</given-names></name></person-group><article-title>The role of left prefrontal cortex in language and memory</article-title><source>Proc Natl Acad Sci USA</source><year>1998</year><volume>95</volume><fpage>906</fpage><lpage>913</lpage><pub-id pub-id-type="pmcid">PMC33815</pub-id><pub-id pub-id-type="pmid">9448258</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.3.906</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barde</surname><given-names>LHF</given-names></name><name><surname>Schwartz</surname><given-names>MF</given-names></name><name><surname>Thompson-Schill</surname><given-names>S</given-names></name></person-group><article-title>The role of left inferior frontal gyrus (LIFG) in semantic short-term memory: A comparison of two case studies</article-title><source>Brain and Language</source><year>2006</year><volume>99</volume><fpage>82</fpage><lpage>83</lpage></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carota</surname><given-names>F</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><article-title>Representational Similarity Mapping of Distributional Semantics in Left Inferior Frontal, Middle Temporal, and Motor Cortex</article-title><source>Cereb Cortex cercor</source><year>2017</year><elocation-id>bhw379v1</elocation-id><pub-id pub-id-type="pmcid">PMC6044349</pub-id><pub-id pub-id-type="pmid">28077514</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhw379</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vives</surname><given-names>M-L</given-names></name><name><surname>De Bruin</surname><given-names>D</given-names></name><name><surname>Van Baar</surname><given-names>JM</given-names></name><name><surname>FeldmanHall</surname><given-names>O</given-names></name><name><surname>Bhandari</surname><given-names>A</given-names></name></person-group><article-title>Uncertainty aversion predicts the neural expansion of semantic representations</article-title><source>Nat Hum Behav</source><year>2023</year><volume>7</volume><fpage>765</fpage><lpage>775</lpage><pub-id pub-id-type="pmid">36997668</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hultén</surname><given-names>A</given-names></name><etal/></person-group><article-title>The neural representation of abstract words may arise through grounding word meaning in language itself</article-title><source>Human Brain Mapping</source><year>2021</year><volume>42</volume><fpage>4973</fpage><lpage>4984</lpage><pub-id pub-id-type="pmcid">PMC8449102</pub-id><pub-id pub-id-type="pmid">34264550</pub-id><pub-id pub-id-type="doi">10.1002/hbm.25593</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><etal/></person-group><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Front Neurosci</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3872725</pub-id><pub-id pub-id-type="pmid">24431986</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrante</surname><given-names>O</given-names></name><etal/></person-group><article-title>FLUX: A pipeline for MEG analysis</article-title><source>NeuroImage</source><year>2022</year><volume>253</volume><elocation-id>119047</elocation-id><pub-id pub-id-type="pmcid">PMC9127391</pub-id><pub-id pub-id-type="pmid">35276363</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119047</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name></person-group><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title><source>Phys Med Biol</source><year>2006</year><volume>51</volume><fpage>1759</fpage><pub-id pub-id-type="pmid">16552102</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Oja</surname><given-names>E</given-names></name></person-group><article-title>Independent component analysis: algorithms and applications</article-title><source>Neural Networks</source><year>2000</year><volume>13</volume><fpage>411</fpage><lpage>430</lpage><pub-id pub-id-type="pmid">10946390</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Cheung</surname><given-names>TPL</given-names></name><name><surname>Devereux</surname><given-names>BJ</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name></person-group><article-title>Syntactic Computations in the Language Network: Characterizing Dynamic Network Properties Using Representational Similarity Analysis</article-title><source>Front Psychol</source><year>2013</year><volume>4</volume><pub-id pub-id-type="pmcid">PMC3656357</pub-id><pub-id pub-id-type="pmid">23730293</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00271</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>F</given-names></name></person-group><article-title>Using Single-Trial Representational Similarity Analysis with EEG to track semantic similarity in emotional word processing</article-title><pub-id pub-id-type="doi">10.48550/arXiv.2110.03529</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>FreeSurfer</article-title><source>NeuroImage</source><year>2012</year><volume>62</volume><fpage>774</fpage><lpage>781</lpage><pub-id pub-id-type="pmcid">PMC3685476</pub-id><pub-id pub-id-type="pmid">22248573</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.01.021</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><etal/></person-group><article-title>Dynamic Statistical Parametric Mapping</article-title><source>Neuron</source><year>2000</year><volume>26</volume><fpage>55</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">10798392</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Experimental design and Example of a sentence sextet.</title><p><bold>a</bold>, Experimental design. Participants were instructed to read sentences silently while eye movements and brain activity were recorded simultaneously using an eye tracker and MEG. Each trial started with a 1.2–1.6 s fixation cross in the centre of a grey screen. Then a square was presented on the left side of the screen. The onset of an upcoming sentence was triggered by gazing at the square for at least 0.2 s. After reading the sentence, participants were asked to fixate on a square below the screen for 0.1 s to terminate the trial. 25% of the sentences were followed by a simple yes-or-no comprehension question to ensure careful reading. <bold>b</bold>, Example of a sentence sextet. 1. Sentences were constructed in triplets, wherein each sentence included a target word (shown in bold for illustration purposes, not in the actual experiment), which could either be the critical word (e.g., “writer”), its orthographic neighbour (e.g., “waiter”), or its semantic neighbour (e.g., “author”). Each triplet was paired with another triplet, embedding a similar structure of target words (e.g., “police/policy/guards”). Pre-target words were the same (e.g., “clever”) within each triplet and between matched triplets.</p></caption><graphic xlink:href="EMS199090-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Schematic illustration of our Representational Similarity Analysis.</title><p>1. For each trial, at each time point t (from -200 to 500 ms relative to the fixation onset on the pre-target word, e.g., “clever”), we extracted the MEG signals across sensors to create a vector, representing the brain activity pattern at time <italic>t</italic>. 2. We quantified representational similarity between each pair by correlating the corresponding vectors. 3. Then we averaged the R-values across all pairs within each condition to obtain the average R values within each condition at <italic>t</italic>: <italic>R<sub>orth</sub>(t), R<sub>sema</sub>(t)</italic>, and <italic>R<sub>between</sub>(t)</italic>. 4. We repeated the procedure at every millisecond after the fixation onset on the pre-target; this yielded 3 time series of pairwise correlations: <italic>R<sub>orth</sub>, R<sub>sema</sub></italic>, and <italic>R<sub>between</sub></italic>.</p></caption><graphic xlink:href="EMS199090-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Hierarchical parafoveal processing and its relationship with reading speed.</title><p><bold>a</bold>, Neuronal evidence for fast orthographic parafoveal processing. The time series of representational similarity (Pearson R-values) for orthographic within-pairs (blue line) and between-pairs (grey line). The orthographic within-pairs showed significantly higher representational similarity values than the between pairs during the 68–186 ms interval (indicated by the light blue shading) after fixation onset on pre-target words (<italic>p</italic> &lt;.001; cluster permutation test). <bold>b</bold>, Neuronal evidence for fast semantic parafoveal processing. The time series of representational similarity (Pearson R-values) for semantic within-pairs (red line) and between-pairs (grey line). In the 137−247 ms interval after the fixation onset on pre-target words (indicated by the light red shading), the semantic within-pair target words showed significantly higher representational similarity values than the between pairs (<italic>p</italic> &lt;.001; cluster permutation test). <bold>c</bold>, Relationship between the orthographic previewing effects with individual reading speed. Orthographic previewing effects were quantified by the mean difference in representational similarity values (Δr) between orthographic within-pairs and between-pairs. This was done within the time interval revealing significant differences in the RSA analysis (68–186 ms after fixation onset on the pre-target words). The reading speed of each participant was quantified as the number of words read per second. Each dot represents one participant. The Spearman correlation revealed a positive correlation between the neuronal orthographic previewing effect and reading speed (R = 0.42, <italic>p</italic> =.011, N = 35, two-sided). <bold>d</bold>, Relationship between the semantic previewing effects with individual reading speed. A Spearman correlation demonstrated that the neuronal semantic previewing effect positively correlated with reading speed (R = 0.34, <italic>p</italic> =.044, N = 35, two-sided).</p></caption><graphic xlink:href="EMS199090-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Neuronal sources of orthographic and semantic parafoveal processing.</title><p><bold>a</bold>, Mean sensor-level topographies across participants for the difference between representational similarity of orthographic-within pairs and between pairs, in the 68−186 ms interval after fixation onset on the pre-target words, for magnetometers (left) and gradiometers (right), respectively. Sensors that showed significant differences in the correlation values were marked by white dots. <bold>b</bold>, Source-level RSA results for the left and right hemispheres. The colour corresponds to the difference in representational similarity between orthographic within-pairs and between-pairs. The threshold was set at the 96th percentile of the data distribution, with sources exhibiting differences larger (orange) or smaller (blue) than this threshold being colour-coded based on their percentile. The location of the left cluster is consistent with the visual word-form area. <bold>c</bold>, Topographic maps of the parafoveal semantic effect derived from a searchlight RSA. The figures on the left and right show the contribution in magnetometers and gradiometers to the parafoveal semantic effect. <bold>d</bold>, Source-level RSA results for the left and right hemisphere. Clusters showing the maximal difference are in the Left Inferior Frontal Gyrus and around the right Posterior Parietal Cortex.</p></caption><graphic xlink:href="EMS199090-f004"/></fig></floats-group></article>