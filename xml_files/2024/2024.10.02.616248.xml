<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS199216</article-id><article-id pub-id-type="doi">10.1101/2024.10.02.616248</article-id><article-id pub-id-type="archive">PPR919632</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>PertEval-scFM: Benchmarking Single-Cell Foundation Models for Perturbation Effect Prediction</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Wenteler</surname><given-names>Aaron</given-names></name><xref ref-type="fn" rid="FN1">†</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Occhetta</surname><given-names>Martina</given-names></name><xref ref-type="fn" rid="FN1">†</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Branson</surname><given-names>Nikhil</given-names></name><xref ref-type="fn" rid="FN1">†</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Curean</surname><given-names>Victor</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Huebner</surname><given-names>Magdalena</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Dee</surname><given-names>William</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Connell</surname><given-names>William</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Chung</surname><given-names>Siu Pui</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hawkins-Hooker</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Ektefaie</surname><given-names>Yasha</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Valdez Córdova</surname><given-names>César Miguel</given-names></name><xref ref-type="fn" rid="FN2">‡</xref><xref ref-type="aff" rid="A7">7</xref><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Gallagher-Syed</surname><given-names>Amaya</given-names></name><xref ref-type="fn" rid="FN2">‡</xref><xref ref-type="fn" rid="FN1">†</xref><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/026zzn846</institution-id><institution>Queen Mary University of London</institution></institution-wrap></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap></aff><aff id="A3"><label>3</label>University of Medicine and Pharmacy of Cluj-Napoca</aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/043mz5j54</institution-id><institution>University of California, San Francisco</institution></institution-wrap></aff><aff id="A5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap></aff><aff id="A6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Harvard University</institution></institution-wrap></aff><aff id="A7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05c22rx21</institution-id><institution>Mila</institution></institution-wrap></aff><aff id="A8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap></aff><author-notes><corresp id="CR1">Correspondence to: Aaron Wenteler &lt;<email>a.wenteler@qmul.ac.uk</email>&gt;, Martina Occhetta &lt;<email>m.occhetta@qmul.ac.uk</email>&gt;.</corresp><fn id="FN1"><label>†</label><p id="P1">Core contributor</p></fn><fn id="FN2"><label>‡</label><p id="P2">Senior author</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>10</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>10</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P3"><italic>In silico</italic> modeling of transcriptional responses to perturbations is crucial for advancing our understanding of cellular processes and disease mechanisms. We present PertEval-scFM, a standardized framework designed to evaluate models for perturbation effect prediction. We apply PertEvalscFM to benchmark zero-shot single-cell foundation model (scFM) embeddings against baseline models to assess whether these contextualized representations enhance perturbation effect prediction. Our results show that scFM embeddings do not provide consistent improvements over baseline models, especially under distribution shift. Overall, this study provides a systematic evaluation of zero-shot scFM embeddings for perturbation effect prediction, highlighting the challenges of this task and revealing the limitations of current-generation scFMs. Our findings underscore the need for specialized models and high-quality datasets that capture a broader range of cellular states. Source code and documentation can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/aaronwtr/PertEval">https://github.com/aaronwtr/PertEval</ext-link>.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P4">Inspired by the success of foundation models in fields such as natural language processing (<xref ref-type="bibr" rid="R10">Devlin et al., 2019</xref>; <xref ref-type="bibr" rid="R6">Brown et al., 2020</xref>; <xref ref-type="bibr" rid="R32">OpenAI, 2024</xref>) and computer vision (<xref ref-type="bibr" rid="R12">Dosovitskiy et al., 2021</xref>), there has been an increase in the development of biological foundation models. Among these, single-cell foundation models (scFMs) leverage vast amounts of unlabeled transcriptomic single-cell RNA sequencing (scRNA-seq) data to learn contextualized representations through self-supervised pre-training (<xref ref-type="bibr" rid="R16">Ericsson et al., 2022</xref>). Finetuning the resulting model on labeled data enhances the performance on downstream applications, such as cell-type classification, gene regulatory network inference, and the prediction of cellular responses to perturbations (<xref ref-type="bibr" rid="R52">Yang et al., 2022</xref>; <xref ref-type="bibr" rid="R25">Kedzierska et al., 2023</xref>; <xref ref-type="bibr" rid="R44">Theodoris et al., 2023</xref>; <xref ref-type="bibr" rid="R40">Rosen et al., 2023</xref>; <xref ref-type="bibr" rid="R8">Cui et al., 2024</xref>; <xref ref-type="bibr" rid="R48">Wen et al., 2023</xref>; <xref ref-type="bibr" rid="R21">Hao et al., 2023</xref>).</p><p id="P5">A perturbation refers to any intervention or event leading to phenotypic alteration of a cell. Perturbation response prediction can provide invaluable insights into cellular mechanisms and disease progression, facilitating the mapping of genotype to phenotype and the identification of potential drug targets (<xref ref-type="bibr" rid="R28">Lotfollahi et al., 2019</xref>). Numerous models, here referred to as <italic>narrow perturbation prediction models</italic> (NPPMs), have been developed specifically for this task (<xref ref-type="bibr" rid="R20">Gavriilidis et al., 2024</xref>). However, perturbation response prediction is a challenging task, as demonstrated by the difficulty of models to improve consistently over simpler baseline methods (<xref ref-type="bibr" rid="R51">Wu et al., 2024</xref>; <xref ref-type="bibr" rid="R5">Branson et al., 2024</xref>; <xref ref-type="bibr" rid="R1">Ahlmann-Eltze et al., 2024</xref>).</p><p id="P6">Recently, there has been a concerted effort to evaluate biological foundation models. The Therapeutic Data Commons, an open science initiative, has curated some datasets, models and benchmarks for single-cell analysis (<xref ref-type="bibr" rid="R47">Velez-Arce et al., 2024</xref>). Additionally, <xref ref-type="bibr" rid="R51">Wu et al. (2024)</xref> and <xref ref-type="bibr" rid="R1">Ahlmann-Eltze et al. (2024)</xref> show that simple baseline models perform comparably to scFMs in predicting transcriptomic response to perturbations. However, their analysis does not account for distribution shift and focuses only on predictions for highly variable genes, many of which show little to no effect in response to a perturbation (<xref ref-type="bibr" rid="R30">Nadig et al., 2024</xref>).</p><p id="P7">Yet, distribution shift is a well-documented issue with scRNA-seq data (<xref ref-type="bibr" rid="R4">Boiarsky et al., 2023</xref>; <xref ref-type="bibr" rid="R29">Marklund et al., 2020</xref>), which often hinders the deployment of models that appear to perform well during evaluation. Distribution shift can occur as a consequence of inherent technical and biological noise, abundant in scRNA-seq data, and while scFMs have been proposed to mitigate such problems, there have been conflicting reports on their ability to do so (<xref ref-type="bibr" rid="R44">Theodoris et al., 2023</xref>; <xref ref-type="bibr" rid="R8">Cui et al., 2024</xref>; <xref ref-type="bibr" rid="R51">Wu et al., 2024</xref>; <xref ref-type="bibr" rid="R1">Ahlmann-Eltze et al., 2024</xref>). This highlights the need for a comprehensive benchmark to evaluate their limitations and failure modes, specifically for distribution shift.</p><sec id="S2"><label>1.1</label><title>Contributions</title><p id="P8">Here, we present PertEval-scFM to address these research gaps by providing: <list list-type="bullet" id="L1"><list-item><p id="P9">A standardized framework for evaluating biologically meaningful perturbation effect prediction in a zero-shot setting. The source code and documentation can be found on our <ext-link ext-link-type="uri" xlink:href="https://anonymous.4open.science/r/PertEval-C674/">GitHub</ext-link>.</p></list-item><list-item><p id="P10">Integration of a spectral graph theory method – SPECTRA (<xref ref-type="bibr" rid="R14">Ektefaie et al., 2024</xref>) – that allows us to assess model generalizability under distribution shift, a crucial consideration for real-world applications of scFMs.</p></list-item><list-item><p id="P11">A toolbox of comprehensive metrics, providing a detailed analysis of model performance, focusing on assessing robustness and sensitivity to distribution shifts.</p></list-item></list></p></sec></sec><sec id="S3"><label>2</label><title>PertEval-scFM</title><p id="P12">PertEval-scFM is designed to assess the zero-shot information content of scFM embeddings for perturbation effect prediction. To achieve this goal, we obtain zero-shot embeddings from five pre-trained scFMs across four datasets, then train a multi-layer perceptron (MLP) probe for each (<xref ref-type="bibr" rid="R24">Jin et al., 2019</xref>). This approach enables fair evaluation of the base information content of embeddings across models, as it evaluates representation quality while removing confounding effects introduced by task-specific prediction heads (<xref ref-type="bibr" rid="R43">Tenney et al., 2019</xref>; <xref ref-type="bibr" rid="R36">Radford et al., 2021</xref>). In <xref ref-type="fig" rid="F1">Figure 1</xref> we present an overview of the pipeline, composed of three main parts: data pre-processing, model training and evaluation. We define each part in the following section.</p><sec id="S4"><label>2.1</label><title>Data pre-processing</title><p id="P13">To interrogate cellular response to perturbations, we use high-dimensional Perturb-seq screens, which combine single-cell RNA sequencing with CRISPR-mediated genetic perturbations, enabling systematic profiling of transcriptional landscapes at single-cell resolution (<xref ref-type="bibr" rid="R11">Dixit et al., 2016</xref>). Perturb-seq data consists of transcriptomic data for unperturbed control cells <inline-formula><mml:math id="M1"><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and perturbed cells <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>P</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>n</italic><sub><italic>c</italic></sub> and <italic>n</italic><sub><italic>p</italic></sub> correspond to the number of control and perturbed cells being measured respectively, and <italic>g</italic> corresponds to the number of genes in the dataset. See <xref ref-type="supplementary-material" rid="SD1">Appendix A.1</xref> for further details.</p><sec id="S5"><label>2.1.1</label><title>Data Preparation</title><p id="P14">Briefly, our pre-processing pipeline consists of normalizing and log-transforming the raw expression count matrix <italic>C</italic>. We then select the top 2,000 highly variable genes (HVGs), <italic>v</italic>, obtaining a reduced control matrix <inline-formula><mml:math id="M3"><mml:mrow><mml:mi>C</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. Additionally, we identify the top 20 differentially expressed genes (DEGs) for each perturbation to ensure that our evaluations capture biologically relevant gene expression changes (see <xref ref-type="supplementary-material" rid="SD1">Appendix A.2</xref>).</p></sec><sec id="S6"><label>2.1.2</label><title>Data Featurization</title><p id="P15">To generate the input features for our baselines, we randomly select 500 cells from <italic>C</italic> to form a pseudo-bulk sample <inline-formula><mml:math id="M4"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula>. To combat noise and sparsity issues, we calculate the average expression across <inline-formula><mml:math id="M5"><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> and repeat this process <italic>n</italic><sub><italic>p</italic></sub> times. The resulting basal gene expression vectors can then be paired with perturbed cells, resulting in control expression feature matrix <inline-formula><mml:math id="M6"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (see <xref ref-type="supplementary-material" rid="SD1">Appendix C.1</xref>).</p><sec id="S7"><title>Single-cell foundation model embeddings</title><p id="P16">To construct the control cell embeddings, we then feed our input matrix <italic>X</italic><sub><italic>c</italic></sub> into the scFM: <disp-formula id="FD1"><label>(1)</label><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>scFM</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1em"/><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>e</italic> is the embedding dimension of the scFM. Perturbed cell embeddings <inline-formula><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> are generated by setting the expression counts of perturbed genes to zero in cells exposed to that perturbation, effectively simulating the perturbation <italic>in silico</italic>. We adopt this perturbation approach as it ensures consistent testing conditions across all models, given the current lack of standardized methods for generating comparable <italic>in silico</italic> representations of perturbations. The control and perturbation embeddings are then concatenated to form the final input for the MLP probe (see <xref ref-type="supplementary-material" rid="SD1">Appendix C.2</xref>). <disp-formula id="FD2"><label>(2)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>scFM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mi>Z</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="S8"><title>Raw expression data</title><p id="P17">To serve as a baseline against which to compare the performance of the scFM embeddings, we use our input matrix <italic>X</italic><sub><italic>c</italic></sub>. Here, we model single-gene perturbations by calculating the gene co-expression matrix <inline-formula><mml:math id="M10"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> between the perturbed genes and the highly variable genes in <italic>X</italic><sub><italic>c</italic></sub>. Similarly, for double-gene perturbations, we calculate the co-expression matrices for the individual perturbations, and then average them to obtain <italic>G</italic><sub><italic>c</italic></sub>. We then concatenate the control and perturbation embeddings to form the final input for the MLP probe (see <xref ref-type="supplementary-material" rid="SD1">Appendix C.1</xref>). <disp-formula id="FD3"><label>(3)</label><mml:math id="M11"><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>GE</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p></sec></sec></sec><sec id="S9"><label>2.2</label><title>Baseline models</title><p id="P18">We establish baseline models against which to compare the performance of the MLP probes trained with scFM embeddings.</p><sec id="S10"><title>MLP baseline</title><p id="P19">The MLP baseline uses log-normalized raw gene expression data directly as input. In doing so, we ensure that any performance differences can be traced back to the semantic information introduced into the embeddings by the scFM. The perturbation effect <inline-formula><mml:math id="M12"><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is predicted as follows: <disp-formula id="FD4"><label>(4)</label><mml:math id="M13"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>η</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>GE</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>GE</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mstyle><mml:mtext>b</mml:mtext></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mstyle><mml:mtext>b</mml:mtext></mml:mstyle><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where dimensions of parameters <italic>η</italic> correspond to <italic>W</italic><sub>1</sub> ∈ ℝ<sup><italic>h</italic>×2<italic>v</italic></sup>, <italic>W</italic><sub>2</sub> ∈ ℝ<sup><italic>v</italic>×<italic>h</italic></sup>, <bold>b</bold><sub>1</sub> ∈ ℝ<sup><italic>h</italic></sup> and <bold>b</bold><sub>2</sub> ∈ ℝ<sup><italic>v</italic></sup>.</p></sec><sec id="S11"><title>GEARS baseline</title><p id="P20">To benchmark zero-shot scFMs against existing task-specific methodologies, we implement GEARS, a current state-of-the-art method which integrates raw gene expression data with known biological priors via a graph-based model (<xref ref-type="bibr" rid="R39">Roohani et al., 2023</xref>). We reproduce the original implementation, modifying only the train-test splits to align with the SPECTRA framework, allowing evaluation of model robustness under distribution shift. All other training configurations, hyperparameters, and pre-processing steps followed the defaults provided in the GEARS implementation.</p></sec><sec id="S12"><title>Mean baseline</title><p id="P21">The mean baseline assumes that a perturbation has little effect on the perturbed cell’s gene expression. This reflects the biological reality that most perturbations result in small changes in gene expression, providing a simple biologically plausible null model highlighting the challenge inherent in distinguishing meaningful perturbation effects from background variability in single-cell data. The predicted perturbation effect, <inline-formula><mml:math id="M14"><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, is then simply computed as the deviation of the cell’s gene expression, <italic>X</italic><sub><italic>c</italic></sub>, from the mean gene expression of all cells in the same context, <inline-formula><mml:math id="M15"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, as defined by <inline-formula><mml:math id="M16"><mml:mrow><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>X</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>.</p></sec></sec><sec id="S13"><label>2.3</label><title>Training</title><sec id="S14"><label>2.3.1</label><title>Mlp Probe For Perturbation Effect Prediction</title><sec id="S15"><title>MLP probe design</title><p id="P22">A 1-hidden layer MLP was selected as a probe for its flexibility and simplicity in handling various types of data representations. For each perturbation, the MLP learns the log fold change perturbation effect <italic>δ</italic>, defined as: <disp-formula id="FD5"><label>(5)</label><mml:math id="M17"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>:=</mml:mo><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M18"><mml:mrow><mml:mi>P</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> represents the perturbed gene expression matrix. The MLP probe predicts the perturbation effect, denoted by <inline-formula><mml:math id="M19"><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, described by the following equation: <disp-formula id="FD6"><label>(6)</label><mml:math id="M20"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>scFM</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext>ReLU</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Z</mml:mi><mml:mrow><mml:mtext>scFM</mml:mtext></mml:mrow></mml:msub><mml:msubsup><mml:mi>W</mml:mi><mml:mn>1</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mstyle><mml:mtext>b</mml:mtext></mml:mstyle><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mn>2</mml:mn><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mstyle><mml:mtext>b</mml:mtext></mml:mstyle><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P23">The model parameters <italic>θ</italic> include the weight matrices <italic>W</italic><sub>1</sub> ∈ ℝ<sup><italic>h</italic>×2<italic>e</italic></sup> and <italic>W</italic><sub>2</sub> ∈ ℝ<sup><italic>e</italic>×<italic>h</italic></sup>, where <italic>h</italic> corresponds to the dimension of the hidden layer, and the bias vectors <bold>b</bold><sub>1</sub> ∈ ℝ<sup><italic>h</italic></sup> and <bold>b</bold><sub>2</sub> ∈ ℝ<sup><italic>e</italic></sup>.</p></sec><sec id="S16"><title>MLP probe parameters</title><p id="P24">To assess whether parameter count impacts model performance, we analyze the sensitivity of MLP probes to varying model capacities. We train a series of MLP probes with increasing parameter counts on both raw gene expression data and scFM embeddings. The results, summarized in <xref ref-type="supplementary-material" rid="SD1">Table D1</xref>, show no meaningful relationship between probe capacity and prediction performance. Additional details are provided in <xref ref-type="supplementary-material" rid="SD1">Appendix D.2</xref>.</p></sec></sec><sec id="S17"><label>2.3.2</label><title>Modeling Distribution Shift</title><p id="P25">To assess the robustness of the MLP probes when using either gene expression data or scFM embeddings, we implement SPECTRA (<xref ref-type="bibr" rid="R14">Ektefaie et al., 2024</xref>), a graph-based method that partitions data into increasingly challenging train-test splits while controlling for <italic>cross-split overlap</italic> between the train and test data.</p><p id="P26">In SPECTRA, edges within the graph represent sample-to-sample similarity. The connectivity of the similarity graph is controlled by the <italic>sparsification probability</italic> (<italic>s</italic>). For each split, this connectivity is adjusted by stochastically removing edges with probability <italic>s</italic>. We introduce the constraint <italic>s &lt; s</italic><sub>max</sub>, where <italic>s</italic><sub>max</sub> is empirically chosen to ensure a sufficient number of samples in both the train and test sets. After sparsification, the train and test sets are sampled from distinct subgraphs. As the sparsification probability increases, the degree of similarity between the train and test sets decreases, making it harder for the model to generalize to unseen perturbations effectively. For further details, see <xref ref-type="supplementary-material" rid="SD1">Appendix E</xref>.</p></sec></sec><sec id="S18"><label>2.4</label><title>Evaluation</title><p id="P27">Following the empirical findings from <xref ref-type="bibr" rid="R23">Ji et al. (2023)</xref>, we adopt <italic>Mean Squared Error (MSE)</italic> as our primary evaluation metric. In addition, to address the current lack of standardized evaluation metrics for perturbation effect prediction, we propose using three complementary metrics: (i) <italic>Area Under the SPECTRA Performance Curve (AUSPC)</italic> to quantify model generalization across distribution shifts, (ii) <italic>E-distance</italic> to quantitatively measure perturbation effect magnitude, and (iii) <italic>contextual alignment</italic> to measure how the overlap between pre-training and fine-tuning datasets influences model performance. Together, these metrics provide a robust basis for comparative scFM evaluation while capturing distinct aspects of predictive performance in transcriptomic perturbation response modeling.</p><sec id="S19"><label>2.4.1</label><title>AUSPC</title><p id="P28">To evaluate robustness under distribution shift, we adapt the approach introduced by <xref ref-type="bibr" rid="R14">Ektefaie et al. (2024)</xref> and formally define the AUSPC as: <disp-formula id="FD7"><label>(7)</label><mml:math id="M21"><mml:mrow><mml:mtext>AUSPC</mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>ϕ</italic>(<italic>s</italic>) is the MSE as a function of the sparsification probability <italic>s</italic> used to define each train-test split. Integrating the MSE across <italic>s</italic> yields a single performance metric that reflects a model’s ability to generalize under increasing distribution shift. The integral is approximated with the trapezoidal rule (see <xref ref-type="supplementary-material" rid="SD1">Appendix E.2</xref>).</p><p id="P29">Motivated by the observation that simple baselines often perform surprisingly well in perturbation prediction, we introduce the ∆AUSPC metric. This metric anchors a model’s robustness to the mean baseline. The ∆AUSPC is defined as: <disp-formula id="FD8"><label>(8)</label><mml:math id="M22"><mml:mrow><mml:mo>Δ</mml:mo><mml:mtext>AUSPC</mml:mtext><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mtext>max</mml:mtext></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math></disp-formula></p><p id="P30">Here, <italic>ϕ</italic><sub><italic>b</italic></sub> represents the MSE of the mean baseline, and <italic>ϕ</italic><sub><italic>m</italic></sub> is the MSE of the model being evaluated. A positive ∆AUSPC indicates that the model outperforms the baseline, while a negative value suggests the opposite. This metric provides a clear measure of a model’s generalizability improvement over simply predicting the mean perturbation effect.</p></sec><sec id="S20"><label>2.4.2</label><title>E-Distance</title><p id="P31">We use the E-distance, introduced by <xref ref-type="bibr" rid="R33">Peidli et al. (2024)</xref>, to quantify the difference between perturbed and control cell gene expression profiles (<xref ref-type="supplementary-material" rid="SD1">Appendix F.1</xref>). This metric captures both within-group variability and distributional differences, offering a robust measure of perturbation effect strength. With E-distance, we can better analyze the characteristics of perturbations that models handle well versus those they struggle with, providing context for model performance—particularly in cases where outlier perturbations may not be immediately apparent using traditional metrics.</p></sec><sec id="S21"><label>2.4.3</label><title>Contextual Alignment</title><p id="P32">While pre-training dataset size is often linked to improved downstream model performance, recent research emphasizes the critical role of data quality over dataset size (<xref ref-type="bibr" rid="R15">El-Nouby et al., 2021</xref>; <xref ref-type="bibr" rid="R19">Fournier et al., 2024</xref>). We therefore suggest the inclusion of a contextual alignment metric, which quantifies the similarity between the pre-training and fine-tuning datasets, and its effect on model performance. We calculate the cross-split overlap between the pre-train and fine-tune datasets using cosine similarity, to determine how representative the pre-training data is of the fine-tuning data (see <xref ref-type="supplementary-material" rid="SD1">Appendix G.1</xref>).</p></sec></sec><sec id="S22"><label>2.5</label><title>Models and Datasets</title><sec id="S23"><label>2.5.1</label><title>Single-Cell Foundation Models</title><p id="P33">PertEval-scFM currently evaluates the performance of the following five scFMs: scBERT (<xref ref-type="bibr" rid="R52">Yang et al., 2022</xref>), Geneformer (<xref ref-type="bibr" rid="R44">Theodoris et al., 2023</xref>), scGPT (<xref ref-type="bibr" rid="R8">Cui et al., 2024</xref>), scFoundation (<xref ref-type="bibr" rid="R21">Hao et al., 2023</xref>) and UCE (<xref ref-type="bibr" rid="R40">Rosen et al., 2023</xref>). See <xref ref-type="supplementary-material" rid="SD1">Appendix B.1</xref> for details on their architecture and pre-training data.</p></sec><sec id="S24"><label>2.5.2</label><title>Datasets</title><sec id="S25"><title>Norman</title><p id="P34">PertEval-scFM is applied to the 105 single-gene and 91 double-gene perturbation datasets derived from a Perturb-seq screen in K562 cells from <xref ref-type="bibr" rid="R31">Norman et al. (2019)</xref>. These datasets contain strong CRISPRa perturbation signals, as well as baseline expression for unperturbed cells, which allows for the systematic evaluation of model performance in predicting the effects of genetic perturbations at single-cell resolution.</p></sec><sec id="S26"><title>Replogle</title><p id="P35">Additionally, we apply our framework to the single-gene perturbation datasets from <xref ref-type="bibr" rid="R38">Replogle et al. (2022)</xref>, which profile transcriptomic responses to CRISPRi-mediated genetic perturbations in both K562 (2,058 perturbations) and RPE1 (2,394 perturbations) cells. Compared to the Norman dataset, the overall perturbation effect signal in <xref ref-type="bibr" rid="R38">Replogle et al. (2022)</xref> is less pronounced, despite the considerably larger number of perturbations (<xref ref-type="bibr" rid="R33">Peidli et al., 2024</xref>). For additional details on the datasets, see <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>.</p></sec></sec></sec></sec><sec id="S27" sec-type="results"><label>3</label><title>Results</title><sec id="S28"><label>3.1</label><title>Evaluation across 2,000 HVGs</title><p id="P36">In our initial evaluation, we assess the models’ ability to predict the effect of perturbations on the top 2,000 HVGs. We present our evaluation results in <xref ref-type="table" rid="T1">Table 1</xref>, <xref ref-type="table" rid="T2">Table 2</xref> and <xref ref-type="fig" rid="F2">Figure 2</xref> and discuss the results for each dataset below.</p><sec id="S29"><title>Norman single-gene</title><p id="P37">For single-gene perturbations, GEARS achieved the highest performance with an AUSPC of 0.00815, followed by the MLP baseline at 0.0448, both significantly outperforming all scFM models. Performance differences between the scFMs were minimal, suggesting similar capabilities in predicting perturbation effects. As the sparsification probability <italic>s</italic> increased from 0.1 to 0.7, MSE values rose across all models. However, scFMs showed a steeper performance drop at higher sparsification levels compared to the MLP baseline, suggesting that the embeddings are less robust to distribution shifts.</p></sec><sec id="S30"><title>Norman double-gene</title><p id="P38">The ranking pattern observed for single-gene perturbations persisted for double-gene perturbations, with GEARS and the MLP baseline performing the best. Most scFM models displayed similar performance, with a notable exception of scGPT, which experienced a significant drop in performance, moving from rank 3 to rank 8. This increased the performance spread, with a larger ∆AUSPC difference (∆AUSPC range of 0.00980 for double-gene perturbations compared to 0.00126 for single-gene), highlighting greater variability in model performance for the more complex perturbations. As the <italic>s</italic> increased, the MSE worsened across all models.</p></sec><sec id="S31"><title>Replogle K562</title><p id="P39">For the K562 cell line, GEARS maintained its superior performance with an AUSPC of 0.0082, followed by scGPT and UCE with an AUSPC of 0.1384, while the MLP baseline only ranked fifth with an AUSPC of 0.1420. Apart from GEARS, none of these differences were statistically significant, as indicated by the overlapping error bars in <xref ref-type="fig" rid="F2">Figure 2b</xref>.</p></sec><sec id="S32"><title>Replogle RPE1</title><p id="P40">In the RPE1 cell line, GEARS again outperformed all other models with an AUSPC of 0.0157, followed by the Mean baseline (0.1251) and the MLP baseline (0.1252). The scFMs exhibited very similar performance, with AUSPC values tightly clustered and no significant differences observed.</p><p id="P41">Overall, GEARS outperforms all zero-shot scFMs and baseline models by an order of magnitude, suggesting that its architecture and training paradigm allow it to better capture underlying biological processes and generalize more effectively across a variety of perturbation types. This underscores the importance of incorporating strong inductive biases in the gene perturbation prediction task and suggests that current scFMs, which rely solely on a masked pretraining objective, are only able to capture average perturbation effects.</p></sec></sec><sec id="S33"><label>3.2</label><title>Additional experiments</title><p id="P42">We conduct additional experiments on the Norman single- and double-gene perturbation datasets. Since these datasets exhibit stronger perturbation effects, they provide a clearer signal to tackle more challenging tasks. Poor performance in this setting would be evidence that the additional semantic information from the scFMs does not enhance predictive performance for perturbation effect prediction.</p><sec id="S34"><label>3.2.1</label><title>Evaluation Across Degs</title><p id="P43">We evaluate the models on predicting the effect of perturbations on the top 20 DEGs per perturbation, providing a more stringent test of model performance than the full set of 2,000 HVGs. Indeed, genetic perturbations typically alter the expression of a limited subset of genes within the transcriptome (<xref ref-type="supplementary-material" rid="SD1">Figure A4</xref>), hence models predicting mean gene expression can still achieve low MSE values across 2,000 HVGs. The results of the evaluation across DEGs are displayed in <xref ref-type="table" rid="T3">Table 3</xref>.</p><sec id="S35"><title>Norman single-gene</title><p id="P44">For single-gene perturbations, GEARS achieved the highest performance (AUSPC 0.266), significantly outperforming all scFMs and baseline models. The MLP baseline ranked second (0.342), followed closely by UCE (0.334), which was the best-performing scFM. As the sparsification probability increased, MSE values worsened across all models, with Geneformer and scGPT exhibiting the steepest decline in performance. scBERT performed best among scFMs across most sparsity levels (∆AUSPC 0.00878), while UCE provided the most stable results throughout (∆AUSPC 0.0108). These results suggest that, while some scFMs exhibit marginal advantages over the mean baseline, the overall gains remain small. Performance gaps remained minimal, with UCE (best) only outperforming Geneformer (worst) by 4.8%.</p></sec><sec id="S36"><title>Norman double-gene</title><p id="P45">A similar ranking pattern was observed for double-gene perturbations, where models outperformed the mean baseline but showed no clear advantage over the MLP. GEARS again demonstrated superior performance (AUSPC 0.223, ∆AUSPC 0.299), far surpassing the MLP baseline (AUSPC 0.371, ∆AUSPC 0.151) and all scFMs. The performance gap widened for more complex perturbations, suggesting that GEARS’ architecture and training paradigm allow it to better model gene interactions and their impact on the effect of perturbations.</p><p id="P46">Overall, this evaluation proves more challenging, evidenced by the order of magnitude increase in MSE. However, the results again highlight that zero-shot scFM embeddings do not provide a meaningful advantage over simple baseline approaches. Consistent with previous findings, scFM models struggled to generalize to perturbation-specific expression shifts, further reinforcing their limitations in predicting biologically relevant perturbation effects.</p></sec></sec></sec><sec id="S37"><label>3.2.2</label><title>E-Distance</title><p id="P47">We analyzed the relationship between perturbation strength and model performance using E-distance to measure perturbation effect magnitude. The results, shown in <xref ref-type="fig" rid="F3">Figure 3a</xref>, confirm that models generally perform worse when predicting perturbations with higher E-distance, which corresponds to stronger perturbation effects. This pattern was consistent across all models, for both single-gene and double-gene perturbations, supporting the hypothesis that training data biased toward moderate perturbation effects limits a model’s ability to generalize to more extreme cases.</p><p id="P48"><xref ref-type="fig" rid="F3">Figure 3b</xref> further illustrates how perturbation strength varies across train-test splits for both single- and double-gene perturbations. At higher sparsification probabilities, low E-distance perturbations become less frequent, while stronger perturbations appear more often. This aligns with previous observations that model performance declines as sparsity increases, as models are increasingly challenged to predict rare, stronger perturbation effects. Two examples illustrate this trend: <italic>AHR</italic> (<xref ref-type="fig" rid="F4">Figure 4a</xref>), a perturbation with low E-distance, exhibited a narrow effect range (–0.1 to 0.25) and was predicted with relatively high accuracy. In contrast, <italic>CEBPE</italic> (<xref ref-type="fig" rid="F4">Figure 4b</xref>), which produced a broader and more pronounced perturbation effect (–0.5 to 1), was predicted with significantly lower accuracy.</p><p id="P49">However, deviations from this trend suggest that the magnitude of the perturbation alone does not fully determine prediction difficulty. <italic>CEBPA</italic> (<xref ref-type="fig" rid="F4">Figure 4d</xref>), despite being a strong perturbation, was predicted with relatively high accuracy. This could be attributed to its localized effect on a small subset of genes, with a long tail of mildly or non-affected genes. Conversely, <italic>IKZF3</italic> (<xref ref-type="fig" rid="F4">Figure 4c</xref>), which elicited a weaker overall perturbation, was predicted with lower accuracy - likely due to its atypical effect distribution (<xref ref-type="supplementary-material" rid="SD1">Appendix H.7</xref>). These results suggest that a model’s capability to predict perturbation effects depends not only on the magnitude of the perturbation, but also on its distribution.</p><p id="P50">Taken together, these findings highlight the importance of a more balanced representation of perturbation effects during training. Ensuring that training data covers a diverse range of perturbation magnitudes and distributions could improve model generalization. However, current scFM embeddings alone do not address this challenge, reinforcing the need for targeted strategies to enhance model robustness in perturbation effect prediction.</p><sec id="S38"><label>3.2.3</label><title>Contextual Alignment</title><p id="P51">Previous work by <xref ref-type="bibr" rid="R8">Cui et al. (2024)</xref> demonstrated that the performance of zero-shot scFM models in cell-type annotation tasks is highly dependent on the overlap between their pre-training datasets and the downstream task data. To investigate whether this reliance on contextual alignment extends to perturbation effect prediction, we analyzed the similarity between pre-training corpus and fine-tuning dataset, as well as its impact on model performance.</p><p id="P52">In <xref ref-type="fig" rid="F5">Figure 5</xref>, we compute the contextual alignment between the pre-training datasets of scGPT and scBERT and the Norman dataset. scBERT exhibits a higher alignment score (0.718) compared to scGPT (0.606), indicating that its pretraining corpus is approximately 19% more similar to the Norman dataset. Despite both models displaying comparable MSE across splits, scBERT demonstrates greater robustness, suggesting that contextual alignment plays a more significant role than dataset scale alone. Notably, scGPT’s pre-training corpus is an order of magnitude larger than scBERT’s, reinforcing the idea that increasing dataset size does not necessarily compensate for a lack of alignment with the downstream task.</p><p id="P53">For this analysis we focus on scGPT and scBERT, however assessing this phenomenon across a broader range of models and perturbation datasets is necessary to fully understand its impact on perturbation effect prediction. Future studies should explore how curating pre-training datasets to better reflect perturbation-specific distributions influences model performance, particularly in cases where strong or rare perturbations are under-represented in the training corpus.</p></sec></sec><sec id="S39"><label>3.3</label><title>Limitations</title><p id="P54">A key limitation of our study is that perturbations are modeled exclusively as knockouts, regardless of their original experimental context. This constraint aligns with prior work and reflects an inherent limitation of current foundation models, as not all architectures support the modeling of gene activation. More broadly, the lack of a standardized approach for representing perturbations in scFMs limits the interpretability and generalizability of findings across studies. To advance the field, there is a pressing need for consensus on how to represent perturbations, ensuring they can robustly capture a diverse range of modalities.</p></sec></sec><sec id="S40" sec-type="conclusions"><label>4</label><title>Conclusion</title><p id="P55">PertEval-scFM addresses the current lack of consensus in benchmarking models for perturbation effect prediction by introducing a modular evaluation toolkit with diverse metrics, designed to assess and interpret model performance. Notably, our framework accounts for distribution shift, a factor often overlooked in previous studies. We applied PertEval-scFM to evaluate whether zero-shot scFM embeddings provide an advantage over raw gene expression data for perturbation effect prediction. Our results across all datasets demonstrate that current-generation scFM embeddings offer no improvement over baseline models when evaluated on 2,000 HVGs or the top 20 DEGs. The E-distance and contextual alignment metrics allow us to further contextualize our results and identify the failure modes of our models. We plan to maintain and expand PertEval-scFM, developing a comprehensive benchmarking suite to facilitate the evaluation of perturbation models, and expect it to become a valuable community resource.</p><sec id="S41"><title>Future work</title><p id="P56">While our findings do not support the use of current scFMs for reliable perturbation effect prediction, we recognize their potential. Key open questions include how to best represent perturbations <italic>in silico</italic> and how to fully leverage large-scale pre-training data to improve prediction accuracy. Existing cell atlases capture only a small fraction of the human phenoscape — the full range of states a cell can occupy (<xref ref-type="bibr" rid="R17">Fleck et al., 2023</xref>) — and often exclude perturbation-induced states. Moreover, specialized models must be designed to fully leverage large-scale datasets for predicting transcriptomic responses to perturbations. The superior performance of GEARS, which incorporates inductive biases tailored to perturbation prediction, exemplifies the importance of model architectures that explicitly encode relevant biological priors. Future efforts should focus on bridging these gaps to enable scFMs to meaningfully contribute to perturbation effect prediction.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS199216-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d11aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S42"><title>Acknowledgments</title><p>The authors would like to thank Martino Mansoldo, Ionelia Buzatu, Pascal Notin, Andreas Bender, Kristof Szalay, Yuge Ji, Charlotte Bunne, Patrick Schwab, Djorde Miladinovic, Francesco Piatti, Michael Barnes, Claudia Cabrera, and Wei Wei for their helpful feedback and discussions.</p><p>A.W., M.O., W.T.D. and S.P.C. receive funding from the UKRI/BBSRC Collaborative Training Partnership in AI for Drug Discovery and Queen Mary University of London. N.B. and A.G.S. receive funding from the Wellcome Trust [218584/Z/19/Z].</p><p>This research utilised Queen Mary’s Apocrita HPC facility, supported by QMUL Research-IT (<xref ref-type="bibr" rid="R26">King et al., 2017</xref>).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahlmann-Eltze</surname><given-names>C</given-names></name><name><surname>Huber</surname><given-names>W</given-names></name><name><surname>Anders</surname><given-names>S</given-names></name></person-group><article-title>Deep learning-based predictions of gene perturbation effects do not yet outperform simple linear methods</article-title><year>2024</year><month>September</month></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name></person-group><article-title>Representation Learning: A Review and New Perspectives</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2013</year><volume>35</volume><issue>8</issue><fpage>1798</fpage><lpage>1828</lpage><pub-id pub-id-type="pmid">23787338</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bergstra</surname><given-names>J</given-names></name><name><surname>Bardenet</surname><given-names>R</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Kégl</surname><given-names>B</given-names></name></person-group><article-title>Algorithms for Hyper-Parameter Optimization</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2011</year><volume>24</volume></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boiarsky</surname><given-names>R</given-names></name><name><surname>Singh</surname><given-names>N</given-names></name><name><surname>Buendia</surname><given-names>A</given-names></name><name><surname>Getz</surname><given-names>G</given-names></name><name><surname>Sontag</surname><given-names>D</given-names></name></person-group><article-title>A Deep Dive into Single-Cell RNA Sequencing Foundation Models</article-title><year>2023</year><month>October</month></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Branson</surname><given-names>N</given-names></name><name><surname>Cutillas</surname><given-names>PR</given-names></name><name><surname>Besseant</surname><given-names>C</given-names></name></person-group><article-title>Understanding the Sources of Performance in Deep Learning Drug Response Prediction Models</article-title><year>2024</year><month>June</month></element-citation></ref><ref id="R6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T</given-names></name><name><surname>Mann</surname><given-names>B</given-names></name><name><surname>Ryder</surname><given-names>N</given-names></name><name><surname>Subbiah</surname><given-names>M</given-names></name><name><surname>Kaplan</surname><given-names>JD</given-names></name><name><surname>Dhariwal</surname><given-names>P</given-names></name><name><surname>Neelakantan</surname><given-names>A</given-names></name><name><surname>Shyam</surname><given-names>P</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><etal/></person-group><article-title>Language Models are Few-Shot Learners</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2020</year><volume>33</volume><fpage>1877</fpage><lpage>1901</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Ning</surname><given-names>B</given-names></name><name><surname>Shi</surname><given-names>T</given-names></name></person-group><article-title>Single-Cell RNA-Seq Technologies and Related Computational Data Analysis</article-title><source>Frontiers in Genetics</source><publisher-name>Frontiers</publisher-name><year>2019</year><month>April</month><volume>10</volume><comment>ISSN 1664-8021</comment><pub-id pub-id-type="pmcid">PMC6460256</pub-id><pub-id pub-id-type="pmid">31024627</pub-id><pub-id pub-id-type="doi">10.3389/fgene.2019.00317</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cui</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Maan</surname><given-names>H</given-names></name><name><surname>Pang</surname><given-names>K</given-names></name><name><surname>Luo</surname><given-names>F</given-names></name><name><surname>Duan</surname><given-names>N</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name></person-group><article-title>scGPT: toward building a foundation model for single-cell multi-omics using generative AI</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2024</year><month>February</month><fpage>1</fpage><lpage>11</lpage><comment>ISSN 1548-7105</comment><pub-id pub-id-type="pmid">38409223</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dao</surname><given-names>T</given-names></name><name><surname>Fu</surname><given-names>DY</given-names></name><name><surname>Ermon</surname><given-names>S</given-names></name><name><surname>Rudra</surname><given-names>A</given-names></name><name><surname>Ré</surname><given-names>C</given-names></name></person-group><article-title>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</article-title><year>2022</year><month>June</month></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>M-W</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><article-title>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</article-title><year>2019</year><month>May</month></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dixit</surname><given-names>A</given-names></name><name><surname>Parnas</surname><given-names>O</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Fulco</surname><given-names>CP</given-names></name><name><surname>Jerby-Arnon</surname><given-names>L</given-names></name><name><surname>Marjanovic</surname><given-names>ND</given-names></name><name><surname>Dionne</surname><given-names>D</given-names></name><name><surname>Burks</surname><given-names>T</given-names></name><name><surname>Raychowdhury</surname><given-names>R</given-names></name><name><surname>Adamson</surname><given-names>B</given-names></name><etal/></person-group><article-title>Perturb-Seq: Dissecting Molecular Circuits with Scalable Single-Cell RNA Profiling of Pooled Genetic Screens</article-title><source>Cell</source><year>2016</year><month>December</month><volume>167</volume><issue>7</issue><fpage>1853</fpage><lpage>1866</lpage><elocation-id>e17</elocation-id><comment>ISSN 0092-8674</comment><pub-id pub-id-type="pmcid">PMC5181115</pub-id><pub-id pub-id-type="pmid">27984732</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2016.11.038</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Dehghani</surname><given-names>M</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Heigold</surname><given-names>G</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><etal/></person-group><article-title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><year>2021</year><month>June</month></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>J</given-names></name><name><surname>Jia</surname><given-names>P</given-names></name><name><surname>Dai</surname><given-names>Y</given-names></name><name><surname>Tao</surname><given-names>C</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Zhi</surname><given-names>D</given-names></name></person-group><article-title>Gene2vec: distributed representation of genes based on co-expression</article-title><source>BMC Genomics</source><year>2019</year><month>February</month><volume>20</volume><issue>1</issue><fpage>82</fpage><comment>ISSN 1471-2164</comment><pub-id pub-id-type="pmcid">PMC6360648</pub-id><pub-id pub-id-type="pmid">30712510</pub-id><pub-id pub-id-type="doi">10.1186/s12864-018-5370-x</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ektefaie</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>A</given-names></name><name><surname>Bykova</surname><given-names>D</given-names></name><name><surname>Marin</surname><given-names>M</given-names></name><name><surname>Zitnik</surname><given-names>M</given-names></name><name><surname>Farhat</surname><given-names>M</given-names></name></person-group><article-title>Evaluating generalizability of artificial intelligence models for molecular datasets</article-title><year>2024</year><month>February</month></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El-Nouby</surname><given-names>A</given-names></name><name><surname>Izacard</surname><given-names>G</given-names></name><name><surname>Touvron</surname><given-names>H</given-names></name><name><surname>Laptev</surname><given-names>I</given-names></name><name><surname>Jegou</surname><given-names>H</given-names></name><name><surname>Grave</surname><given-names>E</given-names></name></person-group><article-title>Are Large-scale Datasets Necessary for Self-Supervised Pre-training?</article-title><year>2021</year><month>December</month></element-citation></ref><ref id="R16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ericsson</surname><given-names>L</given-names></name><name><surname>Gouk</surname><given-names>H</given-names></name><name><surname>Loy</surname><given-names>CC</given-names></name><name><surname>Hospedales</surname><given-names>TM</given-names></name></person-group><article-title>Self-Supervised Representation Learning: Introduction, advances, and challenges</article-title><source>IEEE Signal Processing Magazine</source><publisher-name>IEEE Signal Processing Magazine</publisher-name><year>2022</year><month>May</month><volume>39</volume><issue>3</issue><fpage>42</fpage><lpage>62</lpage><comment>ISSN 1558-0792</comment><pub-id pub-id-type="doi">10.1109/MSP.2021.3134634</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fleck</surname><given-names>JS</given-names></name><name><surname>Camp</surname><given-names>JG</given-names></name><name><surname>Treutlein</surname><given-names>B</given-names></name></person-group><article-title>What is a cell type?</article-title><source>Science</source><publisher-name>American Association for the Advancement of Science</publisher-name><year>2023</year><month>August</month><volume>381</volume><issue>6659</issue><fpage>733</fpage><lpage>734</lpage><pub-id pub-id-type="pmid">37590360</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>SJ</given-names></name><name><surname>Chaffin</surname><given-names>MD</given-names></name><name><surname>Arduini</surname><given-names>A</given-names></name><name><surname>Akkad</surname><given-names>A-D</given-names></name><name><surname>Banks</surname><given-names>E</given-names></name><name><surname>Marioni</surname><given-names>JC</given-names></name><name><surname>Philippakis</surname><given-names>AA</given-names></name><name><surname>Ellinor</surname><given-names>PT</given-names></name><name><surname>Babadi</surname><given-names>M</given-names></name></person-group><article-title>Unsupervised removal of systematic background noise from droplet-based single-cell experiments using CellBender</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>September</month><volume>20</volume><issue>9</issue><fpage>1323</fpage><lpage>1335</lpage><comment>ISSN 1548-7105</comment><pub-id pub-id-type="pmid">37550580</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fournier</surname><given-names>Q</given-names></name><name><surname>Vernon</surname><given-names>RM</given-names></name><name><surname>Sloot</surname><given-names>Avd</given-names></name><name><surname>Schulz</surname><given-names>B</given-names></name><name><surname>Chandar</surname><given-names>S</given-names></name><name><surname>Langmead</surname><given-names>CJ</given-names></name></person-group><article-title>Protein Language Models: Is Scaling Necessary?</article-title><year>2024</year><month>September</month></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gavriilidis</surname><given-names>GI</given-names></name><name><surname>Vasileiou</surname><given-names>V</given-names></name><name><surname>Orfanou</surname><given-names>A</given-names></name><name><surname>Ishaque</surname><given-names>N</given-names></name><name><surname>Psomopoulos</surname><given-names>F</given-names></name></person-group><article-title>A mini-review on perturbation modelling across single-cell omic modalities</article-title><source>Computational and Structural Biotechnology Journal</source><year>2024</year><month>December</month><volume>23</volume><fpage>1886</fpage><lpage>1896</lpage><comment>ISSN 2001-0370</comment><pub-id pub-id-type="pmcid">PMC11076269</pub-id><pub-id pub-id-type="pmid">38721585</pub-id><pub-id pub-id-type="doi">10.1016/j.csbj.2024.04.058</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>M</given-names></name><name><surname>Gong</surname><given-names>J</given-names></name><name><surname>Zeng</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Song</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name></person-group><article-title>Large Scale Foundation Model on Single-cell Transcriptomics</article-title><year>2023</year><month>June</month><pub-id pub-id-type="pmid">38844628</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heumos</surname><given-names>L</given-names></name><name><surname>Ji</surname><given-names>Y</given-names></name><name><surname>May</surname><given-names>L</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>X</given-names></name><name><surname>Ostner</surname><given-names>J</given-names></name><name><surname>Peidli</surname><given-names>S</given-names></name><name><surname>Schumacher</surname><given-names>A</given-names></name><name><surname>Hrovatin</surname><given-names>K</given-names></name><name><surname>Müller</surname><given-names>M</given-names></name><etal/></person-group><article-title>Pertpy: an end-to-end framework for perturbation analysis</article-title><year>2024</year><month>August</month></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>Y</given-names></name><name><surname>Green</surname><given-names>TD</given-names></name><name><surname>Peidli</surname><given-names>S</given-names></name><name><surname>Bahrami</surname><given-names>M</given-names></name><name><surname>Liu</surname><given-names>M</given-names></name><name><surname>Zappia</surname><given-names>L</given-names></name><name><surname>Hrovatin</surname><given-names>K</given-names></name><name><surname>Sander</surname><given-names>C</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>Optimal distance metrics for single-cell RNA-seq populations</article-title><year>2023</year><month>December</month></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>Q</given-names></name><name><surname>Dhingra</surname><given-names>B</given-names></name><name><surname>Cohen</surname><given-names>WW</given-names></name><name><surname>Lu</surname><given-names>X</given-names></name></person-group><article-title>Probing Biomedical Embeddings from Language Models</article-title><year>2019</year><month>April</month></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kedzierska</surname><given-names>KZ</given-names></name><name><surname>Crawford</surname><given-names>L</given-names></name><name><surname>Amini</surname><given-names>AP</given-names></name><name><surname>Lu</surname><given-names>AX</given-names></name></person-group><article-title>Assessing the limits of zero-shot foundation models in single-cell biology</article-title><year>2023</year><month>November</month></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>T</given-names></name><name><surname>Butcher</surname><given-names>S</given-names></name><name><surname>Zalewski</surname><given-names>L</given-names></name></person-group><article-title>Apocrita - High Performance Computing Cluster for Queen Mary University of London</article-title><year>2017</year><month>March</month><pub-id pub-id-type="doi">10.5281/zenodo.438045</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><year>2017</year><month>January</month></element-citation></ref><ref id="R28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lotfollahi</surname><given-names>M</given-names></name><name><surname>Wolf</surname><given-names>FA</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>scGen predicts single-cell perturbation responses</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2019</year><month>August</month><volume>16</volume><issue>8</issue><fpage>715</fpage><lpage>721</lpage><comment>ISSN 1548-7105</comment><pub-id pub-id-type="pmid">31363220</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marklund</surname><given-names>H</given-names></name><name><surname>Xie</surname><given-names>SM</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Balsubramani</surname><given-names>A</given-names></name><name><surname>Hu</surname><given-names>W</given-names></name><name><surname>Yasunaga</surname><given-names>M</given-names></name><name><surname>Phillips</surname><given-names>RL</given-names></name><name><surname>Beery</surname><given-names>S</given-names></name><name><surname>Leskovec</surname><given-names>J</given-names></name><name><surname>Kundaje</surname><given-names>A</given-names></name><etal/></person-group><article-title>Wilds: A benchmark of in-the-wild distribution shifts</article-title><source>arXiv preprint</source><year>2020</year><elocation-id>arXiv:2012.07421</elocation-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadig</surname><given-names>A</given-names></name><name><surname>Replogle</surname><given-names>JM</given-names></name><name><surname>Pogson</surname><given-names>AN</given-names></name><name><surname>McCarroll</surname><given-names>SA</given-names></name><name><surname>Weissman</surname><given-names>JS</given-names></name><name><surname>Robinson</surname><given-names>EB</given-names></name><name><surname>O’Connor</surname><given-names>LJ</given-names></name></person-group><article-title>Transcriptome-wide characterization of genetic perturbations</article-title><year>2024</year><month>July</month></element-citation></ref><ref id="R31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>TM</given-names></name><name><surname>Horlbeck</surname><given-names>MA</given-names></name><name><surname>Replogle</surname><given-names>JM</given-names></name><name><surname>Ge</surname><given-names>AY</given-names></name><name><surname>Xu</surname><given-names>A</given-names></name><name><surname>Jost</surname><given-names>M</given-names></name><name><surname>Gilbert</surname><given-names>LA</given-names></name><name><surname>Weissman</surname><given-names>JS</given-names></name></person-group><article-title>Exploring genetic interaction manifolds constructed from rich single-cell phenotypes</article-title><source>Science</source><publisher-name>American Association for the Advancement of Science</publisher-name><year>2019</year><month>August</month><volume>365</volume><issue>6455</issue><fpage>786</fpage><lpage>793</lpage><pub-id pub-id-type="pmcid">PMC6746554</pub-id><pub-id pub-id-type="pmid">31395745</pub-id><pub-id pub-id-type="doi">10.1126/science.aax4438</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="report"><collab>OpenAI</collab><source>GPT-4 Technical Report</source><year>2024</year><month>March</month></element-citation></ref><ref id="R33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peidli</surname><given-names>S</given-names></name><name><surname>Green</surname><given-names>TD</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Gross</surname><given-names>T</given-names></name><name><surname>Min</surname><given-names>J</given-names></name><name><surname>Garda</surname><given-names>S</given-names></name><name><surname>Yuan</surname><given-names>B</given-names></name><name><surname>Schumacher</surname><given-names>LJ</given-names></name><name><surname>Taylor-King</surname><given-names>JP</given-names></name><name><surname>Marks</surname><given-names>DS</given-names></name><name><surname>Luna</surname><given-names>A</given-names></name><etal/></person-group><article-title>scPerturb: harmonized single-cell perturbation data</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2024</year><month>March</month><volume>21</volume><issue>3</issue><fpage>531</fpage><lpage>540</lpage><comment>ISSN 1548-7105</comment><pub-id pub-id-type="pmid">38279009</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Program, C. S.-C. B</collab><name><surname>Abdulla</surname><given-names>S</given-names></name><name><surname>Aevermann</surname><given-names>B</given-names></name><name><surname>Assis</surname><given-names>P</given-names></name><name><surname>Badajoz</surname><given-names>S</given-names></name><name><surname>Bell</surname><given-names>SM</given-names></name><name><surname>Bezzi</surname><given-names>E</given-names></name><name><surname>Cakir</surname><given-names>B</given-names></name><name><surname>Chaffer</surname><given-names>J</given-names></name><name><surname>Chambers</surname><given-names>S</given-names></name><name><surname>Cherry</surname><given-names>JM</given-names></name><etal/></person-group><article-title>CZ CELL×GENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data</article-title><year>2023</year><month>November</month><pub-id pub-id-type="pmcid">PMC11701654</pub-id><pub-id pub-id-type="pmid">39607691</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkae1142</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Narasimhan</surname><given-names>K</given-names></name><name><surname>Salimans</surname><given-names>T</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><article-title>Improving Language Understanding by Generative Pre-Training</article-title><source>arXiv</source><year>2018</year></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Hallacy</surname><given-names>C</given-names></name><name><surname>Ramesh</surname><given-names>A</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Krueger</surname><given-names>G</given-names></name><etal/></person-group><article-title>Learning Transferable Visual Models From Natural Language Supervision</article-title><source>arXiv:2103.00020 [cs]</source><year>2021</year><month>February</month></element-citation></ref><ref id="R37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Regev</surname><given-names>A</given-names></name><name><surname>Teichmann</surname><given-names>SA</given-names></name><name><surname>Lander</surname><given-names>ES</given-names></name><name><surname>Amit</surname><given-names>I</given-names></name><name><surname>Benoist</surname><given-names>C</given-names></name><name><surname>Birney</surname><given-names>E</given-names></name><name><surname>Bodenmiller</surname><given-names>B</given-names></name><name><surname>Campbell</surname><given-names>P</given-names></name><name><surname>Carninci</surname><given-names>P</given-names></name><name><surname>Clatworthy</surname><given-names>M</given-names></name><etal/></person-group><article-title>The human cell atlas</article-title><source>elife</source><publisher-name>eLife Sciences Publications, Ltd</publisher-name><year>2017</year><volume>6</volume><elocation-id>e27041</elocation-id><pub-id pub-id-type="pmcid">PMC5762154</pub-id><pub-id pub-id-type="pmid">29206104</pub-id><pub-id pub-id-type="doi">10.7554/eLife.27041</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Replogle</surname><given-names>JM</given-names></name><name><surname>Saunders</surname><given-names>RA</given-names></name><name><surname>Pogson</surname><given-names>AN</given-names></name><name><surname>Hussmann</surname><given-names>JA</given-names></name><name><surname>Lenail</surname><given-names>A</given-names></name><name><surname>Guna</surname><given-names>A</given-names></name><name><surname>Mascibroda</surname><given-names>L</given-names></name><name><surname>Wagner</surname><given-names>EJ</given-names></name><name><surname>Adelman</surname><given-names>K</given-names></name><name><surname>Lithwick-Yanai</surname><given-names>G</given-names></name><name><surname>Iremadze</surname><given-names>N</given-names></name><etal/></person-group><article-title>Mapping information-rich genotype-phenotype landscapes with genome-scale Perturb-seq</article-title><source>Cell</source><year>2022</year><month>July</month><volume>185</volume><issue>14</issue><fpage>2559</fpage><lpage>2575</lpage><elocation-id>e28</elocation-id><comment>ISSN 0092-8674</comment><pub-id pub-id-type="pmcid">PMC9380471</pub-id><pub-id pub-id-type="pmid">35688146</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2022.05.013</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Roohani</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Leskovec</surname><given-names>J</given-names></name></person-group><article-title>Predicting transcriptional outcomes of novel multigene perturbations with GEARS</article-title><source>Nature Biotechnology</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>August</month><fpage>1</fpage><lpage>9</lpage><comment>ISSN 1546-1696</comment><pub-id pub-id-type="pmcid">PMC11180609</pub-id><pub-id pub-id-type="pmid">37592036</pub-id><pub-id pub-id-type="doi">10.1038/s41587-023-01905-6</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>Y</given-names></name><name><surname>Roohani</surname><given-names>Y</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Samotorčan</surname><given-names>L</given-names></name><collab>Consortium, T S</collab><name><surname>Quake</surname><given-names>SR</given-names></name><name><surname>Leskovec</surname><given-names>J</given-names></name></person-group><article-title>Universal Cell Embeddings: A Foundation Model for Cell Biology</article-title><year>2023</year><month>November</month></element-citation></ref><ref id="R41"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Spielman</surname><given-names>DA</given-names></name><name><surname>Teng</surname><given-names>S-H</given-names></name></person-group><article-title>Spectral Sparsification of Graphs</article-title><year>2010</year><month>July</month></element-citation></ref><ref id="R42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Svensson</surname><given-names>V</given-names></name><name><surname>Vento-Tormo</surname><given-names>R</given-names></name><name><surname>Teichmann</surname><given-names>SA</given-names></name></person-group><article-title>Exponential scaling of single-cell RNA-seq in the past decade</article-title><source>Nature Protocols</source><publisher-name>Nature Publishing Group</publisher-name><year>2018</year><month>April</month><volume>13</volume><issue>4</issue><fpage>599</fpage><lpage>604</lpage><comment>ISSN 1750-2799</comment><pub-id pub-id-type="pmid">29494575</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenney</surname><given-names>I</given-names></name><name><surname>Xia</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Poliak</surname><given-names>A</given-names></name><name><surname>McCoy</surname><given-names>RT</given-names></name><name><surname>Kim</surname><given-names>N</given-names></name><name><surname>Durme</surname><given-names>BV</given-names></name><name><surname>Bowman</surname><given-names>SR</given-names></name><name><surname>Das</surname><given-names>D</given-names></name><name><surname>Pavlick</surname><given-names>E</given-names></name></person-group><article-title>What do you learn from context? Probing for sentence structure in contextualized word representations</article-title><source>arXiv:1905.06316 [cs]</source><year>2019</year><month>May</month></element-citation></ref><ref id="R44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Theodoris</surname><given-names>CV</given-names></name><name><surname>Xiao</surname><given-names>L</given-names></name><name><surname>Chopra</surname><given-names>A</given-names></name><name><surname>Chaffin</surname><given-names>MD</given-names></name><name><surname>Al Sayed</surname><given-names>ZR</given-names></name><name><surname>Hill</surname><given-names>MC</given-names></name><name><surname>Mantineo</surname><given-names>H</given-names></name><name><surname>Brydon</surname><given-names>EM</given-names></name><name><surname>Zeng</surname><given-names>Z</given-names></name><name><surname>Liu</surname><given-names>XS</given-names></name><name><surname>Ellinor</surname><given-names>PT</given-names></name></person-group><article-title>Transfer learning enables predictions in network biology</article-title><source>Nature</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>June</month><volume>618</volume><issue>7965</issue><fpage>616</fpage><lpage>624</lpage><comment>ISSN 1476-4687</comment><pub-id pub-id-type="pmcid">PMC10949956</pub-id><pub-id pub-id-type="pmid">37258680</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06139-9</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trapnell</surname><given-names>C</given-names></name></person-group><article-title>Defining cell types and states with single-cell genomics</article-title><source>Genome Research</source><publisher-name>Cold Spring Harbor Lab</publisher-name><year>2015</year><month>October</month><volume>25</volume><issue>10</issue><fpage>1491</fpage><lpage>1498</lpage><comment>ISSN 1088-9051, 1549-5469 Company: Cold Spring Harbor Laboratory Press Distributor: Cold Spring Harbor Laboratory Press Institution: Cold Spring Harbor Laboratory Press Label: Cold Spring Harbor Laboratory Press</comment><pub-id pub-id-type="pmcid">PMC4579334</pub-id><pub-id pub-id-type="pmid">26430159</pub-id><pub-id pub-id-type="doi">10.1101/gr.190595.115</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><chapter-title>Attention is All you Need</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2017</year><volume>30</volume></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Velez-Arce</surname><given-names>A</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>MM</given-names></name><name><surname>Lin</surname><given-names>X</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Fu</surname><given-names>T</given-names></name><name><surname>Kellis</surname><given-names>M</given-names></name><name><surname>Pentelute</surname><given-names>BL</given-names></name><name><surname>Zitnik</surname><given-names>M</given-names></name></person-group><article-title>TDC-2: Multimodal Foundation for Therapeutic Science</article-title><year>2024</year><month>June</month></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wen</surname><given-names>H</given-names></name><name><surname>Tang</surname><given-names>W</given-names></name><name><surname>Dai</surname><given-names>X</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Jin</surname><given-names>W</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Tang</surname><given-names>J</given-names></name></person-group><article-title>CellPLM: Pre-training of Cell Language Model Beyond Single Cells</article-title><year>2023</year><month>October</month></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wenteler</surname><given-names>A</given-names></name><name><surname>Cabrera</surname><given-names>CP</given-names></name><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Neduva</surname><given-names>V</given-names></name><name><surname>Barnes</surname><given-names>MR</given-names></name></person-group><article-title>AI approaches for the discovery and validation of drug targets</article-title><source>Cambridge Prisms: Precision Medicine</source><year>2024</year><month>January</month><volume>2</volume><comment>ISSN 2752-6143</comment><pub-id pub-id-type="pmcid">PMC11383977</pub-id><pub-id pub-id-type="pmid">39258224</pub-id><pub-id pub-id-type="doi">10.1017/pcm.2024.4</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>FA</given-names></name><name><surname>Angerer</surname><given-names>P</given-names></name><name><surname>Theis</surname><given-names>FJ</given-names></name></person-group><article-title>SCANPY: large-scale single-cell gene expression data analysis</article-title><source>Genome Biology</source><year>2018</year><month>February</month><volume>19</volume><issue>1</issue><fpage>15</fpage><comment>ISSN 1474-760X</comment><pub-id pub-id-type="pmcid">PMC5802054</pub-id><pub-id pub-id-type="pmid">29409532</pub-id><pub-id pub-id-type="doi">10.1186/s13059-017-1382-0</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Wershof</surname><given-names>E</given-names></name><name><surname>Schmon</surname><given-names>SM</given-names></name><name><surname>Nassar</surname><given-names>M</given-names></name><name><surname>Osinski</surname><given-names>B</given-names></name><name><surname>Eksi</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name></person-group><article-title>PerturBench: Benchmarking Machine Learning Models for Cellular Perturbation Analysis</article-title><year>2024</year><month>August</month></element-citation></ref><ref id="R52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Fang</surname><given-names>Y</given-names></name><name><surname>Tang</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name></person-group><article-title>scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data</article-title><source>Nature Machine Intelligence</source><publisher-name>Nature Publishing Group</publisher-name><year>2022</year><month>October</month><volume>4</volume><issue>10</issue><fpage>852</fpage><lpage>866</lpage><comment>ISSN 2522-5839</comment><pub-id pub-id-type="doi">10.1038/s42256-022-00534-z</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Impact Statement</title></caption><p>This paper aims to advance the field of machine learning by providing a standardized framework for benchmarking perturbation effect prediction models. Our work has the potential to improve the evaluation of computational models used in drug discovery and precision medicine by ensuring rigorous and reproducible assessment of their predictive capabilities. By highlighting the limitations of current zeroshot single-cell foundation models (scFMs) in perturbation effect prediction, we encourage future research to develop more robust and biologically informed architectures.</p><p>While our findings do not have immediate ethical concerns, the broader application of machine learning in biological and clinical settings requires careful consideration of biases in training data and the potential misinterpretation of model predictions. Ensuring that predictive models generalize across diverse biological contexts is crucial to avoiding misleading conclusions that could impact downstream biomedical applications. Future efforts should focus on increasing the diversity and representativeness of training datasets, as well as fostering transparency in model evaluation, to maximize the real-world utility of these approaches.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>PertEval-scFM framework (left to right) – data pre-processing, training of MLP probes under different sparsification conditions; evaluation of trained models with AUSPC, E-distance and contextual alignment metrics.</title></caption><graphic xlink:href="EMS199216-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Average AUSPC (↓) across sparsification probabilities for each model with standard error bars.</title><p>(a) Norman single-gene (left) and double-gene (right) perturbation (b) Replogle K562 (left) and RPE1 (right)</p></caption><graphic xlink:href="EMS199216-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>(a) MSEs for all test perturbations as a function of the E-distance. The predictions displayed are the averaged across all scFMs. (b) The E-distance of all test perturbations stratified per split as a function of the sparsification probability. The mean of the E-distance per split is included in red.</p></caption><graphic xlink:href="EMS199216-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Predictions of models across the top 20 DEGs for 4 perturbations from different splits. Subcaptions indicate perturbation name, sparsification probability. The predictions are included as colored dots, and the target perturbation effect is displayed as a dashed line.</p></caption><graphic xlink:href="EMS199216-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>MSE as a function of the pre-train and fine-tune data cross-split overlap for scGPT and scBERT.</title></caption><graphic xlink:href="EMS199216-f005"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Perturbation effect prediction evaluation across 2,000 HVGs. Models are listed in the specified order.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center" colspan="7" style="border-bottom: solid thin">↓ MSE (10<sup>−2</sup>)</th><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center"/></tr><tr><th valign="bottom" align="center">Dataset</th><th valign="bottom" align="center">Model</th><th valign="bottom" align="center"><italic>S</italic> 0.1</th><th valign="bottom" align="center"><italic>S</italic> 0.2</th><th valign="bottom" align="center"><italic>S</italic> 0.3</th><th valign="bottom" align="center"><italic>S</italic> 0.4</th><th valign="bottom" align="center"><italic>S</italic> 0.5</th><th valign="bottom" align="center"><italic>S</italic> 0.6</th><th valign="bottom" align="center"><italic>S</italic> 0.7</th><th valign="bottom" align="center">↓ AUSPC (10<sup>−2</sup>)</th><th valign="bottom" align="center">↑ ΔAUSPC (10<sup>−2</sup>)</th><th valign="bottom" align="center">Rank</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Norman single-gene</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.887 ± 0.202</td><td valign="top" align="center">0.937 ± 0.177</td><td valign="top" align="center">1.120 ± 0.167</td><td valign="top" align="center">1.693 ± 0.328</td><td valign="top" align="center">1.750 ± 0.401</td><td valign="top" align="center">1.0067 ± 0.427</td><td valign="top" align="center">1.000 ± 0.257</td><td valign="top" align="center">0.815 ± 0.039</td><td valign="top" align="center"><bold>3.7968</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP baseline</td><td valign="top" align="center">6.288 ± 0.282</td><td valign="top" align="center">6.410 ± 0.289</td><td valign="top" align="center">6.699 ± 0.705</td><td valign="top" align="center">6.453 ± 0.584</td><td valign="top" align="center">5.984 ± 0.458</td><td valign="top" align="center">6.502 ± 1.277</td><td valign="top" align="center">7.065 ± 1.022</td><td valign="top" align="center">4.484 ± 0.299</td><td valign="top" align="center"><bold>0.1280</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">6.177 ± 0.204</td><td valign="top" align="center">5.980 ± 0.621</td><td valign="top" align="center">6.497 ± 0.513</td><td valign="top" align="center">6.219 ± 0.308</td><td valign="top" align="center">6.659 ± 0.154</td><td valign="top" align="center">7.413 ± 1.038</td><td valign="top" align="center">8.430 ± 0.540</td><td valign="top" align="center">4.612 ± 0.317</td><td valign="top" align="center">-</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">6.257 ± 0.049</td><td valign="top" align="center">6.132 ± 0.622</td><td valign="top" align="center">6.565 ± 0.520</td><td valign="top" align="center">6.395 ± 0.300</td><td valign="top" align="center">6.550 ± 0.140</td><td valign="top" align="center">7.382 ± 1.155</td><td valign="top" align="center">8.525 ± 0.494</td><td valign="top" align="center">4.651 ± 0.309</td><td valign="top" align="center">-0.0396</td><td valign="top" align="center">8</td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">6.301 ± 0.316</td><td valign="top" align="center">6.341 ± 0.356</td><td valign="top" align="center">6.761 ± 0.765</td><td valign="top" align="center">6.363 ± 0.544</td><td valign="top" align="center">5.924 ± 0.418</td><td valign="top" align="center">6.451 ± 1.200</td><td valign="top" align="center">8.488 ± 0.558</td><td valign="top" align="center">4.537 ± 0.268</td><td valign="top" align="center">0.0748</td><td valign="top" align="center">4</td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">6.421 ± 0.317</td><td valign="top" align="center">6.366 ± 0.356</td><td valign="top" align="center">6.793 ± 0.764</td><td valign="top" align="center">6.440 ± 0.538</td><td valign="top" align="center">5.919 ± 0.417</td><td valign="top" align="center">6.705 ± 1.183</td><td valign="top" align="center">8.601 ± 0.537</td><td valign="top" align="center">4.594 ± 0.246</td><td valign="top" align="center">0.0179</td><td valign="top" align="center">5</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">6.237 ± 0.218</td><td valign="top" align="center">6.340 ± 0.608</td><td valign="top" align="center">6.765 ± 0.428</td><td valign="top" align="center">6.363 ± 0.345</td><td valign="top" align="center">5.926 ± 0.174</td><td valign="top" align="center">6.400 ± 1.144</td><td valign="top" align="center">8.506 ± 1.020</td><td valign="top" align="center">4.525 ± 0.255</td><td valign="top" align="center"><bold>0.0863</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">6.258 ± 0.311</td><td valign="top" align="center">6.132 ± 0.620</td><td valign="top" align="center">6.565 ± 0.514</td><td valign="top" align="center">6.387 ± 0.307</td><td valign="top" align="center">6.551 ± 0.155</td><td valign="top" align="center">7.370 ± 1.065</td><td valign="top" align="center">8.479 ± 0.601</td><td valign="top" align="center">4.647 ± 0.312</td><td valign="top" align="center">-0.0355</td><td valign="top" align="center">7</td></tr><tr style="border-top: solid thin"><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Norman double-gene</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.783 ± 0.044</td><td valign="top" align="center">0.960 ± 0.050</td><td valign="top" align="center">1.153 ± 0.049</td><td valign="top" align="center">1.230 ± 0.289</td><td valign="top" align="center">1.467 ± 0.351</td><td valign="top" align="center">1.223 ± 0.147</td><td valign="top" align="center">1.810 ± 0.287</td><td valign="top" align="center">0.808 ± 0.028</td><td valign="top" align="center"><bold>4.254</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP baseline</td><td valign="top" align="center">5.261 ± 0.100</td><td valign="top" align="center">5.913 ± 0.255</td><td valign="top" align="center">5.728 ± 0.402</td><td valign="top" align="center">6.635 ± 0.161</td><td valign="top" align="center">7.675 ± 0.953</td><td valign="top" align="center">6.050 ± 0.763</td><td valign="top" align="center">5.198 ± 0.593</td><td valign="top" align="center">4.253 ± 0.073</td><td valign="top" align="center"><bold>0.002</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">5.257 ± 0.102</td><td valign="top" align="center">5.910 ± 0.255</td><td valign="top" align="center">5.722 ± 0.401</td><td valign="top" align="center">6.644 ± 0.167</td><td valign="top" align="center">7.674 ± 0.962</td><td valign="top" align="center">6.071 ± 0.772</td><td valign="top" align="center">5.201 ± 0.594</td><td valign="top" align="center">4.255 ± 0.073</td><td valign="top" align="center"><bold>-</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">5.514 ± 0.067</td><td valign="top" align="center">6.145 ± 0.182</td><td valign="top" align="center">6.029 ± 0.458</td><td valign="top" align="center">6.742 ± 0.287</td><td valign="top" align="center">7.937 ± 1.187</td><td valign="top" align="center">7.246 ± 0.707</td><td valign="top" align="center">5.179 ± 0.630</td><td valign="top" align="center">4.503 ± 0.081</td><td valign="top" align="center">-0.248</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">5.515 ± 0.067</td><td valign="top" align="center">6.159 ± 0.196</td><td valign="top" align="center">6.022 ± 0.465</td><td valign="top" align="center">6.757 ± 0.281</td><td valign="top" align="center">7.999 ± 1.240</td><td valign="top" align="center">6.493 ± 0.736</td><td valign="top" align="center">7.110 ± 0.579</td><td valign="top" align="center">4.533 ± 0.081</td><td valign="top" align="center">-0.278</td><td valign="top" align="center">7</td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">5.564 ± 0.051</td><td valign="top" align="center">6.173 ± 0.196</td><td valign="top" align="center">6.050 ± 0.462</td><td valign="top" align="center">6.755 ± 0.279</td><td valign="top" align="center">7.944 ± 1.186</td><td valign="top" align="center">6.382 ± 0.876</td><td valign="top" align="center">5.238 ± 0.578</td><td valign="top" align="center">4.432 ± 0.467</td><td valign="top" align="center">-0.177</td><td valign="top" align="center">4</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">5.515 ± 0.067</td><td valign="top" align="center">6.153 ± 0.189</td><td valign="top" align="center">6.023 ± 0.464</td><td valign="top" align="center">6.766 ± 0.287</td><td valign="top" align="center">8.272 ± 1.377</td><td valign="top" align="center">8.826 ± 0.182</td><td valign="top" align="center">14.906 ± 2.154</td><td valign="top" align="center">5.184 ± 0.132</td><td valign="top" align="center">-0.929</td><td valign="top" align="center">8</td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">5.514 ± 0.066</td><td valign="top" align="center">6.145 ± 0.183</td><td valign="top" align="center">6.029 ± 0.460</td><td valign="top" align="center">6.736 ± 0.289</td><td valign="top" align="center">7.939 ± 1.184</td><td valign="top" align="center">6.352 ± 0.831</td><td valign="top" align="center">5.612 ± 0.665</td><td valign="top" align="center">4.435 ± 0.085</td><td valign="top" align="center">-0.180</td><td valign="top" align="center">5</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Perturbation effect prediction evaluation across 2,000 HVGs. Models are listed in the specified order.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center" colspan="7" style="border-bottom: solid thin">↓ MSE</th><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center"/></tr><tr><th valign="bottom" align="center">Dataset</th><th valign="bottom" align="center">Model</th><th valign="bottom" align="center"><italic>S</italic> 0.1</th><th valign="bottom" align="center"><italic>S</italic> 0.2</th><th valign="bottom" align="center"><italic>S</italic> 0.3</th><th valign="bottom" align="center"><italic>S</italic> 0.4</th><th valign="bottom" align="center"><italic>S</italic> 0.5</th><th valign="bottom" align="center"><italic>S</italic> 0.6</th><th valign="bottom" align="center"><italic>S</italic> 0.7</th><th valign="bottom" align="center">↓ AUSPC</th><th valign="bottom" align="center">↑ ΔAUSPC (10<sup>−2</sup>)</th><th valign="bottom" align="center">Rank</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Repologle K562</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.0096 ± 0.0005</td><td valign="top" align="center">0.0102 ± 0.0006</td><td valign="top" align="center">0.0129 ± 0.0003</td><td valign="top" align="center">0.0133 ± 0.0005</td><td valign="top" align="center">0.0150 ± 0.0003</td><td valign="top" align="center">0.0169 ± 0.0016</td><td valign="top" align="center">0.0175 ± 0.0009</td><td valign="top" align="center">0.0082 ± 0.0001</td><td valign="top" align="center"><bold>13.044</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP baseline</td><td valign="top" align="center">0.2125 ± 0.0007</td><td valign="top" align="center">0.2245 ± 0.0011</td><td valign="top" align="center">0.2343 ± 0.0055</td><td valign="top" align="center">0.2269 ± 0.0084</td><td valign="top" align="center">0.2437 ± 0.0151</td><td valign="top" align="center">0.2445 ± 0.0348</td><td valign="top" align="center">0.2799 ± 0.0279</td><td valign="top" align="center">0.1420 ± 0.0077</td><td valign="top" align="center">-0.3385</td><td valign="top" align="center">5</td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">0.2129 ± 0.0007</td><td valign="top" align="center">0.2272 ± 0.0017</td><td valign="top" align="center">0.2319 ± 0.0093</td><td valign="top" align="center">0.2252 ± 0.0089</td><td valign="top" align="center">0.2356 ± 0.0231</td><td valign="top" align="center">0.2435 ± 0.0267</td><td valign="top" align="center">0.2703 ± 0.0347</td><td valign="top" align="center">0.1386 ± 0.0076</td><td valign="top" align="center">-</td><td valign="top" align="center">4</td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">0.2274 ± 0.0028</td><td valign="top" align="center">0.2374 ± 0.0014</td><td valign="top" align="center">0.2515 ± 0.0008</td><td valign="top" align="center">0.2412 ± 0.0058</td><td valign="top" align="center">0.2506 ± 0.0047</td><td valign="top" align="center">0.2526 ± 0.0202</td><td valign="top" align="center">0.2639 ± 0.0330</td><td valign="top" align="center">0.1475 ± 0.0056</td><td valign="top" align="center">-0.8887</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">0.2071 ± 0.0014</td><td valign="top" align="center">0.2073 ± 0.0019</td><td valign="top" align="center">0.2050 ± 0.0041</td><td valign="top" align="center">0.2032 ± 0.0028</td><td valign="top" align="center">0.2064 ± 0.0035</td><td valign="top" align="center">0.2022 ± 0.0055</td><td valign="top" align="center">0.2012 ± 0.0004</td><td valign="top" align="center">0.1228 ± 0.0016</td><td valign="top" align="center">*</td><td valign="top" align="center">*</td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">0.2396 ± 0.0033</td><td valign="top" align="center">0.2576 ± 0.0109</td><td valign="top" align="center">0.2682 ± 0.0102</td><td valign="top" align="center">0.2644 ± 0.0153</td><td valign="top" align="center">0.2788 ± 0.0141</td><td valign="top" align="center">0.2506 ± 0.0278</td><td valign="top" align="center">0.2639 ± 0.0331</td><td valign="top" align="center">0.1556 ± 0.0113</td><td valign="top" align="center">-1.6977</td><td valign="top" align="center">7</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">0.2107 ± 0.0</td><td valign="top" align="center">0.2245 ± 0.0016</td><td valign="top" align="center">0.2287 ± 0.0086</td><td valign="top" align="center">0.2223 ± 0.0081</td><td valign="top" align="center">0.2321 ± 0.0213</td><td valign="top" align="center">0.2390 ± 0.0245</td><td valign="top" align="center">0.2635 ± 0.0327</td><td valign="top" align="center">0.1384 ± 0.0080</td><td valign="top" align="center"><bold>0.0221</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">0.2114 ± 0.0007</td><td valign="top" align="center">0.2245 ± 0.0016</td><td valign="top" align="center">0.2287 ± 0.0086</td><td valign="top" align="center">0.2223 ± 0.0081</td><td valign="top" align="center">0.2321 ± 0.0212</td><td valign="top" align="center">0.2391 ± 0.0245</td><td valign="top" align="center">0.2635 ± 0.0326</td><td valign="top" align="center">0.1384 ± 0.0080</td><td valign="top" align="center"><bold>0.0220</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr style="border-top: solid thin"><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Repologle RPE1</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.0154 ± 0.0004</td><td valign="top" align="center">0.0183 ± 0.0001</td><td valign="top" align="center">0.0217 ± 0.0006</td><td valign="top" align="center">0.0254 ± 0.0018</td><td valign="top" align="center">0.0285 ± 0.0026</td><td valign="top" align="center">0.0326 ± 0.0048</td><td valign="top" align="center">0.0454 ± 0.0077</td><td valign="top" align="center">0.0157 ± 0.0005</td><td valign="top" align="center"><bold>11.838</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP baseline</td><td valign="top" align="center">0.2068 ± 0.0023</td><td valign="top" align="center">0.2057 ± 0.0027</td><td valign="top" align="center">0.2045 ± 0.0011</td><td valign="top" align="center">0.2058 ± 0.0058</td><td valign="top" align="center">0.2101 ± 0.0053</td><td valign="top" align="center">0.2167 ± 0.0073</td><td valign="top" align="center">0.2112 ± 0.0042</td><td valign="top" align="center">0.1252 ± 0.0024</td><td valign="top" align="center"><bold>0.8892</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">0.2167 ± 0.0015</td><td valign="top" align="center">0.2215 ± 0.0027</td><td valign="top" align="center">0.2190 ± 0.0013</td><td valign="top" align="center">0.224 ± 0.0037</td><td valign="top" align="center">0.2203 ± 0.0122</td><td valign="top" align="center">0.2353 ± 0.0106</td><td valign="top" align="center">0.2246 ± 0.0101</td><td valign="top" align="center">0.1341 ± 0.0035</td><td valign="top" align="center">-</td><td valign="top" align="center">7</td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">0.2053 ± 0.0021</td><td valign="top" align="center">0.2056 ± 0.0023</td><td valign="top" align="center">0.2048 ± 0.0019</td><td valign="top" align="center">0.2062 ± 0.0055</td><td valign="top" align="center">0.2113 ± 0.0064</td><td valign="top" align="center">0.2170 ± 0.0066</td><td valign="top" align="center">0.2103 ± 0.0037</td><td valign="top" align="center">0.1251 ± 0.0023</td><td valign="top" align="center"><bold>0.8922</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">0.2050 ± 0.0029</td><td valign="top" align="center">0.2056 ± 0.0023</td><td valign="top" align="center">0.2047 ± 0.0019</td><td valign="top" align="center">0.2062 ± 0.0054</td><td valign="top" align="center">0.2113 ± 0.0059</td><td valign="top" align="center">0.2185 ± 0.0070</td><td valign="top" align="center">0.2178 ± 0.0068</td><td valign="top" align="center">0.1258 ± 0.0027</td><td valign="top" align="center">*</td><td valign="top" align="center">*</td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">0.2054 ± 0.0021</td><td valign="top" align="center">0.2056 ± 0.0024</td><td valign="top" align="center">0.2048 ± 0.0020</td><td valign="top" align="center">0.2063 ± 0.0056</td><td valign="top" align="center">0.2113 ± 0.0064</td><td valign="top" align="center">0.2173 ± 0.0068</td><td valign="top" align="center">0.2101 ± 0.0035</td><td valign="top" align="center">0.1253 ± 0.0023</td><td valign="top" align="center">0.8764</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">0.2053 ± 0.0021</td><td valign="top" align="center">0.2056 ± 0.0023</td><td valign="top" align="center">0.2047 ± 0.0019</td><td valign="top" align="center">0.2062 ± 0.0055</td><td valign="top" align="center">0.2112 ± 0.0065</td><td valign="top" align="center">0.2171 ± 0.0067</td><td valign="top" align="center">0.2103 ± 0.0038</td><td valign="top" align="center">0.1253 ± 0.0023</td><td valign="top" align="center">0.8768</td><td valign="top" align="center">5</td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">0.2053 ± 0.0021</td><td valign="top" align="center">0.2056 ± 0.0023</td><td valign="top" align="center">0.2048 ± 0.0019</td><td valign="top" align="center">0.2062 ± 0.0054</td><td valign="top" align="center">0.2113 ± 0.0064</td><td valign="top" align="center">0.2170 ± 0.0066</td><td valign="top" align="center">0.2104 ± 0.0037</td><td valign="top" align="center">0.1253 ± 0.0024</td><td valign="top" align="center">0.8791</td><td valign="top" align="center">4</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>Perturbation effect prediction evaluation across the top 20 DEGs per perturbation.</title><p>Note that for double-gene perturbations split 0.5, there were not enough perturbations that passed our quality control to define multiple replicates.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center" colspan="7" style="border-bottom: solid thin">                                                       ↓ MSE</th><th valign="bottom" align="center"/><th valign="bottom" align="center"/><th valign="bottom" align="center"/></tr><tr><th valign="bottom" align="center">Dataset</th><th valign="bottom" align="center">Model</th><th valign="bottom" align="center"><italic>S</italic> 0.1</th><th valign="bottom" align="center"><italic>S</italic> 0.2</th><th valign="bottom" align="center"><italic>S</italic> 0.3</th><th valign="bottom" align="center"><italic>S</italic> 0.4</th><th valign="bottom" align="center"><italic>S</italic> 0.5</th><th valign="bottom" align="center"><italic>S</italic> 0.6</th><th valign="bottom" align="center"><italic>S</italic> 0.7</th><th valign="bottom" align="center">↓ AUSPC</th><th valign="bottom" align="center">↑ ΔAUSPC (10<sup>−2</sup>)</th><th valign="bottom" align="center">Rank</th></tr></thead><tbody><tr><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Norman single-gene</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.284 ± 0.024</td><td valign="top" align="center">0.215 ± 0.037</td><td valign="top" align="center">0.314 ± 0.071</td><td valign="top" align="center">0.256 ± 0.035</td><td valign="top" align="center">0.341 ± 0.014</td><td valign="top" align="center">0.682 ± 0.194</td><td valign="top" align="center">0.888 ± 0.285</td><td valign="top" align="center">0.266 ± 0.018</td><td valign="top" align="center"><bold>7.967</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP gene expression</td><td valign="top" align="center">0.466 ± 0.051</td><td valign="top" align="center">0.468 ± 0.074</td><td valign="top" align="center">0.456 ± 0.039</td><td valign="top" align="center">0.497 ± 0.042</td><td valign="top" align="center">0.521 ± 0.071</td><td valign="top" align="center">0.513 ± 0.123</td><td valign="top" align="center">0.622 ± 0.172</td><td valign="top" align="center">0.342 ± 0.013</td><td valign="top" align="center">0.312</td><td valign="top" align="center">4</td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">0.479 ± 0.050</td><td valign="top" align="center">0.474 ± 0.078</td><td valign="top" align="center">0.489 ± 0.053</td><td valign="top" align="center">0.492 ± 0.047</td><td valign="top" align="center">0.492 ± 0.047</td><td valign="top" align="center">0.525 ± 0.126</td><td valign="top" align="center">0.604 ± 0.144</td><td valign="top" align="center">0.345 ± 0.011</td><td valign="top" align="center">-</td><td valign="top" align="center">5</td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">0.464 ± 0.048</td><td valign="top" align="center">0.464 ± 0.077</td><td valign="top" align="center">0.481 ± 0.052</td><td valign="top" align="center">0.475 ± 0.042</td><td valign="top" align="center">0.483 ± 0.046</td><td valign="top" align="center">0.488 ± 0.106</td><td valign="top" align="center">0.902 ± 0.220</td><td valign="top" align="center">0.351 ± 0.014</td><td valign="top" align="center">-0.564</td><td valign="top" align="center">8</td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">0.469 ± 0.050</td><td valign="top" align="center">0.464 ± 0.077</td><td valign="top" align="center">0.481 ± 0.053</td><td valign="top" align="center">0.475 ± 0.042</td><td valign="top" align="center">0.482 ± 0.045</td><td valign="top" align="center">0.499 ± 0.117</td><td valign="top" align="center">0.608 ± 0.149</td><td valign="top" align="center">0.336 ± 0.011</td><td valign="top" align="center"><bold>0.878</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">0.502 ± 0.052</td><td valign="top" align="center">0.466 ± 0.077</td><td valign="top" align="center">0.489 ± 0.056</td><td valign="top" align="center">0.469 ± 0.040</td><td valign="top" align="center">0.486 ± 0.046</td><td valign="top" align="center">0.567 ± 0.090</td><td valign="top" align="center">0.638 ± 0.166</td><td valign="top" align="center">0.350 ± 0.011</td><td valign="top" align="center">-0.486</td><td valign="top" align="center">7</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">0.463 ± 0.048</td><td valign="top" align="center">0.464 ± 0.077</td><td valign="top" align="center">0.482 ± 0.053</td><td valign="top" align="center">0.475 ± 0.042</td><td valign="top" align="center">0.484 ± 0.047</td><td valign="top" align="center">0.485 ± 0.105</td><td valign="top" align="center">0.828 ± 0.249</td><td valign="top" align="center">0.347 ± 0.015</td><td valign="top" align="center">-0.168</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">0.463 ± 0.048</td><td valign="top" align="center">0.464 ± 0.077</td><td valign="top" align="center">0.482 ± 0.053</td><td valign="top" align="center">0.476 ± 0.042</td><td valign="top" align="center">0.485 ± 0.047</td><td valign="top" align="center">0.484 ± 0.104</td><td valign="top" align="center">0.624 ± 0.162</td><td valign="top" align="center">0.334 ± 0.012</td><td valign="top" align="center"><bold>1.078</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr style="border-top: solid thin"><td valign="middle" align="center" rowspan="8" style="writing-mode:tb-rl;writing-mode:vertical-rl;"><bold>Norman double-gene</bold></td><td valign="top" align="center">GEARS</td><td valign="top" align="center">0.211 ± 0.032</td><td valign="top" align="center">0.200 ± 0.013</td><td valign="top" align="center">0.296 ± 0.052</td><td valign="top" align="center">0.425 ± 0.041</td><td valign="top" align="center">0.335 ± 0.0*</td><td valign="top" align="center">0.473 ± 0.109</td><td valign="top" align="center">0.422 ± 0.077</td><td valign="top" align="center">0.223 ± 0.010</td><td valign="top" align="center"><bold>29.9</bold></td><td valign="top" align="center"><bold>1</bold></td></tr><tr><td valign="top" align="center">MLP gene expression</td><td valign="top" align="center">0.484 ± 0.046</td><td valign="top" align="center">0.538 ± 0.082</td><td valign="top" align="center">0.585 ± 0.061</td><td valign="top" align="center">0.618 ± 0.048</td><td valign="top" align="center">0.690 ± 0.0*</td><td valign="top" align="center">0.552 ± 0.049</td><td valign="top" align="center">0.500 ± 0.056</td><td valign="top" align="center">0.371 ± 0.009</td><td valign="top" align="center"><bold>15.1</bold></td><td valign="top" align="center"><bold>2</bold></td></tr><tr><td valign="top" align="center">Mean baseline</td><td valign="top" align="center">0.549 ± 0.055</td><td valign="top" align="center">0.580 ± 0.075</td><td valign="top" align="center">0.615 ± 0.074</td><td valign="top" align="center">0.653 ± 0.037</td><td valign="top" align="center">0.757 ± 0.0*</td><td valign="top" align="center">0.659 ± 0.047</td><td valign="top" align="center">0.497 ± 0.056</td><td valign="top" align="center">0.522 ± 0.053</td><td valign="top" align="center">-</td><td valign="top" align="center">8</td></tr><tr><td valign="top" align="center">Geneformer</td><td valign="top" align="center">0.527 ± 0.055</td><td valign="top" align="center">0.550 ± 0.069</td><td valign="top" align="center">0.603 ± 0.076</td><td valign="top" align="center">0.661 ± 0.045</td><td valign="top" align="center">0.706 ± 0.0*</td><td valign="top" align="center">0.623 ± 0.054</td><td valign="top" align="center">0.487 ± 0.048</td><td valign="top" align="center">0.409 ± 0.008</td><td valign="top" align="center"><bold>11.3</bold></td><td valign="top" align="center"><bold>3</bold></td></tr><tr><td valign="top" align="center">scBERT</td><td valign="top" align="center">0.528 ± 0.056</td><td valign="top" align="center">0.550 ± 0.069</td><td valign="top" align="center">0.596 ± 0.071</td><td valign="top" align="center">0.661 ± 0.041</td><td valign="top" align="center">0.740 ± 0.0*</td><td valign="top" align="center">0.622 ± 0.049</td><td valign="top" align="center">0.681 ± 0.086</td><td valign="top" align="center">0.418 ± 0.008</td><td valign="top" align="center">10.4</td><td valign="top" align="center">6</td></tr><tr><td valign="top" align="center">scFoundation</td><td valign="top" align="center">0.534 ± 0.057</td><td valign="top" align="center">0.554 ± 0.070</td><td valign="top" align="center">0.606 ± 0.073</td><td valign="top" align="center">0.656 ± 0.045</td><td valign="top" align="center">0.683 ± 0.0*</td><td valign="top" align="center">0.621 ± 0.060</td><td valign="top" align="center">0.497 ± 0.051</td><td valign="top" align="center">0.410 ± 0.008</td><td valign="top" align="center">11.2</td><td valign="top" align="center">5</td></tr><tr><td valign="top" align="center">scGPT</td><td valign="top" align="center">0.527 ± 0.056</td><td valign="top" align="center">0.550 ± 0.069</td><td valign="top" align="center">0.597 ± 0.072</td><td valign="top" align="center">0.673 ± 0.044</td><td valign="top" align="center">0.724 ± 0.0*</td><td valign="top" align="center">0.724 ± 0.028</td><td valign="top" align="center">1.941 ± 0.329</td><td valign="top" align="center">0.500 ± 0.018</td><td valign="top" align="center">2.2</td><td valign="top" align="center">7</td></tr><tr><td valign="top" align="center">UCE</td><td valign="top" align="center">0.527 ± 0.055</td><td valign="top" align="center">0.550 ± 0.069</td><td valign="top" align="center">0.601 ± 0.072</td><td valign="top" align="center">0.656 ± 0.043</td><td valign="top" align="center">0.726 ± 0.0*</td><td valign="top" align="center">0.624 ± 0.053</td><td valign="top" align="center">0.506 ± 0.048</td><td valign="top" align="center">0.410 ± 0.007</td><td valign="top" align="center">11.2</td><td valign="top" align="center">4</td></tr></tbody></table></table-wrap></floats-group></article>