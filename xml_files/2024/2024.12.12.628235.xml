<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202261</article-id><article-id pub-id-type="doi">10.1101/2024.12.12.628235</article-id><article-id pub-id-type="archive">PPR956858</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Plasmo3Net: A Convolutional Neural Network-Based Algorithm for Detecting Malaria Parasites in Thin Blood Smear Images</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Owoloye</surname><given-names>Afolabi J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ligali</surname><given-names>Funmilayo C.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Enejoh</surname><given-names>Ojochenemi A.</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Agosile</surname><given-names>Oluwafemi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Musa</surname><given-names>Adesola Z.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Aina</surname><given-names>Oluwagbemiga</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Idowu</surname><given-names>Emmanuel T.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Oyebola</surname><given-names>Kolapo M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Center for Genomic Research in Biomedicine (CeGRIB), College of Basic and Applied Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00effsg46</institution-id><institution>Mountain Top University</institution></institution-wrap>, <addr-line>Kilometer 12, Lagos-Ibadan Expressway</addr-line>, <country country="NG">Nigeria</country></aff><aff id="A2"><label>2</label>Parasitology and Bioinformatics Unit, Department of Zoology, Faculty of Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rk03822</institution-id><institution>University of Lagos</institution></institution-wrap>, <city>Lagos</city>, <country country="NG">Nigeria</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03kk9k137</institution-id><institution>Nigerian Institute of Medical Research</institution></institution-wrap>, <city>Lagos</city>, <country country="NG">Nigeria</country></aff><aff id="A4"><label>4</label>Biochemistry Department, Faculty of Basic Medical Science, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05rk03822</institution-id><institution>University of Lagos</institution></institution-wrap>, <city>Lagos</city>, <country country="NG">Nigeria</country></aff><aff id="A5"><label>5</label>Genetics, Genomics and Bioinformatics Department, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00czh4843</institution-id><institution>National Biotechnology Research and Development Agency</institution></institution-wrap>, <city>Abuja</city>, <country country="NG">Nigeria</country></aff><author-notes><corresp id="CR1">
<label>*</label><bold>Corresponding author</bold> <email>oyebolakolapo@yahoo.com</email>; <email>kmoyebola@mtu.edu.ng</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Early diagnosis of malaria is crucial for effective control and elimination efforts. Microscopy is a reliable field-adaptable malaria diagnostic method. However, microscopy results are only as good as the quality of slides and images obtained from thick and thin smears. In this study, we developed deep learning algorithms to identify malaria-infected red blood cells (RBCs) in thin blood smears. Three algorithms were developed based on a convolutional neural network (CNN). The CNN was trained on 15,060 images and evaluated using 4,000 images. After a series of fine-tuning and hyperparameter optimization experiments, we selected the top-performing algorithm, which was named Plasmo3Net. The Plasmo3Net architecture was made up of 13 layers: three convolutional, three max-pooling, one flatten, four dropouts, and two fully connected layers, to obtain an accuracy of 99.3%, precision of 99.1%, recall of 99.6%, and F1 score of 99.3%. The maximum training accuracy of 99.5% and validation accuracy of 97.7% were obtained during the learning phase. Four pre-trained deep learning models (InceptionV3, VGG16, ResNet50, and ALexNet) were selected and trained alongside our model as baseline techniques for comparison due to their performance in malaria parasite identification. The topmost transfer learning model was the ResNet50 with 97.9% accuracy, 97.6% precision, 98.3 % recall, and 97.9% F1 score. The accuracy of the Plasmo3Net in malaria parasite identification highlights its potential for automated malaria diagnosis in the future. With additional validation using more extensive and diverse datasets, Plasmo3Net could evolve into a diagnostic workflow suitable for field applications.</p></abstract><kwd-group><kwd>Malaria</kwd><kwd>Deep Learning</kwd><kwd>Convolutional Neural Network</kwd><kwd>Plasmo3Net</kwd><kwd>Diagnostic Workflow</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1.0</label><title>Introduction</title><p id="P2">Malaria is a deadly disease caused by parasites transmitted to humans through the bite of infected female <italic>Anopheles</italic> mosquitoes. Early diagnosis of the disease is crucial to decreasing the number of malaria victims. Microscopy is considered the gold standard for malaria diagnosis and quantification [<xref ref-type="bibr" rid="R1">1</xref>]. However, malaria diagnosis can be enhanced by utilizing digital image processing techniques to reliably detect parasites in images of red blood cells (RBCs) from microscope slides [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>]. The advent of computer vision and convolutional neural networks (CNNs) has enabled successful image classification tasks [<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R6">6</xref>].</p><p id="P3">The connectivity patterns between neurons in a CNN mirrors the organization of the visual cortex, where individual cortical neurons respond to stimuli only within a restricted receptive field of the visual field [<xref ref-type="bibr" rid="R7">7</xref>]. This biological principle inspired the hierarchical structure of learning layers in a CNN model [<xref ref-type="bibr" rid="R8">8</xref>]. CNNs effectively utilize the spatial correlations of visual patterns, such as edges in images, to reduce the number of training parameters [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>]. This improves the accuracy of the feedforward-backpropagation training procedure. As such, CNNs provide a general-purpose learning framework that eliminates the need for substantial feature extraction and fine-tuning. This advantage over traditional classification methods allows deep learning models to effectively capture highly complex data patterns [<xref ref-type="bibr" rid="R10">10</xref>].</p><p id="P4">The early applications of Convolutional Neural Networks (CNNs) date back to the 1990s for tasks such as speech recognition [<xref ref-type="bibr" rid="R11">11</xref>] and text recognition [<xref ref-type="bibr" rid="R12">12</xref>], and later expanded to handwriting recognition [<xref ref-type="bibr" rid="R13">13</xref>] and natural image recognition [<xref ref-type="bibr" rid="R14">14</xref>]. With the advent of ImageNet [<xref ref-type="bibr" rid="R15">15</xref>], CNN's image classification performance received significant recognition. AlexNet also achieved a top-five error rate of 15.3% on the ILSVRC-2012 competition, which had 10,184 categories and 8.9 million images [<xref ref-type="bibr" rid="R15">15</xref>]. Subsequent improvements steadily reduced the top-five error rate, from 14.8% by ZFNet in 2013 [<xref ref-type="bibr" rid="R16">16</xref>] to 6.7% by GoogLeNet in 2014 [<xref ref-type="bibr" rid="R17">17</xref>] and finally to 3.6% by ResNet50 in 2015 [<xref ref-type="bibr" rid="R5">5</xref>]. These advancements have demonstrated the powerful capabilities of CNNs for large-scale image recognition tasks.</p><p id="P5">Previous approaches for computer vision-based classification of <italic>Plasmodium</italic>-infected and uninfected RBCs have mostly focused on archived image sets [<xref ref-type="bibr" rid="R18">18</xref>]. While the outcomes reported in these studies have been promising, it is crucial to demonstrate the robustness and performance of these methods on field datasets. The existing deep-learning models also have considerable room for improvement in this area. In this study, we explored a customized deep-learning algorithm using a convolutional neural network (CNN) architecture and evaluated its robustness and performance metrics on a dataset of blood smear images of <italic>P. falciparum</italic> isolates from Nigeria.</p></sec><sec id="S2" sec-type="methods"><label>2.0</label><title>Methodology</title><p id="P6">The dataset used in this study included images of Giemsa-stained slides of thin blood smears obtained from 48 <italic>Plasmodium falciparum</italic>-infected and 23 healthy individuals during a previous antimalarial therapeutic efficacy study [<xref ref-type="bibr" rid="R19">19</xref>]. Images were digitized using a light microscope x100 resolution. An iPhone 14 with a high-resolution digital camera (4032 x 3024 pixels) was used to capture each field. The experimental workflow included dataset acquisition, image preprocessing, image segmentation, data augmentation, and building a deep learning model based on convolutional neural network (CNN) architecture (<xref ref-type="fig" rid="F1">Figure 1</xref>). To determine the performance of the model on unseen data, we downloaded <italic>P. falciparum</italic> microscope slide images from the National Institutes of Health (NIH) [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. The dataset contained two classes - parasitized RBC images and uninfected RBC image cell images. Each class had 13,794 images. When analyzing the dataset, we performed manual curation to enhance accurate sample labelling. We used 12,200 manually curated images of the parasitized and uninfected classes. Moreover, to check the performance of our model, several transfer learning models such as AlexNet, ResNet50, Inception V3, and VGG16 were trained and evaluated for comparison.</p><sec id="S3"><label>2.1</label><title>Image preprocessing</title><sec id="S4"><label>2.1.1</label><title>Cropping</title><p id="P7">We cropped specific regions of interest within images (we excluded the dark area of the slide image). The coordinates of the crop are contained in four specified values: X, Y, width, and height. The X or the horizontal coordinate of the crop region was the left side of the image where the crop started. The vertical coordinate of the crop region, the y-coordinate, was defined as the upper edge from which cropping begins. The specified width and height parameters specified the rectangular crop area, or the extent of the complete area for extraction 2450 x 2450 pixels.</p></sec><sec id="S5"><label>2.1.2</label><title>Noise reduction</title><p id="P8">A bilateral filter was used to remove noise from digital images while preserving edges (<xref ref-type="fig" rid="F2">Figure 2</xref>). The bilateral filter is a non-linear technique to preserve edges by applying less smoothing to pixels that are different in color from the central pixel while applying more smoothing to similar pixels [<xref ref-type="bibr" rid="R21">21</xref>]. We applied the bilateral filter to the input image using the "cv2.bilateralFilter()" function provided by the OpenCV library in Python. Subsequently, a filtering neighborhood diameter of nine pixels, a sigma value of 75 in the color space, and a sigma value of 75 in the coordinate space were configured. These parameter values were determined to effectively balance noise reduction and edge preservation for the image dataset.</p></sec><sec id="S6"><label>2.1.3</label><title>Gamma correction</title><p id="P9">Gamma correction was applied to enhance the brightness and contrast of the digital images. Gamma correction selectively modified the luminance information of the images while preserving the original hue and saturation values. The images were then converted from the default BGR color space to the hue, saturation, value (HSV) color space. Subsequently, we determined the desired "mid-tone" brightness of 0.8 and calculated the gamma value based on the channel's mean pixel value (V). The gamma value was applied to the V channel of the HSV images. The V channel was then merged with the HSV channels and converted back to the BGR color space. The flexibility to adjust the gamma value based on the image characteristics allows for further optimization and fine-tuning of the desired visual outcomes.</p></sec><sec id="S7"><label>2.1.4</label><title>Color correction (white balancing)</title><p id="P10">Uninfected RBCs occasionally contain artefacts or remains of other types that a deep learning classifier could incorrectly identify as <italic>Plasmodium</italic> parasites [<xref ref-type="bibr" rid="R22">22</xref>]. Thus, color normalization was required before classification. The white balancing operation reduced the image's dynamic range while enhancing the overall contrast and brightness to achieve a better visual quality of content. It was used to improve the visual quality and analyze digital images in different image processing applications (such as computer vision and remote sensing).</p><p id="P11">White balance was achieved by multiplying the input image by a scaling factor of 1.2 full-size image. The scaling factor of 1.2 used in this analysis was an optimal value obtained because of the characteristics of the input image and the desired visual outcome. This was intended to increase the overall brightness of the image. The scaled image was divided by the maximum value along the height and width dimensions of the original image.</p></sec></sec><sec id="S8"><label>2.2</label><title>Image Segmentation</title><p id="P12">The segmentation stage involved the separation of the foreground (RBCs, WBCs, and other blood components) against the background by determining the threshold value that divided the image into two groups based on intensity [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R23">23</xref>]. The images were then converted from the BGR colour space to grayscale to simplify the segmentation process. We then transformed the image into a binary image preprocessing [<xref ref-type="bibr" rid="R24">24</xref>]. The Otsu approach automatically selected a threshold level that optimized the variation between the two histogram classes of the grayscale images. This step resulted in a binary image that highlighted the segmented RBCs. The segmented cells' contours were detected using the "<italic>findContours()</italic>" function. The Sobel edge detection algorithm was applied to compute the gradients of the input images in the horizontal and vertical directions [<xref ref-type="bibr" rid="R25">25</xref>]. Subsequently, the gradients were combined to form a single-edge map highlighting the images' edges and boundaries. To enhance the effectiveness of segmentation techniques and leverage the potential of color picture segmentation, we adopted the procedure described by Diaz <italic>et al.</italic> [<xref ref-type="bibr" rid="R22">22</xref>] concentrated on identifying malaria by applying the moving algorithm k-means clustering in conjunction with the hue, saturation, and intensity (HSI) colour space to obtain full segmentation of RBC (both infected and uninfected).</p></sec><sec id="S9"><label>2.3</label><title>Data augmentation and splitting</title><p id="P13">Data augmentation was applied to the training dataset to enhance accuracy [<xref ref-type="bibr" rid="R26">26</xref>]. This generated 14,980 images (7,490 each for infected and uninfected RBC) after which 4080 images were selected and augmented by random rotation, random zooming, vertical flipping, and horizontal flipping, making a total of 19060 images. The test dataset comprised 4,000 randomly selected images (2,000 each for infected and uninfected), while the remaining 15,060 images (7,530 each for infected and uninfected) were split (80:20) into training and validation datasets.</p></sec><sec id="S10"><label>2.4</label><title>Plasmo3Net Architecture</title><p id="P14">A deep learning algorithm was developed using CNN architecture with 13 layers: three convolutional, three max-pooling, one flatten, four dropouts (5% each), and two fully connected layers (<xref ref-type="fig" rid="F3">Figure 3</xref>). Rectified Linear Unit (ReLU) activation function was used for this configuration. An input image size of 64 × 64 pixels with three channels was sufficient to capture the neighborhood context for a final decision. The initial conv2D layer had 32 filters and a kernel size of 3 × 3. Meanwhile, the subsequent convolutional layer filters were 64, 128, and 128. Following each Conv2D layer, a MaxPool2D layer with a pool size of (2, 2) was applied to down-sample the feature maps. A dropout layer of 5% was then added after the MaxPool2D layer to prevent overfitting. The dropout layer discarded 5% of the neurons. A flatten layer was introduced to convert the 2D output from the previous convolutional layers into a 1D tensor. Following the flatten layer, a fully connected layer with 128 units and ReLU activation was incorporated. A dropout layer with a rate of 5% was included to prevent further overfitting. The final layer of the model produced the classification output, a fully connected layer with 1 unit and a sigmoid activation function. This layer provided a probability for binary classification. The binary cross-entropy loss function was used to calculate the error between predicted actual and predicted output. The model was then compiled using the Adam optimizer [<xref ref-type="bibr" rid="R27">27</xref>], an efficient variant of stochastic gradient descent. Furthermore, the model performance was evaluated using the performance metrics. Moreover, two other architectures were developed; a 16-layer model consisting of four convolutional layers, four max-pooling layers, one flatten, five dropouts, and two fully connected layers, and a 19-layer architecture consisting of five convolutional, five max-pooling, one flatten, six dropouts, and two fully connected layers. The two models were also trained, finetuned, and optimized alongside the Plasmo3Net. However, the best CNN architecture (PlasmoNet) was chosen after experimenting with and fine-tuning the hyperparameters. Models were trained using a Dell Alienware Aurora R13 with Nvidia GeForce 3090, 12th Gen intel Core™ i9, 64 GB DDR5 ram, and 24 threads running on Ubuntu 20.04.6 LTS 64-bit OS.</p></sec><sec id="S11"><label>2.5</label><title>Transfer learning</title><p id="P15">Four pre-trained deep learning models - InceptionV3, VGG16, ResNet50, and AlexNet – were used to validate Plasmo3Net. The pre-trained and proposed Plasmo3Net models had the same dataset of 15,060 images (7,530 parasitized and 7,530 uninfected cell images). For binary classification, all models' final layers were modified to a dense layer with a sigmoid activation function rather than a softmax. The Visual Geometry Group (VGG16) architecture [<xref ref-type="bibr" rid="R28">28</xref>] contained 16 layers of convolution network and three fully connected layers with 138 million trainable parameters. The network received an image with a fixed size of (125x125); hence, the matrix's shape was 125,125,3. The kernel had a stride of one pixel and a size of 3x3. To improve the model's classification, a max pooling was applied over a 2x2 pixel window with a stride size of two, and then a rectified linear unit (ReLU) was used. The final layer has a softmax activation function changed to sigmoid as the model was meant for binary classification. The AlexNet pre-trained architecture comprises five convolutional layers and three fully connected layers [<xref ref-type="bibr" rid="R15">15</xref>]. The first convolutional layer has 96 kernels of size (11, 11) with a stride of 4, followed by an overlapping max-pooling layer with a kernel size of 3x3 and stride two. The overlapping occurred because the stride size was lesser than the kernel size. Max-pooling was adopted to preserve the depth of the image array while reducing its height and width. There were two more max-pooling layers and four convolutional layers. In addition, there were two fully connected layers with 4096 units each. The Softmax function was changed to sigmoid as the model was meant for a binary classification task. The ReLU function was applied to speed up the model after every layer (fully connected and convolutional). The Residual Network (ResNet50) architecture had 50 layers (48 convolutional layers, 1 average-pooling layer, and 1 max-pooling layer) with 23.5 million trainable parameters and 53 thousand non-trainable parameters [<xref ref-type="bibr" rid="R5">5</xref>]. ResNet50 addressed deep neural network vanishing and gradient problems using shortcuts or skip connections to create residual blocks. It comprised five steps, each with a convolutional and identity block (each convolutional block has three convolutional layers, and each identity block has three convolutional layers). The core building block of ResNet50 was the residual block, which allowed the network to learn the residual mapping between the input and output of the block. Convolutional layers, inception modules, auxiliary classifiers, and fully connected layers with 23.8 million trainable parameters constituted the Inception v3 architecture [<xref ref-type="bibr" rid="R29">29</xref>]. The inception v3 network began with a convolutional layer followed by a max-pooling layer that hosted several inception modules, at the heart of the architecture. Each inception module consisted of many parallel convolutional layers with different filter sizes. Concatenating the outputs of these parallel convolutions allowed features of various scales to be learned simultaneously by the network. The Inception modules also utilized 1x1 convolutions to reduce the input channel number, thus decreasing computational complexity. ReLU was the primary activation function used by convolutional layers, inception modules, and fully connected layers. A softmax activation function was employed in the final layer of the network to output class probabilities for classification tasks. Nevertheless, it was replaced with sigmoid when binary classification was carried out. We trained all the models with a batch size of 64 for 25 epochs. The model performance during the training and validation process was evaluated using the accuracy and loss.</p></sec></sec><sec id="S12" sec-type="results"><label>3.0</label><title>Results</title><p id="P16">The accuracy and loss curve for training and validation was plotted for each model to understand the rate of improvement in the model performance epoch (<xref ref-type="fig" rid="F4">Figure 4</xref>). The Plasmo3Net had the least trainable parameters with zero non-trainable parameters (<xref ref-type="table" rid="T1">Table 1</xref>), making it train faster than the other two CNN models and the four pre-trained architectures. The Inception V3 had the highest number of trainable parameters (23.8 million). ResNet50, AlexNet, and VGG16 had total trainable parameters of 23.5 million, 21.5 million, and 15.2 million, respectively. In contrast, the Plasmo3Net has a total number of 683,329 trainable parameters.</p><p id="P17">Comparing the Plasmo3Net with other transfer-learning models, the Plasmo3Net had the best training and validation accuracy (<xref ref-type="fig" rid="F4">Figures 4</xref> and <xref ref-type="fig" rid="F5">5</xref>). The discrepancy between the validation and training accuracy indicated that our model had not been over-fitted. By experimenting with various batch sizes and number of epochs, we discovered a trade-off between training speed and the convergence behavior of the model. We achieved a stable gradient estimate through optimization batch size (64). For the least cost function to converge smoothly, this stability was necessary. Consistent updates to the model parameters were made possible by the less noisy gradients produced by the adjusted batch size.</p><p id="P18">The maximum training accuracy of 99.5% and validation accuracy of 97.7% were obtained during the learning phase of the Plasmo3Net. Less variation exists between training and validation accuracy, indicating that the model could not have been over-fitted. The AlexNet architecture reached a maximum training accuracy of 99.8% and validation accuracy of 91.9%, however, the validation loss had an average of 65%. The training and validation loss of the Plasmo3Net decreased steadily from the first epoch to the last epoch during the training process, while in the transfer learning models, the validation either fluctuated or increased.</p><p id="P19">The Plasmo3Net and the transfer learning models were evaluated during the training phase by making predictions and fitting the model to the unexposed data from the test dataset. Finally, the optimal model was implemented on the test dataset (<xref ref-type="fig" rid="F6">Figure 6</xref>). Their performance metrics examined the model potential with parameters like accuracy, F1 score, recall, precision, sensitivity, and specificity.</p><p id="P20">The Plasmo3Net comparative study used the four pre-trained models, ResNet50, VGG16, Inception-V3, and AlexNet (<xref ref-type="table" rid="T2">Table 2</xref>; <xref ref-type="fig" rid="F4">Figures 4</xref> and <xref ref-type="fig" rid="F5">5</xref>). Of the three CNN architectures developed, the 13-layer model with three convolutional layers gave the best metrics (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>). The two other CNN models were a 16-layer architecture with four convolutional layers and a 19-layer architecture with five convolutional layers. It was observed that the 16-layer performed better than the 19-layer architecture (<xref ref-type="table" rid="T2">Table 2</xref>). The accuracy, precision, F1 score, and specificity of the 16-layer model were 97.7%, 97.8%, 97.7%, and 97.9%, respectively, compared to the 19-layer architecture metrics: 97.3%, 95.9%, 97.4%, 95.8%, respectively. However, the 19-layer architecture’s recall (98.9%) was higher than that of the 16-layer architecture (97.6%)</p><p id="P21">The Plasmo3Net had the best performance considering the accuracy, F1 score, recall, and precision (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figures 2 and 3</xref>). The Plasmo3Net gave the best result among all other models, with a test accuracy of 99.3%. In terms of precision, the VGG-16 architecture performance (98.6%) was slightly higher than the ResNet50 (97.6%). However, the ResNet architecture outperformed the VGG-16 model considering the other three evaluation metrics. The ResNet50 F1 score, recall, and accuracy were 97.9%, 98.3%, and 97.9%, respectively, while the VGG architecture had 97.7%, 96.9%, and 97.8%, respectively.</p><p id="P22">The Inception V3 gave the lowest performance considering all the evaluation parameters (test accuracy 51.3%, precision 51.3%, recall 52.4%, and F1 score 51.9%). When we compared the four-evaluation metrics, the Plasmo3Net provided superior performance. As a result, the Plasmo3Net outperformed the CNN-based transfer-learning models in recognizing parasitic RBCs, even though it had fewer parameters (<xref ref-type="fig" rid="F7">Figures 7</xref> and <xref ref-type="fig" rid="F8">8</xref>). When the performance and the effectiveness of Plasmo3Net were evaluated using the manually curated NIH dataset (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figures 4</xref>), we observed high-performance metric values. The model obtained an accuracy and precision of 97.6% and 96.9%, respectively. The model had a remarkable F1 score and recall rate of 97.6% and 98.3%, respectively.</p></sec><sec id="S13" sec-type="discussion"><label>4.0</label><title>Discussion</title><p id="P23">Early malaria diagnosis is vital for effective control and eradication. Traditional diagnostic tools often fall short, especially in resource-limited settings where accurate and timely detection is crucial. Integrating artificial intelligence into malaria control strategies offers a promising solution to address these challenges and support elimination efforts. To enhance diagnostic accuracy and speed, this study focused on developing an automated deep learning algorithm based on CNN to detect red blood cells (RBCs) infected with <italic>P. falciparum</italic>.</p><p id="P24">The CNN algorithm developed in this study, Plasmo3Net with a 13-layer architecture, obtained a very high performance after hyperparameter optimization. Optimization of the batch size led to a significant reduction in the cost function, demonstrating the effectiveness of this hyperparameter tuning. The Plasmo3Net demonstrated superior performance with higher accuracy compared to an 8-layer CNN architecture with the same number of epochs (25) and batch sizes [<xref ref-type="bibr" rid="R30">30</xref>]. Furthermore, the 13- layer Plasmo3Net architecture with 3 convolutional layers performed better than the 16-layer architecture with 4 convolutional layers as well as the 19-layer model with 5 convolutional layers. The exceptional performance of the 13-layer Plasmo3Net is likely attributed to its compact architecture [<xref ref-type="bibr" rid="R31">31</xref>]. The structure of complex 16- and 19-layer algorithms can lead to models becoming overly tailored to the training set, a phenomenon known as overfitting. In such cases, the models learn not only the underlying patterns in the training data but also the noise and outliers, which compromises their ability to generalize to new, unseen data. This over-specialization could result in a decline in model performance when applied to datasets beyond the training environment, thereby limiting its practical utility [<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. In addition, the additional layers in the 16-layer and 19-layer models may not have effectively captured unique features or contributed meaningfully to the classification process, potentially reducing their overall performance. In the 19-layer model, a very high learning rate could lead to issues with gradient descent not converging, potentially impacting its effectiveness. [<xref ref-type="bibr" rid="R34">34</xref>]. However, our simple 13-layer model successfully extracted sufficient features from the dataset for effective generalization.</p><p id="P25">Insights into Plasmo3Net's learning progress revealed a significant decrease in training loss. The model's accuracy was a result of its high performance throughout the learning phase, demonstrated by both training and validation accuracy, as well as training and validation losses. The Plasmo3Net outperformed a previously customized CNN models which also obtained high accuracy in a ten-fold cross-validation [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>]. Moreso, the performance metrics of Plasmo3Net were superior to other pretrained models selected for comparison. ResNet50 [<xref ref-type="bibr" rid="R35">35</xref>], VGG16 [<xref ref-type="bibr" rid="R36">36</xref>], Inception-V3 [<xref ref-type="bibr" rid="R37">37</xref>], and AlexNet [<xref ref-type="bibr" rid="R36">36</xref>] were selected as baseline models due to their established performance in malaria parasite identification. The superior performance of Plasmo3Net highlights its potential as a reliable tool for malaria parasite identification. Its adaptability as a reference model for training a more diverse red blood cell (RBC) dataset is crucial in the context of global health, where accurate and timely diagnosis significantly influences treatment outcomes.</p><p id="P26">Moreover, Plasmo3Net demonstrated not only high accuracy but also a high precision rate. This indicates that the model effectively reduced false positives while accurately identifying positive cases. The model's precision likely resulted from its sophisticated data preprocessing techniques, as well as its architecture, which enhanced feature extraction and decision-making. The precision of the Plasmo3Net was higher than previously reported models [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R39">39</xref>]. However, it is essential to consider the implications of high precision in the context of recall [<xref ref-type="bibr" rid="R40">40</xref>]. A model optimized for precision may sometimes trade-off recall, potentially leading to missed positive cases [<xref ref-type="bibr" rid="R41">41</xref>]. Interestingly, our model achieved a high recall despite this potential trade-off. In addition, our proposed model achieved the highest F1 score, indicating its proficiency in accurately identifying true positive instances while effectively reducing false positives and false negatives. The Plasmo3Net F1 score surpassed previous 16-layer model architectures containing 4 convolutional layers after batch-size optimization to 32 [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R42">42</xref>].Good performance measures were also obtained by this model when tested on an external dataset.</p><sec id="S14"><title>Limitations and Future Directions</title><p id="P27">Our model showed strong performance in detecting <italic>P. falciparum</italic>-infected RBCs, but several limitations should be acknowledged to properly contextualize this finding. For example, the model was not trained on different <italic>Plasmodium</italic> species and did not account for the parasite’s various lifecycle stages. This limitation could impact the model’s accuracy and sensitivity in identifying specific malaria types, as different life-cycle stages exhibit unique morphological characteristics. To address this limitation, future research could focus on developing a convolutional neural network (CNN)-based multiclass classification model capable of performing speciation and identifying different lifecycle stages. Such advancements would be crucial for accurate diagnosis, effective treatment, research, and policy formulation. Nevertheless, this study has demonstrated the potential of CNNs in advancing image-based analysis and diagnosis of malaria.</p></sec></sec><sec id="S15" sec-type="conclusions"><title>Conclusions</title><p id="P28">This study has developed Plasmo3Net, a CNN-based deep learning algorithm for classifying red blood cells (RBCs) infected with <italic>P. falciparum</italic>. The model demonstrated superior performance compared to previous models, highlighting the importance of hyperparameter optimization, such as batch size and epoch count, and achieving very high-performance metrics. This has laid the groundwork for future investigation and improvement in malaria diagnosis.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental File</label><media xlink:href="EMS202261-supplement-Supplemental_File.pptx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.presentationml.presentation" id="d149aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Acknowledgements</title><p>The authors appreciate the study participants and Ijede community leaders for their cooperation during the field collection. Special gratitude to the students and staff of the Center for Genomic Research in Biomedicine (CeGRIB), Mountain Top University for their clerical support during manuscript drafting.</p><sec id="S17"><title>Funding</title><p>Kolapo M. Oyebola was supported by the European and Developing Countries Clinical Trials Partnership (EDCTP) Career Development Fellowship (TMA2019CDF-2782). The views expressed in this publication are those of the authors and not necessarily those of the European Union.</p></sec></ack><fn-group><fn id="FN1" fn-type="conflict"><p id="P29"><bold>Conflict of Interests</bold></p><p id="P30">The authors declare no conflict of interest.</p></fn><fn id="FN2" fn-type="con"><p id="P31"><bold>Authors’ Contributions</bold></p><p id="P32">Kolapo M. Oyebola and Afolabi J. Owoloye conceived and designed the study. Kolapo M. Oyebola, Funmilayo C. Ligali, Oluwagbemiga O. Aina and Afolabi J. Owoloye collected field samples. Kolapo M. Oyebola and Afolabi J. Owoloye implemented data analysis, including deep learning algorithms. Kolapo M. Oyebola and Afolabi J. Owoloye drafted the manuscript. All authors read, revised and approved the final manuscript.</p></fn></fn-group><sec id="S18" sec-type="data-availability"><title>Availability of Data and Materials</title><p id="P33">The dataset supporting the findings of this article are within the manuscript and its Supplementary File.</p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berzosa</surname><given-names>P</given-names></name><etal/></person-group><article-title>Comparison of three diagnostic methods (microscopy, RDT, and PCR) for the detection of malaria parasites in representative samples from Equatorial Guinea</article-title><source>Malaria journal</source><year>2018</year><volume>17</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC6142353</pub-id><pub-id pub-id-type="pmid">30223852</pub-id><pub-id pub-id-type="doi">10.1186/s12936-018-2481-4</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molina</surname><given-names>A</given-names></name><etal/></person-group><article-title>Automatic identification of malaria and other red blood cell inclusions using convolutional neural networks</article-title><source>Computers in Biology Medicine</source><year>2021</year><volume>136</volume><elocation-id>104680</elocation-id><pub-id pub-id-type="pmid">34329861</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shambhu</surname><given-names>S</given-names></name><etal/></person-group><article-title>Computational methods for automated analysis of malaria parasite using blood smear images: recent advances</article-title><source>Computational Intelligence Neuroscience</source><year>2022</year><volume>2022</volume><issue>1</issue><elocation-id>3626726</elocation-id><pub-id pub-id-type="pmcid">PMC9017520</pub-id><pub-id pub-id-type="pmid">35449742</pub-id><pub-id pub-id-type="doi">10.1155/2022/3626726</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cireşan</surname><given-names>DC</given-names></name><etal/></person-group><source>Mitosis detection in breast cancer histology images with deep neural networks</source><conf-name>Medical Image Computing and Computer-Assisted Intervention–MICCAI 2013: 16th International Conference, Nagoya, Japan, September 22-26, 2013, Proceedings, Part II 16</conf-name><conf-sponsor>Springer</conf-sponsor><year>2013</year></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><etal/></person-group><source>Deep residual learning for image recognition</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2016</year></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mehanian</surname><given-names>C</given-names></name><etal/></person-group><source>Computer-automated malaria diagnosis and quantitation using convolutional neural networks</source><conf-name>Proceedings of the IEEE international conference on computer vision workshops</conf-name><year>2017</year></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><etal/></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><etal/></person-group><article-title>Brain-inspired models for visual object recognition: an overview</article-title><source>Artificial Intelligence Review</source><year>2022</year><volume>55</volume><issue>7</issue><fpage>5263</fpage><lpage>5311</lpage></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Such</surname><given-names>FP</given-names></name><etal/></person-group><article-title>Robust spatial filtering with graph convolutional neural networks</article-title><source>IEEE Journal of Selected Topics in Signal Processing</source><year>2017</year><volume>11</volume><issue>6</issue><fpage>884</fpage><lpage>896</lpage></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salehi</surname><given-names>AW</given-names></name><etal/></person-group><article-title>A study of CNN and transfer learning in medical imaging: Advantages, challenges, future scope</article-title><source>Sustainability</source><year>2023</year><volume>15</volume><issue>7</issue><elocation-id>5930</elocation-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Waibel</surname><given-names>A</given-names></name><etal/></person-group><chapter-title>Phoneme recognition using time-delay neural networks</chapter-title><source>Backpropagation</source><publisher-name>Psychology Press</publisher-name><year>2013</year><fpage>35</fpage><lpage>61</lpage><pub-id pub-id-type="pmid">18282838</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><issue>7553</issue><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simard</surname><given-names>PY</given-names></name><name><surname>Steinkraus</surname><given-names>D</given-names></name><name><surname>Platt</surname><given-names>JC</given-names></name></person-group><source>Best practices for convolutional neural networks applied to visual document analysis</source><conf-name>Icdar</conf-name><conf-sponsor>Edinburgh</conf-sponsor><year>2003</year></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaillant</surname><given-names>R</given-names></name><name><surname>Monrocq</surname><given-names>C</given-names></name><name><surname>Le Cun</surname><given-names>Y</given-names></name></person-group><article-title>Original approach for the localisation of objects in images</article-title><source>IEE Proceedings-Vision, Image Signal Processing</source><year>1994</year><volume>141</volume><issue>4</issue><fpage>245</fpage><lpage>250</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Advances in Neural Information Processing Systems</source><year>2012</year><volume>25</volume></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><source>Visualizing and understanding convolutional networks</source><conf-name>Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 113</conf-name><conf-sponsor>Springer</conf-sponsor><year>2014</year></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>C</given-names></name><etal/></person-group><source>Going deeper with convolutions</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2015</year></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajaraman</surname><given-names>S</given-names></name><etal/></person-group><article-title>Pre-trained convolutional neural networks as feature extractors toward improved malaria parasite detection in thin blood smear images</article-title><source>PeerJ</source><year>2018</year><volume>6</volume><elocation-id>e4568</elocation-id><pub-id pub-id-type="pmcid">PMC5907772</pub-id><pub-id pub-id-type="pmid">29682411</pub-id><pub-id pub-id-type="doi">10.7717/peerj.4568</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oyebola</surname><given-names>KM</given-names></name><etal/></person-group><article-title>Assessing the therapeutic efficacy of artemether-lumefantrine for uncomplicated malaria in Lagos, Nigeria: a comprehensive study on treatment response and resistance markers</article-title><source>Malar J</source><year>2024</year><volume>23</volume><issue>1</issue><fpage>261</fpage><pub-id pub-id-type="pmcid">PMC11360866</pub-id><pub-id pub-id-type="pmid">39210367</pub-id><pub-id pub-id-type="doi">10.1186/s12936-024-05088-6</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kassim</surname><given-names>YM</given-names></name><etal/></person-group><article-title>Clustering-Based Dual Deep Learning Architecture for Detecting Red Blood Cells in Malaria Diagnostic Smears</article-title><source>IEEE J Biomed Health Inform</source><year>2021</year><volume>25</volume><issue>5</issue><fpage>1735</fpage><lpage>1746</lpage><pub-id pub-id-type="pmcid">PMC8127616</pub-id><pub-id pub-id-type="pmid">33119516</pub-id><pub-id pub-id-type="doi">10.1109/JBHI.2020.3034863</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maus</surname><given-names>J</given-names></name><etal/></person-group><article-title>Deep learning based bilateral filtering for edge-preserving denoising of respiratory-gated PET</article-title><source>EJNMMI Phys</source><year>2024</year><volume>11</volume><issue>1</issue><fpage>58</fpage><pub-id pub-id-type="pmcid">PMC11231129</pub-id><pub-id pub-id-type="pmid">38977533</pub-id><pub-id pub-id-type="doi">10.1186/s40658-024-00661-z</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Díaz</surname><given-names>G</given-names></name><name><surname>González</surname><given-names>FA</given-names></name><name><surname>Romero</surname><given-names>E</given-names></name></person-group><article-title>A semi-automatic method for quantification and classification of erythrocytes infected with malaria parasites in microscopic images</article-title><source>Journal of biomedical informatics</source><year>2009</year><volume>42</volume><issue>2</issue><fpage>296</fpage><lpage>307</lpage><pub-id pub-id-type="pmid">19166974</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pare</surname><given-names>S</given-names></name><etal/></person-group><article-title>Image segmentation using multilevel thresholding: a research review</article-title><source>Journal of Science Technology, Transactions of Electrical Engineering</source><year>2020</year><volume>44</volume><issue>1</issue><fpage>1</fpage><lpage>29</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>D</given-names></name><etal/></person-group><article-title>Automatic kernel counting on maize ear using RGB images</article-title><source>Plant Methods</source><year>2020</year><volume>16</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC7268725</pub-id><pub-id pub-id-type="pmid">32518581</pub-id><pub-id pub-id-type="doi">10.1186/s13007-020-00619-z</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>X</given-names></name><etal/></person-group><source>An improved Sobel face gray image edge detection algorithm</source><conf-name>39th chinese control conference (CCC)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2020</year></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abayomi-Alli</surname><given-names>OO</given-names></name><etal/></person-group><source>Data augmentation using principal component resampling for image recognition by deep learning</source><conf-name>Artificial Intelligence and Soft Computing: 19th International Conference, ICAISC 2020, Zakopane, Poland, October 12-14, 2020, Proceedings, Part II19</conf-name><conf-sponsor>Springer</conf-sponsor><year>2020</year></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DPJapa</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>1412.6980</elocation-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Althubiti</surname><given-names>SA</given-names></name><etal/></person-group><article-title>Circuit manufacturing defect detection using VGG16 convolutional neural networks</article-title><source>J Wireless Communications Mobile Computing</source><year>2022</year><volume>2022</volume><issue>1</issue><elocation-id>1070405</elocation-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karsh</surname><given-names>B</given-names></name><name><surname>Laskar</surname><given-names>R</given-names></name><name><surname>Karsh</surname><given-names>R</given-names></name></person-group><article-title>mIV3Net: modified inception V3 network for hand gesture recognition</article-title><source>Multimedia Tools Applications</source><year>2024</year><volume>83</volume><issue>4</issue><fpage>10587</fpage><lpage>10613</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maqsood</surname><given-names>A</given-names></name><etal/></person-group><article-title>Deep malaria parasite detection in thin blood smear microscopic images</article-title><year>2021</year><volume>11</volume><issue>5</issue><elocation-id>2284</elocation-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>L</given-names></name><etal/></person-group><article-title>Separability and Compactness Network for Image Recognition and Superresolution</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2019</year><volume>30</volume><issue>11</issue><fpage>3275</fpage><lpage>3286</lpage><pub-id pub-id-type="pmid">30703043</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bejani</surname><given-names>MM</given-names></name><name><surname>Ghatee</surname><given-names>MJAIR</given-names></name></person-group><article-title>A systematic review on overfitting control in shallow and deep neural networks</article-title><year>2021</year><volume>54</volume><issue>8</issue><fpage>6391</fpage><lpage>6438</lpage></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ying</surname><given-names>X</given-names></name></person-group><article-title>An overview of overfitting and its solutions</article-title><source>Journal ofphysics: Conference series</source><publisher-name>IOP Publishing</publisher-name><year>2019</year></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>LN</given-names></name></person-group><source>Cyclical learning rates for training neural networks</source><conf-name>2017 IEEE winter conference on applications of computer vision (WACV)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2017</year></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hemachandran</surname><given-names>K</given-names></name><etal/></person-group><article-title>Performance analysis of deep learning algorithms in diagnosis of malaria disease</article-title><source>Diagnostics</source><year>2023</year><volume>13</volume><issue>3</issue><fpage>534</fpage><pub-id pub-id-type="pmcid">PMC9914762</pub-id><pub-id pub-id-type="pmid">36766640</pub-id><pub-id pub-id-type="doi">10.3390/diagnostics13030534</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>Z</given-names></name><etal/></person-group><source>CNN-based image analysis for malaria diagnosis</source><conf-name>2016 IEEE international conference on bioinformatics and biomedicine (BIBM)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2016</year></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magotra</surname><given-names>V</given-names></name><name><surname>Rohil</surname><given-names>MK</given-names></name></person-group><article-title>Malaria diagnosis using a lightweight deep convolutional neural network</article-title><source>International Journal of Telemedicine Applications</source><year>2022</year><volume>2022</volume><issue>1</issue><elocation-id>4176982</elocation-id><pub-id pub-id-type="pmcid">PMC9033338</pub-id><pub-id pub-id-type="pmid">35463192</pub-id><pub-id pub-id-type="doi">10.1155/2022/4176982</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ahmed</surname><given-names>KT</given-names></name><etal/></person-group><source>Malaria Parasite Detection Using CNN-Based Ensemble Technique on Blood Smear Images</source><conf-name>2023 International Conference on Electrical, Computer and Communication Engineering (ECCE)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2023</year></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shekar</surname><given-names>G</given-names></name><name><surname>Revathy</surname><given-names>S</given-names></name><name><surname>Goud</surname><given-names>EK</given-names></name></person-group><source>Malaria detection using deep learning</source><conf-name>2020 4th international conference on trends in electronics and informatics (ICOEI)(48184)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2020</year></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saito</surname><given-names>T</given-names></name><name><surname>Rehmsmeier</surname><given-names>MJPo</given-names></name></person-group><article-title>The precision-recall plot is more informative than the ROC plot when evaluating binary classifiers on imbalanced datasets</article-title><year>2015</year><volume>10</volume><issue>3</issue><elocation-id>e0118432</elocation-id><pub-id pub-id-type="pmcid">PMC4349800</pub-id><pub-id pub-id-type="pmid">25738806</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0118432</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckland</surname><given-names>M</given-names></name><name><surname>Gey</surname><given-names>FJJotAsfis</given-names></name></person-group><article-title>The relationship between recall and precision</article-title><year>1994</year><volume>45</volume><issue>1</issue><fpage>12</fpage><lpage>19</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>YS</given-names></name><name><surname>Hong</surname><given-names>PC</given-names></name></person-group><article-title>Applying Machine Learning to Healthcare Operations Management: CNN-Based Model for Malaria Diagnosis</article-title><source>Healthcare (Basel)</source><year>2023</year><volume>11</volume><issue>12</issue><pub-id pub-id-type="pmcid">PMC10298712</pub-id><pub-id pub-id-type="pmid">37372897</pub-id><pub-id pub-id-type="doi">10.3390/healthcare11121779</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental workflow for developing the convolutional neural network</title></caption><graphic xlink:href="EMS202261-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Image preprocessing and segmentation workflow.</title></caption><graphic xlink:href="EMS202261-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Plasmo3Net convolutional neural network architecture</title></caption><graphic xlink:href="EMS202261-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Plasmo3Net learning curve: (A) accuracy, (B) loss.</p></caption><graphic xlink:href="EMS202261-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Pre-trained model learning curve: (A) VGG-16 accuracy, (B) VGG-16 loss, (C) Inception V3 accuracy, (D) Inception V3 loss, (E) ResNet50 accuracy (F) ResNet50 loss (G) AlexNet accuracy (H) AlexNet loss.</p></caption><graphic xlink:href="EMS202261-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Plasmo3Net model prediction outcome.</title></caption><graphic xlink:href="EMS202261-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Confusion matrix for Plasmo3Net evaluation</title></caption><graphic xlink:href="EMS202261-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Evaluation of confusion matrices for pre-trained architectures: (A) ResNet50, (B) VGG-16, (C) AlexNet, and (D) Inception V3.</p></caption><graphic xlink:href="EMS202261-f008"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Convolutional Neural Network Model Parameters</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="middle">Model</th><th align="center" valign="middle">Trainable parameter</th><th align="center" valign="middle">Non-trainable parameter</th></tr></thead><tbody><tr><td align="left" valign="middle">AlexNet</td><td align="center" valign="middle">21,585,281</td><td align="center" valign="middle">0</td></tr><tr><td align="left" valign="middle">Inception V3</td><td align="center" valign="middle">23,868,578</td><td align="center" valign="middle">34,432</td></tr><tr><td align="left" valign="middle">ResNet50</td><td align="center" valign="middle">23,534,592</td><td align="center" valign="middle">53,120</td></tr><tr><td align="left" valign="middle">VGG16</td><td align="center" valign="middle">15,239,489</td><td align="center" valign="middle">0</td></tr><tr><td align="left" valign="middle">16-layer CNN</td><td align="center" valign="middle">913,729</td><td align="center" valign="middle">0</td></tr><tr><td align="left" valign="middle">19-layer CNN</td><td align="center" valign="middle">1,831,745</td><td align="center" valign="middle">0</td></tr><tr><td align="left" valign="middle">Plasmo3Net</td><td align="center" valign="middle">683,329</td><td align="center" valign="middle">0</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Model performance metrics</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="middle">Model</th><th align="center" valign="middle">Accuracy</th><th align="center" valign="middle">Precision</th><th align="center" valign="middle">Recall</th><th align="center" valign="middle">F1 score</th><th align="center" valign="middle">Specificity</th><th align="center" valign="middle">Sensitivity</th></tr></thead><tbody><tr><td align="left" valign="middle">Inception-V3</td><td align="center" valign="middle">51.3</td><td align="center" valign="middle">51.3</td><td align="center" valign="middle">52.4</td><td align="center" valign="middle">51.9</td><td align="center" valign="middle">50.2</td><td align="center" valign="middle">52.4</td></tr><tr><td align="left" valign="middle">AlexNet</td><td align="center" valign="middle">91.3</td><td align="center" valign="middle">93.3</td><td align="center" valign="middle">89.0</td><td align="center" valign="middle">91.1</td><td align="center" valign="middle">93.6</td><td align="center" valign="middle">89.0</td></tr><tr><td align="left" valign="middle">ResNet50</td><td align="center" valign="middle">97.9</td><td align="center" valign="middle">97.6</td><td align="center" valign="middle">98.3</td><td align="center" valign="middle">97.9</td><td align="center" valign="middle">97.6</td><td align="center" valign="middle">98.3</td></tr><tr><td align="left" valign="middle">VGG-16</td><td align="center" valign="middle">97.8</td><td align="center" valign="middle">98.6</td><td align="center" valign="middle">96.9</td><td align="center" valign="middle">97.7</td><td align="center" valign="middle">98.6</td><td align="center" valign="middle">96.9</td></tr><tr><td align="left" valign="middle">16-layer CNN</td><td align="center" valign="middle">97.7</td><td align="center" valign="middle">97.8</td><td align="center" valign="middle">97.6</td><td align="center" valign="middle">97.7</td><td align="center" valign="middle">97.9</td><td align="center" valign="middle">97.6</td></tr><tr><td align="left" valign="middle">19-layer CNN</td><td align="center" valign="middle">97.3</td><td align="center" valign="middle">95.9</td><td align="center" valign="middle">98.9</td><td align="center" valign="middle">97.4</td><td align="center" valign="middle">95.8</td><td align="center" valign="middle">98.9</td></tr><tr><td align="left" valign="middle">Plasmo3Net</td><td align="center" valign="middle">99.3</td><td align="center" valign="middle">99.1</td><td align="center" valign="middle">99.6</td><td align="center" valign="middle">99.3</td><td align="center" valign="middle">99.1</td><td align="center" valign="middle">99.6</td></tr></tbody></table></table-wrap></floats-group></article>