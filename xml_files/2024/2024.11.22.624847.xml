<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200759</article-id><article-id pub-id-type="doi">10.1101/2024.11.22.624847</article-id><article-id pub-id-type="archive">PPR944564</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Introducing SPROUT (Semi-automated Parcellation of Region Outputs Using Thresholding): an adaptable computer vision tool to generate 3D segmentations</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>He</surname><given-names>Yichen</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Camaiti</surname><given-names>Marco</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Roberts</surname><given-names>Lucy E.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Mulqueeney</surname><given-names>James M.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Didziokas</surname><given-names>Marius</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Goswami</surname><given-names>Anjali</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>School of Biosciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05krs5044</institution-id><institution>University of Sheffield</institution></institution-wrap>, <city>Sheffield</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Life Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/039zvsn29</institution-id><institution>Natural History Museum</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label>Department of Ocean &amp; Earth Science, National Oceanography Centre Southampton, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ryk1543</institution-id><institution>University of Southampton</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A4"><label>4</label>Department of Mechanical Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap>, <city>London</city>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1"><email>yichen.he@nhm.ac.uk</email>, <email>marco.camaiti1@nhm.ac.uk</email></corresp><fn id="FN1"><label>*</label><p id="P1">co-first authors.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>24</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">The increased availability of 3D image data requires improving the efficiency of digital segmentation, currently relying on manual labelling, especially when separating structures into multiple components. Automated and semi-automated methods to streamline segmentation have been developed, such as deep learning and smart interpolation, but require pre-labelled data, and specialized hardware and software. Deep learning models in particular often require the manual creation of extensive training data, particularly for complex multi-class segmentations. Here, we introduce SPROUT, a novel, semi-automated computer vision method providing a time-efficient and user-friendly pipeline for segmenting and parcellating image data. SPROUT generates seeds (representing parts of an object) based on specified density thresholds and erosion of connected components, to achieve element separation. Seeds are grown to obtain fully-parcellated segmentations. We compare SPROUT’s performance to that of smart interpolation and apply it to diverse datasets to demonstrate the utility and versatility of this open-source 3D segmentation method.</p></abstract></article-meta></front><body><sec sec-type="intro" id="S1"><title>Introduction</title><p id="P3">Rapid analysis of image data is critical for numerous applications, from scientific research to medicine, engineering, and security. High-resolution 2D and 3D image data are being generated at rates unthinkable just a few years ago (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>), placing increasing strain on users to process these data efficiently. One example of vast increases in image data comes from the field of natural history. The digitisation of natural history specimens plays a crucial role in preserving and making collections of biological, geological, and archaeological data accessible, enabling researchers to study specimens non-invasively and in greater detail (<xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R5">5</xref>). By converting specimens into digital formats, scientists can apply advanced computational techniques to extract and analyse phenotypes in detail (<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R8">8</xref>) In particular, 3D images produced using computed tomography (CT), magnetic resonance (MRI), electron tomography and phase-contrast scanning have become commonplace in natural history, alongside the rise of large 3D image libraries for open data sharing (<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R12">12</xref>).</p><p id="P4">The increased availability of 3D data brings countless benefits, but also new challenges for data processing (<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R13">13</xref>). One of the major bottlenecks in the processing of 3D images is the segmentation of scans. While tools using smart interpolation or deep learning have greatly sped up the process of segmenting whole specimens (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>), there remain significant barriers to quickly separating 3D images into the individual components of an object (parcellation). For example, individually separating the bones of an entire skeleton, particularly when there are varying numbers of elements across a set of images, can present a lengthy and challenging process. As the capability to efficiently generate 3D image data expands at a rapid pace, efficiently extracting detailed and specific information from these images has become a key priority to conduct large-scale analyses.</p><p id="P5">For these imaging modalities, image segmentation offers a solution for extracting detailed regions from a scan (<xref ref-type="bibr" rid="R16">16</xref>). While these terms are used variably in different applications, here we use the term image segmentation to refer to the isolation of specific elements of an image, which has also been described as defining regions of interest from a scan (<xref ref-type="bibr" rid="R17">17</xref>). Many segmentation tasks remain manual at present, particularly when the required data are complicated or domain-specific, meaning that the extraction of data is limited by the expertise and speed of a given user, and is prone to the interference of subjective biases (<xref ref-type="bibr" rid="R18">18</xref>). This poses a significant issue to many fields such as the biological sciences, medical science (e.g., segmenting tumours from medical CT scans, (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>)), engineering and material sciences (e.g., separating the different parts of an engine to study their respective interactions (<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>)), and geology (e.g., separating specific aggregated microstructures into their finer components (<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>)).</p><p id="P6">Recent methodological advances have attempted to improve the efficiency of 3D image segmentation in different ways. Most prominently, deep learning has shown much promise in automatically segmenting biological images. Deep learning models such as U-net (<xref ref-type="bibr" rid="R14">14</xref>) and V-net (<xref ref-type="bibr" rid="R25">25</xref>) have proven effective at segmenting CT datasets based on highly specific training data. Several open-source tools, such as MONAI (<xref ref-type="bibr" rid="R26">26</xref>), nnU-Net (<xref ref-type="bibr" rid="R27">27</xref>), Biomedisa (23, 24), and ilastik (25), as well as segmentation software such as Avizo (Thermo Fisher Scientific, USA), VGStudio (Volume Graphics, Germany), and Dragonfly (Object Research Systems, Canada), implement deep and machine learning techniques for segmentation and provide the ability to train custom models.</p><p id="P7">However, as the complexity and variability of input data increases, or when extensive domain knowledge is required, the amount of training and post-processing required to obtain useful data can also be expected to increase accordingly (<italic>8</italic>, <italic>28</italic>, <italic>29</italic>). Improving the performance of these models can be time-consuming and difficult due to their nature as ‘black boxes’ (<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>), requiring significant efforts to tune models and model architectures to handle new datasets. Additionally, the creation of training data for these models requires high domain knowledge from the user, limiting their performance and applicability, in addition to being time-consuming (<xref ref-type="bibr" rid="R32">32</xref>). In general, the main issues with these AI-based methods are the time scalability and specificity of the tasks, leading to diminishing return scenarios when these factors are not properly accounted for.</p><p id="P8">Another promising avenue to handle these tasks has been through the use of classic computer vision algorithms (<xref ref-type="bibr" rid="R33">33</xref>). Some examples of these methods are thresholding (<xref ref-type="bibr" rid="R34">34</xref>), interpolation (<xref ref-type="bibr" rid="R35">35</xref>), morphological transformations (<xref ref-type="bibr" rid="R36">36</xref>), watershed (<xref ref-type="bibr" rid="R37">37</xref>), and smart-grid tools (<xref ref-type="bibr" rid="R38">38</xref>), as well as seed-based methods (<xref ref-type="bibr" rid="R39">39</xref>). Sophisticated extensions of these approaches exist, such as smart interpolation as implemented in Biomedisa (<xref ref-type="bibr" rid="R35">35</xref>), which uses weighted random walk models incorporating the underlying volumetric data to predict segmentation. Compared to deep learning, these have the advantage of not requiring any training data or expensive graphics processing units (GPU). These methods typically rely on heuristic inputs (e.g., preset thresholds or seed locations) to operate based on the inherent properties of an image, such as the grayscale values of stacked CT images that form a 3D volume. As they do not require any other information besides what is already contained in the images themselves, non-deep learning, computer vision-based workflows are more adaptable to variable inputs (<xref ref-type="bibr" rid="R40">40</xref>). While this does still require some domain knowledge, variability in results can be mitigated by the ability to quickly generate multiple outputs based on different input parameters (i.e., threshold selection), limiting the user’s contribution to a series of relatively simple steps and choices.</p><p id="P9">Although tools using deep learning and smart interpolation have proven quite successful at handling varied biological datasets, they come with some drawbacks. Insufficient resolution of images can significantly impact the ability of these methods to separate elements, due to the presence of voxels with common grayscale values between labels causing the overgrowth of one label onto another. This means that results often require significant manual corrections to yield biologically-relevant segmentations.</p><p id="P10">More sophisticated combinations of these classic computer vision methods have recently emerged, such as the newly released BounTI (<xref ref-type="bibr" rid="R41">41</xref>), an add-on for the image processing software Avizo. BounTI uses an initial threshold to separate regions on a 3D input image. Once a seed with the desired separations is selected, it can be grown to a target threshold. The ideal result of these operations is a multi-class (“parcellated”) segmentation. Importantly, BounTI (<xref ref-type="bibr" rid="R41">41</xref>) performs well with lower-resolution scans, which may even be preferred for some datasets because small image sizes speed up processing time, and can actually achieve better discrimination of grayscale voxels. However, seed creation and growth in BounTI can take significant amounts of time, as choosing an initial threshold is more skill-intensive and sensitive to user error. BounTI also does not support parallel processing, which exacerbates its inefficiency when segmenting large datasets. BounTI also works best with proprietary software (i.e., Avizo) whose cost and required operational knowledge can make it inaccessible to standard users.</p><p id="P11">To face the highlighted challenges in image segmentation, we present an open-source Python toolkit called SPROUT (Semi-automated Parcellation of Region Outputs Using Thresholding). This tool integrates computer vision methods (e.g., thresholding and morphological transformations) to generate fine-grained multi-class segmentations on 3D cross-sectional scans with customized user inputs. Similar to BounTI (<xref ref-type="bibr" rid="R41">41</xref>), SPROUT is based on seed generation and growth and performs well with lower-resolution scans. However, SPROUT incorporates additional methods (e.g., morphological erosion) for seed creation to achieve potential better separations between elements. It also includes the ability to generate merged seeds (combining the elements generated in multiple seeds), reducing the need for manual corrections. Because SPROUT is not based on deep learning, it does not require the time-consuming process of generating training datasets. Additionally, SPROUT supports batch processing and full parallelization, with optional add-ons for data loading and visualisation in Avizo, maximising its efficiency for analysing large datasets.</p><p id="P12">To demonstrate the capabilities of SPROUT, we compare the accuracy and processing time of its output to both raw and manually corrected results from smart interpolation using Biomedisa (35). We also show that SPROUT—a user-friendly and freely-accessible tool—can produce time-efficient, reliable, and meaningful segmentations of 3D image data from a wide range of biological, medical, and engineering fields.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Software requirements</title><p id="P13">The SPROUT implementation requires Python v 3.10 and is built using a range of well-supported libraries. The following libraries are integral to the workflow: <list list-type="bullet" id="L1"><list-item><p id="P14"><bold>NumPy</bold> (v 1.26.4)</p></list-item><list-item><p id="P15"><bold>Pandas</bold> (v 2.21)</p></list-item><list-item><p id="P16"><bold>Scikit-image</bold> (v 0.22.0)</p></list-item><list-item><p id="P17"><bold>Tifffile</bold> (v 2024.2.12)</p></list-item><list-item><p id="P18"><bold>PyYAML</bold> (v 6.0.1)</p></list-item><list-item><p id="P19"><bold>Trimesh</bold> (v 4.3.1)</p></list-item><list-item><p id="P20"><bold>Open3D</bold> (v 0.18.0)</p></list-item><list-item><p id="P21"><bold>Matplotlib</bold> (v 3.8.3)</p></list-item></list></p><p id="P22">The software has been tested and verified to run on Windows 10/11 and Ubuntu 24.04 LTS platforms. It was not tested but is expected to run on MacOS. Avizo add-ons have been tested with Avizo 2020, Avizo 2022 Pro, and Avizo 2023 Pro. Additionally, SPROUT supports multi-core processing, making it well-suited for deployment on high-performance computing (HPC) clusters and cloud computing platforms such as AWS EC2.</p><p id="P23">The source code, including scripts for Avizo add-ons, a detailed tutorial, vignettes, and test scan data, can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/PhAInomics/SPROUT">https://github.com/PhAInomics/SPROUT</ext-link>. Example input files, such as .yaml configuration files for running SPROUT functions and the suture mapping file, are also available in the GitHub repository.</p></sec><sec id="S4"><title>Overview</title><p id="P24">SPROUT was originally developed to solve the time-consuming problem of segmenting individual bones of the mammalian skull from micro-CT scans. It has since been modified and tested on various datasets, evolving into a semi-automated workflow for segmenting fine-grain elements in cross-sectional images, especially of biological specimens. SPROUT has demonstrated potential as an efficient and effective method for producing accurate and biologically meaningful segmentations.</p><p id="P25">Due to the nature of organisms or the quality of scanning systems, clear separations among target elements are not always evident, particularly when elements are connected or semi-connected within a larger structure, as is often the case for cranial bones (<xref ref-type="bibr" rid="R42">42</xref>). This workflow uses a combination of computer vision methods to identify separations between these elements. For many scans, reducing the size of the segmented regions can potentially increase separations among individual elements of interest. This is especially relevant in biological specimens because the connections between components such as bones and organs often appear as thin areas or weak signals (i.e., lower intensities). The workflow extracts separated regions that correspond to these individual elements in a step known as seed generation, where multiple seeds can be generated based on the input parameters.</p><p id="P26">Following the seed generation step, a candidate seed needs to be selected. The separated regions of the candidate seed may not accurately represent the target elements, as seeds are typically smaller. A seed growth step that is similar to region growing (<xref ref-type="bibr" rid="R43">43</xref>) is applied, using the separated regions as seeds (i.e., starting positions) and growing them back to match the target elements (<xref ref-type="fig" rid="F1">Fig. 1</xref>). This combination of seed generation and controlled growth enables precise, multi-class segmentation of biological structures in cross-sectional scans.</p></sec><sec id="S5"><title>Seed Generation</title><p id="P27">The seed generation step applied a threshold to generate a binary segmentation, isolating the foreground (e.g., the target object) from the background. Higher threshold values typically produce smaller foreground regions. Morphological erosion was then applied to the binary segmentation, reducing the foreground to smaller regions. By applying thresholds, erosion, or combining both techniques, the foreground size can be reduced while increasing the separation between regions. A disconnected components algorithm is subsequently used to identify these separated regions, retaining the largest regions based on the target number of segments. SPROUT enables users to easily combine and fine-tune these techniques, making it possible to segment multiple distinct regions effectively.</p><p id="P28">Additionally, for datasets with pre-existing binary segmentations, users can directly input these segmentations into the seed generation step by setting the threshold to one. High-quality binary segmentations can be efficiently generated through various approaches, such as thresholding (<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R42">42</xref>) or deep learning (<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R28">28</xref>), as isolating the foreground is often simpler compared to multi-class segmentation tasks.</p><p id="P29">The make_seeds.py script is used to create seeds for segmentation, using the following key inputs: (1) input and output information, including an input image and an output folder; (2) a threshold value; (3) the number of erosion steps and (4) the target number of segments. Additional, optional, parameters include the direction of the erosion (the default is a sphere with a radius of one) and a boundary mask, which identifies non-target regions to help create more separations—especially useful for complex images.</p></sec><sec id="S6"><title>Seed Selection</title><p id="P30">With different input settings, multiple seeds can be generated; however, only one seed is selected as the candidate seed for the growth step. To choose the best seed, the method offers seed statistics, including the volume of the binary segmentation generated by various thresholds and the volume of each segmented element. This helps the user to understand how the method performs on their data and potentially apply initial filtering using these statistics.</p><p id="P31">It is important to check the seeds. Users can visualise and check seeds by volume rendering them using cross-sectional image manipulation tools such as 3D Slicer (<xref ref-type="bibr" rid="R44">44</xref>), Dragonfly (Object Research Systems, Canada) and Avizo (Thermo Fisher Scientific, USA). We also provide a make_mesh.py function to create coloured meshes of each segmented region, which users can then check using mesh manipulation tools such as MeshLab.</p><p id="P32">Common issues include under-segmentation, over-segmentation, and disappearing regions. These problems arise due to a trade-off between more regions with fewer separations (under-segmentation) or regions with more separations (over-segmentation and disappearing regions, <xref ref-type="fig" rid="F1">Fig. 1</xref>). Seeds can be regenerated by adjusting the input parameters to address these issues. Another solution is manually separating or merging inaccurate regions, which is especially effective when dealing with nearly correct seeds.</p></sec><sec id="S7"><title>Seed Growth</title><p id="P33">Once a candidate seed is selected, the next step is to grow the seed to better represent the original morphology of the target elements, as regions derived from seeds are typically smaller than the actual target elements. We apply morphological dilation to grow each element in the seed. To prevent the dilation from including unwanted regions (e.g., backgrounds), we use a binary segmentation as a boundary to regulate the growth process. A common use case involves applying a range of thresholds, starting from a value higher than the target threshold and gradually decreasing to the target threshold. The target threshold is defined as the threshold value that produces the most accurate binary segmentation. By progressively lowering the threshold, the boundary (i.e., the binary segmentation) grows incrementally, making the growth step steady.</p><p id="P34">The seed growth step is run using the function <ext-link ext-link-type="uri" xlink:href="https://make_grow.py">https://make_grow.py</ext-link>. The key inputs are (1) input and output information, including an input image, the candidate seed and an output folder; (2) a threshold or a list of thresholds and (3) the number of dilations. Other inputs include the direction of the dilation (the default is a sphere with a radius of one), specified segmentation classes to grow, the output folder, and how many intermediate growth results to save.</p><p id="P35">As in the seed selection step, users can view the growth statistics or visualise the growth result. If inaccurate segments are observed, adjustments can be made by modifying growth parameters or performing manual corrections, ensuring that the grown regions accurately represent the biological structures of interest.</p></sec><sec id="S8"><title>Additional tools</title><p id="P36">The steps introduced above are the fundamental functions of the SPROUT workflow. Additional functions have been integrated to enhance SPROUT’s efficiency and versatility in processing complex biological data.</p><sec id="S9"><title>Merged Seed Generation</title><p id="P37">As mentioned earlier, the trade-off between remaining regions and separations can pose a challenge for generating a good seed. To address this, we developed the make_seeds_merged.py function. This function creates a merged seed from multiple seeds, detects separations and retains areas that are completely eroded. Thus, it creates a seed with maximum separations while preserving as much area as possible.</p><p id="P38">The inputs for make_seeds_merged.py are similar to those used for make_seeds.py, including basic input and output information and the number of segments. However, the merged seed generation provides two distinct methods for merging seeds based on the inputs:<list list-type="order" id="L2"><list-item><p id="P39">Threshold List with Fixed Erosion: This approach requires a list of thresholds alongside a fixed erosion number. The function generates seeds at each threshold level with the same erosion and then merges them. This approach allows seeds generated at higher thresholds (with more separations) to be combined with those at lower thresholds (with greater region preservation).</p></list-item><list-item><p id="P40">Single Threshold with Varying Erosions: This approach requires a single threshold and a fixed erosion number. The function generates seeds by applying erosions, starting from none (zero) up to the given erosion count with the input threshold. The seeds are then merged to include minimally eroded areas and more separated regions.</p></list-item></list></p><p id="P41">Through these two types of merged seed generation, make_seeds_merged.py offers flexibility for different segmentation needs, meaning that SPROUT can generate high-quality seeds with little or no manual intervention. Furthermore, if certain patterns, particularly biological ones, can be leveraged, it is possible to automatically set inputs to batch-generate merged seeds and growth results for multiple scans. The primary dataset used for the evaluation of SPROUT, dog limbs, were segmented using this automated version of SPROUT, as discussed in Results.</p></sec><sec id="S10"><title>Batch Processing and Parallelization</title><p id="P42">All functions of SPROUT support batch processing. Users can batch-generate seeds, merged seeds, and growth results with global or individual scan input. Individual scan parameters might be better suited when differently formated specimens or images are batch-processed together. For example, some scans require fewer erosions to achieve good seeds, while others may require more. This flexibility helps maximise segmentation accuracy across a diverse range of scans. Logs are saved, including segmentation statistics, runtime performance, and error handling, providing users with comprehensive data for troubleshooting and optimizing batch runs.</p><p id="P43">The SPROUT workflow also supports various levels of parallel processing to fully utilise available CPU cores. In batch processing, it supports the simultaneous parallel processing of multiple images. Parallelization has also been implemented for generating seeds, merged seeds, meshes and growth results, using each segmentation class concurrently. In addition, a parallelized version of BounTI has been implemented as well. Memory usage has been optimized to ensure minimal memory consumption when processing scans. It is important to note that parallelization requires multi-core computing devices and sufficient memory to read and process multiple scans. We report on the performance of test runs using parallelization in the Results section.</p></sec><sec id="S11"><title>Avizo Add-ons</title><p id="P44">Manual checking, data cleaning and processing are crucial tasks in image analysis, especially in recent evolutionary morphology studies that now commonly use micro-CT scans. With datasets growing increasingly (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>), these manual processes can be quite demanding and time-consuming (<xref ref-type="bibr" rid="R47">47</xref>). As the dataset sizes in these studies are typically not large enough from a statistical standpoint to ignore some errors, ensuring data (e.g., fine-grained segmentation) accuracy is essential. Minimising errors is, therefore, of great importance. To address this, we leverage the existing micro-CT software Avizo (Thermo Fisher Scientific, USA), where we have developed Avizo add-ons with graphic user interfaces (GUIs) for users to apply the SPROUT workflow and manipulate generated results easily.</p><p id="P45">Avizo is a powerful tool with GUIs for manipulating and visualising images such as micro-CT scans. However, using it within our workflow would require manually opening and applying multiple functions and filters. We have built add-ons that minimise unnecessary operations. These add-ons support pre-processing, batch loading of results, result manipulation (e.g., merging and splitting), and GUI-supported generation of seeds, growth results, and sutures.</p></sec></sec><sec id="S12"><title>Data &amp; Results</title><sec id="S13"><title>Evaluation and comparison: dog limbs</title><p id="P46">To test the effectiveness and accuracy of SPROUT segmentation we performed a comparative analysis on nine sets of CT images of the domestic dog <italic>Canis familiaris</italic> (University of Liverpool Small Animal Teaching Hospital). We cropped each CT stack to focus on the forelimb and set SPROUT thresholds to focus on the extraction of the bones. These scans encompass different breeds, voxel sizes and limb orientations. However, as the grayscale values of different regions for these scans are similar, we used a batch-processed pipeline including merged seed generation and growth to generate the segmentation automatically. We produced the merged seed by incrementally increasing the threshold from 100 to 200 by intervals of 5 (<xref ref-type="fig" rid="F2">Fig. 2</xref>).</p><p id="P47">For comparison to the SPROUT segmentations, we also generated results using the smart interpolation tool in Biomedisa, pre-segmenting every 10-50 slices (<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R35">35</xref>). We then manually corrected the Biomedisa results, using this as our ground truth. Smart Interpolation requires pre-segmented slices, which we created for the scapula, humerus, radius and ulna. To compare this directly with SPROUT we removed all of the segments from the SPROUT output except for these four, and re-ordered the labels according to the order used for Smart Interpolation. Pairwise DICE scores (ranges from 0 to 1, is a measure of overlap between two segmentations) were calculated for segmentations from three sets of results: (1) Ground truth segmentation (manually corrected Biomedisa smart interpolation result); (2) Biomedisa smart interpolation result; (3) SPROUT result.</p><p id="P48">Outputs from the three sets of segmentations were remarkably similar. The mean DICE score comparing the ground truth with the uncorrected Biomedia smart interpolation outputs and with the SPROUT outputs are all between 0.98 and 0.99, representing 98-99% similarity between the ground truth and the outputs of both methods (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Differences between segmentations generated by SPROUT and the ground truth modelare observed to be increased separation of elements in the SPROUT output, such as, for example at the proximal portion of the humerus and ulna (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Despite the strong similarity in the final result of the SPROUT and Biomedisa outputs, these results represent marked differences in time input. Using SPROUT we batch processed the nine dog specimens using a single script, which ran for 23 hours. We then visualised the outputs and manually corrected the labels in Avizo. The total manual/user time input for manual correction was less than 10 minutes per specimen. Biomedisa requires a pre-segmentation step of manually labelling slices of each element, which took approximately 30 minutes per specimen for this dataset, whereas SPROUT does not require pre-segmentation. In addition to the difference in user input, SPROUT outputs every element within the scan that is captured by thresholding; when segmenting the limb bones for example SPROUT also segments out the mandible, skull, ribs and bones of the foot (<xref ref-type="fig" rid="F2">Fig. 2</xref>).</p></sec><sec id="S14"><title>Demonstrations of SPROUT on additional datasets: from human hearts to concrete blocks</title><p id="P49">We also performed SPROUT segmentation on five additional volumetric datasets to test the versatility of the method on different scan modalities and subjects (<xref ref-type="table" rid="T2">Table 2</xref>, <xref ref-type="fig" rid="F4">Fig. 4</xref>). This includes MRI volumetric data of a human heart (Pace <italic>et al.</italic> 2015), micro-CT data of a whole biological specimen (<italic>Tiliqua scincoides</italic>, California Academy of Sciences CAS:HERP:254658), (<italic>Orycteropus afer,</italic> AMNH 51909), binary segmentations of</p><p id="P50">foraminifera generated from initial segmentation of micro-CT data (μ-VIS X-ray Imaging Centre, University of Southampton, UK), and micro-CT data of non-biological material (steel-reinforced concrete, Hampe 2024).</p><p id="P51">These datasets demonstrate that the pipeline described above is applicable to a broad range of systems. In applying SPROUT to the micro-CT scan of a whole skeleton of a skink, the MRI scan of a human heart, and the micro-CT scan of a slab of steel-reinforced concrete, the same process as detailed above for the dog limb example was used, with the only modification being in the range of thresholds and the target number of segments used to generate seeds. We expect that this protocol will therefore be sufficient for the majority of users. However, we demonstrate two further examples that deviate slightly from this protocol due to either the modality or complexity of the data.</p></sec><sec id="S15"><title>Structures with incomplete separations between segments: mammal skulls</title><p id="P52">Separating the individual bones of a mammalian skull can represent a significant challenge for segmentation, as there tends to be a large variability in the contact between bones and the grayscale values of the space in between bones, which can be further complicated by biological factors such as suture fusion. To solve these problems, the SPROUT workflow requires specific alterations both in the seed generation and in the seed growth processes. In the aardvark skull example (<italic>Orycteropus afer</italic>: <xref ref-type="fig" rid="F4">Fig. 4</xref>), the creation of a boundary layer mask was required (e.g., here generated using the ‘tophat’ function in Avizo) to help maintain a clearer separation between skull bones during seed generation. This boundary layer, which essentially enhances the separation between elements by creating continuity between the darker voxels of an image (note that bone is represented by the lighter voxels) is input in a modified version of the pipeline that keeps this mask in place when generating seeds. After a good seed is selected, the boundary mask is also input into the seed growth process to better represent the target elements. We apply morphological dilation to grow each element in the seed. A list of binary segmentations, segmented by a range of thresholds from high to low, is used to guide the dilation boundary. This approach regulates the growth of different elements, as binary segmentations from a high threshold have smaller areas to grow. As all seeds dilate at the same rate of expansion, this boundary layer prevents the overgrowth of one seed on top of another, acting as a stopper to growth and allowing the elements to maintain the required separation. This results in a skull with completely separated parts as the final product (<xref ref-type="fig" rid="F4">Fig. 4</xref>).</p></sec><sec id="S16"><title>Binary segmentations: deep learning generated images of planktonic foraminifera</title><p id="P53">As introduced in the seed generation section, SPROUT can also be applied to binary segmentations, which are simpler for deep learning and computer vision tasks as they are often used to distinguish background from the object of interest. In cases where specimens have a variable number of features, such as planktonic foraminifera with differing chamber counts, traditional deep learning techniques may struggle to segment datasets into individual regions accurately. Some chambers are connected together, so a direct disconnected component would not separate the chambers (<xref ref-type="fig" rid="F4">Fig. 4</xref>). SPROUT addresses this by parcellating whole regions within binary segmentations into distinct regions, enabling the extraction of individual features such as chambers even when the number of classes varies across the dataset.</p><p id="P54">As described above, SPROUT typically begins by generating a binary segmentation using a thresholding approach. Erosions and disconnected component analysis are then applied to create seeds with separated regions. In this process, binary segmentations can be treated as grayscale images, where setting the threshold to 1 allows for the use of erosions to separate attached components. For the foraminifera binary segmentation dataset, we set the threshold to 1 and found that ten erosions effectively separated all connected chambers. Although high erosion may remove small chambers due to size variation, SPROUT’s merged seed function effectively preserves them. This approach ensures consistent and fully automated input configurations for all foraminifera binary segmentations. Once regions are separated, dilation is applied to grow each region back to its original size. For the foraminifera dataset, each region that is maintained in the final erosion step is dilated ten times to match the number of erosions, ensuring no overall alteration in the size of each component.</p></sec></sec></sec><sec id="S17" sec-type="discussion"><title>Discussion</title><p id="P55">In this paper, we have presented SPROUT, a new tool for the efficient semi-automated segmentation of 3D-image data. We demonstrated SPROUT’s performance by applying it to diverse datasets from the biological, medical, and engineering sciences. Additionally, we compared the accuracy and time efficiency of SPROUT’s results with those obtained using another widely used semi-automated method—smart interpolation.</p><p id="P56">Deep learning methods are considered to be the state-of-the-art for many image segmentation tasks due to their ability to learn complex patterns and generalize across diverse datasets (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R49">49</xref>). Compared to deep learning-based methods, SPROUT does not require large scan datasets or labelled training images, making it more accessible for domain-specific applications. Many deep learning-based image segmentation methods rely on extensive training data (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R25">25</xref>). While pre-trained deep learning networks may need little to no task-specific training data (<xref ref-type="bibr" rid="R50">50</xref>), their performance can still be limited when applied to highly specialised tasks.</p><p id="P57">In recent years, semi- and self-supervised methods have emerged, allowing unlabeled target data to be utilized for generating high-quality segmentations (<xref ref-type="bibr" rid="R51">51</xref>–<xref ref-type="bibr" rid="R54">54</xref>). However, these approaches may face significant challenges when dealing with domain-specific images with complex segmentation tasks, particularly for 3D images. Due to the limited number of specimens or the expense of scanning them, achieving sufficiently large dataset sizes to support effective use of these tools can be challenging. For example, in natural history collections, a dataset of even a few hundred 3D scans is often considered to be large (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R47">47</xref>), but is insufficient for semi- or self-supervised deep learning. In addition, tasks requiring fine-grained, multi-class segmentation pose additional challenges. Creating high-quality training datasets is time-intensive and requires people with domain knowledge, and building and tuning deep learning models for such purposes can be technically demanding (<xref ref-type="bibr" rid="R55">55</xref>). Finally, in terms of computational costs, SPROUT requires minimal processing power and can be run on CPU RAM without requiring Graphics Processing Unit (GPU) usage, making it more accessible to users with average hardware capabilities.</p><p id="P58">As a non-AI method, SPROUT integrates classic computer vision techniques such as thresholding, morphological transformations, connected components, and region growing. These well-established methods result in straightforward, interpretable input configurations that are easy for users to set, thereby offering a clear advantage of foreseeing the result during method tuning in contrast to the more black-box design of deep learning-based predictions (<xref ref-type="bibr" rid="R56">56</xref>). With the useful features of merged seed generation and batch processing, SPROUT takes advantage of domain-specific datasets with patterns, enabling the pipeline to run automatically (as demonstrated with the dog limbs and planktonic foraminifera datasets detailed in Results). The implementation of parallelization further enhances the pipeline’s efficiency, making it suitable for processing large-scale datasets. SPROUT’s flexibility is also evident in its ability to handle diverse input data modalities (e.g., micro-CT, MRI, SEM), across a wide range of resolutions. Furthermore, it is explicitly designed to process datasets with any specified number of label classes, ensuring compatibility with a wide range of applications.</p><p id="P59">Additionally, the SPROUT pipeline is highly modular, being compatible in its implementation with imaging programs such as Avizo (Thermo Fisher Scientific, USA), Dragonfly (Object Research Systems, Canada), Fiji/ImageJ (<xref ref-type="bibr" rid="R57">57</xref>) and Meshlab (<xref ref-type="bibr" rid="R58">58</xref>). Each of these tools can be used for threshold selection, seed and growth checking and correction. As most of our workflow was implemented using Avizo for these steps, we include Avizo add-ons to make the pre-processing, loading, visualisation and segmentation editing in Avizo more efficient.</p><p id="P60">As demonstrated by our comparison using the dog limb dataset, SPROUT produces similar levels of accuracy to smart interpolation tools, such as that available in Biomedisa (<xref ref-type="bibr" rid="R35">35</xref>). However, interpolation requires the user to pre-segment slices from given image data, and the output will only include the elements that have been pre-segmented. By contrast, SPROUT requires minimal to no input data manipulation and dataset creation, making it unnecessary to use segmentation or visualisation software (e.g., Avizo, Dragonfly) before computation or at any point of the pipeline.</p><p id="P61">In addition, SPROUT is not only an alternative segmentation pipeline to deep learning methods, but can also serve as a tool for generating training data for deep learning models. This is particularly useful in scenarios where manual labeling of data is difficult or time-consuming. By leveraging SPROUT’s capability to produce accurate, fine-grained segmentations without requiring labelled data or expensive hardware, researchers can efficiently create high-quality labelled datasets, which can then be used to train deep learning models for complex tasks, such as multi-class segmentation.</p><p id="P62">In terms of SPROUT’s limitations, the generated outputs require assigning the correct segmentation component ID to each segmented element, as the raw SPROUT output IDs are currently ordered by component size. Therefore, class IDs have to be assigned manually. Several methods could help automate this process. Component IDs could be assigned using anatomical knowledge. For example, Brombacher et al. (<xref ref-type="bibr" rid="R59">59</xref>) assign IDs to foraminifera shell chambers based on the angles and sizes of each chamber.</p><p id="P63">Additionally, 3D image classification AI models could be trained to classify components into the correct IDs (<xref ref-type="bibr" rid="R60">60</xref>, <xref ref-type="bibr" rid="R61">61</xref>) based on appropriately labelled SPROUT outputs.</p><p id="P64">Memory limitations can also be a constraint when running SPROUT, as it requires loading the entire input image into memory for computation. This means that SPROUT cannot process scans larger than the available system memory. When using parallelization to process multiple scans simultaneously, the maximum size of the input image will decrease depending on the number of scans being processed at the same time. In contrast, the sliding window technique (<xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R63">63</xref>), is commonly employed in AI-based 3D image segmentation, allowing users to adjust window sizes to fit within the available GPU memory. However, RAM is much more affordable than GPU (<xref ref-type="bibr" rid="R64">64</xref>). For example, in the field of evolutionary morphology, most consumer-grade computers now are equipped with sufficient memory to handle the largest micro-CT scans of biological specimens. Consequently, memory size generally is not a significant limitation.</p><p id="P65">Ultimately, our paper demonstrates that SPROUT is a powerful tool for the efficient segmentation of 3D images. Through comparisons with other existing methods and demonstrations of its applications to various real data, we highlight the key advantages of this tool, as follows: (1) the ability to generate multi-class segmentations of 3D scans without requiring extensive datasets or training data; (2) provision of an efficient pipeline with easy-to-understand configurations, featuring tools such as merged seed generation, batch processing, and parallelization; (3) the flexibility to process a wide range of data types, resolutions, and variable label class numbers without significant alterations to the workflow or its speed. Furthermore, SPROUT is open-source, relies on common Python libraries, requires only moderate hardware capabilities, and features easily interpretable input configurations that demand limited domain knowledge, making it a significant advancement in the efficiency of 3D segmentation workflows. These advantages extend across multiple disciplines and data platforms, including biological, medical, geological, and engineering sciences, as well as potential applications in other, yet unforeseen fields. SPROUT can also serve as a valuable tool for generating training datasets for deep learning models, especially in scenarios where manually labelling data is time-consuming or impractical. By leveraging SPROUT’s ability to produce accurate, fine-grained segmentations without the need for expensive computational devices, researchers can create high-quality labelled datasets efficiently. These datasets can then be used to train deep learning models for more complex tasks, such as multi-class segmentation or domain-specific applications.</p></sec></body><back><ack id="S18"><title>Acknowledgements</title><p>We’d like to thank A. Sharp and E. Grisan for suggestions and technical insights which helped the development of SPROUT. We thank T. Macrini and G. Pandelis for contributing the aardvark and the skink scans to Digimorph and Morhosource, respectively. Foraminiferal specimens were scanned at the μ-VIS X-ray Imaging Centre at the University of Southampton. Dog scans courtesy of Liverpool Veterinary School Clinical Collaboration Platform, with thanks to T. Maddox. This work was funded by Leverhulme Trust grant RPG-2021-424, NERC grants NE/S007210/1 and NE/P019269/1, Royal Society APEX grant AA21\100124, and BBSRC grant BB/X014819/1. Ethics approval for dog scans granted by University of Liverpool Veterinary Research Ethics Committee (VREC), approval codes VREC1311 (non-greyhound breeds) and VREC1339 (greyhounds).</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cnudde</surname><given-names>V</given-names></name><name><surname>Boone</surname><given-names>MN</given-names></name></person-group><article-title>High-resolution X-ray computed tomography in geosciences: A review of the current technology and applications</article-title><source>Earth-Science Reviews</source><year>2013</year><volume>123</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Mulqueeney</surname><given-names>JM</given-names></name><name><surname>Watt</surname><given-names>EC</given-names></name><name><surname>Salili-James</surname><given-names>A</given-names></name><name><surname>Barber</surname><given-names>NS</given-names></name><name><surname>Camaiti</surname><given-names>M</given-names></name><name><surname>Hunt</surname><given-names>ESE</given-names></name><name><surname>Kippax-Chui</surname><given-names>O</given-names></name><name><surname>Knapp</surname><given-names>A</given-names></name><name><surname>Lanzetti</surname><given-names>A</given-names></name><name><surname>Rangel-de Lázaro</surname><given-names>G</given-names></name><etal/></person-group><article-title>Opportunities and Challenges in Applying AI to Evolutionary Morphology</article-title><source>Integrative Organismal Biology</source><year>2024</year><volume>6</volume><elocation-id>obae036</elocation-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedrick</surname><given-names>BP</given-names></name><name><surname>Heberling</surname><given-names>JM</given-names></name><name><surname>Meineke</surname><given-names>EK</given-names></name><name><surname>Turner</surname><given-names>KG</given-names></name><name><surname>Grassa</surname><given-names>CJ</given-names></name><name><surname>Park</surname><given-names>DS</given-names></name><name><surname>Kennedy</surname><given-names>J</given-names></name><name><surname>Clarke</surname><given-names>JA</given-names></name><name><surname>Cook</surname><given-names>JA</given-names></name><name><surname>Blackburn</surname><given-names>DC</given-names></name><name><surname>Edwards</surname><given-names>SV</given-names></name><etal/></person-group><article-title>Digitization and the Future of Natural History Collections</article-title><source>BioScience</source><year>2020</year><volume>70</volume><fpage>243</fpage><lpage>251</lpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaman</surname><given-names>RS</given-names></name><name><surname>Cellinese</surname><given-names>N</given-names></name></person-group><article-title>Mass digitization of scientific collections: New opportunities to transform the use of biological specimens and underwrite biodiversity science</article-title><source>ZooKeys</source><year>2012</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3406463</pub-id><pub-id pub-id-type="pmid">22859875</pub-id><pub-id pub-id-type="doi">10.3897/zookeys.209.3313</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blagoderov</surname><given-names>V</given-names></name><name><surname>Kitching</surname><given-names>I</given-names></name><name><surname>Livermore</surname><given-names>L</given-names></name><name><surname>Simonsen</surname><given-names>T</given-names></name><name><surname>Smith</surname><given-names>V</given-names></name></person-group><article-title>No specimen left behind: industrial scale digitization of natural history collections</article-title><source>ZK</source><year>2012</year><volume>209</volume><fpage>133</fpage><lpage>146</lpage><pub-id pub-id-type="pmcid">PMC3406472</pub-id><pub-id pub-id-type="pmid">22859884</pub-id><pub-id pub-id-type="doi">10.3897/zookeys.209.3178</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooney</surname><given-names>CR</given-names></name><name><surname>Bright</surname><given-names>JA</given-names></name><name><surname>Capp</surname><given-names>EJR</given-names></name><name><surname>Chira</surname><given-names>AM</given-names></name><name><surname>Hughes</surname><given-names>EC</given-names></name><name><surname>Moody</surname><given-names>CJA</given-names></name><name><surname>Nouri</surname><given-names>LO</given-names></name><name><surname>Varley</surname><given-names>ZK</given-names></name><name><surname>Thomas</surname><given-names>GH</given-names></name></person-group><article-title>Erratum: Corrigendum: Mega-evolutionary dynamics of the adaptive radiation of birds</article-title><source>Nature</source><year>2017</year><volume>552</volume><fpage>430</fpage><pub-id pub-id-type="pmid">29186123</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Medina</surname><given-names>JJ</given-names></name><name><surname>Maley</surname><given-names>JM</given-names></name><name><surname>Sannapareddy</surname><given-names>S</given-names></name><name><surname>Medina</surname><given-names>NN</given-names></name><name><surname>Gilman</surname><given-names>CM</given-names></name><name><surname>McCormack</surname><given-names>JE</given-names></name></person-group><article-title>A rapid and cost-effective pipeline for digitization of museum specimens with 3D photogrammetry</article-title><source>PLoS ONE</source><year>2020</year><volume>15</volume><elocation-id>e0236417</elocation-id><pub-id pub-id-type="pmcid">PMC7425849</pub-id><pub-id pub-id-type="pmid">32790700</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0236417</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulqueeney</surname><given-names>JM</given-names></name><name><surname>Searle-Barnes</surname><given-names>A</given-names></name><name><surname>Brombacher</surname><given-names>A</given-names></name><name><surname>Sweeney</surname><given-names>M</given-names></name><name><surname>Goswami</surname><given-names>A</given-names></name><name><surname>Ezard</surname><given-names>THG</given-names></name></person-group><article-title>How many specimens make a sufficient training set for automated three-dimensional feature extraction?</article-title><source>R Soc Open Sci</source><year>2024</year><volume>11</volume><elocation-id>rsos.240113</elocation-id><pub-id pub-id-type="pmcid">PMC11296157</pub-id><pub-id pub-id-type="pmid">39100182</pub-id><pub-id pub-id-type="doi">10.1098/rsos.240113</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyer</surname><given-names>DM</given-names></name><name><surname>Gunnell</surname><given-names>GF</given-names></name><name><surname>Kaufman</surname><given-names>S</given-names></name><name><surname>McGeary</surname><given-names>TM</given-names></name></person-group><article-title>MORPHOSOURCE: ARCHIVING AND SHARING 3-D DIGITAL SPECIMEN DATA</article-title><source>Paleontol Soc Pap</source><year>2016</year><volume>22</volume><fpage>157</fpage><lpage>181</lpage></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaman</surname><given-names>RS</given-names></name><name><surname>Cellinese</surname><given-names>N</given-names></name></person-group><article-title>Mass digitization of scientific collections: New opportunities to transform the use of biological specimens and underwrite biodiversity science</article-title><source>ZooKeys</source><year>2012</year><volume>7</volume></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davies</surname><given-names>TG</given-names></name><name><surname>Rahman</surname><given-names>IA</given-names></name><name><surname>Lautenschlager</surname><given-names>S</given-names></name><name><surname>Cunningham</surname><given-names>JA</given-names></name><name><surname>Asher</surname><given-names>RJ</given-names></name><name><surname>Barrett</surname><given-names>PM</given-names></name><name><surname>Bates</surname><given-names>KT</given-names></name><name><surname>Bengtson</surname><given-names>S</given-names></name><name><surname>Benson</surname><given-names>RBJ</given-names></name><name><surname>Boyer</surname><given-names>DM</given-names></name><name><surname>Braga</surname><given-names>J</given-names></name><etal/></person-group><article-title>Open data and digital morphology</article-title><year>2017</year><source>Proceedings of the Royal Society B: Biological Sciences</source><volume>284</volume><elocation-id>20170194</elocation-id><pub-id pub-id-type="pmcid">PMC5394671</pub-id><pub-id pub-id-type="pmid">28404779</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2017.0194</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsiang</surname><given-names>AY</given-names></name><name><surname>Brombacher</surname><given-names>A</given-names></name><name><surname>Rillo</surname><given-names>MC</given-names></name><name><surname>Mleneck-Vautravers</surname><given-names>MJ</given-names></name><name><surname>Conn</surname><given-names>S</given-names></name><name><surname>Lordsmith</surname><given-names>S</given-names></name><name><surname>Jentzen</surname><given-names>A</given-names></name><name><surname>Henehan</surname><given-names>MJ</given-names></name><name><surname>Metcalfe</surname><given-names>B</given-names></name><name><surname>Fenton</surname><given-names>IS</given-names></name><name><surname>Wade</surname><given-names>BS</given-names></name><etal/></person-group><article-title>Endless Forams: &gt;34,000 Modern Planktonic Foraminiferal Images for Taxonomic Training and Automated Species Recognition Using Convolutional Neural Networks</article-title><source>Paleoceanog and Paleoclimatol</source><year>2019</year><volume>34</volume><fpage>1157</fpage><lpage>1177</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lürig</surname><given-names>MD</given-names></name><name><surname>Donoughe</surname><given-names>S</given-names></name><name><surname>Svensson</surname><given-names>EI</given-names></name><name><surname>Porto</surname><given-names>A</given-names></name><name><surname>Tsuboi</surname><given-names>M</given-names></name></person-group><article-title>Computer Vision, Machine Learning, and the Promise of Phenomics in Ecology and Evolutionary Biology</article-title><source>Front Ecol Evol</source><year>2021</year><volume>9</volume><elocation-id>642774</elocation-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><source>U-net: Convolutional networks for biomedical image segmentation</source><conf-name>Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015. MICCAI 2015</conf-name><series>Lecture Notes in Computer Science</series><conf-sponsor>Springer</conf-sponsor><conf-loc>Munich, Germany</conf-loc><year>2015</year><volume>9351</volume><fpage>234</fpage><lpage>241</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>ST</given-names></name><name><surname>Alfaro</surname><given-names>ME</given-names></name></person-group><article-title><italic>Sashimi</italic>: A toolkit for facilitating high-throughput organismal image segmentation using deep learning</article-title><source>Methods Ecol Evol</source><year>2021</year><volume>12</volume><fpage>2341</fpage><lpage>2354</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litjens</surname><given-names>G</given-names></name><name><surname>Kooi</surname><given-names>T</given-names></name><name><surname>Bejnordi</surname><given-names>BE</given-names></name><name><surname>Setio</surname><given-names>AAA</given-names></name><name><surname>Ciompi</surname><given-names>F</given-names></name><name><surname>Ghafoorian</surname><given-names>M</given-names></name><name><surname>Van Der Laak</surname><given-names>JAWM</given-names></name><name><surname>Van Ginneken</surname><given-names>B</given-names></name><name><surname>Sánchez</surname><given-names>CI</given-names></name></person-group><article-title>A survey on deep learning in medical image analysis</article-title><source>Medical Image Analysis</source><year>2017</year><volume>42</volume><fpage>60</fpage><lpage>88</lpage><pub-id pub-id-type="pmid">28778026</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>HD</given-names></name><name><surname>Jiang</surname><given-names>XH</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name></person-group><article-title>Color image segmentation: advances and prospects</article-title><source>Pattern Recognition</source><year>2001</year><volume>34</volume><fpage>2259</fpage><lpage>2281</lpage></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joskowicz</surname><given-names>L</given-names></name><name><surname>Cohen</surname><given-names>D</given-names></name><name><surname>Caplan</surname><given-names>N</given-names></name><name><surname>Sosna</surname><given-names>J</given-names></name></person-group><article-title>Inter-observer variability of manual contour delineation of structures in CT</article-title><source>Eur Radiol</source><year>2019</year><volume>29</volume><fpage>1391</fpage><lpage>1399</lpage><pub-id pub-id-type="pmid">30194472</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>M</given-names></name><name><surname>Gilani</surname><given-names>SO</given-names></name><name><surname>Waris</surname><given-names>A</given-names></name><name><surname>Zafar</surname><given-names>K</given-names></name><name><surname>Jamil</surname><given-names>M</given-names></name></person-group><article-title>Brain Tumour Image Segmentation Using Deep Networks</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>153589</fpage><lpage>153598</lpage></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramesh</surname><given-names>KKD</given-names></name><name><surname>Kumar</surname><given-names>GK</given-names></name><name><surname>Swapna</surname><given-names>K</given-names></name><name><surname>Datta</surname><given-names>D</given-names></name><name><surname>Rajest</surname><given-names>SS</given-names></name></person-group><article-title>A Review of Medical Image Segmentation Algorithms</article-title><source>EAI Endorsed Trans Perv Health Tech</source><year>2021</year><volume>7</volume><fpage>e6</fpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Wan</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>A Mechanical Parts Image Segmentation Method Against Illumination for Industry</article-title><year>2020</year><comment>In Review [Preprint]</comment><pub-id pub-id-type="doi">10.21203/rs.3.rs-119471/v1</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Hong</surname><given-names>J</given-names></name></person-group><article-title>Semantic segmentation of mechanical assembly using selective kernel convolution UNet with fully connected conditional random field</article-title><source>Measurement</source><year>2023</year><volume>209</volume><elocation-id>112499</elocation-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guntoro</surname><given-names>P</given-names></name><name><surname>Ghorbani</surname><given-names>Y</given-names></name><name><surname>Koch</surname><given-names>P-H</given-names></name><name><surname>Rosenkranz</surname><given-names>J</given-names></name></person-group><article-title>X-ray Microcomputed Tomography (pCT) for Mineral Characterization: A Review of Data Analysis Methods</article-title><source>Minerals</source><year>2019</year><volume>9</volume><fpage>183</fpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Safari</surname><given-names>H</given-names></name><name><surname>Balcom</surname><given-names>BJ</given-names></name><name><surname>Afrough</surname><given-names>A</given-names></name></person-group><article-title>Characterization of pore and grain size distributions in porous geological samples - An image processing workflow</article-title><source>Computers &amp; Geosciences</source><year>2021</year><volume>156</volume><elocation-id>104895</elocation-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Milletari</surname><given-names>F</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Ahmadi</surname><given-names>S-A</given-names></name></person-group><source>V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</source><year>2016</year><conf-name>2016 Fourth International Conference on 3D Vision (3DV)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Stanford, CA, USA</conf-loc><fpage>565</fpage><lpage>571</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/7785132/">http://ieeexplore.ieee.org/document/7785132/</ext-link></comment></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cardoso</surname><given-names>MJ</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Brown</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>N</given-names></name><name><surname>Kerfoot</surname><given-names>E</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Murrey</surname><given-names>B</given-names></name><name><surname>Myronenko</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>D</given-names></name><name><surname>Nath</surname><given-names>V</given-names></name><etal/></person-group><article-title>MONAI: An open-source framework for deep learning in healthcare</article-title><source>arXiv [Preprint]</source><year>2022</year><pub-id pub-id-type="doi">10.48550/ARXIV.2211.02701</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isensee</surname><given-names>F</given-names></name><name><surname>Jaeger</surname><given-names>PF</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Petersen</surname><given-names>J</given-names></name><name><surname>Maier-Hein</surname><given-names>KH</given-names></name></person-group><article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nat Methods</source><year>2021</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="pmid">33288961</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bardis</surname><given-names>M</given-names></name><name><surname>Houshyar</surname><given-names>R</given-names></name><name><surname>Chantaduly</surname><given-names>C</given-names></name><name><surname>Ushinsky</surname><given-names>A</given-names></name><name><surname>Glavis-Bloom</surname><given-names>J</given-names></name><name><surname>Shaver</surname><given-names>M</given-names></name><name><surname>Chow</surname><given-names>D</given-names></name><name><surname>Uchio</surname><given-names>E</given-names></name><name><surname>Chang</surname><given-names>P</given-names></name></person-group><article-title>Deep Learning with Limited Data: Organ Segmentation Performance by U-Net</article-title><source>Electronics</source><year>2020</year><volume>9</volume><fpage>1199</fpage></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayana</surname><given-names>PA</given-names></name><name><surname>Coronado</surname><given-names>I</given-names></name><name><surname>Sujit</surname><given-names>SJ</given-names></name><name><surname>Wolinsky</surname><given-names>JS</given-names></name><name><surname>Lublin</surname><given-names>FD</given-names></name><name><surname>Gabr</surname><given-names>RE</given-names></name></person-group><article-title>DeepLearning-Based Neural Tissue Segmentation of MRI in Multiple Sclerosis: Effect of Training Set Size</article-title><source>Magnetic Resonance Imaging</source><year>2020</year><volume>51</volume><fpage>1487</fpage><lpage>1496</lpage><pub-id pub-id-type="pmcid">PMC7165037</pub-id><pub-id pub-id-type="pmid">31625650</pub-id><pub-id pub-id-type="doi">10.1002/jmri.26959</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelvecchi</surname><given-names>D</given-names></name></person-group><article-title>Can we open the black box of AI?</article-title><source>Nature</source><year>2016</year><volume>538</volume><fpage>20</fpage><lpage>23</lpage><pub-id pub-id-type="pmid">27708329</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shwartz-Ziv</surname><given-names>R</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><article-title>Opening the Black Box of Deep Neural Networks via Information</article-title><source>arXiv [Preprint]</source><year>2017</year><pub-id pub-id-type="doi">10.48550/ARXIV.1703.00810</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Ratner</surname><given-names>A</given-names></name><name><surname>De Sa</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Selsam</surname><given-names>D</given-names></name><name><surname>Ré</surname><given-names>C</given-names></name></person-group><source>Data Programming: Creating Large Training Sets, Quickly</source><year>2016</year><pub-id pub-id-type="doi">10.48550/ARXIV.1605.07723</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elyan</surname><given-names>E</given-names></name><name><surname>Vuttipittayamongkol</surname><given-names>P</given-names></name><name><surname>Johnston</surname><given-names>P</given-names></name><name><surname>Martin</surname><given-names>K</given-names></name><name><surname>McPherson</surname><given-names>K</given-names></name><name><surname>Moreno-García</surname><given-names>CF</given-names></name><name><surname>Jayne</surname><given-names>C</given-names><suffix>Md</suffix></name></person-group><article-title>Mostafa Kamal Sarker, Computer vision and machine learning for medical image analysis: recent advances, challenges, and way forward</article-title><source>Art Int Surg</source><year>2022</year><pub-id pub-id-type="doi">10.20517/ais.2021.15</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Al-amri</surname><given-names>SS</given-names></name><name><surname>Kalyankar</surname><given-names>NV</given-names></name><name><surname>D</surname><given-names>KS</given-names></name></person-group><source>Image Segmentation by Using Threshold Techniques</source><year>2010</year><pub-id pub-id-type="doi">10.48550/ARXIV.1005.4020</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lõsel</surname><given-names>PD</given-names></name><name><surname>Van De Kamp</surname><given-names>T</given-names></name><name><surname>Jayme</surname><given-names>A</given-names></name><name><surname>Ershov</surname><given-names>A</given-names></name><name><surname>Faragó</surname><given-names>T</given-names></name><name><surname>Pichler</surname><given-names>O</given-names></name><name><surname>Tan Jerome</surname><given-names>N</given-names></name><name><surname>Aadepu</surname><given-names>N</given-names></name><name><surname>Bremer</surname><given-names>S</given-names></name><name><surname>Chilingaryan</surname><given-names>SA</given-names></name><name><surname>Heethoff</surname><given-names>M</given-names></name><etal/></person-group><article-title>Introducing Biomedisa as an open-source online platform for biomedical image segmentation</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><fpage>5577</fpage><pub-id pub-id-type="pmcid">PMC7642381</pub-id><pub-id pub-id-type="pmid">33149150</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-19303-w</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramos-Soto</surname><given-names>O</given-names></name><name><surname>Rodríguez-Esparza</surname><given-names>E</given-names></name><name><surname>Balderas-Mata</surname><given-names>SE</given-names></name><name><surname>Oliva</surname><given-names>D</given-names></name><name><surname>Hassanien</surname><given-names>AE</given-names></name><name><surname>Meleppat</surname><given-names>RK</given-names></name><name><surname>Zawadzki</surname><given-names>RJ</given-names></name></person-group><article-title>An efficient retinal blood vessel segmentation in eye fundus images by using optimized top-hat and homomorphic filtering</article-title><source>Computer Methods and Programs in Biomedicine</source><year>2021</year><volume>201</volume><elocation-id>105949</elocation-id><pub-id pub-id-type="pmid">33567382</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grau</surname><given-names>V</given-names></name><name><surname>Mewes</surname><given-names>AUJ</given-names></name><name><surname>Alcaniz</surname><given-names>M</given-names></name><name><surname>Kikinis</surname><given-names>R</given-names></name><name><surname>Warfield</surname><given-names>SK</given-names></name></person-group><article-title>Improved Watershed Transform for Medical Image Segmentation Using Prior Information</article-title><source>IEEE Trans Med Imaging</source><year>2004</year><volume>23</volume><fpage>447</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">15084070</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parascandolo</surname><given-names>P</given-names></name><name><surname>Cesario</surname><given-names>L</given-names></name><name><surname>Vosilla</surname><given-names>L</given-names></name><name><surname>Pitikakis</surname><given-names>M</given-names></name><name><surname>Viano</surname><given-names>G</given-names></name></person-group><source>Smart Brush: A real time segmentation tool for 3D medical images</source><year>2013</year><conf-name>2013 8th International Symposium on Image and Signal Processing and Analysis (ISPA)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Trieste, Italy</conf-loc><fpage>689</fpage><lpage>694</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/6703826/">http://ieeexplore.ieee.org/document/6703826/</ext-link></comment></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Khalid</surname><given-names>NEA</given-names></name><name><surname>Ibrahim</surname><given-names>S</given-names></name><name><surname>Manaf</surname><given-names>M</given-names></name><name><surname>Ngah</surname><given-names>UK</given-names></name></person-group><source>Seed-based region growing study for brain abnormalities segmentation</source><year>2010</year><conf-name>2010 International Symposium on Information Technology</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Kuala Lumpur, Malaysia</conf-loc><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/5561560/">http://ieeexplore.ieee.org/document/5561560/</ext-link></comment><fpage>856</fpage><lpage>860</lpage></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Mahony</surname><given-names>N</given-names></name><name><surname>Campbell</surname><given-names>S</given-names></name><name><surname>Carvalho</surname><given-names>A</given-names></name><name><surname>Harapanahalli</surname><given-names>S</given-names></name><name><surname>Hernandez</surname><given-names>GV</given-names></name><name><surname>Krpalkova</surname><given-names>L</given-names></name><name><surname>Riordan</surname><given-names>D</given-names></name><name><surname>Walsh</surname><given-names>J</given-names></name></person-group><chapter-title>Deep Learning vs. Traditional Computer Vision</chapter-title><person-group person-group-type="editor"><name><surname>Arai</surname><given-names>K</given-names></name><name><surname>Kapoor</surname><given-names>S</given-names></name></person-group><source>Advances in Computer Vision</source><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc><volume>943</volume><series>Advances in Intelligent Systems and Computing</series><year>2020</year><fpage>128</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-17795-9_10</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Didziokas</surname><given-names>M</given-names></name><name><surname>Pauws</surname><given-names>E</given-names></name><name><surname>Kõlby</surname><given-names>L</given-names></name><name><surname>Khonsari</surname><given-names>RH</given-names></name><name><surname>Moazen</surname><given-names>M</given-names></name></person-group><article-title>BounTI (boundary-preserving threshold iteration): A user-friendly tool for automatic hard tissue segmentation</article-title><source>Journal of Anatomy</source><year>2024</year><volume>245</volume><fpage>829</fpage><lpage>841</lpage><pub-id pub-id-type="pmcid">PMC11547236</pub-id><pub-id pub-id-type="pmid">38760955</pub-id><pub-id pub-id-type="doi">10.1111/joa.14063</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedli</surname><given-names>L</given-names></name><name><surname>Kloukos</surname><given-names>D</given-names></name><name><surname>Kanavakis</surname><given-names>G</given-names></name><name><surname>Halazonetis</surname><given-names>D</given-names></name><name><surname>Gkantidis</surname><given-names>N</given-names></name></person-group><article-title>The effect of threshold level on bone segmentation of cranial base structures from CT and CBCT images</article-title><source>Sci Rep</source><year>2020</year><volume>10</volume><fpage>7361</fpage><pub-id pub-id-type="pmcid">PMC7193643</pub-id><pub-id pub-id-type="pmid">32355261</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-64383-9</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adams</surname><given-names>R</given-names></name><name><surname>Bischof</surname><given-names>L</given-names></name></person-group><article-title>Seeded region growing</article-title><source>IEEE Trans Pattern Anal Machine Intell</source><year>1994</year><volume>16</volume><fpage>641</fpage><lpage>647</lpage></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pieper</surname><given-names>S</given-names></name><name><surname>Halle</surname><given-names>M</given-names></name><name><surname>Kikinis</surname><given-names>R</given-names></name></person-group><source>3D Slicer</source><year>2004</year><conf-name>2004 2nd IEEE International Symposium on Biomedical Imaging: Macro to Nano (IEEE Cat No. 04EX821)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Arlington, VA, USA</conf-loc><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/1398617/">http://ieeexplore.ieee.org/document/1398617/</ext-link></comment><volume>2</volume><fpage>632</fpage><lpage>635</lpage></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maire</surname><given-names>E</given-names></name><name><surname>Withers</surname><given-names>PJ</given-names></name></person-group><article-title>Quantitative X-ray tomography</article-title><source>International Materials Reviews</source><year>2014</year><volume>59</volume><fpage>1</fpage><lpage>43</lpage></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Withers</surname><given-names>PJ</given-names></name><name><surname>Bouman</surname><given-names>C</given-names></name><name><surname>Carmignato</surname><given-names>S</given-names></name><name><surname>Cnudde</surname><given-names>V</given-names></name><name><surname>Grimaldi</surname><given-names>D</given-names></name><name><surname>Hagen</surname><given-names>CK</given-names></name><name><surname>Maire</surname><given-names>E</given-names></name><name><surname>Manley</surname><given-names>M</given-names></name><name><surname>Du Plessis</surname><given-names>A</given-names></name><name><surname>Stock</surname><given-names>SR</given-names></name></person-group><article-title>X-ray computed tomography</article-title><source>Nat Rev Methods Primers</source><year>2021</year><volume>1</volume><fpage>18</fpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>EC</given-names></name><name><surname>Edwards</surname><given-names>DP</given-names></name><name><surname>Bright</surname><given-names>JA</given-names></name><name><surname>Capp</surname><given-names>EJR</given-names></name><name><surname>Cooney</surname><given-names>CR</given-names></name><name><surname>Varley</surname><given-names>ZK</given-names></name><name><surname>Thomas</surname><given-names>GH</given-names></name></person-group><article-title>Global biogeographic patterns of avian morphological diversity</article-title><source>Ecology Letters</source><year>2022</year><volume>25</volume><fpage>598</fpage><lpage>610</lpage><pub-id pub-id-type="pmid">35199925</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lõsel</surname><given-names>P</given-names></name><name><surname>Heuveline</surname><given-names>V</given-names></name></person-group><chapter-title>Enhancing a diffusion algorithm for 4D image segmentation using local information</chapter-title><source>Medical Imaging 2016: Image Processing</source><publisher-name>SPIE</publisher-name><year>2016</year><volume>9784</volume><fpage>707</fpage><lpage>717</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9784/97842L/Enhancing-a-diffusion-algorithm-for-4D-image-segmentation-using-local/10.1117/12.2216202.full">https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9784/97842L/Enhancing-a-diffusion-algorithm-for-4D-image-segmentation-using-local/10.1117/12.2216202.full</ext-link></comment></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirillov</surname><given-names>A</given-names></name><name><surname>Mintun</surname><given-names>E</given-names></name><name><surname>Ravi</surname><given-names>N</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Rolland</surname><given-names>C</given-names></name><name><surname>Gustafson</surname><given-names>L</given-names></name><name><surname>Xiao</surname><given-names>T</given-names></name><name><surname>Whitehead</surname><given-names>S</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Lo</surname><given-names>W-Y</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><etal/></person-group><article-title>Segment Anything</article-title><source>arXiv [Preprint]</source><year>2023</year><pub-id pub-id-type="doi">10.48550/ARXIV.2304.02643</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><article-title>Deep Residual Learning for Image Recognition</article-title><source>arXiv [Preprint]</source><year>2015</year><pub-id pub-id-type="doi">10.48550/ARXIV.1512.03385</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>D</given-names></name><name><surname>Dai</surname><given-names>J</given-names></name><name><surname>Jia</surname><given-names>J</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><source>ScribbleSup: Scribble-Supervised Convolutional Networks for Semantic Segmentation</source><year>2016</year><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Las Vegas, NV, USA</conf-loc><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/7780713/">http://ieeexplore.ieee.org/document/7780713/</ext-link></comment><fpage>3159</fpage><lpage>3167</lpage></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>A Simple Framework for Contrastive Learning of Visual Representations</article-title><source>arXiv [Preprint]</source><year>2020</year><pub-id pub-id-type="doi">10.48550/ARXIV.2002.05709</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name></person-group><article-title>Masked Autoencoders Are Scalable Vision Learners</article-title><source>arXiv [Preprint]</source><year>2021</year><pub-id pub-id-type="doi">10.48550/ARXIV.2111.06377</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Shan</surname><given-names>D</given-names></name><name><surname>Hong</surname><given-names>Q</given-names></name></person-group><source>ScribbleVC: Scribble-supervised Medical Image Segmentation with Vision-Class Embedding</source><year>2023</year><conf-name>Proceedings of the 31st ACM International Conference on Multimedia</conf-name><conf-sponsor>ACM</conf-sponsor><conf-loc>Ottawa ON Canada</conf-loc><fpage>3384</fpage><lpage>3393</lpage><pub-id pub-id-type="doi">10.1145/3581783.3612056</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Z</given-names></name><name><surname>Siddiquee</surname><given-names>MMR</given-names></name><name><surname>Tajbakhsh</surname><given-names>N</given-names></name><name><surname>Liang</surname><given-names>J</given-names></name></person-group><article-title>UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation</article-title><source>IEEE Trans Med Imaging</source><year>2020</year><volume>39</volume><fpage>1856</fpage><lpage>1867</lpage><pub-id pub-id-type="pmcid">PMC7357299</pub-id><pub-id pub-id-type="pmid">31841402</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2019.2959609</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salahuddin</surname><given-names>Z</given-names></name><name><surname>Woodruff</surname><given-names>HC</given-names></name><name><surname>Chatterjee</surname><given-names>A</given-names></name><name><surname>Lambin</surname><given-names>P</given-names></name></person-group><article-title>Transparency of deep neural networks for medical image analysis: A review of interpretability methods</article-title><source>Computers in Biology and Medicine</source><year>2022</year><volume>140</volume><elocation-id>105111</elocation-id><pub-id pub-id-type="pmid">34891095</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>TJ</given-names></name></person-group><article-title>ImageJ for Microscopy</article-title><source>BioTechniques</source><year>2007</year><volume>43</volume><fpage>5</fpage><pub-id pub-id-type="pmid">17936939</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cignoni</surname><given-names>P</given-names></name><name><surname>Callieri</surname><given-names>M</given-names></name><name><surname>Corsini</surname><given-names>M</given-names></name><name><surname>Dellepiane</surname><given-names>M</given-names></name><name><surname>Ganovelli</surname><given-names>F</given-names></name><name><surname>Ranzuglia</surname><given-names>G</given-names></name></person-group><source>MeshLab: an Open-Source Mesh Processing Tool</source><publisher-name>The Eurographics Association [Preprint]</publisher-name><year>2008</year><pub-id pub-id-type="doi">10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brombacher</surname><given-names>A</given-names></name><name><surname>Searle-Barnes</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Ezard</surname><given-names>THG</given-names></name></person-group><article-title>Analysing planktonic foraminiferal growth in three dimensions with foram3D: an R package for automated trait measurements from CT scans</article-title><source>J Micropalaeontol</source><year>2022</year><volume>41</volume><fpage>149</fpage><lpage>164</lpage></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>HR</given-names></name><name><surname>Lee</surname><given-names>CT</given-names></name><name><surname>Shin</surname><given-names>H-C</given-names></name><name><surname>Seff</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>L</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Summers</surname><given-names>RM</given-names></name></person-group><source>Anatomy specific classification of medical images using deep convolutional nets</source><year>2015</year><conf-name>2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)</conf-name><conf-sponsor>IEEE</conf-sponsor><conf-loc>Brooklyn, NY, USA</conf-loc><fpage>101</fpage><lpage>104</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://ieeexplore.ieee.org/document/7163826/">http://ieeexplore.ieee.org/document/7163826/</ext-link></comment></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raffy</surname><given-names>P</given-names></name><name><surname>Pambrun</surname><given-names>J-F</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Dubois</surname><given-names>D</given-names></name><name><surname>Patti</surname><given-names>JW</given-names></name><name><surname>Cairns</surname><given-names>RA</given-names></name><name><surname>Young</surname><given-names>R</given-names></name></person-group><article-title>Deep Learning Body Region Classification of MRI and CT Examinations</article-title><source>J Digit Imaging</source><year>2023</year><volume>36</volume><fpage>1291</fpage><lpage>1301</lpage><pub-id pub-id-type="pmcid">PMC10407003</pub-id><pub-id pub-id-type="pmid">36894697</pub-id><pub-id pub-id-type="doi">10.1007/s10278-022-00767-9</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koç</surname><given-names>CK</given-names></name></person-group><article-title>Analysis of sliding window techniques for exponentiation</article-title><source>Computers &amp; Mathematics with Applications</source><year>1995</year><volume>30</volume><fpage>17</fpage><lpage>24</lpage></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serag</surname><given-names>A</given-names></name><name><surname>Wilkinson</surname><given-names>AG</given-names></name><name><surname>Telford</surname><given-names>EJ</given-names></name><name><surname>Pataky</surname><given-names>R</given-names></name><name><surname>Sparrow</surname><given-names>SA</given-names></name><name><surname>Anblagan</surname><given-names>D</given-names></name><name><surname>Macnaught</surname><given-names>G</given-names></name><name><surname>Semple</surname><given-names>SI</given-names></name><name><surname>Boardman</surname><given-names>JP</given-names></name></person-group><article-title>SEGMA: An Automatic SEGMentation Approach for Human Brain MRI Using Sliding Window and Random Forests</article-title><source>Front Neuroinform</source><year>2017</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC5247463</pub-id><pub-id pub-id-type="pmid">28163680</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2017.00002</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owens</surname><given-names>JD</given-names></name><name><surname>Houston</surname><given-names>M</given-names></name><name><surname>Luebke</surname><given-names>D</given-names></name><name><surname>Green</surname><given-names>S</given-names></name><name><surname>Stone</surname><given-names>JE</given-names></name><name><surname>Phillips</surname><given-names>JC</given-names></name></person-group><article-title>GPU Computing</article-title><source>Proc IEEE</source><year>2008</year><volume>96</volume><fpage>879</fpage><lpage>899</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>A schematic flowchart describing the sequential steps of the SPROUT process, using a mammalian skull as an example. Seeds are created from an initial 3D input on a given threshold value for maximum separation, then grown to a target threshold, obtaining a segmented object as the output.</p></caption><graphic xlink:href="EMS200759-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Application of the SPROUT workflow to segment the forelimb of a French bulldog specimen 256334 (left side) from a medical CT scan.</title></caption><graphic xlink:href="EMS200759-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Boxplot of the DICE scores comparing the ground truth (manual correction of the Biomedisa-generated smart interpolation) segmentations with raw outputs of the Biomedisa smart interpolation, and those of the SPROUT segmentation.</title></caption><graphic xlink:href="EMS200759-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Volumetric representations of the image stacks, merged seeds, and grown results of SPROUT segmentations, using micro-CT, MRI, and binary segmentation inputs. Specimen details are in <xref ref-type="table" rid="T2">Table 2</xref>.</p></caption><graphic xlink:href="EMS200759-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Information on the dog specimens used for the assessment of SPROUT’s performance compared with other segmentation methods.</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="top">Specimen number</th><th align="left" valign="top">Breed</th><th align="left" valign="top">Side</th><th align="left" valign="top">Voxel size x, y (mm)</th><th align="left" valign="top">Voxel size z (mm)</th></tr></thead><tbody><tr><td align="left" valign="top">200000</td><td align="left" valign="top">Greyhound</td><td align="left" valign="top">Left</td><td align="left" valign="top">0.7578125</td><td align="left" valign="top">1</td></tr><tr><td align="left" valign="top">200000</td><td align="left" valign="top">Greyhound</td><td align="left" valign="top">Right</td><td align="left" valign="top">0.7578125</td><td align="left" valign="top">1</td></tr><tr><td align="left" valign="top">242343</td><td align="left" valign="top">Border Collie</td><td align="left" valign="top">Left</td><td align="left" valign="top">0.401</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">242343</td><td align="left" valign="top">Border Collie</td><td align="left" valign="top">Right</td><td align="left" valign="top">0.401</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">255544</td><td align="left" valign="top">French Bulldog</td><td align="left" valign="top">Right</td><td align="left" valign="top">0.273</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">256334</td><td align="left" valign="top">French Bulldog</td><td align="left" valign="top">Left</td><td align="left" valign="top">0.698</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">256334</td><td align="left" valign="top">French Bulldog</td><td align="left" valign="top">Right</td><td align="left" valign="top">0.698</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">257098</td><td align="left" valign="top">Dachshund</td><td align="left" valign="top">Left</td><td align="left" valign="top">0.415</td><td align="left" valign="top">0.3</td></tr><tr><td align="left" valign="top">257098</td><td align="left" valign="top">Dachshund</td><td align="left" valign="top">Right</td><td align="left" valign="top">0.415</td><td align="left" valign="top">0.3</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Sample, source, scan details, and seed threshold values for the example datasets used to demonstrate SPROUT’s applicability to diverse image data and data modalities.</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="top">Scan type</th><th align="left" valign="top">Object</th><th align="left" valign="top">Specim en number</th><th align="left" valign="top">Voxel size x, y (mm)</th><th align="left" valign="top">Voxel size z (mm)</th><th align="left" valign="top">Seed thresholds</th><th align="left" valign="top">Source</th></tr></thead><tbody><tr><td align="left" valign="top">Micro-CT</td><td align="left" valign="top">Whole specimen: <italic>Tiliqua scincoides</italic></td><td align="left" valign="top">CAS:HE RP:254 658</td><td align="left" valign="top">0.120954</td><td align="left" valign="top">0.120954</td><td align="left" valign="top">100 - 220</td><td align="left" valign="top">California Academy of Sciences &amp; oVert TCN (downloaded from Morphosource) ark:/87602/m4/M74717</td></tr><tr><td align="left" valign="top">Micro-CT</td><td align="left" valign="top">Partial specimen: <italic>Orycteropus afer</italic></td><td align="left" valign="top">AMNH 51909</td><td align="left" valign="top">0.202</td><td align="left" valign="top">0.202</td><td align="left" valign="top">173, 181</td><td align="left" valign="top">The University of Texas High-Resolution X-ray CT Facility &amp; American Museum of Natural History (dowloaded from Digimorph)</td></tr><tr><td align="left" valign="top">Binary segmentation</td><td align="left" valign="top">Planktonic Foraminifera (<italic>Menardella limbata</italic>)</td><td align="left" valign="top">st014_bl4_fo1</td><td align="left" valign="top">0.00175</td><td align="left" valign="top">0.00175</td><td align="left" valign="top">1</td><td align="left" valign="top">Ocean Drilling Program (ODP) Site 925, μ-VIS X-ray Imaging Centre, University of Southampton, UK</td></tr><tr><td align="left" valign="top">MRI</td><td align="left" valign="top">Human heart</td><td align="left" valign="top">pat0</td><td align="left" valign="top">0.9</td><td align="left" valign="top">0.9</td><td align="left" valign="top">40 - 120</td><td align="left" valign="top">Pace, D. F. et al. (2015) 10.1007/978-3-319-24574-4_10</td></tr><tr><td align="left" valign="top">Micro-CT</td><td align="left" valign="top">Non-biological: Steel reinforced concrete</td><td align="left" valign="top">N/A</td><td align="left" valign="top">0.0020998</td><td align="left" valign="top">0.0020998</td><td align="left" valign="top">70 - 140</td><td align="left" valign="top">Hampe, M. (2024) <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.10785685">https://doi.org/10.5281/zenodo.10785685</ext-link></td></tr></tbody></table></table-wrap></floats-group></article>