<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200379</article-id><article-id pub-id-type="doi">10.1101/2024.11.19.624267</article-id><article-id pub-id-type="archive">PPR942647</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Uncertainty mapping and probabilistic tractography using Simulation-Based Inference in diffusion MRI: A comparison with classical Bayes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Manzano-Patron</surname><given-names>J.P.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Deistler</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Schröder</surname><given-names>Cornelius</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kypraios</surname><given-names>Theodore</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Gonçalves</surname><given-names>Pedro J.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Macke</surname><given-names>Jakob H.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Sotiropoulos</surname><given-names>Stamatios S.N.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Sir Peter Mansfield Imaging Centre (SMPIC), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ee9ar58</institution-id><institution>University of Nottingham</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Machine Learning in Science, Excellence Cluster Machine Learning, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>University of Tübingen</institution></institution-wrap> &amp; <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0107nyd78</institution-id><institution>Tübingen AI Center</institution></institution-wrap>, <country country="DE">Germany</country></aff><aff id="A3"><label>3</label>School of Mathematical Sciences, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01ee9ar58</institution-id><institution>University of Nottingham</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04sc67422</institution-id><institution>VIB-Neuroelectronics Research Flanders</institution></institution-wrap> (NERF), <country country="BE">Belgium</country></aff><aff id="A5"><label>5</label>Department of Computer Science, Department of Electrical Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05f950310</institution-id><institution>KU Leuven</institution></institution-wrap>, <country country="BE">Belgium</country></aff><aff id="A6"><label>6</label>Department of Empirical Inference, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04fq9j139</institution-id><institution>Max Planck Institute for Intelligent Systems</institution></institution-wrap>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1"><email>Stamatios.Sotiropoulos@nottingham.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Simulation-Based Inference (SBI) has recently emerged as a powerful framework for Bayesian inference: Neural networks are trained on simulations from a forward model, and learn to rapidly estimate posterior distributions. We here present an SBI framework for parametric spherical deconvolution of diffusion MRI data of the brain. We demonstrate its utility for estimating white matter fibre orientations, mapping uncertainty of voxel-based estimates and performing probabilistic tractography by spatially propagating fibre orientation uncertainty. We conduct an extensive comparison against established Bayesian methods based on Markov-Chain Monte-Carlo (MCMC) and find that: a) in-silico training can lead to calibrated SBI networks with accurate parameter estimates and uncertainty mapping for both single and multi-shell diffusion MRI, b) SBI allows amortised inference of the posterior distribution of model parameters given unseen observations, which is orders of magnitude faster than MCMC, c) SBI-based tractography yields reconstructions that have a high level of agreement with their MCMC-based counterparts, equal to or higher than scan-rescan reproducibility of estimates. We further demonstrate how SBI design considerations (such as dealing with noise, defining priors and handling model selection) can affect performance, allowing us to identify optimal practices. Taken together, our results show that SBI provides a powerful alternative to classical Bayesian inference approaches for fast and accurate model estimation and uncertainty mapping in MRI.</p></abstract><kwd-group><kwd>Bayesian inference</kwd><kwd>dMRI</kwd><kwd>Artificial Neural Networks</kwd><kwd>Fibre Orientations</kwd><kwd>Markov-Chain Monte-Carlo</kwd><kwd>Ball &amp; Sticks</kwd><kwd>Parametric Deconvolution</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Identifying the parameters of a model that best represent a set of observations is an omnipresent task in science. For Magnetic Resonance Imaging (MRI) in particular, this is a key task, as on many occasions biophysical quantities of interest are indirectly inferred from measurements. In diffusion MRI (dMRI) of the brain, measurements of the diffusion scatter pattern of water molecules within tissues can be mapped through models to neuronal tissue microstructure and brain connectivity <xref ref-type="bibr" rid="R55">Novikov (2021)</xref>; <xref ref-type="bibr" rid="R3">Alexander et al. (2019)</xref>; <xref ref-type="bibr" rid="R34">Jbabdi et al. (2015)</xref>.</p><p id="P3">Deterministic approaches, from non-linear optimisation to machine learning methods (see <xref ref-type="bibr" rid="R41">Karimi and Warfield (2024)</xref> for a review), can be used to fit such models to data, providing <italic>point estimates</italic> of model parameters. However, these estimates are affected by different sources of uncertainty, from noise in the observations (aleatoric uncertainty), to preprocessing errors, modelling assumptions, and incomplete sampling (epistemic uncertainty). Mapping uncertainty provides a principled manner for assessing confidence in the estimates <xref ref-type="bibr" rid="R36">Jones (2003)</xref>, separating noise and signal, identifying covariance structures in the parameters and aiding experimental design <xref ref-type="bibr" rid="R2">Alexander (2008)</xref>. In dMRI, uncertainty in fibre orientations has been the basis for probabilistic tractography <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R39">Kaden et al. (2007)</xref>; <xref ref-type="bibr" rid="R75">Yendiki et al. (2011)</xref>; <xref ref-type="bibr" rid="R24">Girard et al. (2024)</xref>, which estimates spatial distributions of white matter bundles.</p><p id="P4">Classical approaches for mapping uncertainty in dMRI include bootstrapping and Bayesian inference. Bootstrapping <xref ref-type="bibr" rid="R57">Pajevic and Basser (2003)</xref> involves iteratively fitting the same model to randomly resampled (or perturbed) versions of the original dataset. This provides an empirical distribution for each model parameter, whose width denotes the respective estimation uncertainty <xref ref-type="bibr" rid="R37">Jones (2008)</xref>; <xref ref-type="bibr" rid="R8">Bernstein et al. (2019)</xref>; <xref ref-type="bibr" rid="R27">Gu et al. (2019)</xref>; <xref ref-type="bibr" rid="R54">Ning et al. (2021)</xref>, but can be sensitive to how resampling is performed <xref ref-type="bibr" rid="R71">Whitcher et al. (2008)</xref>;<xref ref-type="bibr" rid="R43">Kauermann et al. (2009)</xref>. On the other hand, Bayesian approaches offer a principled framework for estimating the posterior distribution of the model parameters given the data, providing measures of confidence and covariance in the estimates. For instance, Variational Bayes <xref ref-type="bibr" rid="R11">Chappell et al. (2009)</xref>; <xref ref-type="bibr" rid="R38">Kaden et al. (2008)</xref>; <xref ref-type="bibr" rid="R9">Blei et al. (2017)</xref>, or Markov Chain Monte Carlo (MCMC) sampling <xref ref-type="bibr" rid="R7">Behrens et al. (2003)</xref>; <xref ref-type="bibr" rid="R40">Kaden and Kruggel (2012)</xref>; <xref ref-type="bibr" rid="R67">Sotiropoulos et al. (2013)</xref> provide flexibility in modelling both signal and noise components, and make it possible to explicitly incorporate prior knowledge. These approaches have two main limitations: 1) they require the likelihood of the model to be evaluated analytically, which is not always available or it is too expensive to compute; and 2) although great speed-ups have been achieved (e.g. utilising GPUs <xref ref-type="bibr" rid="R30">Hernandez-Fernandez et al. (2019)</xref> and/or improving the algorithmic design <xref ref-type="bibr" rid="R47">Manzano Patron (2023)</xref>; <xref ref-type="bibr" rid="R28">Harms et al. (2024)</xref>), they are highly iterative in nature, which can result in substantial computational complexity and limited scalability.</p><p id="P5">Simulation-based inference (SBI) <xref ref-type="bibr" rid="R13">Cranmer et al. (2020)</xref> has been developed to tackle such limitations in Bayesian inference. The basic idea is that artificial neural networks are trained on simulations from a forward model, in a way that allows them to learn Bayesian posterior distributions. Importantly, this approach can <italic>amortise</italic> inference: After initial training, performing inference on unseen data only requires a single forward pass through the pretrained neural network, without the need for costly iterative sampling and likelihood evaluations. This amortisation property is particularly effective in scenarios where the same problem needs to be solved many times, as in the voxel-wise inference done in microstructural models of dMRI.</p><p id="P6">In this work, we demonstrate how SBI can be used for mapping orientations and uncertainty in dMRI and, for the first time, all the way up to probabilistic tractography. We comprehensively compare SBI with a classical MCMC-based and heavily-used implementation (FSL-bedpostX) <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012)</xref>; <xref ref-type="bibr" rid="R31">Hernández et al. (2013)</xref>, with respect to both voxel-wise fibre orientation uncertainty estimates and whole-brain white matter tract reconstructions.</p><sec id="S2"><label>1.1</label><title>Related work</title><p id="P7">In Bayesian inference, the posterior distribution <italic>p</italic>(<bold>Ω</bold>|<italic>Y<sub>obs</sub></italic>) of model parameters <bold>Ω</bold> given the measured data <italic>Y<sub>obs</sub></italic> is proportional to the product of the prior <italic>p</italic>(<bold>Ω</bold>) and the likelihood function <italic>p(Y<sub>obs</sub></italic>|<bold>Ω</bold>), the conditional distribution of observations given a model. For problems where the likelihood is intractable or expensive to compute <xref ref-type="bibr" rid="R68">Tavaré et al. (1997)</xref>; <xref ref-type="bibr" rid="R4">Beaumont (2010)</xref>, Approximate Bayesian Computation (ABC) or Likelihood-Free Inference (LFI) approaches have been proposed <xref ref-type="bibr" rid="R66">Sisson et al. (2018)</xref>; <xref ref-type="bibr" rid="R5">Beaumont et al. (2002)</xref>. In ABC, the posterior is approximated by simulating large amounts of data <italic>Y<sub>sim</sub></italic> from a forward stochastic simulator <italic>f<sub>Y</sub></italic> (<bold>Ω</bold>) (synthetic likelihood), with parameters <bold>Ω</bold> randomly drawn from the defined priors p(<bold>Ω</bold>). Samples for which a distance <italic>ρ(Y<sub>sim</sub>, Y<sub>obs</sub>)</italic> is below a pre-defined threshold are accepted as posterior samples (otherwise rejected). SBI recasts this as a density estimation problem, where neural networks are used to learn a mapping between observations and the posterior distribution <xref ref-type="bibr" rid="R5">Beaumont et al. (2002)</xref>; <xref ref-type="bibr" rid="R10">Blum and François (2009)</xref>. This avoids inefficient sampling-rejection schemes and allows problems with high-dimensional observations to be tackled. Such networks, known as Neural Density Estimators (NDEs) <xref ref-type="bibr" rid="R46">Magdon-Ismail and Atiya (1998)</xref>, have enabled inference in a purely data-driven fashion on different target (conditional) densities, such as the posterior distribution of model parameters (Neural Posterior Estimation or NPE) pap (2016); <xref ref-type="bibr" rid="R45">Lueckmann et al. (2017)</xref>; <xref ref-type="bibr" rid="R60">Papamakarios et al. (2017)</xref>; <xref ref-type="bibr" rid="R26">Greenberg et al. (2019)</xref>, a surrogate likelihood (Neural Likelihood Estimation) <xref ref-type="bibr" rid="R61">Papamakarios et al. (2019)</xref>; <xref ref-type="bibr" rid="R20">Durkan et al. (2018)</xref>, or variants based on denoising diffusion models <xref ref-type="bibr" rid="R65">Sharrock et al. (2022)</xref>; <xref ref-type="bibr" rid="R23">Geffner et al. (2023)</xref>; <xref ref-type="bibr" rid="R72">Wildberger et al. (2024)</xref>; <xref ref-type="bibr" rid="R25">Gloeckler et al. (2024)</xref>.</p><p id="P8">A few recent studies have explored inferring parameters from dMRI models using such approaches, demonstrating how SBI can assist in fitting models that are typically challenging to invert. <xref ref-type="bibr" rid="R33">Jallais et al. (2022)</xref> fitted the SANDI model of grey matter <xref ref-type="bibr" rid="R58">Palombo et al. (2020)</xref> to direction-averaged dMRI signal, using the width of the SBI-estimated posteriors to identify regions where the model presented indeterminacies or biases. A generalisation of this work was presented in <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>, proposing a toolbox applicable to a broader range of models with examples of fitting to direction-averaged dMRI signals. In <xref ref-type="bibr" rid="R21">Eggl and Santis (2024)</xref>, the authors used SBI mean posterior to demonstrate that it can provide robust point-estimates with fewer observations than deterministic non-linear fitting. In another recent work, <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref> presented a likelihood-free framework to obtain the full posterior of parameters in a multi-tensor model. An extension of this work for the standard model of white matter <xref ref-type="bibr" rid="R56">Novikov et al. (2018)</xref> was presented in <xref ref-type="bibr" rid="R12">Consagra et al. (2024)</xref>, demonstrating the ability to obtain fiber-specific microstructural features.</p><p id="P9">However, despite the great potential revealed for SBI from these studies, a comprehensive comparison to classical Bayesian approaches, which have been used in dMRI for more than two decades, is still missing. In particular, Bayesian frameworks for mapping orientation uncertainty and performing uncertainty-based probabilistic tractography <xref ref-type="bibr" rid="R7">Behrens et al. (2003</xref>, 2007); <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012)</xref>; <xref ref-type="bibr" rid="R30">Hernandez-Fernandez et al. (2019)</xref> have been collectively used in thousands of studies before <xref ref-type="bibr" rid="R24">Girard et al. (2024)</xref>. Therefore, they naturally provide a “gold-standard” against which SBI approaches can be calibrated, optimised and validated.</p></sec><sec id="S3"><label>1.2</label><title>Contributions</title><p id="P10">In this study we close this gap, expanding on our previous preliminary work <xref ref-type="bibr" rid="R48">Manzano-Patron et al. (2022)</xref> and complementing recent work by others <xref ref-type="bibr" rid="R33">Jallais et al. (2022)</xref>; <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>; <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>; <xref ref-type="bibr" rid="R12">Consagra et al. (2024)</xref>. We devise an SBI framework for mapping fibre orientation uncertainty from diffusion MRI data using parametric spherical deconvolution models. We subsequently use the SBI-derived voxel-wise posterior distributions to perform probabilistic tractography for the first time. Given the inherent difficulty in validating uncertainty estimates, we comprehensively evaluate the accuracy and precision of the estimates against an established MCMC-based classical inference approach, crucially using the same models and priors, as implemented in FSL-bedpostx <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012)</xref>; <xref ref-type="bibr" rid="R31">Hernández et al. (2013)</xref>. We evaluate SBI both for voxel-wise and whole-brain level inference, exploring how estimated uncertainty is spatially propagated.</p><p id="P11">We find that in-silico training can lead to SBI networks that yield accurate mean parameter estimates and uncertainty for unseen observations in both single- and multi-shell dMRI acquisitions, while offering orders of magnitude computational speed-ups for inference. We also show that SBI-based probabilistic tractography for a representative set of more than 40 white matter bundles exhibits a high level of agreement with MCMC-based counterparts; and that this level of agreement is within the range of individual scan-rescan reproducibility of estimates. We further identify optimal SBI design features (such as how to account for noise, how to restrict and constraint priors and how to deal with model selection) that improve training, increase performance and similarity to MCMC estimates. Our findings augment recent studies suggesting that SBI provides new and exciting opportunities for amortised inference in MRI models.</p></sec></sec><sec id="S4" sec-type="methods"><label>2</label><title>Methods</title><sec id="S5"><label>2.1</label><title>NPE theory and network architecture</title><p id="P12">In classical Bayesian inference (like MCMC, see <xref ref-type="fig" rid="F1">Fig. 1</xref>), given a signal and noise model, a likelihood function <italic>p</italic>(<italic>Y<sub>obs</sub></italic>|<bold>Ω</bold>) can be defined. Any prior knowledge of the parameters <bold>Ω</bold> can be embedded in the prior <italic>p</italic>(<bold>Ω</bold>) and the two can be used to estimate the posterior distribution of the parameters given the observed data, i.e. <italic>p</italic>(<bold>Ω</bold>|<italic>Y</italic><sub>obs</sub>) ∼ <italic>p</italic>(<italic>Y</italic><sub>obs</sub>|<bold>Ω</bold>)<italic>p</italic>(<bold>Ω</bold>). This typically necessitates highly iterative processes and potentially expensive likelihood calculations. Furthermore, parameter estimation given a dataset starts from scratch for new observations <italic>Y<sub>obs</sub></italic> (e.g. data for different voxels).</p><p id="P13">Within SBI, this problem can be addressed using Neural Posterior Estimation (NPE) pap (2016); <xref ref-type="bibr" rid="R45">Lueckmann et al. (2017)</xref>; <xref ref-type="bibr" rid="R26">Greenberg et al. (2019)</xref>. In NPE, the mapping between observations and posterior is learned once using a training dataset and can be applied on new unseen data without any expensive calculations, a property known as <italic>amortised inference</italic>. Being able to evaluate the likelihood function is not necessary as long as we have a forward process <italic>f<sub>Y</sub></italic> (<bold>Ω</bold>) to generate simulated data <italic>Y</italic>. As in all Bayesian inference schemes, we can code prior knowledge into our prior distribution <italic>p</italic>(<bold>Ω</bold>) to define what regions of the parameter space are sampled for such forward predictions (or <italic>simulations</italic>). NPE approximates the target posterior using a parameterised conditional density distribution q<sub>Φ</sub>(<bold>Ω</bold>|<italic>Y</italic>) represented by neural networks with parameters Φ. Hence, having a training set of pairs (Ω<sub>i</sub>, <italic>Y<sub>i</sub>),</italic> a NPE is trained to learn the mapping function <italic>q</italic><sub>Φ</sub>(<bold>Ω</bold>|<italic>Y</italic>) ≈ <italic>p</italic>(<bold>Ω</bold>|<italic>Y</italic>). After training, we can use <italic>q</italic><sub>Φ</sub>(<bold>Ω</bold>|<italic>Y</italic>) to obtain samples from the (approximate) posterior <italic>p</italic>(<bold>Ω</bold>|<italic>Y<sub>obs</sub></italic>) with a single forward pass through the network.</p><p id="P14">More formally, we minimise the expected Kullback-Leibler divergence between true posterior distribution <italic>p</italic>(<bold>Ω</bold>|<italic>Y</italic>) and the approximation <italic>q</italic><sub>Φ</sub>(<bold>Ω</bold>|<italic>Y</italic>) pap (2016); <xref ref-type="bibr" rid="R59">Papamakarios et al. (2021)</xref>: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>Φ</mml:mo></mml:munder><mml:mspace width="0.2em"/><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>Φ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mo>~</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="bold">Ω</mml:mo><mml:mo>∣</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>‖</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mo>Φ</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo mathvariant="bold">Ω</mml:mo><mml:mo>∣</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P15">This is equivalent to minimise the negative log-likelihood of the posterior approximation <xref ref-type="bibr" rid="R26">Greenberg et al. (2019)</xref>; <xref ref-type="bibr" rid="R59">Papamakarios et al. (2021)</xref>: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:munder><mml:mrow><mml:mtext>min</mml:mtext></mml:mrow><mml:mo>Φ</mml:mo></mml:munder><mml:mspace width="0.2em"/><mml:mi>ℒ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>Φ</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>L</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mi>log</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mo>Φ</mml:mo></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mo>Ω</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>L</italic> denotes the number of pair-vectors (<bold>Ω</bold><italic><sub>i</sub></italic>, <italic>Y<sub>i</sub></italic>) used to train the networks, where <bold>Ω</bold> ∼ <italic>p</italic>(<bold>Ω</bold>) and <italic>Y<sub>i</sub></italic> ∼ <italic>f<sub>Y</sub></italic> (<bold>Ω</bold><italic><sub>i</sub></italic>). During training, the parameters Φ of the network are optimised to minimise the loss in <xref ref-type="disp-formula" rid="FD2">eq. 2</xref>.</p><p id="P16">Different model choices have been made to parameterise the posterior distribution <italic>q</italic><sub>Φ</sub>(<bold>Ω</bold>|<italic>Y</italic>) in previous applications to dMRI, such as Mixture Density Networks (MDNs) <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>; <xref ref-type="bibr" rid="R12">Consagra et al. (2024)</xref> and Masked Autoencoders (MADE) <xref ref-type="bibr" rid="R33">Jallais et al. (2022)</xref>; <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>. In our work, we use Normalising Flows (NFs) as the basis structure for the density estimators given their flexibility and superior performance in other applications <xref ref-type="bibr" rid="R44">Lueckmann et al. (2021)</xref>; <xref ref-type="bibr" rid="R18">Dockhorn et al. (2020)</xref>; <xref ref-type="bibr" rid="R48">Manzano-Patron et al. (2022)</xref>. NFs can produce complex probability distributions by transforming a base distribution through a series of invertible transformations <italic>f<sub>k</sub></italic>. A key features of NFs is their use of differentiable closed-form functions that support efficient computation of both the transformations and their Jacobian determinants, ensuring rapid density estimation and sampling. In this paper, we use Neural Spline Flows (NSF) <xref ref-type="bibr" rid="R19">Durkan et al. (2019)</xref>, as implemented in the SBI package <xref ref-type="bibr" rid="R69">Tejero-Cantero et al. (2020)</xref>. NSFs comprise of <italic>K</italic> rational-quadratic splines, each parameterised by a neural network.</p></sec><sec id="S6"><label>2.2</label><title>Forward model <italic>f<sub>Y</sub></italic> (<bold>Ω</bold>)</title><p id="P17">In this work, we use variants of the Ball &amp; Sticks model <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012</xref>) as the forward simulator <italic>f<sub>Y</sub></italic>(<bold>Ω</bold>). Our goal is to infer the posterior of parameters <bold>Ω</bold> from dMRI data in each voxel, with <italic>Y<sub>obs</sub></italic>= {Y<sub>1,obs,…,</sub> <italic>Y<sub>j,obs</sub></italic>} of <italic>j =</italic> 1,…, <italic>J</italic> signals, each corresponding to a different diffusion-sensitising gradient. This model has been heavily used for performing parametric spherical deconvolution and uncertainty-based probabilistic tractography over the past 20 years. An MCMC-based implementation for classical Bayesian inference on this model has been developed by members of our team and is part of FSL <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R31">Hernández et al. (2013)</xref>; <xref ref-type="bibr" rid="R30">Hernandez-Fernandez et al. (2019)</xref> and will be used for direct comparisons with SBI.</p><p id="P18">The forward model is multi-compartment, comprising of a weighted sum of <italic>N</italic> anisotropic compartments (sticks), representing distinct white matter fibre compartments, and an isotropic compartment (ball), representing partial volume and non-axonal contributions to the signal. The voxel-wise dMRI signal attenuation <italic>A</italic> in single-shell data (with relatively low b-value) is described as: <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>Ω</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>→</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">g</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">v</mml:mtext><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>S<sub>j</sub></italic> and <italic>S<sub>0</sub></italic> are the signals measured with and without diffusion-sensitising gradients, respectively, with <italic>b<sub>j</sub></italic> value and orientation <italic>g<sub>j</sub></italic> (protocol-defined parameters). The unknown model parameters to estimate are the mean diffusivity <italic>d,</italic> the vector <bold>v</bold><sub><italic>n</italic></sub> describing the <italic>n</italic><sup>th</sup> fibre orientation, represented in spherical coordinates by <italic>[θ<sub>n</sub>, ϕ<sub>n</sub></italic>] (<italic>θ</italic> is the angle from the positive z-axis and <italic>ϕ</italic> is the angle of the projection on the x-y plane from the positive x-axis) and the volume fraction of the <italic>n</italic><sup>th</sup> fibre <italic>f<sub>n</sub></italic> (with ∑ <italic>f<sub>n</sub></italic> ≤ 1). The maximum number of fibres <italic>N</italic> per voxel is typically set to <italic>N</italic> = 3, but the appropriate <italic>N</italic> per voxel needs to be determined (model selection challenge).</p><p id="P19">The above model can be extended for multi-shell data <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012)</xref> as: <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>Ω</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>→</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mtext mathvariant="italic">std </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext mathvariant="bold">g</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">v</mml:mtext><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mtext mathvariant="italic">std</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mtext mathvariant="italic">std</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mtext mathvariant="italic">std</mml:mtext></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic>d</italic> and <italic>d<sub>std</sub></italic> denote the mode and spread of a Gamma distribution of diffusivities that can account for non-monoexponential signal decay. In summary, the vector of model parameters when the maximum number of fibres is n, 1 ≤ <italic>n</italic> ≤ <italic>N</italic>. is given by <bold>Ω</bold> = {<italic>d,d<sub>std</sub></italic>, {<italic>f<sub>1</sub>,</italic> …, f<sub>n</sub>, <italic>θ</italic><sub>1</sub>,…, <italic>θ</italic><sub>n</sub>,<italic>Φ</italic><sub>1</sub>,…, <italic>ϕ</italic><sub>n</sub>}}.</p></sec><sec id="S7"><label>2.3</label><title>SBI design and implementation</title><p id="P20">We designed and implemented a number of NPE networks for the forward models described in the previous section (setting <italic>N</italic> up to 3), using the Python SBI toolbox (v.0.22) <xref ref-type="bibr" rid="R69">Tejero-Cantero et al. (2020)</xref>. We set the depth of the Neural Spline Flows to <italic>K =</italic> 10. For training and inference, we used the attenuation <italic>A<sub>j</sub></italic> as input for the networks, obtained after data pre-processing by dividing the diffusion-weighted signal by the mean of the <italic>b</italic> = 0 volumes (as normalised inputs generally provide more stable behaviour on neural networks). Depending on model complexity and number of compartments considered, different training sizes were used for different SBI architectures, but in general training sample sizes ranged from 3M to 5M samples. Once the networks were trained, we drew <italic>M</italic> = 50 samples from <italic>q</italic><sub>Φ</sub>(Ω|<italic>Y<sub>obs</sub></italic>) to characterise the posterior given unseen observations. Code for the implemented architectures can be found on <ext-link ext-link-type="uri" xlink:href="https://github.com/orgs/SPMIC-UoN/SBI_dMRI">https://github.com/orgs/SPMIC-UoN/SBI_dMRI</ext-link>.</p><p id="P21">We performed extensive comparisons between posterior estimates obtained from SBI and from MCMC (as available in FSL-bedpostX implementation (v.6.0.7) <xref ref-type="bibr" rid="R30">Hernandez-Fernandez et al. (2019)</xref>) on both single and multi-shell models. These included comparisons using both synthetic and in-vivo data, at a voxel (e.g., fibre orientations) and whole brain scale (tractography reconstructions). The equations and parameters underlying the MCMC implementation are provided in Supplementary Material for completeness, as well as the metrics we used for assessing accuracy and precision of parameter estimates.</p><p id="P22">We also investigated how SBI design choices affect accuracy and precision, and agreement with MCMC. In the following sections we present novel explorations on the training design for handling priors, treating noise and addressing model selection challenges. We finally present the data used and data processing aspects all the way to probabilistic tractography.</p><sec id="S8"><label>2.3.1</label><title>Uniform vs restricted priors <italic>p</italic>(<bold>Ω</bold>)</title><p id="P23">Non-informative priors have been used in recent dMRI SBI implementations <xref ref-type="bibr" rid="R33">Jallais et al. (2022)</xref>; <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>; <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>; <xref ref-type="bibr" rid="R12">Consagra et al. (2024)</xref>, as they provide the simplest choice for training and offer uniform coverage over the whole parameter space. However, certain combinations of parameters will never be found in real-world cases or won’t be feasible to detect due to limitations in the measurement process. Furthermore, certain model parameters have non-linear boundary conditions that cannot be easily defined by parametric prior distributions, requiring ad-hoc reparametrisation that often are not trivial <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>. For instance, the sum of volume fractions in a multi-compartment model should be between 0 and 1. Using non-informative priors and forcing NPE to learn from implausible regions in parameter space could make training highly inefficient, potentially reduce the predictive performance, and increase estimation uncertainty.</p><p id="P24">We tested two SBI designs, one with uniform priors and another with restricted priors. While the former is straightforward to implement, the latter requires a more advanced design <xref ref-type="bibr" rid="R16">Deistler et al. (2022b</xref>,<xref ref-type="bibr" rid="R15">a</xref>). For the uniform prior implementation we used: <inline-formula><mml:math id="M5"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>·</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> and </mml:mtext><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The fibre orientations were sampled in the positive hemisphere (i.e. <italic>θ</italic> ∈ [0, π/2], <italic>ϕ</italic> ∈ [0, π]) to avoid degenerate solutions due to antipodal symmetry. To ensure uniform sampling on the sphere, the following correction was applied: <italic>θ</italic> = arccos(1 – 2<italic>u</italic>) and <italic>ϕ</italic> = <italic>π · u,</italic> where <italic>u ∼ U</italic> (0,1) (see Supplementary Material).</p><p id="P25">Subsequently, we designed and trained a joint Restricted Prior distribution that learns feasible regions of the parameter space and implements parameter prior constraints, as summarised in <xref ref-type="table" rid="T1">Table 1</xref>. To achieve that, we trained a classifier to learn what is the signal stemming from “valid” parameter combinations, i.e. for which restrictions and boundary conditions on individual parameters are respected (see <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). To do so, we 1) sampled each parameter independently from uniform priors <italic>p</italic>(<bold>Ω</bold>) specified above, 2) used these samples to generate forward predictions using <italic>f<sub>Y</sub></italic> (<bold>Ω</bold>), 3) tagged the cases that do not meet the boundary conditions into a different class of “non-valid”, and 4) trained a classifier accordingly to separate valid vs non-valid cases. This classifier was wrapped into a density distribution object to produce samples only from the (“valid”) restricted regions of the parameter space. We trained separate Restricted Priors for the cases of 1, 2 and 3 fibres, and compared the efficiency and performance of the inference against the network using the default uniform priors.</p></sec><sec id="S9"><label>2.3.2</label><title>Dealing with noise during training</title><p id="P26">Noisy realisations of the signal were considered during training. For consistency with the MCMC implementation, we assumed zero-mean Gaussian additive noise (i.e. <italic>f<sub>Y</sub></italic> (<bold>Ω</bold>) →<italic>f<sub>Y</sub></italic> (<bold>Ω</bold>) + ε) with ε<italic>~ N</italic>(0, σ), <italic>σ</italic> = <italic>S<sub>0</sub>/SNR</italic> and <italic>SNR</italic> the signal-to-noise ratio of the <italic>b</italic> = 0 signal.</p><p id="P27">Matching the noise level and features between training and test data improves robustness and accuracy of SBI estimates <xref ref-type="bibr" rid="R50">Masutani (2019)</xref>. However, as such matching cannot be performed a-priori for a generalisable implementation, we evaluated the performance of SBI under two strategies for considering noise during training. For a given set of parameter values <bold>Ω<italic><sub>i</sub></italic></bold> we explored two strategies: a) Single-level noise: A single noisy realisation <italic>Y<sub>i</sub></italic> was generated with an SNR value chosen randomly from a prior <inline-formula><mml:math id="M6"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:mi>R</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> b) Multi-level noise: The same noise-free signal was corrupted with different noise levels chosen from multiple predefined SNR intervals (here we used 8 SNR intervals), i.e. <inline-formula><mml:math id="M7"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>N</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>70</mml:mn><mml:mo>,</mml:mo><mml:mn>80</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> I.e. we produce 8 noisy realisations of each <italic>Y<sub>i</sub></italic>, each at a different noise level. We kept the total number of training points equal for both strategies. Hence, the former approach provided for training more combinations of parameters sampled from the priors, each with a single noisy realisation; while the latter approach provided less combinations of parameters for training, but with more examples of noise contamination for each combination.</p></sec><sec id="S10"><label>2.3.3</label><title>Model selection using SBI</title><p id="P28">In the multi-compartment model considered, there is an inherent challenge of deciding how many fibre compartments (i.e. orientations) must be estimated in each voxel. This is a model selection problem that the MCMC implementation addresses using sparsifying, Automatic Relevance Determination (ARD) priors <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref> (see Suppl. Material). These are improper priors that cannot be sampled from hence not well suited and easily adaptable to simulations from the prior, as needed in SBI. To address the model selection problem, we have implemented and evaluated two strategies:</p><p id="P29"><bold>ClassiFiber + Individual NPEs</bold>: Similar to <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>, we subdivided the problem into 2 stages: an initial classification step to select how many fibre compartments correspond to the signal of each voxel, and a second step that fits the corresponding model based on the classifier output. For the first step, we designed the <italic>ClassiFiber</italic>, a feed-forward deep neural net that can take the dMRI signal (or a basis signal representation projection) as input and provide the number of fibres as output. The neural network is formed by 6 layers (input, 256, 128, 64, 32, output), with ReLu activation functions, 1% dropout, adaptive learning rate, Adam optimisation, and a Cross-Entropy loss function. We generated 6 million parameter combinations (2 million for <italic>N</italic> =1, 2 and 3 crossing-fibres configurations sampled from the restricted priors) and generated signal predictions using the multi-shell model. For the second step, we trained 3 independent NPEs for models with <italic>N</italic> =1, 2, or 3 fibre compartments, respectively, using the basis structure of sec.2.1. Hence, a network e.g. with <italic>N</italic> = 2 compartments was fed with only 2-way crossing patterns during training, while a network with <italic>N</italic> = 1 compartment was not trained with any fibre crossing examples. We refer to this setup as <italic>SBI_ClassiFiber</italic>.</p><p id="P30"><bold>Joint NPE</bold>: Contrary to the independent NPEs trained above, we trained a network using a training super-set that included examples of 1,2 and 3 crossing-fibre cases (equal number of 1, 2, and 3 fibre cases). In order to keep a fixed parameter vector size, N = 3 compartments were set into the model. For cases where N &lt; 3 fibre compartments were simulated (i.e. for single fibres and 2-way crossings), the respective volume fractions of the <italic>n<sup>th</sup></italic> degenerate compartments in the training data were set to <inline-formula><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">U</mml:mi></mml:mrow></mml:math></inline-formula> (0, 1<italic>e</italic> – 3), while the values for the orientations (<italic>θ<sub>n</sub></italic>, <italic>ϕ<sub>n</sub>)</italic> were randomly sampled from the priors (e.g. when two fibres were simulated, the volume fraction of the third compartment was effectively set to zero in the training data). This joint dataset was used to train a single NPE, which we evaluated on whether it can inherently learn the correct number of compartments, without a prior classification as above. We refer to this setup as <italic>SBI_joint</italic>.</p></sec></sec><sec id="S11"><label>2.4</label><title>Datasets, processing and tractography</title><sec id="S12"><title>Data</title><p id="P31">We evaluated the different architectures using both synthetic and in-vivo data. Preprocessed in-vivo dMRI data were obtained from a previous study <xref ref-type="bibr" rid="R49">Manzano Patron et al. (2024)</xref>, consisting of 6 re-scans of the same subject, each acquired following a UKBiobank-like multi-shell protocol (2mm isotropic resolution, TR=3s, TE=92ms, MB=3, Siemens Prisma 3T scanner) <xref ref-type="bibr" rid="R52">Miller et al. (2016)</xref>. Fifty gradient directions at b=1000 s/mm<sup>2</sup> and another fifty gradient directions at b=2000 s/mm<sup>2</sup> were obtained, with a mean SNR of 30 for the b=0 s/mm<sup>2</sup> image. Synthetic data allowed toy examples for testing the inference frameworks under scenarios where the true parameter values were known. We used the Ball&amp;Sticks model to generate such synthetic datasets, following the same protocol (b-values and vectors) as in the in-vivo datasets.</p></sec><sec id="S13"><title>Acquisition-specific vs Acquisition-agnostic training</title><p id="P32">As the above approach returns trained networks for a specific set of bvals and bvecs, we also explored using an acquisition-agnostic basis set representation of the data as input to the networks, instead of using directly the dMRI signal attenuation. The signal attenuations were expressed using Spherical Harmonics (SH) <xref ref-type="bibr" rid="R17">Descoteaux et al. (2007)</xref> (for single-shell) and the MAP-MRI basis (for multi-shell) <xref ref-type="bibr" rid="R76">Özarslan et al. (2013)</xref>. Given the number of volumes in the data, we retained the even SH coefficients up to order <italic>L</italic> = 6 (28 coefficients), and for the MAP-MRI up to radial order <italic>r</italic> = 6 (45 coefficients) with Laplacian regularization (no positivity constraints nor anisotropic scaling). Both signal representations were obtained using Dipy implementations <xref ref-type="bibr" rid="R22">Garyfallidis et al. (2014)</xref>.</p></sec><sec id="S14"><title>Probabilistic tractography</title><p id="P33">Using the trained architectures, we drew samples from the posterior distribution of parameters given the observed data. For the same data and model, we drew posterior samples using MCMC as well. Each provided voxel-wise orientation distributions that mapped uncertainty for the estimated parameters. We subsequently used the estimated posterior orientation samples in probabilistic tractography. We ran the standardised FSL-XTRACT tractography protocols <xref ref-type="bibr" rid="R70">Warrington et al. (2020)</xref> using the estimates of each method. These protocols included a range of white matter tracts (commissural, projection, association and limbic). The SBI-derived and MCMC-derived tracts were then directly compared. As both of them reflect stochastic estimation processes, we further explored how the agreement in tract reconstruction between SBI and MCMC compares with scan-rescan reproducibility in MCMC-based tract reconstruction, across the six available repeats.</p></sec></sec></sec><sec id="S15" sec-type="results"><label>3</label><title>Results</title><sec id="S16"><label>3.1</label><title>Single-fibre model</title><p id="P34">We performed initial evaluations in the simple case of a single-fibre model (<italic>N</italic> = 1 in <xref ref-type="disp-formula" rid="FD3">Eq. 3</xref> and <xref ref-type="disp-formula" rid="FD4">Eq. 4</xref>). <xref ref-type="fig" rid="F2">Fig. 2</xref> shows example comparisons for the posterior mean and uncertainty estimated by MCMC and SBI, in both single-shell (B) and multi-shell (C) models. A default SBI training paradigm was used in these first comparisons (sampling from uniform priors and single-level noise strategy), and the same priors were used as in MCMC. Representative examples of the marginal posteriors for <italic>f<sub>1</sub></italic> in different areas of the brain are shown in <xref ref-type="fig" rid="F2">Fig. 2A</xref>, with very good agreement between MCMC and SBI in the means of these posteriors, and some small variation in their coverage. Overall, as can be seen in the whole-brain maps, the agreement between these two very different inference approaches is extremely high, with correlations of the posterior means above 0.95 for both diffusivity and volume fraction, and median differences of 2° for fibre orientations across white matter. Similarly, uncertainty maps show correlations of 0.92, i.e. a similar uncertainty mapping across the brain (lower uncertainty in WM and higher in CSF and GM for the fibre orientation). A very similar picture is depicted in <xref ref-type="fig" rid="F2">Fig. 2C</xref> for the multi-shell model. These first findings suggest high agreement in both accuracy and precision of SBI estimates against MCMC, in the case of a simple model with a default training paradigm. In the next sections, we increase the model complexity and evaluate how different SBI training and designs affect performance.</p></sec><sec id="S17"><label>3.2</label><title>Multi-fibre models - Synthetic data</title><sec id="S18"><label>3.2.1</label><title>Optimal training strategies</title><p id="P35">We subsequently increased the complexity of the models to include crossing fibres (2 ≤ <italic>N ≤</italic> 3). First, we compared different training strategies using synthetic data (where ground truth values for model parameters are known). We investigated 1) increase in estimation accuracy due to usage of restricted priors compared to uniform priors, 2) the optimal strategy for considering noise during training, 3) how alternative signal representations work (e.g., using the raw signal vs spherical harmonics), and 4) how different SBI designs perform (independent vs joint training). <xref ref-type="fig" rid="F3">Fig. 3</xref> shows a collection of comparisons using synthetic data with <italic>N</italic> = 2 fibre compartments, where the anisotropic volume fraction <italic>f<sub>1</sub></italic> and the crossing angle are varied. In each case, the mean of the estimated posterior distribution for volume fractions and crossings angles are shown. Given that the ground truth values are known, these results directly inform on the accuracy of the estimates for different inference approaches.</p><p id="P36">Among the different strategies explored, using a multi-level noise scheme combined with restricted priors provided the best performance in retrieving the ground-truth. These improved both accuracy (see <xref ref-type="fig" rid="F3">Fig. 3A</xref>) and precision (see <xref ref-type="fig" rid="F3">Fig. 3A</xref>). With restricted priors, the sampling efficiency was increased as all of the proposed parameter combinations met the conditions in <xref ref-type="table" rid="T1">Table 1</xref>. This led to speed-ups of 62% compared to sampling from uniform priors, where 39% of proposed parameter combinations were rejected (see <xref ref-type="fig" rid="F2">Fig. 2</xref> for an example of how the parameter space gets constrained). Hence, multi-level noise and restricted-priors are used for any SBI model presented onwards.</p><p id="P37"><xref ref-type="fig" rid="F3">Fig. 3B</xref> shows a comparison of the obtained estimates when SBI is trained against signal attenuation (for a given set of bvals and bvecs) or acquisition-agnostic signal representations, for both single-shell (using spherical harmonics) and multi-shell (using the MAP-MRI projections) scenarios. Some loss of accuracy can be observed in the signal representation cases, particularly at small crossing angles, suggesting that acquisition-agnostic training induces a small penalty in performance compared to training for data specific to a particular acquisition. However, overall good performance is preserved. For the rest of the paper we kept SBI trained against the full signal, but we also present some in-vivo results using the MAP-MRI basis.</p><p id="P38">Finally, <xref ref-type="fig" rid="F3">Fig. 3C</xref> shows the performance of MCMC (with <italic>N</italic> = 2 and no ARD priors) compared against NPEs trained only with 2-fibres examples (Independent) or with a joint superset that contains both 1- and 2-fibres cases (Joint). Both MCMC and <italic>Independent</italic> are always set to fit <italic>N</italic> = 2 fibre compartments (i.e. the right model for these data), while <italic>Joint</italic> inherently performs indirect model selection (it can fit either a <italic>N</italic> =1 or <italic>N</italic> = 2 fibres model). Hence, the former corresponds to SBI_ClassiFiber when the classifier identifies the correct number of compartments (N=2), while the latter corresponds to SBI_joint with <italic>N</italic> ≤ 2. All methods recovered appropriately the ground-truth values, with <italic>Joint</italic> having slightly worse performance than the other two, indicative of its higher complexity and its ability to indirectly perform model selection. Nevertheless, for all simulated cases, it could identify the two fibre compartments supported by the data.</p><p id="P39">The above trends in terms of accuracy for the parameter estimates also hold in case of synthetic data where <italic>N</italic> = 3 fibres have been simulated (see <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>. Taken together the findings suggest that SBI with appropriate training can perform very well in terms of posterior accuracy and precision (and similarly to MCMC) in synthetic data, when the model complexity (i.e. number of compartments is a-priori known).</p></sec><sec id="S19"><label>3.2.2</label><title>Dealing with model selection</title><p id="P40">We subsequently explored more comprehensively the performance of different inference approaches with respect to model selection in 3-way crossing patterns. One of the main challenges when performing inference for a multi-compartment model is the identification of the number of compartments that are relevant to the observed signal. We explored two alternatives to deal with this challenge, SBI_ClassiFiber and SBI_joint. SBI_ClassiFiber performs an initial classification step to decide the appropriate model complexity <italic>N</italic> and then fits the respective NPE with <italic>N</italic> = 1, <italic>N</italic> = 2 or <italic>N</italic> = 3 compartments, each of which are independently trained. SBI_joint aims to learn the inherent structure by being trained jointly against <italic>N</italic> = 1,2,3 cases, avoiding the classification step and the need of training different NPEs. We compared these architectures in synthetic data against MCMC, for which we fit the most complex model (<italic>N</italic> = 3) and used ARD shrinkage priors for the volume fractions of each compartment <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>.</p><p id="P41"><xref ref-type="fig" rid="F4">Fig. 4A</xref> shows the performance for similar synthetic examples as before, where 2-fibre crossings were simulated, but up to 3 compartments could be estimated by all approaches. This allowed us to explore whether underfitting/overfitting occurred, contrary to the previous tests, where the number of compartments in the model was pre-fixed to the actual number of compartments simulated. All methods correctly identified the correct number of fibre compartments and showed good overall performance, with MCMC providing the best mean estimates. SBI architectures were close in estimation accuracy, with SBI_joint, as the most complex NPE, having slightly lower performance.</p><p id="P42">We then tested the model selection performance in a larger set of 10,000 synthetic scenarios sampled from the restricted priors, with <italic>N</italic> = 0 – 3 (i.e. from zero to three fibre compartments, with various combinations of volume fractions and crossing angles). <xref ref-type="fig" rid="F4">Fig. 4B</xref> shows in matrix format, comparisons of the number of fibre compartments detected (matrix rows) when different number of fibres are simulated (matrix columns). These detection matrices are presented for different noise levels and for different inference approaches; diagonal matrices correspond to ideal model selection, while elements above/below the diagonal correspond to over/under-fitting. At very high SNR, MCMC had a correct detection rate (diagonal of the true vs predicted matrices) above 90% for all cases. SBI methods also did well in detecting the correct number of compartments, albeit with a slightly lower performance (particularly for SBI_joint). However, at noisier (and more realistic) scenarios, the performance of SBI methods was more stable than of MCMC, capturing many crossings even at very low SNR. Overall, SBI_ClassiFiber presented the best performance at mid- and low-SNR, identifying half of the 3-way crossings and avoiding overfitting in the non-fibre cases. SBI_joint and MCMC returned similar results at mid-SNR although the former presented a considerably better identification of crossings at low-SNR. A general behaviour observed across all approaches was the tendency to underfit the number of compartments (e.g. return single-fibre when 2 fibres exist) with lower SNR.</p></sec></sec><sec id="S20"><label>3.3</label><title>Multi-fibre models - In-vivo data</title><p id="P43">After evaluating and optimising the performance of SBI with respect to MCMC in terms of accuracy, precision, and model selection in synthetic data, we compared them using in-vivo dMRI multi-shell brain data. As there is no ground-truth available in this case, we used MCMC estimates as the reference to compare against. <xref ref-type="fig" rid="F5">Fig. 5A</xref> shows a comparison of whole-brain mean and uncertainty maps obtained by the different methods for a model with <italic>N</italic> = 2 fibre compartments. Both SBI models provide very high correlations with MCMC (above <italic>r =</italic> 0.9 for all scalar parameters, see <xref ref-type="fig" rid="F5">Fig. 5B</xref>). This inherently implies also a good agreement in the model selection results. Estimated crossing fibres show a good agreement as well (<xref ref-type="fig" rid="F5">Fig. 5.C</xref>), with the median angular difference between SBI and MCMC being around 6° for <bold>v</bold>1 and around 10° for <bold>v</bold><sub>2</sub> (see <xref ref-type="supplementary-material" rid="SD1">Fig. S5A</xref>). The example through the centrum semiovale shown in <xref ref-type="fig" rid="F5">Fig. 5C</xref>, highlights all approaches resolving crossings between callosal and corona radiata projections, with the SBI architectures returning less of fibre fanning within the corpus callosum. Regarding orientation uncertainty, MCMC patterns are overall reproduced through SBI although some broader posteriors were observed, especially in the second fibre orientation (see last two columns of <xref ref-type="fig" rid="F5">Fig. 5A and B</xref>).</p><p id="P44">Similar trends can be observed in <xref ref-type="fig" rid="F6">Fig. 6</xref> for a N = 3 fibres multi-shell model. Panel A) shows mean maps of the diffusivity and volume fractions. As in Fig. 4, SBI_joint showed a more similar number of complex fibre patterns detected to MCMC, with 22.5% and 25.4% of the voxels in the white matter identified as 3-way crossings by each method, respectively, while SBI_ClassiFiber returned 33.3%. High agreement with MCMC was found for the first two compartments by both SBI methods, with correlations of <italic>r<sub>d</sub></italic> = 0.97, <italic>rd<sub>s</sub>td</italic> = 0.88, <italic>r<sub>f</sub><sub>1</sub></italic> = 0.97, and <italic>r<sub>f</sub><sub>2</sub></italic> ϕ 0.88. Larger differences were found in the third compartment; the correlation in the mean <italic>f</italic><sub>3</sub> reduced to 0.7 in SBI_ClassiFiber, and 0.58 in the SBI_joint.</p><p id="P45">Panel B) of figure 6 shows an example case of the orientations in the centrum semiovale, where 3-way crossings can be expected (corpus callosum fibres running left-to-right, corona radiata running superior-to-inferior and association fibres anterior-to-posterior). All methods depicted these, with SBI_joint having slightly lower coherence and crossings overall. Whole-brain orientation estimates are shown in <xref ref-type="supplementary-material" rid="SD1">Fig. S5B</xref>).</p><p id="P46">Uncertainty maps for all three fibre orientations are shown in panel <xref ref-type="fig" rid="F6">Fig. 6C</xref>, where SBI methods exhibited in general higher uncertainty compared to MCMC. Even if the general patterns were similar in all methods (i.e. highest uncertainty in CSF and GM, lower uncertainty in WM), SBI methods returned higher uncertainty in WM with increased model complexity.</p><p id="P47">Finally, as suggested in the previous section, using the MAP-MRI signal representations for a 3-fibres model led to similar parameter estimates (mean posterior) as using the raw dMRI signal (see <xref ref-type="supplementary-material" rid="SD1">Fig. S6</xref>).</p><sec id="S21"><label>3.3.1</label><title>Sources of SBI vs MCMC uncertainty differences</title><p id="P48">So far we have observed that MCMC and SBI return very similar mean parameters, but they can show discrepancies in the width of the posterior, particularly evident in in-vivo data. We further explored these differences in uncertainty quantification and identified scenarios where these occur.</p><p id="P49">A first identified case corresponds to the potential of fitting fewer compartments than supported by the data (e.g. due to inaccurate model selection). <xref ref-type="fig" rid="F8">Fig. 8A</xref> shows the marginal and pairwise joint distribution for all parameters in an example synthetic case where a 3-way crossing pattern is simulated (crossing-angles of 90 degrees, <italic>f<sub>1</sub></italic> = 0.4, <italic>f</italic><sub>2</sub> = 0.3, <italic>f</italic><sub>3</sub> = 0.2), but a 2-fibre model is fitted, i.e. wrong model identification. MCMC returns unimodal marginal distributions for all parameters; it finds one of the 3 possible orientation modes and keeps sampling from it (as it is very <italic>expensive</italic> to jump to a different mode), missing the other modes and providing very low uncertainty (overconfidence). On the other hand, SBI captures all three orientation modes, assigning their samples to (mainly) <italic>v<sub>1</sub></italic> (see red arrows) which, therefore leads to increased uncertainty due to multiple modes arised from fitting a model with the wrong number of compartments (epistemic uncertainty).</p><p id="P50">Another source of differences in uncertainty is compartment swapping, which can occur when volume fractions of fibre compartments are very close to each other e.g. (<italic>f<sub>i</sub></italic> – <italic>f<sub>j</sub></italic>) ≈ 0. In such cases, assigning a compartment to be first, second or third becomes arbitrary, as any combination could explain the signal in the same way. SBI is based on minimising the negative log-likelihood, so when the compartments have similar volume fractions, it will equally likely assign a sample from orientation <bold>v</bold><italic><sub>i</sub></italic> to parameter <bold>V</bold><italic><sub>j</sub></italic> and vice versa. Hence, drawing multiple samples can lead to a multi-modal marginal posterior for a specific fibre orientation parameter with <italic>P</italic> modes, with <italic>P</italic> the number of compartments with similar volume fraction. This compartment swapping is much more difficult to occur with a random-walk Metropolis Hastings MCMC algorithm, which needs very high energies to jump between multiple modes and therefore samples tend to inherently cluster together. Nevertheless, this issue in SBI can be mitigated with a simple re-ordering of the posterior samples based on the similarity of orientations. <xref ref-type="fig" rid="F8">Fig. 8B</xref> shows the marginal and pairwise joint distribution for all parameters in an example case from in-vivo brain data, with compartment swapping, before and after the re-ordering to compensate for the swapping.</p><p id="P51">The above findings suggest that the calculated SBI uncertainty maps are potentially inflated by the way parameter samples are grouped into compartments and uncertainty is estimated. They also hint that individual samples are likely correct in all cases, even if summary uncertainty maps suggest otherwise when compared against summary maps from MCMC. We therefore performed probabilistic tractography, which uses the posterior orientation samples rather than summary uncertainty maps, to test this hypothesis.</p></sec></sec><sec id="S22"><label>3.4</label><title>Probabilistic tractography using SBI estimates</title><p id="P52">Probabilistic tractography spatially propagates orientation uncertainty (i.e. samples from the marginal posterior distribution of the fibre orientations) to build spatial distributions that estimate white matter pathways. We explored how the orientations estimated from the 3 different approaches (MCMC, SBI_ClassiFiber, SBI_joint) affect tractography performance. We used the posterior fibre orientation samples estimated by each method using a <italic>N</italic> = 3 fibre, multi-shell model. <xref ref-type="fig" rid="F7">Fig. 7</xref> shows example Maximum Intensity Projections (MIPs) of the path distributions for a range of 42 commissural, association, limbic and projection tracts <xref ref-type="bibr" rid="R70">Warrington et al. (2020)</xref>. For reference, the population-average UK-Biobank atlas is also shown. All methods reconstructed all tracts successfully and showed a high qualititative agreement between the corresponding paths. SBI-reconstructed tracts had a higher correlation with the atlas (up to 15% more in SBI_ClassiFiber), compared to MCMC-based reconstructions.</p><p id="P53">We then looked into quantitative comparisons between the reconstructed tracts obtained using MCMC vs SBI estimates. <xref ref-type="fig" rid="F9">Fig. 9A</xref> shows scan-rescan reproducibility of each method among 6 consecutive repeats of the same MRI acquisition protocol on the same participant. SBI methods reported higher reproducibility in tract reconstructions, with an overall median correlation <italic>r</italic> of <italic>r =</italic> 0.96 for SBI_joint and <italic>r =</italic> 0.95 for SBI_ClassiFiber (among tracts and repeats), which is consistently higher than in MCMC (<italic>r =</italic> = 0.87). This provides evidence for the hypothesis that posterior orientation samples from SBI are appropriately precise and support the sources of the seemingly higher orientation uncertainty shown in the previous section. In addition, <xref ref-type="fig" rid="F9">Fig. 9B</xref> demonstrates the median (among 6 repeats) correlation between MCMC-based and the corresponding set of SBI-based reconstructed tracts. This is overlayed on a shaded zone (in light blue), which depicts the interquantile range (across the 6 repeats) of the correlations between MCMC-based tract reconstructions themselves. In general, both SBI_ClassiFiber and SBI_joint perform similarly well to MCMC; certain tracts that have multiple 3-way crossings along their route are an exception (AF, AR, SLF1, SFL2, SLF3) as for these SBI_ClassiFiber performs better than SBI_joint. Nevertheless, these plots suggest that differences between MCMC and SBI in tract reconstructions are within the range of (and most of the time smaller than) scan-rescan variability (i.e., the variability of MCMC-based tract reconstructions across the 6 repeats). For the exceptions observed, where correlations between MCMC and SBI methods were lower, SBI methods showed a more precise reconstruction among repeats compared to MCMC (see panel A-top), as also indicated in <xref ref-type="fig" rid="F9">Fig. 9C</xref>, where the averaged reconstructed tracts over the 6 repeats are shown.</p><p id="P54">Taken together, these results indicate that for a range of representative white matter tracts, including those with complex fiber architecture and 3-way crossings along them, SBI-based reconstructions i) agreed better with a population-average white matter atlas, ii) were more precise and reproducible across MRI repeats, iii) had differences with their MCMC counterparts that were within or lower than the magnitude of differences expected due to scan-rescan variability.</p></sec><sec id="S23"><label>3.5</label><title>Computational performance</title><p id="P55">We measured the average computational time required for inference across cases with 1, 2, and 3 fibre compartments (see <xref ref-type="table" rid="T2">Table 2</xref>). Performing inference with the trained SBI models achieved up to 70x speed-up over the CPU-based MCMC implementation. Note that SBI is a CPU-based Python implementation, while BedpostX is highly optimised in C. All in all, with a simple data-level parallelisation across e.g. 8 CPU-cores, inference on a standard in-vivo brain dataset, as the one used here, with the current SBI implementation takes only ∼ 4 minutes.</p></sec></sec><sec id="S24" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P56">In this work, we have presented a Simulation-Based Inference framework for Bayesian fitting of multi-compartment fibre orientation models in diffusion MRI data, mapping parameter uncertainty and utilising the posterior distributions of model parameters given the data to perform probabilistic tractography. Importantly, we performed a direct and comprehensive comparison against classical inference approaches, based on MCMC, to identify optimal training configurations and SBI architectures, and confirm the similarity of voxel-wise estimates and whole-brain tractography reconstructions to an established and heavily used approach. Our findings suggest that SBI with in-silico training can match the performance of classical inference approaches, opening new avenues for amortised inference in challenging likelihood-free scenarios; while offering orders of magnitude of computational speed-up over MCMC.</p><p id="P57">We performed novel explorations for optimal SBI designs, in the context of dMRI modelling. Firstly, we explored how to incorporate noise into training. Previous studies have either not considered or added noise at random values <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>; <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>. We have found that following a multi-level noise scheme improves accuracy and precision of SBI, even if 8-times fewer parameter combinations are considered for training (<xref ref-type="fig" rid="F3">Fig. 3</xref>). From the perspective of neural networks applied to conditional density estimation, this noise-regularisation strategy can be seen as a form of data augmentation and is in line with recent findings <xref ref-type="bibr" rid="R62">Rothfuss et al. (2020)</xref>.</p><p id="P58">Secondly, classical sampling-based, such as MCMC, can be very flexible in handling parameter constraints as prior knowledge. This is more challenging in an SBI framework, where parametric prior distributions are typically expected. We explored how to incorporate boundaries and non-linear constraints in the parameters. We showed that it is possible to train restricted prior objects to learn highly complex and nonlinear constraints for the model parameters that translates into better performance (<xref ref-type="fig" rid="F3">Fig. 3</xref>) and sampling efficiency (<xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). Density estimators can struggle to generalize across the entire parameter space even for simple problems <xref ref-type="bibr" rid="R29">Hermans et al. (2021)</xref>, so a trade-off between enough diversity to capture the high-dimensional parameter space (restricted priors) and having enough, slightly-perturbed, samples (noise strategy) can be beneficial during training, as shown by our results.</p><p id="P59">Thirdly, we explored two strategies for handling model selection in cases of nested multi-compartment models. Recent studies have proposed a classification step that chooses what model complexity to fit <xref ref-type="bibr" rid="R42">Karimi et al. (2024)</xref>; <xref ref-type="bibr" rid="R12">Consagra et al. (2024)</xref> followed by an NPE trained independently on that model. We took a step further and evaluated our implementation of this approach (SBI_ClassiFiber) vs another one where NPE learns the appropriate model complexity by being trained on multiple potential models (SBI_joint). Results in both synthetic and in-vivo brain data suggest a comparable performance of both approaches, with SBI_joint providing slightly worse crossing fibre rate detection, accuracy and precision than SBI_ClassiFiber, but tractography end results suggesting similar or better performance than MCMC (which relied on ARD shrinkage priors <xref ref-type="bibr" rid="R73">Wipf and Nagarajan (2007)</xref>). I.e. training NPE with enough examples from different configurations can be sufficient to learn the model selection at the cost of some precision loss. This is important, as in SBI_ClassiFiber, the classifier and NPEs are trained independently so a misclassification by the former will directly affect the latter without any feedback. However, by learning the complexity indirectly as part of training, model selection and parameter fitting can interact during inference. Such implementations can lead to more advanced SBI network architectures. For instance, in a recent approach <xref ref-type="bibr" rid="R64">Schröder and Macke (2024)</xref>, a classifier conditions the density estimator, but one can envisage feedback between the two in a subsequent design.</p><p id="P60">We evaluated the inference performance of SBI against an MCMC-based framework for the same models, which we have developed in the past <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R31">Hernández et al. (2013)</xref>; <xref ref-type="bibr" rid="R35">Jbabdi et al. (2012)</xref>; Hernandez-Fernandezet al. (2019), and is widely and heavily used as part of FSL <xref ref-type="bibr" rid="R74">Woolrich et al. (2009)</xref>. For both synthetic and in-vivo brain data, we found that SBI closely follows the patterns of MCMC. In terms of agreement in the means of the posterior distributions (as a proxy for accuracy), correlations above 0.9 between SBI and MCMC architectures were found across the brain for most of the parameters in the different models. In terms of uncertainty mapping, correlations above 0.9 were found for simple models (with a single fibre compartment, <xref ref-type="fig" rid="F2">Fig. 2</xref>). As the number of compartments increased, the whole-brain patterns remained (i.e. maximum orientation uncertainty in GM and CSF, lower in WM) but orientation uncertainty estimated by SBI in the WM was generally higher than by MCMC (<xref ref-type="fig" rid="F6">Fig. 6</xref>). Longer training did not change these trends considerably (results not shown). Despite these seemingly large differences in uncertainty mapping, probabilistic tractography (that spatially propagates uncertainty in orientation estimates) using MCMC and SBI approaches returned very similar results. In fact, SBI-based tract reconstructions agreed better with a population-level white matter atlas, were more reproducible across repeats of the same MRI experiment, and their differences with MCMC-based reconstructions had a magnitude smaller than expected from scan-rescan variability (<xref ref-type="fig" rid="F7">Figs. 7</xref>, <xref ref-type="fig" rid="F9">9</xref>).</p><p id="P61">We explored what could drive this discrepancy between uncertainty mapping differences at the voxel vs whole-brain level. One clear difference is how uncertainty is depicted at the two levels: a) At the voxel-level, we used the approach implemented in FSL <xref ref-type="bibr" rid="R6">Behrens et al. (2007)</xref>; <xref ref-type="bibr" rid="R31">Hernández et al. (2013)</xref>, which groups orientation samples into a finite set of <italic>N</italic> fibre compartments and subsequently calculates their mean orientation and spread of samples around it using vector dyadic products <xref ref-type="bibr" rid="R36">Jones (2003)</xref>. This provides a mean orientation and uncertainty around it for each fibre compartment supported per voxel. b) At the whole-brain level probabilistic tractography propagates using directly the orientation samples, i.e. from the joint posterior of orientations. Hence, if the grouping of orientation samples into finite sets is not consistent between MCMC and SBI, large differences can be observed at the voxel level and not at the tractography level. In <xref ref-type="fig" rid="F8">Fig. 8</xref> we presented scenarios and confirmed with examples from the data where this occurred.</p><p id="P62">Fitting the wrong model to data can be one such scenario. This adds epistemic uncertainty and the challenge of what the mapped uncertainty represents. In cases of multi-modal orientation distributions, we observed how SBI could capture all the modes (even with a model with fewer compartments than modes), spreading the samples across modes and adding into orientation uncertainty. While the MCMC implementation sampled only from certain modes, providing samples with high precision, but completely ignoring an existing mode of the target distribution (<xref ref-type="fig" rid="F8">Fig. 8A</xref>). Multiple modes can also arise from non-injective models, where different combinations of parameter values produce similarly likely solutions (i.e., degeneracies). This is a known challenge for certain microstructural models and the multi-modal posterior can be used to evaluate the different likely solutions, as illustrated in Jallais et al <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref>. Here, by similar mechanisms, we have identified other type of degeneracies using SBI due to compartment swapping (<xref ref-type="sec" rid="S21">Section 3.3.1</xref>).</p><sec id="S25"><label>4.1</label><title>Limitations and Future work</title><p id="P63">Even if we aimed to match as closely as possible MCMC to SBI implementations in order to compare their performance, a perfect matching is not feasible as they represent quite different approaches. A number of algorithmic features can be contributing to observed differences. Firstly, the MCMC implementation relies on ARD shrinkage priors, which introduce large a-priori penalties on model complexity and have considerable effects on fibre crossing detection and on uncertainty mapping. Such improper priors cannot be implemented for SBI in a straightforward manner. Secondly, SBI networks use a Kullback-Leibler divergence loss function to train NPEs. Due to the mode-covering property of the KL divergence, the approximate posterior will always cover the true posterior <xref ref-type="bibr" rid="R53">Minka (2005)</xref> with deviations leading to broader posteriors. In density estimators, this is known as <italic>leakage</italic> and is particularly problematic in neural spline flows because they do not have analytical corrections pap (2016); <xref ref-type="bibr" rid="R45">Lueckmann et al. (2017)</xref>;?); <xref ref-type="bibr" rid="R26">Greenberg et al. (2019)</xref>. Longer training or refinements, such as importance sampling have been proposed to compensate to some extent to converge to the true posterior <xref ref-type="bibr" rid="R51">Midgley et al. (2023)</xref>; <xref ref-type="bibr" rid="R14">Dax et al. (2023)</xref> but it is still an open problem that requires more research. Thirdly, the Metropolis-Hastings MCMC implementation used for comparisons is based on local sampling techniques. Hence, the energy needed to jump from one mode of a posterior to another can be very high. In this way, MCMC is more likely to converge to one of the modes and “stick to it” than SBI, missing other likely modes. This also suggests that SBI can perform a better global search than MCMC methods, which highly depends on the initialisation and the progression of the chain. Despite all these factors, the level of similarity between MCMC and SBI in local and global estimates was remarkable and considerably higher than the differences.</p><p id="P64">A limitation specific to SBI is that training against the dMRI signal involves model predictions for a particular acquisition scheme, i.e. a particular set of <italic>b</italic> values and diffusion-sensitising gradient orientations in the case of dMRI. Even if re-training for new data and acquisition schemes can be done once and is relatively cheap, another approach would be to represent the dMRI signal by a number of orthonormal sets of basis functions. This would allow acquisition-agnostic inference, as NPEs are trained against generalisable basis sets rather than signal sampled with a specific scheme. We explored training against such representations, including spherical harmonics (for single-shell) and MAP-MRI (for multi-shell data). Results in both synthetic and in-vivo brain data suggested that NPEs trained against both representations could provide similar accuracy performance compared to NPEs trained against the signal, at the cost of some precision loss (<xref ref-type="fig" rid="F3">Fig. 3B</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>). This agrees with results in <xref ref-type="bibr" rid="R32">Jallais and Palombo (2024)</xref> where signal feature selection was done in a data-driven fashion.</p><p id="P65">In summary, we argue that SBI is a powerful approach for performing Bayesian inference and has shown comparable performance to an established and heavily-used MCMC-based framework. Although further research and validation is needed, SBI could pave the way for tackling unaddressed problems in neuroimaging, given its reliance on in-silico, likelihood-free forward predictions and its scalability to multi-dimensional problems, the two main limitations of classical Bayesian inference approaches.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS200379-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d31aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S26"><title>Acknowledgements</title><p>JP and SS are supported by an ERC Consolidator Grant (101000969), and JP is also supported by a Wellcome Trust bioimaging technology award (313367/Z/24/Z). MD, CS and JHM by the German Research Foundation (DFG) through Germany’s Excellence Strategy (EXC-Number 2064/1, PN 390727645) and SFB1233 (PN 276693517), the German Federal Ministry of Education and Research (Tübingen AI Center, FKZ: 01IS18039) the Carl Zeiss Foundation and the Else Kröner Fresenius Stiftung (Project ClinbrAIn).</p></ack><ref-list><ref id="R1"><element-citation publication-type="book"><source>Fast $\epsilon$-Free Inference of Simulation Models with Bayesian Conditional Density Estimation</source><publisher-name>Curran Associates, Inc</publisher-name><year>2016</year></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DC</given-names></name></person-group><article-title>A general framework for experiment design in diffusion MRI and its application in measuring direct tissue-microstructure features</article-title><source>Magn Reson Med</source><year>2008</year><volume>60</volume><issue>2</issue><fpage>439</fpage><lpage>448</lpage><pub-id pub-id-type="pmid">18666109</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Dyrby</surname><given-names>TB</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>Imaging brain microstructure with diffusion MRI: practicality and applications</article-title><source>NMR in Biomedicine</source><year>2019</year><volume>32</volume><issue>4</issue><pub-id pub-id-type="pmid">29193413</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaumont</surname><given-names>MA</given-names></name></person-group><article-title>Approximate Bayesian Computation in Evolution and Ecology</article-title><source>Annual Review of Ecology, Evolution, and Systematics</source><year>2010</year><volume>41</volume><fpage>379</fpage><lpage>406</lpage><comment>Volume 41, 2010</comment></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaumont</surname><given-names>MA</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Balding</surname><given-names>DJ</given-names></name></person-group><article-title>Approximate Bayesian Computation in Population Genetics</article-title><source>Genetics</source><year>2002</year><volume>162</volume><issue>4</issue><fpage>2025</fpage><lpage>2035</lpage><pub-id pub-id-type="pmcid">PMC1462356</pub-id><pub-id pub-id-type="pmid">12524368</pub-id><pub-id pub-id-type="doi">10.1093/genetics/162.4.2025</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Berg</surname><given-names>HJ</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Rushworth</surname><given-names>M</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name></person-group><article-title>Probabilistic diffusion tractography with multiple fibre orientations: What can we gain?</article-title><source>NeuroImage</source><year>2007</year><volume>34</volume><issue>1</issue><fpage>144</fpage><lpage>155</lpage><pub-id pub-id-type="pmcid">PMC7116582</pub-id><pub-id pub-id-type="pmid">17070705</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.09.018</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Nunes</surname><given-names>R</given-names></name><name><surname>Clare</surname><given-names>S</given-names></name><name><surname>Matthews</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>J</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Characterization and propagation of uncertainty in diffusion-weighted MR imaging</article-title><source>Magn Reson Med</source><year>2003</year><volume>50</volume><issue>5</issue><fpage>1077</fpage><lpage>1088</lpage><pub-id pub-id-type="pmid">14587019</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>AS</given-names></name><name><surname>Chen</surname><given-names>N-k</given-names></name><name><surname>Trouard</surname><given-names>TP</given-names></name></person-group><article-title>Bootstrap analysis of diffusion tensor and mean apparent propagator parameters derived from multiband diffusion MRI</article-title><source>Magn Reson Med</source><year>2019</year><volume>82</volume><issue>5</issue><fpage>1796</fpage><lpage>1803</lpage><pub-id pub-id-type="pmcid">PMC7041889</pub-id><pub-id pub-id-type="pmid">31155758</pub-id><pub-id pub-id-type="doi">10.1002/mrm.27833</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Kucukelbir</surname><given-names>A</given-names></name><name><surname>McAuliffe</surname><given-names>JD</given-names></name></person-group><article-title>Variational Inference: A Review for Statisticians</article-title><source>Journal of the American Statistical Association</source><year>2017</year><issue>518</issue><fpage>859</fpage><lpage>877</lpage><comment>arXiv: 1601.00670</comment></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname><given-names>MGB</given-names></name><name><surname>François</surname><given-names>O</given-names></name></person-group><article-title>Non-linear regression models for Approximate Bayesian Computation</article-title><source>Stat Comput</source><year>2009</year><volume>20</volume><issue>1</issue><fpage>63</fpage><lpage>73</lpage></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chappell</surname><given-names>MA</given-names></name><name><surname>Groves</surname><given-names>AR</given-names></name><name><surname>Whitcher</surname><given-names>B</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Variational Bayesian Inference for a Nonlinear Forward Model</article-title><source>IEEE Transactions on Signal Processing</source><year>2009</year><volume>57</volume><issue>1</issue><fpage>223</fpage><lpage>236</lpage></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Consagra</surname><given-names>W</given-names></name><name><surname>Ning</surname><given-names>L</given-names></name><name><surname>Rathi</surname><given-names>Y</given-names></name></person-group><article-title>A Deep Learning Approach to Multi-Fiber Parameter Estimation and Uncertainty Quantification in Diffusion MRI</article-title><year>2024</year></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname><given-names>K</given-names></name><name><surname>Brehmer</surname><given-names>J</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name></person-group><article-title>The frontier of simulation-based inference</article-title><source>Proc Natl Acad Sci USA</source><year>2020</year><volume>117</volume><issue>48</issue><fpage>30055</fpage><lpage>30062</lpage><pub-id pub-id-type="pmcid">PMC7720103</pub-id><pub-id pub-id-type="pmid">32471948</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dax</surname><given-names>M</given-names></name><name><surname>Green</surname><given-names>SR</given-names></name><name><surname>Gair</surname><given-names>J</given-names></name><name><surname>Pürrer</surname><given-names>M</given-names></name><name><surname>Wildberger</surname><given-names>J</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Buonanno</surname><given-names>A</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><article-title>Neural Importance Sampling for Rapid and Reliable Gravitational-Wave Inference</article-title><source>Phys Rev Lett</source><year>2023</year><volume>130</volume><issue>17</issue><elocation-id>171403</elocation-id><pub-id pub-id-type="pmid">37172245</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Goncalves</surname><given-names>PJ</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><source>Truncated proposals for scalable and hassle-free simulation-based inference</source><conf-name>Thirty-Sixth Conference on Neural Information Processing Systems</conf-name><year>2022a</year></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Gonçalves</surname><given-names>PJ</given-names></name></person-group><article-title>Energy-efficient network activity from disparate circuit parameters</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022b</year><volume>119</volume><issue>44</issue><elocation-id>e2207632119</elocation-id><pub-id pub-id-type="pmcid">PMC9636970</pub-id><pub-id pub-id-type="pmid">36279461</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2207632119</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Descoteaux</surname><given-names>M</given-names></name><name><surname>Angelino</surname><given-names>E</given-names></name><name><surname>Fitzgibbons</surname><given-names>S</given-names></name><name><surname>Deriche</surname><given-names>R</given-names></name></person-group><article-title>Regularized, fast, and robust analytical Q-ball imaging</article-title><source>Magnetic Resonance in Medicine</source><year>2007</year><volume>58</volume><issue>3</issue><fpage>497</fpage><lpage>510</lpage><pub-id pub-id-type="pmid">17763358</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dockhorn</surname><given-names>T</given-names></name><name><surname>Ritchie</surname><given-names>JA</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><article-title>Density Deconvolution with Normalizing Flows</article-title><source>arXiv:2006.09396 [cs, stat]</source><year>2020</year></element-citation></ref><ref id="R19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Bekasov</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name><name><surname>Papamakarios</surname><given-names>G</given-names></name></person-group><chapter-title>Neural Spline Flows</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2019</year><volume>32</volume></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><source>Sequential Neural Methods for Likelihood-free Inference</source><year>2018</year></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggl</surname><given-names>MF</given-names></name><name><surname>Santis</surname><given-names>SD</given-names></name></person-group><article-title>More with less: Simulation-based inference enables accurate diffusion-weighted MRI with minimal acquisition time</article-title><year>2024</year></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garyfallidis</surname><given-names>E</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Amirbekian</surname><given-names>B</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>van der Walt</surname><given-names>S</given-names></name><name><surname>Descoteaux</surname><given-names>M</given-names></name><name><surname>Nimmo-Smith</surname><given-names>I</given-names></name><collab>Dipy Contributors</collab></person-group><article-title>Dipy, a library for the analysis of diffusion MRI data</article-title><source>Front Neuroinform</source><year>2014</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC3931231</pub-id><pub-id pub-id-type="pmid">24600385</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00008</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Geffner</surname><given-names>T</given-names></name><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Mnih</surname><given-names>A</given-names></name></person-group><source>Compositional score modeling for simulation-based inference</source><conf-name>International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2023</year><fpage>11098</fpage><lpage>11116</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Girard</surname><given-names>G</given-names></name><name><surname>Aydogan</surname><given-names>DB</given-names></name><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Descoteaux</surname><given-names>M</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><chapter-title>14. probabilistic tractography</chapter-title><person-group person-group-type="editor"><name><surname>Dell’Acqua</surname><given-names>F</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Descoteaux</surname><given-names>M</given-names></name></person-group><source>Handbook of Diffusion MRI Tractography</source><publisher-name>Elsevier</publisher-name><year>2024</year></element-citation></ref><ref id="R25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gloeckler</surname><given-names>M</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Weilbach</surname><given-names>CD</given-names></name><name><surname>Wood</surname><given-names>F</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><source>All-in-one simulation-based inference</source><conf-name>Forty-first International Conference on Machine Learning</conf-name><year>2024</year></element-citation></ref><ref id="R26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greenberg</surname><given-names>D</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>J</given-names></name></person-group><source>Automatic Posterior Transformation for Likelihood-Free Inference</source><conf-name>Proceedings of the 36th International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2019</year><fpage>2404</fpage><lpage>2414</lpage></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Eklund</surname><given-names>A</given-names></name><name><surname>Özarslan</surname><given-names>E</given-names></name><name><surname>Knutsson</surname><given-names>H</given-names></name></person-group><article-title>Using the Wild Bootstrap to Quantify Uncertainty in Mean Apparent Propagator MRI</article-title><source>Frontiers in Neuroinformatics</source><year>2019</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC6581745</pub-id><pub-id pub-id-type="pmid">31244637</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2019.00043</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harms</surname><given-names>RL</given-names></name><name><surname>Fritz</surname><given-names>FJ</given-names></name><name><surname>Schoenmakers</surname><given-names>S</given-names></name><name><surname>Roebroeck</surname><given-names>A</given-names></name></person-group><article-title>Fast and robust quantification of uncertainty in non-linear diffusion MRI models</article-title><source>NeuroImage</source><year>2024</year><volume>285</volume><elocation-id>120496</elocation-id><pub-id pub-id-type="pmid">38101495</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermans</surname><given-names>J</given-names></name><name><surname>Delaunoy</surname><given-names>A</given-names></name><name><surname>Rozet</surname><given-names>F</given-names></name><name><surname>Wehenkel</surname><given-names>A</given-names></name><name><surname>Louppe</surname><given-names>G</given-names></name></person-group><source>Averting A Crisis In Simulation-Based Inference</source><year>2021</year></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernandez-Fernandez</surname><given-names>M</given-names></name><name><surname>Reguly</surname><given-names>I</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Giles</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><article-title>Using GPUs to accelerate computational diffusion MRI: From microstructure estimation to tractography and connectomes</article-title><source>NeuroImage</source><year>2019</year><volume>188</volume><fpage>598</fpage><lpage>615</lpage><pub-id pub-id-type="pmcid">PMC6614035</pub-id><pub-id pub-id-type="pmid">30537563</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.12.015</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernández</surname><given-names>M</given-names></name><name><surname>Guerrero</surname><given-names>GD</given-names></name><name><surname>Cecilia</surname><given-names>JM</given-names></name><name><surname>García</surname><given-names>JM</given-names></name><name><surname>Inuggi</surname><given-names>A</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><article-title>Accelerating Fibre Orientation Estimation from Diffusion Weighted Magnetic Resonance Imaging Using GPUs</article-title><source>PLOS ONE</source><year>2013</year><volume>8</volume><issue>4</issue><elocation-id>e61892</elocation-id><pub-id pub-id-type="pmcid">PMC3643787</pub-id><pub-id pub-id-type="pmid">23658616</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0061892</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jallais</surname><given-names>M</given-names></name><name><surname>Palombo</surname><given-names>M</given-names></name></person-group><article-title>mu-GUIDE: a framework for quantitative imaging via generalized uncertainty-driven inference using deep learning</article-title><source>eLife</source><year>2024</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC11594529</pub-id><pub-id pub-id-type="pmid">39589260</pub-id><pub-id pub-id-type="doi">10.7554/eLife.101069</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jallais</surname><given-names>M</given-names></name><name><surname>Rodrigues</surname><given-names>PLC</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Wassermann</surname><given-names>D</given-names></name></person-group><article-title>Inverting brain grey matter models with likelihood-free inference: a tool for trustable cytoarchitecture measurements</article-title><source>Melba</source><year>2022</year><volume>1</volume><fpage>1</fpage><lpage>28</lpage></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Haber</surname><given-names>SN</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><article-title>Measuring macroscopic brain connections in vivo</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><issue>11</issue><fpage>1546</fpage><lpage>1555</lpage><pub-id pub-id-type="pmid">26505566</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Savio</surname><given-names>AM</given-names></name><name><surname>Graña</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>Model-based analysis of multishell diffusion MR data for tractography: How to get over fitting problems</article-title><source>Magnetic Resonance in Medicine</source><year>2012</year><volume>68</volume><issue>6</issue><fpage>1846</fpage><lpage>1855</lpage><pub-id pub-id-type="pmcid">PMC3359399</pub-id><pub-id pub-id-type="pmid">22334356</pub-id><pub-id pub-id-type="doi">10.1002/mrm.24204</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><article-title>Determining and visualizing uncertainty in estimates of fiber orientation from diffusion tensor MRI</article-title><source>Magn Reson Med</source><year>2003</year><volume>49</volume><issue>1</issue><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">12509814</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DK</given-names></name></person-group><article-title>Tractography Gone Wild: Probabilistic Fibre Tracking Using the Wild Bootstrap With Diffusion Tensor MRI</article-title><source>IEEE Transactions on Medical Imaging</source><year>2008</year><volume>27</volume><issue>9</issue><fpage>1268</fpage><lpage>1274</lpage><pub-id pub-id-type="pmid">18779066</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaden</surname><given-names>E</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name><name><surname>Knösche</surname><given-names>TR</given-names></name></person-group><article-title>Variational inference of the fiber orientation density using diffusion MR imaging</article-title><source>NeuroImage</source><year>2008</year><volume>42</volume><issue>4</issue><fpage>1366</fpage><lpage>1380</lpage><pub-id pub-id-type="pmid">18603006</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaden</surname><given-names>E</given-names></name><name><surname>Knösche</surname><given-names>TR</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name></person-group><article-title>Parametric spherical deconvolution: Inferring anatomical connectivity using diffusion MR imaging</article-title><source>NeuroImage</source><year>2007</year><volume>37</volume><issue>2</issue><fpage>474</fpage><lpage>488</lpage><pub-id pub-id-type="pmid">17596967</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaden</surname><given-names>E</given-names></name><name><surname>Kruggel</surname><given-names>F</given-names></name></person-group><article-title>Nonparametric Bayesian inference of the fiber orientation distribution from diffusion-weighted MR images</article-title><source>Medical Image Analysis</source><year>2012</year><volume>16</volume><issue>4</issue><fpage>876</fpage><lpage>888</lpage><pub-id pub-id-type="pmid">22381587</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi</surname><given-names>D</given-names></name><name><surname>Warfield</surname><given-names>SK</given-names></name></person-group><article-title>Diffusion mri with machine learning</article-title><source>Imaging Neuroscience</source><year>2024</year><volume>2</volume><fpage>1</fpage><lpage>55</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karimi</surname><given-names>HS</given-names></name><name><surname>Pal</surname><given-names>A</given-names></name><name><surname>Ning</surname><given-names>L</given-names></name><name><surname>Rathi</surname><given-names>Y</given-names></name></person-group><article-title>Likelihood-free posterior estimation and uncertainty quantification for diffusion MRI models</article-title><source>Imaging Neuroscience</source><year>2024</year><volume>2</volume><fpage>1</fpage><lpage>22</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauermann</surname><given-names>G</given-names></name><name><surname>Claeskens</surname><given-names>G</given-names></name><name><surname>Opsomer</surname><given-names>JD</given-names></name></person-group><article-title>Bootstrapping for Penalized Spline Regression</article-title><source>Journal of Computational and Graphical Statistics</source><year>2009</year><volume>18</volume><issue>1</issue><fpage>126</fpage><lpage>146</lpage></element-citation></ref><ref id="R44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Boelts</surname><given-names>J</given-names></name><name><surname>Greenberg</surname><given-names>D</given-names></name><name><surname>Goncalves</surname><given-names>P</given-names></name><name><surname>Macke</surname><given-names>J</given-names></name></person-group><source>Benchmarking Simulation-Based Inference</source><conf-name>Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2021</year><fpage>343</fpage><lpage>351</lpage></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Goncalves</surname><given-names>PJ</given-names></name><name><surname>Bassetto</surname><given-names>G</given-names></name><name><surname>Öcal</surname><given-names>K</given-names></name><name><surname>Nonnenmacher</surname><given-names>M</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><chapter-title>Flexible statistical inference for mechanistic models of neural dynamics</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2017</year><volume>30</volume></element-citation></ref><ref id="R46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Magdon-Ismail</surname><given-names>M</given-names></name><name><surname>Atiya</surname><given-names>A</given-names></name></person-group><chapter-title>Neural Networks for Density Estimation</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><year>1998</year><volume>11</volume></element-citation></ref><ref id="R47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Manzano Patron</surname><given-names>JP</given-names></name></person-group><source>On noise, uncertainty and inference for computational diffusion MRI</source><publisher-name>PhD Thesis, University of Nottingham</publisher-name><year>2023</year></element-citation></ref><ref id="R48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Manzano-Patron</surname><given-names>JP</given-names></name><name><surname>Kypraios</surname><given-names>T</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><source>Amortised inference in diffusion mri biophysical models using artificial neural networks and simulation-based frameworks</source><conf-name>ISMRM (Proc Intl Soc Mag Reson Med 30)</conf-name><year>2022</year></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manzano Patron</surname><given-names>JP</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><article-title>Denoising diffusion MRI: Considerations and implications for analysis</article-title><source>Imaging Neuroscience</source><year>2024</year><volume>2</volume><fpage>1</fpage><lpage>29</lpage></element-citation></ref><ref id="R50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Masutani</surname><given-names>Y</given-names></name></person-group><source>Noise Level Matching Improves Robustness of Diffusion Mri Parameter Inference by Synthetic Q-Space Learning</source><conf-name>2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)</conf-name><year>2019</year><fpage>139</fpage><lpage>142</lpage></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Midgley</surname><given-names>LI</given-names></name><name><surname>Stimper</surname><given-names>V</given-names></name><name><surname>Simm</surname><given-names>GNC</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Hernández-Lobato</surname><given-names>JM</given-names></name></person-group><source>Flow Annealed Importance Sampling Bootstrap</source><year>2023</year></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Bangerter</surname><given-names>NK</given-names></name><name><surname>Thomas</surname><given-names>DL</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Bartsch</surname><given-names>AJ</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><etal/></person-group><article-title>Multimodal population brain imaging in the UK Biobank prospective epidemiological study</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><issue>11</issue><fpage>1523</fpage><lpage>1536</lpage><pub-id pub-id-type="pmcid">PMC5086094</pub-id><pub-id pub-id-type="pmid">27643430</pub-id><pub-id pub-id-type="doi">10.1038/nn.4393</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minka</surname><given-names>T</given-names></name></person-group><source>Divergence measures and message passing</source><year>2005</year></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ning</surname><given-names>L</given-names></name><name><surname>Szczepankiewicz</surname><given-names>F</given-names></name><name><surname>Nilsson</surname><given-names>M</given-names></name><name><surname>Rathi</surname><given-names>Y</given-names></name><name><surname>Westin</surname><given-names>C-F</given-names></name></person-group><article-title>Probing tissue microstructure by diffusion skewness tensor imaging</article-title><source>Sci Rep</source><year>2021</year><volume>11</volume><issue>1</issue><fpage>135</fpage><pub-id pub-id-type="pmcid">PMC7794496</pub-id><pub-id pub-id-type="pmid">33420140</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-79748-3</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novikov</surname><given-names>DS</given-names></name></person-group><article-title>The present and the future of microstructure MRI: From a paradigm shift to normal science</article-title><source>Journal of Neuroscience Methods</source><year>2021</year><fpage>351</fpage><pub-id pub-id-type="pmcid">PMC7987839</pub-id><pub-id pub-id-type="pmid">33096152</pub-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2020.108947</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novikov</surname><given-names>DS</given-names></name><name><surname>Veraart</surname><given-names>J</given-names></name><name><surname>Jelescu</surname><given-names>IO</given-names></name><name><surname>Fieremans</surname><given-names>E</given-names></name></person-group><article-title>Rotationally-invariant mapping of scalar and orientational metrics of neuronal microstructure with diffusion MRI</article-title><source>NeuroImage</source><year>2018</year><volume>174</volume><fpage>518</fpage><lpage>538</lpage><pub-id pub-id-type="pmcid">PMC5949281</pub-id><pub-id pub-id-type="pmid">29544816</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.03.006</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pajevic</surname><given-names>S</given-names></name><name><surname>Basser</surname><given-names>PJ</given-names></name></person-group><article-title>Parametric and non-parametric statistical analysis of DT-MRI data</article-title><source>Journal of Magnetic Resonance</source><year>2003</year><volume>161</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmid">12660106</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palombo</surname><given-names>M</given-names></name><name><surname>Ianus</surname><given-names>A</given-names></name><name><surname>Guerreri</surname><given-names>M</given-names></name><name><surname>Nunes</surname><given-names>D</given-names></name><name><surname>Alexander</surname><given-names>DC</given-names></name><name><surname>Shemesh</surname><given-names>N</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name></person-group><article-title>SANDI: A compartment-based model for non-invasive apparent soma and neurite imaging by diffusion MRI</article-title><source>Neuroimage</source><year>2020</year><volume>215</volume><elocation-id>116835</elocation-id><pub-id pub-id-type="pmcid">PMC8543044</pub-id><pub-id pub-id-type="pmid">32289460</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116835</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Nalisnick</surname><given-names>E</given-names></name><name><surname>Rezende</surname><given-names>DJ</given-names></name><name><surname>Mohamed</surname><given-names>S</given-names></name><name><surname>Lakshminarayanan</surname><given-names>B</given-names></name></person-group><article-title>Normalizing Flows for Probabilistic Modeling and Inference</article-title><source>Journal of Machine Learning Research</source><year>2021</year><volume>22</volume><issue>57</issue><fpage>1</fpage><lpage>64</lpage></element-citation></ref><ref id="R60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Pavlakou</surname><given-names>T</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><chapter-title>Masked autoregressive flow for density estimation</chapter-title><person-group person-group-type="editor"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Luxburg</surname><given-names>UV</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Vishwanathan</surname><given-names>S</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2017</year><volume>30</volume></element-citation></ref><ref id="R61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname><given-names>G</given-names></name><name><surname>Sterratt</surname><given-names>D</given-names></name><name><surname>Murray</surname><given-names>I</given-names></name></person-group><source>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</source><person-group person-group-type="editor"><name><surname>Chaudhuri</surname><given-names>K</given-names></name><name><surname>Sugiyama</surname><given-names>M</given-names></name></person-group><conf-name>Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2019</year><fpage>837</fpage><lpage>848</lpage></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothfuss</surname><given-names>J</given-names></name><name><surname>Ferreira</surname><given-names>F</given-names></name><name><surname>Boehm</surname><given-names>S</given-names></name><name><surname>Walther</surname><given-names>S</given-names></name><name><surname>Ulrich</surname><given-names>M</given-names></name><name><surname>Asfour</surname><given-names>T</given-names></name><name><surname>Krause</surname><given-names>A</given-names></name></person-group><source>Noise Regularization for Conditional Density Estimation</source><year>2020</year></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvador</surname><given-names>R</given-names></name><name><surname>Peña</surname><given-names>A</given-names></name><name><surname>Menon</surname><given-names>DK</given-names></name><name><surname>Carpenter</surname><given-names>TA</given-names></name><name><surname>Pickard</surname><given-names>JD</given-names></name><name><surname>Bullmore</surname><given-names>ET</given-names></name></person-group><article-title>Formal characterization and extension of the linearized diffusion tensor model: Linearized Diffusion Tensor Model</article-title><source>Hum Brain Mapp</source><year>2005</year><volume>24</volume><issue>2</issue><fpage>144</fpage><lpage>155</lpage><pub-id pub-id-type="pmcid">PMC6871750</pub-id><pub-id pub-id-type="pmid">15468122</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20076</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schröder</surname><given-names>C</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name></person-group><source>Simultaneous identification of models and parameters of scientific simulators</source><year>2024</year></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharrock</surname><given-names>L</given-names></name><name><surname>Simons</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Beaumont</surname><given-names>M</given-names></name></person-group><article-title>Sequential neural score estimation: Likelihood-free inference with conditional score based diffusion models</article-title><source>arXiv preprint</source><year>2022</year></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sisson</surname><given-names>SA</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name><name><surname>Beaumont</surname><given-names>MA</given-names></name></person-group><article-title>Overview of Approximate Bayesian Computation</article-title><source>arXiv:1802.09720 [stat]</source><year>2018</year></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>JL</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>RubiX: Combining Spatial Resolutions for Bayesian Inference of Crossing Fibers in Diffusion MRI</article-title><source>IEEE Trans Med Imaging</source><year>2013</year><volume>32</volume><issue>6</issue><fpage>969</fpage><lpage>982</lpage><pub-id pub-id-type="pmcid">PMC3767112</pub-id><pub-id pub-id-type="pmid">23362247</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2012.2231873</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavaré</surname><given-names>S</given-names></name><name><surname>Balding</surname><given-names>DJ</given-names></name><name><surname>Griffiths</surname><given-names>RC</given-names></name><name><surname>Donnelly</surname><given-names>P</given-names></name></person-group><article-title>Inferring coalescence times from DNA sequence data</article-title><source>Genetics</source><year>1997</year><volume>145</volume><issue>2</issue><fpage>505</fpage><lpage>518</lpage><pub-id pub-id-type="pmcid">PMC1207814</pub-id><pub-id pub-id-type="pmid">9071603</pub-id><pub-id pub-id-type="doi">10.1093/genetics/145.2.505</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Boelts</surname><given-names>J</given-names></name><name><surname>Deistler</surname><given-names>M</given-names></name><name><surname>Lueckmann</surname><given-names>J-M</given-names></name><name><surname>Durkan</surname><given-names>C</given-names></name><name><surname>Gonçalves</surname><given-names>P</given-names></name><name><surname>Greenberg</surname><given-names>D</given-names></name><name><surname>Macke</surname><given-names>J</given-names></name></person-group><article-title>sbi: A toolkit for simulation-based inference</article-title><source>JOSS</source><year>2020</year><volume>5</volume><issue>52</issue><fpage>2505</fpage></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warrington</surname><given-names>S</given-names></name><name><surname>Bryant</surname><given-names>KL</given-names></name><name><surname>Khrapitchev</surname><given-names>AA</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Charquero-Ballester</surname><given-names>M</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><article-title>XTRACT - Standardised protocols for automated tractography in the human and macaque brain</article-title><source>NeuroImage</source><year>2020</year><volume>217</volume><elocation-id>116923</elocation-id><pub-id pub-id-type="pmcid">PMC7260058</pub-id><pub-id pub-id-type="pmid">32407993</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116923</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitcher</surname><given-names>B</given-names></name><name><surname>Tuch</surname><given-names>DS</given-names></name><name><surname>Wisco</surname><given-names>JJ</given-names></name><name><surname>Sorensen</surname><given-names>AG</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Using the wild bootstrap to quantify uncertainty in diffusion tensor imaging</article-title><source>Hum Brain Mapp</source><year>2008</year><volume>29</volume><issue>3</issue><fpage>346</fpage><lpage>362</lpage><pub-id pub-id-type="pmcid">PMC6870960</pub-id><pub-id pub-id-type="pmid">17455199</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20395</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wildberger</surname><given-names>J</given-names></name><name><surname>Dax</surname><given-names>M</given-names></name><name><surname>Buchholz</surname><given-names>S</given-names></name><name><surname>Green</surname><given-names>S</given-names></name><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name></person-group><article-title>Flow matching for scalable simulation-based inference</article-title><source>Advances in Neural Information Processing Systems</source><year>2024</year><volume>36</volume></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wipf</surname><given-names>DP</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><source>A New View of Automatic Relevance Determination</source><year>2007</year><fpage>8</fpage></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Patenaude</surname><given-names>B</given-names></name><name><surname>Chappell</surname><given-names>M</given-names></name><name><surname>Makni</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Beckmann</surname><given-names>C</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Bayesian analysis of neuroimaging data in FSL</article-title><source>NeuroImage</source><year>2009</year><volume>45</volume><issue>1</issue><fpage>S173</fpage><lpage>S186</lpage><pub-id pub-id-type="pmid">19059349</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yendiki</surname><given-names>A</given-names></name><name><surname>Panneck</surname><given-names>P</given-names></name><name><surname>Srinivasan</surname><given-names>P</given-names></name><name><surname>Stevens</surname><given-names>A</given-names></name><name><surname>Zöllei</surname><given-names>L</given-names></name><name><surname>Augustinack</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Salat</surname><given-names>D</given-names></name><name><surname>Ehrlich</surname><given-names>S</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><etal/></person-group><article-title>Automated Probabilistic Reconstruction of White-Matter Pathways in Health and Disease Using an Atlas of the Underlying Anatomy</article-title><source>Frontiers in Neuroinformatics</source><year>2011</year><volume>5</volume><fpage>23</fpage><pub-id pub-id-type="pmcid">PMC3193073</pub-id><pub-id pub-id-type="pmid">22016733</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00023</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozarslan</surname><given-names>E</given-names></name><name><surname>Koay</surname><given-names>CG</given-names></name><name><surname>Shepherd</surname><given-names>TM</given-names></name><name><surname>Komlosh</surname><given-names>ME</given-names></name><name><surname>Irfanoglu</surname><given-names>MO</given-names></name><name><surname>Pierpaoli</surname><given-names>C</given-names></name><name><surname>Basser</surname><given-names>PJ</given-names></name></person-group><article-title>Mean apparent propagator (MAP) MRI: a novel diffusion imaging method for mapping tissue microstructure</article-title><source>Neuroimage</source><year>2013</year><volume>78</volume><fpage>16</fpage><lpage>32</lpage><pub-id pub-id-type="pmcid">PMC4059870</pub-id><pub-id pub-id-type="pmid">23587694</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.04.016</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Schematic overview of Markov-Chain Monte-Carlo (MCMC) (an example of Classical Bayesian Inference) and Simulation-Based Inference (SBI).</title><p>For NPE, an artificial neural network (ANN) is trained on a set {<bold>Ω</bold><sub>i</sub>, <bold><italic>Y<sub>i</sub></italic></bold>} obtained from the forward simulator, to learn the latent mapping <italic>q<sub>Φ</sub></italic> between observations and posterior samples.</p></caption><graphic xlink:href="EMS200379-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Comparison of SBI and MCMC estimates for a single-fibre model (<italic>N</italic>= 1) in case of single-shell (A,B) or multi-shell (C) data.</title><p>A) Example posterior density plots of the anisotropic volume fraction <italic>f<sub>1</sub></italic>, as estimated by MCMC (blue) and SBI (orange), in four representative voxels: Centrum Semiovale (where fibre crossings are expected), highly anisotropic Corpus Callosum, CSF-filled ventricles, and gray matter. B) Whole-brain maps of estimated parameters using single-shell data (from left to right): Mean of <italic>d</italic> posterior, mean of <italic>f<sub>1</sub></italic> posterior, mean of <bold><italic>v</italic></bold><sub>1</sub> posterior, orientation uncertainty (width of the <bold><italic>v</italic></bold><sub>1</sub> posterior). C) Similar as in (B) for multi-shell data. For (B, C), density scatter plots represent the agreement of estimates throughout the brain; the correlation <italic>r</italic> is calculated for scalar parameters, and the angular difference (in degrees) is calculated for the mean fibre orientations. NPEs were trained with 1 million samples from uniform priors with random SNR values (within the range of 3 to 80).</p></caption><graphic xlink:href="EMS200379-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Evaluating performance of different SBI designs in estimating model parameters in synthetic fibre crossings <italic>(N =</italic> 2), where ground truth is known.</title><p>dMRI was synthetically generated for patterns with two crossing fibres, varying the anisotropic volume fraction f<sub>1</sub> and their crossing angle (rest of parameters were <italic>d = 0.0012mm<sup>2</sup></italic> <italic>/s, f</italic><sub>2</sub> = 0.2, <italic>SNR</italic> = 25 for all experiments). The ground-truth parameter values employed in these experiments are colour-coded (top-left corner), with each entry of these matrices corresponding to a different combination of <italic>f</italic><sub>1</sub> and crossing angle. The ability to accurately estimate these values is explored with different SBI training designs. Estimates (median values over 100 noisy realisations) for the mean posterior of <italic>f<sub>sum</sub></italic> (top row) and crossing angle(<italic>v<sub>1</sub>, v<sub>2</sub></italic>) (bottom row) are shown for the following cases: A) Different prior definition and noise strategies for SBI training (from left to right): (i) Uniform priors and single-level noise, (ii) Restricted priors and single-level noise, (iii) Restricted priors and multi-level noise. B) SBI trained with raw dMRI signal (acquisition-specific) vs a basis set signal representation (acquisition-agnostic). Spherical harmonics were used for single-shell data, and the MAP-MRI projection for multi-shell data. C) Comparing MCMC fitting a model with N = 2 fibres against NPEs trained only with 2-fibres examples (Independent) or with a joint superset that contains both 1- and 2-fibres cases (Joint). All SBI models were trained with 5 million samples, apart from <italic>Joint</italic> approach which was trained with 6 million samples including 1- and 2-fibre cases (3 million each). Results denoted with (*) correspond to the same SBI design across A, B, C (i.e. trained with only <italic>N</italic> = 2 examples, multi-shell signal, restricted priors and multi-level noise).</p></caption><graphic xlink:href="EMS200379-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Evaluating performance in detecting the right number of fibre compartments in synthetic experiments, of MCMC, SBI_ClassiFiber and SBI_joint, all fitting up to <italic>N</italic> = 3 fibre compartments.</title><p>A) Recovering a two-fibre pattern (same synthetic data as in <xref ref-type="fig" rid="F3">Fig.3</xref>, where <italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub> ≠ 0 and <italic>f</italic><sub>3</sub> = 0), when fitting 3-fibre models to detect over/under-fitting. Estimates (median values over 100 noisy realisations) for the mean posterior of <italic>f</italic><sub>1</sub>, <italic>f</italic><sub>2</sub> and <italic>f</italic><sub>3</sub> and crossing angle (<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>) are shown. Both MCMC and SBI return zero f<sub>3</sub> in all cases, detecting two fibre compartments with the correct volume fractions. B) Performance in compartment identification as a function of SNR for the three different approaches and for 10,000 synthetic patterns containing <italic>N</italic>=0-3 fibres compartments. For each SNR, matrices show for each ground-truth number of fibres (matrix row), the percentage of cases that were estimated as supporting <italic>N</italic> = 0 –3 patterns (matrix column), i.e. each row of each matrix sums up to 100%. The ideal detection rate corresponds to a diagonal matrix; off-diagonal elements indicate over/under-fitting (above and below diagonal, respectively). In all cases, mean posterior <italic>f<sub>n</sub></italic> &gt;5% was used to indicate support for the existence of a compartment. Results for SBI_ClassiFiber refers to the number of fibres supported by the final NPE estimates (i.e. <italic>f<sub>n</sub></italic> &gt;5%), and not to the initial output of the classifier, which can be refined by NPE.</p></caption><graphic xlink:href="EMS200379-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Inference for in-vivo brain data using a <italic>N = 2</italic> fibres multi-shell model and different inference approaches (MCMC, SBI_ClassiFiber, SBI_joint).</title><p>A) Exemplar coronal slice with the mean and uncertainty maps estimated by the different methods for different model parameters. B) Density plots representing the agreement between MCMC and SBI in the white matter; correlation <italic>r</italic> is calculated between scalar maps. C) RGB colour-coded crossing-fibres estimated by each method in the centrum semiovale. The mean posterior orientation is plotted for each compartment.</p></caption><graphic xlink:href="EMS200379-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Inference for in-vivo brain data using a <italic>N</italic> = 3 fibres multi-shell model and different inference approaches (MCMC, SBI_ClassiFiber, and SBI_joint).</title><p>A) Exemplar coronal slice with the mean scalar maps estimated by the different methods for different model parameters (correlation with MCMC is indicated at the bottom of each map). B) RGB colour-coded crossing-fibres estimated by each method in the centrum semiovale. The mean posterior orientation is plotted for each compartment. C) Orientation uncertainty maps for each fiber compartment and for each inference approach.</p></caption><graphic xlink:href="EMS200379-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Probabilistic tractography using MCMC and SBI posterior orientation distributions.</title><p>White matter tract reconstruction for 42 bilateral bundles were obtained using the orientation estimates of each method. The spatial distribution of each of the paths is shown as a maximum intensity projection on axial (top row) and saggital (bottom row) planes (thresholded at probabilities &gt;0.1%). Tract reconstructions from a population-average atlas (using data from UK Biobank and MCMC-based orientation inference) is shown as a reference. Each tract was spatially correlated against the corresponding reconstruction from the atlas. The median of these correlation values across tracts is indicated under each method.</p></caption><graphic xlink:href="EMS200379-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Sources of differences in uncertainty mapped with MCMC and SBI.</title><p>A) Fitting fewer compartments than supported by data: Marginal and pairwise joint distributions of all parameters obtained from a 2-fibre model fitted to synthetic data from a 3-fibre pattern (ground-truth), using MCMC (left) and SBI (right). The marginal posterior distribution for every model parameter is shown first in each row, followed by the joint 2D posterior with every other parameter. When the number of compartments in the model is lower than the number of compartments supported by the data (i.e. effectively a wrong model is fitted), MCMC tends to return unimodal posteriors, missing the third compartment that is supported by the data. This leads to narrow (but wrong) distributions (overconfidence). SBI correctly captures all three modes within the compartments of the model (see red arrows, all captured in the <bold>v</bold><sub>1</sub> posterior in this example). However, this leads to broader posteriors (i.e. uncertainty). B) Compartment swapping: Marginal and pairwise joint distributions of all parameters obtained from SBI for a voxel from in-vivo data (square), where <italic>f<sub>1</sub> ∼ f<sub>2</sub></italic> (blue). In such cases, SBI can include samples from the first and second fibre orientation indistinctively to the corresponding marginal posteriors, making them multimodal (red arrows). A simple re-ordering (based on similarity of orientation samples) can mitigate this (green), leading to unimodal posteriors. Notice that there is nothing wrong with either approaches, as the joint orientation posterior samples stay the same, and what changes is how they are distributed to compartments. For reference, orientation uncertainty maps of <bold>v</bold><sub>2</sub> returned by SBI, before (left) and after the correction (right) are shown.</p></caption><graphic xlink:href="EMS200379-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><p><bold>Scan-rescan reproducibility of probabilistic tractography reconstructions for a range of white matter tracts</bold>, across 6 MRI scan repeats of the same participant. Correlations of the spatial path distributions reconstructed for each tract and using orientation samples from MCMC, SBI_ClassiFiber and SBI_joint are shown. A) Median (across repeats) pairwise tract correlation for each method (i.e. median across rep1 vs rep2, rep1 vs rep3, rep1 vs rep4,… rep5 vs rep6). B) Correlation between SBI and MCMC-based reconstructed tracts (median across 6 repeats SB1 vs MCMC1,SB2 vs MCMC2,…) overlaid on the interquartile range of correlations of MCMC reconstructions across the 6 repeats. C) Averaged path distributions (across the 6 repeats) for a subselection of tracts that had lower agreement between MCMC and SBI.</p></caption><graphic xlink:href="EMS200379-f009"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><p><bold>Parameter constraints and restricted priors</bold> in the Ball&amp;Sticks model. These ensure that i) the sum of the volume fractions ranges within [0, 1], ii) there is some ordering in the volume fractions <italic>f</italic><sub>1</sub> &gt; <italic>f</italic><sub>2</sub> &gt; <italic>f</italic><sub>3</sub>, iii) detectable crossing angles cannot be lower than a minimum, reflecting limitations in angular contrast/resolution in the data.</p></caption><table frame="hsides" rules="all"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-top: hidden"/><th align="center" valign="middle" colspan="4" style="background-color:#CCCCCC; border-right: solid thin">Parameter constraints</th></tr><tr style="border-top: hidden"><th align="center" valign="middle" style="background-color:#CCCCCC">No Fibres</th><th align="center" valign="middle" style="background-color:#CCCCCC">1 Fibre</th><th align="center" valign="middle" style="background-color:#CCCCCC">2 Fibres</th><th align="center" valign="middle" style="background-color:#CCCCCC; border-right: solid thin">3 Fibres</th></tr></thead><tbody><tr><td align="center" valign="middle"><italic>f</italic><sub>1</sub></td><td align="center" valign="middle">∼ <italic>U</italic>[0, 0.0001]</td><td align="center" valign="middle" colspan="3" style="border-right: solid thin">∼ <italic>U</italic>[0.05, 1]</td></tr><tr><td align="center" valign="middle"><italic>f</italic><sub>2</sub></td><td align="center" valign="middle" colspan="2">∼ <italic>U</italic>[0, 0.0001]</td><td align="center" valign="middle" colspan="2" style="border-right: solid thin">∼ <italic>U</italic>[0.05, 1]</td></tr><tr><td align="center" valign="middle"><italic>f</italic><sub>3</sub></td><td align="center" valign="middle" colspan="3">∼ <italic>U</italic>[0, 0.0001]</td><td align="center" valign="middle" style="border-right: solid thin">∼ <italic>U</italic>[0.05, 1]</td></tr><tr><td align="center" valign="middle"><italic>f<sub>sum</sub></italic></td><td align="center" valign="middle">∼ 3 × <italic>U</italic>[0, 0.0001]</td><td align="center" valign="middle"><italic>f</italic><sub>1</sub> ≤ 1</td><td align="center" valign="middle">(<italic>f</italic><sub>1</sub> + <italic>f</italic><sub>2</sub>) ≤ 1</td><td align="center" valign="middle" style="border-right: solid thin">(<italic>f</italic><sub>1</sub> + <italic>f</italic><sub>2</sub> + <italic>f</italic><sub>3</sub>) ≤1</td></tr><tr><td align="center" valign="middle"><italic>f<sub>n</sub></italic></td><td align="center" valign="middle" colspan="2">None</td><td align="center" valign="middle"><italic>f</italic><sub>1</sub> ≥ <italic>f</italic><sub>2</sub></td><td align="center" valign="middle" style="border-right: solid thin"><italic>f</italic><sub>1</sub> ≥ <italic>f</italic><sub>2</sub> ≥ f<sub>3</sub></td></tr><tr><td align="center" valign="middle"><italic>f<sub>n</sub> – f<sub>m</sub></italic> (<italic>m</italic> &gt; <italic>n</italic>)</td><td align="center" valign="middle" colspan="2">None</td><td align="center" valign="middle" colspan="2" style="border-right: solid thin">&gt;0.05</td></tr><tr><td align="center" valign="middle">angle(<bold>v</bold><italic><sub>n</sub></italic>, <bold>V</bold><italic><sub>m</sub></italic>)</td><td align="center" valign="middle" colspan="2">None</td><td align="center" valign="middle">&gt; 10 °</td><td align="center" valign="middle" style="border-right: solid thin">&gt; 30 °</td></tr><tr><td align="center" valign="middle">d<sub>std</sub>/d</td><td align="center" valign="middle" colspan="4" style="border-right: solid thin">≤ 2</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p><bold>Computational performance - Time (in milliseconds) needed to perform inference for each method</bold> and obtain 50 samples from the posterior (averaged in a dataset with 10,000 synthetic voxels). Times are all for a single CPU. Time for the classification in SBI_ClassiFiber is orders of magnitude smaller and not included. MCMC includes 1,000 burn-in iterations and a thinning period of 25 (i.e. 2,250 iterations in total).</p></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="middle"/><th align="center" valign="middle" style="background-color:#CCCCCC">MCMC</th><th align="center" valign="middle" style="background-color:#CCCCCC">SBI_ClassiFiber</th><th align="center" valign="middle" style="background-color:#CCCCCC">SBI_joint</th></tr></thead><tbody><tr><td align="center" valign="middle"><bold>1 fibre</bold></td><td align="center" valign="middle">110.65 ms</td><td align="center" valign="middle">2.86 ms</td><td align="center" valign="middle">-</td></tr><tr><td align="center" valign="middle"><bold>2 fibres</bold></td><td align="center" valign="middle">200.87 ms</td><td align="center" valign="middle">3.74 ms</td><td align="center" valign="middle">3.42 ms</td></tr><tr><td align="center" valign="middle"><bold>3 fibres</bold></td><td align="center" valign="middle">311.27 ms</td><td align="center" valign="middle">4.82 ms</td><td align="center" valign="middle">4.38 ms</td></tr></tbody></table></table-wrap></floats-group></article>