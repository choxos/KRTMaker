<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS200160</article-id><article-id pub-id-type="doi">10.1101/2024.11.12.623268</article-id><article-id pub-id-type="archive">PPR940843</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Comparative fMRI reveals differences in the functional organization of the visual cortex for animacy perception in dogs and humans</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Farkas</surname><given-names>Eszter Borbála</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Hernández-Pérez</surname><given-names>Raúl</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Cuaya</surname><given-names>Laura Veronica</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Rojas-Hortelano</surname><given-names>Eduardo</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Gácsi</surname><given-names>Márta</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Andics</surname><given-names>Attila</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroethology of Communication Lab, Department of Ethology, Institute of Biology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff><aff id="A2"><label>2</label>MTA-ELTE ‘Lendület’ Neuroethology of Communication Research Group, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff><aff id="A3"><label>3</label>ELTE NAP Canine Brain Research Group, Budapest, Hungary</aff><aff id="A4"><label>4</label>Social, Cognitive and Affective Neuroscience Unit, Department of Cognition, Emotion, and Methods in Psychology, Faculty of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03prydq77</institution-id><institution>University of Vienna</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff><aff id="A5"><label>5</label>Facultad de Medicina, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01tmp8f25</institution-id><institution>Universidad Nacional Autónoma de México</institution></institution-wrap>, <city>Santiago de Querétaro</city>, <country country="MX">México</country></aff><aff id="A6"><label>6</label>Department of Ethology, Institute of Biology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01jsq2704</institution-id><institution>Eötvös Loránd University</institution></institution-wrap>, <city>Budapest</city>, <country country="HU">Hungary</country></aff><aff id="A7"><label>7</label>HUN-REN–ELTE Comparative Ethology Research Group, Budapest, Hungary</aff><author-notes><corresp id="CR1">Correspondence: <email>eszter.borbala.farkas@ttk.elte.hu</email> (E.B.F.)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>11</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>11</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The animate-inanimate category distinction is one of the general organizing principles in the primate high-level visual cortex. Much less is known about the visual cortical representations of animacy in non-primate mammals with a different evolutionary trajectory of visual capacities. To compare the functional organization underlying animacy perception of a non-primate to a primate species, here we performed an fMRI study in dogs and humans, investigating how animacy structures neural responses in the visual cortex of the two species. Univariate analyses identified animate-sensitive bilateral occipital and temporal regions, non-overlapping with early visual areas, in both species. Multivariate tests confirmed the categorical representations of animate stimuli in these regions. Regions sensitive to different animate stimulus classes (dog, human, cat) overlapped less in dog than in human brains. Together, these findings reveal that the importance of animate-inanimate distinction is reflected in the organization of higher-level visual cortex, also beyond primates. But a key species difference, that neural representations for animate stimuli are less concentrated in dogs than in humans suggests that certain underlying organizing principles that support the visual perception of animacy in primates may not play a similarly important role in other mammals.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">General principles that govern the organization of the high-level visual cortex as well as several functional specializations were described in visually oriented, highly social primate species (<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R5">5</xref>). The animate-inanimate category distinction is one of the general organizing principles (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>), resulting in widely distributed animacy representations in the human high-level visual cortices. These representations show various diagnostic features of animacy, including mid-level features such as curvature, face-like and body-like features, and features reflecting capacity for movement and agency (<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref>). The animate-inanimate distinction is also represented in homologous cortical areas of nonhuman primates (<xref ref-type="bibr" rid="R11">11</xref>).</p><p id="P3">Much less is known about the functional organization of non-primary visual areas of non-primate mammals, for which vision is secondary to other sensory modalities, with its significance not having increased as dramatically during evolution as for certain primates (<xref ref-type="bibr" rid="R3">3</xref>). For non-primates that are known to use vision for social interaction processing, behavioral evidence suggests that animacy cues are relevant. Cats and dogs distinguish biological motion from other forms of motions (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>) and other animacy cues such as self-propulsion and speed changes can lead to an orienting response in dogs (<xref ref-type="bibr" rid="R14">14</xref>). When viewing natural scenes, dogs focus their gaze on animate entities, especially on their heads, more than on inanimate entities (<xref ref-type="bibr" rid="R15">15</xref>). Although recent neuroscientific works suggest a handful of high-level visual specializations in those non-primate species that primarily use vision for social interaction processing (<xref ref-type="bibr" rid="R3">3</xref>), data on how the representations of animate and inanimate entities are organized in non-primate visual cortices remains scarce.</p><p id="P4">Neuroimaging studies of non-primates directly comparing the visual processing of animate and inanimate entities have so far only been conducted with dogs. Dogs that live as companions constitute a special case for comparison to humans, even with their lower visual acuity and different color vision (<xref ref-type="bibr" rid="R16">16</xref>): dogs’ visual social environment is shared with humans, and they are the only non-human species that can be tested in an awake, unrestrained state (<xref ref-type="bibr" rid="R17">17</xref>). Three recent fMRI studies compared dog’ and humans’ high-level visual functions related to different aspects of animacy but remained inconclusive. Phillips and colleagues (<xref ref-type="bibr" rid="R18">18</xref>) found that a classifier distinguishing representations of dynamic animate and inanimate objects in humans failed to make the same distinction in dogs. In contrast, using static images, Boch and colleagues (<xref ref-type="bibr" rid="R19">19</xref>) identified body sensitivity as a factor accounting for animacy organization in the dog visual cortex. Finally, looking for representations of another diagnostic feature of animacy, faces, Bunford, Hernández-Pérez and colleagues (<xref ref-type="bibr" rid="R20">20</xref>) found areas exhibiting sensitivity for dog and human faces in humans but not in dogs. Together, these studies indicated that, similarly to the human brain, the dog brain exhibits sensitivity for certain aspects of animacy, but its extent and similarity to animacy-sensitivity in humans is still unclear.</p><p id="P5">To better understand the similarities and differences in how animacy structures higher-level visual perception in evolutionarily distant mammal species, we conducted a directly comparative dog-human fMRI study, in which participants viewed short videos of moving animate (dog, human, cat) and inanimate (car) entities. We selected stimuli to be therefore relevant to both species, and dynamic to involve motion-based features of animacy. To test animacy-sensitivity, we measured and compared neural response patterns elicited by these different stimulus classes. We predicted that if the animate-inanimate distinction is one of the general organizing principles of the dog visual cortex as it is in humans, we will find stronger responses to animate stimuli, and distinct response patterns to animate and inanimate stimuli, in non-primary visually responsive areas in the dog brain. Furthermore, we expected that if specific animate stimulus classes share some animacy features that are central for the functional organization of the human but not of the dog visual cortex – perhaps because dogs’ visual-social perception is far less detailed than humans’ –, then the neural responses for dog, human and cat stimuli would be less similar to each other and therefore overlap less in dogs than in humans.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P6">15 family dogs (average (range) age of 7.1 (3-12.6); 7 spayed females, 1 intact-, 7 neutered males), and 13 humans (average age (range) of 31.5 (21-42), 7 females, 6 males) participated in the experiment. All dogs (7 Border Collies, 2 Golden Retrievers, 1 Australian Shepherd, 1 Labradoodle, and 4 mixed breeds) were mesocephalic and were trained to remain still inside the scanner (<xref ref-type="bibr" rid="R21">21</xref>). All human participants had normal or corrected to normal visual acuity and participated voluntarily. Dog owners and humans were recruited from the participant pool of the Department of Ethology at Eötvös Loránd University in Budapest, Hungary. All procedures were in accordance with relevant guidelines and regulations. Procedures involving dogs were approved by the Food Chain Safety and Animal Health Directorate Government Office, Hungary. Procedures involving humans were approved by the Committee of Scientific and Research Ethics (ETT-TUKEB), Hungary. Dog owners and human participants signed an informed consent.</p></sec><sec id="S4"><title>Experimental Design</title><p id="P7">The stimuli consisted of natural videos of dogs, humans, cats, and cars, some showing the whole body or object, some only parts, through close-up or wider shots (<xref ref-type="fig" rid="F1">Figure 1</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Video S1</xref>). The dog (D), human (H), and cat (C) videos constituted the animate (A) stimuli. We selected these animate stimulus classes to have an identical stimulus set for dogs and humans that involves familiar species to maximize the attention of dog subjects: the conspecific and two heterospecific species. Cars were selected as inanimate (iA) stimuli that move, vary in color and shape and all dog participants had ample experience with them. A reason to use a restricted number of stimulus classes was ensuring enough repetitions of each condition to elicit robust neural response in dogs where representations of categories are not established. All the videos were downloaded from YouTube (available under a Creative Commons License), cut to match the length required and resized to a final resolution of 1024 x 576 pixels. The videos displayed various situations and backgrounds using natural colors to minimize the possibility of systematic differences in visual properties across conditions and maximize ecological validity. To balance the differences in visual properties, brightness, contrast, hue, saturation, and motion for every video was calculated and videos were selected so that there were no significant differences in these properties between conditions in any of the runs (one-way ANOVA, ps &gt; 0.05). To account for other low-level visual properties, captured by Gabor filters, the within- and between-category correlation of the outputs of the HMAX model’s C1 units (<xref ref-type="bibr" rid="R22">22</xref>) were compared and no differences were found for any of the categories (two-sample t test, ps &gt; 0.05).</p><p id="P8">We used an event-related design (<xref ref-type="fig" rid="F1">Figure 1</xref>). Each run contained different stimuli, 12 exemplars from each of the four conditions. The length of the videos varied between 3.5 and 5.5 s (mean 4.5 s). Each video was preceded by a grey screen with variable duration of 0.7 - 2.5 s (mean 1.7 s). The length and variability of stimulus duration and interstimulus interval were chosen to minimize predictability but maximize signal quality (<xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>). Runs ended with an additional 10 s of grey screen and lasted 310 s. The order of the videos within a run was randomized for every participant separately. Thirteen dogs completed six runs and two dogs completed only three, all humans completed six runs. Dogs were tested in two runs per session with rest periods of at least 20 min between sessions and a maximum of four runs per day. The dogs’ eyes were observed through an MR-compatible camera during the run to confirm they were awake during scanning. Four runs were discarded and repeated due to the dog falling asleep. Humans were tested in one or two days and in one session with 3 to 6 runs per day depending on the availability of the scanner and the participant.</p><p id="P9">Videos were presented using an MR compatible NordicNeuroLab LCD Monitor and controlled using Psychophysics Toolbox. Dogs viewed the screen directly, while humans viewed it through a mirror. The screen was set up at 155 cm from the head of the participants; the videos covered the entire height (49.8 cm) and width (88.6 cm) of the screen.</p></sec><sec id="S5"><title>fMRI Data Acquisition and Preprocessing</title><p id="P10">Data acquisition was performed at the Medical Imaging Centre at the Semmelweis University on a Philips 3T Ingenia scanner. For dogs, a Philips dStream Pediatric Torso 8ch coil and for humans, a Philips dStream Head 32ch coil was used. Functional data were collected using a gradient-echo-planar imaging (EPI) sequence (dogs and humans: TR=2500 ms, TE=20 ms, flip angle=90°, 2.5 mm-thick slices with 0.5 mm gap; dogs: field of view: 200x150x120 mm, acquisition matrix 80×58; 40 axial slices; humans: field of view: 240x240x122 mm, acquisition matrix 96×94; 41 axial slices). 124 volumes were acquired in each run. Anatomical data was collected using a T1-weighted 3D TFE sequence, with 1×1×1 mm resolution (dogs: at a separate session; humans: the end of the last functional imaging session). We acquired 180 slices covering the whole brain for anatomical localization.</p><p id="P11">Image preprocessing was conducted with FSL version 4.19 (Jenkinson et al., 2012). Statistical analysis of MRI data was performed using FEAT (FMRI Expert Analysis Tool) Version 6.00, part of FSL (FMRIB’s Software Library, <ext-link ext-link-type="uri" xlink:href="http://www.fmrib.ox.ac.uk/fsl">www.fmrib.ox.ac.uk/fsl</ext-link>), time-series statistical analysis using FILM (<xref ref-type="bibr" rid="R25">25</xref>), and representational similarity analysis using PyMVPA software package (<xref ref-type="bibr" rid="R26">26</xref>) and MATLAB (2018b) custom code.</p><p id="P12">Volumes were motion-corrected and filtered using a 128 s high-pass filter. Scan-to-scan movement was calculated using framewise displacement (FD) (<xref ref-type="bibr" rid="R27">27</xref>), all the volumes where FD exceeded 1 mm were excluded (dogs 0.17%; humans 0%). The mean FD across subjects and runs was 0.07 mm for dogs and 0.05 mm for humans. Dog and human images were skull-stripped.</p><p id="P13">For dogs, we calculated a mean functional image that comprised all the volumes of all the runs for a given subject. This mean functional image was then manually transformed into a dog anatomic template (<xref ref-type="bibr" rid="R28">28</xref>) using 12 degree-of-freedom affine transformation, and the transformation matrix was used to transform all the volumes into the template space. Human images were first registered to the anatomical scan using a 7 degree-of-freedom linear transformation, then to the Montreal Neurologic Institute MNI template using 12 degree-of-freedom affine transformation. Next, images were smoothed (Gaussian kernel, FWHM 5 mm) for both dogs and humans.</p></sec><sec id="S6"><title>fMRI Statistical Analysis</title><sec id="S7"><title>RSA Comparing Neural and an EVC Model</title><p id="P14">To map neural responses to low-level visual properties, we assessed, using RSA, where the representational geometry of the stimuli in the dog and human brains was similar to how an early visual cortex (EVC) model represent them.</p><p id="P15">First, representational dissimilarity matrices (DSM) of the stimulus set were calculated to characterize the pairwise dissimilarity of the response patterns elicited by the stimuli (Kriegeskorte, 2009). For this, each stimulus event was modelled convolving its times series with the canonical hemodynamic response function and the resulting, normalized β values were used to describe every voxel by how dissimilar a sphere of voxels (r=2 voxels for dogs and r=3 voxels for humans) around it represents each pair of the stimuli. Dissimilarity was measured as the correlation distance (1 - Pearson correlation). DSMs were calculated this way for each run of every participant and then were averaged across runs and participants. In the case of dogs, this process was repeated for the whole brain. In the case of humans, we restricted the analysis to the occipital and temporal lobes (delineated based on MNI atlas) where the visual areas are, to reduce the computation load.</p><p id="P16">Second, the stimuli were presented to the computational model HMAX (<xref ref-type="bibr" rid="R22">22</xref>). In its first and second layers, the HMAX model simulates the selective tuning of neurons in primary visual cortex (V1) and has been shown to match with the activity patterns of the EVC of the primate brain (<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>). Specifically, we used the second layer of the model (referred here as EVC model), which corresponds to V1 complex cells. This layer yields a response vector of a V1 unit by performing a 2-dimensional convolution of a Gabor filter with an image. The algorithm was applied to each frame of each video using a set of Gabor filters with four different orientations (-45°, 0°, 45°, 90°) and 17 sizes (7 to 39 pixels in 2-pixel steps) per orientation, modeling 68 V1 units for each frame of a video. To obtain a unidimensional response vector for every video stimulus, principal component analyses were performed within the response vectors of every V1 unit across all frames and for all stimuli. The first principal components were used in the following step, because those contained the most explained variance for the given video. Next, the DSM of response vectors was calculated to assess the representational geometry expected based on our stimuli’s low-level visual features.</p><p id="P17">Third, to compare the brain representation and the model, correlation maps were created, in which each voxel was assigned the Pearson correlation between neural DSM and model DSM. To account for the spatial dependency of the searchlight approach (<xref ref-type="bibr" rid="R31">31</xref>) and to reduce the number of assumptions on the maps, we used a random permutation method (<xref ref-type="bibr" rid="R32">32</xref>) for group-level analysis and statistical inference.</p><p id="P18">The stimulus labels were randomly swapped, a random DSM map and then the correlation map between the random DSM map and the DSM of the model were created. This process was repeated 1,000,000 times. Next, the maps were averaged to calculate the correlations expected by chance and their standard deviations. These parameters were then used to transform the original correlation maps to Z-score maps. The maps were then filtered to maintain only the voxels with a Z-score &gt; 3.1 (equivalent to p &lt; 0.001). The random correlation maps were also filtered at Z-score &gt; 3.1. Using these maps, the cluster size distribution expected by chance was calculated and this distribution was used to filter out the clusters with a size smaller than the size expected by chance (p &lt; 0.05).</p></sec><sec id="S8"><title>General Linear Model (GLM)</title><p id="P19">To identify regions that are sensitive to our conditions, univariate analysis with the four conditions (D, H, C, iA) as regressors were conducted using the FSL’s implementation of the General Linear Model.</p><p id="P20">The first-level individual models included the movement parameters as regressors of no interest. The regressors were convolved with the canonical hemodynamic response function, then using all six runs, condition effects were estimated for each participant. At the group level, a random-effects analysis was performed for each species.</p><p id="P21">For further analyses, the visually responsive regions of the dog and human cortex were identified by comparing all conditions to the implicit baseline. To determine the brain regions sensitive to animate stimuli, the videos from all three animate conditions (D, H, C) were grouped and contrasted with the inanimate condition (A&gt;iA contrast). To further characterize the animacy-sensitivity of the dog and human brains, each of the three animate conditions was contrasted with the inanimate condition separately (D&gt;iA, H&gt;iA, C&gt;iA, specific contrasts).</p><p id="P22">For all contrasts, the resulting Z-statistic (Gaussianised T/F) images were thresholded at Z &gt; 3.1 and the threshold for cluster-correction was p = 0.05 (Worsley, 2001).</p><sec id="S9"><title>Response profiles</title><p id="P23">To further characterize the A&gt;iA contrast results, for every subject and for each group level-derived peak, first the parameter estimates (β weights) for each condition in a sphere (radius = 3 voxels) centered around the peak was assessed using FSL’s Featquery tool. Second, these parameter estimates were compared on the group level, using t tests.</p></sec><sec id="S10"><title>Overlap calculation</title><p id="P24">To determine and compare across species the extent to which specific contrast maps overlap, first, the visually response cortex was determined in both species by contrasting all conditions to baseline using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. Then, the number of suprathreshold voxels were assessed for each specific contrast as well as the number of voxels in the visually responsive cortex in dogs and in humans. Then, the percentage of those visually responsive voxels that were suprathreshold in (<xref ref-type="bibr" rid="R1">1</xref>) all three specific contrasts (3-overlap) or (<xref ref-type="bibr" rid="R2">2</xref>) two of the three specific contrasts (2-overlap), were calculated, and the percentages of both 3-overlapping and 2-overlapping voxels were compared between dogs and humans, using t tests.</p></sec><sec id="S11"><title>Species-preference</title><p id="P25">We also assessed the proportions of animate-sensitive voxels that responded strongest to conspecific stimuli or to any of the heterospecific stimulus classes. For that, each animate condition (D, H, C) was contrasted with the implicit baseline, and the average Z-scores per contrast were calculated across all dog and across all human participants in every animate-sensitive voxel. The condition from the contrast with the highest average Z-score was assigned as the preferred species for each voxel. To determine whether the percentage of voxels differed above chance (0.33) for the three conditions, permutation test was performed by randomly swapping the labels of the stimuli, calculating the preference for each voxel and their percentages 1,000,000 times.</p></sec></sec><sec id="S12"><title>Category- and Class-boundary Effect Tests</title><p id="P26">To identify regions with greater representational dissimilarity for between-category (i.e., animate-inanimate) than within-category (i.e., animate-animate) stimulus pairs (category boundary effect test as described in Kriegeskorte et al., 2008), we calculated DSMs per run, per participant for both between-category and within-category pairs across the brain (for dogs: whole brain, for humans: temporal and occipital cortices), using a searchlight approach (sphere with r=2 voxels for dogs, and r=3 voxels for humans). DSMs were then averaged across runs and participants, and a difference map was calculated. To determine whether the difference was significant at the voxel level, we repeated the process 1,000,000 times but randomly swapping stimulus labels, thus generating 1,000,000 permuted difference maps. We transformed each voxel of the original difference map to Z-score by comparing it with the corresponding voxel in the permuted difference maps, generating a Z-score map. To determine cluster size threshold, we repeated the same process for the permuted difference maps, thus generating 1,000,000 permuted Z-score maps. All Z-score maps were thresholded at Z &gt; 3.1. We calculated the cluster size distribution in the permuted Z-score maps and used it to determine the cluster size expected by chance in the original Z-score map. All clusters with a size smaller than expected by chance (p&lt;0.05) were excluded. Category-boundary tests were performed in a similar way for between-class (e.g., dog-car) vs. within-class (e.g., dog-dog) stimulus pairs.</p><sec id="S13"><title>Overlap calculation</title><p id="P27">To determine and compare across species the extent to which class-boundary tests overlap, first, the results were binarized. Then, the percentage of the voxels active in (<xref ref-type="bibr" rid="R1">1</xref>) all three classes (3-overlap) or (<xref ref-type="bibr" rid="R2">2</xref>) two of the three classes (2-overlap), were calculated, and the percentages of both 3-overlapping and 2-overlapping voxels were compared between dogs and humans, using t tests.</p></sec></sec></sec></sec><sec id="S14" sec-type="results"><title>Results</title><sec id="S15"><title>RSA Comparing Neural and an EVC Model</title><p id="P28">For results in dogs and humans, see <xref ref-type="fig" rid="F2">Figure 2</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S1</xref>. We found a significant correlation with the early visual cortex model in clusters in the occipital cortex of dogs and humans. In dogs, the cluster in the right hemisphere was centered in the marginal gyrus (MG) and the cluster in the left hemisphere was centered in the splenial gyrus (SpG). In humans, the cluster extended bilaterally, with main peaks in the bilateral calcarine fissure (CAL).</p></sec><sec id="S16"><title>GLM</title><p id="P29">For visual responsiveness (all stimuli &gt; implicit baseline) results in dogs and humans, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S2</xref>. For GLM results for each contrast (A&gt;iA, D&gt;iA, H&gt;iA, C&gt;iA) in dogs and humans, see <xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S3</xref>. In both species, these contrasts revealed clusters that extended mainly through the temporal and occipital lobes. Specifically, in dogs, for A&gt;iA, we found bilateral clusters in the mid suprasylvian gyrus (mSSG), extending caudally in the left hemisphere, and bilateral clusters in the ectomarginal gyrus (EMG); for D&gt;iA, left SpG extending to mSSG, and a right cluster in the suprasylvian gyrus (SSG); including mid and caudal portions; for H&gt;iA, left clusters including the mid ectosylvian gyrus (mESG), the mSSG, and the SpG, and a right cluster including mid and caudal portions of the SSG; and for C&gt;iA, a left cluster in the caudal part of the SSG. In humans, for all four contrasts, we found clusters bilaterally, involving portions of the inferior temporal gyrus (ITG), middle temporal gyrus (MTG), inferior occipital gyrus (IOG) and the fusiform gyrus (FFG), typically extending more to the temporal lobe in the right hemisphere. Activity response profiles for GLM-derived A&gt;iA peaks in dogs and humans are shown in <xref ref-type="fig" rid="F3">Figure 3B</xref>.</p><sec id="S17"><title>Overlap calculation</title><p id="P30">Comparing the proportion of overlapping activities within the visually responsive cortex, we found that in the dog brain there was a significantly lower proportion of voxels than in the human brain in which all three (D, H, C) contrast maps overlapped at z=3.1 (dogs: 2%, humans: 31%) (t(14.781) = 12.044, p &lt; 0.01). The proportion of overlapping voxels between at least two categories out of the three was also lower in the dog brain compared to that of the human brain (dogs: 11%, humans: 51%) (t(25.925) = 11.236, p &lt; 0.01) (<xref ref-type="fig" rid="F3">Figure 3D</xref>).</p></sec><sec id="S18"><title>Species-preference</title><p id="P31">Analyses of the extent to which voxels in the animate-inanimate contrast responded stronger to dog or human or cat stimuli indicated that in dogs, the majority of animate-preferring voxels (87%) responded strongest to conspecific (dog) stimuli (likelihood of obtaining the observed proportions by chance, using permutation testing: p = 0.004). In humans, the majority of animate-preferring voxels (67%) responded strongest to conspecific (human) stimuli (permutation testing confirmed a marginally significant effect, p = 0.062) (<xref ref-type="fig" rid="F3">Figure 3E</xref>).</p></sec></sec><sec id="S19"><title>Category- and Class-boundary Effect Test</title><p id="P32">For category- and class-level boundary test results in dogs and humans, see <xref ref-type="fig" rid="F4">Figure 4</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S4</xref>. We found temporal and occipital clusters, located similarly to those identified by univariate animacy-sensitivity contrasts. Specifically, in dogs, for A&gt;iA, we found bilateral mSSG/mESG clusters; for D&gt;iA, a left cluster centered in the caudal suprasylvian gyrus (cSSG) and an mSSG-centered right cluster; for H&gt;iA, left clusters in cSSG and caudal ectosylvian gyrus (cESG), and right clusters in EMG and rostral sylvian gyrus (rSG); and for C&gt;iA, a left cluster in mESG, and a right SpG cluster, both extending caudally. In humans, for all contrasts, we found large clusters in both hemispheres, extending from MTG to ITG and IOG, and ventrally to FFG, with similar peaks across contrasts.</p><sec id="S20"><title>Overlap calculation</title><p id="P33">Proportion of overlapping voxels within the visually responsive cortex were significantly lower in dogs than in humans, both in case all three (D, H, C) similarity maps overlapped (dogs: 1%, humans: 26%) and in case at least two of the three similarity maps overlapped (dogs: 14%, humans: 47%), (t(12.217) = 8.518, p &lt; 0.01 and t(20.981) = 9.405, p &lt; 0.01) (<xref ref-type="fig" rid="F4">Figure 4C</xref>).</p></sec></sec></sec><sec id="S21" sec-type="discussion"><title>Discussion</title><p id="P34">This study investigated how animacy structures neural responses in the visually responsive cortex of dogs and humans. We found both similarities and differences across the two species. Univariate analyses identified bilateral occipital and temporal regions in both species responding stronger to animate than inanimate stimuli. These animate-sensitive regions are distinct from functionally determined early visual areas (i.e., that represented the stimuli similarly to an EVC model). Most animate-sensitive peaks preferred all animate stimulus classes over inanimate stimuli, but more animate-sensitive voxels responded stronger to conspecific than to any of the heterospecific stimuli in dogs, revealing conspecific-preference. A similar trend was observed in humans. The areas sensitive to the specific class of animate stimuli, dog, human and cat, overlapped less in dog than in human brains. Multivariate tests revealed regions in both species in which the representational geometry of stimulus pairs within animate and within inanimate categories were more similar than that of stimulus pairs crossing the animate-inanimate boundary. Such boundary effects were identified in both species for dog, human and cat stimuli as well, and these overlapped less in dogs than in humans. The regions exhibiting these categorical representations for animate stimulus classes largely overlapped with univariate animacy-sensitive clusters.</p><p id="P35">The similarities between dog and human neural response patterns, namely animacy-sensitivity and category boundary effects in visual cortical regions, demonstrate that animacy is an organizing principle in the visual perception of both species, and the representations of animate entities exhibit categorical structure. Categorical representation has already been shown across visual domains in multiple primate species. Going beyond these previous results, the present work suggests that animacy and the subordinate categorical feature of the input may determine the location and the structure of the neural response not only in primates’, but at least in some non-primate mammals’ visual perception too. We also note, however, that as animacy is multidimensional in nature (<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref>), dogs’ and humans’ animacy sensitivity, as reported here, do not necessarily originate from the representation of the same animacy dimensions. Nevertheless, the present findings provide evidence that dogs’ visual cortex contains functionally organized areas, as primates’ does.</p><p id="P36">The animacy-sensitive regions reported here correspond to but extend beyond those reported previously (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R19">19</xref>). Specifically, for dogs, we found that animacy sensitivity characterizes 22% of the visually responsive cortex, including regions bilaterally in the EMG and mSSG. Using static stimuli, Boch et al. (2023) found similarly located, although less extensive regions as sensitive for animate (specifically, body and face) stimuli. Various factors may have contributed to this difference between studies, including the use of dynamic stimuli in the present study, as these stimuli also carried motion-related animacy cues (<xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R34">34</xref>), but also scanning-technical factors. Prior results on dog neural responses to (human) faces vs. objects, potentially also driven by animacy sensitivity, also suggested the involvement of bilateral temporal regions (<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>). For humans, animacy sensitivity characterized 20,7% of the visually responsive cortex and included extended regions in the higher-level visual cortex. Whereas previous works often focused on specific aspects of animacy (such as face, body, humanness, agency, biological motion), our highly natural stimuli carried multiple aspects of animacy at the same time, and this could have resulted in the finding of more extensive regions here (<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R11">11</xref>).</p><p id="P37">Beyond similarities, differences in animacy representations were also found between dogs and humans. The main species difference, that the detected neural activity evoked by animate stimulus classes overlapped less in dogs than in humans, may reflect how evolution drove mammalian brain organization for visual perception on a global level. Adaptive visual-social behavior and underlying neural representations shaped by relevant behavioral goals differ substantially between primates and (even visually oriented) non-primate mammals. The greater emphasis on visual social communication in primates, their more nuanced social interactions and within-face movements necessitate the efficient processing of bodily detail-encoding social cues and signals beyond body postures more than in non-primates. Indeed, in non-primates the relative role of perception and interpretation of whole-body actions is arguably greater. Therefore, the behavioral relevance of our stimuli showing moving dogs, humans and cats may have differed for human and dog subjects. Dogs’ focus during the visual perception of animate stimuli may be on bodily actions while humans’ rather on faces and gestures. This notion is supported by previous canine neuroimaging work: On one hand, these studies found no convincing evidence for the existence of face-sensitive areas in dogs, and body sensitivity, weak and restricted to small regions, has been reported for this species in a single study so far. On the other hand, Phillips et al. (<xref ref-type="bibr" rid="R18">18</xref>) found that representations of different actions of animate entities were separable in the dog brain, but the representations of animate and inanimate objects were not. Therefore, the pattern observed in the present study suggests that animacy sensitivity in the dog brain does not primarily stem from the common presence of face and body across all animate stimulus classes, but rather from the representation of biological motion. And it is possible that the representations of different biological motions in dogs are less generalized across the different animate stimulus classes than face and body representations in humans. These findings are in line with the proposal that dogs’ high-level visual cortex may contain distinct agent-responsive areas, and also with the specific sensitivities that have been described in these areas (Boch et al., 2024). Future research should assess the validity of this potential explanation behind the species differences in animacy perception.</p><p id="P38">The finding on conspecificity-preference in dogs, that the majority of animate-sensitive voxels responded maximally to conspecific stimuli, and a similar tendency in humans, corroborate and extend previous results. Recently, Bunford, Hernández-Pérez et al. (2020) identified conspecific-preferring clusters in the visually responsive cortex in both species. In that paper, in humans, conspecific-preference was restricted to face-sensitive regions. The present findings, using more naturalistic and more variable videos, support these previous results and suggest that conspecific-preference may be characteristic for animate-sensitive visual regions, at least in dogs. Conspecific-preference, therefore, may be present at different stages of visual processing of faces and animacy. This suggests that the bias for processing conspecific stimuli does not reflect a functional specialization but rather a more generalized preference, perhaps driven by attention or motivational relevance (<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R38">38</xref>).</p><p id="P39">This work has a few potential limitations. First, inanimate stimuli consisted of cars only, and animate stimuli included only a narrow but highly relevant subset of all animate entities that dogs and humans may encounter, which selections, for reasons detailed in the Methods may also have affected the generalizability of the animacy sensitivity results. Potential differences in the level of relevance between animate and inanimate conditions may have also been present: for example, human stimuli may have been more relevant than cars to dogs. The similarity of the cortical locations of our animacy sensitivity results to previous findings in both dogs (<xref ref-type="bibr" rid="R19">19</xref>) and humans (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>), however, suggests that it was indeed the presence of animacy in dog, human and cat stimuli (and its lack in car stimuli) that drove the measured differences in neural activity. Furthermore, although our stimuli did not involve a wide variety of animate classes, they, being videos, carried not only static but also action-related diagnostic features of animacy, in a highly natural manner.</p><p id="P40">Another potential limitation of any directly comparative visual study of dogs and humans may stem from their different visual capacities: dogs perceive less details of visual stimuli presented on a monitor than humans, due to their different color vision, lower critical flicker-fusion threshold, lower depth perception, and lower visual acuity [missing references]. As a consequence, dogs may have seen our stimuli as more similar than humans have, and this may have affected neural response patterns. While we cannot exclude such effects, we do not think that visual capacity differences between dogs and humans confounded the key species difference presented here, that is the smaller overlap of condition-specific clusters for dogs. On the one hand, it can be assumed that visual capacities influenced the perception of animate and inanimate stimuli similarly. On the other hand, the less detailed visual perception of dogs may lead to less differentiated and thus more overlapping representations of the conditions, relative to those in humans, but not to less overlapping ones, as was the case here.</p><p id="P41">In conclusion, the findings presented here suggest that the importance of the animate-inanimate distinction may be reflected in the organization not only of primate, but more generally of mammalian higher-level visual cortex. The key species difference, that neural representations for animate stimuli cluster less in dogs than in humans demonstrates that different animate stimulus classes may not form a unified category in dogs and suggests that the dog brain may lack those functional specializations that drive the unified response in humans. To understand the principles that organize the visual perception of animate objects in non-primate mammals, other factors need to be considered than those underlying the animacy organization of the primate visual cortex.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS200160-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d35aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgements</title><p>This project was funded by the Hungarian Academy of Sciences [MTA Lendület (Momentum) Programme (LP2017-13/2017), National Brain Programme 3.0 (NAP2022-I-3/2022]; the Eötvös Loránd University; the European Research Council [European Union’s Horizon 2020 research and innovation program (grant number 950159)], the HUN-REN – ELTE Comparative Ethology Research Group (01 031). R.H.-P. and L.V.C. were supported by the Mexican Council of Science and Technology (CONACYT, 407590 and 409258, respectively). R.H.-P. was also supported by the Austrian Science Fund (FWF) (10.55776/ESP602).</p><p>We thank all dog and human participants and dogs’ owners for their participation and contribution to these data.</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P42"><bold>Author Contributions</bold></p><p id="P43">R.H.-P., L.V.C., E.B.F. and A.A. performed conceptualization. R.H.-P., L.V.C. performed investigation. R.H.-P. and E.R.-H. performed formal analysis. R.H.-P., E.B.F and A.A. performed visualization. E.B.F., R.H.-P. and A.A. performed writing – original draft. E.B.F., R.H.-P., M.G. and A.A. performed writing – review &amp; editing.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><year>2020</year><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC8088388</pub-id><pub-id pub-id-type="pmid">32494012</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op De Beeck</surname><given-names>HP</given-names></name></person-group><article-title>Understanding Human Object Vision: A Picture is Worth a Thousand Representations</article-title><source>Annu Rev Psychol</source><year>2023</year><fpage>1</fpage><lpage>33</lpage><pub-id pub-id-type="pmid">36378917</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Mitchell</surname><given-names>JF</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><source>Evolved Mechanisms of High-Level Visual Perception in Primates</source><publisher-name>Elsevier Ltd</publisher-name><year>2020</year></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title><year>2001</year><volume>293</volume><fpage>2470</fpage><lpage>3</lpage><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>J Neurosci</source><year>1997</year><volume>17</volume><fpage>4302</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sha</surname><given-names>L</given-names></name><etal/></person-group><article-title>The animacy continuum in the human ventral vision pathway</article-title><source>J Cogn Neurosci</source><year>2015</year><volume>27</volume><fpage>665</fpage><lpage>678</lpage><pub-id pub-id-type="pmid">25269114</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nat Rev Neurosci</source><year>2014</year><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC4143420</pub-id><pub-id pub-id-type="pmid">24962370</pub-id><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><etal/></person-group><article-title>Untangling the animacy organization of occipitotemporal cortex</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><fpage>7103</fpage><lpage>7119</lpage><pub-id pub-id-type="pmcid">PMC8372013</pub-id><pub-id pub-id-type="pmid">34230104</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2628-20.2021</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><etal/></person-group><article-title>Disentangling five dimensions of animacy in human brain and behaviour</article-title><source>Commun Biol</source><year>2022</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC9663603</pub-id><pub-id pub-id-type="pmid">36376446</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-04194-y</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Quek</surname><given-names>GL</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Annual Review of Vision Science Visual Representations: Insights from Neural Decoding</article-title><year>2023</year><pub-id pub-id-type="pmid">36889254</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><etal/></person-group><article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovács</surname><given-names>K</given-names></name><etal/></person-group><article-title>The effect of oxytocin on biological motion perception in dogs (Canis familiaris)</article-title><source>Anim Cogn</source><year>2016</year><volume>19</volume><fpage>513</fpage><lpage>522</lpage><pub-id pub-id-type="pmid">26742930</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdai</surname><given-names>J</given-names></name><name><surname>Ferdinandy</surname><given-names>B</given-names></name><name><surname>Terencio</surname><given-names>CB</given-names></name><name><surname>Pogány</surname><given-names>Á</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name></person-group><article-title>Perception of animacy in dogs and humans</article-title><source>Biol Lett</source><year>2017</year><volume>13</volume><pub-id pub-id-type="pmcid">PMC5493738</pub-id><pub-id pub-id-type="pmid">28659418</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2017.0156</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Völter</surname><given-names>CJ</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name></person-group><article-title>Pupil size changes reveal dogs’ sensitivity to motion cues</article-title><source>iScience</source><year>2022</year><volume>25</volume><pub-id pub-id-type="pmcid">PMC9424576</pub-id><pub-id pub-id-type="pmid">36051183</pub-id><pub-id pub-id-type="doi">10.1016/j.isci.2022.104801</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Törnqvist</surname><given-names>H</given-names></name><name><surname>Somppi</surname><given-names>S</given-names></name><name><surname>Kujala</surname><given-names>MV</given-names></name><name><surname>Vainio</surname><given-names>O</given-names></name></person-group><article-title>Observing animals and humans: Dogs target their gaze to the biological information in natural scenes</article-title><source>PeerJ</source><year>2020</year><volume>8</volume><elocation-id>e10341</elocation-id><pub-id pub-id-type="pmcid">PMC7749655</pub-id><pub-id pub-id-type="pmid">33362955</pub-id><pub-id pub-id-type="doi">10.7717/peerj.10341</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byosiere</surname><given-names>SE</given-names></name><name><surname>Chouinard</surname><given-names>PA</given-names></name><name><surname>Howell</surname><given-names>TJ</given-names></name><name><surname>Bennett</surname><given-names>PC</given-names></name></person-group><article-title>What do dogs (Canis familiaris) see? A review of vision in dogs and implications for cognition research</article-title><source>Psychon Bull Rev</source><year>2017</year><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmid">29143248</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bunford</surname><given-names>N</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Kis</surname><given-names>A</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name><name><surname>Gácsi</surname><given-names>M</given-names></name></person-group><article-title>Canis familiaris As a Model for Non-Invasive Comparative Neuroscience</article-title><source>Trends Neurosci</source><year>2017</year><volume>40</volume><fpage>438</fpage><lpage>452</lpage><pub-id pub-id-type="pmid">28571614</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>EM</given-names></name><name><surname>Gillette</surname><given-names>KD</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Berns</surname><given-names>GS</given-names></name></person-group><article-title>Through a Dog’s Eyes: fMRI Decoding of Naturalistic Videos from Dog Cortex</article-title><source>J Vis Exp</source><year>2022</year><volume>187</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="pmid">36190286</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boch</surname><given-names>M</given-names></name><name><surname>Wagner</surname><given-names>IC</given-names></name><name><surname>Karl</surname><given-names>S</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Lamm</surname><given-names>C</given-names></name></person-group><article-title>Functionally analogous body- and animacy-responsive areas are present in the dog (Canis familiaris) and human occipito-temporal lobe</article-title><source>Commun Biol</source><year>2023</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC10300132</pub-id><pub-id pub-id-type="pmid">37369804</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-05014-7</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bunford</surname><given-names>N</given-names></name><etal/></person-group><article-title>Comparative brain imaging reveals analogous and divergent patterns of species and face sensitivity in humans and dogs</article-title><source>Journal of Neuroscience</source><year>2020</year><volume>40</volume><fpage>8396</fpage><lpage>8408</lpage><pub-id pub-id-type="pmcid">PMC7577605</pub-id><pub-id pub-id-type="pmid">33020215</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2800-19.2020</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Gácsi</surname><given-names>M</given-names></name><name><surname>Faragó</surname><given-names>T</given-names></name><name><surname>Kis</surname><given-names>A</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name></person-group><article-title>Voice-sensitive regions in the dog and human brain are revealed by comparative fMRI</article-title><source>Current Biology</source><year>2014</year><volume>24</volume><fpage>574</fpage><lpage>578</lpage><pub-id pub-id-type="pmid">24560578</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serre</surname><given-names>T</given-names></name><name><surname>Wolf</surname><given-names>L</given-names></name><name><surname>Bileschi</surname><given-names>S</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Robust Object Recognition with Cortex-Like Mechanisms</article-title><source>IEEE Trans Pattern Anal Mach Intell</source><year>2007</year><volume>29</volume><fpage>411</fpage><lpage>426</lpage><pub-id pub-id-type="pmid">17224612</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><chapter-title>Optimal experimental design for event-related fMRI</chapter-title><source>Human Brain Mapping</source><publisher-name>Wiley-Liss Inc</publisher-name><year>1999</year><fpage>109</fpage><lpage>114</lpage><pub-id pub-id-type="pmcid">PMC6873302</pub-id><pub-id pub-id-type="pmid">10524601</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:2/3&amp;#x0003c;109::AID-HBM7&amp;#x0003e;3.0.CO;2-W</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burock</surname><given-names>MA</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name></person-group><article-title>Randomized event-related experimental designs allow for extremely rapid presentation rates using functional MRI</article-title><year>1998</year><pub-id pub-id-type="pmid">9858388</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ripley</surname><given-names>BD</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title><source>Neuroimage</source><year>2001</year><volume>14</volume><fpage>1370</fpage><lpage>1386</lpage><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M</given-names></name><etal/></person-group><article-title>PyMVPA: A python toolbox for multivariate pattern analysis of fMRI data</article-title><source>Neuroinformatics</source><year>2009</year><volume>7</volume><fpage>37</fpage><lpage>53</lpage><pub-id pub-id-type="pmcid">PMC2664559</pub-id><pub-id pub-id-type="pmid">19184561</pub-id><pub-id pub-id-type="doi">10.1007/s12021-008-9041-y</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><etal/></person-group><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>Neuroimage</source><year>2014</year><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="pmcid">PMC3849338</pub-id><pub-id pub-id-type="pmid">23994314</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Czeibert</surname><given-names>K</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Petneházy</surname><given-names>Ö</given-names></name><name><surname>Kubinyi</surname><given-names>E</given-names></name></person-group><article-title>A detailed canine brain label map for neuroimaging analysis</article-title><source>Biol Futur</source><year>2019</year><comment>in press</comment><pub-id pub-id-type="pmid">34554420</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Object-specific semantic coding in human perirhinal cortex</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><fpage>4766</fpage><lpage>4775</lpage><pub-id pub-id-type="pmcid">PMC6802719</pub-id><pub-id pub-id-type="pmid">24695697</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2828-13.2014</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname><given-names>AC</given-names></name><etal/></person-group><article-title>The Representation of Biological Classes in the Human Brain</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="pmcid">PMC3532035</pub-id><pub-id pub-id-type="pmid">22357845</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Information-based functional brain mapping</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="pmcid">PMC1383651</pub-id><pub-id pub-id-type="pmid">16537458</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stelzer</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><article-title>Statistical inference and multiple testing correction in classification-based multi-voxel pattern analysis (MVPA): Random permutations and cluster size control</article-title><source>Neuroimage</source><year>2013</year><volume>65</volume><fpage>69</fpage><lpage>82</lpage><pub-id pub-id-type="pmid">23041526</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><article-title>Evidence for a Third Visual Pathway Specialized for Social Perception</article-title><source>Trends Cogn Sci</source><year>2021</year><comment>[Preprint]</comment><pub-id pub-id-type="pmcid">PMC7811363</pub-id><pub-id pub-id-type="pmid">33334693</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.11.006</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><article-title>Disentangling Object Category Representations Driven by Dynamic and Static Visual Input</article-title><source>Journal of Neuroscience</source><year>2023</year><volume>43</volume><fpage>621</fpage><lpage>634</lpage><pub-id pub-id-type="pmcid">PMC9888510</pub-id><pub-id pub-id-type="pmid">36639892</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0371-22.2022</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname><given-names>DD</given-names></name><etal/></person-group><article-title>Awake fMRI reveals a specialized region in dog temporal cortex for face processing</article-title><source>PeerJ</source><year>2015</year><volume>3</volume><elocation-id>e1115</elocation-id><pub-id pub-id-type="pmcid">PMC4540004</pub-id><pub-id pub-id-type="pmid">26290784</pub-id><pub-id pub-id-type="doi">10.7717/peerj.1115</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuaya</surname><given-names>LV</given-names></name><name><surname>Hernández-pérez</surname><given-names>R</given-names></name><name><surname>Concha</surname><given-names>L</given-names></name></person-group><article-title>Our Faces in the Dog ‘ s Brain: Functional Imaging Reveals Temporal Cortex Activation during Perception of Human Faces</article-title><source>PLoS One</source><year>2016</year><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC4774982</pub-id><pub-id pub-id-type="pmid">26934715</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0149431</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Denison</surname><given-names>RN</given-names></name><name><surname>Acaro</surname><given-names>MJ</given-names></name><name><surname>Barack</surname><given-names>DL</given-names></name></person-group><article-title>Tasks and their role in visual neuroscience</article-title><source>Neuron</source><year>2023</year><comment>0–7</comment><pub-id pub-id-type="pmid">37040765</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><article-title>Expectation (and attention) in visual cognition</article-title><source>Trends Cogn Sci</source><year>2009</year><comment>[Preprint]</comment><pub-id pub-id-type="pmid">19716752</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental paradigm.</title><p>Still images from two sample video stimuli representing each of the three animate (yellow) conditions: dog (blue), human (pink), cat (green) and the inanimate condition: car (grey) and the design of a single run. Each 310 s long run was composed of 12 videos (3.5-5.5 s, mean 4.5 s) of each of the four conditions followed by an interstimulus interval (0.7-2.5 s, mean 1.7 s). Stimulus design was identical for dog and human subjects. See also <xref ref-type="supplementary-material" rid="SD1">Video S1</xref>.</p></caption><graphic xlink:href="EMS200160-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>RSA comparing neural and EVC model representations.</title><p>A, DSM for the EVC model. B, Results for correlation with the EVC model in dogs (n = 15) and humans (n = 13) superimposed on a template brain (for dogs: Czeibert et al., 2019; for humans: MNI template brain), using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05 in a searchlight analysis (sphere radius = 3 voxels for dogs and 3 voxels for humans). L=left; R=right; EMG=ectomarginal gyrus; SpG=splenial gyrus; mSSG=mid suprasylvian gyrus; MG=marginal gyrus; CAL=calcarine fissure and surrounding cortex.</p></caption><graphic xlink:href="EMS200160-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>GLM results in dogs and humans.</title><p>A, Dog and human contrast maps superimposed on a template brain, using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. Upper row: A &gt; iA (yellow), lower rows: D &gt; iA (blue), H &gt; iA (pink), C &gt; iA (green). B, Response profiles. Bar graphs represent parameter estimates (beta weights) in select GLM-derived peaks (sphere radius = 3 voxels) for each condition. C, Activity proportions and overlaps. Horizontal bars (and percentages) represent the proportions of suprathreshold voxels per condition, relative to the total voxel number, within the visually responsive cortex; vertical overlaps represent activity overlaps. D, Overlap calculation results. The percentage of suprathreshold voxels within the visually responsive cortex for which at least two conditions overlap and in which all three overlap. E, Species-preference. Proportion of animate-preferring voxels for which the strongest response was found for the given condition. L=left; R=right; OG = occipital gyrus; mSSG = mid suprasylvian gyrus; cSSG = caudal suprasylvian gyrus; cSG = sylvian gyrus; SpG = splenial gyrus; mESG = mid ectosylvian gyrus; cCG = caudal composite gyrus; FFG = fusiform gyrus; HC=hippocampus; IOG = inferior occipital gyrus; ITG=inferior temporal gyrus; MTG = middle temporal gyrus. *: p&lt;0.01, +: p=0.062. Error bars represent SE.</p></caption><graphic xlink:href="EMS200160-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Category boundary test results in dogs and humans.</title><p>A, Dog and human results superimposed on a template brain using a cluster threshold of Z&gt;3.1 and a corrected cluster significance threshold of p=0.05. B, Activity proportions and overlaps. Horizontal bars (and percentages) represent the proportions of suprathreshold voxels per condition, relative to the total voxel number, within the visually responsive cortex; vertical overlaps represent activity overlaps. C, Overlap calculation results. The percentage of suprathreshold voxels within the visually responsive cortex for which at least two conditions overlap and in which all three overlap. L=left. *: p&lt;0.01. Error bars represent SE.</p></caption><graphic xlink:href="EMS200160-f004"/></fig></floats-group></article>