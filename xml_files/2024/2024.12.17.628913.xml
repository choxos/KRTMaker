<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202332</article-id><article-id pub-id-type="doi">10.1101/2024.12.17.628913</article-id><article-id pub-id-type="archive">PPR958201</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>EEG of the dancing brain: Decoding sensory, motor and social processes during dyadic dance</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bigand</surname><given-names>Félix</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Bianco</surname><given-names>Roberta</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Abalde</surname><given-names>Sara F.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nguyen</surname><given-names>Trinh</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Novembre</surname><given-names>Giacomo</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroscience of Perception &amp; Action Lab, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/042t93s57</institution-id><institution>Italian Institute of Technology</institution></institution-wrap>, <addr-line>Viale Regina Elena 291</addr-line>, <postal-code>00161</postal-code><city>Rome</city>, <country country="IT">Italy</country></aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>felix.bigand@iit.it</email> (F.B.), <email>giacomo.novembre@iit.it</email> (G.N.)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Real-world social cognition requires processing and adapting to multiple dynamic information streams. Interpreting neural activity in such ecological conditions remains a key challenge for neuroscience. This study leverages advancements in de-noising techniques and multivariate modeling to extract interpretable EEG signals from pairs of participants (male-male, female-female, and male-female) engaged in spontaneous dyadic dance. Using multivariate temporal response functions (mTRFs), we investigated how music acoustics, self-generated kinematics, other-generated kinematics, and social coordination uniquely contributed to EEG activity. Electromyogram recordings from ocular, face, and neck muscles were also modeled to control for artifacts. The mTRFs effectively disentangled neural signals associated with four processes: (I) auditory tracking of music, (II) control of self-generated movements, (III) visual monitoring of partner movements, and (IV) visual tracking of social coordination. We show that the first three neural signals are driven by event-related potentials: the P50-N100-P200 triggered by acoustic events, the central lateralized movement-related cortical potentials triggered by movement initiation, and the occipital N170 triggered by movement observation. Notably, the (previously unknown) neural marker of social coordination encodes the spatiotemporal alignment between dancers, surpassing the encoding of self-or partner-related kinematics taken alone. This marker emerges when partners can see each other, exhibits a topographical distribution over occipital areas, and is specifically driven by movement observation rather than initiation. Using data-driven kinematic decomposition, we further show that vertical bounce movements best drive observers’ EEG activity. These findings highlight the potential of real-world neuroimaging, combined with multivariate modeling, to uncover the mechanisms underlying complex yet natural social behaviors.</p></abstract><kwd-group><kwd>Electroencephalography (EEG)</kwd><kwd>multivariate modeling</kwd><kwd>temporal response function (TRF)</kwd><kwd>real-world behavior</kwd><kwd>dance</kwd><kwd>full-body kinematics</kwd><kwd>spontaneous movement</kwd><kwd>sensorimotor processing</kwd><kwd>social coordination</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">A central challenge in neuroscience is understanding how the brain supports natural behavior in real-world contexts. Neuroimaging studies have traditionally been limited by bulky, motion-sensitive equipment, restricting research to controlled, motionless behaviors. This approach fails to capture how the brain manages the dynamic, multifaceted demands of everyday life, where cognition involves simultaneous neural processes, unconstrained movement, and interaction with ever-changing sensory environments—factors that traditional lab studies are poorly equipped to address (<xref ref-type="bibr" rid="R68">Stangl et al., 2023</xref>). Despite the recent advancements in mobile neuroimaging techniques (<xref ref-type="bibr" rid="R51">Niso et al., 2023</xref>) and algorithms for removing motion artifacts (<xref ref-type="bibr" rid="R35">Kothe and Jung, 2016</xref>), studying brain activity during natural behavior remains underexploited. As a result, it remains unclear how neural processes identified in lab-controlled studies generalize to real-world experiences, limiting our ability to interpret neural signals recorded during free behavior.</p><p id="P3">Here we used human collective dance as a model to study the neural basis of real-world interactions. We reason that dance offers an ideal testbed for several reasons: 1) it is culturally ubiquitous, hence broadly generalizable (<xref ref-type="bibr" rid="R44">Mithen, 2006</xref>; <xref ref-type="bibr" rid="R22">Dunbar, 2012</xref>); 2) it is complex yet controllable through musical structure (<xref ref-type="bibr" rid="R13">D’Ausilio et al., 2015</xref>); and 3) it encapsulates several intertwined neural processes, including auditory-tracking of music, movement control, monitoring others’ movements, and integrating these signals into cohesive experiences (<xref ref-type="bibr" rid="R25">Foster Vander Elst et al., 2023</xref>). These processes—notably targeting a variety of sensory and motor systems—can be effectively measured, for example, using electroencephalography (EEG). Yet, the main analytical challenge lies in disentangling these simultaneous neural signals (capturing sensory, motor, and social functions) from each other, and artifactual signals.</p><p id="P4">We tackled this challenge using multivariate temporal response functions (mTRFs), a computational approach that models the influence of different input variables on neural activity (<xref ref-type="bibr" rid="R38">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="R11">Crosse et al., 2016</xref>). We applied this method to a dataset of 80 participants, forming 40 dyads, who danced spontaneously to music while their brain activity, muscle activity, and full-body movements were recorded. Specifically, we captured EEG (64 channels), 3D full-body kinematics (22 markers), electrooculography (EOG), and electromyography (EMG, from neck and facial muscles), across various experimental conditions—detailed below (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>). mTRFs were meant to isolate four concurrent neural processes: 1) auditory perception of music, 2) motor control of specific body parts or specific movements, 3) visual perception of a partner’s body movements, and 4) visual tracking of social coordination, defined as the spatiotemporal alignment of movements between dancers, whether in-phase or anti-phase. Importantly, EOG and EMG signals were included as model predictors to account for potential muscle artifacts affecting the neural data. Additionally, we used event-related potential (ERP) analyses to anchor our findings in established physiological markers of sensory (auditory and visual evoked potentials) and motor (movement-related cortical potentials) processes (<xref ref-type="bibr" rid="R52">Novembre et al., 2018</xref>; <xref ref-type="bibr" rid="R1">Bach and Ullrich, 1997</xref>; <xref ref-type="bibr" rid="R14">Deecke et al., 1969</xref>).</p><p id="P5">Previous studies have used mTRFs to extract neural tracking of ecological auditory and visual stimuli, such as speech, music, or films (<xref ref-type="bibr" rid="R19">Di Liberto et al., 2015</xref>, <xref ref-type="bibr" rid="R20">2020</xref>; <xref ref-type="bibr" rid="R58">O’Sullivan et al., 2017</xref>; <xref ref-type="bibr" rid="R24">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="R31">Jessen et al., 2019</xref>; <xref ref-type="bibr" rid="R4">Bianco et al., 2024</xref>; <xref ref-type="bibr" rid="R16">Desai et al., 2024</xref>). However, aside from one human study and recent animal research incorporating body kinematics (<xref ref-type="bibr" rid="R46">Musall et al., 2019</xref>; <xref ref-type="bibr" rid="R69">Stringer et al., 2019</xref>; <xref ref-type="bibr" rid="R17">Di Liberto et al., 2021</xref>; <xref ref-type="bibr" rid="R42">Mao et al., 2021</xref>; <xref ref-type="bibr" rid="R73">Tremblay et al., 2023</xref>; <xref ref-type="bibr" rid="R39">Lanzarini et al., 2025</xref>), human studies that concurrently examine both sensory and motor processes using mTRFs—particularly in naturalistic behaviors—remain scarce. Furthermore, to our knowledge, no study has explicitly modeled social processes or addressed the neural activity associated with body-movement artifact leakage, as we do here. As such, our holistic approach aims to demonstrate that naturalistic human behaviors—implying real-time adaptation and movement—can be effectively explored using traditional electrophysiology. Therefore, our study highlights the potential of advanced neural analysis techniques to bridge the gap between lab-controlled and real-world neuroimaging research, enhancing our understanding of the neural basis of natural human behavior.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and methods</title><p id="P6">The EEG, EOG, EMG, and kinematic data analyzed here were collected as part of a previous study (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>), where participant dyads engaged in spontaneous dance under a 2×2 experimental design (<xref ref-type="fig" rid="F1">Figs. 1a and 1b</xref>). The manipulated within-dyad factors were musical input (whether participants danced to the same [synchronous] or different [asynchronous] music) and visual contact (whether participants could see or not see their dancing partner).</p><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P7">80 participants (54 females; mean age: 26.15 years, SD: 6.43 years, 74 right-handed) formed 40 dyads (52% female-male, 41% female-female, and 7% male-male). To minimize inter-individual variability while maximizing generalizability, we recruited only laypersons (i.e. individuals without formal dance training). All participants forming a dyad were familiar with each other and were informed about the social nature of the task during recruitment, when they received the following message (translated from Italian): “You will have to come with someone you know (friend, family member, colleague…) with whom you will dance while listening to music (almost) like in a disco!”. As a measure of participants’ inclination toward social dance, we present the results of a post-hoc questionnaire. Specifically, participants rated the statement <italic>“How often do you dance to music”</italic> with a mean score of 4.363 (SD = 1.052) on a 6-point Likert scale (<italic>1 = Never</italic> to <italic>6 = Very frequently</italic>), and rated the statements <italic>“When at a party, I am likely to be one of the first people dancing”</italic> and <italic>“I do not worry what other people think of my dancing skills”</italic> with mean scores of 3.863 (SD = 1.626) and 3.863 (SD = 1.349), respectively, on a 6-point Likert scale (<italic>1 = Strongly disagree</italic> to <italic>6 = Strongly agree</italic>). Participants had normal or corrected-to-normal vision, normal hearing, and no history of neurological disorders. Data from five dyads were excluded due to recording failure in the motion capture system, leaving data from 70 participants for the analysis. All participants provided written informed consent to participate in the study and were compensated €25 for their participation. All experimental procedures were approved by “Comitato Etico Regionale della Liguria” (794/2021 - DB id 12093) and were carried out under the principles of the revised Helsinki Declaration.</p></sec><sec id="S4"><title>Musical stimuli</title><p id="P8">The musical stimuli consisted of eight songs with an average duration of 39.8 seconds (standard deviation: 1.95 seconds). Each song was presented in all four experimental conditions (see Experimental Design and Procedure below), resulting in a total of 32 trials. These songs were remakes of famous song refrains from electronic dance music and disco-funk genres (see <xref ref-type="bibr" rid="R5">Bigand et al. (2024)</xref>). Each song was adapted using the same four musical instruments: drums, bass, keyboards, and violin (the latter providing the vocal melody). All stimuli followed a 4/4 meter and spanned 20 bars. To create these adaptations, author FB and a professional composer (Raoul Tchoï) transcribed the original 4-bar refrain loops into MIDI format and synthesized them using MIDI instruments in Logic Pro X (Apple, Inc.). The rearranged songs were then systematically structured by repeating the 4-bar loops five times and sequentially adding each instrument to the musical scene, in the following order: (1) drums, (2) bass, (3) keyboards, (4) voice, (5) voice <italic>bis</italic> (i.e. the loop with full instruments was repeated twice). Loudness level across songs was controlled within a range of 1.5 LUFS (a measure accounting for the frequency sensitivity of the human auditory system). The songs were presented to the two participants forming each dyad through two separate EEG-compatible earphones (Etymotic ER3C), each connected to a distinct output channel of an audio interface (RME Fireface UC).</p><p id="P9">Every trial consisted of one song flanked with a fast-rising tone (rise time: 5 ms, fall time: 30 ms, frequency: 494 Hz, duration: 350 ms), preceded by 8 seconds of silence and followed by 7 seconds of silence, following this pattern: beep-silence-song-silence-beep. Trials were controlled using Presentation software (Neurobehavioral Systems), with synchronization between song presentation, EEG, and motion capture recordings achieved via TTL pulses. A TTL pulse was sent at the start of each trial from Presentation to both the EEG system (BioSemi ActiveTwo) and the motion capture system (Vicon; Lock+). This pulse activated the motion capture system, initiating recording, which automatically stopped after one minute. Simultaneously, the TTL pulse was stored alongside the continuous EEG recordings, which remained uninterrupted throughout the experiment. The TTL pulse, whose value varied based on the trial condition, enabled us to epoch the EEG data and retrieve the corresponding trial condition for analysis.</p></sec><sec id="S5"><title>Experimental design and procedure</title><p id="P10">EEG and kinematic data were recorded across four conditions derived from a 2×2 experimental design with visual contact (Yes, No) and musical input (Same, Different) as within-dyad factors. Conditions with or without visual contact were defined by the presence or absence of a curtain between the two participants in each dyad. Musical input was manipulated by presenting either identical or different songs to the participants through earphones. Because each song had a different tempo, the songs played simultaneously to the two participants were either perfectly synchronized (in the same-music condition) or slightly out of sync (in the different-music condition). To minimize inter-trial variability, this degree of asynchrony was maintained constant across trials belonging to the different-music condition (i.e. relative tempo difference between the two songs was precisely 8.5%). This was achieved by presenting participants with songs from different genres during the different-music condition (the tempo associated with electronic dance music songs was on average faster than that of disco-funk songs; see <xref ref-type="bibr" rid="R5">Bigand et al. (2024)</xref>). Trials were organized into four blocks, with each block including eight trials (two trials per condition, each trial featuring a different song). The presentation order of the blocks – and trials within blocks – was randomized, except for the deliberate presentation of subsequent pairs of yes-vision or no-vision trials to minimize the displacement of the curtain.</p><p id="P11">Before the experiment began, participants were told to behave as in a “silent disco,” in which they should face each other, enjoy the music, and remain still during periods of silence before and after the music. To enhance participants’ comfort, the overhead lighting was dimmed using alternating red and blue colored filters, creating a softer, “disco-like” atmosphere. Additionally, the experimenter remained out of sight in a custom-built cabin (enclosed by 1.5m-high panels), ensuring mutual invisibility between the experimenter and participants, as well as concealing the acquisition computers. Participants completed two training trials using songs not included in the main experiment to familiarize themselves with the task and setting. During this phase, they could request volume adjustments to their earphones, which were instructed to be set “as loud as possible without discomfort.” Participants were allowed (but not required) to dance freely within their designated space, keeping their head orientation towards their partner as steady as possible. Speaking or singing during trials was prohibited. Throughout the experiment, participants stood facing each other, each positioned within a marked area of 0.5×0.7 meters, with a separation of 2.5 meters between them.</p></sec><sec id="S6"><title>Kinematics data acquisition and preprocessing</title><p id="P12">3D full-body kinematics were recorded using wearable markers (22 per participant, size=14 mm). Markers were placed on specific body parts, denoted as follows (L = left, R = right, F = front, B = back): (1) LB Head, (2) LF Head, (3) RF Head, (4) RB Head, (5) Sternum, (6) L Shoulder, (7) R Shoulder, (8) L Elbow, (9) L Wrist, (10) L Hand, (11) R Elbow, (12) R Wrist, (13) R Hand, (14) Pelvis, (15) L Hip, (16) R Hip, (17) L Knee, (18) L Ankle, (19) L Foot, (20) R Knee, (21) R Ankle, (22) R Foot (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Additionally, one supplementary marker was placed asymmetrically on either the left or right thigh of each participant. This marker was only used to facilitate Nexus software in the distinction between participants and was not considered in subsequent analyses. Eight optical motion capture cameras (Vicon system) recorded the markers’ trajectories at a sampling rate of 250 Hz. The cameras were positioned to capture the participants from various angles, ensuring that each participant was visible to at least six cameras even when visual contact was obstructed by the curtain. A high-definition video camera, synchronized with all the optical motion capture cameras, recorded the scene from an aerial view (Vicon Vue; 25 Hz sampling frequency; 1,920 × 1,080 pixels). We used a Vicon motion capture system to record full-body 3D positions with high spatial (&lt;1 mm precision) and temporal (250 Hz) resolution. While alternative methods, such as inertial measurement units or accelerometers, could be considered, the feasibility of repeating our study with fewer markers remains to be tested. Notably, full-body tracking was essential here for breaking down complex dance kinematics into the elementary movement components that drove neural signals (see Kinematic feature selection below).</p><p id="P13">Markers’ trajectories were corrected for swaps or mislabels via the Nexus manual labeling tool (Vicon). Then, automated correction of frequent and systematic marker swaps was performed using custom Python code. Any gaps in the marker trajectories were then filled using the automatic gap-filling pipeline in Nexus. The proportion of time with gaps, calculated for each marker and averaged across participants, ranged from a minimum of 0.128% (L Foot) to a maximum of 2.767% (R Hip), with a mean of 0.688% and a standard deviation of 0.739%. Lastly, all trajectories were inspected visually within Nexus software and manually adjusted if they did not match the aerial-view video recording. Subsequent data analyses were carried out in Python using custom code. Marker trajectories comprised 3D positions (along x, y, and z axes) corresponding to each of the 22 body parts, resulting in time-series of posture vectors of 66 dimensions.</p></sec><sec id="S7"><title>VEEG data acquisition and preprocessing</title><p id="P14">We recorded neural activity from both participants simultaneously using a dual-EEG setup with the BioSemi ActiveTwo system. This setup consists of two AD-Boxes, each independently recording and referencing EEG from a single participant. The data from the two AD-Boxes are synchronized at the hardware level: the ‘slave’ AD-Box transmits data via optical fiber to the ‘master’ AD-Box, which then relays all EEG signals and triggers information to the acquisition computer. For a detailed schematic of the BioSemi ActiveTwo dual-EEG configuration, see <xref ref-type="bibr" rid="R2">Barraza et al. (2019)</xref>. For each participant, the EEG was recorded from 64 Ag/AgCl active electrodes (placed on the scalp according to the extended international 10–10 system). To help retain the naturalistic nature of the study, we used 2-meter-long cables, custom-built by the manufacturer to meet our specific requirements. Each EEG amplifier was positioned behind the participant at hip height, with cables taped to the upper back to minimize weight while ensuring they remained loose enough to prevent any perceived constraint or pulling. This setup allowed participants to move relatively freely while remaining within their designated area (see Experimental Design and Procedure).</p><p id="P15">EEG signals were digitized at 1024 Hz using the BioSemi Active Two system. Subsequently, the data were pre-processed and analyzed using Matlab R2022. Measuring EEG from moving participants is susceptible to muscular artifacts in the recordings. To mitigate this issue, we pre-processed the EEG data of the dancing participants using a fully data-driven pipeline that we had previously developed for analyzing EEG data in awake monkeys (<xref ref-type="bibr" rid="R4">Bianco et al., 2024</xref>). This pipeline primarily utilizes open-source algorithms from Fieldtrip (<xref ref-type="bibr" rid="R55">Oostenveld et al., 2011</xref>) and EEGLAB (<xref ref-type="bibr" rid="R15">Delorme and Makeig, 2004</xref>) toolboxes. EEG signals were digitally filtered between 1 and 8 Hz (Butterworth filters, order 3), down-sampled to 100 Hz, and trimmed according to the duration of the trial-specific songs. Faulty or noisy electrodes were provisionally discarded before re-referencing the data using a common average reference. This was done to prevent the leakage of noise to all electrodes during re-referencing. Criteria for flagging faulty or noisy electrodes included prolonged flat lines (lasting more than 5 seconds), abnormal inter-channel correlation (lower than 0.8), or deviations in amplitude metrics from the scalp average (mean, STD, or peak-to-peak values exceeding 3 STD from the scalp average). These assessments were made using EEGlab’s <italic>clean_flatlines</italic> and <italic>clean_channels</italic> functions (<xref ref-type="bibr" rid="R15">Delorme and Makeig, 2004</xref>) and custom Matlab code. To remove movement artifacts, we further denoised the re-referenced data using a validated algorithm for automatic artifact correction: Artifact Subspace Reconstruction (ASR, threshold value 5) (<xref ref-type="bibr" rid="R35">Kothe and Jung, 2016</xref>). This algorithm has been previously applied to human data, including in music-making and dance studies (<xref ref-type="bibr" rid="R63">Ramírez-Moreno et al., 2023</xref>; <xref ref-type="bibr" rid="R71">Theofanopoulou et al., 2024</xref>). Finally, eye-movement artifacts were subtracted from the ASR-cleaned data using another automatic artifact-correction algorithm – ICA, using EEGlab’s <italic>IClabel</italic> function (<xref ref-type="bibr" rid="R60">Pion-Tonachini et al., 2019</xref>). Independent Components that were classified by <italic>IClabel</italic> as eye-movement artifacts (i.e., those for which the ‘eye’ category had the highest probability, with no minimum threshold) were removed. At this stage, noisy or faulty electrodes (as assessed at the start of this preprocessing pipeline) were interpolated by replacing their voltage with the average voltage of the neighboring electrodes (20-mm distance).</p></sec><sec id="S8"><title>EOG and EMG data acquisition and preprocessing</title><p id="P16">Two EOG channels were recorded using surface Ag–AgCl electrodes from all participants. Electrodes were attached using disposable adhesive disks at specific anatomical locations: the left and right outer canthi. Additionally, we also recorded four EMG signals from the cheeks (the left and right zygomata) and the neck (the left and right paraspinal muscles) for control purposes. EOG/EMG signals were digitized at 1024 Hz using the BioSemi Active Two system. The EOG/EMG data were filtered, down-sampled, and trimmed similarly as the EEG data, re-referenced using scalp average, and ASR-cleaned using a threshold value of 5, to maintain consistency with the EEG signals from scalp channels. It should be noted that these signals were measured from a subset of participants (n=58), therefore all subsequent analyses involving this data subset include only these participants.</p></sec><sec id="S9"><title>Multivariate temporal response functions (mTRFs)</title><p id="P17">Events, such as hearing a fast-rising sound or initiating a movement, elicit phase-locked brain activity within a specific time window [t1,t2], which can include post-event (e.g., response to sounds) and pre-event (e.g., movement initiation) components (<xref ref-type="bibr" rid="R41">Luck, 2014</xref>). Temporal response functions (TRFs) can be used to characterize this relationship at the level of EEG electrodes (<xref ref-type="bibr" rid="R38">Lalor et al., 2009</xref>; <xref ref-type="bibr" rid="R11">Crosse et al., 2016</xref>). In this study, we applied mTRFs to delineate the distinct neural processes that occur simultaneously during dyadic dance. Specifically, we first extracted a diverse set of time-resolved variables, representing: (I) musical input, (II) self-generated movements, (III) partner-generated movements, (IV) social coordination, and (V, VI, and VII) ocular, facial and neck muscle activity (<xref ref-type="fig" rid="F1">Fig. 1c</xref>, left). Next, we estimated TRFs (<xref ref-type="fig" rid="F1">Fig. 1c</xref>, middle) to quantify how these variables modulate EEG signals, for each electrode separately (<xref ref-type="fig" rid="F1">Fig. 1c</xref>, right). The following sections provide a detailed explanation of these two steps.</p><p id="P18"><italic>Step 1: Extraction of variables. (I) Music</italic>. Musical input was represented using spectral flux, which captures fluctuations in the acoustic power spectrum. Spectral flux has been shown to outperform other acoustic features, such as the envelope and its derivative, in predicting neural signals elicited by music (<xref ref-type="bibr" rid="R81">Weineck et al., 2022</xref>). To extract it, we first bandpass filtered the musical stimuli into 128 logarithmically spaced frequency bands ranging from 100 to 8000 Hz using a gammatone filter bank. Spectral flux was then computed for each frequency band by calculating the first derivative of the band’s amplitude over time. Finally, the broadband spectral flux, representing overall changes in the spectral content, was derived by averaging the spectral flux across all 128 bands. <italic>(II and III) Self- and other-generated movements</italic>. The movements produced by each participant (self-generated) and their partners (other-generated) were represented using velocity magnitude. To reduce dimensionality, full-body trajectories were decomposed into 15 principal movement patterns that collectively explained over 95% of the kinematic variance (see Methods in Kinematic feature selection below). The velocity magnitude of each principal movement was calculated by taking the first derivative of its position over time and then computing the absolute value of this derivative. Out of the 15 principal movements, preliminary analyses identified bounce as the movement that explained most of the neural encoding of both self- and other-generated movements (see results in Kinematic feature selection below, and <xref ref-type="fig" rid="F2">Fig. 2</xref>). Consequently, only the velocity magnitude of the bounce trajectory was included in subsequent models. <italic>(IV) Social coordination</italic>. To assess social coordination, we extracted a categorical measure to determine whether the bounce movements of participants within a dyad were in-phase or anti-phase. This measure indexed whether both individuals moved in the same direction (in-phase) or opposite directions (anti-phase). We obtained this measure by multiplying the signs of the bounce velocity time series (i.e., the respective directions of movement) across the two participants forming a dyad. <italic>(V, VI, and VII) Ocular, facial, and neck muscle activity</italic>. To control for muscular activity potentially leaking into the EEG signals, we also included EOG (measured from the left and right eyes) and EMG (measured from the cheeks and the neck) time-series in the models. Both EOG channels (left and right eye) were included to capture horizontal saccades, which generate opposite left-right activity (positive values on one side and negative on the other). For cheek and neck muscles, the average signal from the left and right EMG channels was used, respectively. All these variables, each of which is time-resolved, were down-sampled to match the EEG sampling frequency of 100 Hz and trimmed according to the duration of the trial-specific songs. To account for inter-individual variability, all variables were standardized on a per-participant basis by normalizing each time-series to its standard deviation across all trials for the corresponding participant.</p><p id="P19"><italic>Step 2: mTRF estimation</italic>. We estimated TRFs via a multivariate lagged regression, which fitted the optimal linear mapping between the abovementioned variables and EEG at each electrode (mTRF toolbox, encoding model (<xref ref-type="bibr" rid="R11">Crosse et al., 2016</xref>); <xref ref-type="fig" rid="F1">Fig. 1c</xref>). A time-lag window of -250 to 300 ms was selected to encompass commonly observed ERP responses associated with sound perception (<xref ref-type="bibr" rid="R52">Novembre et al., 2018</xref>), execution of fast-repeated movements (<xref ref-type="bibr" rid="R26">Gerloff et al., 1997</xref>), and visual perception of biological movements (<xref ref-type="bibr" rid="R32">Jokisch et al., 2005</xref>). This window also ensured that the contribution of redundant (potentially irrelevant) information was minimized, especially considering the rhythmic structure of the task, with musical beats and some dance movements (e.g., bounce) occurring approximately every 500 ms. Importantly, in a control analysis we confirmed that the selected window did not reduce prediction accuracy compared to a broader [-700, +700 ms] window. For each participant and experimental condition, mTRFs were estimated, including either all variables simultaneously (full model) or all variables except the specified one (reduced models; see below for details). Participant- and condition-specific TRFs were estimated as the average TRF required to predict each of the eight condition trials using data from the remaining seven trials (i.e., TRFs were fit eight times). Regularized (ridge) regression was used to fit the TRFs, maximizing prediction accuracy (the correlation between the predicted and actual EEG data; Pearson’s <italic>r</italic>) without overfitting the training data. The optimal regularization parameter (λ) was selected via leave-one-out cross-validation across trials (i.e., songs), tested over a range from 0 to 10□ (0, 10□□, 10□<sup>3</sup>, …, 10□). This yielded one optimal λ value per trial, condition, and participant. Finally, prediction accuracies for each condition were assessed using a generic approach (<xref ref-type="bibr" rid="R18">Di Liberto and Lalor, 2017</xref>; <xref ref-type="bibr" rid="R31">Jessen et al., 2019</xref>), where the Pearson’s <italic>r</italic> between predicted and actual EEG data was calculated across all eight trials of the <italic>n</italic><sup>th</sup> participant, with predictions based on a generic TRF averaged across the subject-specific TRFs of the <italic>N-1</italic> remaining participants (N=70). The prediction accuracy of a model describes the amount of EEG variance that the model can account for. To evaluate the unique amount of EEG variance that each variable accounts for, we constructed reduced models that included all variables apart from the specified one. The difference in prediction accuracy between the full (comprising all variables) and the reduced model yielded the unique contribution, denoted as <italic>Δr</italic>, of that specific variable to the variance explained in the EEG data (<xref ref-type="fig" rid="F1">Fig. 1d</xref>).</p></sec><sec id="S10"><title>Kinematic feature selection</title><p id="P20">To reduce dimensionality, we used a data-driven method to determine a subset of kinematic variables to use in the TRF models. The kinematic data were decomposed into a set of principal movements using Principal Component Analysis (PCA), following the same pipeline as described in <xref ref-type="bibr" rid="R5">Bigand et al., (2024)</xref>. These principal movements reflect movement primitives that are generalizable across trials, conditions, and participants. This PCA approach has been previously validated for a wide range of human movements, including dance (<xref ref-type="bibr" rid="R74">Troje, 2002</xref>; <xref ref-type="bibr" rid="R12">Daffertshofer et al., 2004</xref>; <xref ref-type="bibr" rid="R72">Toiviainen et al., 2010</xref>; <xref ref-type="bibr" rid="R23">Federolf et al., 2014</xref>; <xref ref-type="bibr" rid="R83">Yan et al., 2020</xref>; <xref ref-type="bibr" rid="R6">Bigand et al., 2021</xref>). The first 15 principal movements – accounting for more than 95% of the kinematic variance – were retained for further analyses (<xref ref-type="bibr" rid="R23">Federolf et al., 2014</xref>; <xref ref-type="bibr" rid="R6">Bigand et al., 2021</xref>). The score time series obtained from the PCA reflected the position of each principal movement over time. These time-series were low-pass filtered below 6 Hz using a Butterworth filter (second-order, zero-phase) to increase the signal-to-noise ratio. These 15 principal movements were reminiscent of common “dance moves” such as body sway, twist, upper-body side bend and rock, bounce, side displacement, head bob, hip swing, and hand movements (see <xref ref-type="fig" rid="F2">Fig. 2</xref> and Video 1) (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>).</p><p id="P21">To determine which principal movements to include in the TRF models, we tested their association with EEG modulations. Previous evidence suggests that TRFs or equivalent models can accurately capture neural activity associated with both the generation (<xref ref-type="bibr" rid="R46">Musall et al., 2019</xref>) and the observation (<xref ref-type="bibr" rid="R58">O’Sullivan et al., 2017</xref>; <xref ref-type="bibr" rid="R31">Jessen et al., 2019</xref>) of biological movement. Accordingly, we tested the unique contribution of the 15 principal movements, either self-generated or generated by (and therefore observed in) the dancing partner. In other words, we fit 30 reduced models and computed the difference in prediction accuracy (<italic>Δr</italic>) between each reduced model and a full model (including all 30 principal movements plus spectral flux) for each participant and condition, using the generic approach outlined above (see Methods in mTRF estimation above). Spectral flux was included in the full model to ensure that the explanatory power of individual principal movements was not influenced by movements correlated with the music as participants were dancing to music. To reduce computational cost, the 15 models for other-generated movements were trained in the visual conditions, while those for self-generated movements were trained in the non-visual conditions. This ensured a balanced number of trials for analyzing both motor control and movement observation activities while testing movement observation under the conditions where it was most likely to occur. The full model was trained across all conditions, allowing for the computation of “self” and “other” <italic>Δr</italic> values, averaged across the two non-visual and visual conditions, respectively.</p><p id="P22">The results of this preliminary analysis revealed that bounce movement—i.e., vertical oscillations of the body achieved through knee flexion and extension—was largely the main contributor to EEG prediction, notably across both self- and other-generated movements (see PM10; <xref ref-type="fig" rid="F2">Fig. 2</xref>), despite accounting for no more than 1% of the total kinematic variance. Specifically, self-generated bounce alone explained &gt;84% of the EEG prediction gain (<italic>Δr</italic> &gt; 0) across all principal movements at electrode Cz, commonly associated with motor activity (<xref ref-type="bibr" rid="R34">Kornhuber and Deecke, 1965</xref>; <xref ref-type="bibr" rid="R14">Deecke et al., 1969</xref>; <xref ref-type="bibr" rid="R65">Shibasaki et al., 1980</xref>; <xref ref-type="bibr" rid="R66">Smulders and Miller, 2011</xref>; <xref ref-type="bibr" rid="R78">Vercillo et al., 2018</xref>). Additionally, other-generated bounce alone accounted for &gt;80% of EEG prediction gain at electrode Oz, a canonical site indicative of motion-evoked visual responses (<xref ref-type="bibr" rid="R37">Kubová et al., 1995</xref>; <xref ref-type="bibr" rid="R1">Bach and Ullrich, 1997</xref>; <xref ref-type="bibr" rid="R62">Puce et al., 2000</xref>; <xref ref-type="bibr" rid="R32">Jokisch et al., 2005</xref>; <xref ref-type="bibr" rid="R58">O’Sullivan et al., 2017</xref>). Given these results, bounce will serve as the primary movement feature in all subsequent analyses. Henceforth, when referring to “movement” in the following sections, we specifically denote “bounce” (except when discussing the results of the body-part-specific analyses).</p></sec><sec id="S11"><title>Statistical analyses</title><p id="P23">We assessed the distinct neural encoding of music, self-generated movements, other-generated movements, and social coordination while controlling for artifact leakage from eye, facial, and neck movements. We created seven reduced models, accounting for: (I) music (all variables minus spectral flux); (II) self-generated movements (all variables minus velocity magnitude of self-generated bounce); (III) other-generated movements (all variables minus velocity magnitude of other-generated bounce); (IV) social coordination (all variables minus interpersonal bounce coordination); and (V, VI and VII) ocular, facial and neck muscle activity (all variables minus EOG, facial EMG, or neck EMG, respectively). We then compared the prediction accuracies of these reduced models to that of a full model encompassing all seven variables, i.e., the unique contribution <italic>Δr</italic> of each variable.</p><p id="P24">To compare the unique contributions of music, self-generated movements, other-generated movements, and social coordination across different experimental conditions (visual contact (yes/no) x music (same/different)), <italic>Δr</italic> values were averaged across relevant electrodes for each participant and predictor. Relevant electrodes were defined independently for each predictor as those that exhibited a prediction gain (<italic>Δr</italic> &gt; 0). For each predictor, this gain was computed across conditions where the associated neural process was expected to occur: all conditions for music and self-related movements, and visual conditions for other-generated movements and coordination. The <italic>Δr</italic> values to be statistically compared were computed for each condition and then averaged across the defined electrodes. This yielded a <italic>Δr</italic> value per participant, condition, and variable (music, self- and other-generated movements, and social coordination).</p><p id="P25">We assessed differences in unique contributions across conditions using a 2×2 repeated-measures ANOVA with the factors “visual contact” and “musical input”. <italic>Δr</italic> values were normally distributed and entered into the ANOVA as the dependent variable. To control for multiple comparisons across the four variables, p-values were Bonferroni-corrected.</p></sec><sec id="S12"><title>Event-related potentials (ERPs)</title><sec id="S13"><title>Extraction of ERPs</title><p id="P26">To aid in interpreting the TRF results—particularly the physiological origins of the TRF model weights—we examined phase-locked neural responses, i.e., event-related potentials (ERPs), evoked by changes in music intensity, self-generated movement velocity, other-generated movement velocity, and social coordination (transitions between in-phase and anti-phase coordination – see below). EEG responses are largely evoked by fast changes in the environment (<xref ref-type="bibr" rid="R67">Somervail et al., 2021</xref>), including fluctuations in the auditory spectrum (<xref ref-type="bibr" rid="R81">Weineck et al., 2022</xref>) and peaks in movement velocity (<xref ref-type="bibr" rid="R77">Varlet et al., 2023</xref>). Therefore, we determined the onset times of events, such as sounds or movements, by identifying peaks in the respective time series using Matlab’s <italic>findpeaks</italic> function (with default parameters). Acoustic onsets were thus aligned with musical notes played by any of the four instruments in the stimuli, while motion onsets were aligned with velocity peaks. Coordination onsets were obtained from the first derivative of the coordination time series, corresponding to transitions between in-phase and anti-phase states. To improve the signal-to-noise ratio, acoustic peaks were filtered by selecting only the most salient, i.e., those that were 3 STD away from the mean of the trial. This step was unnecessary in the case of the other variables, such as movement and coordination, presumably because the kinematic data had already been low-pass filtered, as described earlier. Consequently, the signal-to-noise ratio for these variables was already maximized. The ERP epochs spanned the same time window as the TRFs (-250 to 300 ms).</p></sec><sec id="S14"><title>ERP sensitivity to variables’ intensity</title><p id="P27">ERP amplitude largely depends on the differential intensity of the evoking change, and this sensitivity to differential intensity is supramodal, i.e., it’s a property observed across different sensory systems (<xref ref-type="bibr" rid="R67">Somervail et al., 2021</xref>). Here, to quantify whether ERPs were modulated by the differential amplitude of musical sounds or by the speed of self- and other-generated movements, we categorized acoustic onsets into <italic>soft/loud</italic> and movement onsets into <italic>slow/fast</italic>. For each participant and experimental condition, we selected acoustic and motion onsets with the highest and lowest 20% values of spectral flux or velocity magnitude, respectively. Similarly, to quantify the ERP modulation as a function of coordination, we grouped coordination onsets into their two possible values: change to <italic>in-phase</italic> or <italic>anti-phase</italic>. Following established ERP literature (<xref ref-type="bibr" rid="R32">Jokisch et al., 2005</xref>; <xref ref-type="bibr" rid="R52">Novembre et al., 2018</xref>; <xref ref-type="bibr" rid="R78">Vercillo et al., 2018</xref>), epochs linked to external stimuli (music and other) were baseline corrected using a pre-stimulus interval (-250 to 0 ms), while epochs involving internally-initiated actions (self and coordination) were baseline corrected using the entire epoch duration. Differences between the two groups (<italic>soft</italic> vs. <italic>loud, slow</italic> vs. <italic>fast</italic>, or <italic>in-phase</italic> vs. <italic>anti-phase</italic>) were tested separately for each experimental condition, by means of a cluster-based permutation test (implemented in Fieldtrip, with 1000 permutations [<xref ref-type="bibr" rid="R43">Maris and Oostenveld, 2007</xref>]). This analysis focused on the EEG channels of interest informed by the mTRF results: Fz (music), C3 and C4 (self-generated movements), Oz (other-generated movements), and Oz (social coordination).</p></sec></sec><sec id="S15"><title>Body-part-specific mTRF (self)</title><p id="P28">In the main analysis, motor activity was assessed using mTRFs predicted by the kinematics of self-generated bounce, as this movement explained most motor activity across the 15 principal movements identified via PCA. Hence, the main analysis does not differentiate between body parts, as the bounce movement activates nearly all of them (see <xref ref-type="fig" rid="F2">Fig. 2</xref> and Video 1), making it challenging to determine whether the movement of specific body parts drove specific motor activities. To address this issue, we leveraged kinematic data from all parts of the body to calculate the unique contribution of self-generated motion to the EEG from the left and right hand, left and right foot, and head velocity magnitudes. Specifically, we created a full model that included major body markers (‘LB Head’, ‘LF Head’, ‘RF Head’, ‘RB Head’, ‘Sternum’, ‘L Shoulder’, ‘R Shoulder’, ‘L Hand’, ‘R Hand’, ‘Pelvis’, ‘L Hip’, ‘R Hip’, ‘L Knee’, ‘L Foot’, ‘R Knee’, ‘R Foot’) along with neck EMG controls, and five reduced models, each excluding specific markers: left/right hand markers, left/right foot markers, and the average of the four head markers. Markers expected to be almost intrinsically correlated with hand and foot movements (e.g., elbows, wrists, and ankles) were not included in the full model. As in the main analysis, the unique contribution of each body part’s kinematics to the EEG variance was determined by the difference in prediction accuracy between the full model and each reduced model.</p></sec><sec id="S16"><title>Encoding of social coordination</title><sec id="S17"><title>Coordination beyond self and other?</title><p id="P29">Social coordination was operationalized as the spatiotemporal alignment of movements produced by participants (self-generated) and their partners (other-generated). Specifically, this construct assessed whether participants and their partners not only bounced <italic>at the same time</italic> but also <italic>in the same direction</italic>. As such, social coordination relied on both temporal features (velocity magnitude time-series) and spatial features (velocity sign time-series), with the latter indicating the up versus down phases of movement. In contrast, the measures of self and other were derived solely from temporal features. Consequently, it was essential to conduct a control analysis to assess the extent to which social coordination was influenced by the spatial characteristics of both self- and other-generated movements. To address this, we implemented an mTRF analysis utilizing a comprehensive model that incorporated music, self-generated and other-generated movements (velocity magnitude time-series), social coordination, and the spatial directions of both self- and other-generated movements (velocity sign time-series). For this control analysis, we did not include other control variables, such as muscular activity, because the previous analyses already demonstrated that these do not predict social coordination.</p></sec><sec id="S18"><title>Coordination ERPs driven by self or other?</title><p id="P30">In our main ERP analysis, we extracted “coordination ERPs” by epoching EEG time-series at transition onsets between in-phase and anti-phase coordination (see Extraction of ERPs methods described above). These transitions could potentially arise from changes in movement direction elicited by either the self or the partner. To disentangle these two possibilities—specifically, whether ERPs related to social coordination were driven by self-generated movements (self) or by partner-generated movements (other)—we categorized coordination ERPs into two distinct groups: those triggered by self-movements (i.e., when coordination changes were aligned with shifts in the velocity sign of self-generated movements) and those triggered by partner movements (i.e., when coordination changes aligned with shifts in the velocity sign of other-generated movements). We quantified the <italic>in-phase/anti-phase</italic> ERP modulation separately for these two groups, following the methods outlined previously (see previous ERP analyses). Differences between in-phase and anti-phase onsets were assessed independently for the “self” and “other” groups in each experimental condition using a cluster-based permutation test (implemented in FieldTrip with 1000 permutations [<xref ref-type="bibr" rid="R43">Maris and Oostenveld, 2007</xref>]). This analysis focused on the channel of interest informed by the mTRF results: Oz.</p></sec></sec></sec><sec id="S19" sec-type="results"><title>Results</title><sec id="S20"><title>Multivariate temporal response functions (mTRFs)</title><p id="P31">In our analysis, we assessed the unique contributions of four variables to the EEG by comparing the prediction gain (<italic>Δr</italic>) between a full mTRF model and reduced models excluding each variable of interest (see Methods for details). The results showed that musical sounds, self-generated movements, other-generated movements, and social coordination each made distinct contributions to participants’ neural activity. This allowed us to isolate four neural processes co-occurring during dyadic dance: (I) auditory perception of music, (II) control of movement, (III) visual perception of the partner’s body movements, and (IV) visual tracking of social coordination. These processes were clearly distinguished from ocular, facial, and neck muscle artifacts (V, VI, and VII). The following section provides detailed information on each of these EEG activities.</p><sec id="S21"><label>(I)</label><title>Auditory perception of music</title><p id="P32">The spectral flux of the music uniquely predicted EEG across frontal and parietal electrodes, as evidenced by the prediction gain <italic>Δr</italic> (the difference between the prediction of the full model and that of the reduced model excluding spectral flux) at each electrode (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). A repeated-measures ANOVA, with “musical input” and “visual contact” as factors, yielded a main effect of vision, demonstrating a significant reduction in the prediction gain <italic>Δr</italic> when participants could see each other (<italic>F</italic>(1,57) = 7.48, <italic>p</italic> = .033; <xref ref-type="fig" rid="F4">Fig. 4</xref>). This finding suggests a diminished neural tracking of music when participants could see their partners. The regression weights associated with the music TRF model (representing electrode Fz) highlight three post-stimulus modulations, i.e., a positive-negative-positive pattern with peaks at around +60, +120, and +200 ms post-stimulus, respectively (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). The weights also exhibit a consistent peak around -200 ms pre-stimulus, which, considering the periodic rhythmic nature of the music, is likely evoked by the preceding beat sound. We confirmed so by observing that the sound differential intensity (specifically, the spectral flux value) of the previous beat modulated the amplitude of this -200 ms peak.</p></sec><sec id="S22"><label>(II)</label><title>Control of movement (self-generated)</title><p id="P33">Self-generated movements uniquely predicted EEG across central and occipital electrodes, as indicated by the electrode-specific prediction gain <italic>Δr</italic> (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). The ANOVA did not yield evidence of significant effects of musical input or visual contact on the unique contribution of self-generated movements on EEG signals, suggesting comparable motor control processes across conditions (all <italic>ps</italic> &gt; .224; <xref ref-type="fig" rid="F4">Fig. 4</xref>). The TRF weights associated with self-generated movements (representing the average between electrodes C3 and C4) highlighted three main modulations, i.e., a negative-positive-negative pattern with peaks at around -100, 0, and +80 ms relatively to movement onset, respectively (<xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p></sec><sec id="S23"><label>(III)</label><title>Visual perception of partner’s body movements (other-generated)</title><p id="P34">Other-generated movements uniquely predicted EEG across occipital electrodes, surrounding the visual cortex (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), only when participants could see each other. This was confirmed by the ANOVA, yielding a main effect of visual contact (<italic>F</italic>(1,57) = 83.23, <italic>p</italic> &lt; .001; <xref ref-type="fig" rid="F4">Fig. 4</xref>). This finding is consistent with the expectation that neural tracking of others’ movements can only occur when these movements are observable. The TRF weights associated with other-generated movements (representing electrode Oz) highlighted a biphasic modulation characterized by a positive peak at around +70 ms, and a negative peak at around +160 ms relative to movement onset (<xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p></sec><sec id="S24"><label>(IV)</label><title>Social coordination</title><p id="P35">Social coordination uniquely predicted EEG primarily across occipital electrodes (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), especially when participants could see each other and listened to the same music. This was supported by the ANOVA, which yielded main effects of visual contact (<italic>F</italic>(1,57) = 249.75, <italic>p</italic> &lt; .001) and musical input (<italic>F</italic>(1,57) = 30.22, <italic>p</italic> &lt; .001), along with a significant interaction effect (<italic>F</italic>(1,57) = 50.10, <italic>p</italic> &lt; .001) (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Follow-up comparisons revealed that EEG prediction accuracy was specifically enhanced when participants danced to the same music, but only with visual contact (Δ = 0.0009, <italic>SE</italic> = 0.0001, <italic>p</italic> &lt; .001); no music effect was observed without visual contact (<italic>p</italic> = .676) (<xref ref-type="fig" rid="F4">Fig. 4</xref>). This suggests that the level of coordination between participants is encoded in each participant’s EEG, and that neural tracking occurs primarily when partners are visible and synchronizing to the same musical tempo. In this condition, the TRF weights (representing electrode Oz) exhibited a quadriphasic pattern characterized by negative-positive-negative-positive peaks, at -180, -90, +30, and +160 ms relative to a change in coordination (between in-phase and anti-phase – see Methods), respectively (<xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p></sec><sec id="S25"><title>(V, VI, and VII) Ocular, facial, and neck muscle artifacts</title><p id="P36">EOG and EMG signals uniquely predicted EEG at electrode sites that closely matched artifactual topographical maps documented in previous EEG research (<xref ref-type="fig" rid="F3">Fig. 3a</xref>) (<xref ref-type="bibr" rid="R27">Goncharova et al., 2003</xref>; <xref ref-type="bibr" rid="R61">Plöchl et al., 2012</xref>). Specifically, EMG from facial and neck muscles predicted EEG activity at the scalp periphery, which is typical of muscle contraction topographies (<xref ref-type="bibr" rid="R27">Goncharova et al., 2003</xref>), while EOG predicted EEG activity nearby the eyes (electrodes AF7 and AF8, approaching the lateral canthi), characteristic of eye saccades (<xref ref-type="bibr" rid="R61">Plöchl et al., 2012</xref>). Note that most blinks-related artifacts were presumably removed beforehand via ASR and ICA pipelines (see Methods). The TRF weights for eye, facial, and neck movements displayed features of instantaneous impulse responses (<xref ref-type="fig" rid="F3">Fig. 3b</xref>), indicating that non-cerebral signals propagate to the EEG without measurable delay—a characteristic previously established for artifact leakage (<xref ref-type="bibr" rid="R9">Croft and Barry, 2000</xref>). Additionally, these EOG and EMG signals contributed orders of magnitude more to the EEG than brain processes (compare <italic>Δr</italic> scales within <xref ref-type="fig" rid="F3">Fig. 3a</xref>), another expected property of EMG and EOG activations (<xref ref-type="bibr" rid="R75">Urigüen and Garcia-Zapirain, 2015</xref>). Collectively, these findings underscore the efficiency of our analysis in distinguishing simultaneous neurophysiological processes from each other, as well as from movement-related artifact leakage.</p></sec></sec><sec id="S26"><title>Event-related potentials (ERPs)</title><p id="P37">To elucidate the physiological origins of the temporal responses modeled by the mTRFs, we extracted ERPs by epoching the EEG time series around salient changes (see Methods) in music, self-generated movements, other-generated movements, and social coordination. This analysis was specifically designed to clarify the neurophysiological origin of the temporal responses, or model weights, modeled by the mTRFs. Notably, the focus was not on the condition-specific contribution of these responses to the EEG, as ERP analysis cannot fully account for concurrent contributions from other variables. Rather, the results demonstrated that the ERPs exhibited morphologies closely resembling the TRF weights observed earlier (compare <xref ref-type="fig" rid="F5">Fig. 5</xref> with <xref ref-type="fig" rid="F3">Fig. 3b</xref>) and consistent with typical EEG markers of sensory and motor processes established in laboratory-controlled studies. Detailed ERP results for each process are presented in the following sections.</p><sec id="S27"><label>(I)</label><title>Auditory perception of music</title><p id="P38">The individual sounds embedded within the musical tracks elicited a characteristic triphasic ERP response, consisting of an early positivity (P50), followed by a widespread negativity (N100), and a later positivity (P200), all displaying a frontal topographic distribution (<xref ref-type="fig" rid="F5">Fig. 5</xref>). This pattern aligns with established findings from both ERP (<xref ref-type="bibr" rid="R52">Novembre et al., 2018</xref>; <xref ref-type="bibr" rid="R20">Di Liberto et al., 2020</xref>) and TRF studies (<xref ref-type="bibr" rid="R19">Di Liberto et al., 2015</xref>, <xref ref-type="bibr" rid="R20">2020</xref>; <xref ref-type="bibr" rid="R24">Fiedler et al., 2019</xref>; <xref ref-type="bibr" rid="R31">Jessen et al., 2019</xref>; <xref ref-type="bibr" rid="R33">Kern et al., 2022</xref>) in motionless participants, and closely resembles the regression weights of the music TRF observed in our study with dancing participants (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). These similarities suggest that our music TRF primarily captured phase-locked responses evoked by the individual sounds embedded within the musical stimuli, as observed in previous work (<xref ref-type="bibr" rid="R20">Di Liberto et al., 2020</xref>; <xref ref-type="bibr" rid="R4">Bianco et al., 2024</xref>). To validate this assumption, we further report a known physiological property of these responses—ERP amplitude sensitivity to variations in stimulus intensity (<xref ref-type="bibr" rid="R67">Somervail et al., 2021</xref>)—as evidenced by the amplitude of the P200 being larger in response to loud <italic>vs</italic> soft sounds (<xref ref-type="fig" rid="F5">Fig. 5</xref>).</p></sec><sec id="S28"><label>(II)</label><title>Control of movement (self-generated)</title><p id="P39">ERPs time-locked to self-generated movements displayed a triphasic pattern, characterized by a pre-motor negativity (N-100), a positivity at movement onset (P0) and a post-motor negativity (N100), with a central distribution (<xref ref-type="fig" rid="F5">Fig. 5</xref>). These components are reminiscent of movement-related cortical potentials (<xref ref-type="bibr" rid="R65">Shibasaki et al., 1980</xref>; <xref ref-type="bibr" rid="R29">Hallett, 1994</xref>), which might include steady-state movement-evoked potentials (<xref ref-type="bibr" rid="R26">Gerloff et al., 1997</xref>) or readiness potentials (<xref ref-type="bibr" rid="R34">Kornhuber and Deecke, 1965</xref>; <xref ref-type="bibr" rid="R78">Vercillo et al., 2018</xref>) (see also body-part-specific analyses, and <xref ref-type="fig" rid="F6">Fig. 6</xref>). The pattern closely mirrors the regression weights observed in our TRF model of self-generated movements (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Notably, the amplitude of the ERPs associated with self-generated movements was larger during relatively faster, as opposed to relatively slower movements, a pattern previously suggested to reflect increased motor activity during higher-rate movement execution (<xref ref-type="bibr" rid="R7">Brunia et al., 2011</xref>). These results suggest that the mTRF model effectively captured EEG potentials traditionally linked to motor control, with amplitudes modulated by movement speed.</p></sec><sec id="S29"><label>(III)</label><title>Visual perception of partner’s body movements (other-generated)</title><p id="P40">When the participants could see each other, the observed partner-generated movements elicited biphasic responses in occipital regions, characterized by a positive peak at ∼70 ms (P70) and a negative peak at ∼160 ms (N160) (<xref ref-type="fig" rid="F5">Fig. 5</xref>). This pattern resembles traditional visual responses to biological motion, notably characterized by the N170 component, typically observed around 170 ms post-movement onset (<xref ref-type="bibr" rid="R37">Kubová et al., 1995</xref>; <xref ref-type="bibr" rid="R1">Bach and Ullrich, 1997</xref>; <xref ref-type="bibr" rid="R62">Puce et al., 2000</xref>; <xref ref-type="bibr" rid="R32">Jokisch et al., 2005</xref>). Similar to the responses associated with music and self-generated movements, these ERPs closely align with the regression weights yielded by the TRF model of other-generated movements (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). As for ERPs evoked by motor control (previous section), ERP amplitudes scaled with movement speed, most prominently under visual contact in the different-music condition (by contrast, the modulation in the same-music condition was less apparent, likely obscured by concurrent neural processes not considered in the ERP analysis, such as those related to coordination). The increased ERP amplitudes for faster compared to slower movements (<xref ref-type="fig" rid="F5">Fig. 5</xref>) further highlight a well-established physiological property of sensory ERPs: their sensitivity to variations in stimulus intensity (<xref ref-type="bibr" rid="R67">Somervail et al., 2021</xref>).</p></sec><sec id="S30"><label>(IV)</label><title>Social coordination</title><p id="P41">ERPs time-locked to changes in social coordination were associated with quadriphasic EEG modulations in occipital regions across all conditions. However, ERP amplitude differences between changes to in-phase <italic>vs</italic> anti-phase coordination emerged only when participants could see their partners and listened to same-tempo music (<xref ref-type="fig" rid="F5">Fig. 5</xref>). The response pattern appears to bridge motor control and movement observation processes, showing a triphasic N-P-N sequence, similar to self-generated movement ERPs, followed by a positive occipital peak at 160 ms post-onset—resembling the inverted polarity of the posterior N160 observed for other-generated movements. The presence of a clear pattern in non-visual conditions suggests that these ERPs partially reflect motor activity, as changes in coordination coincide with movement initiation by either the self or the partner, a confound that traditional ERP analysis fails to fully resolve, unlike TRF analysis. Supporting this interpretation, ERP amplitude in non-visual conditions did not vary between changes to in-phase and anti-phase (<xref ref-type="fig" rid="F5">Fig. 5</xref>), and the TRFs—designed to disentangle concurrent processes—did not reveal any EEG activity related to coordination, beyond motor activity, in the non-visual conditions (<xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p></sec></sec><sec id="S31"><title>mTRFs tease apart body-part-specific motor activity</title><p id="P42">Thus far, the EEG activity related to self-generated movements was extracted using the velocity time-series of bounce movements (see Methods), either to predict EEG signal (mTRF analysis) or to time-lock EEG epochs (ERP analysis). To determine whether specific body parts contribute to distinct motor activities, we performed an additional TRF analysis using velocity time series associated with five distinct body parts: left and right hands, left and right feet, and head. Rather than relying on principal movements extracted from PCA, we modeled EEG signals using the kinematics of these specific body parts as input variables in the TRF models (<xref ref-type="fig" rid="F6">Fig. 6</xref>). The unique contributions of the left and right hands (beyond that of all other body parts) to the EEG prediction exhibited lateralized spatial maps at central sites (<xref ref-type="fig" rid="F6">Fig. 6a</xref>), a typical marker of hands’ motor control (<xref ref-type="bibr" rid="R34">Kornhuber and Deecke, 1965</xref>; <xref ref-type="bibr" rid="R14">Deecke et al., 1969</xref>; <xref ref-type="bibr" rid="R65">Shibasaki et al., 1980</xref>; <xref ref-type="bibr" rid="R26">Gerloff et al., 1997</xref>; <xref ref-type="bibr" rid="R66">Smulders and Miller, 2011</xref>; <xref ref-type="bibr" rid="R78">Vercillo et al., 2018</xref>; <xref ref-type="bibr" rid="R54">O’Neill et al., 2024</xref>). Furthermore, feet movements exhibited a more posterior topographical activation than hands (<xref ref-type="fig" rid="F6">Fig. 6b</xref>), reminiscent of EEG differences found when comparing motor activity across hands and feet (<xref ref-type="bibr" rid="R7">Brunia et al., 2011</xref>). Notably, these feet-related EEG activities showed no clear lateralization, which is expected given the organization of the motor cortex (<xref ref-type="bibr" rid="R28">Gordon et al., 2023</xref>; <xref ref-type="bibr" rid="R54">O’Neill et al., 2024</xref>) and the limited spatial resolution of EEG (<xref ref-type="bibr" rid="R57">Osman et al., 2005</xref>). Indeed, as the feet are represented in the deeper, more central regions of the motor cortex, along the inner surface of the longitudinal fissure, it is notoriously difficult to differentiate EEG activity evoked by left vs. right foot movements (<xref ref-type="bibr" rid="R57">Osman et al., 2005</xref>; <xref ref-type="bibr" rid="R30">Jensen et al., 2023</xref>). Finally, head movements were associated with EEG activity not only in motor sites, such as C3 and C4 electrodes but also in occipital regions (<xref ref-type="fig" rid="F6">Fig. 6c</xref>). Importantly, this occipital activation did not result from neck muscle artifact leakage, as neck EMG’s contribution was already accounted for in the full model (see Methods). Moreover, no such occipital activation was found for hand or foot movements, suggesting that head movements specifically involve visual (besides motor) processing. Visual processes could be at play when moving the head (e.g., bouncing or head bobbing) as this involves salient changes in the field of view. Taken together, these findings support the conclusion that our TRF and ERP analyses (see <xref ref-type="fig" rid="F3">Figs. 3</xref> and <xref ref-type="fig" rid="F5">5</xref>) efficiently isolated neural processes related to self-generated movements. Moreover, beyond validating these prior results, this new analysis demonstrates the feasibility of isolating motor activity of specific body parts (note that in the prior analyses, motor activity related to bounce [involving all body parts] was assessed, limiting visibility into body-part-specific motor activity).</p></sec><sec id="S32"><title>Social coordination encoding acts beyond self and other</title><sec id="S33"><title>Coordination beyond self and other</title><p id="P43">In previous analyses, we demonstrated that adding the social coordination variable to models including music, self-, and other-generated movements yielded a gain in EEG prediction, suggesting neural encoding of coordination (<xref ref-type="fig" rid="F3">Figs. 3</xref> and <xref ref-type="fig" rid="F4">4</xref>). To ensure this gain was not solely attributed to the inclusion of spatial direction features – i.e., up versus down phases of bounce movement inherent in the social coordination variable but absent in the self- and other-generated movement variables – we conducted a supplementary mTRF analysis that included the spatial directions of both self- and other-generated movements. This analysis yielded cross-condition differences in unique contributions (i.e., <italic>Δr</italic>) that were identical to those observed in our primary analysis (compare <xref ref-type="fig" rid="F7">Fig. 7a</xref>, top, with <xref ref-type="fig" rid="F3">Figs. 3a</xref> and 4), along with consistent model weights (compare <xref ref-type="fig" rid="F7">Fig. 7a</xref>, bottom, with <xref ref-type="fig" rid="F3">Fig. 3b</xref>). These findings indicate that the encoding of social coordination extends beyond merely representing the spatial directions of self- and other-generated movements in isolation. These results further suggest that the encoding of coordination is not merely driven by a modulation of the partner-evoked visual processes as a function of whether the self is moving congruently or not congruently with the partner (hence suppressing or amplifying the observed movements relative to the field of view). To further show that the encoding of coordination was not solely capturing this, we conducted an additional control analysis for which we re-referenced the other-generated movements to the position of the self. Even following such re-referencing, social coordination yielded a significant and unique contribution to EEG recorded from occipital sites, and this contribution was most pronounced under conditions of visual contact and shared music. Taken together, these findings indicate that the reported encoding of coordination is linked to a high-order process tracking the alignment between self- and partner-related movements, independently of the encoding of self and other taken alone.</p></sec><sec id="S34"><title>Coordination ERPs are time-locked to other-generated movements</title><p id="P44">“Coordination ERPs” were extracted by epoching EEG time-series to shifts from anti-phase to in-phase coordination and, vice versa, from in-phase to anti-phase. Here we investigated how such ERPs changed as a function of whether the shifts were driven by changes in movement direction produced by either the self (movement production) or the other (movement observation) (see Methods). Our analysis indicated that the EEG modulations previously associated with changes to in-phase coordination, specifically observed at occipital sites (Oz) and specifically under conditions of visual contact and same-tempo music (<xref ref-type="fig" rid="F5">Fig. 5</xref>), were present only when these changes were time-locked to other-generated movement changes (<xref ref-type="fig" rid="F7">Fig. 7b</xref>). This indicates that larger amplitude ERPs are evoked when a partner initiates a change in movement direction that leads to in-phase coordination compared to a change in movement direction that leads to anti-phase coordination. This result further strengthens the conclusion that the brain encodes social coordination and that this encoding is specifically driven by movements of the partner being in phase vs. anti-phase concerning self-initiated movements.</p></sec></sec></sec><sec id="S35" sec-type="discussion"><title>Discussion</title><p id="P45">This study demonstrates that the neural processes underlying dance—a complex, natural, and social behavior—can be effectively isolated from EEG signals recorded from dyads dancing together. Using multivariate TRF models applied to dual EEG and full-body kinematics, we disentangled intertwined neural processes, separated them from movement artifacts, and confirmed their physiological origins through ERP analyses. This approach delineated sensory and motor processes underlying free-form, naturalistic dance: (I) auditory tracking of music, (II) control of self-generated movements, and (III) visual monitoring of partner-generated movements. Crucially, we also uncovered a previously unknown neural marker of social processing: (IV) visual encoding of social coordination, which emerges only when partners can make visual contact, is topographically distributed over the occipital areas, and is driven by movement observation rather than initiation. Additionally, movement-specific models highlighted “bounce” as the primary dance move driving EEG activity associated with both self-generated movements and movements observed in the partner. Together, these findings illustrate how advanced neural analysis techniques can illuminate the mechanisms supporting complex natural behaviors.</p><sec id="S36"><title>mTRFs can unravel the complex orchestration of natural behavior</title><p id="P46">Recent advancements in mobile imaging and de-noising techniques have enhanced our ability to study neural activity during real-world behavior (<xref ref-type="bibr" rid="R3">Bateson et al., 2017</xref>; <xref ref-type="bibr" rid="R51">Niso et al., 2023</xref>). However, disentangling the contribution of the multiple simultaneous neural processes remains challenging. In this study, we addressed this challenge within the context of a spontaneous, interactive, yet controlled task, balancing ecological validity with experimental control (<xref ref-type="bibr" rid="R13">D’Ausilio et al., 2015</xref>). Using mTRFs, we successfully isolated four distinct, yet overlapping, neural processes underlying dyadic dance. ERP analyses confirmed that mTRF modeled responses, or model weights, align with well-characterized EEG potentials linked to sensory perception and motor control. This result suggests that mTRFs can capture physiologically established signals, akin to ERP analyses, but in real-world scenarios with multiple concurrent activities—contexts where traditional ERP approaches fall short.</p><p id="P47">ERP analyses struggle to isolate the unique contributions of individual processes amidst overlapping neural activities. This limitation is evident in our results: visual ERP modulation to movement speed was weak under visual contact and same-music conditions (<xref ref-type="fig" rid="F5">Fig. 5</xref>), while mTRFs captured robust visual tracking (<xref ref-type="fig" rid="F3">Figs. 3</xref> and <xref ref-type="fig" rid="F4">4</xref>). This discrepancy likely stems from social coordination activity, which ERP analysis cannot disentangle, and which was particularly prominent in these conditions. Similarly, coordination ERPs appeared in non-visual conditions (<xref ref-type="fig" rid="F5">Fig. 5</xref>), whereas mTRFs showed no corresponding activity, likely reflecting unaccounted self-motor contributions in ERP analyses. Although techniques like frequency tagging have addressed some of these challenges (<xref ref-type="bibr" rid="R76">Varlet et al., 2020</xref>, <xref ref-type="bibr" rid="R77">2023</xref>; <xref ref-type="bibr" rid="R8">Cracco et al., 2022</xref>), they are limited to identifying periodic EEG responses and typically focus on univariate kinematics (e.g., gait cycles or hand trajectories). In contrast, mTRFs offer a precise characterization of neural responses to diverse features, effectively separating them from concurrent activities.</p></sec><sec id="S37"><title>The interplay between music and partner tracking</title><p id="P48">Dyadic dance requires simultaneous sensory processing of music and a partner’s movements, both of which contribute to coordinated behavior (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>). To what extent do these concurrent streams of information influence EEG activity, and how are these effects modulated by social factors like visual contact? Our findings show that model weights and ERPs associated with music, partner movements, and coordination exhibit similar amplitude, suggesting that each element—whether a musical sound, observed movement, or change in coordination—elicits an EEG response of comparable magnitude. Notably, visual processes accounted for less variance at occipital sites than auditory processes at frontal sites (see <italic>Δr</italic> scales in <xref ref-type="fig" rid="F3">Fig. 3a</xref>). This may reflect the broader range of EEG signals in occipital regions, which likely include visual processing of not only partner movements but also other visual cues and, importantly, artifactual leakage from neck movements (see <xref ref-type="fig" rid="F3">Fig. 3a</xref>).</p><p id="P49">In visual-contact conditions, where both music (acoustic) and partner (visual) information were present, we observed a decrease in music tracking (<xref ref-type="fig" rid="F4">Fig. 4</xref>). This reduction may arise from competition between visual and auditory modalities for attentional resources (<xref ref-type="bibr" rid="R82">Woods et al., 1992</xref>; <xref ref-type="bibr" rid="R40">Lavie, 2005</xref>; <xref ref-type="bibr" rid="R45">Molloy et al., 2015</xref>), especially in naturalistic dance, where both auditory and visual inputs drive coordination (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>). Naturalistic dance likely places heightened demands on visual input, as recent findings suggest that visual drivers dominate full-body rhythmic synchronization—a phenomenon not observed in simpler tasks like finger tapping (<xref ref-type="bibr" rid="R48">Nguyen et al., 2024</xref>).</p></sec><sec id="S38"><title>Movement control and observation</title><p id="P50">Our principal component analysis of dance kinematics revealed that bounce movements accounted for most EEG activity associated with self-generated movements (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Intriguingly, these movements predicted EEG activity not only over motor areas (e.g., electrodes C3 and C4), but also at occipital sites (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). To better understand these activities, we further dissected the components of bounce control, pinpointing motor activity specific to different body parts (<xref ref-type="fig" rid="F6">Fig. 6</xref>). In participants engaged in free-form dancing, we successfully replicated established EEG findings observed during isolated movements, with more posterior-medial activity associated with foot movements and more central-lateralized activity for hand movements (<xref ref-type="bibr" rid="R7">Brunia et al., 2011</xref>). Notably, our analysis showed that head displacement was linked to occipital brain activity in addition to central motor activity, likely due to visual responses resulting from changes in the visual field (<xref ref-type="bibr" rid="R70">Testard et al., 2024</xref>). This analysis clarifies why the main mTRF for self-generated bounce movements included activity at occipital sites (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), suggesting that bouncing not only involves motor activity but also induces significant visual changes.</p><p id="P51">Bounce also emerged as the movement most predictive of EEG activity linked to visual tracking of a partner’s movements. This finding raises an intriguing question: what makes bounce particularly captivating compared to other dance movements? Our previous research has highlighted bounce’s key role in fostering interpersonal coordination (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>), serving as a supramodal (audio-visual) pace-setter between participants and their partners. This may explain why bounce is so prominent in predicting EEG activity associated with movement observation. This finding also suggests that EEG signals are particularly sensitive to salient movement changes, rather than merely high-amplitude movements. Indeed, while bounce explained less than 1% of the total kinematic variance (ranking 10th in the PCA), it accounted for over 80% of the EEG variance. This likely reflects bounce’s heightened salience, possibly driven by the fact that this movement was the only one peaking sharply with each musical beat (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>).</p></sec><sec id="S39"><title>Encoding of social coordination</title><p id="P52">Our study reveals that coordination between self- and other-generated movements uniquely predicts EEG signals recorded at occipital electrodes. Recent research in social neuroscience has shown that EEG can delineate separate components supporting coordinated behaviors: some monitor self- and partner-generated actions distinctly, while others integrate the joint action outcome produced by oneself and the partner (<xref ref-type="bibr" rid="R53">Novembre et al., 2016</xref>; <xref ref-type="bibr" rid="R76">Varlet et al., 2020</xref>). In line with this, we identified three distinct neural processes—control of one’s own movements, observation of a partner’s movements, and processing of social coordination (<xref ref-type="fig" rid="F3">Fig. 3</xref>)—and observed heightened coordination tracking in conditions where musical synchrony between participants was greater. Importantly, our findings suggest that the encoding of coordination does not merely combine the individual “self” and “other” processes; rather, it captures a distinct neural representation of their coordination (see Results and <xref ref-type="fig" rid="F7">Fig. 7a</xref>).</p><p id="P53">The temporal response underlying social coordination tracking integrates both motor control (self) and movement observation (other), as evidenced by the N-P-N pattern and a subsequent modulation peaking around 160 ms. Notably, this response appears to be triggered by observing a partner’s movements, rather than initiating one’s own actions. ERPs associated with changes in the partner’s movements—rather than self-initiated actions—were modulated by social coordination at visual sites (see Results and <xref ref-type="fig" rid="F7">Fig. 7b</xref>). This finding suggests that neural tracking of coordination is more reliant on visual monitoring of the partner than on the internal control of one’s own movements, aligning with earlier observations that this process is localized in visual areas and enhanced during visual contact.</p><p id="P54">In summary, we identified a previously unknown neural marker of social processing, with five key observations: 1) it is topographically distributed over the occipital areas; 2) it emerges when participants can see each other and is most pronounced when musical synchrony between them is high; 3) its underlying neural signal integrates components from both self and other processes; yet 4) rather than merely combining the individual “self” and “other” components, it represents a distinct neural encoding of their coordination; and 5) it is primarily anchored to movement observation, not movement initiation.</p></sec><sec id="S40"><title>Bridging traditional physiology with real-world applications</title><p id="P55">Our findings show that neurophysiological signals, traditionally examined in controlled settings, can be disentangled and analyzed within real-world contexts. This highlights the potential for future research to incorporate ecologically valid stimuli and behavioral predictors (e.g., body movements, eye gaze, speech) into multivariate modeling. Such an approach could deepen our understanding of brain processes during live social interactions—a field of growing significance across human adult (<xref ref-type="bibr" rid="R21">Dumas et al., 2010</xref>; <xref ref-type="bibr" rid="R59">Pan et al., 2018</xref>; <xref ref-type="bibr" rid="R36">Koul et al., 2023</xref>; <xref ref-type="bibr" rid="R10">Cross et al., 2024</xref>; <xref ref-type="bibr" rid="R56">Orgs et al., 2024</xref>), developmental (<xref ref-type="bibr" rid="R79">Wass et al., 2018</xref>, <xref ref-type="bibr" rid="R80">2020</xref>; <xref ref-type="bibr" rid="R47">Nguyen et al., 2020</xref>, <xref ref-type="bibr" rid="R50">2021</xref>, <xref ref-type="bibr" rid="R49">2023</xref>) and animal studies (<xref ref-type="bibr" rid="R85">Zhang and Yartsev, 2019</xref>; <xref ref-type="bibr" rid="R64">Rose et al., 2021</xref>; <xref ref-type="bibr" rid="R84">Yang et al., 2021</xref>).</p></sec></sec></body><back><ack id="S41"><title>Acknowledgments</title><p>F.B., S.F.A., and G.N. are supported by the European Research Council (ERC, MUSICOM, 948186). R.B. is supported by the European Union (MSCA, PHYLOMUSIC, 101064334). T.N. is supported by the European Union (MSCA, SYNCON, 101105726). We thank Alison Rigby for her help with data collection, and Raoul Tchoï for his help creating the stimuli.</p></ack><sec id="S42" sec-type="data-availability"><title>Code accessibility</title><p id="P56">All original code is publicly available on Github repositories: <ext-link ext-link-type="uri" xlink:href="https://github.com/felixbgd/dancing_brain">https://github.com/felixbgd/dancing_brain</ext-link></p></sec><sec id="S43" sec-type="data-availability"><title>Data accessibility</title><p id="P57">EEG, EMG, EOG, kinematic, and musical data have been deposited to IIT Dataverse and are publicly available as of the date of publication:</p><p id="P58"><ext-link ext-link-type="uri" xlink:href="https://dataverse.iit.it/privateurl.xhtml?token=bb0689c8-137d-4742-9f94-d7c6b0148827">https://dataverse.iit.it/privateurl.xhtml?token=bb0689c8-137d-4742-9f94-d7c6b0148827</ext-link></p></sec><fn-group><fn id="FN1"><p id="P59"><bold><underline>Multimedia</underline></bold></p><p id="P60"><bold>Video 1. The principal (dance) movements, related to <xref ref-type="fig" rid="F2">Fig. 2</xref></bold></p><p id="P61">Video showing original movement data (left) and their decomposition into 15 principal movements (PMs) explaining &gt;95% of the kinematics variance (right). Representative data are displayed (excerpt from a single trial, corresponding to when participants listened to the full refrain of the song). For the sake of clarity, the PMs are animated with different levels of exaggeration (i.e. the PM scores were amplified by a factor of 1.5 (PM3), 2 (PM4,7,9,11,15), 3 (PM8), or not amplified (all other PMs)). The PMs are reminiscent of common dance moves (spelled out in italics).</p></fn><fn fn-type="conflict" id="FN2"><p id="P62">Conflict of interest: The authors declare no conflict of interest.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>M</given-names></name><name><surname>Ullrich</surname><given-names>D</given-names></name></person-group><article-title>Contrast dependency of motion-onset and pattern-reversal VEPs: Interaction of stimulus type, recording site and response component</article-title><source>Vision Research</source><year>1997</year><volume>37</volume><fpage>1845</fpage><lpage>1849</lpage><pub-id pub-id-type="pmid">9274769</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barraza</surname><given-names>P</given-names></name><name><surname>Dumas</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Blanco-Gomez</surname><given-names>G</given-names></name><name><surname>van den Heuvel</surname><given-names>MI</given-names></name><name><surname>Baart</surname><given-names>M</given-names></name><name><surname>Pérez</surname><given-names>A</given-names></name></person-group><article-title>Implementing EEG hyperscanning setups</article-title><source>MethodsX</source><year>2019</year><volume>6</volume><fpage>428</fpage><lpage>436</lpage><pub-id pub-id-type="pmcid">PMC6411510</pub-id><pub-id pub-id-type="pmid">30906698</pub-id><pub-id pub-id-type="doi">10.1016/j.mex.2019.02.021</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bateson</surname><given-names>AD</given-names></name><name><surname>Baseler</surname><given-names>HA</given-names></name><name><surname>Paulson</surname><given-names>KS</given-names></name><name><surname>Ahmed</surname><given-names>F</given-names></name><name><surname>Asghar</surname><given-names>AUR</given-names></name></person-group><article-title>Categorisation of Mobile EEG: A Researcher’s Perspective</article-title><source>BioMed Research International</source><year>2017</year><volume>2017</volume><elocation-id>5496196</elocation-id><pub-id pub-id-type="pmcid">PMC5733835</pub-id><pub-id pub-id-type="pmid">29349078</pub-id><pub-id pub-id-type="doi">10.1155/2017/5496196</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Bigand</surname><given-names>F</given-names></name><name><surname>Quarta</surname><given-names>E</given-names></name><name><surname>Grasso</surname><given-names>S</given-names></name><name><surname>Arnese</surname><given-names>F</given-names></name><name><surname>Ravignani</surname><given-names>A</given-names></name><name><surname>Battaglia-Mayer</surname><given-names>A</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name></person-group><article-title>Neural encoding of musical expectations in a non-human primate</article-title><source>Current Biology</source><year>2024</year><volume>34</volume><fpage>444</fpage><lpage>450</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmid">38176416</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigand</surname><given-names>F</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Abalde</surname><given-names>SF</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name></person-group><article-title>The geometry of interpersonal synchrony in human dance</article-title><source>Current Biology</source><year>2024</year><date-in-citation>Accessed June 25, 2024</date-in-citation><comment>0 Available at: <ext-link ext-link-type="uri" xlink:href="https://www.cell.com/current-biology/abstract/S0960-9822(24)00698-5">https://www.cell.com/current-biology/abstract/S0960-9822(24)00698-5</ext-link></comment><pub-id pub-id-type="pmcid">PMC11266842</pub-id><pub-id pub-id-type="pmid">38908371</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2024.05.055</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bigand</surname><given-names>F</given-names></name><name><surname>Prigent</surname><given-names>E</given-names></name><name><surname>Berret</surname><given-names>B</given-names></name><name><surname>Braffort</surname><given-names>A</given-names></name></person-group><article-title>Decomposing spontaneous sign language into elementary movements: A principal component analysis-based approach</article-title><source>PLOS ONE</source><year>2021</year><volume>16</volume><elocation-id>e0259464</elocation-id><pub-id pub-id-type="pmcid">PMC8555838</pub-id><pub-id pub-id-type="pmid">34714862</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0259464</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunia</surname><given-names>CHM</given-names></name><name><surname>van Boxtel</surname><given-names>GJM</given-names></name><name><surname>Böcker</surname><given-names>KBE</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kappenman</surname><given-names>ES</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><chapter-title>Negative Slow Waves as Indices of Anticipation: The Bereitschaftspotential, the Contingent Negative Variation, and the Stimulus-Preceding Negativity</chapter-title><source>The Oxford Handbook of Event-Related Potential Components</source><publisher-name>Oxford University Press</publisher-name><year>2011</year><date-in-citation>Accessed May 16, 2024</date-in-citation><comment>pp 0</comment><pub-id pub-id-type="doi">10.1093/oxfordhb/9780195374148.013.0108</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>van Belle</surname><given-names>G</given-names></name><name><surname>Quenon</surname><given-names>L</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Orgs</surname><given-names>G</given-names></name></person-group><article-title>EEG Frequency Tagging Reveals the Integration of Form and Motion Cues into the Perception of Group Movement</article-title><source>Cerebral Cortex</source><year>2022</year><volume>32</volume><fpage>2843</fpage><lpage>2857</lpage><pub-id pub-id-type="pmcid">PMC9247417</pub-id><pub-id pub-id-type="pmid">34734972</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab385</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Croft</surname><given-names>RJ</given-names></name><name><surname>Barry</surname><given-names>RJ</given-names></name></person-group><article-title>Removal of ocular artifact from the EEG: a review</article-title><source>Neurophysiologie Clinique/Clinical Neurophysiology</source><year>2000</year><volume>30</volume><fpage>5</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">10740792</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>ES</given-names></name><name><surname>Darda</surname><given-names>KM</given-names></name><name><surname>Moffat</surname><given-names>R</given-names></name><name><surname>Muñoz</surname><given-names>L</given-names></name><name><surname>Humphries</surname><given-names>S</given-names></name><name><surname>Kirsch</surname><given-names>LP</given-names></name></person-group><article-title>Mutual gaze and movement synchrony boost observers’ enjoyment and perception of togetherness when watching dance duets</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><elocation-id>24004</elocation-id><pub-id pub-id-type="pmcid">PMC11473960</pub-id><pub-id pub-id-type="pmid">39402066</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-72659-7</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title><source>Front Hum Neurosci</source><year>2016</year><volume>10</volume><date-in-citation>Accessed May 16, 2024</date-in-citation><pub-id pub-id-type="pmcid">PMC5127806</pub-id><pub-id pub-id-type="pmid">27965557</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daffertshofer</surname><given-names>A</given-names></name><name><surname>Lamoth</surname><given-names>CJC</given-names></name><name><surname>Meijer</surname><given-names>OG</given-names></name><name><surname>Beek</surname><given-names>PJ</given-names></name></person-group><article-title>PCA in studying coordination and variability: a tutorial</article-title><source>Clinical Biomechanics</source><year>2004</year><volume>19</volume><fpage>415</fpage><lpage>428</lpage><pub-id pub-id-type="pmid">15109763</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Ausilio</surname><given-names>A</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name><name><surname>Fadiga</surname><given-names>L</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name></person-group><article-title>What can music tell us about social interaction?</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><fpage>111</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">25641075</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deecke</surname><given-names>L</given-names></name><name><surname>Scheid</surname><given-names>P</given-names></name><name><surname>Kornhuber</surname><given-names>HH</given-names></name></person-group><article-title>Distribution of readiness potential, pre-motion positivity, and motor potential of the human cerebral cortex preceding voluntary finger movements</article-title><source>Exp Brain Res</source><year>1969</year><volume>7</volume><fpage>158</fpage><lpage>168</lpage><pub-id pub-id-type="pmid">5799432</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><year>2004</year><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desai</surname><given-names>M</given-names></name><name><surname>Field</surname><given-names>AM</given-names></name><name><surname>Hamilton</surname><given-names>LS</given-names></name></person-group><article-title>A comparison of EEG encoding models using audiovisual stimuli and their unimodal counterparts</article-title><source>PLOS Computational Biology</source><year>2024</year><volume>20</volume><elocation-id>e1012433</elocation-id><pub-id pub-id-type="pmcid">PMC11412666</pub-id><pub-id pub-id-type="pmid">39250485</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1012433</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Barsotti</surname><given-names>M</given-names></name><name><surname>Vecchiato</surname><given-names>G</given-names></name><name><surname>Ambeck-Madsen</surname><given-names>J</given-names></name><name><surname>Del Vecchio</surname><given-names>M</given-names></name><name><surname>Avanzini</surname><given-names>P</given-names></name><name><surname>Ascari</surname><given-names>L</given-names></name></person-group><article-title>Robust anticipation of continuous steering actions from electroencephalographic data during simulated driving</article-title><source>Sci Rep</source><year>2021</year><volume>11</volume><elocation-id>23383</elocation-id><pub-id pub-id-type="pmcid">PMC8642531</pub-id><pub-id pub-id-type="pmid">34862442</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-02750-w</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Indexing cortical entrainment to natural speech at the phonemic level: Methodological considerations for applied research</article-title><source>Hearing Research</source><year>2017</year><volume>348</volume><fpage>70</fpage><lpage>77</lpage><pub-id pub-id-type="pmid">28246030</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="pmcid">PMC7053998</pub-id><pub-id pub-id-type="pmid">32122465</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumas</surname><given-names>G</given-names></name><name><surname>Nadel</surname><given-names>J</given-names></name><name><surname>Soussignan</surname><given-names>R</given-names></name><name><surname>Martinerie</surname><given-names>J</given-names></name><name><surname>Garnero</surname><given-names>L</given-names></name></person-group><article-title>Inter-Brain Synchronization during Social Interaction</article-title><source>PLOS ONE</source><year>2010</year><volume>5</volume><elocation-id>e12166</elocation-id><pub-id pub-id-type="pmcid">PMC2923151</pub-id><pub-id pub-id-type="pmid">20808907</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0012166</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunbar</surname><given-names>RI</given-names></name></person-group><article-title>On the evolutionary function of song and dance</article-title><source>Music, language, and human evolution</source><year>2012</year><fpage>201</fpage><lpage>214</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federolf</surname><given-names>P</given-names></name><name><surname>Reid</surname><given-names>R</given-names></name><name><surname>Gilgien</surname><given-names>M</given-names></name><name><surname>Haugen</surname><given-names>P</given-names></name><name><surname>Smith</surname><given-names>G</given-names></name></person-group><article-title>The application of principal component analysis to quantify technique in sports</article-title><source>Scandinavian Journal of Medicine &amp; Science in Sports</source><year>2014</year><volume>24</volume><fpage>491</fpage><lpage>499</lpage><pub-id pub-id-type="pmid">22436088</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiedler</surname><given-names>L</given-names></name><name><surname>Wöstmann</surname><given-names>M</given-names></name><name><surname>Herbst</surname><given-names>SK</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Late cortical tracking of ignored speech facilitates neural selectivity in acoustically challenging conditions</article-title><source>NeuroImage</source><year>2019</year><volume>186</volume><fpage>33</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">30367953</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster Vander Elst</surname><given-names>O</given-names></name><name><surname>Foster</surname><given-names>NHD</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name></person-group><article-title>The Neuroscience of Dance: A Conceptual Framework and Systematic Review</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2023</year><volume>150</volume><elocation-id>105197</elocation-id><pub-id pub-id-type="pmid">37100162</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerloff</surname><given-names>C</given-names></name><name><surname>Toro</surname><given-names>C</given-names></name><name><surname>Uenishi</surname><given-names>N</given-names></name><name><surname>Cohen</surname><given-names>LG</given-names></name><name><surname>Leocani</surname><given-names>L</given-names></name><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><article-title>Steady-state movement-related cortical potentials: a new approach to assessing cortical activity associated with fast repetitive finger movements</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1997</year><volume>102</volume><fpage>106</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">9060861</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goncharova</surname><given-names>II</given-names></name><name><surname>McFarland</surname><given-names>DJ</given-names></name><name><surname>Vaughan</surname><given-names>TM</given-names></name><name><surname>Wolpaw</surname><given-names>JR</given-names></name></person-group><article-title>EMG contamination of EEG: spectral and topographical characteristics</article-title><source>Clinical Neurophysiology</source><year>2003</year><volume>114</volume><fpage>1580</fpage><lpage>1593</lpage><pub-id pub-id-type="pmid">12948787</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>EM</given-names></name><etal/></person-group><article-title>A somato-cognitive action network alternates with effector regions in motor cortex</article-title><source>Nature</source><year>2023</year><volume>617</volume><fpage>351</fpage><lpage>359</lpage><pub-id pub-id-type="pmcid">PMC10172144</pub-id><pub-id pub-id-type="pmid">37076628</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-05964-2</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hallett</surname><given-names>M</given-names></name></person-group><article-title>Movement-related cortical potentials</article-title><source>Electromyogr Clin Neurophysiol</source><year>1994</year><volume>34</volume><fpage>5</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">8168458</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>MA</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Valencia</surname><given-names>GO</given-names></name><name><surname>Klassen</surname><given-names>BT</given-names></name><name><surname>van den Boom</surname><given-names>MA</given-names></name><name><surname>Kaufmann</surname><given-names>TJ</given-names></name><name><surname>Schalk</surname><given-names>G</given-names></name><name><surname>Brunner</surname><given-names>P</given-names></name><name><surname>Worrell</surname><given-names>GA</given-names></name><name><surname>Hermes</surname><given-names>D</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name></person-group><article-title>A motor association area in the depths of the central sulcus</article-title><source>Nat Neurosci</source><year>2023</year><volume>26</volume><fpage>1165</fpage><lpage>1169</lpage><pub-id pub-id-type="pmcid">PMC10322697</pub-id><pub-id pub-id-type="pmid">37202552</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01346-z</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessen</surname><given-names>S</given-names></name><name><surname>Fiedler</surname><given-names>L</given-names></name><name><surname>Münte</surname><given-names>TF</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Quantifying the individual auditory and visual brain response in 7-month-old infants watching a brief cartoon movie</article-title><source>NeuroImage</source><year>2019</year><volume>202</volume><elocation-id>116060</elocation-id><pub-id pub-id-type="pmid">31362048</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jokisch</surname><given-names>D</given-names></name><name><surname>Daum</surname><given-names>I</given-names></name><name><surname>Suchan</surname><given-names>B</given-names></name><name><surname>Troje</surname><given-names>NF</given-names></name></person-group><article-title>Structural encoding and recognition of biological motion: evidence from event-related potentials and source analysis</article-title><source>Behavioural Brain Research</source><year>2005</year><volume>157</volume><fpage>195</fpage><lpage>204</lpage><pub-id pub-id-type="pmid">15639170</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kern</surname><given-names>P</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Doelling</surname><given-names>K</given-names></name></person-group><article-title>Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e80935</elocation-id><pub-id pub-id-type="pmcid">PMC9836393</pub-id><pub-id pub-id-type="pmid">36562532</pub-id><pub-id pub-id-type="doi">10.7554/eLife.80935</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornhuber</surname><given-names>HH</given-names></name><name><surname>Deecke</surname><given-names>L</given-names></name></person-group><article-title>Hirnpotentialänderungen bei Willkürbewegungen und passiven Bewegungen des Menschen: Bereitschaftspotential und reafferente Potentiale</article-title><source>Pflügers Arch</source><year>1965</year><volume>284</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">14341490</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="patent"><person-group person-group-type="author"><name><surname>Kothe</surname><given-names>CAE</given-names></name><name><surname>Jung</surname><given-names>T-P</given-names></name></person-group><source>Artifact removal techniques with signal reconstruction</source><year>2016</year><date-in-citation>Accessed May 16, 2024</date-in-citation><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://patents.google.com/patent/US20160113587A1/en">https://patents.google.com/patent/US20160113587A1/en</ext-link></comment></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koul</surname><given-names>A</given-names></name><name><surname>Ahmar</surname><given-names>D</given-names></name><name><surname>Iannetti</surname><given-names>GD</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name></person-group><article-title>Spontaneous dyadic behavior predicts the emergence of interpersonal neural synchrony</article-title><source>NeuroImage</source><year>2023</year><volume>277</volume><elocation-id>120233</elocation-id><pub-id pub-id-type="pmid">37348621</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubová</surname><given-names>Z</given-names></name><name><surname>Kuba</surname><given-names>M</given-names></name><name><surname>Spekreijse</surname><given-names>H</given-names></name><name><surname>Blakemore</surname><given-names>C</given-names></name></person-group><article-title>Contrast dependence of motion-onset and pattern-reversal evoked potentials</article-title><source>Vision Research</source><year>1995</year><volume>35</volume><fpage>197</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">7839616</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Power</surname><given-names>AJ</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Foxe</surname><given-names>JJ</given-names></name></person-group><article-title>Resolving Precise Temporal Processing Properties of the Auditory System Using Continuous Stimuli</article-title><source>Journal of Neurophysiology</source><year>2009</year><volume>102</volume><fpage>349</fpage><lpage>359</lpage><pub-id pub-id-type="pmid">19439675</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanzarini</surname><given-names>F</given-names></name><name><surname>Maranesi</surname><given-names>M</given-names></name><name><surname>Rondoni</surname><given-names>EH</given-names></name><name><surname>Albertini</surname><given-names>D</given-names></name><name><surname>Ferretti</surname><given-names>E</given-names></name><name><surname>Lanzilotto</surname><given-names>M</given-names></name><name><surname>Micera</surname><given-names>S</given-names></name><name><surname>Mazzoni</surname><given-names>A</given-names></name><name><surname>Bonini</surname><given-names>L</given-names></name></person-group><article-title>Neuroethology of natural actions in freely moving monkeys</article-title><source>Science</source><year>2025</year><volume>387</volume><fpage>214</fpage><lpage>220</lpage><pub-id pub-id-type="pmid">39787237</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavie</surname><given-names>N</given-names></name></person-group><article-title>Distracted and confused?: Selective attention under load</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="pmid">15668100</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><chapter-title>An Introduction to the Event-Related Potential Technique</chapter-title><publisher-name>MIT Press</publisher-name><year>2014</year><edition>second</edition></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>D</given-names></name><name><surname>Avila</surname><given-names>E</given-names></name><name><surname>Caziot</surname><given-names>B</given-names></name><name><surname>Laurens</surname><given-names>J</given-names></name><name><surname>Dickman</surname><given-names>JD</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><article-title>Spatial modulation of hippocampal activity in freely moving macaques</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>3521</fpage><lpage>3534</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC8571030</pub-id><pub-id pub-id-type="pmid">34644546</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.09.032</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mithen</surname><given-names>SJ</given-names></name></person-group><chapter-title>The Singing Neanderthals: The Origins of Music, Language, Mind, and Body</chapter-title><publisher-name>Harvard University Press</publisher-name><year>2006</year></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molloy</surname><given-names>K</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name><name><surname>Lavie</surname><given-names>N</given-names></name></person-group><article-title>Inattentional Deafness: Visual Load Leads to Time-Specific Suppression of Auditory Evoked Responses</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><fpage>16046</fpage><lpage>16054</lpage><pub-id pub-id-type="pmcid">PMC4682776</pub-id><pub-id pub-id-type="pmid">26658858</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2931-15.2015</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="pmcid">PMC6768091</pub-id><pub-id pub-id-type="pmid">31551604</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Bánki</surname><given-names>A</given-names></name><name><surname>Markova</surname><given-names>G</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Hunnius</surname><given-names>S</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name></person-group><chapter-title>Chapter 1 - Studying parent-child interaction with hyperscanning</chapter-title><source>Progress in Brain Research New Perspectives on Early Social-cognitive Development</source><publisher-name>Elsevier</publisher-name><year>2020</year><fpage>1</fpage><lpage>24</lpage><date-in-citation>Accessed July 4, 2024</date-in-citation><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0079612320300455">https://www.sciencedirect.com/science/article/pii/S0079612320300455</ext-link></comment><pub-id pub-id-type="pmid">32859283</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Lagacé-Cusiac</surname><given-names>R</given-names></name><name><surname>Everling</surname><given-names>JC</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name><name><surname>Grahn</surname><given-names>JA</given-names></name></person-group><article-title>Audiovisual integration of rhythm in musicians and dancers</article-title><source>Atten Percept Psychophys</source><year>2024</year><volume>86</volume><fpage>1400</fpage><lpage>1416</lpage><pub-id pub-id-type="pmid">38557941</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Reisner</surname><given-names>S</given-names></name><name><surname>Lueger</surname><given-names>A</given-names></name><name><surname>Wass</surname><given-names>SV</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Markova</surname><given-names>G</given-names></name></person-group><article-title>Sing to me, baby: Infants show neural tracking and rhythmic movements to live and dynamic maternal singing</article-title><source>Developmental Cognitive Neuroscience</source><year>2023</year><volume>64</volume><elocation-id>101313</elocation-id><pub-id pub-id-type="pmcid">PMC10618693</pub-id><pub-id pub-id-type="pmid">37879243</pub-id><pub-id pub-id-type="doi">10.1016/j.dcn.2023.101313</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>T</given-names></name><name><surname>Schleihauf</surname><given-names>H</given-names></name><name><surname>Kayhan</surname><given-names>E</given-names></name><name><surname>Matthes</surname><given-names>D</given-names></name><name><surname>Vrtička</surname><given-names>P</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name></person-group><article-title>Neural synchrony in mother–child conversation: Exploring the role of conversation patterns</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2021</year><volume>16</volume><fpage>93</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC7812624</pub-id><pub-id pub-id-type="pmid">32591781</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsaa079</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niso</surname><given-names>G</given-names></name><name><surname>Romero</surname><given-names>E</given-names></name><name><surname>Moreau</surname><given-names>JT</given-names></name><name><surname>Araujo</surname><given-names>A</given-names></name><name><surname>Krol</surname><given-names>LR</given-names></name></person-group><article-title>Wireless EEG: A survey of systems and studies</article-title><source>NeuroImage</source><year>2023</year><volume>269</volume><elocation-id>119774</elocation-id><pub-id pub-id-type="pmid">36566924</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novembre</surname><given-names>G</given-names></name><name><surname>Pawar</surname><given-names>VM</given-names></name><name><surname>Bufacchi</surname><given-names>RJ</given-names></name><name><surname>Kilintari</surname><given-names>M</given-names></name><name><surname>Srinivasan</surname><given-names>M</given-names></name><name><surname>Rothwell</surname><given-names>JC</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Iannetti</surname><given-names>GD</given-names></name></person-group><article-title>Saliency Detection as a Reactive Process: Unexpected Sensory Events Evoke Corticomuscular Coupling</article-title><source>J Neurosci</source><year>2018</year><volume>38</volume><fpage>2385</fpage><lpage>2397</lpage><pub-id pub-id-type="pmcid">PMC5830523</pub-id><pub-id pub-id-type="pmid">29378865</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2474-17.2017</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Novembre</surname><given-names>G</given-names></name><name><surname>Sammler</surname><given-names>D</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name></person-group><article-title>Neural alpha oscillations index the balance between self-other integration and segregation in real-time joint action</article-title><source>Neuropsychologia</source><year>2016</year><volume>89</volume><fpage>414</fpage><lpage>425</lpage><pub-id pub-id-type="pmid">27449708</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neill</surname><given-names>GC</given-names></name><name><surname>Seymour</surname><given-names>RA</given-names></name><name><surname>Mellor</surname><given-names>S</given-names></name><name><surname>Alexander</surname><given-names>N</given-names></name><name><surname>Tierney</surname><given-names>TM</given-names></name><name><surname>Bernachot</surname><given-names>L</given-names></name><name><surname>Hnazaee</surname><given-names>MF</given-names></name><name><surname>Spedden</surname><given-names>ME</given-names></name><name><surname>Timms</surname><given-names>RC</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name><etal/></person-group><article-title>Combining video telemetry and wearable MEG for naturalistic imaging</article-title><year>2024</year><date-in-citation>Accessed December 16, 2024</date-in-citation><elocation-id>2023.08.01.551482</elocation-id><pub-id pub-id-type="doi">10.1101/2023.08.01.551482v2</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Intell Neuroscience</source><year>2011</year><volume>2011</volume><comment>1:1-1:9</comment><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orgs</surname><given-names>G</given-names></name><name><surname>Vicary</surname><given-names>S</given-names></name><name><surname>Sperling</surname><given-names>M</given-names></name><name><surname>Richardson</surname><given-names>DC</given-names></name><name><surname>Williams</surname><given-names>AL</given-names></name></person-group><article-title>Movement synchrony among dance performers predicts brain synchrony among dance spectators</article-title><source>Sci Rep</source><year>2024</year><volume>14</volume><elocation-id>22079</elocation-id><pub-id pub-id-type="pmcid">PMC11436841</pub-id><pub-id pub-id-type="pmid">39333777</pub-id><pub-id pub-id-type="doi">10.1038/s41598-024-73438-0</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osman</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>K-M</given-names></name><name><surname>Syre</surname><given-names>P</given-names></name><name><surname>Russ</surname><given-names>B</given-names></name></person-group><article-title>Paradoxical lateralization of brain potentials during imagined foot movements</article-title><source>Cognitive Brain Research</source><year>2005</year><volume>24</volume><fpage>727</fpage><lpage>731</lpage><pub-id pub-id-type="pmid">15894471</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Sullivan</surname><given-names>AE</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Visual Cortical Entrainment to Motion and Categorical Speech Features during Silent Lipreading</article-title><source>Frontiers in Human Neuroscience</source><year>2017</year><volume>10</volume><date-in-citation>Accessed July 21, 2023</date-in-citation><pub-id pub-id-type="pmcid">PMC5225113</pub-id><pub-id pub-id-type="pmid">28123363</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00679</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Y</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name><name><surname>Song</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name></person-group><article-title>Interpersonal synchronization of inferior frontal cortices tracks social interactive learning of a song</article-title><source>NeuroImage</source><year>2018</year><volume>183</volume><fpage>280</fpage><lpage>290</lpage><pub-id pub-id-type="pmid">30086411</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pion-Tonachini</surname><given-names>L</given-names></name><name><surname>Kreutz-Delgado</surname><given-names>K</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>ICLabel: An automated electroencephalographic independent component classifier, dataset, and website</article-title><source>NeuroImage</source><year>2019</year><volume>198</volume><fpage>181</fpage><lpage>197</lpage><pub-id pub-id-type="pmcid">PMC6592775</pub-id><pub-id pub-id-type="pmid">31103785</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.026</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Plöchl</surname><given-names>M</given-names></name><name><surname>Ossandón</surname><given-names>JP</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><article-title>Combining EEG and eye tracking: identification, characterization, and correction of eye movement artifacts in electroencephalographic data</article-title><source>Frontiers in Human Neuroscience</source><year>2012</year><volume>6</volume><date-in-citation>Accessed June 25, 2024</date-in-citation><pub-id pub-id-type="pmcid">PMC3466435</pub-id><pub-id pub-id-type="pmid">23087632</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00278</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>A</given-names></name><name><surname>Allison</surname><given-names>T</given-names></name></person-group><article-title>ERPS EVOKED BY VIEWING FACIAL MOVEMENTS</article-title><source>Cognitive Neuropsychology</source><year>2000</year><date-in-citation>Accessed May 16, 2024</date-in-citation><pub-id pub-id-type="pmid">20945181</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramírez-Moreno</surname><given-names>MA</given-names></name><name><surname>Cruz-Garza</surname><given-names>JG</given-names></name><name><surname>Acharya</surname><given-names>A</given-names></name><name><surname>Chatufale</surname><given-names>G</given-names></name><name><surname>Witt</surname><given-names>W</given-names></name><name><surname>Gelok</surname><given-names>D</given-names></name><name><surname>Reza</surname><given-names>G</given-names></name><name><surname>Contreras-Vidal</surname><given-names>JL</given-names></name></person-group><article-title>Brain-to-brain communication during musical improvisation: a performance case study</article-title><source>F1000Res</source><year>2023</year><volume>11</volume><fpage>989</fpage><pub-id pub-id-type="pmcid">PMC10558998</pub-id><pub-id pub-id-type="pmid">37809054</pub-id><pub-id pub-id-type="doi">10.12688/f1000research.123515.4</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>MC</given-names></name><name><surname>Styr</surname><given-names>B</given-names></name><name><surname>Schmid</surname><given-names>TA</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name><name><surname>Yartsev</surname><given-names>MM</given-names></name></person-group><article-title>Cortical representation of group social communication in bats</article-title><source>Science</source><year>2021</year><volume>374</volume><elocation-id>eaba9584</elocation-id><pub-id pub-id-type="pmcid">PMC8775406</pub-id><pub-id pub-id-type="pmid">34672724</pub-id><pub-id pub-id-type="doi">10.1126/science.aba9584</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shibasaki</surname><given-names>H</given-names></name><name><surname>Barrett</surname><given-names>G</given-names></name><name><surname>Halliday</surname><given-names>E</given-names></name><name><surname>Halliday</surname><given-names>AM</given-names></name></person-group><article-title>Components of the movement-related cortical potential and their scalp topography</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1980</year><volume>49</volume><fpage>213</fpage><lpage>226</lpage><pub-id pub-id-type="pmid">6158398</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smulders</surname><given-names>FTY</given-names></name><name><surname>Miller</surname><given-names>JO</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Kappenman</surname><given-names>ES</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><chapter-title>The Lateralized Readiness Potential</chapter-title><source>The Oxford Handbook of Event-Related Potential Components</source><publisher-name>Oxford University Press</publisher-name><year>2011</year><date-in-citation>Accessed May 16, 2024</date-in-citation><comment>pp 0</comment><pub-id pub-id-type="doi">10.1093/oxfordhb/9780195374148.013.0115</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somervail</surname><given-names>R</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Novembre</surname><given-names>G</given-names></name><name><surname>Bufacchi</surname><given-names>RJ</given-names></name><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Crepaldi</surname><given-names>M</given-names></name><name><surname>Hu</surname><given-names>L</given-names></name><name><surname>Iannetti</surname><given-names>GD</given-names></name></person-group><article-title>Waves of Change: Brain Sensitivity to Differential, not Absolute, Stimulus Intensity is Conserved Across Humans and Rats</article-title><source>Cerebral Cortex</source><year>2021</year><volume>31</volume><fpage>949</fpage><lpage>960</lpage><pub-id pub-id-type="pmcid">PMC7786352</pub-id><pub-id pub-id-type="pmid">33026425</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhaa267</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stangl</surname><given-names>M</given-names></name><name><surname>Maoz</surname><given-names>SL</given-names></name><name><surname>Suthana</surname><given-names>N</given-names></name></person-group><article-title>Mobile cognition: imaging the human brain in the ‘real world’</article-title><source>Nat Rev Neurosci</source><year>2023</year><volume>24</volume><fpage>347</fpage><lpage>362</lpage><pub-id pub-id-type="pmcid">PMC10642288</pub-id><pub-id pub-id-type="pmid">37046077</pub-id><pub-id pub-id-type="doi">10.1038/s41583-023-00692-y</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><year>2019</year><volume>364</volume><elocation-id>eaav7893</elocation-id><pub-id pub-id-type="pmcid">PMC6525101</pub-id><pub-id pub-id-type="pmid">31000656</pub-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Testard</surname><given-names>C</given-names></name><name><surname>Tremblay</surname><given-names>S</given-names></name><name><surname>Parodi</surname><given-names>F</given-names></name><name><surname>DiTullio</surname><given-names>RW</given-names></name><name><surname>Acevedo-Ithier</surname><given-names>A</given-names></name><name><surname>Gardiner</surname><given-names>KL</given-names></name><name><surname>Kording</surname><given-names>K</given-names></name><name><surname>Platt</surname><given-names>ML</given-names></name></person-group><article-title>Neural signatures of natural behaviour in socializing macaques</article-title><source>Nature</source><year>2024</year><volume>628</volume><fpage>381</fpage><lpage>390</lpage><pub-id pub-id-type="pmid">38480888</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theofanopoulou</surname><given-names>C</given-names></name><name><surname>Paez</surname><given-names>S</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Todd</surname><given-names>E</given-names></name><name><surname>Ramírez-Moreno</surname><given-names>MA</given-names></name><name><surname>Khaleghian</surname><given-names>B</given-names></name><name><surname>Sánchez</surname><given-names>AM</given-names></name><name><surname>Barceló</surname><given-names>L</given-names></name><name><surname>Gand</surname><given-names>V</given-names></name><name><surname>Contreras-Vidal</surname><given-names>JL</given-names></name></person-group><article-title>Mobile brain imaging in butoh dancers: from rehearsals to public performance</article-title><source>BMC Neurosci</source><year>2024</year><volume>25</volume><fpage>62</fpage><pub-id pub-id-type="pmcid">PMC11539292</pub-id><pub-id pub-id-type="pmid">39506628</pub-id><pub-id pub-id-type="doi">10.1186/s12868-024-00864-1</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toiviainen</surname><given-names>P</given-names></name><name><surname>Luck</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>MR</given-names></name></person-group><article-title>Embodied Meter: Hierarchical Eigenmodes in Music-Induced Movement</article-title><source>Music Perception</source><year>2010</year><volume>28</volume><fpage>59</fpage><lpage>70</lpage></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tremblay</surname><given-names>S</given-names></name><name><surname>Testard</surname><given-names>C</given-names></name><name><surname>DiTullio</surname><given-names>RW</given-names></name><name><surname>Inchauspé</surname><given-names>J</given-names></name><name><surname>Petrides</surname><given-names>M</given-names></name></person-group><article-title>Neural cognitive signals during spontaneous movements in the macaque</article-title><source>Nat Neurosci</source><year>2023</year><volume>26</volume><fpage>295</fpage><lpage>305</lpage><pub-id pub-id-type="pmid">36536242</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>NF</given-names></name></person-group><article-title>Decomposing biological motion: A framework for analysis and synthesis of human gait patterns</article-title><source>Journal of Vision</source><year>2002</year><volume>2</volume><fpage>2</fpage><pub-id pub-id-type="pmid">12678652</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urigüen</surname><given-names>JA</given-names></name><name><surname>Garcia-Zapirain</surname><given-names>B</given-names></name></person-group><article-title>EEG artifact removal—state-of-the-art and guidelines</article-title><source>J Neural Eng</source><year>2015</year><volume>12</volume><elocation-id>031001</elocation-id><pub-id pub-id-type="pmid">25834104</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varlet</surname><given-names>M</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Nijhuis</surname><given-names>P</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name></person-group><article-title>Neural tracking and integration of ‘self’ and ‘other’ in improvised interpersonal coordination</article-title><source>NeuroImage</source><year>2020</year><volume>206</volume><elocation-id>116303</elocation-id><pub-id pub-id-type="pmid">31654761</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varlet</surname><given-names>M</given-names></name><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Schmidt</surname><given-names>RC</given-names></name><name><surname>Keller</surname><given-names>PE</given-names></name></person-group><article-title>Neural tracking of visual periodic motion</article-title><source>European Journal of Neuroscience</source><year>2023</year><volume>57</volume><fpage>1081</fpage><lpage>1097</lpage><pub-id pub-id-type="pmid">36788113</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vercillo</surname><given-names>T</given-names></name><name><surname>O’Neil</surname><given-names>S</given-names></name><name><surname>Jiang</surname><given-names>F</given-names></name></person-group><article-title>Action–effect contingency modulates the readiness potential</article-title><source>NeuroImage</source><year>2018</year><volume>183</volume><fpage>273</fpage><lpage>279</lpage><pub-id pub-id-type="pmcid">PMC6450698</pub-id><pub-id pub-id-type="pmid">30114465</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.028</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wass</surname><given-names>SV</given-names></name><name><surname>Noreika</surname><given-names>V</given-names></name><name><surname>Georgieva</surname><given-names>S</given-names></name><name><surname>Clackson</surname><given-names>K</given-names></name><name><surname>Brightman</surname><given-names>L</given-names></name><name><surname>Nutbrown</surname><given-names>R</given-names></name><name><surname>Covarrubias</surname><given-names>LS</given-names></name><name><surname>Leong</surname><given-names>V</given-names></name></person-group><article-title>Parental neural responsivity to infants’ visual attention: How mature brains influence immature brains during social interaction</article-title><source>PLOS Biology</source><year>2018</year><volume>16</volume><elocation-id>e2006328</elocation-id><pub-id pub-id-type="pmcid">PMC6292577</pub-id><pub-id pub-id-type="pmid">30543622</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2006328</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wass</surname><given-names>SV</given-names></name><name><surname>Whitehorn</surname><given-names>M</given-names></name><name><surname>Haresign</surname><given-names>IM</given-names></name><name><surname>Phillips</surname><given-names>E</given-names></name><name><surname>Leong</surname><given-names>V</given-names></name></person-group><article-title>Interpersonal Neural Entrainment during Early Social Interaction</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><fpage>329</fpage><lpage>342</lpage><pub-id pub-id-type="pmid">32160569</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weineck</surname><given-names>K</given-names></name><name><surname>Wen</surname><given-names>OX</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Zoefel</surname><given-names>B</given-names></name></person-group><article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e75515</elocation-id><pub-id pub-id-type="pmcid">PMC9467512</pub-id><pub-id pub-id-type="pmid">36094165</pub-id><pub-id pub-id-type="doi">10.7554/eLife.75515</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>DL</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name><name><surname>Algazi</surname><given-names>A</given-names></name></person-group><article-title>Intermodal selective attention. I. Effects on event-related potentials to lateralized auditory and visual stimuli</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1992</year><volume>82</volume><fpage>341</fpage><lpage>355</lpage><pub-id pub-id-type="pmid">1374703</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Goodman</surname><given-names>JM</given-names></name><name><surname>Moore</surname><given-names>DD</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Bensmaia</surname><given-names>SJ</given-names></name></person-group><article-title>Unexpected complexity of everyday manual behaviors</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><fpage>3564</fpage><pub-id pub-id-type="pmcid">PMC7367296</pub-id><pub-id pub-id-type="pmid">32678102</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17404-0</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Wireless multilateral devices for optogenetic studies of individual and social behaviors</article-title><source>Nat Neurosci</source><year>2021</year><volume>24</volume><fpage>1035</fpage><lpage>1045</lpage><pub-id pub-id-type="pmcid">PMC8694284</pub-id><pub-id pub-id-type="pmid">33972800</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-00849-x</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Yartsev</surname><given-names>MM</given-names></name></person-group><article-title>Correlated Neural Activity across the Brains of Socially Interacting Bats</article-title><source>Cell</source><year>2019</year><volume>178</volume><fpage>413</fpage><lpage>428</lpage><elocation-id>e22</elocation-id><pub-id pub-id-type="pmcid">PMC6625887</pub-id><pub-id pub-id-type="pmid">31230710</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2019.05.023</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" orientation="portrait" position="float"><caption><title>Significance statement</title></caption><p>Real-world brain function involves integrating multiple information streams simultaneously. However, due to a shortfall of computational methods, laboratory-based neuroscience often examines neural processes in isolation. Using multivariate modeling of EEG data from pairs of participants freely dancing to music, we demonstrate that it is possible to tease apart physiologically established neural processes associated with music perception, motor control, and observation of a partner’s movement. Crucially, we identify a previously unknown neural marker of social coordination that encodes the spatiotemporal alignment between dancers, beyond self-or partner-related kinematics alone. These findings highlight the potential of computational neuroscience to uncover the biological mechanisms underlying real-world social and motor behaviors, advancing our understanding of how the brain supports dynamic and interactive activities.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental materials and methods.</title><p><bold>a</bold>, Experimental setup. We applied the mTRF method to a previously collected dataset (<xref ref-type="bibr" rid="R5">Bigand et al., 2024</xref>) for which dyads of participants danced spontaneously in response to music while we recorded electroencephalography (EEG, 64 channels), electrooculography (EOG), electromyography (EMG, from neck and face muscles), and 3D full-body kinematics (22 markers). <bold>b</bold>, Experimental design. Data were collected under the experimental conditions of the original study, which utilized a 2×2 factorial design. The two manipulated factors were musical input (whether participants listened to the same or different music presented through earphones) and visual contact (whether participants could see or not see each other). <bold>c</bold>, Overview of the modeling paradigm. We estimated multivariate Temporal Response Functions (mTRFs), which learned the optimal linear mapping between the set of variables of interest (here music, self- and other-generated movements, social coordination, as well as other control variables (not shown) such as ocular, facial and neck muscle activity) and the EEG data. <bold>d</bold>, Model comparisons. To assess the unique contribution of each variable (regressor) to the EEG data, we trained reduced models encompassing all variables apart from the specified one. The difference in prediction accuracy between the reduced and full model (encompassing all variables), denoted Δr, yields the unique contribution of this variable.</p></caption><graphic xlink:href="EMS202332-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Neural encoding of self- and other-related movements across different principal movements (PMs).</title><p>Bars represent the unique contribution (Δr) of each PM (grand-average) to the EEG signal recorded from the self (electrode Cz) or the other (electrode Oz). Δr values represent the difference in EEG prediction accuracy between the PM-specific reduced models and the full model, for self- and other-generated movements, respectively. Error bars represent ±1 standard error mean (SEM). Gray circle diagrams illustrate the proportion (%) of kinematic variance explained by each PM, with the first 15 PMs accounting for more than 95% of the total variance. Together, these results indicate that bounce (PM10) was the strongest predictor of EEG activity, whether self-generated or observed in others, despite accounting for only ∼1% of kinematic variance.</p></caption><graphic xlink:href="EMS202332-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Distinct EEG activities related to music, self- and other-generated movements, social coordination, and muscle artifacts.</title><p><bold>a</bold>, Topographical maps of the unique contribution of each model variable to the predicted EEG. Δr topographical maps represent the grand-average difference in EEG prediction accuracy between the reduced models (excluding the variable of interest) and the full model (including all four variables, plus ocular, facial, and neck muscle activity; see <xref ref-type="fig" rid="F1">Fig. 1d</xref>), for each EEG electrode and experimental condition. <bold>b</bold>, Ridge regression weights for TRFs corresponding to music (Fz), self-generated movements (averaged across C3, C4), other-generated movements (Oz), social coordination (Oz), and ocular (F8), facial (T8), and neck (Oz) muscle activity for the full-model TRF. Grand-average weights are shown. Shaded areas represent ±1 SEM.</p></caption><graphic xlink:href="EMS202332-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Comparison of unique contributions across experimental conditions.</title><p>Bars indicate the grand-average unique contributions (averaged over electrodes showing a gain; Δr &gt;0) of each model variable, across conditions. Error bars represent ±1 SEM. Stars indicate significant main effects of visual contact and musical input, as well as the interaction between the two factors (2×2 repeated measures ANOVA, Bonferroni-corrected; *p<sub>bonf</sub>&lt;.05, **p<sub>bonf</sub>&lt;.01, ***p<sub>bonf</sub>&lt;.001).</p></caption><graphic xlink:href="EMS202332-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Event-related potential (ERP) analysis.</title><p>ERPs evoked by salient changes in music, self-generated movements, other-generated movements, and social coordination. EEG time-series were epoched to peaks of spectral flux for music, peaks of velocity magnitude for self- and other-generated movements, and changes between in-phase and anti-phase for social coordination. ERPs amplitudes were compared across two groups of epochs within each variable: loud vs. soft sounds for music (Fz), fast vs. slow movements for self- (averaged across C3, C4) and other- (Oz) generated movements, and changes to in-phase vs. to anti-phase for social coordination (Oz). Grand-average ERPs are shown for the two groups of epochs within each variable and across all experimental conditions. Colored shaded areas represent ±1 SEM, while grey shaded regions highlight significant differences in ERP amplitude between groups of epochs at a given time point (permutation test over time, at the electrode of interest, cluster-corrected). Topographical maps display amplitude differences across electrodes within the time windows of identified clusters.</p></caption><graphic xlink:href="EMS202332-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>mTRFs tease apart body-part-specific motor activity.</title><p><bold>a</bold>, Topographical maps of the unique contribution of (self-generated) left- and right-hand movements to the predicted EEG. Δr topographical maps represent the grand-average difference in EEG prediction accuracy between the reduced models (excluding the body part of interest) and the full model (including all body parts, plus the neck control variable), for each EEG electrode and across all trials, regardless of experimental condition. We ran the TRF models without considering experimental conditions, given that no statistical difference was found across conditions in our main analysis (see <xref ref-type="fig" rid="F4">Fig. 4</xref>). Separate TRF models for hands were derived by excluding each marker (‘L Hand’ or ‘R Hand’) from the full model. <bold>b</bold>, Same as (a), but for left- and right-foot movements. Separate TRF models for feet were derived by excluding each marker (‘L Foot’ or ‘R Foot’) from the full model. <bold>c</bold>, Same as (a) and (b), but for head movements. The head TRF model was derived by excluding all four head markers together.</p></caption><graphic xlink:href="EMS202332-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Tracking of social coordination beyond self and other.</title><p><bold>a</bold>, Results of the mTRF models associated with social coordination, incorporating spatial directions of self- and other-generated movements. Top: Social coordination uniquely predicted EEG activity at similar electrode sites than in the main analysis (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). Statistics revealed the exact same differences in unique contribution as observed in our main analysis (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Bottom: TRF regression weights exhibited similar patterns as in the main analysis (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). <bold>b</bold>, Coordination-related ERPs time-locked to self-generated (top) or other-generated (bottom) movement changes, at electrode Oz. ERPs related to changes to in-phase and anti-phase coordination are represented by continuous and dashed lines, respectively. Grand-average ERPs are shown for the two groups of trials associated with each variable across all experimental conditions. Colored shaded areas represent ±1 SEM, while grey shaded regions highlight significant differences in ERP amplitude between groups of trials at a given time point (permutation test over time, at Oz, cluster-corrected). Topographical maps display amplitude differences across electrodes within the time windows of identified clusters.</p></caption><graphic xlink:href="EMS202332-f007"/></fig></floats-group></article>