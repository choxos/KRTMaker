<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202027</article-id><article-id pub-id-type="doi">10.1101/2024.12.11.628010</article-id><article-id pub-id-type="archive">PPR954386</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Age-related decline in neural phase-locking to envelope and temporal fine structure revealed by frequency following responses: A potential signature of cochlear synaptopathy impairing speech intelligibility</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Ponsot</surname><given-names>Emmanuel</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Devolder</surname><given-names>Pauline</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Dhooge</surname><given-names>Ingeborg</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Verhulst</surname><given-names>Sarah</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/025xvn046</institution-id><institution>STMS Lab</institution></institution-wrap> (CNRS / Ircam / <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02en5vm52</institution-id><institution>Sorbonne Université</institution></institution-wrap>, Ministère de la Culture), <addr-line>1 place Igor Stravinsky</addr-line>, <postal-code>75004</postal-code><city>Paris</city>, <country country="FR">France</country></aff><aff id="A2"><label>2</label>Hearing Technology @ WAVES, Department of Information Technology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cv9y106</institution-id><institution>Ghent University</institution></institution-wrap>, <addr-line>Technologiepark 126</addr-line>, <postal-code>9052</postal-code><city>Zwijnaarde</city>, <country country="BE">Belgium</country></aff><aff id="A3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cv9y106</institution-id><institution>Ghent University</institution></institution-wrap>, Department of Head and Skin, <country country="BE">Belgium</country></aff><aff id="A4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00xmkp704</institution-id><institution>Ghent University Hospital</institution></institution-wrap>, Department of Ear, Nose and Throat, <country country="BE">Belgium</country></aff><author-notes><corresp id="CR1">Corresponding author: Emmanuel Ponsot, Ph.D., Ircam, 1 place Igor Stravinsky, 75004 Paris, France, Tel: +331 44 78 13 14, <email>emmanuel.ponsot@ircam.fr</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>12</month><year>2024</year></pub-date><pub-date pub-type="preprint"><day>12</day><month>12</month><year>2024</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Assessing the contribution of cochlear synaptopathy (CS) to the variability in speech-in-noise intelligibility among individuals remains a challenge. While several studies have proposed biomarkers for CS based on neural phase-locking to the temporal envelope (ENV), fewer have investigated how CS affects the coding of temporal fine structure (TFS), despite its crucial role in speech-in-noise perception. In this study, we specifically examined whether TFS-based markers of CS could be derived from electrophysiological responses and psychophysical detection thresholds of spectral modulation (SM) in a complex tone, which serves as a parametric model of speech. We employed an integrated approach, combining psychophysical testing with frequency-following response (FFR) measurements in three groups of participants: young normal-hearing (yNH), older normal-hearing (oNH), and older hearing-impaired (oHI) individuals. We expanded on previous work by assessing phase-locking to both ENV, using a 4 kHz rectangular amplitude-modulated (RAM) tone, and TFS, using a low-frequency (&lt;1.5 kHz) SM complex tone. Overall, FFR results showed significant reductions in neural phaselocking to both ENV and TFS components with age and hearing loss. Specifically, the strength of TFS-related FFRs, particularly the component corresponding to the harmonic closest to the peak of the spectral envelope (~500 Hz), was negatively correlated with age, even after adjusting for audiometric thresholds. This TFS marker also correlated with ENV-related FFRs derived from the RAM tone, suggesting a shared decline in phase-locking capacity across low and high cochlear frequencies. Computational simulations of the auditory periphery indicated that the observed FFR strength reduction with age is consistent with approximately 50% loss of auditory nerve fibers, aligning with histopathological data. However, the TFS-based FFR marker did not account for variability in speech intelligibility observed in the same participants. Psychophysical measurements showed no age-related effects and were unrelated to the TFS-based FFR marker, highlighting the need for further psychophysical research to establish a behavioral counterpart. Altogether, our results demonstrate that FFRs to vowel-like stimuli can serve as a complementary electrophysiological marker for assessing neural coding fidelity to stimulus TFS. This approach could provide a valuable tool for better understanding the impact of CS on an important coding dimension for speech-in-noise perception.</p></abstract><kwd-group><title>Keywords:</title><kwd>cochlear synaptopathy</kwd><kwd>temporal fine structure</kwd><kwd>neural phase-locking</kwd><kwd>spectral modulation</kwd><kwd>speech intelligibility</kwd><kwd>frequency-following response</kwd><kwd>aging</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Understanding speech in noisy environments is at the core of human social interactions; yet, many individuals present significant difficulties in these situations. Speech-in-noise (SPiN) performance is shown to be impaired by multiple factors which degrade sensory or cognitive processes, from audiometric thresholds to selective attention capacities (e.g., <xref ref-type="bibr" rid="R56">Parbery-Clark et al., 2009</xref>; <xref ref-type="bibr" rid="R55">Oberfeld &amp; Kloeckner-Nowotny, 2016</xref>; <xref ref-type="bibr" rid="R69">Varnet et al., 2021</xref>), but recent studies demonstrated that these factors only provide a limited account to understand listeners’ performance variability (<xref ref-type="bibr" rid="R62">Ruggles et al., 2011</xref>; <xref ref-type="bibr" rid="R7">Bharadwaj et al., 2015</xref>). A striking illustration is provided by the large-scale study conducted by <xref ref-type="bibr" rid="R69">Varnet et al. (2021)</xref> wherein over 500 individuals of various ages and audiometric thresholds performed a very simplified speech-in-noise perception task (monaural, simple sentences embedded in speech-shaped noise). Listeners with comparable audiometric deficits exhibited a huge variability of speech-in-noise performance, even among listeners with clinically-normal hearing thresholds, despite the simplicity of adopted speech material which was far from the complexity of real cocktail-partylike listening and thus minimizing the contribution of linguistic and cognitive factors. Additionally, a recent study based on a database of ~100k individuals who consulted an audiology department for various hearing-in-noise problems led to the estimate that more than 10% had clinically normal audiograms (<xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>). Recent research studies are hence focusing on the role of sensory coding deficits different from hearing sensitivity for explaining this variability.</p><p id="P3">There are two research paths that have been followed over the past decades to unravel the origins of individual differences in speech intelligibility deficits. The first track focuses on suprathreshold psychoacoustic tasks that capture the individual sensitivity to detecting spectro-temporal sound modulations (STM; <xref ref-type="bibr" rid="R5">Bernstein et al., 2013</xref>, <xref ref-type="bibr" rid="R6">2016</xref>; <xref ref-type="bibr" rid="R47">Mehraei et al, 2014</xref>, <xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). STMs constitute first-order models of speech units that are vowels and formant transitions (see <xref ref-type="fig" rid="F1">Fig. 1</xref>) critical for speech intelligibility (<xref ref-type="bibr" rid="R24">Elliott and Theunissen, 2009</xref>). Several studies investigating STM detection in hearing-impaired (HI) listeners have reported correlations between STM detection thresholds (smallest detectable modulation depth) and SPiN scores, even after accounting for differences in audiometric thresholds (<xref ref-type="bibr" rid="R5">Bernstein et al., 2013</xref>, <xref ref-type="bibr" rid="R6">2016</xref>; <xref ref-type="bibr" rid="R47">Mehraei et al, 2014</xref>, <xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). Because these correlations were specifically driven by STM signals with low-frequency carriers (&lt;1 kHz) and observed with spectrally-modulated (SM) signals, i.e. that do not carry temporal modulation, it was hypothesized that the variability in SPiN performances reflects an impaired ability to use TFS information in the stimulus waveform (<xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). From an audiological point of view, STM stimuli became appealing candidates for providing a better characterization of auditory deficits compared to pure-tone audiometry; clinical tests and large-scale studies are therefore currently conducted in that direction (<xref ref-type="bibr" rid="R85">Zaar et al., 2023a</xref>, <xref ref-type="bibr" rid="R86">2023b</xref>, <xref ref-type="bibr" rid="R87">2023c</xref>). Interestingly, studies investigating STM perception in normal-hearing (NH) listeners have reported large interindividual differences in detection thresholds (<xref ref-type="bibr" rid="R53">Oetjen and Verhey, 2015</xref>, <xref ref-type="bibr" rid="R54">2017</xref>), but these studies did not address the question of whether this variability would relate to TFS-processing deficits and further, to potential differences in SPiN scores. However, while psychoacoustic S(T)M sensitivity may be a promising predictor of SPiN, it cannot directly be used to quantify or treat the pathophysiology of SPiN deficits.</p><p id="P4">Therefore, a second line of research focused on the role of cochlear synaptopathy (CS) for speech intelligibility. CS refers to the loss or damage of synapses between inner hair cells (IHC) and auditory-nerve (AN) fibers (<xref ref-type="bibr" rid="R34">Kujawa &amp; Liberman, 2009</xref>), and constitutes a recently discovered form of sensorineural hearing loss besides the well-known outer and-inner hair cell damage, OHC &amp; IHC). CS cannot be detected using the clinical audiogram or otoacoustic emissions and is therefore often referred to as “hidden” hearing loss but compromises suprathreshold temporal coding (Shaheen et al., 2015; Parthasarathy and Kujawa, 2018). Over the past ~15 years, animal studies have provided substantial knowledge about the mechanisms of CS, showing that it develops with the normal aging process, and, strikingly, that it can also occur after temporary noise exposure without associated OHC damage (Liberman &amp; Liberman, 2015). Computational studies simulating this deficit shows that it indeed strongly impacts the quality of the neural signal sent from the periphery to higher auditory stages (Bharadwaj et al., 2014; <xref ref-type="bibr" rid="R72">Verhulst et al., 2018</xref>; <xref ref-type="bibr" rid="R35">Lesica, 2018</xref>; <xref ref-type="bibr" rid="R70">Vasilikov et al., 2021</xref>). Yet, recent studies investigating the conditions and the extent to which CS affects the processing of suprathreshold sounds in humans, as involved in SPiN understanding, remain at their premises, and their outcomes are still mixed (<xref ref-type="bibr" rid="R37">Liberman et al., 2016</xref>; <xref ref-type="bibr" rid="R15">Bramhall et al., 2019</xref>; <xref ref-type="bibr" rid="R29">Guest et al., 2019</xref>, <xref ref-type="bibr" rid="R60">Prendergast et al., 2019</xref>, <xref ref-type="bibr" rid="R27">Garret et al., 2024</xref>). A second problem in connecting CS to sound perception deficits relates to the impossibility to quantify CS directly in humans. Through the combined use of animal models of CS and computational models of human generators of auditory evoked potentials such as the auditory brainstem response (e.g., Verhulst et al., 2016, <xref ref-type="bibr" rid="R75">2018</xref>; <xref ref-type="bibr" rid="R25">Encina-llamas et al., 2019</xref>; <xref ref-type="bibr" rid="R70">Vasilkov et al., 2021</xref>; <xref ref-type="bibr" rid="R44">Märcher-Rørsted et al., 2022</xref>), several indirect electrophysiological markers of CS have been proposed.</p><p id="P5">Notably, the frequency following response (FFR) to sustained, sometimes modulated stimuli, reflects the fidelity of neural AN or brainstem encoding of temporal information (<xref ref-type="bibr" rid="R9">Bidelman et al., 2018</xref>; <xref ref-type="bibr" rid="R33">Krizman and Kraus, 2019</xref>). Most of these studies consider the envelope component of the FFR, referred to as FFR<sub>env</sub>, or the <italic>envelope following response</italic> (EFR). <xref ref-type="bibr" rid="R27">Garrett (2024)</xref> and <xref ref-type="bibr" rid="R48">Mepani et al. (2021)</xref> analyzed the FFR<sub>env</sub> to a rectangular amplitude-modulated (RAM) 4-kHz tone. This stimulus was designed using a computational model as a specific marker of CS (<xref ref-type="bibr" rid="R73">Verhulst et al., 2018</xref>, <xref ref-type="bibr" rid="R70">Vasilkov et al., 2021</xref>). In a large cohort of 20-60 y.o. participants with clinically-normal audiograms, significant correlations between the FFR<sub>env</sub> magnitude and SPiN performance were found (<xref ref-type="bibr" rid="R48">Mepani et al., 2021</xref>). Similar conclusions regarding the FFR<sub>env</sub> in older listeners with normal audiograms were drawn by <xref ref-type="bibr" rid="R27">Garrett (2024)</xref>, wherein both studies interpreted the relation between between FFR<sub>env</sub> of the RAM tone and SPiN performance as reflecting the extent to which CS compromises envelope coding fidelity and thereby contributes to SPiN variability. A second branch of FFRs focuses on the temporal-fine structure (TFS) component of the FFR recorded to vowel sounds with formants (FFR<sub>tfs</sub>). TFS refers to the fast time-varying information contained within evoking sound and is neurally encoded for temporal fluctuations up to 1-2 kHz, where AN phase-locking is possible (<xref ref-type="bibr" rid="R51">Moore, 2007</xref>). In response to vowel-like sounds, the FFR<sub>tfs</sub> exhibit a harmonic structure with highest peaks corresponding to resolved harmonics of in the input waveform, notably the first two formant peaks (<xref ref-type="bibr" rid="R1">Aiken &amp; Picton, 2008</xref>; <xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>, <xref ref-type="bibr" rid="R3">2017</xref>). Previous works mainly studied effects of aging and hearing-loss on FFR<sub>tfs</sub> were reduced FFR<sub>tfs</sub> strength was observed in older compared to younger individuals with normal NH thresholds in the frequency range of the stimuli. Similar trends were seen in response to static or dynamic pure tones (<xref ref-type="bibr" rid="R20">Clinard et al., 2010</xref>; <xref ref-type="bibr" rid="R22">Clinard and Cotter, 2015</xref>; <xref ref-type="bibr" rid="R44">Märcher-Rørsted et al., 2022</xref>), as well as to speech-like sounds (<xref ref-type="bibr" rid="R4">Anderson et al., 2012</xref>, <xref ref-type="bibr" rid="R43">Mamo et al., 2016</xref>). FFR<sub>tfs</sub> to vowel sounds were also found to be smaller in HI compared to NH participants (<xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>; <xref ref-type="bibr" rid="R50">Molis et al. 2023</xref>). Although it is suggested age- or hearing-loss TFS-coding decline, as reflected in the FFR<sub>tfs</sub>, may contribute to poorer speech understanding, this relationship was not tested empirically in these studies. At the same time, further work is needed to develop a viable low-frequency FFR<sub>tfs</sub> counterpart to the FFR<sub>env</sub> derived with the RAM tone reflecting high-frequency processing, as it is well known that TFS based auditory cues play a pivotal role for SPiN understanding (<xref ref-type="bibr" rid="R23">Drullman 1995</xref>; <xref ref-type="bibr" rid="R38">Lorenzi et al., 2006</xref>; <xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>).</p><p id="P6">To provide a better understanding on the triangular relationship between electrophysiological FFR markers and S(T)M-based psychoacoustic markers of supra-threshold temporal processing this study takes an integrated approach to study their interrelationship as well as the connection to CS and speech intelligibility deficits. We addressed the relevance of SM stimuli to assess TFS-coding fidelity and suggest novel psychophysical and electrophysiological markers of CS based on phase-locking capacities in the low frequencies and complemented these measurements with computational modeling. We considered two groups of young (~20 y.o) and older (~ 50 y.o.) individuals with clinically-normal audiograms, and a third age-matched older group with clinical hearing loss. We hypothesize that a main effect of age-related CS is observed between the younger and older normal-hearing groups, and that a main effects of OHC damage is observed between the older normal and hearing-impaired groups. We tested the extent to which our psychophysical and electrophysiological measurements were able to predict speech-in-noise performance variability measured in all individuals, specifically under a low-pass frequency filtering condition aiming to enhance the role of TFS cues. Our results suggest that FFRs to SM signals provide a novel tool to monitor CS and can be used to help teasing apart the exact origins of speech-in-noise deficits that cannot be explained using standard audiometric hearing thresholds.</p></sec><sec id="S2" sec-type="materials | methods"><title>Material and Methods</title><sec id="S3"><title>Participants and initial hearing screening</title><p id="P7">Forty-nine subjects were initially recruited in our study. These subjects were divided into three groups depending on their age and pure-tone audiogram, as described below: young normal-hearing (yNH), older normal-hearing (oNH) and older hearing-impaired (oHI). The age range of recruitment of participants was 20-25 for the yNH group, and 40-60 for oNH and oHI groups. Hearing status was first self-assessed by the subjects using a questionnaire where we used the following exclusion criteria: (i) chronic tinnitus or hyperacusis, (ii) middle-ear pathologies and/or history of middle-ear surgery, (iii) pregnancy and (iv) any known genetic hearing loss. A clinical evaluation of the participants’ hearing status was then conducted using tympanometry and otoscopy to more formally exclude cases of conductive hearing losses and/or middle/outer ear pathologies. Pure-tone audiometry was conducted at conventional frequencies (0.125, 0.25, 0.5, 1, 2, 3, 4, 6 and 8 kHz) using an Equinox Interacoustics audiometer with Interacoustics TDH-39 headphones and also at extended high frequencies (10, 12.5, 14, 16 kHz) using circumoral Sennheiser HAD-200 headphones. For participants with air-conduction thresholds exceeding 20 dB HL between 0.25 and 4 kHz, bone conduction was also measured using an Interacoustics bone vibrator placed on the mastoid to exclude potential middle-ear pathology (air-bone gaps were all &lt; 15 dB HL).</p><p id="P8">Individuals were considered as normal-hearing if their threshold between 0.125 and 4 kHz did not exceed 20 dB HL, and thus assigned to the yNH or oNH groups depending on their age (see above). The inclusion criterion for individuals in the oHI group was a high-frequency sloping bilateral hearing loss with a threshold higher than 20 dB HL at 4000 Hz. Because several subjects clearly showed unreliable ABRs or EFRs/FFRs, we had to exclude one participant from the oHI group, two participants from the oNH group and one participant from the yNH group. In the end, the present study was made of the yNH group (n= 15, 12 females, age 21 ± 1), the oNH group (n= 16, 11 females, age 47 ± 6) and the oHI group (n= 14, 8 females, age 52 ± 6). The audiograms of the individuals of these three groups are plotted in <xref ref-type="fig" rid="F2">Figure 2A</xref>.</p><p id="P9">The study was approved by the ethical commission of Ghent University Hospital (UZ Ghent protocol number BC-08340 E01) and was conducted in accordance with the Declaration of Helsinki. All subjects were informed about the experimental procedures, provided written informed consent, and received financial compensation for their participation.</p></sec><sec id="S4"><title>Distortion product of otoacoustic emissions (DPOAEs)</title><p id="P10">OAEs were measured to assess OHC-integrity. We used the Universal Smart Box (Intelligent Hearing System) with Etymotic ER10D probes and analyzed with the corresponding software (SmartDPOAE and SmartEPCAM). For the DPgram, two tone pairs were presented simultaneously with f2=1.22*f1 and f2 ranging from 500 to 8000 Hz with 2 freqs/octave. The intensities were kept at 65 and 55 dB SPL for L1 and L2 respectively in a first run, (a second run was conducted with L1 and L2 at 55 and 40 dB SPL, respectively, but the results are not included in the present paper). 32 sweeps were taken and the SNR on each frequency was used for further analysis. DPOAE input/output thresholds were also measured at 1, 2 and 4 kHz as described by Verhulst et al. (2016) but are not included in the present analysis. Tone pairs were presented at f2=1.22*f1. The intensity of L2 ranged from 35 to 70 dB SPL and L1 was computed based on the Scissors paradigm of Kummer et al. (1998) (L1=0.4*L2+39). For TEOAEs, clicks of 80 <italic>μ</italic>s were produced at 80 dB SPL during 2000 sweeps but were not included neither. These values were finally noise-floor corrected, i.e. by replacing them with the noise floor amplitude when lower.</p></sec><sec id="S5"><title>Psychophysical assessment of S(T)M sensitivity</title><sec id="S6"><title>Stimuli</title><p id="P11">Three types of ripple stimuli were used, with parameters derived from previous psychoacoustical studies (<xref ref-type="bibr" rid="R5">Bernstein et al., 2013</xref>; <xref ref-type="bibr" rid="R6">2016</xref>; <xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>; <xref ref-type="bibr" rid="R85">Zaar et al., 2023a</xref>). We tested different SM configurations with both noise and harmonic carriers, as well STM with harmonic carriers, and hypothesized that they would lead to comparable results (<xref ref-type="bibr" rid="R85">Zaar et al., 2023</xref>). We used the same technique as described in details in recent studies (<xref ref-type="bibr" rid="R85">Zaar et al., 2023a</xref>) to synthesize these types of stimuli, so we only report here the main aspects of this synthesis technique as well as the specific parameters that were used in the present study.</p><p id="P12">Briefly, these stimuli are similarly synthesized using band-restricted carriers that are then multiplied by a modulator to impose a ripple structure on their envelope. The carrier is made of log-spaced sinusoids with random phases, with spacing on the frequency axis either extremely dense, resulting in a pink noise carrier, or following a harmonic structure to create a complex tone with a given fundamental frequency f<sub>0</sub>. The modulator is defined in the spectro-temporal domain, and is used to imposed either spectral modulations only or spectro-temporal modulations (STMs) on the envelope of the stimuli. The parameters of the modulator are: the modulation depth (parameter m, ranging from 0 to 1, corresponding to unmodulated to fully modulated), the phase (in radians), the scale or density (in cycles/octave, or c/o), and the rate or frequency (in Hz). In the present study, all the stimuli were 500-ms long (including 20-ms ramps at onset and offset) and had scales at 2 c/o, they varied depending on the type carrier (noise/harmonic) and the absence/presence of a temporal modulation. Importantly for the present study, the phase of the modulator was fixed at 2*pi/3, which was required because our goal was to assess the relationship between perceptual sensitivity and electrophysiological response, the latter requiring the repetition of the exact same stimulus over many trials. The parameters specific to each type of stimulus are reported below.</p></sec><sec id="S7"><title>SM<sub>noise</sub>: spectral modulation with noise carrier</title><p id="P13">The SM<sub>noise</sub> was constructed with noise carrier made by adding 400 random-phase, log-spaced sinusoids in the range [300-1500 Hz]. The modulator was only carrying spectral modulations with a scale of 2 c/o; the rate was 0 Hz.</p></sec><sec id="S8"><title>SM<sub>harm</sub>: spectral modulation with harmonic carrier</title><p id="P14">The SM<sub>harm</sub> was constructed with a harmonic carrier made by adding 13 random-phase sinusoids spaced every 110 Hz in the similar [300-1500 Hz] frequency range as for the SM<sub>noise</sub> (i.e. starting at 330 Hz up to 1430 Hz), resulting in a complex tone with a virtual fundamental frequency f<sub>0</sub> of 110 Hz. The modulator was only carrying spectral modulations with a scale of 2 c/o; the rate was 0 Hz.</p></sec><sec id="S9"><title>STM<sub>harm</sub>: spectro-temporal modulation with harmonic carrier</title><p id="P15">The STM<sub>harm</sub> was constructed exactly as SM<sub>harm</sub>, except that the modulator carrying spectral modulations with a scale of 2 c/o and also temporal modulations at a rate of 4 Hz. The 4 Hz value was chosen in light of previous studies (<xref ref-type="bibr" rid="R6">Bernstein et al., 2016</xref>; <xref ref-type="bibr" rid="R49">Miller et al. 2018</xref>) who found that the sensitivity to 2-c/o, 4-Hz STMs showed maximal correlations with SRTs. Bands of noise were also added below and above the [300-1500 Hz] frequency range of the carrier in order to mask potential temporal off-frequency cues that could be created by the temporal modulation (<xref ref-type="bibr" rid="R52">Narne et al., 2016</xref>). These noises were made by adding 100, random-phase, log-spaced sinusoids in the [200-300 Hz] (i.e. below) and [1500-1800 Hz] (i.e. above) regions, with levels 15 dB below the level of the sinusoids used to create the carrier.</p></sec><sec id="S10" sec-type="methods"><title>Procedure</title><p id="P16">An adaptive measurement procedure similar to previous literature on this topic was used to measure ripple sensitivity for the three types of S(T)M stimuli described above. Our reasoning regarding the choice of the tracking parameters was similar to <xref ref-type="bibr" rid="R85">Zaar et al. (2023)</xref>: our approach was to build a task that would not be too difficult and would provide fast and reliable estimates of thresholds, especially for listeners with hearing loss.</p><p id="P17">A three-interval two-alternative forced choice (3I-2AFC) procedure, where the modulation depth of the modulator was changed following a two-down, one-up procedure (<xref ref-type="bibr" rid="R85">Zaar et al., 2023</xref>). In all trials, the first stimulus was always unmodulated as a reference for the participant, and the modulated stimulus was presented either in the second or third interval with equal probability. Participants were instructed to select the stimulus (2<sup>nd</sup> or 3<sup>rd</sup>) that was different from the other two. The interstimulus interval was 300 ms. Within a given trial, the starting phases of the carrier components were identical (« frozen ») across the three intervals such that the only difference was the imposed spectro-temporal modulation on the carrier; these values were refreshed from trial to trial by drawing values from a uniform distribution [0, 2π]. The modulation depth value was here defined in decibels full scale (0 dB FS) simply by taking 20 log10 (m), where m is the linear depth; 0 dB FS thus corresponds to a fully modulated stimulus. As in <xref ref-type="bibr" rid="R49">Miller et al. (2018)</xref>, we used an initial step size of 6 dB, which was reduced to 4 dB after the first reversal for the next two reversals, and finally reduced to 2 dB until 6 reversals were reached. The thresholds used for the analysis were not those returned by an average of the modulation depth over the last reversals (see below).</p><p id="P18">Two interleaved tracks were collected per condition, and an additional third track was run if the difference between the thresholds measured (calculated here as the average modulation depth across these last 6 reversals) in the two tracks was greater than 3 dB. The modulation depth value was always limited to 0 dB FS. If the tracks did not converge because a given listener was unable to reach performance threshold by producing three consecutive incorrect responses at full modulation depth the procedure stopped and a novel experiment started using a constant stimulus procedure; this happened on 4 conditions for subjects from the oNH group (corresponding to ~8% of measurements) and 5 conditions for subjects from the oHI group (corresponding to ~12% of the measurements). This constant stimulus procedure was made of 60 trials presented in random order: 20 trials at full modulation depth i.e. 0 dB FS, 20 trials with modulation depth of -3 dB FS, and 20 trials with modulation depth of -6 dB FS. The threshold was then inferred from the responses collected to all these stimuli (see below).</p><p id="P19">In all cases, the thresholds were inferred from all the responses collected using a psychometric function fitting approach (detailed below) for two reasons; first because it allows us processing the data collected using a common approach for the adaptive and constant stimuli procedures, and second because we noticed after a detailed, visual inspection of the data of individual tracks with the adaptive procedure that some subjects exhibited large fluctuations around threshold, where the modulation depth first seemed to reach the threshold value, and then started to increase, which might result in poor threshold estimates. Although the reasons of why this happened were surprising to us and still remain unclear, it is an issue that could be related to task difficulty, or to the difficulty of deploying a constant attention over a certain amount time for some older and/or impaired individuals.</p><p id="P20">Because such fluctuations could lead to poorer estimates of underlying thresholds (a lack of precision that is important to address given that our goal was to examine the relationship between psychophysical thresholds and EEG data), we retained a method that, by allowing to reconstruct the whole psychometric function from all responses and then infer the threshold from this function, would be less affected by potential lapses of attention compared to a simple averaging of the modulation depth value over the final reversals. We therefore estimated the thresholds (in dB FS) corresponding to 75% correct responses after reconstruction of the whole psychometric functions for condition (using <italic>psignifit</italic> routine developed by <xref ref-type="bibr" rid="R80">Whichmann and Hill, 2001</xref>), regardless of the measurement procedure (adaptive or constant stimuli) used. A threshold was thus obtained for each individual and condition. We verified that replacing these thresholds inferred from the psychometric function by those returned by the adaptive procedure after simply averaging the last 6 reversals (an estimation of the 70.7% point on the psychometric function) yielded comparable results and would have led to similar conclusions.</p><p id="P21">All participants were tested identically on a dedicated laptop in a sound-proof room at UGent hospital (UZ). Stimuli were built and presented using an in-house Matlab program, converted through a professional soundcard (RME Fireface UCX) and delivered through headphones (Sennheiser HAD-300). Responses were entered through keyboard arrows (left arrow for 2<sup>nd</sup> interval, right arrow for 3<sup>rd</sup> interval). The whole experimental setup was calibrated using a Bruel &amp; Kjær 2238 Mediator sound-level meter, coupled with the mounting plate provided for circumaural headphones. For each participant, stimuli were presented monaurally to the same ear as for the other experiments. The stimulus presented in each interval was always normalized in level and presented at 75 dB SPL (rms), i.e. there was level roving. Participants with hearing impairment were tested unaided, without their hearing aids. Measurement of sensitivity for the three types of stimuli (SM<sub>noise</sub>, SM<sub>harm</sub>, STM<sub>harm</sub>) was made in three separate conditions, which were conducted in a random order for each participant. A short training was provided at the beginning of the task for each of the three stimulus conditions, with longer stimuli (1000 ms) and only large modulation depths (either full modulation depth of -6dB). During the whole experiment, participants received visual trial-by-trial feedback regarding the correctness of their response, to ensure maximal engagement of attention in the task.</p></sec></sec><sec id="S11"><title>Assessment of speech-in-noise intelligibility</title><p id="P22">Previous works highlighted a significant relationship between S(T)M detection thresholds and speech-reception thresholds (SRTs) in simple SPiN tasks (<xref ref-type="bibr" rid="R6">Bernstein et al., 2016</xref>; <xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). We therefore chose a standard monaural SPiN task based on a Matrix test with speech-shaped noise (SSN). Speech-in-noise intelligibility was assessed across three conditions, an unaltered, broadband (BB) condition, as well as in two frequency filtering, low-pass (LP) and high-pass (HP) conditions, detailed below. These additional conditions were designed to better tease apart the contribution of TFS and ENV cues. We reasoned that in the LP condition, participants could use both TFS and ENV cues whereas in the HP condition, only ENV cues could be used.</p><sec id="S12"><title>Stimuli and procedure</title><p id="P23">Speech intelligibility was measured using the Flemish Matrix sentence test (<xref ref-type="bibr" rid="R39">Luts et al., 2014</xref>), which consists of a closed set of non-predictable five-word sentences (10 names, 10 verbs, 10 numerals, 10 colors and 10 objects) pronounced by a single speaker. A different wordlist was used for each condition, but the same lists were used for all participants. Participants’ task was to click on the words they recognized among the list of all possible entries from the word-matrix presented on the computer screen to measure their speech reception thresholds. The speechshaped noise had a fixed intensity of 75 dB SPL in all the conditions described below. The speech level was varied (starting at noise level -4 dB) using a one-up, one-down adaptive procedure to track the speech reception thresholds corresponding to 50% of correct word recognition. Initial step size was 5 dB and was then decreased depending on the number of correct words in each trial, reaching values smaller than 1 dB in the last reversal. Thresholds were measured in three different frequency filtering conditions; and the same filtering was used for both the speech and noise material: broadband/BB (no filtering), low-pass/LP (cutoff at 1500 Hz) and high-pass/HP (cutoff at 1650 Hz). The noise always started 500 ms before and ended 500 ms after each sentence. We initially included an additional condition to measure of speech reception thresholds in quiet (SPiQ) in a LP filtering condition, but the starting level of the speech signal (75 dB SPL) was high and did not allow measurements to converge to the threshold of 50% of correct responses within the limit of 20 trials for a few individuals having low thresholds; results from this condition are therefore not considered in the subsequent analyses. Three participants from the oHI who had the highest (worst) hearing thresholds in the high frequencies were actually not able to perceive the noise in the HP condition, and therefore their SRTs rather reflect HP speech reception thresholds in quiet and not in noise; these subjects are highlighted in <xref ref-type="fig" rid="F3">Figure 3</xref> and following analyses were thus conducted both with and without these three subjects to check for the robustness of our results. The conditions were presented in different blocks; the BB condition was always presented first, followed by the LP and HP conditions whose order was randomized across participants. For each condition, measurement was made using two interleaved tracks. A short training block was used to familiarize participants with the task.</p><p id="P24">For each of the three noise conditions considered for the analyses, SRTs were computed using the mean SNR/level over the last six reversals of each track, and averaged over the two tracks. The experimental setup was the same as for the measurement of STM sensitivity described above. Importantly, the stimuli were delivered monaurally to each participant in the same ear as the one used for the other tests.</p></sec></sec><sec id="S13"><title>EEG measurements</title><sec id="S14"><title>EEG protocol and data acquisition</title><p id="P25">Measurements were conducted using the Universal Smart Box (Intelligent Hearing System) with the SmartEP continuous acquisition module (SEPCAM). Participants were seated in a reclining armchair in a quiet room, watching a silent movie in order to keep the participant relaxed and awake (Purcell et al., 2004). Ambu Neuroline snap electrodes were connected to vertex, nose and both mastoids after applying NuPrep® gel with a cotton swab for reducing the skin-impedance to a maximum of 3 kΩ. The sampling frequency was 10 kHz. The stimuli were presented with ER-2 inserts (Etymotic Research) and the ears were covered with earmuffs to minimize the noise intrusion level. All electrical devices other than the measurement equipment were turned off and unplugged.</p></sec><sec id="S15"><title>Auditory brainstem response (ABR)</title><p id="P26">ABR-responses were measured to obtain an extensive representation of the auditory nerve function. The used parameters are based on the studies of <xref ref-type="bibr" rid="R37">Liberman et al. (2016)</xref> and <xref ref-type="bibr" rid="R70">Vasilkov et al. (2021)</xref>. A click-stimulus of 100 <italic>μ</italic>s was presented with a rate of 11 Hz. Two intensities (75 and 90 dB peSPL (rms)) were measured, with both 3000 alternating sweeps, filtered between 10 and 1500 Hz. Positive and negative peaks were pointed on the ABR-curves manually by a professional audiologist. Both amplitudes and latencies of selected peaks were extracted from these data. ABR results are not detailed in the present paper; they were primarily included to verify the reliability of our EEG measurements based on previous data from our group collected using a similar protocol. In total, four participants were excluded from the analysis because they showed unreliable ABRs (see <xref ref-type="sec" rid="S3">Participants</xref> section).</p></sec><sec id="S16"><title>Frequency following response (FFR) with the RAM stimulus</title><p id="P27">The RAM stimulus was used to specifically assess ENV-coding fidelity (<xref ref-type="bibr" rid="R70">Vasilkov et al., 2021</xref>). It uses a high-frequency pure tone carrier at 4 kHz that was 100% rectangularly amplitude modulated at 110 Hz and with a duty cycle of 25% (RAM25) as described in <xref ref-type="bibr" rid="R70">Vasilkov et al. (2021)</xref>. The stimuli were presented at 75 dB SPL (rms) for 200 ms during 1000 alternating sweeps, and the signal was filtered between 10 and 5000 Hz.</p></sec><sec id="S17"><title>Frequency following response (FFR) with the SM<sub>harm</sub> stimulus</title><p id="P28">The SM<sub>harm</sub> stimuli were used to specifically assess TFS-coding fidelity. Two SM<sub>harm</sub> stimuli identical to those used for the psychophysical measurement; one with a 100% modulation depth (0 dB FS) and another one with a 50% modulation depth (i.e. – 6 dB FS). These stimuli were 400-ms long, had a level 75 dB SPL (rms), and were each presented using 3000 repetitions of alternating polarities monaurally to the participant’s best ear. The initial idea of using a stimulus with a 50% modulation, in addition to the stimulus with full modulation depth, was to see whether it could provide a contrast measure and therefore allow for a better assessment of the relationship with psychophysical modulation detection thresholds. Yet, we found no measurable and consistent differences between the responses to these sounds, so the responses collected with these two stimuli were aggregated to derive an average FFR for analysis.</p></sec><sec id="S18"><title>FFR pre-processing and analysis</title><p id="P29">All recordings were first band-pass filtered between 60 and 2000 Hz, in order to highlight the frequency range of interest for subcortical responses induced by our stimuli. The same pipeline was used to analyze the responses to the RAM and SM<sub>harm</sub> stimuli. For each stimulus type, we considered the responses recorded over the total duration for analysis. We first performed baseline correction by removing the mean computed over all responses. We then performed epoch rejection using a sliding window; the range of the signal (max – min) was extracted from each epoch, and an epoch was rejected when its range exceeded 3 times the median value computed over 200 epochs. For the RAM stimuli, an average number of 945 ± 36 (M±SD) trials were available for each participant for further analysis. For the SM<sub>harm</sub> stimuli (after aggregating the responses to the two modulation depths), an average number of 2848 ± 82 trials were available for each participant for further analysis.</p><p id="P30">We then followed the now widely adopted methodology for extracting the TFS and the ENV components from FFRs (<xref ref-type="bibr" rid="R1">Aiken &amp; Picton, 2008</xref>; <xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>), where FFRenv (sometimes also referred to as envelope following response, or EFR, in other studies) and FFR<sub>tfs</sub> are computed using the sum or difference of responses to positive and negative stimulus polarities, respectively. Although some works showed that these FFRs are corrupted by cochlear microphonics (Lucchetti et al., 2018; <xref ref-type="bibr" rid="R13">Borijin et al.,2022</xref>), if present, these effects would be similar across groups and would thus not explain the effects reported in that study. We then used a bootstrap-based correction algorithm to estimate the frequency spectra after noise-floor correction (Zhu et al., 2013, but see <xref ref-type="bibr" rid="R70">Vasilikov et al., 2021</xref>). Finally, a scalar value representing the overall magnitude of the responses (later used for statistical analyses) was computed by extracting and summing the magnitude of the harmonics (multiples of f0=110 Hz) that were significantly above noise-floor; we retained harmonics #1 to #7 for both the RAM stimulus and the SM<sub>harm</sub> stimulus.</p></sec></sec><sec id="S19"><title>Computational simulations with a model of the auditory periphery</title><p id="P31">We used a biophysically-inspired model of the auditory periphery (<xref ref-type="bibr" rid="R72">Verhulst et al., 2018</xref>) to simulate auditory nerve (AN) responses across the different types of fibers with high, medium and low spontaneous rates (H, M, L) to the exact same SM<sub>harm</sub> stimulus as for the FFR empirical characterization, i.e. with an input level of 75 dB SPL. AN responses were extracted to both positive and negative stimulus polarities, and either summed or subtracted (across all different types of fibers) to assess the ENV and TFS part of the Wave 1 response, respectively, exactly as for the FFR analyses. A FFT of the resulting response was then computed to derive its frequency spectrum, and the first seven harmonics were averaged. Different hearing profiles were tested to simulate the hearing characteristics of the three groups of participants engaged in the present study; we tested 3x3 combinations of OHC loss (no OHC loss; normal audiometric thresholds ~ up to 1-kHz, and then sloping audiogram above; flat audiometric thresholds ~30 dB HL up to 1-kHz, and then sloping audiogram above) and synaptopathy (number of H/M/LSR fibers at each CF: (13/3/3), (7/0/0), (4,0,0)).</p></sec><sec id="S20"><title>Statistical analyses</title><p id="P32">All statistical analyses were performed using R Statistical Software (v4.3.1; <xref ref-type="bibr" rid="R61">R Core Team 2022</xref>). Mixed analyses of variance (mixedANOVA) were conducted using a univariate approach with Huynh-Feldt correction for the degrees of freedom (Huynh &amp; Feldt, 1976); the ~ 2 correction factor <inline-formula><mml:math id="M1"><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:math></inline-formula> is reported, and partial <inline-formula><mml:math id="M2"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is reported as measure of association strength. Two-tailed t-tests were conducted as post hoc tests. Correlations were analyzed using Pearson correlation coefficient (r) and are reported in the text and the figures as uncorrected. Most correlations, unless mentioned otherwise, were computed after aggregating all individuals for the three groups (N=45). The significance level for all tests was set at <italic>a</italic> = 0.05; correction for multiple comparisons (Bonferroni) was considered for discussing the robustness of the effects when required. Multiple linear regressions were conducted using the lm function in R.</p></sec></sec><sec id="S21" sec-type="results"><title>Results</title><sec id="S22"><title>Hearing thresholds and OAEs</title><p id="P33"><xref ref-type="fig" rid="F2">Figure 2</xref> shows hearing sensitivity assessed at threshold using pure tone audiometry (PTA, <xref ref-type="fig" rid="F2">Fig. 2A</xref>), DPOAEs (<xref ref-type="fig" rid="F2">Fig. 2B</xref>) and the age of the individuals in our three groups (young normalhearing, yNH; older normal-hearing, oNH; older hearing-impaired, oHI; <xref ref-type="fig" rid="F2">Fig. 2C</xref>). ONH had higher hearing thresholds compared to yNH at both standard frequencies (t(29)=5.01, <italic>p</italic> &lt; .001) as well as extended high frequencies (t(29)=9.31, <italic>p</italic> &lt; .001). OHI had higher hearing thresholds compared to oNH at both standard frequencies (t(28)=7.66, <italic>p</italic> &lt; .001) as well as extended high frequencies (t(28)=7.65, <italic>p</italic> &lt; .001). The average differences in thresholds across frequencies below 1kHz, which corresponds to the frequency spectrum of the SM<sub>harm</sub> stimulus, were significant between yNH and oNH (t(29)=4.51, <italic>p</italic> &lt; .001) as well as between oNH and oHI (t(28)=2.70, <italic>p</italic> = .012). Yet, these differences remained in a relatively narrow range of 5-15 dB. As expected with our inclusion criterion, oNH and oHI groups exhibited strongest threshold differences above 1 kHz. The DPgram analysis (<xref ref-type="fig" rid="F2">Fig. 2B</xref>) shows similar results. When averaged across all frequencies, DPOAE amplitudes were higher for yNH compared to oNH (t(29)=4.58, <italic>p</italic> &lt; .001), and also higher for oNH compared to oHI (t(28)=4.35, <italic>p</italic> &lt; .001). When restricting these comparisons to the low-frequency range (below 1kHz), these differences were just significant after correcting for multiple comparisons (n=2); (yNH vs. oNH: t(29)=2.36, <italic>p</italic> = .05; oNH vs. oHI: t(28)=2.43, <italic>p</italic> = .04). As shown in <xref ref-type="fig" rid="F2">Fig. 2C</xref>, DPOAEs correlated with hearing thresholds when considering either the low-frequency (r=0.59, <italic>p</italic> &lt; .001, aggregating across all N=45 individuals) or the high-frequency (r=0.71, <italic>p</italic> &lt; .001, aggregating across all N=45 individuals) ranges. The DPOAEs of some individuals in the oHI group were around noise-floor level in the high-frequency range, which resulted in a flooring effect (see left panel). In sum, PTA and DPOAE measurements were consistent and strongly paralleled each other, supporting the view that both could be taken as alternative proxies of OHC damage. Together, these measurements suggest that the differences in OHC damage in the low frequencies (below 1 kHz) between our three groups, if any, could be considered as rather small, and that the main differences between groups were in the high frequencies and in the extended high frequencies.</p></sec><sec id="S23"><title>SRTs across frequency filtering conditions</title><p id="P34"><xref ref-type="fig" rid="F3">Figure 3A</xref> shows the results of the speech-in-noise tests, expressed in speech reception thresholds (SRT), for the three groups and the three frequency filtering conditions (BB, LP, HP). Descriptively, as expected, SRTs were lower (better) in the BB condition than in the LP and HP conditions where less cues are available. We conducted a mixedANOVA on the SRTs, with the group (yNH, oNH, oHI) as between-subject factor and condition (BB, LP, HP) as within-subject factor. We found a significant effect of group (<italic>F</italic>(2,42) = 7.58, <italic>p</italic> &lt; .001, <inline-formula><mml:math id="M3"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.265), a significant effect of condition (<italic>F</italic>(2,84) = 78.92, <inline-formula><mml:math id="M4"><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:math></inline-formula> = 0.632, <italic>p</italic> &lt; .001, <inline-formula><mml:math id="M5"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.653), and a significant group x condition interaction (<italic>F</italic>(4,84) = 9.63, <inline-formula><mml:math id="M6"><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:math></inline-formula> = 0.632, <italic>p</italic> &lt; .001, <inline-formula><mml:math id="M7"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.314). Post-hoc two tailed t-tests were then conducted to compare the three groups for each stimulus type; p-values (uncorrected) are reported <xref ref-type="fig" rid="F3">Fig. 3A</xref>. SRTs were lower for yNH compared to oNH, and for oNH compared to oHI in BB and HP conditions, and these differences were significant (Ps &lt; .05) after Bonferroni correction for multiple comparisons (n=3). In the HP condition, the difference between oNH and oHI was not significant after excluding the three outliers (see above), but other group differences remained significant. In comparison, there was no significant group differences in the LP condition after correction for multiple comparisons (n=3). To further address the potential factors limiting speech-in-noise understanding, we computed the correlations between the individual thresholds across these three conditions (see <xref ref-type="supplementary-material" rid="SD1">Fig. S1B</xref>, left panels). For the yNH group, SRTs in the BB were significantly correlated to the SRTs in both the LP (p &lt; .001) and HP conditions (p = .002). Interestingly, these analyses revealed that for the oNH group, SRTs in the BB were significantly correlated to the SRTs in the LP condition only (p = 0.03), while in contrast, for the oHI group, SRTs in the BB were significantly correlated to the SRTs in the HP condition only (p &lt; 0.001).</p></sec><sec id="S24"><title>Psychophysical detection thresholds for S(T)M sounds</title><p id="P35"><xref ref-type="fig" rid="F3">Figure 3B</xref> shows the results of the psychophysical tests assessing detection thresholds for the three groups and the three types of stimuli (SM<sub>harm</sub>, SM<sub>noise</sub>, STM<sub>harm</sub>). First, at a descriptive level, we observed a large variability across individuals in all conditions, with differences amounting 5 to 15 dB between extreme listeners, with no obvious differences between groups. Thresholds were comparable between SM<sub>harm</sub> and SM<sub>noise</sub>, in the range of -10 to -12 dB; in comparison, they were higher (worst) for the STM<sub>harm</sub> stimulus. A mixedANOVA conducted on these thresholds, with the group (yNH, oNH, oHI) as between-subject factor and stimulus type (SM<sub>harm</sub>, SM<sub>noise</sub>, STM<sub>harm</sub>) as within-subject factor, supported these observations. There was a significant effect of stimulus type only (<italic>F</italic>(2,84) = 21.39, <inline-formula><mml:math id="M8"><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:math></inline-formula> = 0.806, <italic>p</italic> &lt; .001, <inline-formula><mml:math id="M9"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.338); the effect of group was not significant (<italic>F</italic>(2,42) = 0.95, p=.396, <inline-formula><mml:math id="M10"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.043) and the group x stimulus type interaction was also not significant (<italic>F</italic>(4,84) = 0.92 <inline-formula><mml:math id="M11"><mml:mover accent="true"><mml:mi>ε</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:math></inline-formula> = 0.806, <italic>p</italic> = .456, <inline-formula><mml:math id="M12"><mml:msubsup><mml:mi>η</mml:mi><mml:mi>p</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> = 0.042).</p><p id="P36">To address the extent to which the same auditory processes are engaged with the different types of stimuli, we tested whether the inter-individual variability of thresholds for one stimulus type was related to the inter-individual for another stimulus type. We computed the correlations across conditions, after aggregating all individuals from the three groups together (see <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). These correlations were significant between SM<sub>harm</sub> and SM<sub>noise</sub> (p &lt; .001) as well as between STM<sub>harm</sub> and SM<sub>noise</sub> (p = .007), but were not significant between SM<sub>harm</sub> and STM<sub>harm</sub>(Ps &gt; .05) These results thus only partly support our hypothesis that the thresholds measured for these different types of stimuli reflect the exact same combination of processes.</p></sec><sec id="S25"><title>FFRs to the RAM tone and the SM<sub>harm</sub> stimulus</title><p id="P37">These analyses were not conducted on FFR<sub>tfs</sub> (bottom), and are referred to as NA (not applicable).</p><p id="P38"><xref ref-type="fig" rid="F4">Figures 4</xref> and <xref ref-type="fig" rid="F5">5</xref> show the average ENV and TFS components of FFRs, i.e. FFR<sub>env</sub> and FFR<sub>tfs</sub>, measured for the three groups of listeners (yNH, oNH, oHI) with the RAM tone and the SM<sub>harm</sub> stimulus, respectively. For the RAM tone (<xref ref-type="fig" rid="F4">Fig. 4</xref>), clear peaks are observed at various harmonics of the fundamental frequency (f<sub>0</sub> = 110 Hz) in the FFR<sub>env</sub> (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, top panels). In sharp contrast, FFR<sub>tfs</sub> of the RAM tone does not carry any clear structure; peaks are in most cases not even above noise floor (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, bottom panels). This difference in response between FFR<sub>env</sub> and FFR<sub>tfs</sub> is expected because the RAM tone uses a high-frequency carrier at 4 kHz, which is far above the limit of TFS coding, so only its ENV is encoded and this is reflected in the FFR. For the SM<sub>harm</sub> stimulus, peaks at multiple of f<sub>0</sub> can be observed in both the FFR<sub>env</sub> and FFR<sub>tfs</sub>; yet, it is important to note that the y-scale is reduced by a factor of 10 between <xref ref-type="fig" rid="F4">Figure 4</xref> and <xref ref-type="fig" rid="F5">5</xref>. These data for the SM<sub>harm</sub> corroborate previous observations with complex tones or vowels showing that both the ENV and the TFS are neurally encoded, yielding FFR<sub>env</sub> and FFR<sub>tfs</sub> spectra with peaks of comparable magnitude. Also, as expected, the peaks in FFR<sub>env</sub> were present for lower harmonics ((#1 to #4) for each individual and type of stimuli while the peaks in FFR<sub>tfs</sub> were mostly visible at higher harmonics (#4 to #6).</p><p id="P39">Descriptively, these plots show higher responses in all cases for yNH compared to oNH, and also for oNH compared to oHI listeners. To analyze these group differences more quantitatively, we followed previous approaches: we extracted the average magnitude of the seven first harmonics (which contain most of the energy related to the neural phase-locking to the stimulus ENV and TFS components) in the spectra of FFR<sub>env</sub> and FFR<sub>tfs</sub> to compute one single scalar value for each individual, type of stimulus, and components (ENV / TFS), except for the TFS component of the FFR obtained with the RAM tone that showed no measurable structure. This scalar, then expressed in dB, reflects the strength of coding of the ENV / TFS component and is here referred to as FFR<sub>env</sub> and FFR<sub>tfs</sub> strength, respectively. We considered the same harmonics #1 to #7 for analyzing the results obtained with the RAM tone and the SM<sub>harm</sub>, but we verified that our results were robust to other choices of harmonic numbers. The results of these analyses are presented in <xref ref-type="fig" rid="F4">Figures 4B</xref> and <xref ref-type="fig" rid="F5">5B</xref>; t-tests were used to compare the three groups, and uncorrected p-values are directly reported in the panel of these figures. There was a clear primary effect of age: yNH had significantly greater FFR strength than oNH or oHI, and this effect was found considering either FFRenv or FFR<sub>tfs</sub>, and considering for both the RAM tone and the SM<sub>harm</sub> stimulus. Second, there was an effect of hearing thresholds: oNH had significantly greater FFR strength than oHI, and this effect was found considering either FFRenv or FFR<sub>tfs</sub> components obtained with the RAM tone, but not for any of the components derived with the SM<sub>harm</sub> stimulus (the difference observed between oNH and oHI for the FFR component at p = .02 did not survive correction for multiple comparison, n=3).</p><p id="P40">Using correlational analyses, we examined the relationships between the scalar indexes reflecting FFRenv and FFR<sub>tfs</sub> strength for the two types of stimuli, the RAM tone and the SM<sub>harm</sub> stimulus, which primarily engage higher and lower portions of the CFs, respectively. We found that these measurements were significantly related (<xref ref-type="supplementary-material" rid="SD1">Fig S3</xref>). We found strong and significant correlations between the FFRenv obtained with the two stimuli (r=0.69, <italic>p</italic> &lt; .001; <xref ref-type="supplementary-material" rid="SD1">Fig. S3A</xref>), and between the FFRenv and the FFR<sub>tfs</sub> obtained with the SM<sub>harm</sub> (r=0.70, <italic>p</italic> &lt; .001; <xref ref-type="supplementary-material" rid="SD1">Fig. S3B</xref>). The correlation between the FFRenv obtained with the RAM tone and the FFR<sub>tfs</sub> obtained with the SM<sub>harm</sub> was also significant (r=0.37, <italic>p</italic> = .01; <xref ref-type="supplementary-material" rid="SD1">Fig. S3C</xref>), and appeared to be primarily driven by higher harmonics (harmonic #4 to harmonic #7; see <xref ref-type="fig" rid="F3">Fig. 3C</xref>). In what follows, we only consider RAM-FFRenv as a marker of neural ENV coding to assess the relationship with other variables, as it corresponds to the ENV-based marker with the highest signal-to-noise ratio.</p></sec><sec id="S26"><title>Relationships between SRTs and psychophysical and neural markers of TFS coding</title><p id="P41">We here further addressed whether the psychophysical or electrophysiological measurements presented above would allow accounting for the variance in SPiN thresholds across listeners. In a first step, we computed simple regressions between SRTs and individual variables from our empirical measurements. We first examined the relationships between SRTs in the BB, HP and LP conditions, and FFRs (<xref ref-type="fig" rid="F6">Figure 6A</xref>). For the SM<sub>harm</sub> stimulus, used to assess TFS coding fidelity in the low frequencies, all correlations were not significant (Ps &gt; .05). For the RAM tone stimulus, used to assess ENV coding fidelity in the high frequencies, the correlations between FFRenv and SRTs in the BB and HP conditions were significantly negative (r=-0.47, <italic>p</italic> = .001 and r=-0.46, <italic>p</italic> = .001, respectively). The interpretation of these correlations is further discussed in the next section, where other predictors such as PTA or age are included in the models. Next, we examined the relationships between SRTs in BB and LP conditions and psychophysical thresholds obtained in particular with SM<sub>harm</sub> and SM<sub>noise</sub> (<xref ref-type="fig" rid="F6">Figure 6B</xref>) for which our measurements seem most reliable (see above). These correlations were non-significant (for SRT in the LP condition and the thresholds for the SM<sub>noise</sub>, the correlation was r=0.34, <italic>p</italic> = .021 before PTA correction, and r = 0.32; <italic>p</italic> = 0.03 after PTA correction, but this was not significant after correction for multiple comparisons). Finally, we analyzed the relationship between FFR<sub>tfs</sub> and detection thresholds collected with the SM<sub>harm</sub>, both hypothesized as potential proxies of TFS coding (see <xref ref-type="fig" rid="F6">Fig. 6C</xref>). We found no significant correlation between both measurements (<italic>p</italic> &gt; .05)</p><p id="P42">In a second step, we tested the extent to which multiple linear regression models including several predictors in addition to the FFR-based predictors could account for the interindividual variability in SRTs. We adopted a hypothesis-driven approach, starting from the full model (without interactions) that included all possible predictors, and progressively removed them one by one until we converged toward a stable model with a minimum of significant predictor(s) remaining. The outputs (estimates, p-values) of the different regressions are shown in Table S1; we only briefly summarize the main conclusions below.</p><p id="P43">The full model included five different predictors, FFR<sub>tfs</sub> from the SM<sub>harm</sub> stimulus (or SM-FFR FFR<sub>tfs</sub>), FFR<sub>env</sub> from the RAM tone (or RAM-FFR<sub>env</sub>), hearing thresholds in the low frequencies 0.125-1 kHz (HT<sub>LF</sub>), hearing thresholds in the high frequencies 2-8 kHz (HT<sub>LF</sub>), SM<sub>harm</sub> psychophysical thresholds, and age.</p><p id="P44">First, we aimed to predict SRT in the BB condition. The full model returned only HT<sub>HF</sub> as a significant predictor (Est=0.08; p &lt; .001). Yet, it is known that HT<sub>HF</sub> obscures the potential effect of our predictors of synaptopathy; HT<sub>H</sub> is found to enhance synaptopathy and thus be correlated with these predictors. When HT<sub>HF</sub> is removed, RAM-FFR<sub>env</sub> indeed became significantly negatively correlated (Est=-0.09; p =0.03). Table 1 shows that this significance hold for all nested models, when other predictors were removed one by one. The same conclusion was reached when yNH subjects were excluded. In sum, there is a robust negative correlation between SRT in the BB condition and RAM-FFR<sub>env</sub>. Next, we followed the exact same approach to account for SRT in the HP condition. We reached the same conclusion as with SRT in the BB condition: there was a robust negative correlation between SRT in the HP condition and RAM-FFR<sub>env</sub>. This correlation still held after removing other predictors one by one, and also when the three outlier subjects in this task (see above) were excluded. Thirdly and lastly, we aimed predict SRT in the LP condition. The full model returned SM-FFR<sub>tfs</sub> and age as significant predictors (p &lt; .05). Following the same reasoning, we removed age as predictor in the model, since age is known to enhance synaptopathy, and as such may obscure the true effect of other predictors. When this was done, no significant predictors remained. Age was therefore put back to the model, and the other predictors were removed one by one. Once this was done, SM-FFR<sub>tfs</sub> and age became again significant predictors (p &lt; .05), showing positive correlations with SRT. We asked whether these observations related to the age factor would hold after excluding yNH individuals from the regression, since the main age difference between our subjects is driven by individuals from that group. When doing so, age was not a significant predictor anymore, and FFR<sub>tfs</sub> was the only significant predictor in all models tested.</p></sec><sec id="S27"><title>Relationships between age on FFR<sub>tfs</sub> and FFR<sub>env</sub> assessed using correlational analyses</title><p id="P45">To complement the age-related effects highlighted by the group-level comparisons (see above), we used a correlational approach to further assess the effect of age on RAM-FFRs and SM-FFR<sub>tfs</sub>. For these analyses, we considered the individuals from the oNH and oHI groups only to avoid any ‘bias’ induced by the lower average age of the yNH group. We did not find any significant correlation between RAM-FFR<sub>env</sub> and age (<italic>p</italic> &gt; 0.05; see <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>). However, we found a significant correlation between SM-FFR<sub>tfs</sub> and age (<xref ref-type="fig" rid="F7">Fig. 7A</xref>, r=-0.47; <italic>p</italic> = 0.008). Because PTA and age are two intrinsically linked and highly correlated factors, we tested whether this correlation would hold after accounting for variation in PTA. To perform this correction, we first conducted a linear regression between SM-FFR<sub>tfs</sub> and PTA and then assessed the relationship between the residuals from this regression, which reflect the variance in SM-FFR<sub>tfs</sub> that cannot be explained by the variance in PTA with age; we found that the correlation remained significant after correcting for PTA (<xref ref-type="fig" rid="F7">Fig. 7B</xref>, r=-0.45; <italic>p</italic> = 0.013). We then asked which specific harmonic components of the SM-FFR<sub>tfs</sub> would primarily underlie this correlation. To do so, we conducted correlations between the magnitude of individual harmonics (from #1 to #7) and age, and found that only magnitude for harmonic #4 was significant (r=-0.57; <italic>p</italic> = 0.001), even after adjusting for PTA (r=-0.55; <italic>p</italic> = 0.002). This result indicated that the SM-FFR<sub>tfs</sub> correlation with age was primarily driven by harmonic #4 (the only correlation that remained significant after Bonferroni correction for multiple comparisons, n=7), which incidentally also corresponds to the harmonic of highest energy contribution to the total FFR.</p><p id="P46">The fact that a negative correlation was observed only with the TFS-based FFR marker but not with the ENV-based FFR marker can likely be understood by considering the differences in frequencies covered by their respective carrier stimuli. Indeed, while the SM<sub>harm</sub> covers lower frequencies (&lt;1.5 kHz) at which older individuals all had comparable hearing thresholds (<xref ref-type="fig" rid="F2">Fig. 2A</xref>), the RAM tone covers high frequencies where differences in hearing thresholds also contribute to the variability among oNH and oHI listeners and would presumably make the relationship with age less clear.</p></sec><sec id="S28"><title>Impacts of AN synaptopathy and OHC damage on FFR strength estimated from simulations with a computational model of the periphery</title><p id="P47">We run computational simulations using a biophysically-inspired modeled of the auditory periphery (<xref ref-type="bibr" rid="R74">Verhulst et al., 2018</xref>) to assess quantitatively the effects of OHC damage and cochlear synaptopathy on FFR derived with the SM<sub>harm</sub> (modulation depth of 100%). Results are shown in <xref ref-type="fig" rid="F8">Figure 8</xref>. While amounts of OHC damage consistent with the potential loss in our cohorts of listeners would lead to small increases of FFR<sub>tfs</sub> strength of 1 to 3 dB, a simulation of different synaptopathy, here modeled as a partial loss of HSR fibers and total loss of MSR and LSR fibers (see the legend of <xref ref-type="fig" rid="F8">Fig. 8</xref> for details), leads to a stronger decrease of FFR<sub>tfs</sub> strength in the range 7-15 dB, consistent with the effects highlighted in <xref ref-type="fig" rid="F5">Figure 5B</xref>. It is important to note that very similar effects are predicted by the model on FFR<sub>env</sub>.</p></sec></sec><sec id="S29" sec-type="discussion"><title>Discussion</title><p id="P48">In this study, we investigated the impact of age and hearing loss on the neural and perceptual coding of the temporal envelope (ENV) and temporal fine structure (TFS) in complex sounds. We conducted frequency-following responses (FFRs) and psychophysical measurements in three groups of listeners: young normal-hearing (yNH), older normal-hearing (oNH), and older hearing-impaired (oHI). Our primary goals were to (1) evaluate whether these measures could serve as potential proxies for cochlear synaptopathy (CS) and (2) assess their ability to predict interindividual variability in speech-in-noise (SPiN) performance. To assess ENV coding, we used a 110-Hz rectangular amplitude-modulated (RAM) tone with a 4-kHz high-frequency carrier, based on the methodology of <xref ref-type="bibr" rid="R70">Vasilikov et al. (2021)</xref>. For TFS coding, we employed a low-frequency (300-1500 Hz), harmonic complex tone (f<sub>0</sub>= 110 Hz) with spectral modulations (SM<sub>harm</sub>), a stimulus derived from psychoacoustic studies on supra-threshold hearing deficits (<xref ref-type="bibr" rid="R6">Bernstein et al., 2016</xref>; <xref ref-type="bibr" rid="R85">Zaar et al., 2023</xref>a). This dual approach allowed us to assess both ENV and TFS coding at the electrophysiological (FFR) level and via psychophysical modulation detection thresholds. Speech-in-noise performance was evaluated in three conditions: unaltered broadband (BB) stimuli, low-pass (LP) filtered stimuli, and high-pass (HP) filtered stimuli. The LP and HP conditions were included to compare the relative contributions of ENV vs. TFS cues in speech perception.</p><sec id="S30"><title>SRTs across low-pass and high-pass frequency filtering conditions point towards different sources of neural coding deficits</title><p id="P49">We used the approach of recent studies that consists in low-pass / high-pass filtering the speech and the noise material to preferentially target the frequency ranges in which the ENV / TFS coding mechanisms operate, respectively (<xref ref-type="bibr" rid="R72">Verhulst and Warzybock, 2018</xref>, <xref ref-type="bibr" rid="R27">Garrett et al., 2024</xref>). Overall, results of speech-in-noise tests (see <xref ref-type="fig" rid="F3">Fig. 3</xref>) exhibited lower (better) SRTs in the BB compared to the LP and HP conditions for all groups, a trend that is expected because less cues are available in the frequency-filtered conditions compared to the intact (unfiltered) condition. In both BB and HP condition, we found that SRTs for yNH were significantly lower than for oNH, and that SRTs for oNH were significantly lower than oHI (although the latter was not robust for the HP condition when outlier subjects were excluded). In contrast, in the LP condition, there was no significant difference between groups. These results can be best understood by considering the PTAs of the participants, which differed mainly in the high frequencies between the three groups (<xref ref-type="fig" rid="F2">Fig. 2</xref>), therefore primarily limiting their processing capacities in that region. To further understand the engaged processes, we examined the relationships between SRTs across the different frequency filtering conditions. These correlation analyses suggest that SPiN performance of oNH and oHI individuals would be degraded by different components of hearing. The correlation between SRTs in BB and HP conditions observed for oHI individuals suggests that their deficits are primarily limited by differences in hearing thresholds in HFs. Consistent with that interpretation, we found a significant correlation between SRT in the HP condition and hearing thresholds in HFs for the oHI group, but not for the oNH group in which individuals among that group exhibit similar, close-to-normal thresholds (<xref ref-type="supplementary-material" rid="SD1">Fig. S1B</xref>, right panels). In contrast, the correlation between SRTs in BB and LP conditions observed for oNH, who have similar hearing thresholds at all frequencies, suggests that their deficits would be primarily linked to their processing of LFs, possibly related to TFS processing. In sum, the correlational analysis based on the three conditions portraits various deficit profiles for oNH and oHI listeners, but the results highlighted by the psychophysical and neural markers considered in that study provide important information to go beyond these observations (see below).</p></sec><sec id="S31"><title>FFRenv to RAM tone: a proxy of phase-locking capacities to ENV in the high frequencies</title><p id="P50">First, our results with the 4-kHz RAM tone replicate previous findings: FFR<sub>env</sub> (often referred to as EFR in prior studies) led to a clear response indicating strong phase-locking to the rectangular envelope fluctuations of the sound (f0 of 110-Hz), with strong peaks related the first harmonics, from #1 to #4 (<xref ref-type="bibr" rid="R70">Vasilikov et al., 2021</xref>; <xref ref-type="bibr" rid="R31">Keshishzadeh et al., 2020</xref>; <xref ref-type="bibr" rid="R48">Mepani et al., 2021</xref>; <xref ref-type="bibr" rid="R68">Van der Biest et al., 2023</xref>). As in previous studies (<xref ref-type="bibr" rid="R70">Vasilikov et al., 2021</xref>; <xref ref-type="bibr" rid="R31">Keshishzadeh et al., 2020</xref>), we found clear and significant differences of FFR<sub>env</sub> strength between all three groups of listeners (<xref ref-type="fig" rid="F4">Fig. 4A</xref>), indicating effects of both age and hearing loss on ENV coding. Correlational analyses were partly consistent with these results. FFR<sub>env</sub> was strongly negatively correlated to hearing thresholds (r=-0.59, <italic>p</italic> &lt; .001), but the correlation with age was not significant (p &gt; .05). The large variability in hearing thresholds among older participants likely hindered the relationship with age. Although CS is expected to increase with both age and hearing loss, the present findings are in line with previous studies showing that correlations between FFR<sub>env</sub> and hearing thresholds can be observed, but not between FFR<sub>env</sub> and age directly (<xref ref-type="bibr" rid="R31">Keshishzadeh et al., 2020</xref>). These results suggest that FFR<sub>env</sub> is primarily sensitive to the impact of CS on ENV processing in the high frequencies (<xref ref-type="bibr" rid="R70">Vasilikov et al., 2021</xref>). The contribution of FFR<sub>env</sub> to account for the variability in SRTs is further discussed with respect to the multiple regression analyses (see below). In contrast, and as expected since the 4-kHz carrier tone is at a frequency above the human limit of phase-locking to TFS (<xref ref-type="bibr" rid="R76">Verschooten et al., 2019</xref>), the FFR<sub>tfs</sub> obtained with the RAM tone does not show phase-locking locking to the modulation frequency and its harmonics (<xref ref-type="fig" rid="F4">Fig. 4A</xref>).</p></sec><sec id="S32"><title>FFR<sub>tfs</sub> to a SM<sub>harm</sub> stimulus: a proxy of phase-locking capacities to TFS in the low frequencies</title><p id="P51">Results obtained with the SM<sub>harm</sub> stimulus show that the FFR was driven by both the ENV and the TFS components contained in that low-frequency stimulus. As for the RAM tone, significant differences observed between the three groups again indicate that FFR strength is negatively impacted by both hearing loss and age. The SM<sub>harm</sub> also led to a FFR showing clear phase-locking to the envelope of the stimulus at 110 Hz, which corresponds to its fundamental frequency (FFR<sub>env</sub>; see <xref ref-type="fig" rid="F5">Fig. 5A</xref>), and the first 3-4 harmonics, as observed with the RAM tone. However, the amplitude of that response was not as high as for the RAM-FFR<sub>env</sub>, a result that can be easily understood given that the temporal envelope of the SM<sub>harm</sub> is not defined as strongly as for the RAM tone where by construction the envelope amplitude is decreased by 95% at every period. As with the RAM tone (see above), SM-FFR<sub>env</sub> was correlated to hearing thresholds (r=-0.69, <italic>p</italic> &lt; .001) but was not correlated to age (p &gt; .05). Compared to the RAM stimulus that did not yield any FFR<sub>tfs</sub>, the SM<sub>harm</sub> stimulus led to a measurable FFR<sub>tfs</sub>. It evidenced a clear harmonic structure, with the harmonic of highest amplitude matching harmonic #4 (440 Hz) corresponding to that nearest to the first formant peak (or here the ‘spectral modulation in the acoustical waveform. This observation is consistent with previous reports (e.g. <xref ref-type="bibr" rid="R32">Krishnan, 2002</xref>; <xref ref-type="bibr" rid="R1">Aiken &amp; Picton, 2008</xref>; <xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>, <xref ref-type="bibr" rid="R3">2017</xref>).</p><p id="P52">We found that SM-FFR<sub>tfs</sub> was correlated to hearing thresholds (r=-0.38, <italic>p</italic> = .009), but that it was also strongly correlated to age, a correlation that remained significant after adjusting for hearing thresholds (see <xref ref-type="fig" rid="F7">Fig. 7A and 7B</xref>). This result reveals that SM-FFR<sub>tfs</sub> is particularly sensitive to the impact of age, and that it more directly reflects TFS coding fidelity in the low frequencies without being additionally affected by other potential differences in OHC damage. These data provide additional evidence that aging impacts the strength of the TFS component of the FFR. Many previous works indeed reported effects of aging on FFR<sub>tfs</sub> to tones or vowellike stimuli. A strong magnitude reduction of FFRs with vowel-like stimuli (complex tones / vowel stimuli / voiced speech) was observed in older listeners with hearing loss compared to young normal-hearing listener, and that reduction was interpreted as primarily reflecting the effect of aging (<xref ref-type="bibr" rid="R22">Clinard and Cotter, 2015</xref>, <xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>). A reduction in FFR<sub>tfs</sub> strength was observed to static or dynamic pure tones (<xref ref-type="bibr" rid="R20">Clinard et al., 2010</xref>; <xref ref-type="bibr" rid="R46">Marmel et al., 2013</xref>; <xref ref-type="bibr" rid="R44">Märcher-Rørsted et al., 2022</xref>), as well as for more complex stimuli or speech-like sounds (<xref ref-type="bibr" rid="R4">Anderson et al., 2012</xref>, <xref ref-type="bibr" rid="R8">Bidelman et al., 2014</xref>; <xref ref-type="bibr" rid="R43">Mamo et al., 2016</xref>; <xref ref-type="bibr" rid="R18">Carcagno and Plack, 2020</xref>) for older compared to younger individuals, all having similar NH thresholds in the frequency range of the stimuli. FFRs to vowel sounds were also found to be smaller in HI compared to NH participants (<xref ref-type="bibr" rid="R2">Ananthakrishnan et al., 2016</xref>; <xref ref-type="bibr" rid="R50">Molis et al. 2023</xref>), and more specifically so concerning their TFS component. This evidenced reduction of FFR in older individuals is consistent with a degradation of TFS-coding with aging.</p><p id="P53">Yet, only few studies directly addressed the potential effect of CS on their results (<xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>; <xref ref-type="bibr" rid="R44">Märcher-Rørsted et al., 2022</xref>). The analysis of our data in light of computational simulations suggest that the size of the difference in FFR<sub>tfs</sub> strength between extreme individuals of ~15-20 dB is consistent with a loss of ~50-80% of HSR fibers, which is line with recent measurements obtained in histopathological human bone analyses (<xref ref-type="bibr" rid="R83">Wu et al., 2019</xref>, 2021). Importantly, computational simulations show that this decrease cannot be explained by OHC damage (<xref ref-type="fig" rid="F8">Fig. 8</xref>), which would in contrast predict an increase – yet, minor – of the response. Even though CS since cannot practically be distinguished from IHC loss in the model used (<xref ref-type="bibr" rid="R72">Verhulst et al., 2018</xref>), recent studies already showed that a loss of IHC would have yielded much smaller effects (see <xref ref-type="bibr" rid="R44">Märcher-Rørsted et al., 2022</xref>). Moreover, another model of the auditory periphery (Zilany et al., 2014; URear) gave us decrease in FFR<sub>tfs</sub> strength of comparable magnitude (not shown) to those reported here using the model from <xref ref-type="bibr" rid="R72">Verhulst et al. (2018)</xref>. Therefore, the effects observed here are best accounted for by a decrease in the number of AN fibers, a prediction that appears to be robust.</p><p id="P54">The present model simulations suggest that a total removal of LSR and MSR fibers is not sufficient to predict any decrease in FFR strength of comparable magnitude to the one we observed empirically, and that a loss of ~50-80% of HSR fibers seems required. This result appears consistent with previous reports that FFRs to high sound levels would primarily result from neural encoding in HSR AN fibers through off-frequency contributions (<xref ref-type="bibr" rid="R25">Encina Llamas et al., 2019</xref>; <xref ref-type="bibr" rid="R18">Carcagno and Plack, 2020</xref>; <xref ref-type="bibr" rid="R66">Temboury-Gutierrez et al., 2024</xref>). If this hypothesis is true, adding high-frequency noise to mask off-frequency contributions of SM<sub>harm</sub> stimuli could yield a different pattern of result, in particular the observed negative correlation with age could be different (<xref ref-type="bibr" rid="R18">Carcagno and Plack, 2020</xref>). However, all of this remains to be addressed empirically.</p></sec><sec id="S33"><title>Integrating ENV-based and TFS-based markers: age-related effects reflecting a common deficit in neural phase-locking</title><p id="P55">An important result that emerged from our analyses concerns the fact that the age-FFR<sub>tfs</sub> relationship was specifically and robustly driven by the magnitude of the 4<sup>th</sup> harmonic of the FFR<sub>tfs</sub> or, alternatively, by the sum of the magnitude of harmonics #4-#7 (<xref ref-type="fig" rid="F7">Fig. 7</xref>). In sharp contrast, there was no correlation between age and the magnitude of the lower harmonics #1-#3 either considered individually or as a sum. This result suggests that age, and thus CS, would primarily impact phase locking capacities of cochlear neurons or AN fibers at quite high rates of ~500 Hz. This is in line with previous studies that reported a decrease in FFR<sub>tfs</sub>, in which the frequency static or dynamic tones was above 500 Hz (<xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>; <xref ref-type="bibr" rid="R45">Märcher-Rørsted et al., 2022</xref>). Consistent with this result, it is interesting to note that the correlation between RAM-FFRenv and SPiN scores reported in <xref ref-type="bibr" rid="R48">Mepani et al. (2021)</xref> was found to be specifically driven by the high harmonics (harmonics #3-#5) of the FFRenv, corresponding to frequencies between 360 and 600 Hz. High-frequency carrier sounds modulated in amplitude and complex tones with low frequency components both monitor the capacity of auditory neurons to phase-lock to fast temporal information, the only difference being that this tracking concerns the ENV component of sound for neurons tuned to high CFs vs. the TFS component of sound for neurons tuned to low CFs, respectively, If this reasoning is correct, we would expect a relationship between the strength of the RAM-FFR<sub>env</sub> and the strength of the SM-FFR<sub>tfs</sub>. This prediction was strongly supported by our data: we found a strong correlation between the two markers (p&lt; .01), and more strikingly, this correlation was driven by the summed magnitude of the high harmonics (#4-#7) in both markers (r=0.53, <italic>p</italic> &lt; .001), and even more precisely by the magnitude of harmonic #4 (r=0.44, <italic>p</italic> = .003). Even though this harmonic corresponds to the one of highest amplitude in the SM stimulus, making it difficult to disambiguate the specific role of its frequency from the fact that this is also the one mostly contributing to the input signal, it is important to note that the harmonic #4 in the RAM tone has no such dominating role in the input signal. Therefore, we believe that these correlations may rather suggest that the impact of age in low and high frequencies reflects a common decline in neural phase-locking for this frequency of ~500 Hz. Beyond this finding, these results also nicely illustrate that although TFS and ENV coding are considered as separate dimensions before and within the cochlea, they are then merged into a single firing rate code. From a more practical perspective, our results suggest that considering FFR to complex low-frequency tones would constitute an efficient strategy to characterize slow as well as fast neural phaselocking capacities (<xref ref-type="bibr" rid="R19">Chauvette et al., 2022</xref>). Indeed, previous studies reported that it was not possible to obtain reliable FFRs using RAM tones or noises made of low-frequency carriers, suggesting that it might be best to limit RAM-FFR<sub>env</sub> markers for the monitoring of CS to high-frequency carriers (<xref ref-type="bibr" rid="R31">Keshishzadeh et al., 2020</xref>).</p><p id="P56">We more globally characterized the relationships between the FFRs obtained with the RAM tone and the SM<sub>harm</sub>, as these stimuli are designed to differently engage processing of ENV / TFS cues in distinct frequency regions. There was a strong and significant correlation between the strength of the SM-FFR<sub>env</sub> and the strength of the RAM-FFR<sub>env</sub> (<xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>), suggesting that the deficit in ENV processing between the low and high frequencies would be strongly related. Since they rely on carriers that recruit specifically low or high frequencies, these responses likely reflect neural coding of sound engaged at different CFs. This result therefore suggests a strong relationship between the phase-locking capacities to temporal envelopes at different CFs on the cochlea. There was also also a strong correlation between SM-FFR<sub>env</sub> and SM-FFR<sub>tfs</sub>, which primarily reflect coding fidelity of ENV or TFS cues of that stimulus, respectively (<xref ref-type="bibr" rid="R1">Aiken and Picton, 2008</xref>). This result thus suggests a link between the degree of deficit in ENV and TFS processing in the low frequencies, although more work is needed to really tease apart the relationship between these two metrics, which likely not independently reflect the coding of ENV and TFS features of the stimulus. However, these correlations, and in particular the one between RAM-FFR<sub>env</sub> strength and SM-FFR<sub>tfs</sub> strength were limited, also suggesting that they might reflect partially distinct aspects of auditory processing. This observation would be in line with <xref ref-type="bibr" rid="R42">Mai et al. (2023)</xref> who found FFR significantly decreased with age and high-frequency (≥ 2 kHz) hearing loss and also suggested distinct roles of FFR<sub>env</sub> and FFR<sub>tfs</sub>.</p><p id="P57">Finally, our results suggest that the ability of neurons to extract fast rates of information in that neural code would be impaired by age-driven CS. Recent studies provide compelling evidence regarding for subcortical origins of speech-like FFRs and their relationship to SPiN capacities (<xref ref-type="bibr" rid="R10">Bidelman et al., 2020a</xref>, <xref ref-type="bibr" rid="R11">2020b</xref>), and more specifically that FFRs to these range of high frequencies primarily result from neural generators at the level of the auditory nerve, compared to lower frequencies that would result from neural generators in the inferior colliculus and cochlear nucleus (<xref ref-type="bibr" rid="R67">Tichko and Skoe, 2017</xref>; <xref ref-type="bibr" rid="R9">Bidelman et al. 2018</xref>). The present results are in full agreement with this view. Since the computational models used to simulate FFRs do not include multiple generators, removing AN fibers impacted FFRs similarly at all frequencies, resulting in a similar decrease predicted for the TFS and the ENV components of the FFR (see <xref ref-type="fig" rid="F8">Fig. 8</xref>), but we believe that this relationship is stronger in the case of TFS coding assessed in the low frequencies, where measurements are less additionally corrupted by the differences in the amount of outer-hair cell damage, since individuals have more comparable hearing thresholds in that range.</p></sec><sec id="S34"><title>Psychophysical detection of SM stimuli may not directly reflect TFS coding</title><p id="P58">Psychophysical detection thresholds for SM<sub>harm</sub> stimuli were surprisingly highly variable across individuals, and led to non-significant differences across groups. Also, thresholds were higher (poorer) than in previous studies that used very similar conditions and stimuli, such as those of <xref ref-type="bibr" rid="R6">Bernstein et al. (2016)</xref>, <xref ref-type="bibr" rid="R49">Miller et al. (2018)</xref>. We verified the robustness of our results to different analysis methods (reconstruction of the psychometric functions vs. averaging of the last reversals in the adaptive track). First of all, we observed an overall correlation between SM<sub>harm</sub> and SM<sub>noise</sub> thresholds, suggesting a similarity between the thresholds measured with a SM envelope imposed on a noise vs. a harmonic carrier, which is consistent with the findings from <xref ref-type="bibr" rid="R85">Zaar et al. (2023a)</xref>. Although previous studies observed higher (better) thresholds in the STM compared to the SM condition, consistent with the view that STM sounds provide an additional temporal cue for detecting the modulation compared to SM sounds that only carry a spectral, TFS-based cue (<xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>; <xref ref-type="bibr" rid="R85">Zaar et al., 2023a</xref>), here we found that the thresholds with the STM<sub>harm</sub> stimulus were higher and more variable, especially for yNH individuals, and did not correlate with thresholds for SM<sub>harm</sub>. It could be that our STM<sub>harm</sub> stimuli, which included sidebands of noise, sounded very different from SM<sub>harm</sub> and SM<sub>noise</sub> to the participants and could therefore introduce some confusion regarding the task they had to perform. Overall, our empirical measurements show that the correspondence between the individual thresholds across the different stimuli conditions only lead to moderate correlation coefficients, which makes the thresholds obtained for the three types of stimuli not directly comparable. Thus, we mostly considered in our analyses the thresholds obtained with the SM<sub>harm</sub>, as this is the stimulus used for the FFR measurements, and because they correlated with thresholds obtained for SM<sub>noise</sub>, which was generally the stimulus used in previous works showing correlations with SRTs (<xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>).</p><p id="P59">Contrary to previous works showing a correlation between SM thresholds and SRTs, we did not find any significant relationship between the two (<xref ref-type="bibr" rid="R5">Bernstein et al., 2013</xref>; <xref ref-type="bibr" rid="R6">2016</xref>; <xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). We believe that there are a number of experimental parameters that distinguish our study from previous studies, which may explain this apparent discrepancy. First of all, these results might be very sensitive to the type of SPiN task. <xref ref-type="bibr" rid="R85">Zaar et al. (2023a)</xref> found correlations between STM thresholds and SRTs when assessed in a complex multi-talker spatial SPiN task (speech and multi-talker babble with masker speakers located at different spatial positions), but not in a co-located SPiN task (speech and speech-shape noise colocated in space) that was comparable to previous works (<xref ref-type="bibr" rid="R49">Miller et al., 2018</xref>). Several other experimental aspects may also play a role: the age and degree of hearing loss variability of the cohort of participants included, as well as the sound level at which they were tested. Here, SRTs and S(T)M detection thresholds were measured with stimuli at the same intensity for all participants, while in previous studies there was a level compensation based on the audiogram of the participant. <xref ref-type="bibr" rid="R6">Bernstein et al. (2016)</xref> and <xref ref-type="bibr" rid="R49">Miller et al. (2018)</xref> measured SPIN with hearing aids and <xref ref-type="bibr" rid="R5">Bernstein et al. (2013)</xref> compensated for hearing loss by presenting the sentences at higher intensity (above 90 dB HL). <xref ref-type="bibr" rid="R85">Zaar et al. (2023a)</xref> also tested HI participants with some compensation of their hearing loss. With our procedure, some frequency portions of the stimuli could not be processed in listeners, and the extreme case this resulted in an overall level that was not loud enough for a few participants with highest thresholds. Furthermore, previous studies such as <xref ref-type="bibr" rid="R49">Miller et al. (2018)</xref>, only included HI listeners with large differences in their degrees of hearing loss and covering a wide range of ages. The present study included participants with NH and the cohort of HI listeners had only moderate hearing loss and a quite narrow age range. All these differences could be responsible for the differences between our observations and previous results.</p></sec><sec id="S35"><title>Results highlight the contribution of FFRenv for better predicting speech-in-noise intelligibility but fail to reveal further benefit of considering FFR<sub>tfs</sub></title><p id="P60">We investigated the extent to which the neural and psychophysical measures provided by these experiments could predict the variability in SRTs assessed in the same listeners. First, analyses highlighted robust negative correlations between RAM-FFR<sub>env</sub> and SRT in the BB as well as in the HP condition, replicating previous studies. However, there was no robust correlation between SM-FFR<sub>tfs</sub> and SRT, specifically in the LP condition that was designed to maximize this potential relationship. Recent studies suggest that it is not easy to capture that the contribution of TFS in SPiN intelligibility. SPiN processing, even implemented in a laboratory-based task, necessarily engages a range of bottom-up and top-down cognitive processes that extend far beyond purely sensory mechanisms, and only a limited amount variance could be accounted for by any ideal marker of CS. Many recent studies suggest that cognitive factors could even play a larger role in accounting for the variability in most SPiN tasks, as compared to CS (<xref ref-type="bibr" rid="R58">Parthasaraty et al., 2020</xref>; <xref ref-type="bibr" rid="R17">Camerer et al., 2019</xref>; <xref ref-type="bibr" rid="R28">Gómez-Álvarez et al., 2023</xref>; <xref ref-type="bibr" rid="R88">Zink et al., 2024</xref>). Moreover, although our SPiN test used an LP filtering condition to favor the processing of TFS cues, this test might not be the most appropriate. Another aspect already discussed in the literature is the type of noise used for the SPIN test. Several studies showed that the role played by TFS cues is more easily measurable in the context of speech masked by multiple speakers, as compared to the case of a single speaker masked by signals with a complex, non-periodic TFS structure such as white noise or speech-shaped noise (Bharadwaj et al., 2019; <xref ref-type="bibr" rid="R62">Ruggles et al., 2012</xref>; Zeng et al., 2005). Also, the use of speech digits instead of the sentences of the Matrix test, could result in less recruitment of cognitive aspects that could impact our results (<xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>; <xref ref-type="bibr" rid="R62">Ruggles et al., 2012</xref>). <xref ref-type="bibr" rid="R81">Won et al. (2016)</xref> successfully evidenced a direct relationship between the fidelity of formant encoding in FFR<sub>tfs</sub> (estimated by taking the difference between the estimated peak on each subject and theoretical formant peak using PRAAT) and vowel confusion matrices measured in the same individuals; unfortunately, the same study did not test the extent to which it would contribute to SPiN variability in a more complex task. In sum, considering SPiN tasks with very simple speech stimuli maximizing the importance of TFS cues, such as isolated vowels or concurrent vowel trajectories (<xref ref-type="bibr" rid="R82">Woods &amp; McDermott, 2015</xref>), would help highlighting the extent to which it may be accounted for by FFR<sub>tfs</sub>.</p></sec><sec id="S36"><title>Further research is required to use psychophysical SM detection thresholds as a direct reliable behavioral proxy of TFS coding fidelity</title><p id="P61">One possible explanation for the absence of a relationship between SM psychophysical thresholds and FFR<sub>tfs</sub> is that these two measures may reflect different aspects of auditory processing. In our study, the SM-FFR was assessed using spectrally modulated sounds with 50% and 100% modulation depths to better capture formant encoding. We hypothesized that the amplitude differences between harmonics in the SM-FFR spectrum for these two modulation depths would be a better predictor of SM thresholds than absolute amplitude alone (cf. <xref ref-type="bibr" rid="R81">Won et al., 2016</xref>). However, no significant difference was observed in the FFR responses between the two modulation depths, leading us to use the average response as the neural metric. These results raise questions about whether psychophysical SM detection thresholds are sufficiently sensitive to reflect TFS processing. To address this, future studies might explore other psychophysical tests using stimuli that are more directly related to TFS processing, such as the TFS1 test (<xref ref-type="bibr" rid="R64">Sęk &amp; Moore, 2012</xref>) or frequency modulation (FM) thresholds using low-frequency carriers (<xref ref-type="bibr" rid="R13">Borjigin et al., 2022</xref>; <xref ref-type="bibr" rid="R58">Parthasarathy et al., 2020</xref>). Moreover, psychophysical experiments aimed at characterizing sensory capacities should account for potential cognitive and attentional factors that could influence performance. <xref ref-type="bibr" rid="R13">Borjigin et al. (2022)</xref> demonstrated how attentional lapses during psychophysical tasks can account for nearly 50% of the variance in amplitude modulation (AM) threshold measurements, underscoring the importance of controlling for these factors in future studies. Despite the lack of correlation between the FFR and psychophysical measures in our study, this does not undermine the possibility that either psychophysical measures of SM detection/discrimination or FFRs to SM stimuli could be viable tools for assessing TFS coding and/or CS in the future, depending on the research context. For instance, <xref ref-type="bibr" rid="R44">Märcher-Rørsted (2022)</xref> provided strong evidence that FFRs to pure tones are reduced with age, suggesting that simple tones could be used to monitor CS. However, the potential for using complex SM stimuli in psychophysical tests warrants further investigation. These stimuli are particularly appealing because they serve as parameterized models of vowel sounds (<xref ref-type="bibr" rid="R59">Ponsot et al., 2021</xref>), which could improve the link between TFS coding and performance on SPiN tasks.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS202027-supplement-supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d44aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S37"><title>Acknowledgments</title><p>We acknowledge the contribution of Heleen Van Der Biest and Saartje Vanaudenaerde for their effort in collecting the data of this paper. We would also like to thank Fotis Drakopoulos and Sarineh Keshishzadeh for their advices and help when designing and conducting the first EEG experiments using STM sounds. This work was supported by a postdoctoral fellowship by the Fondation pour l’Audition [FPA 2020-005F2], the European Research Council [ERC-StG-678120 to S.V., RobSpear] and the ANR [ANR-22-CE28-0010 to E.P., InspectSyn].</p></ack><sec id="S38" sec-type="data-availability"><title>Data availability statement</title><p id="P62">The data generated and analyzed during this study will be made publicly available on the Open Science Framework (OSF) upon publication.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P63"><underline>Conflict of Interest</underline>: None</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aiken</surname><given-names>SJ</given-names></name><name><surname>Picton</surname><given-names>TW</given-names></name></person-group><article-title>Envelope and spectral frequency-following responses to vowel sounds</article-title><source>Hearing research</source><year>2008</year><volume>245</volume><issue>1-2</issue><fpage>35</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">18765275</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ananthakrishnan</surname><given-names>S</given-names></name><name><surname>Krishnan</surname><given-names>A</given-names></name><name><surname>Bartlett</surname><given-names>E</given-names></name></person-group><article-title>Human frequency following response: neural representation of envelope and temporal fine structure in listeners with normal hearing and sensorineural hearing loss</article-title><source>Ear and hearing</source><year>2016</year><volume>37</volume><issue>2</issue><elocation-id>e91</elocation-id><pub-id pub-id-type="pmcid">PMC4767571</pub-id><pub-id pub-id-type="pmid">26583482</pub-id><pub-id pub-id-type="doi">10.1097/AUD.0000000000000247</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ananthakrishnan</surname><given-names>S</given-names></name><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Krishnan</surname><given-names>A</given-names></name></person-group><article-title>Human frequency following responses to vocoded speech</article-title><source>Ear and hearing</source><year>2017</year><volume>38</volume><issue>5</issue><elocation-id>e256</elocation-id><pub-id pub-id-type="pmcid">PMC5570627</pub-id><pub-id pub-id-type="pmid">28362674</pub-id><pub-id pub-id-type="doi">10.1097/AUD.0000000000000432</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>S</given-names></name><name><surname>Parbery-Clark</surname><given-names>A</given-names></name><name><surname>White-Schwoch</surname><given-names>T</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Aging affects neural precision of speech encoding</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>41</issue><fpage>14156</fpage><lpage>14164</lpage><pub-id pub-id-type="pmcid">PMC3488287</pub-id><pub-id pub-id-type="pmid">23055485</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2176-12.2012</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>JG</given-names></name><name><surname>Mehraei</surname><given-names>G</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Gallun</surname><given-names>FJ</given-names></name><name><surname>Theodoroff</surname><given-names>SM</given-names></name><name><surname>Leek</surname><given-names>MR</given-names></name></person-group><article-title>Spectrotemporal modulation sensitivity as a predictor of speech intelligibility for hearing-impaired listeners</article-title><source>Journal of the American Academy of Audiology</source><year>2013</year><volume>24</volume><issue>4</issue><fpage>293</fpage><lpage>306</lpage><pub-id pub-id-type="pmcid">PMC3973426</pub-id><pub-id pub-id-type="pmid">23636210</pub-id><pub-id pub-id-type="doi">10.3766/jaaa.24.4.5</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>JG</given-names></name><name><surname>Danielsson</surname><given-names>H</given-names></name><etal/><name><surname>Lunner</surname><given-names>T</given-names></name></person-group><article-title>Spectrotemporal modulation sensitivity as a predictor of speech-reception performance in noise with hearing aids</article-title><source>Trends in Hearing</source><year>2016</year><volume>20</volume><elocation-id>2331216516670387</elocation-id><pub-id pub-id-type="pmcid">PMC5098798</pub-id><pub-id pub-id-type="pmid">27815546</pub-id><pub-id pub-id-type="doi">10.1177/2331216516670387</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bharadwaj</surname><given-names>HM</given-names></name><name><surname>Masud</surname><given-names>S</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><article-title>Individual differences reveal correlates of hidden hearing deficits</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>5</issue><fpage>2161</fpage><lpage>2172</lpage><pub-id pub-id-type="pmcid">PMC4402332</pub-id><pub-id pub-id-type="pmid">25653371</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3915-14.2015</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name><name><surname>Villafuerte</surname><given-names>JW</given-names></name><name><surname>Moreno</surname><given-names>S</given-names></name><name><surname>Alain</surname><given-names>C</given-names></name></person-group><article-title>Age-related changes in the subcortical-cortical encoding and categorical perception of speech</article-title><source>Neurobiology of aging</source><year>2014</year><volume>35</volume><issue>11</issue><fpage>2526</fpage><lpage>2540</lpage><pub-id pub-id-type="pmid">24908166</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name></person-group><article-title>Subcortical sources dominate the neuroelectric auditory frequencyfollowing response to speech</article-title><source>NeuroImage</source><year>2018</year><volume>175</volume><fpage>56</fpage><lpage>69</lpage><pub-id pub-id-type="pmid">29604459</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name><name><surname>Bhagat</surname><given-names>S</given-names></name></person-group><article-title>Brainstem correlates of cochlear nonlinearity measured via the scalp-recorded frequency-following response</article-title><source>NeuroReport</source><year>2020a</year><volume>31</volume><fpage>702</fpage><lpage>707</lpage><pub-id pub-id-type="pmcid">PMC7275900</pub-id><pub-id pub-id-type="pmid">32453027</pub-id><pub-id pub-id-type="doi">10.1097/WNR.0000000000001452</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name><name><surname>Momtaz</surname><given-names>S</given-names></name></person-group><article-title>Subcortical sources drive the relation between frequency-following responses (FFRs) and speech-in-noise perception</article-title><source>bioRxiv</source><year>2020b</year><elocation-id>2020-03</elocation-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bidelman</surname><given-names>GM</given-names></name><name><surname>Momtaz</surname><given-names>S</given-names></name></person-group><article-title>Subcortical rather than cortical sources of the frequency-following response (FFR) relate to speech-in-noise perception in normal-hearing listeners</article-title><source>Neuroscience letters</source><year>2021</year><volume>746</volume><elocation-id>135664</elocation-id><pub-id pub-id-type="pmcid">PMC7897268</pub-id><pub-id pub-id-type="pmid">33497718</pub-id><pub-id pub-id-type="doi">10.1016/j.neulet.2021.135664</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borjigin</surname><given-names>A</given-names></name><name><surname>Hustedt-Mai</surname><given-names>AR</given-names></name><name><surname>Bharadwaj</surname><given-names>HM</given-names></name></person-group><article-title>Individualized Assays of Temporal Coding in the Ascending Human Auditory System</article-title><source>Eneuro</source><year>2022</year><volume>9</volume><issue>2</issue><pub-id pub-id-type="pmcid">PMC8925652</pub-id><pub-id pub-id-type="pmid">35193890</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0378-21.2022</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borjigin</surname><given-names>A</given-names></name><name><surname>Bharadwaj</surname><given-names>H</given-names></name></person-group><article-title>Individual Differences Reveal the Utility of Temporal Fine- Structure Processing for Speech Perception in Noise</article-title><source>bioRxiv</source><year>2023</year><comment>2023-09</comment></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bramhall</surname><given-names>N</given-names></name><name><surname>Beach</surname><given-names>E</given-names></name><name><surname>Epp</surname><given-names>B</given-names></name><name><surname>LePrell</surname><given-names>CG</given-names></name><name><surname>Lopez-Poveda</surname><given-names>EA</given-names></name><name><surname>Plack</surname><given-names>C</given-names></name><etal/><name><surname>Canlon</surname><given-names>B</given-names></name></person-group><article-title>The search for noise-induced cochlear synaptopathy in humans: Mission impossible?</article-title><source>Hearing Research</source><year>2019</year><volume>377</volume><fpage>88</fpage><lpage>103</lpage><pub-id pub-id-type="pmid">30921644</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bramhall</surname><given-names>NF</given-names></name><name><surname>McMillan</surname><given-names>GP</given-names></name><name><surname>Kampel</surname><given-names>SD</given-names></name></person-group><article-title>Envelope following response measurements in young veterans are consistent with noise-induced cochlear synaptopathy</article-title><source>Hearing Research</source><year>2021</year><volume>408</volume><elocation-id>108310</elocation-id><pub-id pub-id-type="pmcid">PMC10857793</pub-id><pub-id pub-id-type="pmid">34293505</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2021.108310</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamerer</surname><given-names>AM</given-names></name><name><surname>AuBuchon</surname><given-names>A</given-names></name><name><surname>Fultz</surname><given-names>SE</given-names></name><name><surname>Kopun</surname><given-names>JG</given-names></name><name><surname>Neely</surname><given-names>ST</given-names></name><name><surname>Rasetshwane</surname><given-names>DM</given-names></name></person-group><article-title>The role of cognition in common measures of peripheral synaptopathy and hidden hearing loss</article-title><source>American journal of audiology</source><year>2019</year><volume>28</volume><issue>4</issue><fpage>843</fpage><lpage>856</lpage><pub-id pub-id-type="pmcid">PMC7210438</pub-id><pub-id pub-id-type="pmid">31647880</pub-id><pub-id pub-id-type="doi">10.1044/2019_AJA-19-0063</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carcagno</surname><given-names>S</given-names></name><name><surname>Plack</surname><given-names>CJ</given-names></name></person-group><article-title>Effects of age on electrophysiological measures of cochlear synaptopathy in humans</article-title><source>Hearing Research</source><year>2020</year><volume>396</volume><elocation-id>108068</elocation-id><pub-id pub-id-type="pmcid">PMC7593961</pub-id><pub-id pub-id-type="pmid">32979760</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2020.108068</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chauvette</surname><given-names>L</given-names></name><name><surname>Fournier</surname><given-names>P</given-names></name><name><surname>Sharp</surname><given-names>A</given-names></name></person-group><article-title>The frequency-following response to assess the neural representation of spectral speech cues in older adults</article-title><source>Hearing Research</source><year>2022</year><volume>418</volume><elocation-id>108486</elocation-id><pub-id pub-id-type="pmid">35344807</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clinard</surname><given-names>CG</given-names></name><name><surname>Tremblay</surname><given-names>KL</given-names></name><name><surname>Krishnan</surname><given-names>AR</given-names></name></person-group><article-title>Aging alters the perception and physiological representation of frequency: evidence from human frequency-following response recordings</article-title><source>Hearing research</source><year>2010</year><volume>264</volume><issue>1-2</issue><fpage>48</fpage><lpage>55</lpage><pub-id pub-id-type="pmcid">PMC2868068</pub-id><pub-id pub-id-type="pmid">19944140</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2009.11.010</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clinard</surname><given-names>CG</given-names></name><name><surname>Tremblay</surname><given-names>KL</given-names></name></person-group><article-title>Aging degrades the neural encoding of simple and complex sounds in the human brainstem</article-title><source>Journal of the American Academy of Audiology</source><year>2013</year><volume>24</volume><issue>07</issue><fpage>590</fpage><lpage>599</lpage><pub-id pub-id-type="pmid">24047946</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clinard</surname><given-names>CG</given-names></name><name><surname>Cotter</surname><given-names>CM</given-names></name></person-group><article-title>Neural representation of dynamic frequency is degraded in older adults</article-title><source>Hearing Research</source><year>2015</year><volume>323</volume><fpage>91</fpage><lpage>98</lpage><pub-id pub-id-type="pmid">25724819</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drullman</surname><given-names>R</given-names></name></person-group><article-title>Temporal envelope and fine structure cues for speech intelligibility</article-title><source>The Journal of the Acoustical Society of America</source><year>1995</year><volume>97</volume><issue>1</issue><fpage>585</fpage><lpage>592</lpage><pub-id pub-id-type="pmid">7860835</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>TM</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>The modulation transfer function for speech intelligibility</article-title><source>PLoS computational biology</source><year>2009</year><volume>5</volume><issue>3</issue><elocation-id>e1000302</elocation-id><pub-id pub-id-type="pmcid">PMC2639724</pub-id><pub-id pub-id-type="pmid">19266016</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000302</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Encina-Llamas</surname><given-names>G</given-names></name><name><surname>Harte</surname><given-names>JM</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>B</given-names></name><name><surname>Epp</surname><given-names>B</given-names></name></person-group><article-title>Investigating the effect of cochlear synaptopathy on envelope following responses using a model of the auditory nerve</article-title><source>Journal of the Association for Research in Otolaryngology</source><year>2019</year><volume>20</volume><fpage>363</fpage><lpage>382</lpage><pub-id pub-id-type="pmcid">PMC6646444</pub-id><pub-id pub-id-type="pmid">31102010</pub-id><pub-id pub-id-type="doi">10.1007/s10162-019-00721-7</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallun</surname><given-names>FJ</given-names></name><name><surname>Coco</surname><given-names>L</given-names></name><name><surname>Koerner</surname><given-names>TK</given-names></name><name><surname>Larrea-Mancera</surname><given-names>ESLD</given-names></name><name><surname>Molis</surname><given-names>MR</given-names></name><name><surname>Eddins</surname><given-names>DA</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><article-title>Relating suprathreshold auditory processing abilities to speech understanding in competition</article-title><source>Brain Sciences</source><year>2022</year><volume>12</volume><issue>6</issue><fpage>695</fpage><pub-id pub-id-type="pmcid">PMC9221421</pub-id><pub-id pub-id-type="pmid">35741581</pub-id><pub-id pub-id-type="doi">10.3390/brainsci12060695</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name><name><surname>Mauermann</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>JL</given-names></name><name><surname>Gonzales</surname><given-names>L</given-names></name><name><surname>Henry</surname><given-names>KS</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><article-title>Deciphering compromised speech-in-noise intelligibility in older listeners: the influence of cochlear synaptopathy</article-title><year>2024</year><pub-id pub-id-type="doi">10.1101/2020.06.09.142950</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gómez-Álvarez</surname><given-names>M</given-names></name><name><surname>Johannesen</surname><given-names>PT</given-names></name><name><surname>Coelho-de-Sousa</surname><given-names>SL</given-names></name><name><surname>Klump</surname><given-names>GM</given-names></name><name><surname>Lopez-Poveda</surname><given-names>EA</given-names></name></person-group><article-title>The relative contribution of cochlear synaptopathy and reduced inhibition to age-related hearing impairment for people with normal audiograms</article-title><source>Trends in Hearing</source><year>2023</year><volume>27</volume><elocation-id>23312165231213191</elocation-id><pub-id pub-id-type="pmcid">PMC10644751</pub-id><pub-id pub-id-type="pmid">37956654</pub-id><pub-id pub-id-type="doi">10.1177/23312165231213191</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guest</surname><given-names>H</given-names></name><name><surname>Munro</surname><given-names>KJ</given-names></name><name><surname>Prendergast</surname><given-names>G</given-names></name><name><surname>Plack</surname><given-names>CJ</given-names></name></person-group><article-title>Reliability and interrelations of seven proxy measures of cochlear synaptopathy</article-title><source>Hearing Research</source><year>2019</year><volume>375</volume><fpage>34</fpage><lpage>43</lpage><pub-id pub-id-type="pmcid">PMC6423440</pub-id><pub-id pub-id-type="pmid">30765219</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2019.01.018</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isarangura</surname><given-names>S</given-names></name><name><surname>Eddins</surname><given-names>AC</given-names></name><name><surname>Ozmeral</surname><given-names>EJ</given-names></name><name><surname>Eddins</surname><given-names>DA</given-names></name></person-group><article-title>The effects of duration and level on spectral modulation perception</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>2019</year><volume>62</volume><issue>10</issue><fpage>3876</fpage><lpage>3886</lpage><pub-id pub-id-type="pmcid">PMC7838824</pub-id><pub-id pub-id-type="pmid">31638883</pub-id><pub-id pub-id-type="doi">10.1044/2019_JSLHR-H-18-0449</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshishzadeh</surname><given-names>S</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><article-title>The derived-band envelope following response and its sensitivity to sensorineural hearing deficits</article-title><source>Hearing research</source><year>2020</year><volume>392</volume><elocation-id>107979</elocation-id><pub-id pub-id-type="pmid">32447097</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnan</surname><given-names>A</given-names></name></person-group><article-title>Human frequency-following responses: representation of steady-state synthetic vowels</article-title><source>Hearing research</source><year>2002</year><volume>166</volume><issue>1-2</issue><fpage>192</fpage><lpage>201</lpage><pub-id pub-id-type="pmid">12062771</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizman</surname><given-names>J</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Analyzing the FFR: A tutorial for decoding the richness of auditory function</article-title><source>Hearing research</source><year>2019</year><volume>382</volume><elocation-id>107779</elocation-id><pub-id pub-id-type="pmcid">PMC6778514</pub-id><pub-id pub-id-type="pmid">31505395</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2019.107779</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kujawa</surname><given-names>SG</given-names></name><name><surname>Liberman</surname><given-names>MC</given-names></name></person-group><article-title>Adding insult to injury: cochlear nerve degeneration after “temporary” noise-induced hearing loss</article-title><source>Journal of Neuroscience</source><year>2009</year><volume>29</volume><issue>45</issue><fpage>14077</fpage><lpage>14085</lpage><pub-id pub-id-type="pmcid">PMC2812055</pub-id><pub-id pub-id-type="pmid">19906956</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2845-09.2009</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesica</surname><given-names>NA</given-names></name></person-group><article-title>Why Do Hearing Aids Fail to Restore Normal Auditory Perception?</article-title><source>Trends in Neurosciences</source><year>2018</year><volume>41</volume><issue>4</issue><fpage>174</fpage><lpage>185</lpage><pub-id pub-id-type="pmcid">PMC7116430</pub-id><pub-id pub-id-type="pmid">29449017</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2018.01.008</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levitt</surname><given-names>H</given-names></name></person-group><article-title>Transformed up-down methods in psychoacoustics</article-title><source>The Journal of the Acoustical Society of America</source><year>1971</year><volume>49</volume><issue>2B</issue><fpage>467</fpage><lpage>477</lpage><pub-id pub-id-type="pmid">5541744</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>MC</given-names></name><name><surname>Epstein</surname><given-names>MJ</given-names></name><etal/><name><surname>Maison</surname><given-names>SF</given-names></name></person-group><article-title>Toward a differential diagnosis of hidden hearing loss in humans</article-title><source>PloS One</source><year>2016</year><volume>11</volume><issue>9</issue><elocation-id>e0162726</elocation-id><pub-id pub-id-type="pmcid">PMC5019483</pub-id><pub-id pub-id-type="pmid">27618300</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0162726</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorenzi</surname><given-names>C</given-names></name><name><surname>Gilbert</surname><given-names>G</given-names></name><etal/><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><article-title>Speech perception problems of the hearing impaired reflect inability to use temporal fine structure</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><issue>49</issue><fpage>18866</fpage><lpage>18869</lpage><pub-id pub-id-type="pmcid">PMC1693753</pub-id><pub-id pub-id-type="pmid">17116863</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0607364103</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Luts</surname><given-names>H</given-names></name><name><surname>Jansen</surname><given-names>S</given-names></name><name><surname>Dreschler</surname><given-names>W</given-names></name><name><surname>Wouters</surname><given-names>J</given-names></name></person-group><source>Development and normative data for the FlemishIDutch Matrix test</source><year>2014</year></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magits</surname><given-names>S</given-names></name><name><surname>Moncada-Torres</surname><given-names>A</given-names></name><name><surname>Van Deun</surname><given-names>L</given-names></name><name><surname>Wouters</surname><given-names>J</given-names></name><name><surname>van Wieringen</surname><given-names>A</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><article-title>The effect of presentation level on spectrotemporal modulation detection</article-title><source>Hearing research</source><year>2019</year><volume>371</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="pmid">30439570</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mai</surname><given-names>G</given-names></name><name><surname>Tuomainen</surname><given-names>J</given-names></name><name><surname>Howell</surname><given-names>P</given-names></name></person-group><article-title>Relationship between speech-evoked neural responses and perception of speech in noise in older adults</article-title><source>The Journal of the Acoustical Society of America</source><year>2018</year><volume>143</volume><issue>3</issue><fpage>1333</fpage><lpage>1345</lpage><pub-id pub-id-type="pmid">29604686</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mai</surname><given-names>G</given-names></name><name><surname>Howell</surname><given-names>P</given-names></name></person-group><article-title>The possible role of early-stage phase-locked neural activities in speech-in-noise perception in human adults across age and hearing loss</article-title><source>Hearing Research</source><year>2023</year><volume>427</volume><elocation-id>108647</elocation-id><pub-id pub-id-type="pmid">36436293</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mamo</surname><given-names>SK</given-names></name><name><surname>Grose</surname><given-names>JH</given-names></name><name><surname>Buss</surname><given-names>E</given-names></name></person-group><article-title>Speech-evoked ABR: effects of age and simulated neural temporal jitter</article-title><source>Hearing research</source><year>2016</year><volume>333</volume><fpage>201</fpage><lpage>209</lpage><pub-id pub-id-type="pmcid">PMC4788984</pub-id><pub-id pub-id-type="pmid">26368029</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2015.09.005</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Märcher-Rørsted</surname><given-names>J</given-names></name><name><surname>Encina-Llamas</surname><given-names>G</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name><name><surname>Liberman</surname><given-names>MC</given-names></name><name><surname>Wu</surname><given-names>P-z</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name></person-group><article-title>Age-related reduction in frequency-following responses as a potential marker of cochlear neural degeneration</article-title><source>Hearing Research</source><year>2022</year><volume>414</volume><elocation-id>108411</elocation-id><pub-id pub-id-type="pmid">34929535</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Märcher-Rørsted</surname><given-names>J</given-names></name></person-group><article-title>Characterizing cochlear neural degeneration using frequency-following responses</article-title><source>PhD Thesis</source><publisher-name>DTU</publisher-name><year>2022</year></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marmel</surname><given-names>F</given-names></name><name><surname>Linley</surname><given-names>D</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Gockel</surname><given-names>HE</given-names></name><name><surname>Hopkins</surname><given-names>K</given-names></name><name><surname>Plack</surname><given-names>CJ</given-names></name></person-group><article-title>Subcortical neural synchrony and absolute thresholds predict frequency discrimination independently</article-title><source>Journal of the Association for Research in Otolaryngology</source><year>2013</year><volume>14</volume><fpage>757</fpage><lpage>766</lpage><pub-id pub-id-type="pmcid">PMC3767871</pub-id><pub-id pub-id-type="pmid">23760984</pub-id><pub-id pub-id-type="doi">10.1007/s10162-013-0402-3</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehraei</surname><given-names>G</given-names></name><name><surname>Gallun</surname><given-names>FJ</given-names></name><name><surname>Leek</surname><given-names>MR</given-names></name><name><surname>Bernstein</surname><given-names>JG</given-names></name></person-group><article-title>Spectrotemporal modulation sensitivity for hearing-impaired listeners: Dependence on carrier center frequency and the relationship to speech intelligibility</article-title><source>The Journal of the Acoustical Society of America</source><year>2014</year><volume>136</volume><issue>1</issue><fpage>301</fpage><lpage>316</lpage><pub-id pub-id-type="pmcid">PMC4187385</pub-id><pub-id pub-id-type="pmid">24993215</pub-id><pub-id pub-id-type="doi">10.1121/1.4881918</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mepani</surname><given-names>AM</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name><name><surname>Bennett</surname><given-names>K</given-names></name><etal/><name><surname>Maison</surname><given-names>SF</given-names></name></person-group><article-title>Envelope following responses predict speech-in-noise performance in normalhearing listeners</article-title><source>Journal of Neurophysiology</source><year>2021</year><volume>125</volume><issue>4</issue><fpage>1213</fpage><lpage>1222</lpage><pub-id pub-id-type="pmcid">PMC8282226</pub-id><pub-id pub-id-type="pmid">33656936</pub-id><pub-id pub-id-type="doi">10.1152/jn.00620.2020</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CW</given-names></name><name><surname>Bernstein</surname><given-names>JG</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Wu</surname><given-names>YH</given-names></name><name><surname>Bentler</surname><given-names>RA</given-names></name><name><surname>Tremblay</surname><given-names>K</given-names></name></person-group><article-title>The effects of static and moving spectral ripple sensitivity on unaided and aided speech perception in noise</article-title><source>Journal of Speech, Language, and Hearing Research</source><year>2018</year><volume>61</volume><issue>12</issue><fpage>3113</fpage><lpage>3126</lpage><pub-id pub-id-type="pmcid">PMC6440313</pub-id><pub-id pub-id-type="pmid">30515519</pub-id><pub-id pub-id-type="doi">10.1044/2018_JSLHR-H-17-0373</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molis</surname><given-names>MR</given-names></name><name><surname>Bologna</surname><given-names>WJ</given-names></name><name><surname>Madsen</surname><given-names>BM</given-names></name><name><surname>Muralimanohar</surname><given-names>RK</given-names></name><name><surname>Billings</surname><given-names>CJ</given-names></name></person-group><article-title>Frequency Following Responses to Tone Glides: Effects of Age and Hearing Loss</article-title><source>Journal of the Association for Research in Otolaryngology</source><year>2023</year><volume>24</volume><issue>4</issue><fpage>429</fpage><lpage>439</lpage><pub-id pub-id-type="pmcid">PMC10504227</pub-id><pub-id pub-id-type="pmid">37438572</pub-id><pub-id pub-id-type="doi">10.1007/s10162-023-00900-7</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><source>Cochlear hearing loss: physiological, psychological and technical issues</source><publisher-name>John Wiley &amp; Sons</publisher-name><year>2007</year></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narne</surname><given-names>VK</given-names></name><name><surname>Sharma</surname><given-names>M</given-names></name><name><surname>Van Dun</surname><given-names>B</given-names></name><name><surname>Bansal</surname><given-names>S</given-names></name><name><surname>Prabhu</surname><given-names>L</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><article-title>Effects of spectral smearing on performance of the spectral ripple and spectro-temporal ripple tests</article-title><source>The Journal of the Acoustical Society of America</source><year>2016</year><volume>140</volume><issue>6</issue><fpage>4298</fpage><lpage>4306</lpage><pub-id pub-id-type="pmid">28039998</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oetjen</surname><given-names>A</given-names></name><name><surname>Verhey</surname><given-names>JL</given-names></name></person-group><article-title>Spectro-temporal modulation masking patterns reveal frequency selectivity</article-title><source>The Journal of the Acoustical Society of America</source><year>2015</year><volume>137</volume><issue>2</issue><fpage>714</fpage><lpage>723</lpage><pub-id pub-id-type="pmid">25698006</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oetjen</surname><given-names>A</given-names></name><name><surname>Verhey</surname><given-names>JL</given-names></name></person-group><article-title>Characteristics of spectro-temporal modulation frequency selectivity in humans</article-title><source>The Journal of the Acoustical Society of America</source><year>2017</year><volume>141</volume><issue>3</issue><fpage>1887</fpage><lpage>1895</lpage><pub-id pub-id-type="pmid">28372116</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberfeld</surname><given-names>D</given-names></name><name><surname>Kloeckner-Nowotny</surname><given-names>F</given-names></name></person-group><article-title>Individual differences in selective attention predict speech identification at a cocktail party</article-title><source>Elife</source><year>2016</year><volume>5</volume><elocation-id>e16747</elocation-id><pub-id pub-id-type="pmcid">PMC5441891</pub-id><pub-id pub-id-type="pmid">27580272</pub-id><pub-id pub-id-type="doi">10.7554/eLife.16747</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parbery-Clark</surname><given-names>A</given-names></name><name><surname>Skoe</surname><given-names>E</given-names></name><name><surname>Lam</surname><given-names>C</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Musician enhancement for speechin-noise</article-title><source>Ear and hearing</source><year>2009</year><volume>30</volume><issue>6</issue><fpage>653</fpage><lpage>661</lpage><pub-id pub-id-type="pmid">19734788</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Bartlett</surname><given-names>EL</given-names></name><name><surname>Kujawa</surname><given-names>SG</given-names></name></person-group><article-title>Age-related changes in neural coding of envelope cues: peripheral declines and central compensation</article-title><source>Neuroscience</source><year>2019</year><volume>407</volume><fpage>21</fpage><lpage>31</lpage><pub-id pub-id-type="pmcid">PMC8600413</pub-id><pub-id pub-id-type="pmid">30553793</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.12.007</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname><given-names>A</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Bennett</surname><given-names>K</given-names></name><name><surname>DeGruttola</surname><given-names>V</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><article-title>Bottom- up and top-down neural signatures of disordered multi-talker speech perception in adults with normal hearing</article-title><source>Elife</source><year>2020</year><volume>9</volume><elocation-id>e51419</elocation-id><pub-id pub-id-type="pmcid">PMC6974362</pub-id><pub-id pub-id-type="pmid">31961322</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51419</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponsot</surname><given-names>E</given-names></name><name><surname>Varnet</surname><given-names>L</given-names></name><name><surname>Wallaert</surname><given-names>N</given-names></name><name><surname>Daoud</surname><given-names>E</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name><name><surname>Neri</surname><given-names>P</given-names></name></person-group><article-title>Mechanisms of spectrotemporal modulation detection for normal-and hearing-impaired listeners</article-title><source>Trends in hearing</source><year>2021</year><volume>25</volume><date-in-citation>2331216520978029</date-in-citation><pub-id pub-id-type="pmcid">PMC7905488</pub-id><pub-id pub-id-type="pmid">33620023</pub-id><pub-id pub-id-type="doi">10.1177/2331216520978029</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prendergast</surname><given-names>G</given-names></name><name><surname>Couth</surname><given-names>S</given-names></name><name><surname>Millman</surname><given-names>RE</given-names></name><name><surname>Guest</surname><given-names>H</given-names></name><name><surname>Kluk</surname><given-names>K</given-names></name><name><surname>Munro</surname><given-names>KJ</given-names></name><name><surname>Plack</surname><given-names>CJ</given-names></name></person-group><article-title>Effects of age and noise exposure on proxy measures of cochlear synaptopathy</article-title><source>Trends in hearing</source><year>2019</year><volume>23</volume><elocation-id>2331216519877301</elocation-id><pub-id pub-id-type="pmcid">PMC6767746</pub-id><pub-id pub-id-type="pmid">31558119</pub-id><pub-id pub-id-type="doi">10.1177/2331216519877301</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="book"><collab>R Core Team</collab><source>R: A language and environment for statistical computing</source><publisher-name>R Foundation for Statistical Computing</publisher-name><publisher-loc>Vienna, Austria</publisher-loc><year>2022</year><comment>URL<ext-link ext-link-type="uri" xlink:href="https://www.R-project.org/">https://www.R-project.org/</ext-link></comment></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruggles</surname><given-names>D</given-names></name><name><surname>Bharadwaj</surname><given-names>H</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name></person-group><article-title>Normal hearing is not enough to guarantee robust encoding of suprathreshold features important in everyday communication</article-title><source>Proceedings of the National Academy of Sciences</source><year>2011</year><volume>108</volume><issue>37</issue><fpage>15516</fpage><lpage>15521</lpage><pub-id pub-id-type="pmcid">PMC3174666</pub-id><pub-id pub-id-type="pmid">21844339</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1108912108</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sachs</surname><given-names>MB</given-names></name><name><surname>Voigt</surname><given-names>HF</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name></person-group><article-title>Auditory nerve representation of vowels in background noise</article-title><source>Journal of neurophysiology</source><year>1983</year><volume>50</volume><issue>1</issue><fpage>27</fpage><lpage>45</lpage><pub-id pub-id-type="pmid">6875649</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sęk</surname><given-names>A</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name></person-group><article-title>Implementation of two tests for measuring sensitivity to temporal fine structure</article-title><source>Intl Journal of Audiology</source><year>2012</year><volume>51</volume><issue>1</issue><fpage>58</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">22050366</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suresh</surname><given-names>CH</given-names></name><name><surname>Krishnan</surname><given-names>A</given-names></name></person-group><article-title>Frequency-following response to steady-state vowel in quiet and background noise among marching band participants with normal hearing</article-title><source>American Journal of Audiology</source><year>2022</year><volume>31</volume><issue>3</issue><fpage>719</fpage><lpage>736</lpage><pub-id pub-id-type="pmid">35944059</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temboury-Gutierrez</surname><given-names>M</given-names></name><name><surname>Märcher-Rørsted</surname><given-names>J</given-names></name><name><surname>Bille</surname><given-names>M</given-names></name><name><surname>Yde</surname><given-names>J</given-names></name><name><surname>Encina-Llamas</surname><given-names>G</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name></person-group><article-title>Electrocochleographic frequency-following responses as a potential marker of age-related cochlear neural degeneration</article-title><source>Hearing Research</source><year>2024</year><volume>446</volume><elocation-id>109005</elocation-id><pub-id pub-id-type="pmid">38598943</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tichko</surname><given-names>P</given-names></name><name><surname>Skoe</surname><given-names>E</given-names></name></person-group><article-title>Frequency-dependent fine structure in the frequency-following response: The byproduct of multiple generators</article-title><source>Hearing research</source><year>2017</year><volume>348</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmid">28137699</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Der Biest</surname><given-names>H</given-names></name><name><surname>Keshishzadeh</surname><given-names>S</given-names></name><name><surname>Keppler</surname><given-names>H</given-names></name><name><surname>Dhooge</surname><given-names>I</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><article-title>Envelope following responses for hearing diagnosis: Robustness and methodological considerations</article-title><source>The Journal of the Acoustical Society of America</source><year>2023</year><volume>153</volume><issue>1</issue><fpage>191</fpage><lpage>208</lpage><pub-id pub-id-type="pmid">36732231</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varnet</surname><given-names>L</given-names></name><name><surname>Léger</surname><given-names>AC</given-names></name><name><surname>Boucher</surname><given-names>S</given-names></name><name><surname>Bonnet</surname><given-names>C</given-names></name><name><surname>Petit</surname><given-names>C</given-names></name><name><surname>Lorenzi</surname><given-names>C</given-names></name></person-group><article-title>Contributions of age-related and audibility-related deficits to aided consonant identification in presbycusis: A causal-inference analysis</article-title><source>Frontiers in Aging Neuroscience</source><year>2021</year><volume>13</volume><elocation-id>640522</elocation-id><pub-id pub-id-type="pmcid">PMC7956988</pub-id><pub-id pub-id-type="pmid">33732140</pub-id><pub-id pub-id-type="doi">10.3389/fnagi.2021.640522</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasilkov</surname><given-names>V</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><name><surname>Mauermann</surname><given-names>M</given-names></name><name><surname>Verhulst</surname><given-names>S</given-names></name></person-group><article-title>Enhancing the sensitivity of the envelope-following response for cochlear synaptopathy screening in humans: the role of stimulus envelope</article-title><source>Hearing Research</source><year>2021</year><volume>400</volume><elocation-id>108132</elocation-id><pub-id pub-id-type="pmid">33333426</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasilkov</surname><given-names>V</given-names></name><name><surname>Liberman</surname><given-names>MC</given-names></name><name><surname>Maison</surname><given-names>SF</given-names></name></person-group><article-title>Isolating auditory-nerve contributions to electrocochleography by high-pass filtering: A better biomarker for cochlear nerve degeneration?</article-title><source>JASA Express Letters</source><year>2023</year><volume>3</volume><issue>2</issue><pub-id pub-id-type="pmcid">PMC9969351</pub-id><pub-id pub-id-type="pmid">36858988</pub-id><pub-id pub-id-type="doi">10.1121/10.0017328</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Altoè</surname><given-names>A</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name></person-group><article-title>Computational modeling of the human auditory periphery: Auditory-nerve responses, evoked potentials and hearing loss</article-title><source>Hearing Research</source><year>2018</year><volume>360</volume><fpage>55</fpage><lpage>75</lpage><pub-id pub-id-type="pmid">29472062</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Warzybok</surname><given-names>A</given-names></name></person-group><article-title>Contributions of Low-and High-Frequency Sensorineural Hearing Deficits to Speech Intelligibility in Noise</article-title><source>bioRxiv</source><year>2018</year><elocation-id>358127</elocation-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Altoè</surname><given-names>A</given-names></name><name><surname>Vasilkov</surname><given-names>V</given-names></name></person-group><article-title>Computational modeling of the human auditory periphery: Auditory-nerve responses, evoked potentials and hearing loss</article-title><source>Hearing Research</source><year>2018</year><volume>360</volume><fpage>55</fpage><lpage>75</lpage></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verhulst</surname><given-names>S</given-names></name><name><surname>Ernst</surname><given-names>F</given-names></name><name><surname>Garrett</surname><given-names>M</given-names></name><etal/></person-group><article-title>Suprathreshold Psychoacoustics and EnvelopeFollowing Response Relations: Normal-Hearing, Synaptopathy and Cochlear Gain Loss</article-title><source>Acta Acustica united with Acustica</source><year>2018</year><volume>104</volume><issue>5</issue><fpage>800</fpage><lpage>803</lpage></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verschooten</surname><given-names>E</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name><name><surname>Moore</surname><given-names>BC</given-names></name><name><surname>Joris</surname><given-names>PX</given-names></name><name><surname>Heinz</surname><given-names>MG</given-names></name><name><surname>Plack</surname><given-names>CJ</given-names></name></person-group><article-title>The upper frequency limit for the use of phase locking to code temporal fine structure in humans: A compilation of viewpoints</article-title><source>Hearing Research</source><year>2019</year><volume>377</volume><fpage>109</fpage><lpage>121</lpage><pub-id pub-id-type="pmcid">PMC6524635</pub-id><pub-id pub-id-type="pmid">30927686</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2019.03.011</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viswanathan</surname><given-names>V</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Heinz</surname><given-names>MG</given-names></name></person-group><article-title>Temporal fine structure influences voicing confusions for consonant identification in multi-talker babble</article-title><source>The Journal of the Acoustical Society of America</source><year>2021</year><volume>150</volume><issue>4</issue><fpage>2664</fpage><lpage>2676</lpage><pub-id pub-id-type="pmcid">PMC8514254</pub-id><pub-id pub-id-type="pmid">34717498</pub-id><pub-id pub-id-type="doi">10.1121/10.0006527</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viswanathan</surname><given-names>V</given-names></name><name><surname>Bharadwaj</surname><given-names>HM</given-names></name><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Heinz</surname><given-names>MG</given-names></name></person-group><article-title>Modulation masking and fine structure shape neural envelope coding to predict speech intelligibility across diverse listening conditions</article-title><source>The Journal of the Acoustical Society of America</source><year>2021</year><volume>150</volume><issue>3</issue><fpage>2230</fpage><lpage>2244</lpage><pub-id pub-id-type="pmcid">PMC8483789</pub-id><pub-id pub-id-type="pmid">34598642</pub-id><pub-id pub-id-type="doi">10.1121/10.0006385</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voigt</surname><given-names>HF</given-names></name><name><surname>Sachs</surname><given-names>MB</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name></person-group><article-title>Representation of whispered vowels in discharge patterns of AN fibers</article-title><source>Hearing research</source><year>1982</year><volume>8</volume><issue>1</issue><fpage>49</fpage><lpage>58</lpage><pub-id pub-id-type="pmid">7142032</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wichmann</surname><given-names>FA</given-names></name><name><surname>Hill</surname><given-names>NJ</given-names></name></person-group><article-title>The psychometric function: II. Bootstrap-based confidence intervals and sampling</article-title><source>Perception &amp; psychophysics</source><year>2001</year><volume>63</volume><issue>8</issue><fpage>1314</fpage><lpage>1329</lpage><pub-id pub-id-type="pmid">11800459</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Won</surname><given-names>JH</given-names></name><name><surname>Tremblay</surname><given-names>K</given-names></name><name><surname>Clinard</surname><given-names>CG</given-names></name><name><surname>Wright</surname><given-names>RA</given-names></name><name><surname>Sagi</surname><given-names>E</given-names></name><name><surname>Svirsky</surname><given-names>M</given-names></name></person-group><article-title>The neural encoding of formant frequencies contributing to vowel identification in normal-hearing listeners</article-title><source>The Journal of the Acoustical Society of America</source><year>2016</year><volume>139</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC4706540</pub-id><pub-id pub-id-type="pmid">26826999</pub-id><pub-id pub-id-type="doi">10.1121/1.4931909</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>KJ</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><article-title>Attentive tracking of sound sources</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><issue>17</issue><fpage>2238</fpage><lpage>2246</lpage><pub-id pub-id-type="pmid">26279234</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>P</given-names></name><name><surname>Liberman</surname><given-names>L</given-names></name><name><surname>Bennett</surname><given-names>K</given-names></name><name><surname>De Gruttola</surname><given-names>V</given-names></name><name><surname>O’malley</surname><given-names>J</given-names></name><name><surname>Liberman</surname><given-names>M</given-names></name></person-group><article-title>Primary neural degeneration in the human cochlea: evidence for hidden hearing loss in the aging ear</article-title><source>Neuroscience</source><year>2019</year><volume>407</volume><fpage>8</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC6369025</pub-id><pub-id pub-id-type="pmid">30099118</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroscience.2018.07.053</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>ED</given-names></name></person-group><article-title>Neural representation of spectral and temporal information in speech</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2008</year><volume>363</volume><issue>1493</issue><fpage>923</fpage><lpage>945</lpage><pub-id pub-id-type="pmcid">PMC2606788</pub-id><pub-id pub-id-type="pmid">17827107</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2007.2151</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaar</surname><given-names>J</given-names></name><name><surname>Simonsen</surname><given-names>LB</given-names></name><name><surname>Dau</surname><given-names>T</given-names></name><name><surname>Laugesen</surname><given-names>S</given-names></name></person-group><article-title>Toward a clinically viable spectro- temporal modulation test for predicting supra-threshold speech reception in hearing-impaired listeners</article-title><source>Hearing Research</source><year>2023a</year><volume>427</volume><elocation-id>108650</elocation-id><pub-id pub-id-type="pmid">36463632</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaar</surname><given-names>J</given-names></name><name><surname>Simonsen</surname><given-names>LB</given-names></name><name><surname>Sanchez-Lopez</surname><given-names>R</given-names></name><name><surname>Laugesen</surname><given-names>S</given-names></name></person-group><article-title>The Audible Contrast Threshold (ACT™) test: a clinical spectro-temporal modulation detection test</article-title><source>medRxiv</source><year>2023b</year><elocation-id>2023-10</elocation-id><pub-id pub-id-type="pmid">39243488</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Zaar</surname><given-names>J</given-names></name><name><surname>Ihly</surname><given-names>P</given-names></name><name><surname>Nishiyama</surname><given-names>T</given-names></name><name><surname>Laugesen</surname><given-names>S</given-names></name><name><surname>Santurette</surname><given-names>S</given-names></name><name><surname>Tanaka</surname><given-names>C</given-names></name><etal/><name><surname>Jürgens</surname><given-names>T</given-names></name></person-group><source>Predicting speech-in-noise reception in hearing-impaired listeners with hearing aids using the Audible Contrast Threshold (ACT™) test</source><year>2023c</year></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zink</surname><given-names>ME</given-names></name><name><surname>Zhen</surname><given-names>L</given-names></name><name><surname>McHaney</surname><given-names>JR</given-names></name><name><surname>Klara</surname><given-names>J</given-names></name><name><surname>Yurasits</surname><given-names>K</given-names></name><name><surname>Cancel</surname><given-names>V</given-names></name><etal/><name><surname>Parthasarathy</surname><given-names>A</given-names></name></person-group><article-title>Increased listening effort and cochlear neural degeneration underlie behavioral deficits in speech perception in noise in normal hearing middle-aged adults</article-title><source>bioRxiv</source><year>2024</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>(A) Spectrogram representation of a STM stimulus used in the present study, showing the rate and scale parameters. (B) SM stimuli with harmonic carrier (SM<sub>harm</sub>) used in the psychophysical detection tasks as well as for the FFR measurements illustrated in (C) and (D), respectively.</p></caption><graphic xlink:href="EMS202027-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>(A) Pure-tone audiograms (PTA) in standard frequencies and extended high-frequency (yNH group in green, oNH group in blue, oHI group in red); stars in the extended high-frequency range show maximum presentation levels permitted with the experimental setup. (B) DPOAEs for the three groups of participants. (C) Relationship between hearing thresholds (HTs, expressed in hearing levels, HL) and DPOAEs either in the low-frequency (LF) range (left panel) or in the high-frequency (HF) range (right panel) obtained averaging the measurements below 1kHz or above 2kHz, respectively. (D) Age of individuals in the three groups.</p></caption><graphic xlink:href="EMS202027-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>(A) Speech reception thresholds (SRTs) obtained for the three groups of listeners in the three frequency filtering conditions (broadband/intact: BB; low-pass: LP; high-pass: HP). Outliers from the oHI group in the HP conditions (see the text for details) are highlighted. P-values (uncorrected) from post hoc two-tailed t-tests conducted across groups are reported for each condition. (B) Psychophysical detection thresholds obtained for the three groups of listeners against the three types of S(T)M stimuli. No significant differences (Ps &gt; .05) were found between groups for each type of stimulus.</p></caption><graphic xlink:href="EMS202027-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>(A) Average ENV and TFS components of FFRs to the RAM stimulus obtained in the three groups of subjects (yNH, oNH and oHI) after noise-floor correction (see Methods). Peaks at multiples of f0 (110 Hz), which correspond to neural phase-locking to stimulus characteristics, align with dashed lines; peaks falling outside these lines correspond to electrical noise in the measurement system. There was a clear harmonic structure in the FFRenv (top), but not in the FFR<sub>tfs</sub> (bottom). Note that the amplitude of the first peak (110Hz) in FFRenv for the yNH group extends beyond the displayed range (value is ~ 0.4 <italic>μ</italic>V, see panel B). (B) Right panels show the magnitude of harmonics #1 to #10 extracted from FFRenv (top) for the three groups (shaded regions, when visible, show standard error of the mean). Right panels show FFR strength for ENV component assessed by averaging the amplitude of harmonics #1 to #7, and then taking the log of this value. P-values (uncorrected) from two-tailed t-tested conducted on these scalar values expressed in dB are reported in the panel.</p></caption><graphic xlink:href="EMS202027-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>(A, B) Same as <xref ref-type="fig" rid="F4">Fig. 4</xref>, for both the FFR<sub>env</sub> and FFR<sub>tfs</sub> to the SM<sub>harm</sub> stimulus (<xref ref-type="fig" rid="F2">Fig. 2B</xref>).</p></caption><graphic xlink:href="EMS202027-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>(A). Relationships between SRTs the three conditions (BB, LP, HP) and FFRs for the SM stimulus (SM-FFR<sub>tfs</sub>, top panels) and RAM tone (RAM-FFR<sub>env</sub>, bottom panels). (B) Relationships between SRTs and psychophysical detection thresholds measured with the SM<sub>harm</sub> (top panels) and the SM<sub>harm</sub> stimulus (bottom panels). (C) Relationship between and psychophysical detection thresholds and FFR<sub>tfs</sub> (SM-FFR<sub>tfs</sub>) both obtained with the same SM<sub>harm</sub> stimulus.</p></caption><graphic xlink:href="EMS202027-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>(A-D) Relationships between SM-FFR<sub>tfs</sub> strength and age without correction (left panels) or after PTA-correction (right panels; see the text for details), considering either the average magnitude of harmonics #1-#7 (top panels) or harmonic #4 (bottom panels). In all panels, black lines show the linear regressions lines from analyses performed on the aggregated data of oNH and oHI individuals.</p></caption><graphic xlink:href="EMS202027-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>(A) Illustration of the computational model used in the present study (<xref ref-type="bibr" rid="R72">Verhulst et al., 2018</xref>) and the specific parameters adjusted for simulating different types of impairment. Model parameters were adjusted to simulate the combined effects of two aspects of hearing loss: (i) by adjusting the transmission line model coefficients to simulate different levels of OHC loss that qualitatively captured the variability of audiograms measured across our cohort of oHI listeners, as shown on the right panel where the modeled audiograms are superimposed with the measured audiograms (green: no OHC loss, red: sloping #1 and sloping #2); (ii) by reducing the number of AN fibers in the model to simulate different levels of synaptopathy (green: no synaptopathy corresponding to [13 HSR, 3 MSR, 3 LSR] AN fibers in the model, blue: [7 HSR, 0 MSR, 0 LSR] and [4 HSR, 0 MSR, 0 LSR]). The sum of AN responses across the remaining fibers was taken as an estimate of FFR. FFR<sub>env</sub> and FFR<sub>tfs</sub> were estimated as for the empirical work, by summing or subtracting the estimated FFRs to the stimuli with positive and negative polarities, respectively. (B) Combined effects of simulated OHC loss or/and synaptopathy estimated on FFR strength in response to the SM<sub>harm</sub> stimulus used in the experiments; ENV and TFS components on the y and x axes, respectively. Marker edge colors reflect the simulation of synaptopathy in the model (black: no; blue: 54% or 31% of remaining HSR fibers) and marker face colors reflect the simulation of OHC damage (green: no; red: sloping #1 or sloping #2 audiograms). Large changes in FFR strength (7-15 dB) both on ENV and TFS components can be observed for the two levels of synaptopathy simulated, while in contrast the different levels of simulated OHC loss yield much smaller effects (1-3 dB).</p></caption><graphic xlink:href="EMS202027-f008"/></fig></floats-group></article>