<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS152377</article-id><article-id pub-id-type="doi">10.1101/2021.10.25.465583</article-id><article-id pub-id-type="archive">PPR410829</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Deep neural networks and visuo-semantic models explain complementary components of human ventral-stream representational dynamics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Jozwik</surname><given-names>Kamila M</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Mur</surname><given-names>Marieke</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Psychology, University of Cambridge, Cambridge, UK</aff><aff id="A2"><label>2</label>Institute of Cognitive Science, University of Osnabrück, Osnabrück, Germany</aff><aff id="A3"><label>3</label>Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany</aff><aff id="A4"><label>4</label>Zuckerman Mind Brain Behavior Institute, Columbia University, New York, USA</aff><aff id="A5"><label>5</label>Department of Psychology, Western University, London, Canada</aff><aff id="A6"><label>6</label>Department of Computer Science, Western University, London, Canada</aff><author-notes><corresp id="CR1"><bold>Correspondence: <italic><email>jozwik.kamila@gmail.com</email>, <email>mmur@uwo.ca</email></italic></bold></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>09</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>08</day><month>08</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Deep neural networks (DNNs) are promising models of the cortical computations supporting human object recognition. However, despite their ability to explain a significant portion of variance in neural data, the agreement between models and brain representational dynamics is far from perfect. We address this issue by asking which representational features are currently unaccounted for in neural timeseries data, estimated for multiple areas of the ventral stream via source-reconstructed magnetoencephalography (MEG) data acquired in human participants (9 females, 6 males) during object viewing. We focus on the ability of visuo-semantic models, consisting of human-generated labels of object features and categories, to explain variance beyond the explanatory power of DNNs alone. We report a gradual reversal in the relative importance of DNN versus visuo-semantic features as ventral-stream object representations unfold over space and time. While lower-level visual areas are better explained by DNN features, especially during the early phase of the response (&lt; 128 ms after stimulus onset), higher-level cortical dynamics are best accounted for by visuo-semantic features during a later time window (starting 146 ms after stimulus onset). Among the visuo-semantic features, object parts and basic categories drive the advantage over DNNs. These results show that a significant component of the variance unexplained by DNNs in higher-level cortical dynamics is structured, and can be explained by readily nameable aspects of the objects. We conclude that current DNNs fail to fully capture dynamic representations in higher-level human visual cortex and suggest a path toward more accurate models of ventral stream computations.</p></abstract><kwd-group><kwd>vision</kwd><kwd>object recognition</kwd><kwd>features</kwd><kwd>categories</kwd><kwd>recurrent deep neural networks</kwd><kwd>source-reconstructed MEG data</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">When we view objects in our visual environment, the neural representation of these objects dynamically unfolds over time across the cortical hierarchy of the ventral visual stream. In brain recordings from both humans and nonhuman primates, this dynamic representational unfolding can be quantified from neural population activity, showing a staggered emergence of ecologically relevant object information such as facial features, followed by object categories, and then the individuation of these inputs into specific exemplars (<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R10">10</xref>). These neural reverberations are thought to reflect the cortical computations that support object recognition.</p><p id="P3">Deep neural networks (DNNs) have recently emerged as a promising computational framework for modeling these cortical computations (<xref ref-type="bibr" rid="R11">11</xref>). DNNs explain significant amounts of variance in neural data obtained from visual cortex in both humans and nonhuman primates (<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R22">22</xref>). The progression of object representations from shallower to deeper layers of DNNs roughly matches the progression from lower- to higher-level object representations measured in visual cortex as neural responses unfold over time and space (<xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R21">21</xref>). At the same time, DNNs also leave substantial amounts of variance in brain responses unexplained (<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R23">23</xref>). More recent DNNs that incorporate dynamics through recurrent processing provide additional explanatory power, possibly by better approximating the dynamic computations that the brain relies on for perceptual inference (<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R31">31</xref>). However, even the most state-of-the-art DNNs still leave a significant amount of variance in brain representations unexplained (<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R20">20</xref>), and differences among task-trained feedforward architectures are small (<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>), even after training and fitting (<xref ref-type="bibr" rid="R22">22</xref>). This raises the question of what representational features are left unaccounted for in the dynamic neural data.</p><p id="P4">To address this question, we enriched our modeling strategy with visuo-semantic object information. By “visuo-semantic”, we mean nameable properties of visual objects. Our visuo-semantic models consist of object labels generated by human observers, describing lower-level object features such as “green”, higher-level object features such as “eye”, and categories such as “face”. These object properties explain significant amounts of response variance in higher-level primate visual cortex (<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R45">45</xref>). Moreover, visuo-semantic models outperform DNNs (AlexNet (<xref ref-type="bibr" rid="R46">46</xref>) and VGG (<xref ref-type="bibr" rid="R47">47</xref>) architectures) at predicting perceived object similarity in humans (<xref ref-type="bibr" rid="R48">48</xref>). In addition, a recent fMRI study showed that combining DNNs with a semantic feature model is beneficial for explaining visual object representations at advanced processing stages of the ventral visual stream, especially perirhinal cortex (<xref ref-type="bibr" rid="R49">49</xref>). Given these findings, we hypothesized that visuo-semantic models capture representational features in ventral-stream neural dynamics that DNNs fail to account for.</p><p id="P5">We tested this hypothesis on temporally resolved magnetoencephalography (MEG) data, which can capture representational dynamics at a millisecond timescale. Human brain data acquired at this rapid sampling rate provide rich information about temporal dynamics, and by extension, about the underlying neural computations. For example, in a MEG study examining a visual delayed-match-to-sample task, time series analyses revealed distinct representational states over cue, delay, and response periods within individual experimental trials (<xref ref-type="bibr" rid="R9">9</xref>). In another MEG study that used source reconstruction to localize time series to distinct areas of the ventral stream, time series analyses revealed temporal inter-dependencies between areas suggestive of recurrent information processing (<xref ref-type="bibr" rid="R10">10</xref>).</p><p id="P6">In this work, we used representational similarity analysis to test both DNNs and visuo-semantic models for their ability to explain the representational dynamics observed across multiple ventral stream areas in the human brain. As DNNs, we used feedforward CORnet-Z and locally recurrent CORnet-R, which are inspired by the visual hierarchy of monkey visual cortex (<xref ref-type="bibr" rid="R27">27</xref>). As visuo-semantic models, we used existing human-generated object labels of colors, textures, shapes, object parts, subordinate categories, basic categories, and superordinate categories (<xref ref-type="bibr" rid="R45">45</xref>). We analyzed previously published source-reconstructed MEG data acquired in healthy human participants while they were viewing object images from a range of categories (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R10">10</xref>). We investigated three distinct stages of processing in the ventral cortical hierarchy: lower-level visual areas V1-3, intermediate visual areas V4t/LO, and higher-level visual areas IT/PHC. At each stage of processing, we tested both model classes for their ability to explain variance in the temporally evolving representations. This strategy allowed us to test what visuo-semantic object information is unaccounted for by DNNs where and when during ventral-stream processing.</p><p id="P7">Our modeling strategy revealed that DNNs and visuo-semantic models explain complementary components of ventral-stream representational dynamics. DNNs outperformed visuo-semantic models in lower-level visual cortex, especially during the early phases of the response (&lt; 128 ms after stimulus onset), but failed to capture a significant component of representational variance in higher-level visual cortex, for a prolonged time window starting at 146 ms after stimulus onset. This variance was accounted for by visuo-semantic labels of object parts and basic categories. For intermediate visual cortex, the explanatory power of the two model classes did not significantly differ, but the unique contribution of the DNNs peaked before that of the visuo-semantic models (around 100 versus 150 ms, respectively). The visuo-semantic peak occurred just after a period of feedback information flow from higher-level to intermediate visual cortex. Our findings point toward priorities for future modeling efforts, which should move to better capturing the dynamic computations that give rise to the emergence of visuo-semantic features represented in higher-level human visual cortex.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>DNNs better explain lower-level visual representations, visuo-semantic models better explain higher-level visual representations</title><p id="P8">We first evaluated the overall ability of the DNN and visuo-semantic models to explain the time course of information processing along the human ventral visual stream. We hypothesized that visuo-semantic models capture representational features in neural data that DNNs may fail to account for. <xref ref-type="fig" rid="F1">Figure 1</xref> shows an overview of our approach. We computed representational dissimilarity matrix (RDM) movies from the source-reconstructed MEG data to characterize how the ventral-stream object representations evolved over time in each participant. We computed a RDM movie for each participant and region of interest (ROI) and explained variance in the movies using a DNN model and a visuo-semantic model. The DNN model consisted of internal object representations in layers of CORnet-Z, a purely feedforward model, and CORnet-R, a locally recurrent variant (<xref ref-type="bibr" rid="R27">27</xref>), to account for both feedforward and locally recurrent computations. The visuo-semantic model consisted of human-generated labels of object features (e.g., “brown”, “furry”, “round”, “ear”; 119 labels) and categories (e.g., “great dane”, “dog”, “organism”; 110 labels) for the object images presented during the MEG experiment (<xref ref-type="bibr" rid="R45">45</xref>). We computed model predictions by linearly combining either all DNN layers or all visuo-semantic labels to best explain variance in the RDM movies across time. We evaluated the model predictions on data for images left out during fitting. For each model, we tested if and when the variance explained in the RDM movies exceeded the prestimulus baseline using a one-sided Wilcoxon signed-rank test. We also tested if and when the amounts of explained variance differed between the two models using a two-sided Wilcoxon signed-rank test. We controlled the expected false discovery rate at 0.05 across time points.</p><p id="P9">For lower-level visual cortex (V1-3), the DNN model explained significant amounts of variance between 60 and 638, and 818 and 884 ms after stimulus onset, while the visuo-semantic model did so between 118 and 660 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2a</xref>). The DNN model explained more variance than the visuo-semantic model during the early (66 - 128 ms) as well as the late (422 - 516 ms, 520 - 544 ms, 820 - 844 ms) phases of the response. For intermediate visual cortex (V4t/LO), the DNN model explained variance between 62 and 610 ms after stimulus onset, while the visuo-semantic model explained variance between 110 and 562 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2a</xref>). The amount of explained variance did not significantly differ between the two models. The results for lower-level visual cortex indicate that the DNN model outperformed the visuo-semantic model at explaining object representations, especially during the early phase of the response (&lt; 128 ms after stimulus onset). In contrast, for higher-level visual cortex (IT/PHC), the visuo-semantic model outperformed the DNN model. The DNN model explained variance only between 182 and 270 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2a</xref>). The visuo-semantic model explained variance during a longer time window, between 96 and 658 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2a</xref>). Furthermore, the visuo-semantic model explained more variance than the DNN model between 146 and 488 ms after stimulus onset (specifically 146 - 188 ms, 196 - 234 ms, 326 - 344 ms, 348 - 402 ms, 412 - 464 ms, 468 - 488 ms). In summary, the results across the ventral stream regions show a reversal in which model best explains variance in the RDM movies, from the DNN model in lower-level visual cortex to the visuo-semantic model in higher-level visual cortex.</p></sec><sec id="S4"><title>Visuo-semantic models explain unique variance in higher-level visual representations</title><p id="P10">Our results suggest that DNNs and visuo-semantic models explain complementary components of human ventral-stream representational dynamics. To explicitly test this hypothesis, we assessed the unique contributions of the two models. For this, we first computed the best RDM predictions for each model class, and then used the resulting cross-validated RDM predictions in a second-level general linear model (GLM) in which we combined the two model classes. We computed the unique contribution of a model class by subtracting the variance explained by the reduced model (i.e. the GLM without the model class of interest) from the variance explained by the full model (including both model classes). For lower-level visual cortex (V1-3), the DNN model explained unique variance between 60 and 638, and 818 and 884 ms after stimulus onset, while the visuo-semantic model did so between 124 and 412 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2b</xref>). For intermediate visual cortex (V4t/LO), the DNN model explained unique variance between 62 and 610 ms after stimulus onset, while the visuo-semantic model did so between 118 and 546 ms after stimulus onset (<xref ref-type="fig" rid="F2">Figure 2b</xref>). These results indicate that the DNN and visuo-semantic models each explained a significant amount of unique variance in lower-level and intermediate visual cortex compared to the baseline period. However, for lower-level visual cortex, the DNN model explained more unique variance than the visuo-semantic model during the early (66 - 128 ms) as well as the late phases of the response (422 - 516 ms, 520 - 544 ms, 820 - 844 ms). For intermediate visual cortex, the unique variance explained did not significantly differ between the two models. For higher-level visual cortex (IT/PHC), only the visuo-semantic model explained unique variance, between 104 and 640 ms after stimulus onset (specifically 104 - 464 ms, 468 - 500, 542 - 578, and 608 - 640). Furthermore, the visuo-semantic model explained significantly more unique variance than the DNN model between 146 and 488 ms after stimulus onset (specifically 146 - 188 ms, 196 - 234 ms, 326 - 344 ms, 348 - 402 ms, 412 - 464 ms, 468 - 488 ms, <xref ref-type="fig" rid="F2">Figure 2b</xref>). These results indicate that, in the context of a visuo-semantic predictor, the tested DNNs explain unique variance at lower-level but not higher-level stages of visual processing which instead show a unique contribution of visuo-semantic models. Visuo-semantic models appear to explain components of the higher-level visual representations that DNNs fail to fully capture.</p></sec><sec id="S5"><title>Object parts and basic categories contribute to the unique variance explained by visuo-semantic models in higher-level visual representations</title><p id="P11">To better understand which components of the visuo-semantic model contribute to explaining unique variance in the higher-level visual representations, we repeated our analyses separately for subsets of object features and subsets of categories. We grouped the visuo-semantic labels into the following subsets: object parts, color, shape, and texture, and subordinate, basic, and superordinate categories (<xref ref-type="fig" rid="F1">Figure 1b</xref>). We found that, among the categories, subordinate and basic categories explained variance in higher-level visual cortex (IT/PHC) (<xref ref-type="fig" rid="F3">Figure 3a</xref>). Furthermore, each of these models explained unique variance in higher-level visual cortex, while the DNN model did not (<xref ref-type="fig" rid="F3">Figure 3b</xref>). Among the object features, only object parts explained variance in higher-level visual cortex (<xref ref-type="fig" rid="F3">Figure 3a</xref>). Furthermore, object parts explained unique variance in higher-level visual cortex, while the DNN model did not (<xref ref-type="fig" rid="F3">Figure 3b</xref>). We next evaluated the three best predictors among the object features and categories together in the context of the DNN predictor. While object parts, subordinate categories, basic categories, and DNNs all explained variance in higher-level visual cortex, only object parts and basic categories explained unique variance (<xref ref-type="fig" rid="F3">Figure 3b</xref>).</p></sec><sec id="S6"><title>Deep neural networks and visuo-semantic models explain complementary components of human ventral-stream representational dynamics</title><p id="P12">To summarize our results, we computed a model difference score by subtracting the unique variance explained by the visuo-semantic model from that explained by the DNN model in the dynamic ventral-stream representations. <xref ref-type="fig" rid="F4">Figure 4</xref> displays the model difference score for each of the three regions of interest for the first 300 ms of image processing. Results show a gradual reversal in the relative importance of DNN versus visuo-semantic features in explaining ventral-stream object representations as they unfold over space and time. DNN features lead in lower-level visual areas V1-3 early in time (&lt; 128 ms after stimulus onset), while visuo-semantic features lead in higher-level visual areas IT/PHC later in time (&gt; 146 ms after stimulus onset). Among the visuo-semantic features, object parts and basic categories drive the advantage over DNNs. While the relative contributions of DNN and visuo-semantic features did not significantly differ in intermediate visual areas V4t/LO, the observed timing of events across the three regions of interest provides complementary information. Within the first 100 ms after stimulus onset, the relative contribution of DNN features peaks in lower-level and intermediate visual areas. Around 120 ms after stimulus onset, the relative contribution of visuo-semantic features peaks in <italic>higher-level</italic> visual areas. Around 150 ms after stimulus onset, the relative contribution of visuo-semantic features peaks in <italic>intermediate</italic> visual areas, following a period of feedback information flow from higher-level to intermediate visual areas (<xref ref-type="bibr" rid="R10">10</xref>). Together, these results suggest that DNNs and visuo-semantic models explain complementary components of human ventral-stream representational dynamics. Moreover, our results show that a significant component of the variance unexplained by DNNs in higher-level visual areas is structured, and can be explained by relatively simple, readily nameable aspects of the images.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P13">Neural representations of visual objects dynamically unfold over time as we are making sense of the visual world around us. These representational dynamics are thought to reflect the cortical computations that support human object recognition. Here we show that DNNs and human-derived visuo-semantic models explain complementary components of representational dynamics in the human ventral visual stream, estimated via source-reconstructed MEG data. We report a gradual reversal in the importance of DNN and visuo-semantic features from lower- to higher-level visual areas. DNN features explain variance over and above visuo-semantic features in lower-level visual areas V1-3 especially during the early phase of the response (&lt; 128 ms after stimulus onset). In contrast, visuo-semantic features explain variance over and above DNN features in higher-level visual areas IT/PHC during a later time window (starting at 146 ms after stimulus onset). Among the visuo-semantic features, object parts and basic categories drive the explanatory advantage over DNNs. Consistent with our hypothesis, our findings suggest that current DNNs fail to fully capture the visuo-semantic features represented in higher-level human visual cortex, and suggest a path towards more accurate models of ventral stream computations.</p><p id="P14">Our finding that DNNs outperform visuo-semantic models at explaining lower-level cortical dynamics replicates and extends prior functional magnetic resonance imaging (fMRI) work, which showed that DNNs explain response variance across all stages of the ventral stream while visuo-semantic models predominantly explain response variance in higher-level visual cortex (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R49">49</xref>). However, the contribution of such models to explaining representational dynamics has not been investigated. Using source-reconstructed MEG data, we show that the advantage of DNNs over visuo-semantic models is strongest within the first 128 ms after stimulus onset. During this early time window, the response is likely dominated by feedforward and local recurrent processing as opposed to top-down feedback signals from higher-level areas (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R11">11</xref>). Our results show the importance of analyzing temporally resolved neuroimaging data for revealing when in time competing models account for the rapid dynamic unfolding of human ventral-stream representations.</p><p id="P15">Our findings show that DNNs, despite reaching human-level performance on large-scale object recognition tasks (<xref ref-type="bibr" rid="R20">20</xref>), fail to fully capture visuo-semantic features represented in higher-level human visual cortex, in particular object parts and basic categories. These results suggest that current DNNs do not fully develop the representational features that give rise to the categorical divisions observed in higher-level visual representations (<xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R45">45</xref>). In line with this, prior fMRI work showed that DNNs only adequately accounted for higher-level visual representations after adding new representational features (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>). The new features were either explicit semantic features (<xref ref-type="bibr" rid="R49">49</xref>) or were created by linearly combining DNN features to emphasize categorical divisions observed in the higher-level visual representations, including the division between faces and nonfaces and between animate and inanimate objects (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R50">50</xref>). Our results are also consistent with an earlier MEG study which showed that adding semantic features to a simpler HMAX model was beneficial for modeling object representations in visual cortex starting around 200 ms after stimulus onset (<xref ref-type="bibr" rid="R51">51</xref>). DNNs may, at least in part, use different object features for object recognition than humans do. This conclusion is consistent with prior reports that DNNs rely more strongly on lower-level image features such as texture for object categorization (<xref ref-type="bibr" rid="R52">52</xref>).</p><p id="P16">Our study makes several important contributions to the existing body of work on modeling ventral-stream computations with DNNs. First, our results suggest that introducing locally recurrent connections to DNNs, to more closely match the architecture of the ventral visual stream, is not sufficient to fully capture the representational dynamics observed in higher-level human visual cortex. Second, our results tie together space and time through analysis of source-reconstructed MEG data. We show that DNNs outperform visuo-semantic models in lower-level visual areas V1-3 during the first 128 ms of processing, while visuo-semantic models outperform DNNs in higher-level visual areas IT/PHC starting at 146 ms after image onset. Third, we show that a significant component of the unexplained variance in higher-level cortical dynamics is structured, and can be explained by readily nameable aspects of object images, specifically object parts and basic categories. In prior behavioral work using the same image set and visuo-semantic labels, we showed that category labels, but not object parts, outperformed DNNs at explaining object similarity judgements (<xref ref-type="bibr" rid="R48">48</xref>). This highlights a difference with the current results, which show a role for both object parts and categories in accounting for response variance in higher-level visual cortex over and above DNNs. These results are consistent with the idea that, compared to responses in ventral visual cortex, behavioral similarity judgements may more strongly emphasize semantic object information (<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R53">53</xref>). Future studies should extend this work to richer stimulus and model sets.</p><p id="P17">To build more accurate models of human ventral stream computations, we need to provide DNNs with a more human-like learning experience. Two important areas for improvement are visual diet and learning objectives. Each of these shape the internal object representations that develop during visual learning. Humans have a rich visual diet and learn to distinguish between ecologically relevant categories at multiple levels of abstraction, including faces, humans, and animals (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R53">53</xref>). DNNs have a more constrained visual diet and are trained on category divisions that do not entirely match the ones that humans learn in the real world. For example, the most common large-scale image dataset for training DNNs with category supervision (<xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R54">54</xref>), the ILSVRC 2012 dataset (<xref ref-type="bibr" rid="R54">54</xref>), contains subordinate categories that most humans would not be able to distinguish, including dog breeds such as “schipperke” and “groenendael”, and lacks some higher-level categories relevant to humans, including “face” and “animal”. The path forward is unfolding along two main directions. The first is enrichment of the visual diet of DNNs by better matching the visual variability present in the real world, for example by increasing variability in viewpoint or by training on videos instead of static images (<xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R56">56</xref>). The second is to more closely match human learning objectives, for example by introducing more human-like category objectives or unsupervised objectives (<xref ref-type="bibr" rid="R57">57</xref>–<xref ref-type="bibr" rid="R60">60</xref>). Training DNNs on more human-like visual diets and learning objectives may give rise to representational features that more closely match the visuo-semantic features represented in human higher-level visual cortex.</p></sec><sec id="S8" sec-type="methods"><title>Methods</title><sec id="S9"><title>Stimuli</title><p id="P18">Stimuli were 92 colored images of real-world objects spanning a range of categories, including humans, non-human animals, natural objects, and manmade objects (12 human body parts, 12 human faces, 12 animal bodies, 12 animal faces, 23 natural objects, and 21 manmade objects). Objects were segmented from their backgrounds (<xref ref-type="fig" rid="F1">Figure 1a</xref>) and presented to human participants and models on a gray background.</p></sec><sec id="S10"><title>Visuo-semantic models</title><p id="P19">Feature and category visuo-semantic models have been described in (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R48">48</xref>), where further details can be found.</p><sec id="S11"><title>Definition of visuo-semantic models</title><p id="P20">To create visuo-semantic models, human observers generated feature labels (e.g., “eye”) and category labels (e.g., “animal”) for the 92 images (<xref ref-type="bibr" rid="R45">45</xref>). The visuo-semantic models are schematically represented in <xref ref-type="fig" rid="F1">Figure 1b</xref>. Feature labels were divided into object parts, colors, textures and shapes, while category labels were divided into subordinate categories, basic categories and superordinate categories. Labels were obtained in a set of two experiments. In Experiment 1, a group of 15 human observers (mean age=26 years; 11 females) generated feature and category labels for the object images. Human observers were native English speakers and had normal or corrected-to-normal vision. In the instruction, we defined features as “visible elements of the shown object, including object parts, colors, textures and shapes”. We defined a category as “a group of objects that the shown object is an example of”. The instruction contained two example images, not part of the 92 object-image set, with category and feature descriptions. We asked human volunteers to list a minimum of five descriptions, both for categories and for features. The 92 images were shown, in random order, on a computer screen using a web-based implementation, with text boxes next to each image for human observers to type category or feature descriptions. We subsequently selected, for categories and features separately, those descriptions that were generated by at least three out of 15 human observers. This threshold corresponds to the number of human observers that, on average, mentioned a particular category or feature for a particular image. The threshold is relatively lenient, but it allows the inclusion of a rich set of descriptions, which were further pruned in Experiment 2. We subsequently removed descriptions that were either inconsistent with the instructions or redundant. Observers generated 212 feature labels and 197 category labels. These labels are the model dimensions. In Experiment 2, a separate group of 14 human observers (mean age=28 years; seven females) judged the applicability of each model dimension to each image, thereby validating the dimensions generated in Experiment 1, and providing, for each image, its value (present or absent) on each of the dimensions. Human observers were native English speakers and had normal or corrected-to-normal vision. During the experiment, the object images and the descriptions, each in random order, were shown on a computer screen using a web-based implementation. The object images formed a column, while the descriptions formed a row; together they defined a matrix with one entry, or checkbox, for each possible image-description pair. We asked the human observers to judge for each description, whether it correctly described each object image, and if so, to tick the associated checkbox. The image values on the validated model dimensions define the model (when agreed by at least 75% of human observers from Experiment 2). To increase the stability of the models during subsequent fitting, we iteratively merged binary vectors that were highly correlated (r &gt; 0.9), alternately computing pairwise correlations between the vectors, and averaging highly-correlated vector pairs, until all pairwise correlations were below threshold. The final full feature and category models consisted of 119 and 110 dimensions, respectively.</p></sec><sec id="S12"><title>Construction of the visuo-semantic representational dissimilarity matrices</title><p id="P21">To compare the models to the measured brain representations, the models and the data should reside in the same representational space. This motivates transforming our models to RDM space. For each model dimension, we computed, for each pair of images, the squared difference between their values on that dimension. The squared difference reflects the dissimilarity between the two images in a pair. Given that a specific feature or category can either be present or absent in a particular image, image dissimilarities along a single model dimension are binary: they are zero if a feature or category is present or absent in both images, and one if a feature or category is present in one image but absent in the other. The dissimilarities were stored in an RDM, yielding as many RDMs as model dimensions. The full feature RDM model consists of 119 RDMs; the full category RDM model consists of 110 RDMs. Subsets of the feature and category RDM models consist of the following number of RDMs: object parts (82), shape (15), color (10), texture (12), subordinate categories (38), basic categories (67), superordinate categories (5).</p></sec></sec><sec id="S13"><title>Deep neural networks</title><p id="P22">CORnet-Z and CORnet-R architectures have been described in (<xref ref-type="bibr" rid="R27">27</xref>), where further details can be found.</p><sec id="S14"><title>Architecture and training</title><p id="P23">We used feedforward (CORnet-Z) and locally recurrent (CORnet-R) (<xref ref-type="bibr" rid="R27">27</xref>) models in our analyses. The architectures of the two DNNs are schematically represented in <xref ref-type="fig" rid="F1">Figure 1b</xref>. The architecture of CORnets is inspired by the anatomy of monkey visual cortex where each processing stage in the model is thought to correspond to areas V1, V2, V4, and IT respectively (<xref ref-type="bibr" rid="R27">27</xref>). The output of the last model area is mapped to the model’s behavioral choices using a linear decoder. We chose these DNNs because they have similar architectures with one being feedforward and the other locally recurrent, their architecture is inspired by the primate visual system, they are one of the best models for predicting visual responses in human IT (<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>) and monkey IT (<xref ref-type="bibr" rid="R20">20</xref>), and their architectures are relatively simple compared to other DNNs. Each “visual area” in CORnet-Z (“Zero”) consists of a single convolution, followed by a ReLU nonlinearity and max pooling. CORnet-R (“Recurrent”) introduces local recurrent dynamics within an area. The recurrence occurs only within an area; there are no bypass or feedback connections between areas. For each area, the input is down-scaled twofold and the number of channels is increased twofold by passing the input through a convolution, followed by group normalization (<xref ref-type="bibr" rid="R61">61</xref>) and a ReLU nonlinearity. The area’s internal state (initially zero) is added to the result and passed through another convolution, again followed by group normalization and a ReLU nonlinearity, resulting in the new internal state of the area. At time step 0 “t0” there is no input to “V2” and above layers, and as a consequence no image-elicited activity is present. From time step “t1” onwards, the image-elicited activity is present in all “visual areas” as the output of the previous area is immediately propagated forward. CORnet-R was trained using five time steps (“t0” - “t4”). Both DNNs were trained on 1.2 million images from the ILSVRC data base (<xref ref-type="bibr" rid="R54">54</xref>). The task was to classify each image as containing an object in one of 1,000 possible categories.</p></sec><sec id="S15"><title>Construction of the DNN representational dissimilarity matrices</title><p id="P24">Representations of the 92 images were computed from the layers of CORnet-Z and CORnet-R. For CORnet-Z, we included the decoder layer and the final processing stage (output) from each “visual area” layer, which resulted in 5 layers. For CORnet-R, we included the decoder layer and the final processing stage from each “visual area” layer for each time step, which resulted in 21 layers. For each layer of CORnet-Z and CORnet-R, we extracted the unit activations in response to the images and converted these into one activation vector per image. For each pair of images, we computed the dissimilarity (1 minus Spearman’s correlation) between the activation vectors. This yielded an RDM for each DNN layer. The resulting RDMs capture which stimulus information is emphasized and which is de-emphasized by the DNNs at different stages of processing.</p></sec></sec><sec id="S16"><title>MEG source-reconstructed data</title><p id="P25">Acquisition and analysis of the MEG data have been described in (<xref ref-type="bibr" rid="R6">6</xref>), where further details can be found. The source reconstruction of the MEG data has been described in (<xref ref-type="bibr" rid="R10">10</xref>), where further details can be found.</p><sec id="S17" sec-type="subjects"><title>Participants</title><p id="P26">Sixteen healthy human volunteers participated in the MEG experiment (mean age = 26, 10 females). MEG source reconstruction analyses were performed for a subset of 15 participants for whom structural and functional MRI data were acquired. Participants had normal or corrected-to-normal vision. Before scanning, the participants received information about the procedure of the experiment and gave their written informed consent for participating. The experiment was conducted in accordance with the Ethics Committee of the Massachusetts Institute of Technology Institutional Review Board and the Declaration of Helsinki.</p></sec><sec id="S18"><title>Experimental design and task</title><p id="P27">Stimuli were presented at the centre of the screen for 500 ms, while participants performed a paper clip detection task. Stimuli were overlaid with a light gray fixation cross and displayed at a width of 2.9° visual angle. Participants completed 10 to 14 runs. Each image was presented twice in every run in random order. Participants were asked to press a button and blink their eyes in response to a paper clip image shown randomly every 3 to 5 trials. These trials were excluded from further analyses. Each participant completed two MEG sessions.</p></sec><sec id="S19"><title>MEG data acquisition and preprocessing</title><p id="P28">MEG signals were acquired from 306 channels (204 planar gradiometers, 102 magnetometers) using an Elekta Neuromag TRIUX system (Elekta) at a sampling rate of 1,000 Hz. The data were bandpass filtered between 0.03 and 330 Hz, cleaned using spatiotemporal filtering, and down-sampled to 500 Hz. Baseline correction was performed using a time window of 100 ms before stimulus onset.</p></sec><sec id="S20"><title>MEG source reconstruction</title><p id="P29">The source reconstructions were performed using the MNE Python toolbox (<xref ref-type="bibr" rid="R62">62</xref>). We used participant individual structural T1 scans to obtain volume conduction estimates using single layer boundary element models (BEMs) based on the inner skull boundary. Instead of BEMs being based on the FreeSurfer watershed algorithm originally used in the MNE Python toolbox, we extracted BEMs using FieldTrip as the original method yielded poor reconstruction results. The source space consisted of 10,242 source points per hemisphere. The source points were positioned along the gray/white matter boundary, as estimated via FreeSurfer. We defined source orientations as surface normals with a loose orientation constraint. We used an iterative closest point procedure for MEG/MRI alignment based on fiducials and digitizer points along the head surface, after initial alignment based on fiducials. We estimated the sensor noise covariance matrix from the baseline period (100 ms to 0 ms before stimulus onset) and regularized it according to the Ledoit–Wolf procedure (<xref ref-type="bibr" rid="R63">63</xref>). We projected source activations onto the surface normal, obtaining one activation estimate per point in source space and time. Source reconstruction allowed us to estimate temporal dynamics in specific brain regions. Source reconstruction provides an estimate of what brain regions the information is coming from rather than a direct measurement of representations in different brain regions (see (<xref ref-type="bibr" rid="R64">64</xref>) for a discussion on that topic).</p></sec><sec id="S21"><title>Definition of regions of interest</title><p id="P30">We used a multimodal brain atlas (<xref ref-type="bibr" rid="R65">65</xref>) to define ROIs. We defined three ROIs covering low-level (V1–3), intermediate (V4t/LO1–3), and high-level visual areas (IT/PHC, consisting of TE1-2p, FFC, VVC, VMV2–3, PHA1–3). We converted the atlas annotation files to fsaverage coordinates (<xref ref-type="bibr" rid="R66">66</xref>) and mapped them to each participant using spherical averaging.</p></sec><sec id="S22"><title>Construction of the MEG representational dissimilarity matrices</title><p id="P31">We computed temporally changing RDM movies from the source-reconstructed MEG data for each participant, ROI, hemisphere, and session. We first extracted a trial-average multi-variate source time series for each stimulus. We then computed an RDM at each time point by estimating the pattern distance between all pairs of images using correlation distance (1 minus Pearson correlation). The RDM movies were averaged across hemispheres and sessions, resulting in one RDM movie for each participant and ROI.</p></sec></sec><sec id="S23"><title>Weighted representational modeling</title><p id="P32">Weighted representational modeling used here has been described in (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R67">67</xref>), where further details can be found. We could predict the brain representations by making the assumption that each model dimension contributes equally to the representation. We use the squared Euclidean distance as our representational dissimilarity measure, which is the sum across dimensions of the squared response difference for a given pair of stimuli. The squared differences simply sum across dimensions, so the model prediction would be the sum of the single-dimension model RDMs. However, we expect that not all model dimensions contribute equally to brain representations. To improve model performance, we linearly combined the different model dimensions (features and categories for the visuo-semantic models, layers for the DNNs, see below for details) to yield an object representation that best predicts the source-reconstructed MEG data. Because the squared differences sum across dimensions in the squared Euclidean distance, weighting the dimensions and computing the RDM is equivalent to a weighted sum of the single-dimension RDMs. When a dimension is multiplied by weight <italic>w</italic>, then the squared differences along that dimension are multiplied by <italic>w</italic><sup>2</sup>. We can therefore perform the fitting on the RDMs. We estimated the representational weights, one for each single-dimension RDM, using regularized (L2) linear regression, implemented in MATLAB using Glmnet (<ext-link ext-link-type="uri" xlink:href="https://hastie.su.domains/glmnet_matlab/?">https://hastie.su.domains/glmnet_matlab/?</ext-link>). Glmnet implements elastic net regularized regression using cyclical coordinate descent. We used standard settings (including standardization of the predictors before fitting), except that we constrained the weights to be non-negative. To prevent biased model performance estimates due to overfitting to a particular set of images, model performance was estimated by cross-validation to a subset of the images held out during model fitting. For each cross-validation fold, we randomly selected 84 of the 92 images as the training set and eight images as the test set (with the constraint that test images had to contain four animate objects (two faces and two body parts) and four inanimate objects) and used the corresponding pairwise dissimilarities to estimate the model weights. The model weights were then used to predict the pairwise dissimilarities for the eight left-out images. This procedure was repeated many times until predictions were obtained for all pairwise dissimilarities. For each cross-validation fold (using different test and train images each time), we determined the best regularization parameter (i.e. the one with the minimum squared error between prediction and data) using nested cross-validation to held-out images within the training set. We performed the fitting procedure for each participant, each time point, and each brain region of the MEG source-reconstructed data.</p></sec><sec id="S24"><title>Variance and unique variance analysis</title><p id="P33">We used a GLM to evaluate variance and unique variance explained by the models in the source-reconstructed MEG data (<xref ref-type="bibr" rid="R10">10</xref>). For each model m, the variance explained <italic>R</italic><sup>2</sup> was computed by GLM fitting using X = “model m” and Y = data. For each model m, the unique variance explained was computed by subtracting the total variance explained by the reduced GLM (excluding the model of interest) from the total variance explained by the full GLM. Specifically, for model m, we fit the GLM using X = “all models but m” and Y = data, then we subtracted the resulting <italic>R</italic><sup>2</sup> from the total <italic>R</italic><sup>2</sup> (fit the GLM using X = “all models” and Y = data). We performed this procedure for each participant and ROI. We used non-negative least squares to find optimal weights as RDMs consist of dissimilarity estimates, which cannot be negative. A constant term was included in the GLM model to correct for homogeneous changes in dissimilarity across the whole RDM. To evaluate the significance of the variance explained and unique variance explained by each model across participants, we first subtracted an estimate of the prestimulus baseline in each participant and then performed a one-sided Wilcoxon signed-rank test against 0. The prestimulus baseline was defined as the average (unique) variance explained between 200 - 0 ms before stimulus onset. We also tested if and when the (unique) variance explained differed between the two models using a two-sided Wilcoxon signed-rank test. We controlled the expected false discovery rate at 0.05 across time points for each model evaluation, model comparison, and ROI. Variance and unique variance explained and standard error lines were low-pass filtered at 80 Hz (Butterworth IIR filter; order 6) for easier visibility. Statistical inference is based on unsmoothed data.</p></sec></sec></body><back><ack id="S25"><title>Acknowledgements</title><p>This research was supported by the Wellcome Trust grant [206521/Z/17/Z] awarded to KMJ; the Alexander von Humboldt Foundation postdoctoral fellowship awarded to KMJ, the German Research Council grants [CI241/1-1, CI241/3-1 CI241/7-1] awarded to RMC, the European Research Council grant [ERC-StG-2018-803370] awarded to RMC, and a Natural Sciences and Engineering Research Council of Canada Discovery Grant [RGPIN-2019-06741] awarded to MM. We thank Martin Schrimpf for giving his input on the CORnets Methods section. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><sec id="S26" sec-type="data-availability"><title>Data Availability</title><p id="P34">The datasets generated during the current study are available from the corresponding authors on request.</p></sec><sec id="S27" sec-type="data-availability"><title>Code Availability</title><p id="P35">The code generated during the current study is available from the corresponding authors on request.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P36"><bold>Competing Financial Interests</bold></p><p id="P37">The authors declare no competing interests.</p></fn><fn fn-type="con" id="FN2"><p id="P38"><bold>Author Contributions</bold></p><p id="P39">KMJ, TCK, RMC, NK, and MM designed the experiments. KMJ collected behavioral data for visuo-semantic model building. RMC collected MEG data. TCK provided source-reconstructed MEG data and second-level GLM code. KMJ performed the analyses. KMJ and MM wrote the paper. All authors edited the paper. NK and MM supervised the work.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugase</surname><given-names>Yasuko</given-names></name><name><surname>Yamane</surname><given-names>Shigeru</given-names></name><name><surname>Ueno</surname><given-names>Shoogo</given-names></name><name><surname>Kawano</surname><given-names>Kenji</given-names></name></person-group><article-title>Global and fine information coded by single neurons in the temporal visual cortex</article-title><source>Nature</source><year>1999</year><volume>400</volume><issue>6747</issue><fpage>869</fpage><lpage>873</lpage><pub-id pub-id-type="pmid">10476965</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>Chou P</given-names></name><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><name><surname>Poggio</surname><given-names>Tomaso</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Fast Readout of Object Identity from Macaque Inferior Temporal Cortex</article-title><source>Science</source><year>2005</year><volume>310</volume><issue>5749</issue><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname><given-names>Ethan M</given-names></name><name><surname>Freedman</surname><given-names>David J</given-names></name><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name><name><surname>Miller</surname><given-names>Earl K</given-names></name><name><surname>Poggio</surname><given-names>Tomaso</given-names></name></person-group><article-title>Dynamic Population Coding of Category Information in Inferior Temporal and Prefrontal Cortex</article-title><source>Journal of Neurophysiology</source><year>2008</year><volume>100</volume><issue>3</issue><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="pmcid">PMC2544466</pub-id><pub-id pub-id-type="pmid">18562555</pub-id><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>Thomas</given-names></name><name><surname>Tovar</surname><given-names>David A</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Representational dynamics of object vision : The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">23908380</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>Alex</given-names></name><name><surname>Taylor</surname><given-names>Kirsten I</given-names></name><name><surname>Devereux</surname><given-names>Barry</given-names></name><name><surname>Randall</surname><given-names>Billi</given-names></name><name><surname>Tyler</surname><given-names>Lorraine K</given-names></name></person-group><article-title>From Perception to Conception: How Meaningful Objects Are Processed over Time</article-title><source>Cerebral Cortex</source><year>2013</year><volume>23</volume><issue>1</issue><fpage>187</fpage><lpage>197</lpage><pub-id pub-id-type="pmcid">PMC3619663</pub-id><pub-id pub-id-type="pmid">22275484</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs002</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>Radoslaw Martin</given-names></name><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name><name><surname>Oliva</surname><given-names>Aude</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>62</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>Leyla</given-names></name><name><surname>Meyers</surname><given-names>Ethan M</given-names></name><name><surname>Leibo</surname><given-names>Joel Z</given-names></name><name><surname>Poggio</surname><given-names>Tomaso</given-names></name></person-group><article-title>The dynamics of invariant object recognition in the human visual system</article-title><source>Journal of Neurophysiology</source><year>2014</year><volume>111</volume><issue>1</issue><fpage>91</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC4280161</pub-id><pub-id pub-id-type="pmid">24089402</pub-id><pub-id pub-id-type="doi">10.1152/jn.00394.2013</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghuman</surname><given-names>Avniel Singh</given-names></name><name><surname>Brunet</surname><given-names>Nicolas M</given-names></name><name><surname>Li</surname><given-names>Yuanning</given-names></name><name><surname>Konecky</surname><given-names>Roma O</given-names></name><name><surname>Pyles</surname><given-names>John A</given-names></name><name><surname>Walls</surname><given-names>Shawn A</given-names></name><name><surname>Destefino</surname><given-names>Vincent</given-names></name><name><surname>Wang</surname><given-names>Wei</given-names></name><name><surname>Richardson</surname><given-names>R Mark</given-names></name></person-group><article-title>Dynamic encoding of face information in the human fusiform gyrus</article-title><source>Nature Communications</source><year>2014</year><volume>5</volume><issue>1</issue><fpage>5672</fpage><pub-id pub-id-type="pmcid">PMC4339092</pub-id><pub-id pub-id-type="pmid">25482825</pub-id><pub-id pub-id-type="doi">10.1038/ncomms6672</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>Martin N</given-names></name><name><surname>Bankson</surname><given-names>Brett B</given-names></name><name><surname>Harel</surname><given-names>Assaf</given-names></name></person-group><article-title>Chris I Baker, and Radoslaw M Cichy. The representational dynamics of task and object processing in humans</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e32816</elocation-id><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>Sörensen</surname><given-names>Lynn KA</given-names></name><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><name><surname>Hauk</surname><given-names>Olaf</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Recurrence required to capture the dynamic computations of the human ventral visual stream</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>43</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><name><surname>McClure</surname><given-names>Patrick</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><source>Deep Neural Networks in Computational Neuroscience</source><publisher-name>Oxford University Press</publisher-name><volume>2019</volume></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Cadieu</surname><given-names>Charles F</given-names></name><name><surname>Solomon</surname><given-names>Ethan a</given-names></name><name><surname>Seibert</surname><given-names>Darren</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2014</year><volume>111</volume><fpage>8619</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>Seyed-Mahdi</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>11</issue><elocation-id>e1003915</elocation-id><pub-id pub-id-type="pmcid">PMC4222664</pub-id><pub-id pub-id-type="pmid">25375136</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MaJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>27</issue><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>Radoslaw Martin</given-names></name><name><surname>Khosla</surname><given-names>Aditya</given-names></name><name><surname>Pantazis</surname><given-names>Dimitrios</given-names></name><name><surname>Torralba</surname><given-names>Antonio</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><volume>2017</volume><pub-id pub-id-type="pmcid">PMC4901271</pub-id><pub-id pub-id-type="pmid">27282108</pub-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>The temporal evolution of conceptual object representations revealed through models of behavior, semantics and deep neural networks</article-title><source>NeuroImage</source><year>2018</year><volume>178</volume><fpage>172</fpage><lpage>182</lpage><pub-id pub-id-type="pmid">29777825</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>Kamila Maria</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Cichy</surname><given-names>Radoslaw Martin</given-names></name><name><surname>Mur</surname><given-names>Marieke</given-names></name></person-group><source>Deep convolutional neural networks, features, and categories perform similarly at explaining primate high-level visual representations</source><conf-name>Conference on Cognitive Computational Neuroscience</conf-name><year>2018</year></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>Iris IA</given-names></name><name><surname>Greene</surname><given-names>Michelle R</given-names></name><name><surname>Baldassano</surname><given-names>Christopher</given-names></name><name><surname>Fei-Fei</surname><given-names>Li</given-names></name><name><surname>Beck</surname><given-names>Diane M</given-names></name><name><surname>Baker</surname><given-names>Chris I</given-names></name></person-group><article-title>Distinct contributions of functional and deep neural network features to representational similarity of scenes in human brain and behavior</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e32962</elocation-id><pub-id pub-id-type="pmcid">PMC5860866</pub-id><pub-id pub-id-type="pmid">29513219</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32962</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonner</surname><given-names>Michael F</given-names></name><name><surname>Epstein</surname><given-names>Russell A</given-names></name></person-group><article-title>Computational mechanisms underlying cortical responses to the affordance properties of visual scenes</article-title><source>PLOS Computational Biology</source><year>2018</year><volume>14</volume><issue>4</issue><elocation-id>e1006111</elocation-id><pub-id pub-id-type="pmcid">PMC5933806</pub-id><pub-id pub-id-type="pmid">29684011</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006111</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Kubilius</surname><given-names>Jonas</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Issa</surname><given-names>Elias B</given-names></name><name><surname>Kar</surname><given-names>Kohitij</given-names></name><name><surname>Prescott-Roy</surname><given-names>Jonathan</given-names></name><name><surname>Rajalingham</surname><given-names>Rishi</given-names></name><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Brain-Score: Which Artificial Neural Network is most Brain-Like?</article-title><source>bioRxiv</source><volume>2018</volume></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeman</surname><given-names>Astrid A</given-names></name><name><surname>Ritchie</surname><given-names>J Brendan</given-names></name><name><surname>Bracci</surname><given-names>Stefania</given-names></name><name><surname>de Beeck</surname><given-names>Hans Op</given-names></name></person-group><article-title>Orthogonal Representations of Object Shape and Category in Deep Convolutional Neural Networks and Human Visual Cortex</article-title><source>ScientificReports</source><year>2020</year><volume>10</volume><issue>1</issue><elocation-id>2453</elocation-id><pub-id pub-id-type="pmcid">PMC7016009</pub-id><pub-id pub-id-type="pmid">32051467</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-59175-0</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>Katherine R</given-names></name><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><name><surname>Walther</surname><given-names>Alexander</given-names></name><name><surname>Mehrer</surname><given-names>Johannes</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Diverse deep neural networks all predict human it well, after training and fitting</article-title><source>Journal of Cognitive Neuroscience</source><year>2020</year><volume>33</volume><fpage>2044</fpage><lpage>2064</lpage><pub-id pub-id-type="pmid">34272948</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Kalfas</surname><given-names>I</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><article-title>The ventral visual pathway represents animal appearance over animacy, unlike human behavior and deep neural networks</article-title><source>The Journal of neuroscience</source><year>2019</year><volume>39</volume><fpage>6513</fpage><lpage>6525</lpage><pub-id pub-id-type="pmcid">PMC6697402</pub-id><pub-id pub-id-type="pmid">31196934</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1714-18.2019</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>Randall C</given-names></name><name><surname>Wyatte</surname><given-names>Dean</given-names></name><name><surname>Herd</surname><given-names>Seth</given-names></name><name><surname>Mingus</surname><given-names>Brian</given-names></name><name><surname>Jilk</surname><given-names>David J</given-names></name></person-group><article-title>Recurrent processing during object recognition</article-title><source>Frontiers in Psychology</source><year>2013</year><month>APR</month><volume>4</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC3612699</pub-id><pub-id pub-id-type="pmid">23554596</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00124</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liao</surname><given-names>Qianli</given-names></name><name><surname>Poggio</surname><given-names>Tomaso</given-names></name></person-group><article-title>Bridging the Gaps Between Residual Learning</article-title><source>Recurrent Neural Networks and Visual Cortex</source><year>2016</year><volume>047</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>McClure</surname><given-names>Patrick</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition</article-title><source>Frontiers in Psychology</source><year>2017</year><volume>8</volume><elocation-id>1551</elocation-id><pub-id pub-id-type="pmcid">PMC5600938</pub-id><pub-id pub-id-type="pmid">28955272</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01551</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>Jonas</given-names></name><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Nayebi</surname><given-names>Aran</given-names></name><name><surname>Bear</surname><given-names>Daniel</given-names></name><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</article-title><year>2018</year></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>Hanlin</given-names></name><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Lotter</surname><given-names>William</given-names></name><name><surname>Moerman</surname><given-names>Charlotte</given-names></name><name><surname>Paredes</surname><given-names>Ana</given-names></name><name><surname>Caro</surname><given-names>Josue Ortega</given-names></name><name><surname>Hardesty</surname><given-names>Walter</given-names></name><name><surname>Cox</surname><given-names>David</given-names></name><name><surname>Kreiman</surname><given-names>Gabriel</given-names></name></person-group><article-title>Recurrent computations for visual pattern completion</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><elocation-id>201719397</elocation-id><pub-id pub-id-type="pmcid">PMC6126774</pub-id><pub-id pub-id-type="pmid">30104363</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1719397115</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>Kohitij</given-names></name><name><surname>Kubilius</surname><given-names>Jonas</given-names></name><name><surname>Schmidt</surname><given-names>Kailyn</given-names></name><name><surname>Issa</surname><given-names>Elias B</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>6</issue><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="pmcid">PMC8785116</pub-id><pub-id pub-id-type="pmid">31036945</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajaei</surname><given-names>Karim</given-names></name><name><surname>Mohsenzadeh</surname><given-names>Yalda</given-names></name><name><surname>Ebrahimpour</surname><given-names>Reza</given-names></name><name><surname>Khaligh-Razavi</surname><given-names>Seyed-Mahdi</given-names></name></person-group><article-title>Beyond core object recognition: Recurrent processes account for object recognition under occlusion</article-title><source>PLoS Computational Biology</source><year>2019</year><fpage>30</fpage><pub-id pub-id-type="pmcid">PMC6538196</pub-id><pub-id pub-id-type="pmid">31091234</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007001</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name><name><surname>Mehrer</surname><given-names>Johannes</given-names></name><name><surname>Charest</surname><given-names>Ian</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Recurrent neural networks can explain flexible trading of speed and accuracy in biological vision</article-title><source>PLOS Computational Biology</source><year>2020</year><volume>16</volume><issue>10</issue><elocation-id>e1008215</elocation-id><pub-id pub-id-type="pmcid">PMC7556458</pub-id><pub-id pub-id-type="pmid">33006992</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008215</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>Kamila Maria</given-names></name><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Kanwisher</surname><given-names>Nancy</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>To find better neural network models of human vision, find better neural network models of primate vision</article-title><source>bioRxiv</source><volume>2019</volume></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>Kamila M</given-names></name><name><surname>Lee</surname><given-names>Michael</given-names></name><name><surname>Marques</surname><given-names>Tiago</given-names></name><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Bashivan</surname><given-names>Pouya</given-names></name></person-group><article-title>Large-scale hyperparameter search for predicting human brain responses in the Algonauts challenge</article-title><source>bioRxiv</source><volume>2019</volume></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><article-title>Inferotemporal cortex and object vision</article-title><source>Annual Review of Neuroscience</source><year>1996</year><volume>19</volume><fpage>109</fpage><lpage>139</lpage><pub-id pub-id-type="pmid">8833438</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamane</surname><given-names>Yukako</given-names></name><name><surname>Carlson</surname><given-names>Eric T</given-names></name><name><surname>Bowman</surname><given-names>Katherine C</given-names></name><name><surname>Wang</surname><given-names>Zhihong</given-names></name><name><surname>Connor</surname><given-names>Charles E</given-names></name></person-group><article-title>A neural code for three-dimensional object shape in macaque inferotemporal cortex</article-title><source>Nature Neuroscience</source><year>2008</year><volume>11</volume><issue>11</issue><fpage>1352</fpage><lpage>1360</lpage><pub-id pub-id-type="pmcid">PMC2725445</pub-id><pub-id pub-id-type="pmid">18836443</pub-id><pub-id pub-id-type="doi">10.1038/nn.2202</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>Winrich A</given-names></name><name><surname>Tsao</surname><given-names>Doris Y</given-names></name><name><surname>Livingstone</surname><given-names>Margaret S</given-names></name></person-group><article-title>A face feature space in the macaque temporal lobe</article-title><source>Nature Neuroscience</source><year>2009</year><volume>12</volume><issue>9</issue><fpage>1187</fpage><lpage>1196</lpage><pub-id pub-id-type="pmcid">PMC2819705</pub-id><pub-id pub-id-type="pmid">19668199</pub-id><pub-id pub-id-type="doi">10.1038/nn.2363</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname><given-names>Elias B</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Precedence of the eye region in neural processing of faces</article-title><source>The Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>47</issue><fpage>16666</fpage><lpage>82</lpage><pub-id pub-id-type="pmcid">PMC3542390</pub-id><pub-id pub-id-type="pmid">23175821</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2391-12.2012</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><year>1997</year><volume>17</volume><issue>11</issue><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><year>1998</year><volume>392</volume><month>April</month><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>Paul E</given-names></name><name><surname>Jiang</surname><given-names>Yuhong</given-names></name><name><surname>Shuman</surname><given-names>Miles</given-names></name><name><surname>Kanwisher</surname><given-names>Nancy</given-names></name></person-group><article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title><source>Science</source><year>2001</year><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>a</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><year>2001</year><volume>293</volume><issue>5539</issue><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Mur</surname><given-names>Marieke</given-names></name><name><surname>Ruff</surname><given-names>Douglas a</given-names></name><name><surname>Kiani</surname><given-names>Roozbeh</given-names></name><name><surname>Bodurka</surname><given-names>Jerzy</given-names></name><name><surname>Esteky</surname><given-names>Hossein</given-names></name><name><surname>Tanaka</surname><given-names>Keiji</given-names></name><name><surname>Bandettini</surname><given-names>Peter a</given-names></name></person-group><article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><issue>6</issue><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>Alexander G</given-names></name><name><surname>Nishimoto</surname><given-names>Shinji</given-names></name><name><surname>Vu</surname><given-names>An T</given-names></name><name><surname>Gallant</surname><given-names>Jack L</given-names></name></person-group><article-title>A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</article-title><source>Neuron</source><year>2012</year><volume>76</volume><issue>6</issue><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="pmcid">PMC3556488</pub-id><pub-id pub-id-type="pmid">23259955</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>Da</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Bandettini</surname><given-names>Pa</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Categorical, Yet Graded - Single-Image Activation Profiles of Human Category-Selective Cortical Regions</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>25</issue><fpage>8649</fpage><lpage>8662</lpage><pub-id pub-id-type="pmcid">PMC3752067</pub-id><pub-id pub-id-type="pmid">22723705</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2334-11.2012</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>Kamila M</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Mur</surname><given-names>Marieke</given-names></name></person-group><article-title>Visual features as stepping stones toward semantics: Explaining object similarity in IT and perception with non-negative least squares</article-title><source>Neuropsychologia</source><year>2016</year><volume>83</volume><fpage>201</fpage><lpage>226</lpage><pub-id pub-id-type="pmcid">PMC4783588</pub-id><pub-id pub-id-type="pmid">26493748</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.10.023</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group><source>Imagenet classification with deep convolutional neural networks</source><conf-name>Advances in neural information processing systems</conf-name><year>2012</year><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv</source><volume>2014</volume></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>Kamila M</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Storrs</surname><given-names>Katherine R</given-names></name><name><surname>Mur</surname><given-names>Marieke</given-names></name></person-group><article-title>Deep Convolutional Neural Networks Outperform Feature-Based But Not Categorical Models in Explaining Object Similarity Judgments</article-title><source>Frontiers in Psychology</source><year>2017</year><volume>8</volume><elocation-id>1726</elocation-id><pub-id pub-id-type="pmcid">PMC5640771</pub-id><pub-id pub-id-type="pmid">29062291</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01726</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devereux</surname><given-names>Barry J</given-names></name><name><surname>Clarke</surname><given-names>Alex</given-names></name><name><surname>Tyler</surname><given-names>Lorraine K</given-names></name></person-group><article-title>Integrated deep visual and semantic attractor neural networks predict fMRI pattern-information along the ventral object processing pathway</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><elocation-id>10636</elocation-id><pub-id pub-id-type="pmcid">PMC6045572</pub-id><pub-id pub-id-type="pmid">30006530</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-28865-1</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>Katherine R</given-names></name><name><surname>Khaligh-Razavi</surname><given-names>Seyed-Mahdi</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Noise ceiling on the crossvalidated performance of reweighted models of representational dissimilarity: Addendum to Khaligh-Razavi &amp; Kriegeskorte (2014)</article-title><source>bioRxiv</source><year>2020</year></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clarke</surname><given-names>Alex</given-names></name><name><surname>Devereux</surname><given-names>Barry J</given-names></name><name><surname>Randall</surname><given-names>Billi</given-names></name><name><surname>Tyler</surname><given-names>Lorraine K</given-names></name></person-group><article-title>Predicting the Time Course of Individual Objects with MEG</article-title><source>Cerebral Cortex</source><year>2015</year><fpage>3602</fpage><lpage>3612</lpage><pub-id pub-id-type="pmcid">PMC4269546</pub-id><pub-id pub-id-type="pmid">25209607</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhu203</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>Robert</given-names></name><name><surname>Rubisch</surname><given-names>Patricia</given-names></name><name><surname>Michaelis</surname><given-names>Claudio</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Brendel</surname><given-names>Wieland</given-names></name></person-group><article-title>Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</article-title><source>arXiv</source><year>2019</year></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>Marieke</given-names></name><name><surname>Meys</surname><given-names>Mirjam</given-names></name><name><surname>Bodurka</surname><given-names>Jerzy</given-names></name><name><surname>Goebel</surname><given-names>Rainer</given-names></name><name><surname>Bandettini</surname><given-names>Peter</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Human object-similarity judgments reflect and transcend the primate-it object representation</article-title><source>Frontiers in Psychology</source><year>2013</year><volume>4</volume><fpage>128</fpage><pub-id pub-id-type="pmcid">PMC3605517</pub-id><pub-id pub-id-type="pmid">23525516</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>Olga</given-names></name><name><surname>Deng</surname><given-names>Jia</given-names></name><name><surname>Su</surname><given-names>Hao</given-names></name><name><surname>Krause</surname><given-names>Jonathan</given-names></name><name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name><name><surname>Ma</surname><given-names>Sean</given-names></name><name><surname>Huang</surname><given-names>Zhiheng</given-names></name><name><surname>Karpathy</surname><given-names>Andrej</given-names></name><name><surname>Khosla</surname><given-names>Aditya</given-names></name><name><surname>Bernstein</surname><given-names>Michael</given-names></name><name><surname>Berg</surname><given-names>Alexander C</given-names></name><etal/></person-group><article-title>ImageNet large scale visual recognition challenge</article-title><source>International Journal of Computer Vision</source><year>2015</year><volume>115</volume><fpage>211</fpage><lpage>252</lpage></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbu</surname><given-names>Andrei</given-names></name><name><surname>Mayo</surname><given-names>David</given-names></name><name><surname>Alverio</surname><given-names>Julian</given-names></name><name><surname>Luo</surname><given-names>William</given-names></name><name><surname>Wang</surname><given-names>Christopher</given-names></name><name><surname>Gutfreund</surname><given-names>Dan</given-names></name><name><surname>Tenenbaum</surname><given-names>Josh</given-names></name><name><surname>Katz</surname><given-names>Boris</given-names></name></person-group><article-title>ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>Chengxu</given-names></name><name><surname>Andonian</surname><given-names>Alex</given-names></name><name><surname>Yamins</surname><given-names>Daniel</given-names></name></person-group><article-title>Unsupervised learning from video with deep neural embeddings</article-title><source>CoRR</source><year>2019</year><comment>abs/1905.11954</comment></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>Johannes</given-names></name><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>Jones</surname><given-names>Emer C</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name></person-group><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>8</issue><elocation-id>e2011417118</elocation-id><pub-id pub-id-type="pmcid">PMC7923360</pub-id><pub-id pub-id-type="pmid">33593900</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2011417118</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>Irina</given-names></name><name><surname>Chang</surname><given-names>Le</given-names></name><name><surname>Langston</surname><given-names>Victoria</given-names></name><name><surname>Hassabis</surname><given-names>Demis</given-names></name><name><surname>Summerfield</surname><given-names>Christopher</given-names></name><name><surname>Tsao</surname><given-names>Doris</given-names></name><name><surname>Botvinick</surname><given-names>Matthew</given-names></name></person-group><article-title>Unsupervised deep learning identifies semantic disentanglement in single inferotemporal neurons</article-title><source>arXiv</source><year>2020</year><pub-id pub-id-type="pmcid">PMC8578601</pub-id><pub-id pub-id-type="pmid">34753913</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>Chengxu</given-names></name><name><surname>Yan</surname><given-names>Siming</given-names></name><name><surname>Nayebi</surname><given-names>Aran</given-names></name><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Frank</surname><given-names>Michael C</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name></person-group><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>3</issue><elocation-id>e2014196118</elocation-id><pub-id pub-id-type="pmcid">PMC7826371</pub-id><pub-id pub-id-type="pmid">33431673</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>Talia</given-names></name><name><surname>Alvarez</surname><given-names>George A</given-names></name></person-group><article-title>Instance-level contrastive learning yields human brain-like representation without category-supervision</article-title><source>bioRxiv</source><year>2020</year></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Yuxin</given-names></name><name><surname>He</surname><given-names>Kaiming</given-names></name></person-group><article-title>Group Normalization</article-title><source>arXiv</source><year>2018</year></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>Alexandre</given-names></name></person-group><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroscience</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3872725</pub-id><pub-id pub-id-type="pmid">24431986</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ledoit</surname><given-names>Olivier</given-names></name><name><surname>Wolf</surname><given-names>Michael</given-names></name></person-group><article-title>A well-conditioned estimator for large-dimensional covariance matrices</article-title><source>Journal of Multivariate Analysis</source><year>2004</year><volume>88</volume><issue>2</issue><fpage>365</fpage><lpage>411</lpage></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauk</surname><given-names>Olaf</given-names></name><name><surname>Stenroos</surname><given-names>Matti</given-names></name><name><surname>Treder</surname><given-names>Matthias S</given-names></name></person-group><article-title>Towards an objective evaluation of EEG/MEG source estimation methods – The linear approach</article-title><source>NeuroImage</source><year>2022</year><volume>255</volume><elocation-id>119177</elocation-id><pub-id pub-id-type="pmid">35390459</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>Matthew F</given-names></name><name><surname>Coalson</surname><given-names>Timothy S</given-names></name><name><surname>Robinson</surname><given-names>Emma C</given-names></name><name><surname>Hacker</surname><given-names>Carl D</given-names></name><name><surname>Harwell</surname><given-names>John</given-names></name><name><surname>Yacoub</surname><given-names>Essa</given-names></name><name><surname>Ugurbil</surname><given-names>Kamil</given-names></name><name><surname>Andersson</surname><given-names>Jesper</given-names></name><name><surname>Beckmann</surname><given-names>Christian F</given-names></name><name><surname>Jenkinson</surname><given-names>Mark</given-names></name><name><surname>Smith</surname><given-names>Stephen M</given-names></name><etal/></person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><issue>7615</issue><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC4990127</pub-id><pub-id pub-id-type="pmid">27437579</pub-id><pub-id pub-id-type="doi">10.1038/nature18933</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischl</surname><given-names>Bruce</given-names></name><name><surname>Sereno</surname><given-names>Martin I</given-names></name><name><surname>Tootell</surname><given-names>Roger BH</given-names></name><name><surname>Dale</surname><given-names>Anders M</given-names></name></person-group><article-title>High-resolution intersubject averaging and a coordinate system for the cortical surface</article-title><source>Human Brain Mapping</source><year>1999</year><pub-id pub-id-type="pmcid">PMC6873338</pub-id><pub-id pub-id-type="pmid">10619420</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1097-0193(1999)8:4&lt;272::AID-HBM10&gt;3.0.CO;2-4</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaniuth</surname><given-names>Philipp</given-names></name><name><surname>Hebart</surname><given-names>Martin N</given-names></name></person-group><article-title>Feature-reweighted RSA: A method for improving the fit between computational models, brains, and behavior</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="pmid">35580810</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance Statement</title></caption><p>When we view objects such as faces and cars in our visual environment, their neural representations dynamically unfold over time at a millisecond scale. These dynamics reflect the cortical computations that support fast and robust object recognition. Deep neural networks (DNNs) have emerged as a promising framework for modeling these computations but cannot yet fully account for the neural dynamics. Using magnetoencephalography data acquired in human observers during object viewing, we show that readily nameable aspects of objects, such as “eye”, “wheel”, and “face”, can account for variance in the neural dynamics over and above DNNs. These findings suggest that DNNs and humans may in part rely on different object features for visual recognition and provide guidelines for model improvement.</p></boxed-text><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Schematic overview of approach: stimulus set, models, data, and model fitting.</title><p><bold>a)</bold> Stimulus set. Stimuli are 92 colored images of real-world objects spanning a range of categories, including humans, non-human animals, natural objects, and manmade objects (human face images are not shown and are replaced by gray squares according to bioRxiv’s policy on posting images of individuals). <bold>b)</bold> Visuo-semantic models and deep neural networks (DNNs). Visuo-semantic models consist of human-generated labels of object features and categories for the 92 images. Example labels are shown for the dog face encircled in panel (a). DNNs are feedforward and locally recurrent CORnet architectures trained with category supervision on ILSVRC. These architectures are inspired by the processing stages of the primate ventral visual stream: from V1 to inferior temporal cortex (IT). <bold>c)</bold> Object representations for each model. We characterized object representations by computing representational dissimilarity matrices (RDMs). We computed one RDM per model dimension, i.e. one for each visuo-semantic label or DNN layer. For each visuo-semantic model dimension, RDMs were computed by extracting the value for each image on that dimension and computing pairwise dissimilarities (squared difference) between the values. For each CORnet-Z and CORnet-R layer, RDMs were computed by extracting an activity pattern across model units for each image and computing pairwise dissimilarities (1 minus Spearman’s r) between the activity patterns. <bold>d)</bold> Human source-reconstructed MEG data for an example participant. MEG data were acquired in 15 healthy adult human participants while they were viewing the 92 images (stimulus duration: 500 ms). We analyzed source-reconstructed data from three ROIs: V1-3, V4t/LO, and IT/PHC. We computed an RDM for each participant, region, and time point. RDMs were computed by extracting an activity pattern for each image and computing pairwise dissimilarities (1 minus Pearson’s r) between the activity patterns. <bold>e)</bold> Schematic overview of model fitting procedure. We tested two model classes: a visuo-semantic model consisting of all category and feature RDMs and a DNN model consisting of all CORnet-Z and CORnet-R layer RDMs. The respective model RDMs serve as predictors. We fitted the two models to the MEG RDMs for each participant, region, and time point, using cross-validated non-negative least squares regression.</p></caption><graphic xlink:href="EMS152377-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>DNNs better explain lower-level visual representations, visuo-semantic models better explain higher-level visual representations.</title><p><bold>a)</bold> Variance explained by the visuo-semantic (blue) and DNN (green) models in the source-reconstructed MEG data. To estimate the variance explained by each model class, we took the model components (visuo-semantic features or DNN layers) and ran a non-negative least squares regression. Variance explained was computed as the variance explained by the model predictions in data for images left out during fitting. Significant variance explained is indicated by a horizontal line above the graph (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected). The shaded area around the lines shows the standard error of the mean across participants. The x axis shows time relative to stimulus onset. The gray horizontal bar on the x axis indicates the stimulus duration. <bold>b)</bold> Unique variance explained by the visuo-semantic and DNN models in the source-reconstructed MEG data. To estimate the unique variance explained by each model class, we took the model predictions (one RDM movie per model class, also used in panel a) and ran a second-level non-negative least squares regression. Unique variance explained was computed by subtracting the variance explained by the reduced GLM (excluding the model class of interest) from the total variance explained by the full GLM (including both model classes). Conventions are the same as in panel a.</p></caption><graphic xlink:href="EMS152377-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Object parts and basic categories contribute to the unique variance explained by visuo-semantic models in higher-level visual representations.</title><p><bold>a)</bold> Variance explained by the object features (parts, shape, color, texture; shades of blue), categories (subordinate, basic, superordinate; shades of pink), and DNN model (green) in the source-reconstructed MEG data. Conventions are the same as in <xref ref-type="fig" rid="F2">Figure 2a</xref>. <bold>b)</bold> Unique variance explained by the features, categories, and DNN model in the source-reconstructed MEG data. Conventions are the same as in <xref ref-type="fig" rid="F2">Figure 2b</xref>.</p></caption><graphic xlink:href="EMS152377-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Summary of findings: deep neural networks and visuo-semantic models explain complementary components of human ventral-stream representational dynamics.</title><p>Between 66 and 128 ms after stimulus onset, DNNs outperform visuo-semantic models in lower-level areas V1-3 (grey line, positive deflection). This early time window is thought to be dominated by feedforward and local recurrent processing. In contrast, starting 146 ms after stimulus onset, visuo-semantic models outperform DNNs in higher-level visual areas IT/PHC (red line, negative deflection). This result shows that DNNs fail to account for a significant component of variance in higher-level cortical dynamics, which is instead accounted for by visuo-semantic features, in particular object parts and basic categories. The peak of visuo-semantic model performance in higher-level areas (red vertical line) precedes the peak in intermediate areas (blue vertical line). This sequence of events aligns with the timing of feedback information flow from higher-level to intermediate areas (light grey rectangle and arrow) as reported in (<xref ref-type="bibr" rid="R10">10</xref>). The shaded area around the lines shows the standard error of the mean across participants.</p></caption><graphic xlink:href="EMS152377-f004"/></fig></floats-group></article>