<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS175595</article-id><article-id pub-id-type="doi">10.1101/2021.10.25.465687</article-id><article-id pub-id-type="archive">PPR410503</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">4</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Finding structure during incremental speech comprehension</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Lyu</surname><given-names>Bingjiang</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Marslen-Wilson</surname><given-names>William D.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Fang</surname><given-names>Yuxing</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tyler</surname><given-names>Lorraine K.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Changping Laboratory, Beijing, 102206, China</aff><aff id="A2"><label>2</label>Centre for Speech, Language and the Brain, Department of Psychology, University of Cambridge, Cambridge, CB2 3EB, United Kingdom</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>bingjiang.lyu@gmail.com</email> (BL), <email>lkt10@cam.ac.uk</email> (LKT).</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>12</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>11</day><month>05</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">A core aspect of human speech comprehension is the ability to incrementally integrate consecutive words into a structured and coherent interpretation, aligning with the speaker's intended meaning. This rapid process is subject to multi-dimensional probabilistic constraints, including both linguistic knowledge and non-linguistic information within specific contexts, and it is their interpretative coherence that drives successful comprehension. To study the neural substrates of this process, we extract word-by-word measures of sentential structure from BERT, a deep language model, which effectively approximates the coherent outcomes of the dynamic interplay among various types of constraints. Using representational similarity analysis, we tested BERT parse depths and relevant corpus-based measures against the spatiotemporally resolved brain activity recorded by electro/magnetoencephalography when participants were listening to the same sentences. Our results provide a detailed picture of the neurobiological processes involved in the incremental construction of structured interpretations. These findings show when and where coherent interpretations emerge through the evaluation and integration of multifaceted constraints in the brain, which engages bilateral brain regions extending beyond the classical fronto-temporal language system. Furthermore, this study provides empirical evidence supporting the use artificial neural networks as computational models for revealing the neural dynamics underpinning complex cognitive processes in the brain.</p></abstract><kwd-group><kwd>incremental speech comprehension</kwd><kwd>artificial neural networks</kwd><kwd>deep language models</kwd><kwd>representational similarity analysis</kwd><kwd>computational cognitive neuroscience</kwd><kwd>EEG/MEG</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Human speech comprehension involves a complex set of processes that transform an auditory input into the speaker’s intended meaning, wherein each word is sequentially recognized and integrated with the preceding words to obtain a coherent interpretation (<xref ref-type="bibr" rid="R56">Marslen-Wilson and Tyler 1980</xref>; <xref ref-type="bibr" rid="R51">Lyu et al. 2019</xref>; <xref ref-type="bibr" rid="R17">Choi et al. 2021</xref>). Crucially, rather than simple linear concatenation, individual words are combined according to the nonlinear and often discontinuous structure embedded in an utterance as it is delivered over time (<xref ref-type="bibr" rid="R26">Everaert et al. 2015</xref>). For example, in the sentence <italic>“The boy who chased the cat was…”</italic>, it is the structurally close word <italic>“boy”</italic>, rather than the linearly close word <italic>“cat”</italic>, that is combined with <italic>“was”</italic>. However, the neural dynamics underpinning the incremental construction of a structured interpretation from a spoken sentence is still unclear.</p><p id="P3">Previous neuroimaging studies on the structure of language primarily focused on syntax (<xref ref-type="bibr" rid="R59">Matchin and Hickok 2020</xref>), contrasting grammatical sentences against word lists or sentences with syntactic violations (<xref ref-type="bibr" rid="R66">Nelson et al. 2017</xref>; <xref ref-type="bibr" rid="R48">Law and Pylkkanen 2021</xref>), manipulating the syntactic complexity in sentences (<xref ref-type="bibr" rid="R68">Pallier et al. 2011</xref>), or studying artificial grammatical rules elicited by structured, unintelligible strings (<xref ref-type="bibr" rid="R30">Friederici et al. 2006</xref>). Nevertheless, finding the structure in an unfolding sentence also depends on the constraints jointly placed by other linguistic properties and non-linguistic information such as the broad world knowledge (<xref ref-type="bibr" rid="R9">Bever 1970</xref>; <xref ref-type="bibr" rid="R80">Tyler and Marslen-Wilson 1977</xref>).</p><p id="P4">Unlike the <italic>two-stage model</italic> (<xref ref-type="bibr" rid="R28">Frazier and Rayner 1982</xref>; <xref ref-type="bibr" rid="R27">Frazier 1987</xref>) which posits an initial parsing stage relying solely on syntax, the <italic>constraint-based</italic> approach to sentence processing (<xref ref-type="bibr" rid="R52">MacDonald et al. 1994</xref>; <xref ref-type="bibr" rid="R79">Trueswell and Tanenhaus 1994</xref>) proposes that speech comprehension is concurrently governed by multiple types of probabilistic constraints (e.g., syntax, semantics, world knowledge), generated by individual words as they are sequentially heard. There is no delay in the utilization of these multifaceted constraints once they become available, neither is a fixed priority assigned to one type of constraint over another; rather, it is the <italic>interpretative coherence</italic> of all available constraints that forms the basis for successful language comprehension (<xref ref-type="bibr" rid="R1">Altmann 1998</xref>). Although lexical constraints of individual words can be estimated from large corpora data, it has been challenging to model the dynamic interplay between various types of linguistic and nonlinguistic constraints in a specific context, especially at the sentence level and beyond.</p><p id="P5">Contemporary deep language models (DLMs) have made great strides in a wide array of natural language processing tasks, including text generation, parsing and translation (<xref ref-type="bibr" rid="R81">Vaswani et al. 2017</xref>; <xref ref-type="bibr" rid="R19">Devlin et al. 2019</xref>; <xref ref-type="bibr" rid="R13">Brown et al. 2020</xref>; <xref ref-type="bibr" rid="R67">Ouyang et al. 2022</xref>). While current DLMs are still imperfect in terms of human-level language understanding related to reasoning and complex physical or social situations (<xref ref-type="bibr" rid="R12">Bisk et al. 2020</xref>), they are arguably valuable models of general linguistic capacities, due to their ability to identify and leverage relevant statistical regularities of linguistic and non-linguistic world knowledge present in massive training data (<xref ref-type="bibr" rid="R50">Linzen and Baroni 2021</xref>; <xref ref-type="bibr" rid="R69">Pavlick 2022</xref>). Human language comprehension requires a contextualized integration of multifaceted constraints (<xref ref-type="bibr" rid="R57">Marslen-Wilson 1975</xref>; <xref ref-type="bibr" rid="R80">Tyler and Marslen-Wilson 1977</xref>; <xref ref-type="bibr" rid="R47">Kuperberg 2007</xref>). In this regard, DLMs excel in flexible combination of different types of features (e.g., syntactic structure and semantic meaning) embedded in their rich internal representations (<xref ref-type="bibr" rid="R54">Manning et al. 2020</xref>; <xref ref-type="bibr" rid="R8">Bengio et al. 2021</xref>; <xref ref-type="bibr" rid="R50">Linzen and Baroni 2021</xref>; <xref ref-type="bibr" rid="R69">Pavlick 2022</xref>). Their deep contextualized representations capture the distributed regularities that jointly determine the coherent interpretation of a given sentence, providing context-dependent composition and quantitative measures of the underlying sentential structure. These properties relate back to Elman’s recurrent neural network (<xref ref-type="bibr" rid="R24">Elman 1990</xref>, <xref ref-type="bibr" rid="R25">1993</xref>) which automatically picks up and encodes lexical syntactic/semantic information in the hidden states.</p><p id="P6">Recent studies have revealed an overall congruence between language representations in DLMs and those observed in the human brain while processing the same spoken or written input (<xref ref-type="bibr" rid="R72">Schrimpf et al. 2021</xref>; <xref ref-type="bibr" rid="R14">Caucheteux et al. 2022</xref>; <xref ref-type="bibr" rid="R16">Caucheteux and King 2022</xref>; <xref ref-type="bibr" rid="R32">Goldstein et al. 2022</xref>; <xref ref-type="bibr" rid="R35">Heilbron et al. 2022</xref>; <xref ref-type="bibr" rid="R77">Toneva et al. 2022</xref>; <xref ref-type="bibr" rid="R15">Caucheteux et al. 2023</xref>), suggesting the potential value of DLMs as a computational tool to investigate the neural basis of language comprehension. To move beyond comparing the similarities between entire model hidden states and brain activity, probing techniques that can extract specific contents from DLMs (<xref ref-type="bibr" rid="R37">Hewitt and Liang 2019</xref>; <xref ref-type="bibr" rid="R76">Tenney et al. 2019</xref>) make it possible to study the neural dynamics relevant to processing such specific information. The important advance here is that we can leverage the deep learning strengths of DLMs to create rigorously quantified models of the broader and multifaceted constraint environment in which a structured interpretation is constructed. Such models can be compared, dynamically, with more restricted and interpretable factors that capture the specific linguistic combinatorial constraints necessary for successful language comprehension.</p><p id="P7">Here, we take this approach further by designing sentences with contrasting linguistic structures and using a structural probe technique (<xref ref-type="bibr" rid="R38">Hewitt and Manning 2019</xref>) to extract word-by-word contextualized representations of sentential structures from a widely-used DLM, namely, BERT (<xref ref-type="bibr" rid="R19">Devlin <italic>et al</italic>. 2019</xref>). This provides the neurocomputational specificity required to elucidate the neural dynamics underlying the incremental construction of a structured interpretation from an unfolding spoken sentence. After a detailed evaluation of BERT structural measures according to the hypothesized <italic>constraint-based</italic> approach and human behavioural results, we used spatiotemporal searchlight representational similarity analysis (ssRSA) (<xref ref-type="bibr" rid="R46">Kriegeskorte et al. 2008</xref>) to test these quantitative structural measures and relevant lexical properties against source-localized EMEG data recorded while participants were listening to the same sentences. Our findings reveal how the structured interpretation of a spoken sentence is incrementally built under multifaceted probabilistic constraints in the brain.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">We constructed 60 sets of sentences with varying sentential structures (see <xref ref-type="sec" rid="S11">Methods</xref>) and presented them to human listeners. We also input them word-by-word to BERT to extract incremental structural representations. These natural spoken sentences were constructed to balance off specifically linguistic constraints on interpretation against varying non-linguistic constraints as the sentence is incrementally interpreted, providing a realistic simulation of real-life language use. In each stimulus set, there are two target sentences differing only in the transitivity of the first verb (Verb1) encountered, i.e., how likely it is that Verb1 takes a direct object [see (1) and (2) below and <xref ref-type="fig" rid="F1">Figure 1</xref>]: <list list-type="simple" id="L1"><list-item><label>(1)</label><p id="P9"><italic>The dog found in the park was covered in mud</italic>.</p></list-item><list-item><label>(2)</label><p id="P10"><italic>The dog walked in the park was covered in mud</italic>.</p></list-item></list></p><p id="P11">In the first sentence, Verb1 (i.e., <italic>“found”</italic>) has high transitivity (HiTrans) and strongly prefers a direct object (e.g., ball), while in the second sentence, Verb1 (i.e., <italic>“walked”</italic>) has relatively low transitivity (LoTrans) and is often used without a following direct object. Critically, (a) the structural interpretation of these sentences is ambiguous at the point Verb1 is encountered and (b) the preferred human resolution of this ambiguity depends on the real-time integration of linguistic and non-linguistic constraints as more of the sentence is heard. In the example above, the sequence <italic>“The dog found…”</italic> could initially have either an Active interpretation – where the dog has found something, or a Passive interpretation – where the dog is found by someone (<xref ref-type="fig" rid="F1">Figure 1</xref>). Because <italic>“find”</italic> is primarily a transitive verb, the human listener is likely to be biased towards an initial Active interpretation. Similarly, the sequence <italic>“The dog walked…”</italic>, where <italic>walk</italic> is primarily used as an intransitive verb (without a direct object), could also bias the listener to an Active interpretation, where the dog is doing the walking, rather than the less frequent Passive interpretation where someone is taking the dog for a walk (i.e., walking the dog).</p><p id="P12">This initial structural interpretation up to Verb1 does not, however, just depend on linguistic knowledge such as Verb1 transitivity. It also depends on non-linguistic information, i.e., how likely the subject is (or is not) to adopt the Active (agent) role to perform the specified action (<xref ref-type="bibr" rid="R22">Dowty 1991</xref>; <xref ref-type="bibr" rid="R58">Marslen-Wilson et al. 1993</xref>), that is, “thematic role” properties of the subject noun. Although it could be reflected by statistical regularities in language, thematic role preference hinges more on world knowledge, plausibility, or real-world statistics. So, regardless of Verb1 transitivity, the Active interpretation should be more strongly favored in <italic>“The <bold>king</bold> found/walked…”</italic> given the higher agenthood of the <italic>“king”</italic> and thus the greater implausibility of a Passive interpretation involving a <italic>“king”</italic> relative to a <italic>“dog”</italic>. Hence, the word-by-word interpretation of the sentential structure – and of the real-world event structure evoked by this interpretation – is determined by the constraints jointly placed by the subject noun and Verb1, which is manifested by the interpretative coherence between non-linguistic world knowledge (i.e., thematic role preference) and linguistic knowledge (i.e., verb transitivity).</p><p id="P13">As the sentence evolves, and the prepositional phrase <italic>“in the park”</italic> that follows Verb1 is incrementally processed, there is further modulation of the preferred interpretation, again reflecting both Verb1 transitivity and the plausibility of the event being constructed. Specifically, the Passive interpretation will become more preferred in a HiTrans sentence, given the absence of an expected direct object for the highly transitive Verb1, so Verb1 tends to be interpreted as a passive verb [i.e., the head of a reduced relative clause in <italic>“The dog <bold>(that was) found in the park…</bold>”</italic>]. Conversely, in a LoTrans sentence, the Active interpretation of Verb1 is strengthened by the incoming prepositional phrase, which is in accord with the verb’s intransitive use and the event conjured up by the sequence of words heard so far (e.g., <italic>“The dog walked in the park…”</italic>). Hence, these two sentence types are likely to differ in the structural interpretation preferred by the end of the prepositional phrase. However, with the appearance of the actual main verb (e.g., <italic>“was covered”</italic> in the example sentences), the Active interpretation of Verb1 as the main verb will be completely rejected, which resolves the potential ambiguity and confirms the Passive interpretation in both HiTrans and LoTrans sentences.</p><p id="P14">In brief, understanding these complex sentences require listeners to integrate discontinuous words to solve a long-distance dependency between the subject noun and the actual main verb separated by an intervening clause. This engages the neurobiological processes of integration across different lexical constraints and multiple levels of the sentence processing system. For example, the incremental building, maintenance and update of sentential structure over time might primarily involve activity in the fronto-temporal regions (<xref ref-type="bibr" rid="R29">Friederici 2012</xref>), while estimating the plausibility of the event interpreted from the sentence with prior knowledge of the world may elicit neural responses in the default mode network (<xref ref-type="bibr" rid="R86">Yeshurun et al. 2021</xref>).</p><sec id="S3"><title>Human incremental structural interpretations</title><p id="P15">As the first step, and to quantify how the stimulus sentences exemplified a constraint-based account of incremental structural interpretation, we conducted two pre-tests where participants listened to sentence fragments, starting from sentence onset and continuing either until the end of Verb1 or to the end of the prepositional phrase (<xref ref-type="fig" rid="F2">Figure 2A</xref>), and then produced a continuation to complete the sentence (see <xref ref-type="sec" rid="S11">Methods</xref>). Based on the continuations provided by the listeners at these two gating points, we can infer their online structural interpretations.</p><p id="P16">In the continuations after Verb1, a direct object was more likely to be found in HiTrans sentences, indicating a transitive use of Verb1, while an opposite pattern was found for a PP continuation, indicating an intransitive use of Verb1 (<xref ref-type="fig" rid="F2">Figure 2B</xref>). As expected, the probability of a main verb (MV) in the continuations after the prepositional phrase was lower in LoTrans sentences (<xref ref-type="fig" rid="F2">Figure 2C</xref>), suggesting that listeners preferred the Active interpretation and tended to interpret Verb1 as the main verb by the end of the prepositional phrase in LoTrans sentences, and vice versa in HiTrans sentences. Crucially, neither of the two pre-tests resulted in a complete separation between HiTrans and LoTrans sentences; instead, they were characterized by two different but overlapping probabilistic distributions. This finding suggests that Passive and Active interpretations varied in plausibility in each sentence type before the actual main verb was presented, reflecting the probabilistic constraints jointly placed by the combination of the specific subject noun, Verb1, and the prepositional phrase in each sentence.</p><p id="P17">To relate these human interpretative preferences to the broader landscape of distributional language data, we developed corpus-based measures of the thematic role preference of the subject noun (i.e., how likely it is interpreted as an agent that conducts an action) and the transitivity of Verb1 in each sentence, from which we derived a Passive index and an Active index (see <xref ref-type="sec" rid="S11">Methods</xref>). These indices separately capture the interpretative coherence between these two lexical properties towards Passive and Active interpretations. Both high subject noun agenthood and low Verb1 transitivity coherently preferred an Active interpretation as the prepositional phrase was heard (i.e., a high Active index), and vice versa for the Passive interpretation (i.e., a high Passive index). In accord with the <italic>constraint-based</italic> hypothesis, we found that human interpretative preference for the two types of sentences was significantly correlated with the lexical constraints placed by the subject noun and Verb1 (<xref ref-type="fig" rid="F2">Figure 2D</xref>).</p></sec><sec id="S4"><title>Incremental structural representations extracted from BERT</title><p id="P18">Next, we extracted structural representations at various positions in the same sentences from BERT and evaluated them according to the <italic>constraint-based</italic> hypothesis and human behavioural results. This evaluation is needed to motivate the use of BERT structural measures to reveal how the structured interpretation of a spoken sentence is incrementally built in the brain.</p><p id="P19">Typically, the structure of a sentence can be represented by a dependency parse tree (<xref ref-type="bibr" rid="R18">de Marneffe et al. 2006</xref>) where words are situated at different depths given their structural dependency (<xref ref-type="fig" rid="F3">Figure 3A</xref>). Each edge links two structurally proximate words as being the head and the dependent separately (e.g., a verb and its direct object). However, such a parse tree is context-free, that is, it only captures the syntactic relation between each pair of words and abstracts away from the specific lexical (and higher order) contents of the sentence that constrain its structural interpretation. This context-free parse depth is always the same for words at the same position in sentences with the same structured interpretation (e.g., <italic>“found”</italic> and <italic>“walked”</italic> in either of the two parse trees in <xref ref-type="fig" rid="F3">Figure 3A</xref>).</p><p id="P20">To obtain structural measures that also encode the specific lexical contents in a sentence, we adopted a structural probing technique (<xref ref-type="bibr" rid="R38">Hewitt and Manning 2019</xref>) to reconstruct a sentence’s structure by estimating each word’s parse depth based on their contextualized representations generated by BERT (see <xref ref-type="sec" rid="S11">Methods</xref>). Note that BERT is a multi-layer DLM (24 layers in the version used in this study) which may distribute different aspects of its computational solutions over multiple layers. Accordingly, we trained a structural probing model for each layer, and selected the one with the most accurate structural representations while also including its neighboring layers to cover relevant upstream and downstream information. Following this strategy, we used the BERT structural measures obtained from layers 12-16 with the best performance achieved in layer 14 (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 1</xref> and <xref ref-type="sec" rid="S11">Methods</xref>).</p><p id="P21">We input each sentence word-by-word to the trained BERT structural probing models, focusing on the incremental structural representation being built as it progressed from Verb1 to the main verb (see examples in <xref ref-type="fig" rid="F3">Figures 3B and 3C</xref>). Note that we defined the first word after the prepositional phrase as the main verb since its appearance is sufficient to resolve the intended structure where Verb1 is a passive verb. We found that, for each type of sentences, the BERT parse depth of words at the same position formed a distribution ranging around the corresponding context-free parse depths in either the Passive or the Active interpretation (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 2</xref>), suggesting a word-specific rather than position-specific structural representation. In addition, we quantified the contributions of words at different positions to the variances encoded in BERT parse depth vectors. Our analysis revealed that content words contributed significantly more than function words (i.e., the determiners) (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 3</xref> for details).</p><p id="P22">Then we visualized BERT’s word-by-word structural measures, focusing on the dependency between the subject noun and Verb1 that is core to the current interpretation of the sentence – whether the subject noun is the agent or the patient of Verb1. To this end, we built a 3-dimensional vector including BERT parse depths of the first three words up to Verb1 for each sentence (e.g., <italic>“The dog found…”</italic>). This 3D vector was updated incrementally with each additional word in the input, thereby capturing the dynamic interpretation of the structural dependency between the subject noun and Verb1, influenced by the context provided by subsequent words in a specific sentence. Like the probabilistic interpretation found within each type of sentences in human listeners, trajectories of individual HiTrans and LoTrans sentences are considerably distributed and intertwined (see the upper panel of <xref ref-type="fig" rid="F3">Figure 3D</xref>), implying that BERT structural interpretations are sensitive to the idiosyncratic contents in each sentence.</p><p id="P23">To make sense of these trajectories, we also vectorized the context-free parse depth of the first three words indicating Passive and Active interpretations (<xref ref-type="fig" rid="F3">Figure 3A</xref>) separately and located them in the 3D vector space as landmarks (hollow triangle and circle in <xref ref-type="fig" rid="F3">Figure 3D</xref>), so that the plausibility of either interpretation can be estimated by a sentence’s distance from its landmark. As shown by the trajectories of the median BERT parse depth of the two sentence types (see the lower panel of <xref ref-type="fig" rid="F3">Figure 3D</xref>), in general, HiTrans sentences continuously moved towards the Passive interpretation landmark after Verb1, with a significant change of distances detected at the main verb (see the orange bars in <xref ref-type="fig" rid="F3">Figure 3E</xref>). LoTrans sentences started by approaching the Active interpretation landmark but were reorientated to the Passive counterpart with the appearance of the actual main verb, with significant changes of distances detected at both Verb1 and main verb (see the purple bars in <xref ref-type="fig" rid="F3">Figure 3E</xref>). These results resemble the pattern of human interpretative preference observed in the continuation pre-tests, where the Passive and Active interpretations were separately preferred in HiTrans and LoTrans sentences by the end of the prepositional phrase in a probabilistic manner (<xref ref-type="fig" rid="F2">Figure 2C</xref>), before the Passive interpretation was established with the appearance of the actual main verb.</p></sec><sec id="S5"><title>BERT structural measures are correlated with constraints driving human interpretation</title><p id="P24">To further assess whether BERT’s preferences for structural interpretation align with the constraints considered by human listeners during speech comprehension, we correlated BERT structural measures with relevant corpus-based measures and human behavioural data (see <xref ref-type="sec" rid="S11">Methods</xref>).</p><p id="P25">We first focused on BERT’s interpretative mismatch quantified as the distance between an unfolding sentence and each of the two landmarks in the model space, which was dynamically updated as the sentence unfolded (<xref ref-type="fig" rid="F3">Figure 3D</xref>). Consistently, from the incoming prepositional phrase to the main verb, sentences that are closer to the Passive landmark in the model space have higher Verb1 transitivity, a higher Passive index but a lower Active index, while sentences closer to the Active interpretation landmark exhibited higher Active index and lower Passive index (<xref ref-type="fig" rid="F4">Figures 4A and 4B</xref>). Moreover, at the beginning of the prepositional phrase, the change of distance towards either interpretation landmark between two consecutive words is also correlated with these constraints (<xref ref-type="fig" rid="F4">Figures 4C and 4D</xref>), suggesting an immediate update in the structural interpretation in combination with the accumulated constraints from the preceding subject noun and Verb1.</p><p id="P26">Moreover, we found that both the incremental BERT parse depth vectors as a whole (which are captured by their principal components) and the BERT parse depth of Verb1 (which is the most indicative marker of the interpretation preferred) are correlated with the constraints placed by the subject noun and Verb1 (<xref ref-type="fig" rid="F4">Figures 4E to 4H</xref>). Moreover, the significant effects consistently found as the sentence unfolds suggest that properties of preceding words are used to constrain the interpretation of the upcoming input, which is key to resolving discontinuous structural dependencies. In addition, we found that BERT structural interpretations were also correlated with the main verb probability in the continuation pre-test which directly reflects human interpretation preference (black bars in <xref ref-type="fig" rid="F4">Figure 4</xref>).</p><p id="P27">Overall, these results illustrated, at which position in a sentence, relevant lexical constraints started being encoded by BERT, which also validated the contextualized BERT structural measures in terms of the <italic>constraint-based</italic> hypothesis and human behavioural results, and motivated the use of them to probe the neural processes involved during the incremental structural interpretation of spoken sentences.</p></sec><sec id="S6"><title>Neural dynamics of incremental structural interpretation</title><p id="P28">To study how the structured interpretation of a spoken sentence is built word-by-word in the brain, we used ssRSA to test the incremental BERT structural measures in source-localized EMEG collected when the same sentences were delivered to human listeners (see <xref ref-type="fig" rid="F5">Figure 5</xref> for the pipeline of ssRSA). This combination of methods gains improved neurocomputational specificity by probing the spatiotemporally resolved neural activity with detailed structural representations rather than the entire hidden states of BERT. We compared the representational geometry of BERT structural measures with that of neural responses inside a spatiotemporal searchlight moving across the cortical surface, significant model fits showed when and where the incremental structural interpretations or relevant lexical constraints emerge and update in the brain. Given the probabilistic interpretations in BERT and human listeners reported above, we combined HiTrans and LoTrans sentences as one group to increase the range of pair-wise dissimilarity to be modelled in ssRSA.</p><p id="P29">We began with the BERT parse depth vector containing the parse depth of each word in an incremental input, providing a dynamic structural representation updated as the sentence unfolded. Then, we tested the interpretative mismatch between the incremental BERT parse depth vector and the corresponding context-free parse depth vector for the Passive or the Active interpretation. The degree of this mismatch is proportional to the evidence for or against the two interpretations, i.e., the smaller the distance, the more positively loaded this interpretation. Besides these two measures based on the entire incremental input, we also focused on Verb1 since the potential structural ambiguity lies in whether Verb1 is interpreted as a passive verb or the main verb. Given the context-free parse depth of Verb1 that is 2 in the passive interpretation and 0 in the Active interpretation (<xref ref-type="fig" rid="F3">Figure 3A</xref>), with each incoming later word, an increased BERT Verb1 parse depth towards 2 or a decreased value towards 0 reflects separately the preference biased to a Passive or an Active interpretation (<xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 7</xref>). All quantitative measures tested in ssRSA are summarized in <xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 1</xref>).</p><p id="P30">For the listener’s neural activity, we focused on three critical epochs in each sentence: (a) Verb1 – when its structural dependency with the preceding subject noun was initially established despite potential ambiguity, (b) the preposition – when the initial structural interpretation started being updated, to be either strengthened or weakened by the incoming preposition phrase, and (c) main verb – when the intended Passive interpretation was finally confirmed. We aligned the continuous EMEG data to the onset of Verb1, the preposition, and the main verb respectively and obtained three 600-ms epochs.</p><p id="P31">We found that the incremental BERT parse depth vectors exhibited significant fits to brain activity consistently in all three epochs, as the corresponding word was being heard at that time (<xref ref-type="fig" rid="F6">Figures 6A to 6C</xref>). In Verb1 epoch, effects in bilateral frontal and anterior-to-middle temporal regions started immediately from Verb1 onset and continued until the uniqueness point – the point at which the word has been uniquely identified. These early effects could be due to the different subject nouns included in the BERT parse depth vectors. While the BERT parse depth of Verb1 <italic>per se</italic> showed similar effects but with greater duration which peaked exactly at Verb1 uniqueness point (<xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 10</xref>). As the sentence unfolded, effects of BERT parse depth vectors were found in the left fronto-temporal regions in the two later epochs, starting after the recognition of the preposition or the main verb separately.</p><p id="P32">Turning to the interpretative mismatch for the two possible interpretations, we only observed significant effects of the mismatch for Active interpretation in Verb1 epoch (<xref ref-type="fig" rid="F6">Figure 6D</xref>). However, it was the mismatch for Passive interpretation that fitted brain activity in the preposition and main verb epochs (<xref ref-type="fig" rid="F6">Figures 6E and 6F</xref>, marginal significance in main verb epoch with cluster-wise <italic>P</italic> = 0.06). These results suggest that listeners, in general, tended to have an initial preference for an Active interpretation (even before the recognition of Verb1) but might start favoring a Passive interpretation when the prepositional phrase began to be heard. This finding is consistent with the tendency to process the first noun encountered at the beginning of a sentence as the agent (<xref ref-type="bibr" rid="R9">Bever 1970</xref>; <xref ref-type="bibr" rid="R40">Jackendoff and Jackendoff 2002</xref>; <xref ref-type="bibr" rid="R53">Mahowald et al. 2023</xref>). Note that our approach does not constitute a direct test for the hypothesis of parallel parsing, as we did not uncover evidence supporting the maintenance of parallel representations of different syntactic structures in the brain; rather, we only found one preferred structure in each epoch.</p><p id="P33">Effects of the BERT parse depth vectors and those of the interpretative mismatch for the preferred structural interpretation have substantial overlaps in terms of their spatiotemporal patterns in the brain, characterized primarily by a transition from bilateral to left-lateralized fronto-temporal regions as the sentence unfolds. Across the three epochs, the most sustained effects were observed in the left inferior frontal gyrus (IFG) and the anterior temporal lobe (ATL). Notably, with the identification of the actual main verb, effects of the eventually resolved structure also involved regions in the left prefrontal and inferior parietal regions (<xref ref-type="fig" rid="F6">Figure 6C</xref>) which belong to the multiple-demand network (<xref ref-type="bibr" rid="R23">Duncan 2010</xref>). The involvement of the prefrontal regions could be indicative of the varying working memory demands (e.g., the different number of open nodes in the sentence structures for Active and Passive interpretations before the actual main verb is recognized) for building the structure of the unfolding sentence (<xref ref-type="bibr" rid="R68">Pallier <italic>et al</italic>. 2011</xref>; <xref ref-type="bibr" rid="R66">Nelson <italic>et al</italic>. 2017</xref>).</p></sec><sec id="S7"><title>Structural ambiguity resolution probed using BERT Verb1 parse depth</title><p id="P34">As mentioned above, the potential ambiguity between a Passive and an Active interpretation centers around whether Verb1 is considered as a passive verb or the main verb, which is resolved upon the appearance of the actual main verb. We probed how this is implemented in the brain using the dynamic BERT parse depth of Verb1. Specifically, the cognitive demands required by this resolution process can be characterized by the change between the updated BERT parse depth of Verb1 when the actual main verb is presented and its initial value when Verb1 is first encountered (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 7</xref> for the dynamic change of BERT Verb1 parse depth).</p><p id="P35">We first tested the change of Verb1 parse depth in the main verb epoch. Significant fits to brain activity emerged in the left posterior temporal and inferior parietal regions upon the main verb uniqueness point, and then extended to more anterior temporal regions (<xref ref-type="fig" rid="F7">Figure 7A</xref>). After the main verb offset, the declining effects of the Verb1 parse depth change in the left anterior temporal region seamlessly overlapped with the arising effects of the updated Verb1 parse depth (<xref ref-type="fig" rid="F7">Figures 7B and 7C</xref>). These results indicate that the recognition of the actual main verb immediately triggered an update of the previous interpretation of Verb1, with the resolved interpretation emerged in the left temporal lobe and was later delivered to the right posterior temporal and parietal areas. It is also worth noting that the left hippocampus was activated for both measures of Verb1 parse depth after the actual main verb is recognized, suggesting that the episodic memory of experienced events might contribute to the updating of structural interpretations (<xref ref-type="bibr" rid="R10">Bicknell et al. 2010</xref>; <xref ref-type="bibr" rid="R63">Metusalem et al. 2012</xref>; <xref ref-type="bibr" rid="R2">Altmann and Ekves 2019</xref>). These results address the dynamic update of structured interpretation by focusing on the BERT parse depth of Verb1, which complements those of the interpretative mismatch based on the incremental BERT parse depth vector incorporating constraints of all the words heard so far (<xref ref-type="fig" rid="F6">Figure 6F</xref>).</p></sec><sec id="S8"><title>Emergent structural interpretations driven by multifaceted constraints in the brain</title><p id="P36">Next, we further asked how the multifaceted constraints, considered by human listeners and encoded in BERT parse depths, drive the structured interpretation in the brain. When and where in the brain do these constraints emerge? How are their neural effects related to those of the final resolved sentential structure? To address these questions, we first tested the subject noun thematic role properties obtained from corpus data. Significant effects of agenthood and patienthood were found in the preposition epoch (<xref ref-type="fig" rid="F8">Figure 8A</xref>) and in the main verb epoch (<xref ref-type="fig" rid="F8">Figure 8B</xref>) separately. Notably, effects of the subject noun itself preceded those of incremental BERT parse depth vectors modelling the sentence fragments in the same epoch (compare <xref ref-type="fig" rid="F8">Figure 8A</xref> with <xref ref-type="fig" rid="F6">Figure 6B</xref>, and <xref ref-type="fig" rid="F8">Figure 8B</xref> with <xref ref-type="fig" rid="F6">Figure 6C</xref>). These findings indicate that subject noun thematic role might be evaluated before building the overall structural interpretation of the utterance delivered so far. Specifically, the initial preference for an Active interpretation during Verb1, while present as the preposition started (i.e., subject noun agenthood in <xref ref-type="fig" rid="F8">Figure 8A</xref>), was superseded by the preference for a Passive interpretation as the rest of the prepositional phrase (<xref ref-type="fig" rid="F6">Figure 6A</xref>) and the main verb (<xref ref-type="fig" rid="F6">Figure 6B</xref>) were heard.</p><p id="P37">Despite being jointly constrained by subject noun thematic role preference and Verb1 transitivity in a probabilistic manner, the structural interpretation temporarily held just before the recognition of the actual main verb could differ across sentences (e.g., Passive interpretation in <italic>“The dog found in the park…”</italic> and Active interpretation in <italic>“The dog walked in the park…”</italic>). Therefore, in contrast to the Passive or Active index specialized for one particular structural interpretation, we constructed a non-directional index that merely quantifies the degree of interpretative coherence for one interpretation, whether Passive or Active (see <xref ref-type="sec" rid="S11">Methods</xref> for details). Thus, a higher value only indicates greater interpretative coherence between the subject noun and Verb1 regardless of which interpretation is preferred.</p><p id="P38">Effects of this non-directional measure of interpretative coherence appeared very soon after the main verb onset in both hemispheres and lasted till its offset (<xref ref-type="fig" rid="F8">Figure 8C</xref>), suggesting an immediate evaluation of the previously integrated constraints from the subject noun and Verb1 when a listener realized that the sentence had not finished yet. Moreover, these effects roughly co-occurred with the effects of subject noun patienthood (compare <xref ref-type="fig" rid="F8">Figures 8B and 8C</xref>), indicating that a patient role for the subject noun was considered as the main verb was being recognized. Intriguingly, the most sustained regions associated with this non-directional index, including the left ATL, angular gyrus (AG) and precuneus, are also the classical areas of the default mode network (DMN). This finding is consistent with recent claims that the DMN integrates external input with internal prior knowledge to make sense of an input stimulus such as speech (<xref ref-type="bibr" rid="R86">Yeshurun <italic>et al</italic>. 2021</xref>). In particular, precuneus and AG have been found to be involved in building thematic relationships and event structures (<xref ref-type="bibr" rid="R4">Baldassano et al. 2017</xref>; <xref ref-type="bibr" rid="R39">Humphreys et al. 2021</xref>).</p><p id="P39">Following the declining effects of the non-directional index upon the recognition of the main verb, we found significant effects of the Passive index in right anterior fronto-temporal regions (<xref ref-type="fig" rid="F8">Figure 8D</xref>), suggesting that the intended Passive interpretation was eventually established in all sentences. Previous studies have revealed that the relatively narrow sentence-specific information and the broad world knowledge are processed in the left and right hemispheres separately (<xref ref-type="bibr" rid="R41">Jung-Beeman 2005</xref>; <xref ref-type="bibr" rid="R62">Metusalem et al. 2016</xref>; <xref ref-type="bibr" rid="R78">Troyer et al. 2022</xref>). Relevant to this, in the main verb epoch, we found effects of the BERT parse depth vector and those of the Passive index in the left and right hemispheres respectively, arising almost at the same time as the main verb was recognized (compare <xref ref-type="fig" rid="F6">Figure 6C</xref> and <xref ref-type="fig" rid="F8">Figure 8D</xref>). Therefore, a critical question is whether and how the online structural interpretation of a specific sentence is facilitated by the interpretative coherence conjured up from lexical constraints that also depend on broad world knowledge (e.g., thematic role).</p><p id="P40">To address this question, we adopted non-negative matrix factorization to decompose the whole-brain ssRSA fits of the Passive index and the BERT parse depth vector found in the main verb epoch into two sets of components given their temporal synchronizations (see <xref ref-type="sec" rid="S11">Methods</xref>). We then conducted multivariate Granger causality analyses (GCA) to infer directed connections among them. We found only GC connections from the components of Passive index to those of BERT parse depth vector (<xref ref-type="fig" rid="F8">Figure 8E</xref>). Specifically, we identified information flows from the right hemisphere components of the Passive index to the left hemisphere components of BERT parse depth vector, suggesting that a sentence’s structure represented in the left hemisphere might be influenced by the coarse estimate of the event plausibility concurrently determined by broad world knowledge in the right hemisphere (<xref ref-type="bibr" rid="R41">Jung-Beeman 2005</xref>) (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 16</xref> for more details).</p></sec><sec id="S9"><title>Comparisons between BERT and corpus/behavioural measures in fitting neural activity</title><p id="P41">To directly assess the performance of BERT structural measures with that of traditional measures extracted from corpus or behavioural data in fitting listeners’ neural activity, we also conducted ssRSA with model RDMs of corpus-based or behavioural measures. In the Verb1 epoch, we tested Verb1 transitivity obtained from either corpus data or human continuations, however, neither of them exhibited significant model fits, which contrasted with the pronounced effects of BERT Verb1 parse depth (<xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 11</xref>). Similarly, in the PP1 and MV epochs, the probabilities of PP and MV continuations, as determined from behavioral data, did not show any significant model fits (<xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 12 and Appendix 1-figure 13</xref>). Furthermore, the effects of BERT parse depth vector in these two epochs (<xref ref-type="fig" rid="F6">Figure 6B and 6C</xref>) remained largely unchanged after controlling for the variance explained by the behavioural measures. These findings suggest that BERT structural measures, compared to corpus-based and behavioral measures, are better at fitting the neural dynamics during incremental speech comprehension. This might be attributed to the capacity of DLMs to capture more nuanced and contextually rich representations (<xref ref-type="bibr" rid="R50">Linzen and Baroni 2021</xref>; <xref ref-type="bibr" rid="R69">Pavlick 2022</xref>).</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P42">In this study, we investigated the neural dynamics involved in constructing structured interpretations from speech. We combined spatiotemporally resolved brain activity of human listeners, quantitative structural representations derived from a DLM (i.e., BERT), and corpus-based and behavioural measures. Our study revealed the emergence and update of a structured interpretation, jointly constrained by different lexical properties related to both linguistic and non-linguistic world knowledge, in an extensive set of brain regions beyond the core fronto-temporal language network. Specifically, our results show (1) a shift from bi-hemispheric lateral frontal-temporal regions to left-lateralized regions in representing the current structured interpretation as a sentence unfolds, (2) a pattern of sequential activations in the left lateral temporal regions, updating the structured interpretation as syntactic ambiguity is resolved, and (3) the influence of lexical interpretative coherence activated in the right hemisphere over the resolved sentence structure represented in the left hemisphere. These findings provide empirical evidence for the <italic>constraint-based</italic> approach to sentence processing and deepen the understanding of specific spatiotemporal patterning and neuro-computational properties underpinning incremental speech comprehension.</p><p id="P43">Using artificial neural networks (ANNs) to study the neural substrates of human cognition complements the long-time pursuit of generative rules and interpretable models (<xref ref-type="bibr" rid="R45">Kriegeskorte and Douglas 2018</xref>). ANNs have informed our understanding of various cognitive processes in the brain by providing quantifiable predictions that aim to connect behaviors and relevant neural activity (<xref ref-type="bibr" rid="R84">Yamins and DiCarlo 2016</xref>; <xref ref-type="bibr" rid="R70">Rabovsky et al. 2018</xref>; <xref ref-type="bibr" rid="R21">Donhauser and Baillet 2019</xref>; <xref ref-type="bibr" rid="R43">Kietzmann et al. 2019</xref>; <xref ref-type="bibr" rid="R85">Yang et al. 2019</xref>; <xref ref-type="bibr" rid="R5">Bao et al. 2020</xref>; <xref ref-type="bibr" rid="R73">Sheahan et al. 2021</xref>; <xref ref-type="bibr" rid="R20">Doerig et al. 2023</xref>; <xref ref-type="bibr" rid="R31">Giordano et al. 2023</xref>). This is crucial for quantifying the outcome of complex, interrelated constraints that arise in specific contexts, such as spoken sentences, and constructing the representational geometry to be probed in the brain. Where DLMs are concerned, recent studies have systematically compared the internal representations of DLMs to those observed in the human brain during language processing, which highlights the importance of predictive coding and contextual information (<xref ref-type="bibr" rid="R72">Schrimpf <italic>et al</italic>. 2021</xref>; <xref ref-type="bibr" rid="R14">Caucheteux <italic>et al</italic>. 2022</xref>; <xref ref-type="bibr" rid="R16">Caucheteux and King 2022</xref>; <xref ref-type="bibr" rid="R32">Goldstein <italic>et al</italic>. 2022</xref>; <xref ref-type="bibr" rid="R35">Heilbron <italic>et al</italic>. 2022</xref>; <xref ref-type="bibr" rid="R77">Toneva <italic>et al</italic>. 2022</xref>; <xref ref-type="bibr" rid="R15">Caucheteux <italic>et al</italic>. 2023</xref>). Furthermore, these studies have motivated the use of DLMs as a computational tool, or hypothesis, to study the neural substrates of language.</p><p id="P44">Here we asked a more specific question, that is, how a sequence of spoken words is incrementally structured and coherently interpreted in the brain. Our goal was to use quantitative measures of sentence structure that capture the interplay between different types of constraints that simultaneously influence this process. As a potential solution, we extracted detailed structural measures specific to the contents in each sentence from the hidden states of BERT, which was trained on massive corpora from real-life language use. Although DLMs such as BERT are not specifically designed to parse sentences, they can learn from training corpora the multi-dimensional properties related to sentence structure and dependency (<xref ref-type="bibr" rid="R54">Manning <italic>et al</italic>. 2020</xref>). In line with this, our analyses confirmed that BERT structural measures incorporate relevant lexical constraints and that they exhibit both behavioural and neural alignments with human listeners.</p><p id="P45">Taking advantage of the contextualized BERT structural measures, our ssRSA results provide neural evidence for the construction of a coherent interpretation driven by the interaction between linguistic and non-linguistic knowledge evoked by individual words as they are heard sequentially in a spoken sentence. Specifically, neural representations of an unfolding sentence’s structure initially emerged in bilateral fronto-temporal regions and became left-lateralized when more complex syntactic properties, rather than canonical linear adjacency, were considered to build a structured interpretation (e.g., beyond Verb1 in our stimulus sentences). Meanwhile, we found right-hemisphere activations associated with broad world knowledge, which is essential for understanding the intended meaning conveyed by the speaker (<xref ref-type="bibr" rid="R10">Bicknell <italic>et al</italic>. 2010</xref>). In addition to the core fronto-temporal language network, we found that the multiple-demand network and the default mode network were also involved during online construction of structured interpretations, which may reflect additional cognitive demands for resolving potential structural ambiguity and evaluating the plausibility of underlying events (<xref ref-type="bibr" rid="R74">Smallwood et al. 2021</xref>).</p><p id="P46">Moreover, our results show that, compared to corpus-based and behavioural measures, BERT structural measures are more effective in fitting listeners’ neural activity, possibly due to its advanced ability in modelling specific contexts within each sentence. Nevertheless, it is important to recognize the important role of corpus-based and behavioral measures as explanatory variables. They are crucial not only in interpreting BERT measures but also in understanding their alignment with listeners’ neural activity. This includes, for instance, the temporal sequence of activations of key lexical constraints and the emerging structure of a sentence (e.g., effects of subject noun patienthood leading those of BERT parse depth vector in the MV epoch, as seen in <xref ref-type="fig" rid="F8">Figure 8B</xref> and <xref ref-type="fig" rid="F6">Figure 6C</xref>) and the spatial distribution of their model fits in the brain (e.g., contrasting model fits of Passive index and BERT parse depth vector in the MV epoch across different hemispheres, as shown in <xref ref-type="fig" rid="F8">Figure 8D</xref> and <xref ref-type="fig" rid="F6">Figure 6C</xref>). Such an integrative approach allows for a more comprehensive understanding of the complex mental processes underpinning speech comprehension, which takes advantages of the interpretability of traditional measures and the deep contextualized representations of DLMs.</p><p id="P47">There are two points to note about the use of BERT. Firstly, unlike autoregressive DLMs trained using left-to-right attention and the next-word prediction task, BERT is trained to predict masked words in a sentence with a bi-directional attention mechanism. The additional right-to-left attention provides updated representations of preceding words every time an incoming word is added to the input (e.g., representation of <italic>“dog”</italic> in <italic>“The dog…”</italic> is different from that in <italic>“The dog found…”</italic>). This feature of BERT is useful for tracking the dynamic change of the representation of a specific word as its context evolves (e.g., Verb1 in this study), particularly in sentences with structural ambiguity. Although autoregressive DLMs also update hidden states as the input unfolds and could be used to study complex sentential structures (<xref ref-type="bibr" rid="R42">Jurayj et al. 2022</xref>), the updated contextual effects are reflected in the hidden states of the right-most incoming word, while those of the preceding words on the left remain unchanged (i.e., the representation of <italic>“dog”</italic> is the same in <italic>“The dog…”</italic> and <italic>“The dog found…”</italic>). This is different from BERT, where the updated contextual effects are reflected in the hidden states of all preceding words.</p><p id="P48">Secondly, although we input each sentence word-by-word to BERT, however, unlike human listeners or recurrent neural networks, BERT process two consecutive inputs (e.g., <italic>“The dog…”</italic> and <italic>“The dog found…”</italic>) independently, and there is no direct relationship between the hidden states of these two inputs. In fact, human listeners would not start over from the beginning of a sentence as it unfolds word-by-word, but update it continually as each word is heard and use whatever information currently available to build a coherent interpretation (<xref ref-type="bibr" rid="R61">McRae and Matsuki 2013</xref>). This difference, however, does not impede our objective of extracting contextualized structural representations at critical points within a sentence. In the case of BERT case, the representation of each word is continuously updated in a bi-directional manner as a new word is added. This process accounts for the constraints imposed by all the words of the input and their interactions, forming a coherent interpretation. Nonetheless, for other hypotheses in speech comprehension, such as parallel parsing and how various grammatically correct sentence structures are maintained and compete in the brain, DLMs with recurrent memory might be more suitable. Such models can better stimulate the continuous, dynamic updating of interpretations that characterizes human sentence processing.</p><p id="P49">In summary, recent developments in DLMs have shown great potential in capturing the dynamic interplay between syntax, semantics, and broader world knowledge that is essential for successful language comprehension. The empirical evidence from this study supports the notion that DLMs, when utilized as potential models of brain computation and integrated with advanced neuroimaging techniques within a well-defined framework can offer significant insights in to human cognition (<xref ref-type="bibr" rid="R45">Kriegeskorte and Douglas 2018</xref>; <xref ref-type="bibr" rid="R20">Doerig <italic>et al</italic>. 2023</xref>). Future DLMs, especially those with more human-like model architecture (<xref ref-type="bibr" rid="R60">McClelland et al. 2020</xref>) and subjected to rigorous evaluation (<xref ref-type="bibr" rid="R11">Binz and Schulz 2023</xref>), hold the potential to shed light on the neural implementation of various rapid incremental processes that support the rapid transition from sound to meaning in the brain.</p></sec><sec id="S11" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S12" sec-type="subjects"><title>Participants</title><p id="P50">Seventeen right-handed native British English speakers participated in this study and provided written consent. One participant was excluded from subsequent analysis due to sleepiness during EMEG scanning, the other sixteen participants were included in the following analyses (aged between 19 and 38 years, 26.5 years on average; 7 females). All participants had normal hearing, and none had any pre-existing neurological condition or mental health issues. This study was approved by the Cambridge Psychology Research Ethics Committee.</p></sec><sec id="S13"><title>Stimuli</title><p id="P51">We constructed 60 sets of six spoken sentences (360 in total) with varying sentential structures. As shown in the example sentence set (<xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 3</xref>), unambiguous (UNA), high transitivity (HiTrans) and low transitivity (LoTrans) sentences contain a long-distance dependency between the subject noun and the main verb introduced by a full or reduced relative clause inserted in between. Whereas there is no long-distance dependency in the sentences of passive (PAS) and two direct object (DO1 and DO2) conditions.</p><p id="P52">Unlike the first verb (Verb1) in the UNA sentences which is unambiguously interpreted as the head of a relative clause, Verb1 in both HiTrans and LoTrans sentences can also be considered alternatively as the “main verb” before the actual main verb (e.g., <italic>was covered</italic> in the example set in <xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 3</xref>) was heard. By varying the nature of Verb1 in the reduced relative clause (e.g., <italic>found/walked</italic>), we manipulated the preference for the two plausible structural interpretations in HiTrans and LoTrans sentences before the appearance of the actual main verb (i.e., a Passive interpretation where Verb1 is the head of a relative clause - the subject noun undergoes the action specified by Verb1; an Active interpretation where Verb1 is the main verb - the subject noun performs the action specified by Verb1).</p><p id="P53">Specifically, in the LoTrans sentences, Verb1 was selected to be optionally transitive according to CELEX (<xref ref-type="bibr" rid="R3">Baayen et al. 1993</xref>) and Google n-gram corpus (books.google.com/ngrams), meaning that it can either take a direct object or not. Thus, a listener could be initially “garden-pathed” into the alternative Active interpretation where the Verb1 is considered as the main verb when a following prepositional phrase fits its intransitive use. Whereas Verb1 in HiTrans sentences was selected to have a higher preference for taking a direct object than Verb1 in LoTrans sentences [subcategorization frame (SCF) probability for direct object according to VALEX (<xref ref-type="bibr" rid="R44">Korhonen et al. 2006</xref>): HiTrans 0.71 ± 0.16, LoTrans 0.44 ± 0.19, two-tailed two-sample t-test, t<sub>(117)</sub> = 8.45, p = 9.3 x 10<sup>-14</sup>]. Therefore, Verb1 in HiTrans sentences was more likely to be recognised as the head of a reduced relative clause given the appearance of a prepositional phrase (e.g., <italic>in the park</italic>) rather than a highly expected direct object.</p><p id="P54">In all the six types of sentences, the subject noun phrase comprised a single-word noun and a preceding determine “<italic>The</italic>”. In each sentence set, sentences of the first four types had the same subject noun, while DO1 and DO2 sentences shared a different subject noun. The reduced relative clause in HiTrans and LoTrans sentences consisted of a head verb, i.e., Verb1 (e.g., <italic>found/walked</italic>) which was followed by a three-word prepositional phrase (e.g., <italic>in the park</italic>).</p><p id="P55">Note that, in 15 out of the 60 sentence sets, the actual main verb in UNA, RR and GP conditions was preceded by an auxiliary verb (e.g., “<italic>was covered</italic>” in the example set in <xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 3</xref>). In the following analyses, we defined the first word after the prepositional phrase as the main verb since its appearance is sufficient to resolve the intended Passive interpretation where Verb1 is a passive verb (i.e., the head of a reduced relative clause).</p><p id="P56">Note that, although UNA, PAS, DO1 and DO2 conditions were not included in subsequent analyses, they added variety to the types of syntactic construction of the stimuli and ecological validity of the experiment, which also prevented potential adaption to a particular sentence structure.</p></sec><sec id="S14" sec-type="methods"><title>Procedure</title><p id="P57">The experimental stimuli (360 spoken sentences recorded by a female native British English speaker with a neutral intonation throughout) were equally divided into four blocks with 90 experimental trials in each. To maintain participants’ attention while they were listening to the stimuli, the experimental trials in each block were interspersed with 9 additional trials consisting of questions related to the contents in the preceding sentence. These questions were presented in written form on the screen, and a “yes” or “no” response was required by button pressing. Each of these question trials was followed by a filler trial (including a normal spoken sentence outside the experimental stimuli) to ensure that no residual task effects would be picked up in the next experimental trial. Each block started with two filler trials. All question trials and filler trials (20 in each block) were excluded from the following analyses. The order of blocks and the order of trials within each block were pseudorandomized across participants.</p><p id="P58">Each experimental trial began with a fixation cross presented at the centre of the screen with a random period ranging from 750ms to 1250ms (1000ms on average) before the onset of the spoken sentence. Participants were asked to look at the fixation cross and to avoid eye movement or blinking while listening to the spoken sentences. There was a 1000ms silence from the end of each sentence followed by a “blink cue” that lasted for 1400ms during which participants could blink. E-Prime Studio version 2 (Psychology Software Tools Inc., PA, USA) was used to present stimuli and record participants’ responses.</p><p id="P59">Auditory stimuli were delivered binaurally through MEG-compatible ER3A insert earphones (Etymotic Research Inc., IL, USA). There was a 26ms ± 2ms delay in sound delivery due to the transmission of auditory signal from the stimulus computer to the earphones. This sound delivery delay was corrected in the following analyses. To ensure that participants were able to hear the stimuli through both earphones, a short hearing test was conducted before the main experiment.</p></sec><sec id="S15"><title>Sentence continuation pre-tests</title><p id="P60">To obtain human incremental interpretations for each of the HiTrans and LoTrans sentences (120 in total), we conducted two continuation pre-tests which involved two different groups of native British English speakers (30 participants in the first pre-test, 18 participants in the second, aged between 18 and 34 years) who did not participate in the main experiment.</p><p id="P61">Specifically, participants wore headphones and were seated in front of a computer. They listened to a fragment of one of the HiTrans/LoTrans sentences starting from its onset and continuing until a certain position in the sentence, and then they were asked to complete this sentence by producing a meaningful continuation. The sentence fragment was binaurally presented up to the Verb1 (e.g., <italic>The dog found…</italic>) in the first pre-test, and was presented up to the end of the prepositional phrase in the second pre-test (e.g., <italic>The dog found in the park…</italic>). In the second pre-test, participants were allowed to provide a full stop as a continuation if they thought that what they had heard was a complete sentence.</p><p id="P62">Based on the continuations obtained in the first pre-test, we calculated the probability of direct object or prepositional phrase in the continuations immediately after the Verb1, i.e., DO probability and PP probability. This provided contextualized measures of the transitive or the intransitive use of Verb1 given the preceding subject noun phrase. Specifically, we defined Verb1 transitivity as DO probability/(1-DO probability) and defined Verb1 intransitivity as (1-DO probability)/DO probability. Given the continuations collected in the second pre-test, we calculated the probability of a main verb in the continuations immediately after the prepositional phrase, i.e., MV probability, which directly reflected a listener’s structural interpretation by the end of the prepositional phrase. The absence of a main verb in the continuation after the prepositional phrase indicated that the Active interpretation was taken and the Verb1 was considered as the “main verb”, whereas the appearance of a main verb in the continuation indicated that the Passive interpretation was taken (i.e., Verb1 was interpreted as a passive verb), and thus a main verb was needed to complete the sentence.</p></sec><sec id="S16"><title>EMEG and MRI acquisition</title><p id="P63">Participants were seated in a magnetically shielded room (IMEDCO GMBH, Switzerland) with their head placed in the helmet of the MEG scanner. MEG data were collected using a Neuromag Vector View system (Elekta, Helsinki, Finland) with 102 magnetometers and 204 planar gradiometers at 1kHz sampling rate. Simultaneous EEG was recorded at 1kHz sampling rate from 70 Ag-AgCl electrodes within an elastic cap (ESACYCAP GmbH, Herrsching-Breitbrunn, Germany). Vertical and horizontal eye movements were recorded by two EOG electrodes attached below and lateral to the left eye, cardiac signals were recorded by two ECG electrodes attached separately to the right shoulder blade and left torso. Five head position indicator (HPI) coils were used to monitor head motion. A 3D digitizer was used to record the position of EEG electrodes, HPI coils and head points on participants’ scalp relative to the 3 anatomical fiducials (i.e., nasion and bilateral preauricular points). To source localize EMEG data, T1-weighted MPRAGE structural MRI image with 1mm isotropic resolution was acquired using a Siemens Prisma 3T scanner (Siemens, Erlangen, Germany). All EMEG and MRI data were collected at the MRC Cognition and Brain Sciences Unit, University of Cambridge.</p></sec><sec id="S17"><title>EMEG preprocessing and source localization</title><p id="P64">Maxfilter (Elekta, Helsinki, Finland) was applied to raw MEG data for bad channel removal and head-motion compensation. Signals outside the brain were removed using the temporal extension of signal-space separation (<xref ref-type="bibr" rid="R75">Taulu and Simola 2006</xref>). EMEG data were low-pass filtered at 40 Hz and high-pass filtered at 0.5 Hz with a 5<sup>th</sup> order bidirectional Butterworth filter using SPM12 (Wellcome Trust Centre for Neuroimaging, UCL). Independent component analysis (ICA) was conducted using EEGLAB (SCCN, UCSD), components related to blink, eye-movement and physiological noises were removed according to the correlation with EOG, ECG signals and further visual inspection. The preprocessed EMEG data were then downsampled to 200 Hz. Three epochs were extracted from the continuous EMEG recordings of each HiTrans or LoTrans sentence with auditory delivery delay corrected - V1 epoch was aligned to the onset of the Verb1, PP1 epoch was aligned to the onset of the preposition, MV epoch was aligned to the onset of the main verb. All the three epochs were 600ms in length. For all three epochs, baseline correction was performed using the signal from a silent period (i.e., -200 ms to 0 ms relative to sentence onset). Finally, automatic artefact rejection was conducted to exclude trials with signals that exceeded pre-defined amplitude thresholds (60 ft/mm for gradiometers, 3000 ft for magnetometers and 200 uV for EEG electrodes). The uniqueness point of V1/PP1/MV was defined as the earliest point in time when this word can be fully recognized after removing all of its phonological competitors. We first identified the phoneme by which this word can be uniquely recognized according to CELEX (<xref ref-type="bibr" rid="R3">Baayen <italic>et al</italic>. 1993</xref>). Then, we manually labelled the offset of this phoneme in the auditory file of the spoken sentence.</p><p id="P65">EMEG data source localization was performed using SPM12. Source space was modelled by a cortical mesh consisting of 8196 vertices. The sensor positions were co-registered to individual T1-weighted structural image by aligning fiducials and the digitized head shape to the outer scalp mesh. MEG forward model was constructed using the single-shell model (<xref ref-type="bibr" rid="R71">Sarvas 1987</xref>), while EEG forward model was built using the boundary element model (<xref ref-type="bibr" rid="R64">Mosher et al. 1999</xref>). Inversion of EMEG data was conducted for V1, PP1 and MV epochs separately using the least-squares minimum norm method (<xref ref-type="bibr" rid="R34">Hamalainen and Ilmoniemi 1994</xref>) and an empirical Bayesian MEG and EEG data fusion scheme implemented in SPM12 (<xref ref-type="bibr" rid="R36">Henson et al. 2009</xref>).</p></sec><sec id="S18"><title>Incremental structural representations of BERT</title><p id="P66">To obtain incremental structural representations of BERT, we adopted a structural probing approach (<xref ref-type="bibr" rid="R38">Hewitt and Manning 2019</xref>) to quantify a sentence’s structure by estimating each word’s parse depth in the corresponding dependency parse tree based on the contextualized word embeddings from the hidden states of BERT, which explicitly considers the specific contents of the words in the input. Specifically, a structural probing model was trained to find an optimal linear transformation to be applied to the BERT contextualized embeddings of words in the input sentence, so that the squared L2 norm of the transformed word embeddings provided the best estimate for each word’s parse depth of in the dependency parse tree of this sentence.</p><p id="P67">We followed the procedure described in a previous study (<xref ref-type="bibr" rid="R38">Hewitt and Manning 2019</xref>) and trained a structural probing model for each BERT layer with the annotated corpus from Penn Treebank (<xref ref-type="bibr" rid="R55">Marcus et al. 1993</xref>). Contextualized word embeddings were extracted from each of the 24 layers in a pre-trained version of BERT (BERT-large-cased) using HuggingFace (<xref ref-type="bibr" rid="R83">Wolf et al. 2019</xref>). For each BERT layer, the training process was repeated 10 times with different random initializations, the averaged BERT parse depth was used in the following analyses. The performance of structural probing models trained by different BERT layers was evaluated by root accuracy. Root accuracy is defined as the percentage of the sentences in which the samllest parse depth is assigned to the main verb (i.e., the root of the dependency parse tree has a parse depth of 0) when the whole sentence is input to the model.</p><p id="P68">Each HiTrans or LoTrans sentence was input word-by-word to the trained BERT structural probing models, which resulted in a vector consisting of the parse depth of each word in the incremental input (e.g., a 3-dimensional BERT parse depth vector for the input <italic>“The dog found…”</italic>). Taking advantage of the bi-directional attention mechanism of BERT, the parse depth of each preceding word was constantly updated as the input unfolded word-by-word, capturing the incrementality of speech comprehension. Besides, we defined interpretive mismatch as the cosine distance between an incremental BERT parse depth vector and the corresponding incremental context-free parse depth vector for the Passive or the Active interpretation. The smaller the interpretive mismatch with one particular interpretation, the higher the preference for this interpretation given BERT structural representations.</p><p id="P69">To determine the contribution of the words at different positions in a sentence to the incremental BERT parse depth vectors, we shuffled the parse depths of the words at a particular position across sentences at a time and kept the parse depths of the other words unchanged. Then we calculated the Spearman distance (i.e., 1-Spearman’s rho) between the original BERT parse depth vector and the shuffled counterpart. The higher this distance, the more important the words at this position are to the BERT parse depth vectors (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 3</xref>).</p></sec><sec id="S19"><title>Corpus-based measures of multifaceted constraints and their interpretative coherence</title><p id="P70">We quantified subject noun thematic role properties and Verb1 transitivity preference based on a concatenated corpus (3.4 billion tokens, 162.1 million sentences) consisting of the British National Corpus, the Wikipedia dump (by October 2020) and ukWaC (<xref ref-type="bibr" rid="R7">Baroni et al. 2009</xref>). A dependency parser (<xref ref-type="bibr" rid="R65">Mrini et al. 2020</xref>) was first applied to each sentence in the concatenated corpus to specify the subject noun and the verb(s) related to it. For the subject noun in each sentence, we used a semantic role labelling model (<xref ref-type="bibr" rid="R49">Li et al. 2020</xref>) to obtain its thematic role. For simplicity, we only considered the thematic role of an agent or a patient (<xref ref-type="bibr" rid="R22">Dowty 1991</xref>).</p><p id="P71">For each subject noun in HiTrans and LoTrans sentences, we counted separately how many times it took the thematic role of an agent or a patient in the concatenated corpus. Then we defined its agenthood as the ratio of the number of its appearances as an agent to that of its appearances as a patient, and versa visa for its patienthood. We also counted separately how many times the Verb1 in each HiTrans or LoTrans sentence took a direct object or alternative SCFs in the corpus. Each Verb1’s transitivity was defined as the ratio of the frequency it took a direct object to that it took alternative SCFs, and vice versa for its intransitivity. By doing so, we obtained subject noun agenthood and patienthood, Verb1 transitivity and intransitivity. Note that the Verb1 (in)transitivity estimated from the human continuations in the pre-test is context-dependent given the specific preceding subject noun, while the Verb1 (in)transitivity estimated from the concatenated corpus is context-independent in the sense that it accounted for every appearance of this verb without being biased to a specific context.</p><p id="P72">We further derived Passive/Active index capturing the interpretative coherence between the subject noun and Verb1 as they affected Passive and Active interpretations separately. The Passive index was obtained by multiplying subject noun patienthood with Verb1 transitivity, given that both high subject noun patienthood and high Verb1 transitivity coherently prefer a Passive interpretation as the prepositional phrase is heard (e.g., <italic>The <bold>dog found</bold> in the park…</italic>). In contrast, the Active index was obtained by multiplying subject noun agenthood by Verb1 intransitivity, capturing the preference for an Active interpretation (e.g., <italic>The <bold>king walked</bold> in the garden…</italic>). Besides, we also calculated a contextualized version Passive/Active index by using Verb1 (in)transitivity derived from human continuation pre-tests instead of that derived from the concatenated corpus. In addition, we derived a non-directional index by applying logarithmic transformation to the ratio measures of lexical constraints (i.e., subject noun agenthood or patienthood, Verb1 intransitivity or transitivity) before multiplying them. This manipulation removed the directionality of the Passive or the Active index, so that the non-directional index only indicates the interpretative coherence between the subject noun and Verb1 regardless of which interpretation is considered (see illustrations of directionality in <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 17</xref>).</p></sec><sec id="S20"><title>Spatiotemporal searchlight representational similarity analysis (ssRSA)</title><p id="P73">ssRSA was conducted to compare the (dis)similarity structure of BERT structural measures or the multifaceted probablistic constraints with the (dis)similarity structure of observed spatiotemporal patterns of listeners’ brain activity. We used a spatiotemporal searchlight with a 10mm spatial radius and 30ms temporal radius (i.e., a 60 ms sliding time window) which was mapped across the whole brain in the source-localized EMEG.</p><p id="P74">For brain activity, we constructed data representational dissimilarity matrix (RDM) by vectorizing the source-localized EMEG data within each spatiotemporal searchlight for all the trials (i.e., 60 HiTrans sentences and 60 LoTrans sentences) and calculated the pairwise Pearson’s correlation distance (i.e., 1 - Pearson’s r) among them, which resulted in a 120 x 120 data RDM. Multivariate normalisation was applied to improve the reliability of distance measures and reduce the task-irrelevant heteroscedastic structure across trials and vertices (<xref ref-type="bibr" rid="R33">Guggenmos et al. 2018</xref>). Model RDMs of the same size (i.e., 120 x 120) were constructed by calculating either the absolute pair-wise difference for a scalar measure (e.g., SN agenthood/patienthood, Passive/Active index, BERT Verb1 parse depth) or the cosine distance among the incremental BERT parse depth vectors of the 120 sentences (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 1</xref> for a summary of all the model RDMs). We used ratio measures to represent subject noun agenthood/patienthood, Verb1 transitivity/intransitivity and Passive/Active index because they provided the directionality needed to differentiate the two opposite aspects of the same lexical constraint in the model RDMs (see illustrations in <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 11</xref>) and made it possible to test them separately in the brain.</p><p id="P75">Each model RDM was compared against the data RDM of a searchlight centered at each vertex and time point using Spearman’s rank correlation, which resulted in a time-series of model fit (i.e., rank correlation coefficient rho) for each vertex. For each time point, a one-tailed one-sample t-test was conducted at each vertex with the fits of all participants for this model RDM to test whether the mean model fit is significantly above zero. Cluster permutation tests were performed for multiple comparison correction with 5,000 nonparametric permutations, vertex-wise <italic>P</italic> &lt; 0.01 and cluster-wise <italic>P</italic> &lt; 0.05.</p></sec><sec id="S21"><title>Granger causality analysis (GCA) based on ssRSA model fits</title><p id="P76">GCA was conducted to investigate the relationship between multifaceted constraints and BERT structural measures in terms of their effects in the brain, i.e., ssRSA model fits of the corresponding model RDMs. For a given model RDM, each participant’s non-thresholded whole-brain model fit time-series were normalized and concatenated across participants, which resulted in a vertex by time-point matrix. Non-negative matrix factorization (NMF) was applied to this concatenated model fit matrix with negative model fits zeroed. NMF was repeated 20 times with random starting values using a multiplicative update algorithm in MATLAB, results with the least root mean square residual (RMS) was used in the following analyses. The optimal number of NMF factors was determined by searching for the one with the least RMS in a wide range of factor numbers (from 2 to 50). With the optimal number of factors, the resulted time-series of NMF factors from all participants were reshaped into a factor by time-point by participant matrix which was used as the input of multivariate GCA implemented by the Multivariate GCA toolbox (<xref ref-type="bibr" rid="R6">Barnett and Seth 2014</xref>). Multivariate GCA was conducted using two sets of NMF factors derived from the fits of two model RDMs with Akaike information criterion (AIC) adopted for GCA model order estimation. GC significance was determined by a permutation test in which 1,000 surrogate data sets were created by randomly rearranging short time windows (of length model order) from the original factor time-course. <italic>P</italic>-values below 0.005 were refined via a tail approximation from the Generalised Pareto Distribution using PALM (<xref ref-type="bibr" rid="R82">Winkler et al. 2016</xref>). Multiple comparisons were controlled with false discovery rate (FDR) alpha &lt; 0.05 for both between- and within-model RDM GC connections.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix 1</label><media xlink:href="EMS175595-supplement-Appendix_1.pdf" mimetype="application" mime-subtype="pdf" id="d8aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgements</title><p>This research was funded by European Research Council Advanced Investigator Grant to L.K.T. under the European Community’s Horizon 2020 Research and Innovation Programme (2014-2022 ERC Grant Agreement 669820). B.L. was supported by the Ministry of Science and Technology of China and Changping Laboratory. We thank Billi Randall and Barry Devereux for their valuable contributions to early experimental design and to stimulus development; and Hun S. Choi, Benedict Vassileiou, John Hewitt, Tao Li, Yi Zhu, Nai Ding and Giorgio Marinato for helpful discussions.</p></ack><sec id="S23" sec-type="data-availability"><title>Data and materials availability</title><p id="P77">Preprocessed E/MEG data and scripts are available in the following repository <ext-link ext-link-type="uri" xlink:href="https://osf.io/7u8jp/">https://osf.io/7u8jp/</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P78"><bold>Author contributions</bold></p><p id="P79">Conceptualization: L.K.T., W.D.M., B.L.</p><p id="P80">Investigation, Data curation: B.L., Y.F.</p><p id="P81">Methodology, Formal Analysis &amp; Visualization: B.L.</p><p id="P82">Funding acquisition &amp; Project administration: L.K.T.</p><p id="P83">Supervision: L.K.T., W.D.M.</p><p id="P84">Writing – original draft, review &amp; editing: B.L., W.D.M., L.K.T.</p></fn><fn id="FN2" fn-type="conflict"><p id="P85"><bold>Declaration of interests</bold></p><p id="P86">Authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altmann</surname><given-names>GT</given-names></name></person-group><article-title>Ambiguity in sentence processing</article-title><source>Trends in Cognitive Sciences</source><year>1998</year><volume>2</volume><fpage>146</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">21227111</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altmann</surname><given-names>GTM</given-names></name><name><surname>Ekves</surname><given-names>Z</given-names></name></person-group><article-title>Events as intersecting object histories: A new theory of event representation</article-title><source>Psychological Review</source><year>2019</year><volume>126</volume><fpage>817</fpage><lpage>840</lpage><pub-id pub-id-type="pmid">31144837</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baayen</surname><given-names>RH</given-names></name><name><surname>Piepenbrock</surname><given-names>R</given-names></name><name><surname>van</surname><given-names>HR</given-names></name></person-group><article-title>The {CELEX} lexical data base on {CD-ROM}</article-title><year>1993</year></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Discovering Event Structure in Continuous Narrative Perception and Memory</article-title><source>Neuron</source><year>2017</year><volume>95</volume><fpage>709</fpage><lpage>721</lpage><elocation-id>e705</elocation-id><pub-id pub-id-type="pmcid">PMC5558154</pub-id><pub-id pub-id-type="pmid">28772125</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.041</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><year>2020</year><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC8088388</pub-id><pub-id pub-id-type="pmid">32494012</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>The MVGC multivariate Granger causality toolbox: a new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><year>2014</year><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baroni</surname><given-names>M</given-names></name><name><surname>Bernardini</surname><given-names>S</given-names></name><name><surname>Ferraresi</surname><given-names>A</given-names></name><name><surname>Zanchetta</surname><given-names>E</given-names></name></person-group><article-title>The WaCky wide web: a collection of very large linguistically processed web-crawled corpora</article-title><source>Language Resources and Evaluation</source><year>2009</year><volume>43</volume><fpage>209</fpage><lpage>226</lpage></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>Deep Learning for AI</article-title><source>Communications of the ACM</source><year>2021</year><volume>64</volume><fpage>58</fpage><lpage>65</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bever</surname><given-names>TG</given-names></name></person-group><chapter-title>The cognitive basis for linguistic structures</chapter-title><person-group person-group-type="editor"><name><surname>Hayes</surname><given-names>JR</given-names></name></person-group><source>Cognition and the Development of Language</source><publisher-name>John Wiley</publisher-name><publisher-loc>New York</publisher-loc><year>1970</year></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bicknell</surname><given-names>K</given-names></name><name><surname>Elman</surname><given-names>JL</given-names></name><name><surname>Hare</surname><given-names>M</given-names></name><name><surname>McRae</surname><given-names>K</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><article-title>Effects of event knowledge in processing verbal arguments</article-title><source>Journal of Memory and Language</source><year>2010</year><volume>63</volume><fpage>489</fpage><lpage>505</lpage><pub-id pub-id-type="pmcid">PMC2976562</pub-id><pub-id pub-id-type="pmid">21076629</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2010.08.004</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Binz</surname><given-names>M</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name></person-group><article-title>Using cognitive psychology to understand GPT-3</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2023</year><volume>120</volume><elocation-id>e2218523120</elocation-id><pub-id pub-id-type="pmcid">PMC9963545</pub-id><pub-id pub-id-type="pmid">36730192</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2218523120</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Bisk</surname><given-names>Y</given-names></name><name><surname>Holtzman</surname><given-names>A</given-names></name><name><surname>Thomason</surname><given-names>J</given-names></name><name><surname>Andreas</surname><given-names>J</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Chai</surname><given-names>J</given-names></name><name><surname>Lapata</surname><given-names>M</given-names></name><name><surname>Lazaridou</surname><given-names>A</given-names></name><name><surname>May</surname><given-names>J</given-names></name><name><surname>Nisnevich</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>N</given-names></name><etal/></person-group><source>Experience Grounds Language</source><publisher-name>Association for Computational Linguistics</publisher-name><year>2020</year><month>November</month><fpage>8718</fpage><lpage>8735</lpage><comment>Online:</comment></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T</given-names></name><name><surname>Mann</surname><given-names>B</given-names></name><name><surname>Ryder</surname><given-names>N</given-names></name><name><surname>Subbiah</surname><given-names>M</given-names></name><name><surname>Kaplan</surname><given-names>JD</given-names></name><name><surname>Dhariwal</surname><given-names>P</given-names></name><name><surname>Neelakantan</surname><given-names>A</given-names></name><name><surname>Shyam</surname><given-names>P</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name></person-group><article-title>Language models are few-shot learners</article-title><source>Advances in neural information processing systems</source><year>2020</year><volume>33</volume><fpage>1877</fpage><lpage>1901</lpage></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Deep language algorithms predict semantic comprehension from brain activity</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><elocation-id>16327</elocation-id><pub-id pub-id-type="pmcid">PMC9522791</pub-id><pub-id pub-id-type="pmid">36175483</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-20460-9</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title><source>Nature Human Behaviour</source><year>2023</year><volume>7</volume><fpage>430</fpage><lpage>441</lpage><pub-id pub-id-type="pmcid">PMC10038805</pub-id><pub-id pub-id-type="pmid">36864133</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Brains and algorithms partially converge in natural language processing</article-title><source>Communications Biology</source><year>2022</year><volume>5</volume><fpage>134</fpage><pub-id pub-id-type="pmcid">PMC8850612</pub-id><pub-id pub-id-type="pmid">35173264</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03036-1</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choi</surname><given-names>HS</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Lyu</surname><given-names>B</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Decoding the Real-Time Neurobiological Properties of Incremental Semantic Interpretation</article-title><source>Cereb Cortex</source><year>2021</year><volume>31</volume><fpage>233</fpage><lpage>247</lpage><pub-id pub-id-type="pmcid">PMC7727355</pub-id><pub-id pub-id-type="pmid">32869058</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhaa222</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="confproc"><person-group person-group-type="editor"><name><surname>de Marneffe</surname><given-names>M-C</given-names></name><name><surname>MacCartney</surname><given-names>B</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name></person-group><source>Generating typed dependency parses from phrase structure parses</source><conf-name>Proceedings of the 5th International Conference on Language Resources and Evaluation</conf-name><conf-sponsor>European Language Resources Association</conf-sponsor><conf-loc>Genoa, Italy</conf-loc><conf-date>2006 May 22-28</conf-date><year>2006</year><fpage>449</fpage><lpage>454</lpage></element-citation></ref><ref id="R19"><element-citation publication-type="confproc"><person-group person-group-type="editor"><name><surname>Devlin</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>M-W</given-names></name><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Toutanova</surname><given-names>K</given-names></name></person-group><source>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</source><conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><conf-date>2019 June 2-7</conf-date><year>2019</year><conf-loc>Minneapolis, MN, USA</conf-loc><conf-sponsor>Association for Computational Linguistics</conf-sponsor><fpage>4171</fpage><lpage>4186</lpage></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>RP</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name><name><surname>Richards</surname><given-names>B</given-names></name><name><surname>Ismael</surname><given-names>J</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kietzmann</surname><given-names>TC</given-names></name></person-group><article-title>The neuroconnectionist research programme</article-title><source>Nature Reviews Neuroscience</source><year>2023</year><volume>24</volume><fpage>431</fpage><lpage>450</lpage><pub-id pub-id-type="pmid">37253949</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Two Distinct Neural Timescales for Predictive Speech Processing</article-title><source>Neuron</source><year>2019</year><pub-id pub-id-type="pmcid">PMC6981026</pub-id><pub-id pub-id-type="pmid">31806493</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowty</surname><given-names>D</given-names></name></person-group><article-title>Thematic Proto-Roles and Argument Selection</article-title><source>Language</source><year>1991</year><volume>67</volume><fpage>547</fpage><lpage>619</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><year>2010</year><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="pmid">20171926</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title>Finding Structure in Time</article-title><source>Cognitive Science</source><year>1990</year><volume>14</volume><fpage>179</fpage><lpage>211</lpage></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title>Learning and development in neural networks: the importance of starting small</article-title><source>Cognition</source><year>1993</year><volume>48</volume><fpage>71</fpage><lpage>99</lpage><pub-id pub-id-type="pmid">8403835</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everaert</surname><given-names>MBH</given-names></name><name><surname>Huybregts</surname><given-names>MAC</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group><article-title>Structures, Not Strings: Linguistics as Part of the Cognitive Sciences</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><fpage>729</fpage><lpage>743</lpage><pub-id pub-id-type="pmid">26564247</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>L</given-names></name></person-group><article-title>Syntactic processing: evidence from Dutch</article-title><source>Natural Language &amp; Linguistic Theory</source><year>1987</year><volume>5</volume><fpage>519</fpage><lpage>559</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>L</given-names></name><name><surname>Rayner</surname><given-names>K</given-names></name></person-group><article-title>Making and correcting errors during sentence comprehension: Eye movements in the analysis of structurally ambiguous sentences</article-title><source>Cognitive Psychology</source><year>1982</year><volume>14</volume><fpage>178</fpage><lpage>210</lpage></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>The cortical language circuit: from auditory perception to sentence comprehension</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><fpage>262</fpage><lpage>268</lpage><pub-id pub-id-type="pmid">22516238</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Bahlmann</surname><given-names>J</given-names></name><name><surname>Heim</surname><given-names>S</given-names></name><name><surname>Schubotz</surname><given-names>RI</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name></person-group><article-title>The brain differentiates human and non-human grammars: functional localization and structural connectivity</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2006</year><volume>103</volume><fpage>2458</fpage><lpage>2463</lpage><pub-id pub-id-type="pmcid">PMC1413709</pub-id><pub-id pub-id-type="pmid">16461904</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0509389103</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Esposito</surname><given-names>M</given-names></name><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Intermediate acoustic-to-semantic representations link behavioral and neural responses to natural sounds</article-title><source>Nature Neuroscience</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10076214</pub-id><pub-id pub-id-type="pmid">36928634</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01285-9</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Zada</surname><given-names>Z</given-names></name><name><surname>Buchnik</surname><given-names>E</given-names></name><name><surname>Schain</surname><given-names>M</given-names></name><name><surname>Price</surname><given-names>A</given-names></name><name><surname>Aubrey</surname><given-names>B</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Feder</surname><given-names>A</given-names></name><name><surname>Emanuel</surname><given-names>D</given-names></name><name><surname>Cohen</surname><given-names>A</given-names></name><name><surname>Jansen</surname><given-names>A</given-names></name><etal/></person-group><article-title>Shared computational principles for language processing in humans and deep language models</article-title><source>Nature Neuroscience</source><year>2022</year><volume>25</volume><fpage>369</fpage><lpage>380</lpage><pub-id pub-id-type="pmcid">PMC8904253</pub-id><pub-id pub-id-type="pmid">35260860</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01026-4</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title><source>Neuroimage</source><year>2018</year><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="pmid">29499313</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamalainen</surname><given-names>MS</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name></person-group><article-title>Interpreting magnetic fields of the brain: minimum norm estimates</article-title><source>Medical &amp; Biological Engineering &amp; Computing</source><year>1994</year><volume>32</volume><fpage>35</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">8182960</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Armeni</surname><given-names>K</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2022</year><volume>119</volume><elocation-id>e2201968119</elocation-id><pub-id pub-id-type="pmcid">PMC9371745</pub-id><pub-id pub-id-type="pmid">35921434</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Mouchlianitis</surname><given-names>E</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>MEG and EEG data fusion: simultaneous localisation of face-evoked responses</article-title><source>Neuroimage</source><year>2009</year><volume>47</volume><fpage>581</fpage><lpage>589</lpage><pub-id pub-id-type="pmcid">PMC2912501</pub-id><pub-id pub-id-type="pmid">19398023</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.04.063</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="confproc"><person-group person-group-type="editor"><name><surname>Hewitt</surname><given-names>J</given-names></name><name><surname>Liang</surname><given-names>P</given-names></name></person-group><source>Designing and interpreting probes with control tasks</source><conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Hong Kong, China</conf-loc><conf-date>2019 November 3-7</conf-date><year>2019</year><fpage>2733</fpage><lpage>2743</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="confproc"><person-group person-group-type="editor"><name><surname>Hewitt</surname><given-names>J</given-names></name><name><surname>Manning</surname><given-names>CD</given-names></name></person-group><source>A structural probe for finding syntax in word representations</source><conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Minneapolis, MN, USA</conf-loc><conf-date>2019 June 2-7</conf-date><year>2019</year><fpage>4129</fpage><lpage>4138</lpage></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Humphreys</surname><given-names>GF</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Simons</surname><given-names>JS</given-names></name></person-group><article-title>A Unifying Account of Angular Gyrus Contributions to Episodic and Semantic Cognition</article-title><source>Trends in Neurosciences</source><year>2021</year><volume>44</volume><fpage>452</fpage><lpage>463</lpage><pub-id pub-id-type="pmid">33612312</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jackendoff</surname><given-names>R</given-names></name><name><surname>Jackendoff</surname><given-names>RS</given-names></name></person-group><source>Foundations of language: Brain, meaning, grammar, evolution</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>USA</publisher-loc><year>2002</year><pub-id pub-id-type="pmid">15377127</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung-Beeman</surname><given-names>M</given-names></name></person-group><article-title>Bilateral brain processes for comprehending natural language</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><fpage>512</fpage><lpage>518</lpage><pub-id pub-id-type="pmid">16214387</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jurayj</surname><given-names>W</given-names></name><name><surname>Rudman</surname><given-names>W</given-names></name><name><surname>Eickhof</surname><given-names>C</given-names></name></person-group><source>Garden Path Traversal in GPT-2</source><conf-name>Proceedings of the 5th BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Abu Dhabi, United Arab Emirates</conf-loc><conf-date>2022 December 8</conf-date><year>2022</year><fpage>305</fpage><lpage>313</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sorensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2019</year><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Korhonen</surname><given-names>A</given-names></name><name><surname>Krymolowski</surname><given-names>Y</given-names></name><name><surname>Briscoe</surname><given-names>T</given-names></name></person-group><source>A large subcategorization lexicon for natural language processing applications</source><conf-name>Proceedings of International Conference on Language Resources and Evaluation</conf-name><conf-loc>Genoa, Italy</conf-loc><year>2006</year><fpage>1015</fpage><lpage>1020</lpage></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Douglas</surname><given-names>PK</given-names></name></person-group><article-title>Cognitive computational neuroscience</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><fpage>1148</fpage><lpage>1160</lpage><pub-id pub-id-type="pmcid">PMC6706072</pub-id><pub-id pub-id-type="pmid">30127428</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0210-5</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuperberg</surname><given-names>GR</given-names></name></person-group><article-title>Neural mechanisms of language comprehension: challenges to syntax</article-title><source>Brain Research</source><year>2007</year><volume>1146</volume><fpage>23</fpage><lpage>49</lpage><pub-id pub-id-type="pmid">17400197</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Law</surname><given-names>R</given-names></name><name><surname>Pylkkanen</surname><given-names>L</given-names></name></person-group><article-title>Lists with and without Syntax: A New Approach to Measuring the Neural Processing of Syntax</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><fpage>2186</fpage><lpage>2196</lpage><pub-id pub-id-type="pmcid">PMC8018759</pub-id><pub-id pub-id-type="pmid">33500276</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1179-20.2021</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Jawale</surname><given-names>PA</given-names></name><name><surname>Palmer</surname><given-names>M</given-names></name><name><surname>Srikumar</surname><given-names>V</given-names></name></person-group><source>Structured Tuning for Semantic Role Labeling</source><conf-name>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</conf-name><year>2020</year><comment>Online</comment></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linzen</surname><given-names>T</given-names></name><name><surname>Baroni</surname><given-names>M</given-names></name></person-group><article-title>Syntactic Structure from Deep Learning</article-title><source>Annual Review of Linguistics</source><year>2021</year><volume>7</volume><fpage>195</fpage><lpage>212</lpage></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>B</given-names></name><name><surname>Choi</surname><given-names>HS</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>Neural dynamics of semantic composition</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2019</year><volume>116</volume><fpage>21318</fpage><lpage>21327</lpage><pub-id pub-id-type="pmcid">PMC6800340</pub-id><pub-id pub-id-type="pmid">31570590</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1903402116</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname><given-names>MC</given-names></name><name><surname>Pearlmutter</surname><given-names>NJ</given-names></name><name><surname>Seidenberg</surname><given-names>MS</given-names></name></person-group><article-title>The lexical nature of syntactic ambiguity resolution</article-title><source>Psychological Review</source><year>1994</year><volume>101</volume><fpage>676</fpage><lpage>703</lpage><pub-id pub-id-type="pmid">7984711</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahowald</surname><given-names>K</given-names></name><name><surname>Diachek</surname><given-names>E</given-names></name><name><surname>Gibson</surname><given-names>E</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name><name><surname>Futrell</surname><given-names>R</given-names></name></person-group><article-title>Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages</article-title><source>Cognition</source><year>2023</year><volume>241</volume><elocation-id>105543</elocation-id><pub-id pub-id-type="pmid">37713956</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manning</surname><given-names>CD</given-names></name><name><surname>Clark</surname><given-names>K</given-names></name><name><surname>Hewitt</surname><given-names>J</given-names></name><name><surname>Khandelwal</surname><given-names>U</given-names></name><name><surname>Levy</surname><given-names>O</given-names></name></person-group><article-title>Emergent linguistic structure in artificial neural networks trained by self-supervision</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2020</year><volume>117</volume><fpage>30046</fpage><lpage>30054</lpage><pub-id pub-id-type="pmcid">PMC7720155</pub-id><pub-id pub-id-type="pmid">32493748</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1907367117</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marcus</surname><given-names>MP</given-names></name><name><surname>Santorini</surname><given-names>B</given-names></name><name><surname>Marcinkiewicz</surname><given-names>MA</given-names></name></person-group><article-title>Building a Large Annotated Corpus of English: The Penn Treebank</article-title><source>Computational Linguistics</source><year>1993</year><volume>19</volume><fpage>313</fpage><lpage>330</lpage></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name></person-group><article-title>The temporal structure of spoken language understanding</article-title><source>Cognition</source><year>1980</year><volume>8</volume><fpage>1</fpage><lpage>71</lpage><pub-id pub-id-type="pmid">7363578</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><article-title>Sentence perception as an interactive parallel process</article-title><source>Science</source><year>1975</year><volume>189</volume><fpage>226</fpage><lpage>228</lpage><pub-id pub-id-type="pmid">17733889</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Koster</surname><given-names>C</given-names></name></person-group><article-title>Integrative Processes in Utterance Resolution</article-title><source>Journal of Memory and Language</source><year>1993</year><volume>32</volume><fpage>647</fpage><lpage>666</lpage></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matchin</surname><given-names>W</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><article-title>The cortical organization of syntax</article-title><source>Cereb Cortex</source><year>2020</year><volume>30</volume><fpage>1481</fpage><lpage>1498</lpage><pub-id pub-id-type="pmcid">PMC7132936</pub-id><pub-id pub-id-type="pmid">31670779</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhz180</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Hill</surname><given-names>F</given-names></name><name><surname>Rudolph</surname><given-names>M</given-names></name><name><surname>Baldridge</surname><given-names>J</given-names></name><name><surname>Schutze</surname><given-names>H</given-names></name></person-group><article-title>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2020</year><volume>117</volume><fpage>25966</fpage><lpage>25974</lpage><pub-id pub-id-type="pmcid">PMC7585006</pub-id><pub-id pub-id-type="pmid">32989131</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1910416117</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McRae</surname><given-names>K</given-names></name><name><surname>Matsuki</surname><given-names>K</given-names></name></person-group><article-title>Constraint-based models of sentence processing</article-title><source>Sentence processing</source><year>2013</year><volume>519</volume><fpage>51</fpage><lpage>77</lpage></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metusalem</surname><given-names>R</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title>Hemispheric asymmetry in event knowledge activation during incremental language comprehension: A visual half-field ERP study</article-title><source>Neuropsychologia</source><year>2016</year><volume>84</volume><fpage>252</fpage><lpage>271</lpage><pub-id pub-id-type="pmcid">PMC4825852</pub-id><pub-id pub-id-type="pmid">26878980</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2016.02.004</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metusalem</surname><given-names>R</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Urbach</surname><given-names>TP</given-names></name><name><surname>Hare</surname><given-names>M</given-names></name><name><surname>McRae</surname><given-names>K</given-names></name><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title>Generalized event knowledge activation during online sentence comprehension</article-title><source>Journal of Memory and Language</source><year>2012</year><volume>66</volume><fpage>545</fpage><lpage>567</lpage><pub-id pub-id-type="pmcid">PMC3375826</pub-id><pub-id pub-id-type="pmid">22711976</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2012.01.001</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name><name><surname>Lewis</surname><given-names>PS</given-names></name></person-group><article-title>EEG and MEG: forward solutions for inverse methods</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>1999</year><volume>46</volume><fpage>245</fpage><lpage>259</lpage><pub-id pub-id-type="pmid">10097460</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Mrini</surname><given-names>K</given-names></name><name><surname>Dernoncourt</surname><given-names>F</given-names></name><name><surname>Tran</surname><given-names>QH</given-names></name><name><surname>Bui</surname><given-names>T</given-names></name><name><surname>Chang</surname><given-names>W</given-names></name><name><surname>Nakashole</surname><given-names>N</given-names></name></person-group><source>Rethinking Self-Attention: Towards Interpretability in Neural Parsing</source><publisher-name>Association for Computational Linguistics</publisher-name><year>2020</year><fpage>731</fpage><lpage>742</lpage></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>MJ</given-names></name><name><surname>El Karoui</surname><given-names>I</given-names></name><name><surname>Giber</surname><given-names>K</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Cohen</surname><given-names>LD</given-names></name><name><surname>Koopman</surname><given-names>HJ</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Naccache</surname><given-names>L</given-names></name><name><surname>Hale</surname><given-names>JT</given-names></name><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Neurophysiological dynamics of phrase-structure building during sentence processing</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2017</year><volume>114</volume><fpage>E3669</fpage><lpage>E3678</lpage><pub-id pub-id-type="pmcid">PMC5422821</pub-id><pub-id pub-id-type="pmid">28416691</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1701590114</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Almeida</surname><given-names>D</given-names></name><name><surname>Wainwright</surname><given-names>CL</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Slama</surname><given-names>K</given-names></name><name><surname>Ray</surname><given-names>A</given-names></name></person-group><article-title>Training language models to follow instructions with human feedback</article-title><source>Advances in neural information processing systems</source><year>2022</year></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pallier</surname><given-names>C</given-names></name><name><surname>Devauchelle</surname><given-names>A-D</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Cortical representation of the constituent structure of sentences</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2011</year><volume>108</volume><fpage>2522</fpage><lpage>2527</lpage><pub-id pub-id-type="pmcid">PMC3038732</pub-id><pub-id pub-id-type="pmid">21224415</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1018711108</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pavlick</surname><given-names>E</given-names></name></person-group><article-title>Semantic Structure in Deep Learning</article-title><source>Annual Review of Linguistics</source><year>2022</year><volume>8</volume><fpage>447</fpage><lpage>471</lpage></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabovsky</surname><given-names>M</given-names></name><name><surname>Hansen</surname><given-names>SS</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><article-title>Modelling the N400 brain potential as change in a probabilistic representation of meaning</article-title><source>Nature Human Behaviour</source><year>2018</year><volume>2</volume><fpage>693</fpage><lpage>705</lpage><pub-id pub-id-type="pmid">31346278</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarvas</surname><given-names>J</given-names></name></person-group><article-title>Basic mathematical and electromagnetic concepts of the biomagnetic inverse problem</article-title><source>Physics in Medicine &amp; Biology</source><year>1987</year><volume>32</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="pmid">3823129</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name><name><surname>Tuckute</surname><given-names>G</given-names></name><name><surname>Kauf</surname><given-names>C</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2021</year><volume>118</volume><elocation-id>e2105646118</elocation-id><pub-id pub-id-type="pmcid">PMC8694052</pub-id><pub-id pub-id-type="pmid">34737231</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheahan</surname><given-names>H</given-names></name><name><surname>Luyckx</surname><given-names>F</given-names></name><name><surname>Nelli</surname><given-names>S</given-names></name><name><surname>Teupe</surname><given-names>C</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>Neural state space alignment for magnitude generalization in humans and recurrent networks</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>1214</fpage><lpage>1226</lpage><elocation-id>e1218</elocation-id><pub-id pub-id-type="pmid">33626322</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Bernhardt</surname><given-names>BC</given-names></name><name><surname>Leech</surname><given-names>R</given-names></name><name><surname>Bzdok</surname><given-names>D</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name></person-group><article-title>The default mode network in cognition: a topographical perspective</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><fpage>503</fpage><lpage>513</lpage><pub-id pub-id-type="pmid">34226715</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name></person-group><article-title>Spatiotemporal signal space separation method for rejecting nearby interference in MEG measurements</article-title><source>Physics in Medicine &amp; Biology</source><year>2006</year><volume>51</volume><fpage>1759</fpage><lpage>1768</lpage><pub-id pub-id-type="pmid">16552102</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tenney</surname><given-names>I</given-names></name><name><surname>Xia</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Poliak</surname><given-names>A</given-names></name><name><surname>McCoy</surname><given-names>RT</given-names></name><name><surname>Kim</surname><given-names>N</given-names></name><name><surname>Van Durme</surname><given-names>B</given-names></name><name><surname>Bowman</surname><given-names>SR</given-names></name><name><surname>Das</surname><given-names>D</given-names></name></person-group><source>What do you learn from context? probing for sentence structure in contextualized word representations</source><conf-name>the 7th International Conference on Learning Representations</conf-name><conf-loc>New Orleans, LA, USA</conf-loc><year>2019</year><conf-date>2019 May 6-9</conf-date></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toneva</surname><given-names>M</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name><name><surname>Wehbe</surname><given-names>L</given-names></name></person-group><article-title>Combining computational controls with natural text reveals aspects of meaning composition</article-title><source>Nature Computational Science</source><year>2022</year><volume>2</volume><fpage>745</fpage><lpage>757</lpage><pub-id pub-id-type="pmcid">PMC9912822</pub-id><pub-id pub-id-type="pmid">36777107</pub-id><pub-id pub-id-type="doi">10.1038/s43588-022-00354-6</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troyer</surname><given-names>M</given-names></name><name><surname>McRae</surname><given-names>K</given-names></name><name><surname>Kutas</surname><given-names>M</given-names></name></person-group><article-title>Wrong or right? Brain potentials reveal hemispheric asymmetries to semantic relations during word-by-word sentence reading as a function of (fictional) knowledge</article-title><source>Neuropsychologia</source><year>2022</year><volume>170</volume><elocation-id>108215</elocation-id><pub-id pub-id-type="pmcid">PMC9238440</pub-id><pub-id pub-id-type="pmid">35364091</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2022.108215</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Trueswell</surname><given-names>JC</given-names></name><name><surname>Tanenhaus</surname><given-names>MK</given-names></name></person-group><chapter-title>Toward a lexicalist framework of constraint-based syntactic ambiguity resolution</chapter-title><publisher-name>Lawrence Erlbaum Associates, Inc</publisher-name><source>Perspectives on sentence processing</source><publisher-loc>Hillsdale, NJ, US</publisher-loc><year>1994</year><fpage>155</fpage><lpage>179</lpage></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><article-title>The On-Line Effects of Semantic Context on Syntactic Processing</article-title><source>Journal of Verbal Learning and Verbal Behavior</source><year>1977</year><volume>16</volume><fpage>683</fpage><lpage>692</lpage></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>A</given-names></name><name><surname>Shazeer</surname><given-names>N</given-names></name><name><surname>Parmar</surname><given-names>N</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Jones</surname><given-names>L</given-names></name><name><surname>Gomez</surname><given-names>AN</given-names></name><name><surname>Kaiser</surname><given-names>Ł</given-names></name><name><surname>Polosukhin</surname><given-names>I</given-names></name></person-group><article-title>Attention is all you need</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Faster permutation inference in brain imaging</article-title><source>Neuroimage</source><year>2016</year><volume>141</volume><fpage>502</fpage><lpage>516</lpage><pub-id pub-id-type="pmcid">PMC5035139</pub-id><pub-id pub-id-type="pmid">27288322</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.05.068</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T</given-names></name><name><surname>Debut</surname><given-names>L</given-names></name><name><surname>Sanh</surname><given-names>V</given-names></name><name><surname>Chaumond</surname><given-names>J</given-names></name><name><surname>Delangue</surname><given-names>C</given-names></name><name><surname>Moi</surname><given-names>A</given-names></name><name><surname>Cistac</surname><given-names>P</given-names></name><name><surname>Rault</surname><given-names>T</given-names></name><name><surname>Louf</surname><given-names>Re</given-names></name><name><surname>Funtowicz</surname><given-names>M</given-names></name><name><surname>Brew</surname><given-names>J</given-names></name></person-group><article-title>HuggingFace’s Transformers: State-of-the-art Natural Language Processing</article-title><source>ArXiv</source><year>2019</year><elocation-id>abs/1910.03771</elocation-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="pmid">26906502</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">30643294</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeshurun</surname><given-names>Y</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>The default mode network: where the idiosyncratic self meets the shared social world</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><fpage>181</fpage><lpage>192</lpage><pub-id pub-id-type="pmcid">PMC7959111</pub-id><pub-id pub-id-type="pmid">33483717</pub-id><pub-id pub-id-type="doi">10.1038/s41583-020-00420-w</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Example spoken sentence stimuli and plausible structured interpretations.</title><p>The two target sentences in each set differ only in the transitivity of the first verb (Verb1). Each sentence has two possible structured interpretations before the actual main verb is presented: an active interpretation, where the subject noun (SN) performs the action, and a passive interpretation, where the SN is the recipient of the action. The interpretative preference hinges on the likelihood of the SN acting as an agent or a patient (i.e., its thematic role) in conjunction with the transitivity of Verb1. As the sentence progresses to the prepositional phrase, a combination of higher SN agenthood and greater Verb1 intransitivity (i.e., a higher Active index) generally favors an Active interpretation. Conversely, increased SN patienthood coupled with higher Verb1 transitivity (i.e., a higher Passive index) may lead to a Passive interpretation. Note that while the SN is the same for the two target sentences within the same set, it varies across different sentence sets. All images were generated using Midjourney for illustrative purposes.</p></caption><graphic xlink:href="EMS175595-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Human incremental structural interpretations derived from continuation pre-tests.</title><p><bold>(A)</bold> An example set of target sentences differing only in the transitivity of Verb1, HiTrans: high transitivity, LoTrans: low transitivity. Det: determiner, SN: subject noun, V1: Verb1, PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence. <bold>(B)</bold> Probability of a direct object (left) and a prepositional phrase (right) continuation after Verb1. <bold>(C)</bold> Probability of a main verb in the continuations after Verb1, which indicates an Active interpretation. <bold>(D)</bold> Correlations between corpus-based lexical constraints and probabilistic interpretations in the two pre-tests. (Spearman rank correlation, black dots indicate significance determined by 10,000 permutations, <italic>P<sub>FDR</sub></italic> &lt; 0.05 corrected).</p></caption><graphic xlink:href="EMS175595-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Incremental interpretation of sentential structure by BERT.</title><p><bold>(A)</bold> Context-free dependency parse trees of two plausible structural interpretations. Left: Passive interpretation where V1 is the head of a reduced relative clause. Right: Active interpretation where V1 is the main verb. <bold>(B)</bold> Incremental input to BERT structural probing model, with the lightness of dots encoding different positions in the target sentences. Det: determiner, SN: subject noun, V1: Verb1, PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence. <bold>(C)</bold> BERT structural probing model is trained to output a parse depth vector, representing the parse depths of all the words in the sentence input. The BERT parse depth for a specific word is updated incrementally as the sentence unfolds word-by-word. In this example, the parse depth of “<italic>found</italic>” increases with the presence of the prepositional phrase, indicating an increased preference for the Passive interpretation [according to the context-free parse depths in (A)]. <bold>(D)</bold> Incremental interpretation of the dependency between SN and V1 in the model space consisting of the parse depth of Det, SN and V1. Upper: Each colored circle represents the parse depth vector up to V1 derived at a certain position in the sentence [with the same color scheme as in (B)]. The hollow triangle and circle represent the context-free dependency parse vectors for Passive and Active interpretations in (A). Lower: incremental interpretation of the two target sentence types represented by the trajectories of median parse depth. <bold>(E)</bold> Distance from Passive and Active landmarks in the model space as the sentence unfolds [between each colored circle and the two landmarks in the upper panel of (D)] (two-tailed two-sample t-test, *: <italic>P</italic> &lt; 0.05, **: <italic>P</italic> &lt; 0.001, error bars represent SEM).</p></caption><graphic xlink:href="EMS175595-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Correlation between incremental BERT structural measures and explanatory variables.</title><p>BERT structural measures include <bold>(A</bold>, <bold>B)</bold> BERT interpretative mismatch represented by each sentence’s distance from the two landmarks in model space (<xref ref-type="fig" rid="F3">Figure 3D</xref>); <bold>(C</bold>, <bold>D)</bold> Dynamic updates of BERT interpretative mismatch represented by each sentence’s movement to the two landmarks; <bold>(E</bold>, <bold>F)</bold> Overall structural representations captured by the first two principal components (i.e., PC1 and PC2) of BERT parse depth vectors; <bold>(G</bold>, <bold>H)</bold> BERT Verb1 (V1) parse depth and its dynamic updates. Explanatory variables include lexical constraints derived from massive corpora and the main verb probability derived from the continuation pre-test (Spearman correlation, permutation test, <italic>P</italic><sub>FDR</sub> &lt; 0.05, multiple comparisons corrected for all BERT layers, results shown here are based on layer 14, see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figures 4-6</xref> for the results of all layers, see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 7</xref> for the dynamic change of Verb1 parse depth); PP1-PP3: prepositional phrase, MV: main verb, END: the last word in the sentence.</p></caption><graphic xlink:href="EMS175595-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Illustration of the pipeline for ssRSA.</title><p>For each pair of sentences, we extract their BERT or corpus-based measures and calculate the dissimilarity between these measures, resulting in a model representational dissimilarity matrix (RDM). Meanwhile, we also extract the neural activity recorded while participants are listening to these sentences and calculate their dissimilarity to create a data RDM. Specifically, we use a spatiotemporal searchlight in EMEG source space, which moves across the brain and captures the neural activity within a 10-mm-radius sphere over a 60-ms sliding time window. By correlating the model RDM with data RDMs from all spatiotemporal searchlights, we can identify whether, and if so, when and where the brain represents the information captured by the model RDM. The ssRSA is conducted in V1, PP1 and MV epochs respectively, with HiTrans and LoTrans sentences combined as one group (i.e., 120 sentences in total).</p></caption><graphic xlink:href="EMS175595-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Neural dynamics underpinning the emerging structure and interpretation of an unfolding sentence.</title><p><bold>(A-C)</bold> ssRSA results of BERT parse depth vector up to Verb1 (V1), the preposition (PP1) and the main verb (MV) in epochs separately time-locked to their onsets. <bold>(D-F)</bold> ssRSA results of the mismatch for the preferred structural interpretation (the specific BERT layer from which BERT structural measures were derived was denoted in parentheses). From top to bottom in each panel: vertex t-mass (each vertex’s summed t-value during its significant period); heatmap of time-series of ROI peak t-value (the highest t-value in an ROI at each time-point) with a green bar indicating effect onset and ROI t-mass (each ROI’s summed mean t-value during its significant period); cluster t-mass time-series (summed t-value of all the significant vertices of a cluster at each time-point). [cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05 in (A-E); marginal significance in (F) with cluster-wise <italic>P</italic> = 0.06]. Solid vertical lines indicate the timings of onset, average uniqueness point (UP), and average offset of the word time-locked in the epoch with grey shades indicating the range of one SD. LH/RH: left/right hemisphere. See <xref ref-type="supplementary-material" rid="SD1">Appendix 1-table 2</xref> for full anatomical labels. See <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 8</xref> for Spearman’s rho time-series of ROIs in individual participants, see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 9</xref> for the significant results of other BERT layers in the MV epoch.</p></caption><graphic xlink:href="EMS175595-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Neural dynamics updating the incremental structural interpretation.</title><p><bold>(A)</bold> ssRSA results of BERT Verb1 (V1) parse depth change at the main verb (MV) relative to the parse depth of V1 when it is first encountered. <bold>(B)</bold> ssRSA results of the updated BERT V1 parse depth when the input sentence reaches the MV. <bold>(C)</bold> Spatiotemporal overlap between the effects in (A) and (B). (cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05). See <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 14</xref> for Spearman’s rho time-series of ROIs in individual participants.</p></caption><graphic xlink:href="EMS175595-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Neural dynamics of multifaceted probabilistic constraints underpinning incremental structural interpretations.</title><p><bold>(A</bold>, <bold>B)</bold> ssRSA results of SN agenthood and SN patienthood (i.e., plausibility of SN being the agent or the patient of V1) in PP1 and MV epochs separately. <bold>(C)</bold> ssRSA results of non-directional index (i.e., interpretative coherence between SN and V1 regardless of the structure preferred) in MV epoch. <bold>(D)</bold> ssRSA results of Passive index (i.e., interpretative coherence for the Passive interpretation) in MV epoch. <bold>(E)</bold> Influence of the Passive interpretative coherence on the emerging sentential structure in MV epoch revealed by the Granger causal analysis (GCA) based on the non-negative matrix factorization (NMF) components of whole-brain ssRSA results (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 16</xref> for more details) [(A-D) cluster-based permutation test, vertex-wise <italic>P</italic> &lt; 0.01, cluster-wise <italic>P</italic> &lt; 0.05; (E) permutation test <italic>P</italic><sub>FDR</sub> &lt; 0.05]. See <xref ref-type="supplementary-material" rid="SD1">Appendix 1-figure 15</xref> for Spearman’s rho time-series of ROIs in individual participants.</p></caption><graphic xlink:href="EMS175595-f008"/></fig></floats-group></article>