<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS144473</article-id><article-id pub-id-type="doi">10.1101/2021.12.17.473253</article-id><article-id pub-id-type="archive">PPR434136</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Bayesian inference in ring attractor networks</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kutschireiter</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Basnak</surname><given-names>Melanie A</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Drugowitsch</surname><given-names>Jan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Harvard Medical School, Department of Neurobiology, 200 Longwood Avenue, Boston, MA-02115, United States</aff><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>anna_kutschireiter@hms.harvard.edu</email>, <email>jan_drugowitsch@hms.harvard.edu</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>19</day><month>04</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>18</day><month>04</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Working memories are thought to be held in attractor networks in the brain. These attractors should keep track of the uncertainty associated with each memory, so as to weigh it properly against conflicting new evidence. However, conventional attractors do not represent uncertainty. Here we show how uncertainty could be incorporated into an attractor, specifically a ring attractor that encodes head direction. First, we introduce the first rigorous normative framework (the circular Kalman filter) for benchmarking the performance of a ring attractor under conditions of uncertainty. Next we show that the recurrent connections within a conventional ring attractor can be re-tuned to match this benchmark. This allows the amplitude of network activity to grow in response to confirmatory evidence, while shrinking in response to poor-quality or strongly conflicting evidence. This “Bayesian ring attractor” performs near-optimal angular path integration and evidence accumulation. Indeed, we show that a Bayesian ring attractor is consistently more accurate than a conventional ring attractor. Moreover, near-optimal performance can be achieved without exact tuning of the network connections. Finally, we use large-scale connectome data to show that the network can achieve near-optimal performance even after we incorporate biological constraints. Our work demonstrates how attractors can implement a dynamic Bayesian inference algorithm in a biologically plausible manner, and it makes testable predictions with direct relevance to the head direction system, as well as any neural system that tracks direction, orientation, or periodic rhythms.</p></abstract><kwd-group><kwd>Working memory</kwd><kwd>Bayesian inference</kwd><kwd>Ring attractor networks</kwd><kwd>Head direction neurons</kwd><kwd>Kalman filter</kwd><kwd>Population coding</kwd><kwd>Bump attractor</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Attractor networks are thought to form the basis of working memory (<xref ref-type="bibr" rid="R65">Wang 2001</xref>; <xref ref-type="bibr" rid="R12">Compte 2006</xref>) as they can exhibit persistent, stable activity patterns (attractor states) even after network inputs have ceased (<xref ref-type="bibr" rid="R23">Hansel and Sompolinsky 1998</xref>). An attractor network can gravitate toward a stable state even if its input is based on partial (unreliable) information; this is why attractors have been suggested as a mechanism for pattern completion (<xref ref-type="bibr" rid="R25">Hopfield 1982</xref>). However, the characteristic stability of any attractor network also creates a problem: once the network has settled into its attractor state, it will no longer be possible to see that its inputs might have been unreliable. In this situation, the attractor state will simply represent a point estimate (or “best guess”) of the remembered input, without any associated sense of uncertainty. However, real memories often include a sense of uncertainty (e.g., (<xref ref-type="bibr" rid="R54">Rademaker et al. 2012</xref>; <xref ref-type="bibr" rid="R40">Li et al. 2021</xref>)), and uncertainty has clear behavioral effects (<xref ref-type="bibr" rid="R16">Ernst and Banks 2002</xref>; <xref ref-type="bibr" rid="R53">Piet et al. 2018</xref>; <xref ref-type="bibr" rid="R18">Fetsch et al. 2009</xref>). This motivates us to ask how an attractor network can conjunctively encode a memory and its associated uncertainty.</p><p id="P3">A ring attractor is a special case of an attractor that can encode a circular variable (<xref ref-type="bibr" rid="R34">Knierim and Zhang 2012</xref>). For example, there is good evidence that the neural networks that encode head direction (HD) are ring attractors (<xref ref-type="bibr" rid="R68">Zhang 1996</xref>; <xref ref-type="bibr" rid="R60">Skaggs et al. 1994</xref>; <xref ref-type="bibr" rid="R55">Redish et al. 1996</xref>; <xref ref-type="bibr" rid="R52">Peyrache et al. 2015</xref>; <xref ref-type="bibr" rid="R58">Seelig and Jayaraman 2015</xref>; <xref ref-type="bibr" rid="R32">Kim et al. 2017</xref>; <xref ref-type="bibr" rid="R63">Turner-Evans et al. 2017</xref>; <xref ref-type="bibr" rid="R1">Ajabi et al. 2021</xref>). In a conventional ring attractor, inputs push a “bump” of activity around the ring, with only short-lived changes in bump amplitude or shape (<xref ref-type="bibr" rid="R2">Amari 1977</xref>; <xref ref-type="bibr" rid="R15">Ermentrout 1998</xref>); the rapid decay to a stereotyped bump shape is by design, such that this type of conventional ring attractor network is unable to track uncertainty. However, it would be useful to modify these conventional ring attractors so that they can encode the uncertainty associated with HD estimates. HD estimates are constructed from two types of observations — angular velocity observations and HD observations (<xref ref-type="bibr" rid="R35">Knierim and Zhang 2012</xref>; <xref ref-type="bibr" rid="R24">Heinze et al. 2018</xref>). Angular velocity observations arise from vestibular or proprioceptive signals, as well as optic flow; these observations indicate the head’s rotational movement, and thus a change in HD (<xref ref-type="bibr" rid="R60">Skaggs et al. 1994</xref>; <xref ref-type="bibr" rid="R67">Xie et al. 2002</xref>; <xref ref-type="bibr" rid="R64">Turner-Evans et al. 2017</xref>; <xref ref-type="bibr" rid="R22">Green et al. 2017</xref>). These angular velocity observations are integrated over time (“remembered”) to update the system’s internal estimate of HD, in a process termed angular path integration. Ideally, a ring attractor would track the uncertainty associated with angular path integration errors. Meanwhile, HD observations arise from visual landmarks or other sensory cues that provide information about the head’s current orientation (<xref ref-type="bibr" rid="R68">Zhang 1996</xref>; <xref ref-type="bibr" rid="R58">Seelig and Jayaraman 2015</xref>). These sensory observations can change the system’s internal HD estimate, and once that change has occurred, it is generally persistent (remembered). But like any sensory signal, these sensory observations are noisy; they are not unambiguous evidence of HD. Therefore, the way that a ring attractor responds to each new visual landmark observation should ideally depend on the uncertainty associated with its current HD estimate. This type of uncertainty-weighted cue integration is a hallmark of Bayesian inference (<xref ref-type="bibr" rid="R37">Knill and Pouget 2004</xref>), and would require a network to keep track of its own uncertainty.</p><p id="P4">In this study, we address three related questions. First, how should an ideal observer integrate uncertain evidence over time to estimate a circular variable? For a linear variable, this is typically done with a Kalman filter; here we introduce an extension of Kalman filtering for circular statistics; we call this the circular Kalman filter. This algorithm provides a high-level description of how the brain <italic>should</italic> integrate evidence over time to estimate HD, or indeed any other circular or periodic variable. Second, how could a neural network actually implement the circular Kalman filter? We show how this algorithm could be implemented by a neural network whose basic connectivity pattern resembles that of a conventional ring attractor. With properly tuned network connections, we show that the bump amplitude grows in response to confirmatory evidence, whereas it shrinks in response to strongly conflicting evidence, or the absence of evidence. We call this network a Bayesian ring attractor. Third, how does the performance of a Bayesian ring attractor compare to the performance of a conventional ring attractor? In a conventional ring attractor, bump amplitude is pulled rapidly back to a stable baseline value, whereas in a Bayesian ring attractor, bump amplitude is allowed to float up or down as the system’s certainty fluctuates. As a result, we show that a Bayesian ring attractor has consistently more accurate internal estimates (or “working memory”) of the variable it is designed to encode than a conventional ring attractor.</p><p id="P5">Together, these results provide a principled theoretical foundation for how ring attractor networks can be tuned to conjointly encode a memory and its associated uncertainty. Although we focus on the brain’s HD system as a concrete example, our results are relevant to any other brain system that encodes a circular or periodic variable.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Circular Kalman filtering: a Bayesian algorithm for tracking a circular variable</title><p id="P6">We begin by asking how an ideal observer should dynamically integrate uncertain evidence to estimate a circular variable. At each time point, the observer’s estimate is represented by a probability distribution on a circle, where the peak of the distribution represents the best guess, and the width of the distribution represents the uncertainty associated with that guess (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). This distribution is termed a posterior belief (<xref ref-type="bibr" rid="R37">Knill and Pouget 2004</xref>; <xref ref-type="bibr" rid="R14">Dehaene et al. 2021</xref>). Each new observation updates this belief. Importantly, any given observation has limited reliability, that is, it provides incomplete information. Therefore, we also characterize these observations by probability distributions. In the brain’s HD system, an estimate of head direction (<italic>ϕ</italic>) is based on angular velocity observations (<italic>v</italic>) as well as HD observations (<italic>z</italic>), each with associated reliability (<italic>κ</italic><sub><italic>v</italic></sub> and <italic>κ</italic><sub><italic>z</italic></sub>). What limits reliability is noise; specifically, we assume that angular velocity observations are corrupted by Gaussian noise, while direction observations are corrupted by von Mises noise (because a directional variable is circular, and von Mises approximates Gaussian on a circle). The distribution of probable head directions <inline-formula><mml:math id="M1"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> at current time <italic>t</italic> depends on the accumulated history of these observations up to time <italic>t</italic>.</p><p id="P7">The process of updating an estimate based on noisy observations is often accomplished using a Kalman filter (<xref ref-type="bibr" rid="R28">Kalman 1960</xref>; <xref ref-type="bibr" rid="R29">Kalman and Bucy 1961</xref>), but this assumes that the encoded variable is linear; a circular variable requires a different approach. Because filtering on a circle is analytically intractable (<xref ref-type="bibr" rid="R38">Kurz et al. 2016</xref>), we choose to approximate the distribution of probable head directions by a von Mises distribution, with mean <italic>μ</italic> and certainty (i.e., precision) <italic>k</italic>, so that <inline-formula><mml:math id="M2"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mi>V</mml:mi><mml:mi>M</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This allows us to update the estimated circular variable <inline-formula><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> using a technique called projection filtering (<xref ref-type="bibr" rid="R7">Brigo et al. 1999</xref>; <xref ref-type="bibr" rid="R39">Kutschireiter et al. 2022</xref>): <disp-formula id="FD1"><label>(1)</label><mml:math id="M4"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD2"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P8">Here, <italic>f</italic>(<italic>κ</italic><sub><italic>t</italic></sub>) is a monotonically increasing nonlinear function that controls the speed of decay in certainty <italic>κ</italic><sub><italic>t</italic></sub> (see <xref ref-type="sec" rid="S11">Methods</xref>). <xref ref-type="disp-formula" rid="FD1">Equations (1)</xref> and <xref ref-type="disp-formula" rid="FD2">(2)</xref> describe an algorithm that we call the <bold>circular Kalman filter</bold> (<xref ref-type="bibr" rid="R39">Kutschireiter et al. 2022</xref>). This algorithm provides a general solution for estimating the evolution of a circular variable over time, based on noisy observations.</p><p id="P9">To understand the circular Kalman filter intuitively, it is helpful to think of the observer’s estimate as a vector (<xref ref-type="fig" rid="F1">Fig. 1b</xref>), whose direction represents the current best guess <italic>μ</italic><sub><italic>t</italic></sub>, and whose length represents the certainty <italic>κ</italic><sub><italic>t</italic></sub> associated with that guess. The circular Kalman filter tells us how this vector should change at each time point, based on new observations of angular velocity and direction. Here we outline the intuition behind the circular Kalman filter, focusing on the HD system as a specific example.</p><sec id="S4"><title>Angular velocity observations</title><p>We can think of each angular velocity observation as a vector which points at a tangent to the current HD estimate (<xref ref-type="fig" rid="F1">Fig. 1c</xref>) and rotates the current HD estimate (first RHS term in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>). Angular velocity observations are noisy and so decrease the HD estimate’s certainty (<italic>κ</italic><sub><italic>t</italic></sub>), meaning that the observer’s estimate vector becomes shorter (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). Thus, when angular velocity observations are the only inputs to the HD network — i.e., when HD observations are absent — the certainty <italic>κ</italic><sub><italic>t</italic></sub> associated with the HD estimate will progressively decay, with a speed of decay that depends on <italic>κ</italic><sub><italic>v</italic></sub> (first RHS term in <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>).</p></sec><sec id="S5"><title>HD observations</title><p>We can treat each HD observation as a vector, with a length <italic>κ</italic><sub><italic>z</italic></sub> representing the reliability associated with that observation (e.g., the reliability of a visual landmark observation). We add the HD observation vector to the current HD estimate to obtain the updated HD estimate. The updated estimate’s direction depends on the relative lengths of both vectors. A relatively longer HD observation vector, i.e., a more reliable observation relative to the current HD estimate’s certainty, results in a stronger impact on the updated HD estimate (<xref ref-type="fig" rid="F1">Fig. 1d</xref>, second RHS term in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>). In line with principles of reliability-weighted Bayesian cue combination (<xref ref-type="bibr" rid="R37">Knill and Pouget 2004</xref>) HD observations increase the observer’s certainty if they are confirmatory (i.e., they indicate that the current estimate is correct or nearly so, <xref ref-type="fig" rid="F1">Fig. 1d</xref>). Interestingly, however, if HD observations strongly conflict with the current estimate, they actually <italic>decrease</italic> certainty (<xref ref-type="fig" rid="F1">Fig. 1e</xref>). This notable result is a consequence of the circular nature of the inference task (<xref ref-type="bibr" rid="R48">Murray and Morgenstern 2010</xref>). It stands in contrast to the standard (non-circular) Kalman filter, where an analogous observation would <italic>always</italic> increase the observer’s certainty (<xref ref-type="bibr" rid="R66">Wilson and Finkel 2009</xref>). It is thus a key distinction between the standard Kalman filter and the circular Kalman filter.</p><p id="P10">To summarize, the circular Kalman filter describes how an ideal observer should integrate a stream of unreliable information over time to update an estimate of a circular variable. This algorithm serves as a normative standard to judge the performance of any network in the brain that tracks a circular or periodic variable. Specifically, in the HD system, the circular Kalman filter tells us that angular velocity observations should rotate the HD estimate while reducing the certainty of that estimate. Meanwhile, HD observations should update the HD estimate weighted by their reliability, and they should either increase certainty (if compatible with the current estimate) or reduce it (if strongly conflicting with the current estimate). Note that the circular Kalman filter can integrate HD observations from multiple sources by simply adding all their vectors to the current HD estimate vector (<xref ref-type="fig" rid="F1">Fig. 1f</xref>).</p></sec></sec><sec id="S6"><title>Neural encoding of a probability distribution</title><p id="P11">Thus far, we have developed a normative algorithmic description of how an observer <italic>should</italic> integrate evidence over time to estimate a circular variable. This algorithm requires the observer to represent their current estimate as a probability distribution on a circle. How could a neural network encode this probability distribution? Consider a ring attractor network where adjacent neurons have adjacent tuning preferences, so that the population activity pattern is a spatially localized “bump”. The bump’s center of mass is generally interpreted as a point estimate (or best guess) of the encoded circular variable (<xref ref-type="bibr" rid="R5">Ben-Yishai et al. 1995</xref>; <xref ref-type="bibr" rid="R68">Zhang 1996</xref>). In the HD system, this would be the best guess of head direction. Meanwhile, we let the bump amplitude encode certainty, so that higher amplitude corresponds to higher certainty. Of course, there are other ways to encode certainty — e.g., using bump width rather than bump amplitude. However, there are two good reasons for focusing on bump amplitude. First, as we will see below, this implementation allows the parameters of the encoded probability distribution to be “read out” in a particularly straightforward way. Second, recent data from the mouse HD system shows that the appearance of a visual cue (which increases certainty) causes bump amplitude to increase; moreover, when the bump amplitude is high, the network is relatively insensitive to the appearance of a visual cue that conflicts with the current HD estimate, again suggesting that bump amplitude is a proxy for certainty (<xref ref-type="bibr" rid="R27">Johnson et al. 2005</xref>; <xref ref-type="bibr" rid="R1">Ajabi et al. 2021</xref>).</p><p id="P12">Formally, then, the activity of a neuron i with preferred HD <italic>ϕ</italic><sub><italic>i</italic></sub>. can be written as follows (<xref ref-type="fig" rid="F2">Fig. 2a</xref>): <disp-formula id="FD3"><label>(3)</label><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mtext>other</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>components</mml:mtext></mml:mrow></mml:math></disp-formula> where <italic>μ</italic><sub><italic>t</italic></sub> is the HD point estimate, <italic>κ</italic><sub><italic>t</italic></sub> is the associated certainty, and the “other components” might include a constant (representing baseline activity) or minor contributions of higher-order Fourier components. Note that <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref> does not imply that the tuning curve must be cosine-shaped. Rather, it implies that the cosine <italic>component</italic> of the tuning curve is scaled by certainty. This is satisfied, for example, by any unimodal bump profile whose overall gain is governed by certainty. A particularly interesting case that matches <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref> is a linear probabilistic population code (<xref ref-type="bibr" rid="R42">Ma et al. 2006</xref>; <xref ref-type="bibr" rid="R4">Beck et al. 2011</xref>) with von Mises-shaped tuning curves and independent Poisson neural noise (<xref ref-type="supplementary-material" rid="SD1">SI and Fig. S1</xref>).</p><p id="P13">Importantly, this neural representation would allow downstream neurons to read out the parameters of the probability distribution <inline-formula><mml:math id="M7"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in a straightforward manner. Specifically, downstream neurons could take a weighted sum of the population firing rates (<xref ref-type="sec" rid="S11">Methods</xref>) to recover two parameters, <inline-formula><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. In other words, the parameters <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub> would be accessible to downstream neurons via simple (linear) neural operations. This is notable because <italic>θ</italic><sub>1</sub> and <italic>θ</italic><sub>2</sub> represent the von Mises distribution <inline-formula><mml:math id="M9"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in terms of Cartesian vector coordinates in the 2D plane, whereas <italic>μ</italic> and <italic>k</italic> are its polar coordinates (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). This is related to the phasor representation of neural activity (<xref ref-type="bibr" rid="R41">Lyu et al. 2022</xref>), which also translates bump position and amplitude to polar coordinates in the 2D plane (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). If the amplitude of the activity bump scales with certainty, the phasor representation of neural activity equals the vector representation of the von Mises distribution (<xref ref-type="fig" rid="F2">Fig 2b,c</xref>).</p></sec><sec id="S7"><title>Neural network implementation of the circular Kalman filter</title><p id="P14">Now that we have specified how our model network represents the probability distribution <inline-formula><mml:math id="M10"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> over possible head directions, we can proceed to considering the dynamics of this network — specifically, how it responds to incoming information, or the lack of information. The circular Kalman filter algorithm describes the vector operations required to dynamically update the probability distribution <inline-formula><mml:math id="M11"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with each new observation of angular velocity or head direction. Here we show how the circular Kalman filter could be implemented by a neural network, which we call a Bayesian ring attractor. We describe the features of this network with regard to the HD system, but the underlying concepts are general ones which could be applied to any network that encodes a circular or periodic variable. The dynamics of the Bayesian ring attractor network are given by <disp-formula id="FD4"><label>(4)</label><mml:math id="M12"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>r</italic><sub><italic>t</italic></sub> denotes a population activity vector, with neurons ordered by their preferred head directions <italic>ϕ</italic><sub><italic>i</italic></sub>, <italic>τ</italic> is the cell-intrinsic leak time constant, <italic>W</italic> is the matrix of excitatory recurrent connectivity, <inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is a vector of HD observations, and <italic>g</italic> is a nonlinear function that determines global inhibition, and that we discuss in more detail further below. Let us now consider each of these terms in detail.</p><p id="P15">First, HD observations enter the network via the input vector <inline-formula><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, in the form of a cosine-shaped spatial pattern whose amplitude scales with reliability <italic>κ</italic><sub><italic>z</italic></sub> (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). This implements the vector addition required for the proper integration of these observations. Specifically, the weight assigned to each HD observation is determined by the amplitude of <inline-formula><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, relative to the amplitude of the activity bump in the HD population. Thus, observations are weighted by their reliability, relative to the certainty of the current HD estimate, as in the circular Kalman filter (<xref ref-type="fig" rid="F1">Fig. 1d-e</xref>). An HD observation that tends to confirm the current HD estimate will increase the amplitude of the bump in HD cells, and thus the estimate’s certainty.</p><p id="P16">Second, the matrix of recurrent connectivity <italic>W</italic> has spatially symmetric and asymmetric components (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). The symmetric component consists of local excitatory connections that each neuron makes onto adjacent neurons with similar HD preferences. This holds the bump of activity at its current location in the absence of any other input. The overall strength of the symmetric component (<italic>w</italic><sub><italic>sym</italic></sub>) is a free parameter which we can tune. Meanwhile, the asymmetric component consists of excitatory connections that each neuron makes onto adjacent neurons with shifted HD preferences. This component tends to push the bump of activity around the ring (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Angular velocity observations modulate the overall strength of the asymmetric component (<italic>w</italic><sub><italic>asym</italic></sub>), so that positive and negative angular velocity observations push the bump in opposite directions.</p><p id="P17">Third, decreasing certainty in the HD estimate arising from an increasing angular path integration error is implemented by the global inhibition term, –<italic>g</italic>(<italic>r</italic><sub><italic>t</italic></sub>) ⋅ <italic>r</italic><sub><italic>t</italic></sub> (<xref ref-type="fig" rid="F3">Fig. 3c</xref>). Here, the function <italic>g</italic>’s output increases linearly with bump amplitude in the HD population, resulting in an overall quadratic inhibition (see <xref ref-type="sec" rid="S11">Methods</xref>). Together with the leak, this quadratic inhibition approximates the nonlinear certainty decay <italic>f</italic>(<italic>κ</italic><sub><italic>t</italic></sub>) in the circular Kalman filter (<xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>). The approximation becomes precise in the limit of large posterior certainties <italic>κ</italic><sub><italic>t</italic></sub>.</p><p id="P18">With the appropriate parameter values, the amplitude of the bump decays slowly as long as new HD observations are unavailable, because global inhibition and leak work together to pull the bump amplitude slowly downward (<xref ref-type="fig" rid="F3">Fig. 3e</xref>). This is by design: the circular Kalman filter tells us that certainty decays slowly without a continuous stream of new HD observations. This situation differs from conventional ring attractors, whose bump amplitudes are commonly designed to rapidly decay to their stable (attractor) states. In a hypothetical network that perfectly implemented the circular Kalman filter, the bump amplitude would decay to zero. However, in our Bayesian ring attractor, which merely approximates the circular Kalman filter, the bump amplitude decays to a low but nonzero baseline amplitude (<italic>κ</italic> *).</p><p id="P19">As an illustrative example, we simulated a network of 80 HD neurons (see <xref ref-type="sec" rid="S11">Methods</xref>). We let HD follow a random walk (diffusion on a circle), and we used the time derivative of HD (angular velocity) to modulate the asymmetric component of the connectivity matrix D. As HD changes, we rotate the cosine-shaped bump in the external input vector <inline-formula><mml:math id="M16"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, simulating the effect of a visual cue whose position on the retina depends on HD. This network exhibits a spatially localized bump whose position tracks HD, with an accuracy similar to that of the circular Kalman filter itself (<xref ref-type="fig" rid="F3">Fig. 3e</xref>). Meanwhile, the amplitude of the bump accurately tracks the fluctuating certainty of the HD estimate in the circular Kalman filter, reflecting how noisy angular velocity and HD observations interact to modulate this certainty (see <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>). When the visual cue is removed, the bump amplitude decays toward baseline (<xref ref-type="fig" rid="F3">Fig. 3e</xref>). In the limit of infinitely many neurons, this type of network can be tuned to implement the circular Kalman filter exactly for sufficiently high HD certainties. What this simulation shows is that network performance can come close to benchmark performance even with a relatively small number of neurons (<xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>).</p><p id="P20">Interestingly, when we vary the certainty associated with HD observations, we can observe two operating regimes in the network. When HD observations have high certainty (high <italic>κ</italic><sub><italic>z</italic></sub>), bump amplitude is high and accurately tracks changes HD certainty (<italic>κ</italic><sub><italic>z</italic></sub>). Thus, in this regime, the network performs proper Bayesian inference (<xref ref-type="fig" rid="F3">Fig. 3f</xref>). Conversely, when HD observations have low certainty (low <italic>κ</italic><sub><italic>z</italic></sub>), bump amplitude is low but constant, because it is essentially pegged to its baseline value (the network’s attractor state). In this regime, bump amplitude exaggerates the certainty of the HD estimate, and the network looks more like a conventional ring attractor. We will analyze these two regimes further in the next section.</p></sec><sec id="S8"><title>Bayesian versus conventional ring attractors</title><p id="P21">Conventional ring attractors (<xref ref-type="bibr" rid="R68">Zhang 1996</xref>; <xref ref-type="bibr" rid="R11">Compte 2000</xref>; <xref ref-type="bibr" rid="R65">Wang 2001</xref>) are commonly designed to operate close to their attractor states, so that bump amplitude is nearly constant. This is not true of the Bayesian ring attractor described above, where bump amplitude varies by design. The motivation for this design choice was the idea that, if bump amplitude varies with certainty, the network’s estimate of HD would better match the true HD, because evidence integration would be closer to Bayes-optimal. Here we show that this idea is correct.</p><p id="P22">Specifically, we measure the average accuracy of the network’s HD encoding (over many simulations), for both the Bayesian ring attractor and a conventional ring attractor. To model a conventional ring attractor, we use the same equations as we used for the Bayesian ring attractor, but we adjust the network connection strengths so that the bump amplitude decays to its stable baseline value very quickly (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Specifically, we strengthen both local recurrent excitatory connections (<italic>w</italic><sub><italic>sym</italic></sub>) and global inhibition (<italic>g</italic>(<italic>r</italic><sub><italic>t</italic></sub>)) while maintaining their balance, because their overall strengths are what controls the speed (<italic>β</italic>) of the bump’s return to its baseline amplitude (<italic>κ</italic> *) in the regime near <italic>κ</italic> *, assuming no change in the cell-intrinsic leak time constant τ (see <xref ref-type="sec" rid="S11">Methods</xref>). With stronger overall connections, the bump amplitude decays to its stable baseline value more quickly. We then adjust the strength of global inhibition without changing the local excitation strength to maximize the accuracy of the network’s HD encoding; note that this changes <italic>κ</italic> * but not <italic>β</italic>. This yields a conventional ring attractor where the bump amplitude is almost always fixed at a stable value (<italic>κ</italic> *), with <italic>κ</italic> * optimized for maximal encoding accuracy. Even after this optimization of the conventional ring attractor, it does not rival the accuracy of the Bayesian ring attractor. The Bayesian attractor performs consistently better, regardless of the amount of information available to the network, i.e., the level of certainty in the new HD observations (<xref ref-type="fig" rid="F4">Figure 4b</xref>).</p><p id="P23">This performance difference arises because the conventional ring attractor does not keep track of the certainty associated with the current HD estimate. Ideally, the weight assigned to each HD observation depends on the current certainty of the current HD estimate, as well as the reliability of the observation itself (<xref ref-type="fig" rid="F4">Figure 4c</xref>). A conventional ring attractor will assign more reliable observations a higher weight, but does not take into account the certainty of the current HD estimate. By contrast, the Bayesian ring attractor takes all these factors into account (Figure 5c).</p><p id="P24">To obtain more insight into the effect of bump decay speed (<italic>β</italic>) on network performance, we can also simulate many versions of our network with different values of <italic>β</italic>, which we generate by varying the overall strength of balanced local recurrent excitatory (<italic>w</italic><sub><italic>symmetric</italic></sub>) and global inhibitory connections (<italic>g</italic>(<italic>r</italic><sub><italic>t</italic></sub>)). We in turn vary the overall strength of global inhibition in order to find the best baseline bump amplitude (<italic>κ</italic> *) for each value of <italic>β</italic>. The network with the best performance overall had a slow bump decay speed (low <italic>β</italic>), as expected (<xref ref-type="fig" rid="F4">Figures 4d,e</xref>). As the bump decay speed <italic>β</italic> increased further, performance dropped. However, this could be partially mitigated by increasing baseline bump amplitude (<italic>κ</italic> *) to prevent over-weighting of new observations.</p><p id="P25">We have seen that a slow bump decay (low <italic>β</italic>), i.e., the ability to deviate from the attractor state, is essential for uncertainty-related evidence weighting. That said, lower values of <italic>β</italic> are not always better. In the limit of very slow decay (<italic>β</italic> → 0), bump amplitude would grow so large that new HD observations have little influence, so that the network is nearly “blind” to visual landmarks. Conversely, in the limit of fast dynamics (<italic>β</italic> → ∞) the network is highly responsive to new observations; however, it also has almost no ability to weight those new observations relative to other observations in the recent past. In essence, <italic>β</italic> controls the speed of temporal discounting in evidence integration. Ideally, the bump decay speed <italic>β</italic> should be matched to the expected speed at which stored evidence becomes outdated and thus loses its value.</p><p id="P26">To summarize, we can frame the distinction between a conventional ring attractor and a Bayesian ring attractor as a difference in the speed of the bump’s decay to its stable state. In a conventional ring attractor, the bump decays quickly to its stable state, whereas in a Bayesian ring attractor, it decays slowly. Slow decay maximizes the accuracy of HD encoding because it allows the network to track its own internal certainty. Nonetheless, reasonable performance can be achieved even if the bump’s decay is fast, because a conventional ring attractor can still assign more informative observations a higher weight; it simply fails to assign the current HD estimate its proper weight.</p></sec><sec id="S9"><title>Tuning a biological ring attractor for Bayesian performance</title><p id="P27">Thus far, we have focused on model ring attractors with connection weights built from spatial cosine functions (<xref ref-type="supplementary-material" rid="SD1">Figure S3a</xref>), because this makes the mathematical treatment of these networks more tractable. However, this raises the question of whether a biological neural network can actually implement an approximation of the circular Kalman filter, even without these idealized connection weights. The most well-studied biological ring attractor network is the HD system of the fruit fly <italic>Drosophila melanogaster</italic> (<xref ref-type="bibr" rid="R32">Kim et al. 2017</xref>), and the detailed connections in this network have recently been mapped using large-scale electron microscopy connectomics (<xref ref-type="bibr" rid="R26">Hulse et al. 2021</xref>). We therefore asked whether the motifs from this connectomic data set -- and, by extension, motifs that could be found in any biological ring attractor network -- could potentially implement dynamic Bayesian inference.</p><p id="P28">To address this issue, we modeled the key cell types in this network (<xref ref-type="bibr" rid="R63">D. B. Turner-Evans et al. 2020</xref>; <xref ref-type="bibr" rid="R57">Scheffer et al. 2020</xref>; <xref ref-type="bibr" rid="R26">Hulse et al. 2021</xref>) (HD cells, angular velocity cells, and global inhibition cells), using connectome data to establish the patterns of connectivity between each cell type (<xref ref-type="supplementary-material" rid="SD1">SI text &amp; Fig. S4b-f</xref>). We then analytically tuned the relative connection strengths between different cell types such that the dynamics of the bump parameters in the HD population implement an approximation of the circular Kalman filter. We also added a nonlinear element in the global inhibition layer, as this is required to approximate the circular Kalman filter. We found that this network achieves a HD encoding accuracy which is indistinguishable from that of our idealized Bayesian ring attractor network (<xref ref-type="supplementary-material" rid="SD1">Fig. S4g,h</xref>). Thus, even when we use connectome data to incorporate biological constraints on the network, the network is still able to implement dynamic Bayesian inference.</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P29">Uncertainty can affect navigation strategy (<xref ref-type="bibr" rid="R44">Merkle et al. 2006</xref>; <xref ref-type="bibr" rid="R45">Merkle and Wehner 2010</xref>), spatial cue integration (<xref ref-type="bibr" rid="R10">Cheng et al. 2007</xref>; <xref ref-type="bibr" rid="R8">Campbell et al. 2021</xref>), and spatial memory (<xref ref-type="bibr" rid="R36">Knight et al. 2014</xref>). This provides a motivation for understanding how uncertainty is represented in the neural networks that encode and store spatial variables for navigation. There is good reason to think that these networks are built around attractors. Thus, it is crucial to understand how attractors in general -- and ring attractors in particular -- might track uncertainty in spatial variables like head direction.</p><p id="P30">In this study, we have shown that a ring attractor can track uncertainty by operating in a dynamic regime away from its stable baseline states (its attractor states). In this regime, bump amplitude can vary, because we have tuned the local excitatory and global inhibitory connections in the ring attractor so they are relatively weak. By contrast, stronger overall connections produce a more conventional ring attractor that operates closer to its attractor states. Because the “Bayesian” ring attractor has a variable bump amplitude, bump amplitude grows when recent HD observations have been more reliable; in this situation, the network automatically ascribes more weight to its current estimate, relative to new evidence. Importantly, we have shown that nearly-optimal evidence weighting does not require exact tuning of the network connections. Indeed, even when we used connectome data to implement a network with realistic biological connectivity constraints, the network could still support near-optimal evidence weighting.</p><p id="P31">A key element of our approach is that bump amplitude is used to represent the internal certainty of the system’s HD estimate. In our framework, internal certainty determines the weight ascribed to new evidence, relative to past evidence. As such, the representation of internal certainty plays a crucial role in maximizing the accuracy of our Bayesian ring attractor. This stands in contrast to recent network models of the HD system that do not encode internal certainty, even though they assign more weight to more reliable HD observations, and less weight to less reliable HD observations (<xref ref-type="bibr" rid="R61">Sun et al. 2018</xref>; <xref ref-type="bibr" rid="R62">Sun et al. 2020</xref>). Notably, our network also automatically adjusts its cue integration weights to perform close-to-optimal Bayesian inference for HD observations of varying reliability. This differs from previous approaches (<xref ref-type="bibr" rid="R17">Esnaola-Acebes et al. 2021</xref>) that required hand-tuned weights.</p><p id="P32">Another important element of our approach was that we benchmarked our network model against a rigorous normative standard, the circular Kalman filter, which were derived analytically in (<xref ref-type="bibr" rid="R39">Kutschireiter et al. 2022</xref>) and here describe in terms of intuitive vector operations for the first time. Being able to rely on the circular Kalman filter was important because it allowed us to analytically derive the proper parameter values of our network model, so that the network’s estimate matched the estimate of an ideal observer. A remarkable property of the circular Kalman filter is that new HD observations will actually <italic>decrease</italic> certainty if they conflict strongly with the current estimate. This is not a property of a standard (non-circular) Kalman filter or a neural network designed to emulate it (<xref ref-type="bibr" rid="R66">Wilson and Finkel 2009</xref>). The power of conflicting evidence to decrease certainty is particular to the circular domain. Our Bayesian ring attractor network automatically reproduces this important aspect of the circular Kalman filter. Of course, the circular Kalman filter has applications beyond neural network benchmarking, as the accurate estimation of orientation or any other periodic variable has broad applications in the field of engineering.</p><p id="P33">In the brain’s HD system, the internal estimate of HD is based on not only HD observations (visual landmarks, etc.), but also angular velocity observations. The process of integrating these angular velocity observations over time is called angular path integration. Angular path integration is inherently noisy, and so uncertainty will grow progressively when HD observations are lacking. Our Bayesian ring attractor network is notable in explicitly treating angular path integration as a problem of probabilistic inference. Each angular velocity observation has limited reliability, and this causes the bump amplitude to decay in our network as long as HD observations are absent, in a manner that well-approximates the certainty decay of an ideal observer. In this respect, our network differs from previous investigations of ring attractors having variable bump amplitude (<xref ref-type="bibr" rid="R9">Carroll et al. 2014</xref>).</p><p id="P34">Our work makes several testable predictions. First, we predict that the HD system should contain the connectivity motifs required for a Bayesian ring attractor. Our analysis of <italic>Drosophila</italic> brain connectome data supports this idea; we expect similar network motifs to be present in the HD networks of other animals, such as that of mice (<xref ref-type="bibr" rid="R52">Peyrache et al. 2015</xref>; <xref ref-type="bibr" rid="R1">Ajabi et al. 2021</xref>), monkeys (<xref ref-type="bibr" rid="R56">Robertson et al. 1999</xref>), humans (<xref ref-type="bibr" rid="R3">Baumann and Mattingley 2010</xref>), or bats (<xref ref-type="bibr" rid="R19">Finkelstein et al. 2015</xref>). In the future, it will be interesting to determine whether synaptic inhibition in these networks is nonlinear, as predicted by our models.</p><p id="P35">Second, we predict that bump amplitude in the HD system should vary dynamically, with higher amplitudes in the presence of reliable external HD cues, such as salient visual landmarks. In particular, when bump amplitude is high, the bump’s position should be less sensitive to the appearance of new external HD cues. Notably, an experimental study from the mouse HD system provides some initial support for these predictions (<xref ref-type="bibr" rid="R1">Ajabi et al. 2021</xref>). This study found that the amplitude of population activity in HD neurons (what we call bump amplitude) increases in the presence of a reliable visual HD cue. Bump amplitude also varied spontaneously when all visual cues were absent (in darkness); intriguingly, when the bump amplitude was higher in darkness, the bump position was slower to change in response to the appearance of a visual cue, suggesting a lower sensitivity to the cue. In the future, more experiments will be needed to clarify the relationship between bump amplitude, certainty, and cue integration. In particular, it is puzzling that multiple studies (<xref ref-type="bibr" rid="R69">Zugaro et al. 2001</xref>; <xref ref-type="bibr" rid="R59">Shinder and Taube 2014</xref>; <xref ref-type="bibr" rid="R58">Seelig and Jayaraman 2015</xref>; <xref ref-type="bibr" rid="R22">Green et al. 2017</xref>; <xref ref-type="bibr" rid="R64">D. Turner-Evans et al. 2017</xref>; <xref ref-type="bibr" rid="R1">Ajabi et al. 2021</xref>) have found that bump amplitude increases with angular velocity, as higher angular velocities should not increase certainty.</p><p id="P36">In the future, more investigation will be needed to understand evidence accumulation on longer timescales. The circular Kalman filter is a recursive estimator: at each time step, it only considers the observer’s internal estimate from the previous time step, as well as the current observation of new evidence. However, when the environment changes, it would be useful to use a longer history of past observations (and past internal estimates) to readjust the weight assigned to the changing sources of evidence. Available data suggests that Hebbian plasticity can progressively strengthen the influence of the external sensory cues that are most reliably correlated with HD (<xref ref-type="bibr" rid="R34">Knierim et al. 1998</xref>; <xref ref-type="bibr" rid="R36">Knight et al. 2014</xref>; <xref ref-type="bibr" rid="R20">Fisher et al. 2019</xref>; <xref ref-type="bibr" rid="R31">Kim et al. 2019</xref>). The interaction of Hebbian plasticity with attractor dynamics could provide a mechanism for extending statistical inference to longer timescales (<xref ref-type="bibr" rid="R60">Skaggs et al. 1994</xref>; <xref ref-type="bibr" rid="R30">Keinath et al. 2018</xref>; <xref ref-type="bibr" rid="R46">Milford et al. 2004</xref>; <xref ref-type="bibr" rid="R47">Mulas et al. 2016</xref>; <xref ref-type="bibr" rid="R49">Ocko et al. 2018</xref>; <xref ref-type="bibr" rid="R51">Page et al. 2014</xref>; <xref ref-type="bibr" rid="R50">Page and Jeffery 2018</xref>; <xref ref-type="bibr" rid="R13">Cope et al. 2017</xref>).</p><p id="P37">In summary, our work shows how ring attractors could implement dynamic Bayesian inference in the HD system. Our results have significance beyond the encoding of head direction -- e.g., they are potentially relevant for the grid cell ensemble, which appears to be organized around ring attractors even though it encodes linear rather than circular variables. Moreover, our models could apply equally to any brain system that needs to compute an internal estimate of a circular or periodic variable, such as visual object orientation (<xref ref-type="bibr" rid="R40">Li et al. 2021</xref>; <xref ref-type="bibr" rid="R6">van Bergen et al. 2015</xref>) or circadian time. More generally, our results demonstrate how canonical network motifs, like those common in ring attractor networks, can work together to perform close-to-optimal Bayesian inference, a problem with fundamental significance for neural computation.</p></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12"><title>Ideal observer model: the circular Kalman filter</title><p id="P38">Our ideal observer model - the circular Kalman filter (circKF) [<xref ref-type="bibr" rid="R39">Kutschireiter et al., 2022</xref>] - performs dynamic Bayesian inference for circular variables. It computes the posterior probability of an unobserved (true) HD <inline-formula><mml:math id="M17"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> at each point in time <italic>t</italic>, conditioned on a continuous stream of noisy angular velocity observations <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="M19"><mml:msub><mml:mi>v</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi>ℝ</mml:mi></mml:math></inline-formula>, and HD observations <inline-formula><mml:math id="M20"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula> with <inline-formula><mml:math id="M21"><mml:msub><mml:mi>z</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:math></inline-formula>. Specifically, we assume that these observations are generated from the true angular velocity <italic>ϕ</italic><sub><italic>t</italic></sub> and HD <italic>ϕ</italic><sub><italic>t</italic></sub>, corrupted by zero-mean noise at each point in time, via <disp-formula id="FD5"><label>(5)</label><mml:math id="M22"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi>𝒩</mml:mi><mml:mspace width="0.2em"/><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ϕ</mml:mi><mml:mo>˙</mml:mo></mml:mover><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.2em"/><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD6"><label>(6)</label><mml:math id="M23"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi>𝒱</mml:mi><mml:mi>ℳ</mml:mi><mml:mspace width="0.2em"/><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.2em"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P39">Here, <inline-formula><mml:math id="M24"><mml:mrow><mml:mi>𝒩</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes a Gaussian with mean <italic>μ</italic> and variance <inline-formula><mml:math id="M25"><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>𝒱</mml:mi><mml:mi>ℳ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes a von Mises distribution of a circular random variable with mean <italic>μ</italic> and precision <italic>κ</italic>, and <italic>κ</italic><sub><italic>v</italic></sub> and <italic>κ</italic><sub><italic>z</italic></sub> denote the precision of the angular velocity and HD observations, respectively. Note that as <italic>dt</italic> → 0, the precision of both angular velocity and HD observations approach 0, in line with the intuition that reducing a time step size <italic>dt</italic> results in more observations per unit time, which should be accounted for by less precision per observation to avoid “oversampling”. More formally, the squareroot scaling of the HD observation precision with <inline-formula><mml:math id="M26"><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> ensures that the Fisher information of the observations about the true HD scales linearly in time and <italic>κ</italic><sub><italic>z</italic></sub> in the continuum limit <italic>dt</italic> → 0 [<xref ref-type="bibr" rid="R39">Kutschireiter et al. 2022</xref>, Theorem 2]. The same applies to the <italic>dt</italic><sup>–1</sup> scaling of the Gaussian variance of the angular velocity observations, again achieving a Fisher information that scales linearly in time.</p><p id="P40">To support integrating information over time, the model assumes that current HD <italic>ϕ</italic><sub><italic>t</italic></sub> depends on past HD <italic>ϕ</italic><sub><italic>t–dt</italic></sub>. Specifically, in absence of further evidence, the model assumes that HD diffuses on a circle, <disp-formula id="FD7"><label>(7)</label><mml:math id="M27"><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>𝒩</mml:mi><mml:mspace width="0.2em"/><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ϕ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.2em"/><mml:mtext>mod</mml:mtext><mml:mspace width="0.2em"/><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> with a diffusion coefficient that decreases with <italic>κ</italic><sub><italic>ϕ</italic></sub>. In Results, we assume <italic>κ</italic><sub><italic>ϕ</italic></sub> → 0, implying that HD can change arbitrarily across consecutive time steps, which was sufficient to convey intuition into the algorithm’s workings. However, when simulating stochastic HD trajectories, we assume they evolve according to <xref ref-type="disp-formula" rid="FD7">Eq. (7)</xref> with <italic>κ</italic><sub><italic>ϕ</italic></sub> &gt; 0, which needs to be accounted for when performing inference. Thus, we here assume a non-zero <italic>κ</italic><sub><italic>ϕ</italic></sub> for completeness and reproducibility.</p><p id="P41">The circKF in <xref ref-type="disp-formula" rid="FD1">Eqs. (1)</xref> and <xref ref-type="disp-formula" rid="FD2">(2)</xref> assumes that the posterior distribution over HD can be approximated by a von Mises distribution with time-dependent mean <italic>μ</italic><sub><italic>t</italic></sub> and certainty <italic>κ</italic><sub><italic>t</italic></sub>, i.e. <inline-formula><mml:math id="M28"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mi>𝒱</mml:mi><mml:mi>ℳ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Such an approximation is justified if the posterior is sufficiently unimodal, and can, for instance, be compared to a similar approximation employed by extended Kalman filters for non-circular variables.</p><p id="P42">An alternative parametrization of the von Mises distribution to its mean <italic>μ</italic><sub><italic>t</italic></sub> and precision <italic>κ</italic><sub><italic>t</italic></sub>, are its natural parameters, <inline-formula><mml:math id="M29"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>sin</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. Geometrically, the natural parameters can be interpreted as the Cartesian coordinates of a “probability vector”, and (<italic>μ</italic><sub><italic>t</italic></sub>, <italic>κ</italic><sub><italic>t</italic></sub>) as its polar co-ordinates (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). As we show in the <xref ref-type="supplementary-material" rid="SD1">SI</xref>, the natural parameter parametrization makes including HD observations (<xref ref-type="disp-formula" rid="FD6">Eq. (6)</xref>) in the circKF straightforward. In fact, it becomes a vector addition. In contrast, including angular velocity observations (<xref ref-type="disp-formula" rid="FD5">Eq. (5)</xref>) is mathematically intractable, such that the circKF relies on an approximation method called projection filtering [<xref ref-type="bibr" rid="R7">Brigo et al., 1999</xref>] to find closed-form dynamic expressions for posterior mean and certainty (see [<xref ref-type="bibr" rid="R39">Kutschireiter et al., 2022</xref>] for technical details, and the <xref ref-type="supplementary-material" rid="SD1">SI</xref> for a more accessible description of the circKF).</p><p id="P43">Taken together, the circKF for the model specified by <xref ref-type="disp-formula" rid="FD5">Eqs. (5)</xref>-<xref ref-type="disp-formula" rid="FD7">(7)</xref> reads: <disp-formula id="FD8"><label>(8)</label><mml:math id="M30"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mspace width="0.2em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD9"><label>(9)</label><mml:math id="M31"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mspace width="0.2em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt><mml:mspace width="0.2em"/><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>f</italic>(<italic>κ</italic><sub><italic>t</italic></sub>) is a monotonically increasing nonlinear function, <disp-formula id="FD10"><label>(10)</label><mml:math id="M32"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mi>A</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>with</mml:mtext><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> and <inline-formula><mml:math id="M33"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M34"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denote the modified Bessel functions of the first kind of order 0 and 1, respectively. Setting <italic>κ</italic><sub><italic>ϕ</italic></sub> → 0 yields <xref ref-type="disp-formula" rid="FD1">Eqs. (1)</xref> and <xref ref-type="disp-formula" rid="FD2">(2)</xref>. Importantly, a non-zero <italic>κ</italic><sub><italic>ϕ</italic></sub> does not conceptually change the general vector operations we present in <xref ref-type="fig" rid="F1">Fig. 1</xref>.</p><p id="P44">For a sufficiently large <italic>κ</italic> (i.e., high certainty), the nonlinearity <italic>f</italic>(<italic>κ</italic>) approaches the linear function, <italic>f</italic>(<italic>κ</italic>) → 2<italic>κ</italic> – 2. In our <bold>quadratic approximation</bold>, we thus replace the non-linearity by a quadratic decay: <disp-formula id="FD11"><label>(11)</label><mml:math id="M35"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt><mml:mtext>sin</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> which well-approximates the circKF in the high certainty regime.</p></sec><sec id="S13"><title>Network model</title><p id="P45">We derived a rate-based network model that implements (approximations of) the circKF, by encoding the von Mises posterior parameters in activity <inline-formula><mml:math id="M36"><mml:mrow><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> of a neural population with <italic>N</italic> neurons. Thereby, we focused on the simplest kind of network model that supports such an approximation, which is of the form: <disp-formula id="FD12"><label>(12)</label><mml:math id="M37"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>W</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mtext mathvariant="bold">I</mml:mtext><mml:mi>t</mml:mi><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>τ</italic> is the cell-intrinsic leak time constant, <inline-formula><mml:math id="M38"><mml:mrow><mml:mi>g</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:msub><mml:mi>ℝ</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is a scalar nonlinearity, and the elements of r<sub><italic>t</italic></sub> are assumed to be ordered by the respective neuron’s preferred HD, <italic>ϕ</italic><sub>1</sub>,…, <italic>ϕ</italic><sub><italic>N</italic></sub> (see <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref>). We decomposed the recurrent connectivity matrix into <inline-formula><mml:math id="M39"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>const</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>odd</mml:mtext></mml:mrow></mml:msubsup><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>sin</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <italic>W</italic><sup>const</sup> denotes a matrix with constant entries, and <italic>W</italic><sup>cos</sup> and <italic>W</italic><sup>sin</sup> refer to cosine- and sine-shaped connectivity profiles (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). In the main text we denote <italic>w</italic><sub>1</sub><sup>even</sup> by <italic>w</italic><sub>sym</sub> and <italic>w</italic><sub>1</sub><sup>odd</sup> by <italic>w</italic><sub>asym</sub> to intuitively describe the connectivity pattern shown in <xref ref-type="fig" rid="F3">Fig. 3d</xref> rather than the type of function (<italic>even</italic> and <italic>odd</italic>) or the Fourier component (subscript) that describes network connectivity in the continuous infinite-N limit. Irrespective of notation, the network’s circular symmetry makes the entries of these matrices only depend on the relative distance in preferred HD, and are given by <inline-formula><mml:math id="M40"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>const</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>,</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>cos</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="M41"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mtext>sin</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The scaling factor <inline-formula><mml:math id="M42"><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mi>N</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> was chosen to facilitate matching our analytical results from the continuum network to the network structure outlined here. We further considered a cosine-shaped external input of the form <inline-formula><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Φ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> that is peaked around an input location Φ<sub><italic>t</italic></sub>. Here, <italic>I</italic><sub><italic>t</italic></sub>(<italic>dt</italic>) denotes the input pattern in the infinitesimal time bin <italic>dt</italic>.</p><p id="P46">As described in Results, we assume the population activity r<sub><italic>t</italic></sub> to encode the HD belief parameters <italic>μ</italic><sub><italic>t</italic></sub> and <italic>κ</italic><sub><italic>t</italic></sub> in the phase and amplitude of the activity’s first Fourier component. As we show in the <xref ref-type="supplementary-material" rid="SD1">SI</xref>, the described network dynamics thus lead to the following dynamics of the cosine-profile parameters <italic>μ</italic><sub><italic>t</italic></sub> and <italic>κ</italic><sub><italic>t</italic></sub>: <disp-formula id="FD13"><label>(13)</label><mml:math id="M44"><mml:mi>d</mml:mi><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>odd</mml:mtext></mml:mrow></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>sin</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>Φ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD14"><label>(14)</label><mml:math id="M45"><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mtext>cos</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>Φ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p id="P47">To derive these dynamics, we make the following three assumptions. First, we assume the network to be <italic>rate-based.</italic> Second, our analysis assumes a continuum of neurons, i.e. <italic>N</italic> → ∞. For numerical simulations, and the network description below, we used a finite-sized network of size <italic>N</italic> that corresponds to a discretization of the continuous network. <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref> demonstrates only a very weak dependence of our results on the exact number of neurons in the network. Third, our analysis and simulations focused on the first Fourier mode of the bump profile, and is thus independent of the exact shape of the profile (as long as <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref> holds).</p></sec><sec id="S14"><title>Network parameters for Bayesian inference</title><p id="P48">Having identified how the dynamics of the <italic>μ</italic><sub><italic>t</italic></sub> and <italic>κ</italic><sub><italic>t</italic></sub> encoded by the network (<xref ref-type="disp-formula" rid="FD13">Eqs. (13)</xref> &amp; <xref ref-type="disp-formula" rid="FD14">(14)</xref>) depend on the network parameters, we now tuned these parameters to match these dynamics to those of the mean and certainty of the circKF (<xref ref-type="disp-formula" rid="FD8">Eqs. (8)</xref> &amp; <xref ref-type="disp-formula" rid="FD9">(9)</xref>). Here, we first do so to achieve an exact match to the circKF, without the quadratic approximation. After that we describe the quadratic approximation that is used in the main text and leads to the Bayesian ring attractor network. Specifically, an exact match to the circKF requires the following network parameters: <list list-type="bullet"><list-item><p id="P49">Odd recurrent connectivities are modulated by angular velocity observations, <inline-formula><mml:math id="M46"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>odd</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, which shifts the activity profile without changing its amplitude [<xref ref-type="bibr" rid="R60">Skaggs et al., 1994</xref>, <xref ref-type="bibr" rid="R68">Zhang, 1996</xref>].</p></list-item><list-item><p id="P50">HD observations <italic>z</italic><sub><italic>t</italic></sub> are represented as the peak position Φ<sub><italic>t</italic></sub> of a cosine-shaped external input whose amplitude is modulated by the reliability of the observation, i.e., <inline-formula><mml:math id="M47"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. The inputs might contain additional Fourier modes (e.g., a constant baseline), but those do not affect the dynamics in <xref ref-type="disp-formula" rid="FD13">Eqs. (13)</xref> and <xref ref-type="disp-formula" rid="FD14">(14)</xref>.</p></list-item><list-item><p id="P51">The even component of the recurrent excitatory input needs to exactly balance the internal activity decay, i.e., <inline-formula><mml:math id="M48"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p id="P52">The decay nonlinearity is modulated by the reliability of the angular velocity observations, and is given by <inline-formula><mml:math id="M49"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M50"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> equals the nonlinearity that governs the certainty decay in the circKF (<xref ref-type="disp-formula" rid="FD10">Eq. (10)</xref>). This can be achieved, e.g., through interaction with an inhibitory neuron (or a pool of inhibitory neurons) with activation function <inline-formula><mml:math id="M51"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> that computes the activity bump’s amplitude <inline-formula><mml:math id="M52"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p></list-item></list></p><p id="P53">A network with these parameters is <italic>not</italic> an attractor network, as its activity decays to zero in the absence of external inputs.</p><p id="P54">To arrive at the Bayesian ring attractor, we approximate the decay non-linearity by a quadratic approximation that takes the form <inline-formula><mml:math id="M53"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>quad</mml:mtext></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="M54"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> denotes the rectification nonlinearity. The resulting recurrent inhibition can be shown to be quadratic in the amplitude <italic>κ</italic><sub><italic>t</italic></sub>, and has the further benefit of introducing an attractor state at a positive bump aplitude (see below). In the large population limit, <italic>N</italic> → ∞, this leads to the amplitude dynamics (see <xref ref-type="supplementary-material" rid="SD1">SI</xref> for derivation) <disp-formula id="FD15"><label>(15)</label><mml:math id="M55"><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>quad</mml:mtext></mml:mrow></mml:msup><mml:msubsup><mml:mi>k</mml:mi><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mo>Φ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P55">The dynamics of the phase <italic>μ</italic><sub><italic>t</italic></sub> does not depend on the form of <inline-formula><mml:math id="M56"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and thus remains to be given by <xref ref-type="disp-formula" rid="FD13">Eq. (13)</xref>. If we set the network parameters to <inline-formula><mml:math id="M57"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>quad</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M58"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, while sensory input, i.e. angular velocity <italic>v</italic><sub><italic>t</italic></sub> and HD observations <italic>z</italic><sub><italic>t</italic></sub>, enter in the same way as before, the network implements the quadratic approximation to the circKF (<xref ref-type="disp-formula" rid="FD8">Eqs. (8)</xref> &amp; <xref ref-type="disp-formula" rid="FD11">(11)</xref>).</p></sec><sec id="S15"><title>General ring-attractor networks with fixed point <italic>κ</italic><sup>*</sup> and decay speed <italic>β</italic></title><p id="P56">In absence of HD observations (<italic>I</italic><sub><italic>t</italic></sub> = 0), the amplitude dynamics in <xref ref-type="disp-formula" rid="FD15">Eq. (15)</xref> has a stable <bold>fixed point</bold> at <inline-formula><mml:math id="M59"><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>quad</mml:mtext></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> and no preferred phase, making it a ring-attractor network. Linearizing the <italic>κ</italic><sub><italic>t</italic></sub> dynamics around this fixed points reveals that it is approached with <bold>decay speed</bold> <inline-formula><mml:math id="M60"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula>. Therefore, we can tune the parameters to achieve a particular fixed point <italic>κ</italic><sup>*</sup> and decay speed <italic>β</italic> by setting <inline-formula><mml:math id="M61"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M62"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mtext>quad</mml:mtext></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>*</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. A large value of <italic>β</italic> requires increasing both <italic>w</italic><sup>even</sup> and <italic>w</italic><sup>quad</sup>, and yields faster dynamics and thus indicates more rigid attractor dynamics. In the limit of <italic>β</italic> → ∞ the attractor becomes completely rigid in the sense that, upon any perturbation, it immediately moves back to its attractor state. In the main text we assume conventional ring attractors to operate close to this rigid regime. For the Bayesian ring attractor, we find <italic>κ</italic><sup>*</sup> = 1 and <inline-formula><mml:math id="M63"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Further, in our simulations in <xref ref-type="fig" rid="F4">Fig. 4</xref>, we explored network dynamics with a range of <italic>κ</italic><sup>*</sup> and <italic>β</italic> values by adjusting network parameters accordingly.</p></sec><sec id="S16"><title>Simulation details</title><sec id="S17"><title>Numerical integration</title><p id="P57">Our simulations in <xref ref-type="fig" rid="F3">Figs. 3</xref> and <xref ref-type="fig" rid="F4">4</xref> used artificial data that matched the assumptions underlying our models. In particular, the ‘true’ HD <italic>ϕ</italic><sub><italic>t</italic></sub> followed a diffusion on the circle, <xref ref-type="disp-formula" rid="FD7">Eq. (7)</xref>, and observations were drawn at each point in time from <xref ref-type="disp-formula" rid="FD5">Eqs. (5)</xref> and <xref ref-type="disp-formula" rid="FD6">(6)</xref>. To simulate trajectories and observations, we used the Euler-Maruyama scheme [<xref ref-type="bibr" rid="R33">Kloeden and Platen, 2010</xref>], which supports the numerical integration of stochastic differential equations. Specifically, for a chosen discretization time step Δ<italic>t</italic>, this scheme is equivalent to drawing trajectories and observations from <xref ref-type="disp-formula" rid="FD7">Eqs. (7)</xref>, <xref ref-type="disp-formula" rid="FD5">(5)</xref> and <xref ref-type="disp-formula" rid="FD6">(6)</xref> directly while substituting <italic>dt</italic> → Δ<italic>t</italic>. The same time-discretization scheme was used to numerically integrate the SDEs of the circKF, <xref ref-type="disp-formula" rid="FD8">Eqs (8)</xref> and <xref ref-type="disp-formula" rid="FD9">(9)</xref>, and its quadratic approximation, <xref ref-type="disp-formula" rid="FD11">Eq. (11)</xref>.</p></sec><sec id="S18"><title>Performance measures</title><p id="P58">To measure performance, in <xref ref-type="fig" rid="F3">Figs. 3f</xref> &amp; <xref ref-type="fig" rid="F4">4b/d</xref> we computed the circular average distance [<xref ref-type="bibr" rid="R43">Mardia and Jupp, 2000</xref>] of the estimate <italic>μ</italic><sub><italic>T</italic></sub> from the true HD <italic>ϕ</italic><sub><italic>T</italic></sub> at the end of a simulation of length <italic>T</italic> = 20 from <italic>P</italic> = 5′000 simulated trajectories by <inline-formula><mml:math id="M64"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>P</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>P</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>(</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>ϕ</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>. The absolute value of the imaginary-valued circular average, <inline-formula><mml:math id="M65"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> denotes an empirical accuracy (or ‘inference accuracy’), and thus measures how well the estimate <italic>μ</italic><sub><italic>T</italic></sub> matches the true HD <italic>ϕ</italic><sub><italic>T</italic></sub>. Here, a value of 1 denotes an exact match. The inference accuracy is related to the circular variance via <inline-formula><mml:math id="M66"><mml:mrow><mml:msub><mml:mrow><mml:mtext>Var</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>. In <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref>, we provide histograms with samples <inline-formula><mml:math id="M67"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with different numerical values of <inline-formula><mml:math id="M68"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>, to provide some intuition for the spread of estimates for a given value of the performance measure.</p><p id="P59">We estimated performance through such averages for all HD observation reliabilities <italic>κ</italic><sub><italic>z</italic></sub> in <xref ref-type="fig" rid="F3">Figs. 3f</xref> &amp; <xref ref-type="fig" rid="F4">4b</xref>. For the inset of <xref ref-type="fig" rid="F4">Fig. 4b</xref>, and for <xref ref-type="fig" rid="F4">Fig. 4d</xref>, we additionally performed a grid search over the fixed-point <italic>κ</italic><sup>*</sup> (inset of <xref ref-type="fig" rid="F4">Fig. 4b</xref>), or both the fixed-point <italic>κ</italic><sup>*</sup> and of the decay speed <italic>β</italic> (<xref ref-type="fig" rid="F4">Fig. 4d</xref>). For each setting of <italic>κ</italic><sup>*</sup> and <italic>β</italic> we assessed the performance by computing an average over this performance for a range of observation reliabilities <italic>κ</italic><sub><italic>z</italic></sub>, weighted by how likely each observation reliability is a-priori assumed to be. The latter was specified by a log-normal prior, <inline-formula><mml:math id="M69"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>Lognormal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, favouring intermediate reliabilitiy levels. We chose <inline-formula><mml:math id="M70"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M71"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for the prior parameters, but our results did not strongly depend on this parameter choice. The performance loss shown in <xref ref-type="fig" rid="F4">Fig. 4d</xref> also relied on such a weighted average across <italic>κ</italic><sub>z</sub>’s for a particle filter benchmark (PF, see <xref ref-type="supplementary-material" rid="SD1">SI</xref> for details). The loss itself was then defined as <inline-formula><mml:math id="M72"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Performance</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Performance PF</mml:mtext></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p></sec><sec id="S19"><title>Update weights for HD observations</title><p id="P60">In <xref ref-type="fig" rid="F4">Fig. 4c</xref>, we computed the weight with which a single HD observation with <inline-formula><mml:math id="M73"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>90</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> changes the HD estimate. We defined this weight as the change in HD estimate, normalized by the value of the maximum possible change, <inline-formula><mml:math id="M74"><mml:mrow><mml:mi>w</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mi>π</mml:mi></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>π</mml:mi></mml:mfrac><mml:msup><mml:mrow><mml:mi>tan</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mspace width="0.2em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="M75"><mml:mrow><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mspace width="0.2em"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> denotes a function that ensures a linear scaling of the Fisher information with sampling time step (see ref [<xref ref-type="bibr" rid="R39">Kutschireiter et al., 2022</xref>], Theorem 2, for details about this function). Thus, by design of the observation model, the Fisher information of a single observation with reliability <italic>κ</italic><sub><italic>z</italic></sub> during a time interval Δ<italic>t</italic> is given by <inline-formula><mml:math id="M76"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>. We plot the weight as a function of the Fisher information of a single HD observation (how reliable is the observation?) and the Fisher information of the current HD estimate (how certain is the current estimate?), which is given by <disp-formula id="FD16"><label>(16)</label><mml:math id="M77"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mspace width="0.2em"/><mml:mi>𝔼</mml:mi><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:mfrac><mml:mtext>log</mml:mtext><mml:mi>𝒱</mml:mi><mml:mi>ℳ</mml:mi><mml:mspace width="0.2em"/><mml:mo>(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S20"><title>Details on numerical simulations</title><p id="P61">In our network simulations, we set the leak time constant <italic>τ</italic> to an arbitrary, but non-zero, value. Effectively, this resulted in a cosine-shaped activity profile. Note that by setting higher-order recurrent connectivities accordingly, other profile shapes could be realized, without affecting the validity of our analysis above. From the neural activity vector r<sub><italic>t</italic></sub>, we retrieved the natural parameters <italic>θ</italic><sub><italic>t</italic></sub> with a decoder matrix <inline-formula><mml:math id="M78"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>, such that <inline-formula><mml:math id="M79"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mtext>r</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and subsequently computed the position of the bump by <inline-formula><mml:math id="M80"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>arctan</mml:mi><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, and the encoded certainty (length of the population vector) by <inline-formula><mml:math id="M81"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p><p id="P62">In all our simulations, times are measured in units of inverse diffusion time constant <italic>κ</italic><sub><italic>ϕ</italic></sub>, where we set <italic>κ</italic><sub><italic>ϕ</italic></sub> = 1<italic>s</italic> for convenience. We used the following simulation parameters. For <xref ref-type="fig" rid="F3">Fig. 3e</xref>, we used <italic>κ</italic><sub><italic>v</italic></sub> = 2, <italic>κ</italic><sub><italic>z</italic></sub> = 10 (during ‘Visual cue’ period), <italic>κ</italic><sub><italic>z</italic></sub> = 0 (during ‘Darkness’ period). For <xref ref-type="fig" rid="F3">Figs. 3f</xref> &amp; <xref ref-type="fig" rid="F4">4b/d</xref> we used <italic>κ</italic><sub><italic>v</italic></sub> = 1, <italic>T</italic> = 20, and averaged results over <italic>P</italic> = 5000 simulation runs. For <xref ref-type="fig" rid="F4">Fig. 4e</xref> we used <italic>κ</italic><sub><italic>v</italic></sub> = 1, <italic>κ</italic><sub><italic>z</italic></sub> = 1, <italic>T</italic> = 10. We used Δ<italic>t</italic> = 0.01 for all simulations.</p><p id="P63">Trajectory simulations and general analyses were performed on a MacBook Pro (Mid 2019) running 2.3 GHz 8-core Intel Core i9. Parameter scans were run on the Harvard Medical School O<sub>2</sub> HPC cluster. For all our simulations, we used Python 3.9.1 with NumPy 1.19.2. Jupyter notebooks, Python scripts, and data to reproduce the figures will be made available upon acceptance of the manuscript.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>SI</label><media xlink:href="EMS144473-supplement-SI.pdf" mimetype="application" mime-subtype="pdf" id="d22aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S21"><title>Acknowledgements</title><p>We thank Rachel Wilson for fruitful discussions and input throughout the whole research phase, and for her comments on the manuscript. We would like to thank Habiba Noamany for assisting us in navigating the neuprint database, and for informed comments on the manuscript. We would further like to thank Johannes Bill &amp; Albert Chen for discussions and feedback on the manuscript, Philipp Reinhard for going on a typo hunt in the SI, and the entire Drugowitsch lab for valuable and insightful discussions.</p><p>The work was funded by the NIH (R34NS123819; J.D.), the James S. McDonnell Foundation (Scholar Award #220020462; J.D.), the Swiss National Science Foundation (grant numbers P2ZHP2 184213 and P400PB 199242; A.K.), and a Grant in the Basic and Social Sciences by the Harvard Medical School Dean’s Initiative award program (J.D.).</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P64"><bold>Author contributions</bold></p><p id="P65">Conceptualization, A.K., M.A.B., J.D.; Methodology, A.K., J.D.; Software, A.K.; Formal analysis, A.K., J.D.; Investigation, A.K, M.A.B., J.D.; Resources, J.D; Writing - Original Draft: A.K., J.D.; Writing - Review &amp; Editing: A.K., M.A.B., J.D.; Visualization, A.K.; Supervision, J.D.; Funding Acquisition, A.K., J.D.</p></fn><fn id="FN2" fn-type="conflict"><p id="P66"><bold>Declaration of interests</bold></p><p id="P67">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ajabi</surname><given-names>Z</given-names></name><name><surname>Keinath</surname><given-names>AT</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name><name><surname>Brandon</surname><given-names>MP</given-names></name></person-group><article-title>Population Dynamics of the Thalamic Head Direction System during Drift and Reorientation</article-title><source>bioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.08.30.458266</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><article-title>Dynamics of Pattern Formation in Lateral-Inhibition Type Neural Fields</article-title><source>Biological Cybernetics</source><year>1977</year><volume>27</volume><issue>2</issue><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1007/BF00337259</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baumann</surname><given-names>O</given-names></name><name><surname>Mattingley</surname><given-names>JB</given-names></name></person-group><article-title>Medial Parietal Cortex Encodes Perceived Heading Direction in Humans</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><issue>39</issue><fpage>12897</fpage><lpage>901</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.30770-10.2010</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Marginalization in Neural Circuits with Divisive Normalization</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>43</issue><fpage>15310</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1706-11.2011</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>R</given-names></name><name><surname>Bar-Or</surname><given-names>RL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Theory of Orientation Tuning in Visual Cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>1995</year><volume>92</volume><issue>9</issue><fpage>3844</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Bergen</surname><given-names>RS</given-names></name><name><surname>Ji Ma</surname><given-names>W</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name><name><surname>Jehee</surname><given-names>JFM</given-names></name></person-group><article-title>Sensory Uncertainty Decoded from Visual Cortex Predicts Behavior</article-title><source>Nature Neuroscience</source><year>2015</year><volume>18</volume><issue>12</issue><fpage>1728</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1038/nn.4150</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brigo</surname><given-names>D</given-names></name><name><surname>Hanzon</surname><given-names>B</given-names></name><name><surname>Le Gland</surname><given-names>F</given-names></name></person-group><article-title>Approximate Nonlinear Filtering by Projection on Exponential Manifolds of Densities</article-title><source>Bernoulli</source><year>1999</year><volume>5</volume><issue>3</issue><fpage>495</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.2307/3318714</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>MG</given-names></name><name><surname>Attinger</surname><given-names>A</given-names></name><name><surname>Ocko</surname><given-names>SA</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><article-title>Distance-Tuned Neurons Drive Specialized Path Integration Calculations in Medial Entorhinal Cortex</article-title><source>Cell Reports</source><year>2021</year><volume>36</volume><issue>10</issue><elocation-id>109669</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109669</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carroll</surname><given-names>S</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name></person-group><article-title>Encoding Certainty in Bump Attractors</article-title><source>Journal of Computational Neuroscience</source><year>2014</year><volume>37</volume><issue>1</issue><fpage>29</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1007/s10827-013-0486-0</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>K</given-names></name><name><surname>Shettleworth</surname><given-names>SJ</given-names></name><name><surname>Huttenlocher</surname><given-names>J</given-names></name><name><surname>Rieser</surname><given-names>JJ</given-names></name></person-group><article-title>Bayesian Integration of Spatial Information</article-title><source>Psychological Bulletin</source><year>2007</year></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname><given-names>A</given-names></name></person-group><article-title>Synaptic Mechanisms and Network Dynamics Underlying Spatial Working Memory in a Cortical Network Model</article-title><source>Cerebral Cortex</source><year>2000</year><volume>10</volume><issue>9</issue><fpage>910</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.9.910</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname><given-names>A</given-names></name></person-group><article-title>Computational and in Vitro Studies of Persistent Activity: Edging towards Cellular and Synaptic Mechanisms of Working Memory</article-title><source>Neuroscience</source><year>2006</year><volume>139</volume><issue>1</issue><fpage>135</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2005.06.011</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname><given-names>Alex J</given-names></name><name><surname>Sabo</surname><given-names>Chelsea</given-names></name><name><surname>Vasilaki</surname><given-names>Eleni</given-names></name><name><surname>Barron</surname><given-names>Andrew B</given-names></name><name><surname>Marshall</surname><given-names>James AR</given-names></name></person-group><article-title>A Computational Model of the Integration of Landmarks and Motion in the Insect Central Complex</article-title><person-group person-group-type="editor"><name><surname>Graham</surname><given-names>Paul</given-names></name></person-group><source>PLOS ONE</source><year>2017</year><volume>12</volume><issue>2</issue><elocation-id>e0172325</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0172325</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>Guillaume P</given-names></name><name><surname>Coen-Cagli</surname><given-names>Ruben</given-names></name><name><surname>Pouget</surname><given-names>Alexandre</given-names></name></person-group><article-title>Investigating the Representation of Uncertainty in Neuronal Circuits</article-title><source>PLOS Computational Biology</source><year>2021</year><volume>17</volume><issue>2</issue><elocation-id>e1008138</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008138</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ermentrout</surname><given-names>Bard</given-names></name></person-group><article-title>Neural Networks as Spatio-Temporal Pattern-Forming Systems</article-title><source>Reports on Progress in Physics</source><year>1998</year><volume>61</volume><issue>4</issue><fpage>353</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1088/0034-4885/61/4/002</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>Marc O</given-names></name><name><surname>Banks</surname><given-names>Martin S</given-names></name></person-group><article-title>Humans Integrate Visual and Haptic Information in a Statistically Optimal Fashion</article-title><source>Nature</source><year>2002</year><volume>415</volume><issue>6870</issue><fpage>429</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esnaola-Acebes</surname><given-names>Jose M</given-names></name><name><surname>Roxin</surname><given-names>Alex</given-names></name><name><surname>Wimmer</surname><given-names>Klaus</given-names></name></person-group><article-title>Bump Attractor Dynamics Underlying Stimulus Integration in Perceptual Estimation Tasks</article-title><source>Neuroscience</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.03.15.434192</pub-id><comment>Preprint</comment></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fetsch</surname><given-names>Christopher R</given-names></name><name><surname>Turner</surname><given-names>Amanda H</given-names></name><name><surname>DeAngelis</surname><given-names>Gregory C</given-names></name><name><surname>Angelaki</surname><given-names>Dora E</given-names></name></person-group><article-title>Dynamic Reweighting of Visual and Vestibular Cues during Self-Motion Perception</article-title><source>Journal of Neuroscience</source><year>2009</year><volume>29</volume><issue>49</issue><fpage>15601</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2574-09.2009</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>Arseny</given-names></name><name><surname>Derdikman</surname><given-names>Dori</given-names></name><name><surname>Rubin</surname><given-names>Alon</given-names></name><name><surname>Foerster</surname><given-names>Jakob N</given-names></name><name><surname>Las</surname><given-names>Liora</given-names></name><name><surname>Ulanovsky</surname><given-names>Nachum</given-names></name></person-group><article-title>Three-Dimensional Head-Direction Coding in the Bat Brain</article-title><source>Nature</source><year>2015</year><volume>517</volume><issue>7533</issue><fpage>159</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1038/nature14031</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>Yvette E</given-names></name><name><surname>Lu</surname><given-names>Jenny</given-names></name><name><surname>D’Alessandro</surname><given-names>Isabel</given-names></name><name><surname>Wilson</surname><given-names>Rachel I</given-names></name></person-group><article-title>Sensorimotor Experience Remaps Visual Input to a Heading-Direction Network</article-title><source>Nature</source><year>2019</year><pub-id pub-id-type="doi">10.1038/s41586-019-1772-4</pub-id><comment>no. December 2018 (November)</comment></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgopoulos</surname><given-names>Ap</given-names></name><name><surname>Kettner</surname><given-names>Re</given-names></name><name><surname>Schwartz</surname><given-names>Ab</given-names></name></person-group><article-title>Primate Motor Cortex and Free Arm Movements to Visual Targets in Three- Dimensional Space. II. Coding of the Direction of Movement by a Neuronal Population</article-title><source>The Journal of Neuroscience</source><year>1988</year><volume>8</volume><issue>8</issue><fpage>2928</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.08-08-02928.1988</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>J</given-names></name><name><surname>Adachi</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>KK</given-names></name><name><surname>Hirokawa</surname><given-names>JD</given-names></name><name><surname>Magani</surname><given-names>PS</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><article-title>A Neural Circuit Architecture for Angular Integration in Drosophila</article-title><source>Nature</source><year>2017</year><volume>546</volume><issue>7656</issue><fpage>101</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/nature22343</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><article-title>Modeling Feature Selectivity in Local Cortical Circuits</article-title><source>Methods in Neuronal Modeling: From Ions to Networks</source><year>1998</year><volume>69</volume></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heinze</surname><given-names>S</given-names></name><name><surname>Narendra</surname><given-names>A</given-names></name><name><surname>Cheung</surname><given-names>A</given-names></name></person-group><article-title>Principles of Insect Path Integration</article-title><source>Current Biology : CB</source><year>2018</year><volume>28</volume><issue>17</issue><fpage>R1043</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.058</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Neural Networks and Physical Systems with Emergent Collective Computational Abilities</article-title><source>Proceedings of the National Academy of Sciences</source><year>1982</year><volume>79</volume><issue>8</issue><fpage>2554</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hulse</surname><given-names>BK</given-names></name><name><surname>Haberkern</surname><given-names>H</given-names></name><name><surname>Franconville</surname><given-names>R</given-names></name><name><surname>Turner-Evans</surname><given-names>DB</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Noorman</surname><given-names>M</given-names></name><etal/></person-group><article-title>A Connectome of the Drosophila Central Complex Reveals Network Motifs Suitable for Flexible Navigation and Context-Dependent Action Selection</article-title><source>ELife</source><year>2021</year><volume>10</volume><issue>October</issue><elocation-id>e66039</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66039</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Seeland</surname><given-names>K</given-names></name><name><surname>David Redish</surname><given-names>A</given-names></name></person-group><article-title>Reconstruction of the Postsubiculum Head Direction Signal from Neural Ensembles</article-title><source>Hippocampus</source><year>2005</year><volume>15</volume><issue>1</issue><fpage>86</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1002/hipo.20033</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><article-title>A New Approach to Linear Filtering and Prediction Problems</article-title><source>Transactions of the ASME Journal of Basic Engineering</source><year>1960</year><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id><comment>(Series D)</comment></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name><name><surname>Bucy</surname><given-names>RS</given-names></name></person-group><article-title>New Results in Linear Filtering and Prediction Theory</article-title><source>Journal of Basic Engineering</source><year>1961</year><volume>83</volume><issue>1</issue><fpage>95</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1115/1.3658902</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keinath</surname><given-names>AT</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><article-title>Environmental Deformations Dynamically Shift the Grid Cell Spatial Metric</article-title><person-group person-group-type="editor"><name><surname>Colgin</surname><given-names>Laura</given-names></name><name><surname>Frank</surname><given-names>Michael J</given-names></name></person-group><source>ELife</source><year>2018</year><volume>7</volume><issue>October</issue><elocation-id>e38169</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.38169</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SS</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><article-title>Generation of Stable Heading Representations in Diverse Visual Scenes</article-title><source>Nature</source><year>2019</year><issue>December 2018</issue><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1767-1</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SS</given-names></name><name><surname>Rouault</surname><given-names>H</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><article-title>Ring Attractor Dynamics in the Drosophila Central Brain</article-title><source>Science</source><year>2017</year><volume>356</volume><issue>6340</issue><fpage>849</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1126/science.aal4835</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kloeden</surname><given-names>PE</given-names></name><name><surname>Platen</surname><given-names>E</given-names></name></person-group><chapter-title>Numerical Solution of Stochastic Differential Equations</chapter-title><source>Corr. 3. print. Applications of Mathematics 23</source><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><year>2010</year></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Kudrimoti</surname><given-names>HS</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Interactions Between Idiothetic Cues and External Landmarks in the Control of Place Cells and Head Direction Cells</article-title><source>Journal of Neurophysiology</source><year>1998</year><volume>80</volume><issue>1</issue><fpage>425</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.1.425</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><article-title>Attractor Dynamics of Spatially Correlated Neural Activity in the Limbic System</article-title><source>Annual Review of Neuroscience</source><year>2012</year><volume>35</volume><issue>1</issue><fpage>267</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150351</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knight</surname><given-names>R</given-names></name><name><surname>Piette</surname><given-names>CE</given-names></name><name><surname>Page</surname><given-names>H</given-names></name><name><surname>Walters</surname><given-names>D</given-names></name><name><surname>Marozzi</surname><given-names>E</given-names></name><name><surname>Nardini</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>S</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>Weighted Cue Integration in the Rodent Head Direction System</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2014</year><volume>369</volume><issue>1635</issue><elocation-id>20120512</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2012.0512</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>DC</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>The Bayesian Brain: The Role of Uncertainty in Neural Coding and Computation</article-title><source>Trends in Neurosciences</source><year>2004</year><volume>27</volume><issue>12</issue><fpage>712</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2004.10.007</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurz</surname><given-names>G</given-names></name><name><surname>Gilitschenski</surname><given-names>I</given-names></name><name><surname>Hanebeck</surname><given-names>UD</given-names></name></person-group><article-title>Recursive Bayesian Filtering in Circular State Spaces</article-title><source>IEEE Aerospace and Electronic Systems Magazine</source><year>2016</year><volume>31</volume><issue>3</issue><fpage>70</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1109/MAES.2016.150083</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutschireiter</surname><given-names>A</given-names></name><name><surname>Rast</surname><given-names>L</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name></person-group><article-title>Projection Filtering with Observed State Increments with Applications in Continuous-Time Circular Filtering</article-title><source>IEEE Transactions on Signal Processing</source><year>2022</year><fpage>1</fpage><pub-id pub-id-type="doi">10.1109/TSP.2022.3143471</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HH</given-names></name><name><surname>Sprague</surname><given-names>TC</given-names></name><name><surname>Yoo</surname><given-names>AH</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name></person-group><article-title>Joint Representation of Working Memory and Uncertainty in Human Cortex</article-title><source>Neuron</source><year>2021</year><volume>109</volume><issue>22</issue><fpage>3699</fpage><lpage>3712</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.08.022</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lyu</surname><given-names>Cheng</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Maimon</surname><given-names>Gaby</given-names></name></person-group><article-title>Building an Allocentric Travelling Direction Signal via Vector Computation</article-title><source>Nature</source><year>2022</year><volume>601</volume><issue>7891</issue><fpage>92</fpage><lpage>97</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma Wei</surname><given-names>Ji</given-names></name><name><surname>Beck Jeffrey</surname><given-names>M</given-names></name><name><surname>Latham Peter</surname><given-names>E</given-names></name><name><surname>Pouget</surname><given-names>Alexandre</given-names></name></person-group><article-title>Bayesian Inference with Probabilistic Population Codes</article-title><source>Nature Neuroscience</source><year>2006</year><volume>9</volume><issue>11</issue><fpage>1432</fpage><lpage>38</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mardia Kanti</surname><given-names>V</given-names></name><name><surname>Jupp Peter</surname><given-names>E</given-names></name></person-group><source>Directional Statistics</source><publisher-name>John Wiley &amp; Sons</publisher-name><year>2000</year></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkle</surname><given-names>Tobias</given-names></name><name><surname>Knaden</surname><given-names>Markus</given-names></name><name><surname>Wehner</surname><given-names>Rüdiger</given-names></name></person-group><article-title>Uncertainty about Nest Position Influences Systematic Search Strategies in Desert Ants</article-title><source>Journal of Experimental Biology</source><year>2006</year><volume>209</volume><issue>18</issue><fpage>3545</fpage><lpage>49</lpage></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkle</surname><given-names>Tobias</given-names></name><name><surname>Wehner</surname><given-names>Rüdiger</given-names></name></person-group><article-title>Desert Ants Use Foraging Distance to Adapt the Nest Search to the Uncertainty of the Path Integrator</article-title><source>Behavioral Ecology</source><year>2010</year><volume>21</volume><issue>2</issue><fpage>349</fpage><lpage>55</lpage></element-citation></ref><ref id="R46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Milford</surname><given-names>MJ</given-names></name><name><surname>Wyeth</surname><given-names>GF</given-names></name><name><surname>Prasser</surname><given-names>D</given-names></name></person-group><source>RatSLAM: A Hippocampal Model for Simultaneous Localization and Mapping</source><conf-name>IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA ’ 04. 2004.</conf-name><year>2004</year><volume>1</volume><fpage>403</fpage><lpage>408</lpage></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulas</surname><given-names>Marcello</given-names></name><name><surname>Waniek</surname><given-names>Nicolai</given-names></name><name><surname>Conradt</surname><given-names>Jörg</given-names></name></person-group><article-title>Hebbian Plasticity Realigns Grid Cell Activity with External Sensory Cues in Continuous Attractor Models</article-title><source>Frontiers in Computational Neuroscience</source><year>2016</year><month>February</month><volume>10</volume></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray Richard</surname><given-names>F</given-names></name><name><surname>Morgenstern</surname><given-names>Yaniv</given-names></name></person-group><article-title>Cue Combination on the Circle and the Sphere</article-title><source>Journal of Vision</source><year>2010</year><volume>10</volume><issue>11</issue><fpage>15</fpage></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ocko</surname><given-names>Samuel A</given-names></name><name><surname>Hardcastle</surname><given-names>Kiah</given-names></name><name><surname>Giocomo</surname><given-names>Lisa M</given-names></name><name><surname>Ganguli</surname><given-names>Surya</given-names></name></person-group><article-title>Emergent Elasticity in the Neural Code for Space</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><volume>115</volume><issue>50</issue></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page Hector</surname><given-names>JI</given-names></name><name><surname>Jeffery Kate</surname><given-names>J</given-names></name></person-group><article-title>Landmark-Based Updating of the Head Direction System by Retrosplenial Cortex: A Computational Model</article-title><source>Frontiers in Cellular Neuroscience</source><year>2018</year><month>July</month><volume>12</volume><elocation-id>191</elocation-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>Hector JI</given-names></name><name><surname>Walters</surname><given-names>Daniel M</given-names></name><name><surname>Knight</surname><given-names>Rebecca</given-names></name><name><surname>Piette</surname><given-names>Caitlin E</given-names></name><name><surname>Jeffery</surname><given-names>Kathryn J</given-names></name><name><surname>Stringer</surname><given-names>Simon M</given-names></name></person-group><article-title>A Theoretical Account of Cue Averaging in the Rodent Head Direction System</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2014</year><volume>369</volume><issue>1635</issue><elocation-id>20130283</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0283</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>Adrien</given-names></name><name><surname>Lacroix</surname><given-names>Marie M</given-names></name><name><surname>Petersen</surname><given-names>Peter C</given-names></name><name><surname>Buzsáki</surname><given-names>György</given-names></name></person-group><article-title>Internally Organized Mechanisms of the Head Direction Sense</article-title><source>Nature Neuroscience</source><year>2015</year><volume>18</volume><issue>4</issue><fpage>569</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1038/nn.3968</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piet</surname><given-names>Alex T</given-names></name><name><surname>El Hady</surname><given-names>Ahmed</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name><name><surname>El Hady</surname><given-names>Ahmed</given-names></name><name><surname>Brody</surname><given-names>Carlos D</given-names></name></person-group><article-title>Rats Adopt the Optimal Timescale for Evidence Integration in a Dynamic Environment</article-title><source>Nature Communications</source><year>2018</year><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-06561-y</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rademaker</surname><given-names>Rosanne L</given-names></name><name><surname>Tredway</surname><given-names>Caroline H</given-names></name><name><surname>Tong</surname><given-names>Frank</given-names></name></person-group><article-title>Introspective Judgments Predict the Precision and Likelihood of Successful Maintenance of Visual Working Memory</article-title><source>Journal of Vision</source><year>2012</year><volume>12</volume><issue>13</issue><elocation-id>21</elocation-id><pub-id pub-id-type="doi">10.1167/12.13.21</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>A David</given-names></name><name><surname>Elga</surname><given-names>Adam N</given-names></name><name><surname>Touretzky</surname><given-names>David S</given-names></name></person-group><article-title>A Coupled Attractor Model of the Rodent Head Direction System</article-title><source>Network: Computation in Neural Systems</source><year>1996</year><volume>7</volume><issue>4</issue><fpage>671</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_7_4_004</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robertson</surname><given-names>Robert G</given-names></name><name><surname>Rolls</surname><given-names>Edmund T</given-names></name><name><surname>Georges-François</surname><given-names>Pierre</given-names></name><name><surname>Panzeri</surname><given-names>Stefano</given-names></name></person-group><article-title>Head Direction Cells in the Primate Pre-Subiculum</article-title><source>Hippocampus</source><year>1999</year><volume>9</volume><issue>3</issue><fpage>206</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1999)9:3&lt;206::AID-HIPO2&gt;3.0.CO;2-H</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheffer</surname><given-names>Louis K</given-names></name><name><surname>Xu</surname><given-names>C Shan</given-names></name><name><surname>Januszewski</surname><given-names>Michal</given-names></name><name><surname>Lu</surname><given-names>Zhiyuan</given-names></name><name><surname>Takemura</surname><given-names>Shin-ya</given-names></name><name><surname>Hayworth</surname><given-names>Kenneth J</given-names></name><name><surname>Huang</surname><given-names>Gary B</given-names></name><etal/></person-group><article-title>A Connectome and Analysis of the Adult Drosophila Central Brain</article-title><person-group person-group-type="editor"><name><surname>Marder</surname><given-names>Eve</given-names></name><name><surname>Eise</surname><given-names>Michael B</given-names></name><name><surname>Pipkin</surname><given-names>Jason</given-names></name><name><surname>Doe</surname><given-names>Chris Q</given-names></name></person-group><source>ELife</source><year>2020</year><month>September</month><volume>9</volume><elocation-id>e57443</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57443</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname><given-names>Johannes D</given-names></name><name><surname>Jayaraman</surname><given-names>Vivek</given-names></name></person-group><article-title>Neural Dynamics for Landmark Orientation and Angular Path Integration</article-title><source>Nature</source><year>2015</year><volume>521</volume><issue>7551</issue><fpage>186</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1038/nature14446</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinder</surname><given-names>Michael E</given-names></name><name><surname>Taube</surname><given-names>Jeffrey S</given-names></name></person-group><article-title>Self-Motion Improves Head Direction Cell Tuning</article-title><source>Journal of Neurophysiology</source><year>2014</year><volume>111</volume><issue>12</issue><fpage>2479</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1152/jn.00512.2013</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skaggs</surname><given-names>William</given-names></name><name><surname>Knierim</surname><given-names>James</given-names></name><name><surname>Kudrimoti</surname><given-names>Hemant</given-names></name><name><surname>McNaughton</surname><given-names>Bruce</given-names></name></person-group><chapter-title>A Model of the Neural Basis of the Rats Sense of Direction</chapter-title><person-group person-group-type="editor"><name><surname>Tesauro</surname><given-names>G</given-names></name><name><surname>Touretzky</surname><given-names>D</given-names></name><name><surname>Leen</surname><given-names>T</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>MIT Press</publisher-name><year>1994</year><volume>7</volume><comment><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf">https://proceedings.neurips.cc/paper/1994/file/024d7f84fff11dd7e8d9c510137a2381-Paper.pdf</ext-link></comment></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name><name><surname>Yue</surname><given-names>S</given-names></name></person-group><source>An Analysis of a Ring Attractor Model for Cue Integration</source><series>Lecture Notes in Computer Science(Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 10928 LNAI</series><year>2018</year><fpage>459</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-95972_49</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Yue</surname><given-names>S</given-names></name><name><surname>Mangan</surname><given-names>M</given-names></name></person-group><article-title>A Decentralised Neural Model Explaining Optimal Integration of Navigational Strategies in Insects</article-title><person-group person-group-type="editor"><name><surname>Ramaswami</surname><given-names>M</given-names></name><name><surname>Eisen</surname><given-names>MB</given-names></name><name><surname>Heinze</surname><given-names>S</given-names></name></person-group><source>ELife</source><year>2020</year><volume>9</volume><issue>June</issue><elocation-id>e54026</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54026</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner-Evans</surname><given-names>DB</given-names></name><name><surname>Jensen</surname><given-names>KT</given-names></name><name><surname>Ali</surname><given-names>S</given-names></name><name><surname>Paterson</surname><given-names>T</given-names></name><name><surname>Sheridan</surname><given-names>A</given-names></name><name><surname>Ray</surname><given-names>RP</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><etal/></person-group><article-title>The Neuroanatomical Ultrastructure and Function of a Biological Ring Attractor</article-title><source>Neuron</source><year>2020</year><month>September</month><pub-id pub-id-type="doi">10.1016/j.neuron.2020.08.006</pub-id><comment>S0896627320306139</comment></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner-Evans</surname><given-names>D</given-names></name><name><surname>Wegener</surname><given-names>S</given-names></name><name><surname>Rouault</surname><given-names>H</given-names></name><name><surname>Franconville</surname><given-names>R</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Seelig</surname><given-names>JD</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><article-title>Angular Velocity Integration in a Fly Heading Circuit</article-title><source>ELife</source><year>2017</year><month>May</month><volume>6</volume><elocation-id>e23496</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23496</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><article-title>Synaptic Reverberation Underlying Mnemonic Persistent Activity</article-title><source>Trends in Neurosciences</source><year>2001</year><volume>24</volume><issue>8</issue><fpage>455</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01868-3</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>R</given-names></name><name><surname>Finkel</surname><given-names>L</given-names></name></person-group><article-title>A Neural Implementation of the Kalman Filter</article-title><source>Advances in Neural Information Processing Systems</source><year>2009</year><volume>22</volume><fpage>9</fpage></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><article-title>Double-Ring Network Model of the Head-Direction System</article-title><source>Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics</source><year>2002</year><volume>66</volume><issue>4</issue><fpage>9</fpage><pub-id pub-id-type="doi">10.1103/PhysRevE.66.041902</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><article-title>Representation of Spatial Orientation by the Intrinsic Dynamics of the Head-Direction Cell Ensemble: A Theory</article-title><source>The Journal of Neuroscience</source><year>1996</year><volume>16</volume><issue>6</issue><fpage>2112</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-06-02112.1996</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zugaro</surname><given-names>MB</given-names></name><name><surname>Tabuchi</surname><given-names>E</given-names></name><name><surname>Fouquier</surname><given-names>C</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name><name><surname>Wiener</surname><given-names>SI</given-names></name></person-group><article-title>Active Locomotion Increases Peak Firing Rates of Anterodorsal Thalamic Head Direction Cells</article-title><source>Journal of Neurophysiology</source><year>2001</year><volume>86</volume><issue>2</issue><fpage>692</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.2.692</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance Statement</title></caption><p>Data from human subjects as well as animals shows that working memories are associated with a sense of uncertainty. Indeed, a sense of uncertainty is what allows an observer to properly weigh new evidence against their current memory. However, we do not understand how the brain tracks uncertainty. Here we describe a simple and biologically plausible network model that can track the uncertainty associated with a working memory. The representation of uncertainty in this model improves the accuracy of its working memory, as compared to conventional models, because it assigns the proper weight to new conflicting evidence. Our model provides a new interpretation for observed fluctuations in brain activity, and it makes testable new predictions.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Tracking HD with the circular Kalman filter.</title><p><bold>a</bold>) Angular velocity observations provide noisy information about the true angular velocity <italic>ϕ</italic>˙<sub><italic>t</italic></sub>, while HD observations provide noisy information about the true HD <italic>ϕ</italic><sub><italic>t</italic></sub>.</p><p><bold>b</bold>) At every point in time, the posterior belief <italic>p</italic>(<italic>ϕ</italic><sub><italic>t</italic></sub>) is approximated by a von Mises distribution.</p><p>It is fully characterized by its mean parameter <italic>μ</italic><sub><italic>t</italic></sub>, which determines the position of the peak, and its certainty parameter <italic>κ</italic><sub><italic>t</italic></sub>. Interpreted as the polar coordinates in the 2D plane, these parameters provide a convenient vector representation of the posterior belief (inset).</p><p><bold>c</bold>) A angular velocity observation <italic>v</italic><sub><italic>t</italic></sub> is a vector tangent to the current HD estimate. Angular velocity observations continually rotate the current HD estimate; meanwhile, noise accumulation progressively decreases certainty.</p><p><bold>d</bold>) Each HD observation <italic>z</italic><sub><italic>t</italic></sub> is a vector whose length corresponds to the reliability associated with that observation. Adding this vector to the current HD estimate produces an updated HD estimate. If the HD observation is compatible with the current HD estimate, then the updated HD estimate has increased certainty.</p><p><bold>e</bold>) HD observations in conflict with the current belief (e.g., opposite direction of current estimate) decrease the certainty associated with the HD estimate.</p><p><bold>f</bold>) Multiple HD cues can be integrated simultaneously via vector addition.</p></caption><graphic xlink:href="EMS144473-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Encoding the HD belief in neural population activity</title><p><bold>a)</bold> Neural population activity profile (e.g., average firing rate) encoding the HD estimate <italic>μ</italic> = <italic>π</italic>/4 with different values of certainty <italic>κ</italic>. Neurons are sorted by preferred head directions <italic>ϕ</italic><sub><italic>i</italic></sub>.</p><p><bold>b)</bold> Vector representation of estimate <italic>μ</italic> = <italic>π</italic>/4 for different values of certainty <italic>κ</italic>. This vector representation can be obtained by linearly decoding the population activity in a) (“phasor representation”). It also corresponds to the vector representation of the von Mises distribution in c). Thus, it links neural activities to the probability distributions they encode.</p><p><bold>c)</bold> Von Mises probability densities for different values of certainty <italic>κ</italic> and fixed HD estimate <italic>μ</italic> = <italic>π</italic>/4. Note that, unlike the population activity in a), the density sharpens around the mean with increasing certainty.</p></caption><graphic xlink:href="EMS144473-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>A recurrent neural network implementation of the circular Kalman filter.</title><p><bold>a)-c)</bold>: Network motifs sufficient to implement the circular Kalman filter.</p><p><bold>a)</bold> A cosine-shaped input to the network provides HD observation input. The strengths of this input is modulated by observation reliability <italic>κ</italic><sub><italic>z</italic></sub>.</p><p><bold>b)</bold> Rotations of the HD estimate are mediated by symmetric recurrent connectivities, whose strength is modulated by angular velocity observations.</p><p><bold>c)</bold> Decay in amplitude, which implements decreasing certainty in HD estimate, arises from leak and global inhibition.</p><p><bold>d)</bold> Rotation-symmetric recurrent connectivities (here: neurons are sorted according to their preferred HD) can be decomposed into constant, symmetric, asymmetric, and higher-order frequency components (here denoted by dots). Red and blue denote excitatory and inhibitory components, respectively.</p><p><bold>e)</bold> The dynamics of the Bayesian ring attractor implement the dynamics of the ideal observer’s belief, as shown in a simulation of a network with 80 neurons. Here, we assume that vision provides the network with HD observations. When a ‘visual cue’ was present, both HD observations and angular velocity observations were available. During ‘darkness’, only angular velocity observations were available.</p><p><bold>f)</bold> The Bayesian ring attractor network tracks the HD estimate with the same precision (top; higher = lower circular distance to true HD) as the circular Kalman filter (circKF, <xref ref-type="disp-formula" rid="FD1">Eqs. (1)</xref> and <xref ref-type="disp-formula" rid="FD2">(2)</xref>) if HD observations are reliable (large <italic>κ</italic><sub><italic>z</italic></sub>), but with slightly lower precision once they become less reliable (small <italic>κ</italic><sub><italic>z</italic></sub>). This drop co-occurs with an overestimate in the estimate’s confidence <italic>κ</italic><sub><italic>t</italic></sub> (bottom). The shown accuracies and certainties are averages over 5,000 simulation runs (see <xref ref-type="sec" rid="S11">Methods</xref> for simulation details).</p></caption><graphic xlink:href="EMS144473-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Ring attractors with slow dynamics approximate Bayesian inference.</title><p><bold>a)</bold> The ring attractor network in <xref ref-type="disp-formula" rid="FD3">Eq. (3)</xref>) can be characterized by fixed point amplitude <italic>κ</italic> * and decay speed <italic>β</italic> which depend on the network connectivities. Thus, the network can operate in different regimes: a regime, where the bump amplitude is nearly constant (“Conventional attractor”), a regime where amplitude dynamics are tuned to implement a Bayesian ring attractor, or a regime with optimal performance (“Best network”, determined numerically).</p><p><bold>b)</bold> HD estimation performance as measured by inference accuracy (as defined by 1 – <italic>circVar</italic>, see <xref ref-type="sec" rid="S11">Methods</xref>). For the “conventional” attractor, we chose <italic>κ</italic> * to numerically maximize performance averaged across all levels of observation reliability, weighted by a prior <italic>p</italic>(<italic>κ</italic><sub><italic>z</italic></sub>) on this reliability (see <xref ref-type="sec" rid="S11">Methods</xref>).</p><p><bold>c)</bold> The weight with which a single observation contributes to the HD estimate varies with informativeness of both the HD observations and the current HD estimate. We here illustrate this for an HD observation that is orthogonal to the current HD estimate, resulting in the largest possible estimate change (<inline-formula><mml:math id="M82"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:mn>90</mml:mn><mml:mtext>deg</mml:mtext></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref>). The weight itself quantifies how much the observation impacts the HD estimate as a function of how informative this observation is (vertical axis, measured by Fisher information of a 10ms observation) and our certainty in the HD estimate (horizontal axis, also measured by Fisher information) before this observation. A weight of one implies that the observation replaces the previous HD estimate, whereas a weight of zero implies that the observation does not impact this estimate. The update weight of the Bayesian attractor is close to optimal (visually indistinguishable from the circKF; not shown here, but see <xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>), and forms a nonlinear curve through this parameter space. Fisher information per observation is directly related to the observation reliability <italic>κ</italic><sub><italic>z</italic></sub>, and the vertical red bar shows the equivalent range of observation reliabilities, <inline-formula><mml:math id="M83"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, shown in panel b. <bold>d)</bold> Overall inference performance loss (compared to a particle filter; performance measured by avg. inference accuracy, as in b, 0%: same average inference accuracy as particle filter, 100%: average. inference accuracy = 0), averaged across all levels of observation reliability (see <xref ref-type="sec" rid="S11">Methods</xref>) as a function of the bump amplitude parameters <italic>κ</italic> * and <italic>β</italic>. The plot only shows performance loss for above-zero <italic>β</italic>’s and <italic>κ</italic>’s, as <italic>β</italic> → 0 or <italic>κ</italic> → 0 would cause the network’s required connectivity strengths to approach infinity.</p><p><bold>e)</bold> Simulated example trajectories of HD estimate/bump positions of HD estimate/bump positions (top) and certainties/bump amplitudes (bottom). To avoid cluttering, we are not showing the Bayesian ring attractor (visually indistinguishable from circKF and best network).</p></caption><graphic xlink:href="EMS144473-f004"/></fig></floats-group></article>