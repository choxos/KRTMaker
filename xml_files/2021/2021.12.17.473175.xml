<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS157191</article-id><article-id pub-id-type="doi">10.1101/2021.12.17.473175</article-id><article-id pub-id-type="archive">PPR434067</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Functional organization of social perception in the human brain</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Santavirta</surname><given-names>Severi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Karjalainen</surname><given-names>Tomi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nazari-Farsani</surname><given-names>Sanaz</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hudson</surname><given-names>Matthew</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Putkinen</surname><given-names>Vesa</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Seppälä</surname><given-names>Kerttu</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Lihua</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Glerean</surname><given-names>Enrico</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Hirvonen</surname><given-names>Jussi</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Karlsson</surname><given-names>Henry K.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Nummenmaa</surname><given-names>Lauri</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A8">8</xref></contrib></contrib-group><aff id="A1"><label>1</label>Turku PET Centre, University of Turku and Turku University Hospital, Turku, Finland</aff><aff id="A2"><label>2</label>School of Psychology, University of Plymouth, Plymouth, United Kingdom</aff><aff id="A3"><label>3</label>Department of Medical Physics, Turku University Hospital, Turku, Finland</aff><aff id="A4"><label>4</label>Department of Nuclear Medicine, Huashan Hospital, Fudan University, Shanghai, China</aff><aff id="A5"><label>5</label>Department of Neuroscience and Biomedical Engineering, Aalto University School of Science, Espoo, Finland</aff><aff id="A6"><label>6</label>Department of Radiology, University of Turku and Turku University Hospital, Turku, Finland</aff><aff id="A7"><label>7</label>Medical Imaging Center, Department of Radiology, Tampere University and Tampere University Hospital, Tampere, Finland</aff><aff id="A8"><label>8</label>Department of Psychology, University of Turku, Turku, Finland</aff><author-notes><corresp id="CR1"><bold>Address correspondence to</bold> Severi Santavirta Turku PET Centre c/o Turku University Kiinamyllynkatu 4-6, 20520 Turku, Finland <email>svtsan@utu.fi</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>17</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Humans rapidly extract diverse and complex information from ongoing social interactions, but the perceptual and neural organization of the different aspects of social perception remains unresolved. We showed short film clips with rich social content to 97 healthy participants while their haemodynamic brain activity was measured with fMRI. The clips were annotated moment-to-moment for 112 social features. Cluster analysis revealed that 13 dimensions were sufficient for describing the social perceptual space. Regression analysis was used to map regional neural response profiles to different social features. Multivariate pattern analysis was then utilized to establish the spatial specificity of these responses. The results revealed a gradient in the processing of social information in the brain. Posterior temporal and occipital regions were broadly tuned to most social dimensions and the classifier revealed that these responses showed spatial specificity for social dimensions; in contrast Heschl gyri and parietal areas were also broadly associated with different social signals, yet the spatial patterns of responses did not differentiate social dimensions. Frontal and subcortical regions responded only to a limited number of social dimensions and the spatial response patterns did not differentiate social dimension. Altogether these results highlight the distributed nature of social processing in the brain.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Humans live in a complex and ever-changing social world, but how do we make sense of the high-dimensional and time-variable information constantly conveyed by our conspecifics? Prior functional imaging studies localized specific aspects of social perception into different brain regions (<xref ref-type="bibr" rid="R7">Brooks et al., 2020</xref>). Fusiform gyrus (FG) is consistently involved in the perception of faces (<xref ref-type="bibr" rid="R34">Haxby et al., 2000</xref>) and lateral occipitotemporal cortex (LOTC) in the perception of bodies (<xref ref-type="bibr" rid="R20">Downing et al., 2001</xref>). Temporoparietal junction (TPJ) is in turn involved in reflecting the mental states of others (<xref ref-type="bibr" rid="R68">Saxe &amp; Kanwisher, 2003</xref>) as well as processing social context and focusing attention (<xref ref-type="bibr" rid="R9">Carter &amp; Huettel, 2013</xref>). Polysensory areas in the superior temporal sulcus (STS) have been associated with multiple higher-order aspects of social perception (<xref ref-type="bibr" rid="R18">Deen et al., 2015</xref>; <xref ref-type="bibr" rid="R39">Isik et al., 2017</xref>; <xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>; <xref ref-type="bibr" rid="R54">Nummenmaa &amp; Calder, 2009</xref>), while medial frontal cortex (MFC) has been extensively studied in the context of self-representation and theory of mind (<xref ref-type="bibr" rid="R4">Amodio &amp; Frith, 2006</xref>). Finally, speech-based social communication is accomplished by a network consisting of superior temporal gyrus (STG) and its proximal areas STS (Wernicke area in left pSTS), TPJ, angular gyrus, middle temporal gyrus (MTG) and inferior frontal gyrus (Broca’s area in the left IFG) (<xref ref-type="bibr" rid="R62">Price, 2012</xref>).</p><p id="P3">Humans can however reliably process numerous simultaneously occurring features of the social world ranging from others’ facial identities and emotions to their intentions and mental contents to the fine-grained affective qualities of the social interaction. Given the computational limits of the human brain, it is unlikely that all features and dimensions of the social domain are processed by distinct areas and systems (<xref ref-type="bibr" rid="R38">Huth et al., 2012</xref>). Although the brain basis of perceiving specific isolated social features has been successfully delineated, the phenomenological as well as neural organization of the different social perceptual processes have remained poorly understood. Neural responses to complex stimuli cannot necessarily be predicted on statistical combination of responses to simple stimuli (<xref ref-type="bibr" rid="R22">Felsen &amp; Dan, 2005</xref>). Therefor studies based on neural responses to isolated social features may not directly generalize to real-world social perception (<xref ref-type="bibr" rid="R2">Adolphs et al., 2016</xref>) where social features such as facial identities, body movements, and nonverbal communication often overlap with distinct temporal occurrence patterns.</p><p id="P4">In psychological domains including actions (<xref ref-type="bibr" rid="R38">Huth et al., 2012</xref>), language (<xref ref-type="bibr" rid="R37">Huth et al., 2016</xref>), and emotions (<xref ref-type="bibr" rid="R46">Koide-Majima et al., 2020</xref>), neuroimaging studies have tackled this issue by first generating a comprehensive set of modelled dimensions for the complex dynamic stimulus. Then, using dimension reduction techniques, they assess the representational similarities of the modelled dimensions, or the representational similarities of the brain activation patterns associated with each dimension. For example, a recent study found that linguistic and visual semantic representations converge so that visual representations locate on the border of occipital cortex and that linguistic representations are located anterior to the visual representations (<xref ref-type="bibr" rid="R61">Popham et al., 2021</xref>). However, a detailed representational space for social features at both perceptual and neural level is currently lacking.</p><p id="P5">We define social perception as perception of all possible information relevant to interpret social interaction. To our knowledge, there is no consensus on a combined taxonomy for this broad definition. In social psychology, social situation has been described as a triad of person, situation and consequent behaviour (<xref ref-type="bibr" rid="R51">Lewin, 1936</xref>) where these elements have close interact between each other (<xref ref-type="bibr" rid="R24">Funder, 2006</xref>). However, data-driven taxonomies have only been proposed for the elements separately. Person perception has been extensively studied and person characteristics can be categorised as a limited set of trait dimensions, such as Big Five (<xref ref-type="bibr" rid="R25">Goldberg, 1990</xref>) or Big Six (<xref ref-type="bibr" rid="R50">Lee &amp; Ashton, 2004</xref>). For psychological situations, data-driven lexical studies have proposed limited dimensionality (<xref ref-type="bibr" rid="R60">Parrigon et al., 2017</xref>; <xref ref-type="bibr" rid="R64">Rauthmann et al., 2014</xref>). Recently, in behavioural domain categorization of human actions have also been proposed (<xref ref-type="bibr" rid="R73">Thornton &amp; Tamir, 2022</xref>). For two reasons, these established taxonomies are suboptimal for studying social perception as whole. First, these taxonomical studies base their results on questionnaires regarding social situations or rated similarities of different words describing social situations instead of the actual perception of social situations in real-life dynamic environment. Second, since the three elements have a close interaction, it would be sensible to study them together. Therefor our approach involves first collecting ratings of a large set of perceived social features from the stimulus used in this neuroimaging study and then limiting the social perceptual space with clustering analysis.</p><p id="P6">Commonly applied univariate analyses modeling the BOLD response in each voxel or region separately cannot reveal the specificity of spatial brain activation patterns resulting from the perception of different social features. Consequently, they do not allow testing whether different perceptual features can be reliably discriminated based on their spatial brain activation patterns. Multivariate pattern analysis (MVPA) allows the analysis of information carried by fine-grained spatial patterns of brain activation (<xref ref-type="bibr" rid="R74">Tong &amp; Pratte, 2012</xref>). Pattern recognition studies have established that regional multivariate patterns allow distinguishing brain activation related to multiple high-level social features such as faces (<xref ref-type="bibr" rid="R33">Haxby et al., 2001</xref>) and their racial group (<xref ref-type="bibr" rid="R8">Brosch et al., 2013</xref>) in FG and facial expressions in FG and STS (<xref ref-type="bibr" rid="R29">Harry et al., 2013</xref>; <xref ref-type="bibr" rid="R67">Said et al., 2010</xref>; <xref ref-type="bibr" rid="R79">Wegrzyn et al., 2015</xref>). Perception of different goal-oriented motor actions with different levels of abstraction can be decoded in LOTC and in inferior parietal lobe, suggesting that these regions process the abstract concepts of the goal-oriented actions, not just their low-level visual properties (<xref ref-type="bibr" rid="R85">Wurm &amp; Lingnau, 2015</xref>). Furthermore, decoding of goal-oriented actions was successful in LOTC when subjects observed the actions in both first and third person perspectives (<xref ref-type="bibr" rid="R59">Oosterhof et al., 2012</xref>). It however remains unresolved how specific these regional response profiles are across different social perceptual features.</p><sec id="S2"><title>The current study</title><p id="P7">In this fMRI study, we mapped the perceptual and neural representations of naturalistic social episodes using both univariate and multivariate analyses. We used short film clips as stimuli because cinema contains rich and complex social scenarios and as it also elicits strong and consistent neural responses in functional imaging studies (<xref ref-type="bibr" rid="R31">Hasson et al., 2010</xref>; <xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>). We first aimed at establishing a perception-based taxonomy of the social dimensions that human observers use for describing social scenarios, and then mapping the brain basis of this social perceptual space. We mapped the perceptual space of social processes based on subjective annotations of a large array of social features (n=112) in the films (n=96). We then used dimension reduction techniques to establish the representational space of social perception, and to reduce the multidimensional space into a limited set of reliable perceptual dimensions of social features. Using combination of univariate regression analysis, and multivariate pattern analysis we established that posterior temporal and occipital regions are the main hubs for social perception and that brain shows a gradient in social perceptual processing from broadly tuned but spatially dimension-specific responses in posterior temporal and occipital regions towards more selective responses in frontal and subcortical areas.</p></sec></sec><sec id="S3" sec-type="materials | methods"><title>Materials and methods</title><sec id="S4" sec-type="subjects"><title>Participants</title><p id="P8">Altogether 102 volunteers participated in the study. The exclusion criteria included a history of neurological or psychiatric disorders, alcohol or substance abuse, BMI under 20 or over 30, current use of medication affecting the central nervous system and the standard MRI exclusion criteria. Two additional subjects were scanned but excluded from further analyses because unusable MRI data due to gradient coil malfunction. Two subjects were excluded because of anatomical abnormalities in structural MRI and additional three subjects were excluded due to visible motion artefacts in preprocessed functional neuroimaging data. This yielded a final sample of 97 subjects (50 females, mean age of 31 years, range 20 – 57 years). All subjects gave an informed, written consent and were compensated for their participation. The ethics board of the Hospital District of Southwest Finland had approved the protocol and the study was conducted in accordance with the Declaration of Helsinki.</p></sec><sec id="S5"><title>Stimulus</title><p id="P9">To map brain responses to different social features, we used our previously validated socioemotional “localizer” paradigm that allows reliable mapping of various social and emotional functions (<xref ref-type="bibr" rid="R41">Karjalainen et al., 2017</xref>; <xref ref-type="bibr" rid="R42">Karjalainen et al., 2019</xref>; <xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>; <xref ref-type="bibr" rid="R56">Nummenmaa et al., 2021</xref>). The original experiment using this stimulus described the experimental design and stimulus selection in detail (<xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>). Briefly, the subjects viewed a medley of 96 movie clips (median duration 11.2 seconds, range 5.3 – 28.2 seconds, total duration 19 min 44 seconds) that have been curated to contain large variability of social and emotional content. The videos were extracted from mainstream Hollywood movies with audio track in English. To limit experiment duration 87 of the previously validated 137 clips were selected. 71 of these clips contained people in various social situations and contexts (one person: 15, two people: 22, more than two people: 34). To distinguish person perception from other audiovisual perception the stimulus contained four clips with animals and 12 control clips without people (showing e.g. scenery and objects). Additionally, nine erotic scenes showing heterosexual intercourse were added to broaden the emotional content of the original stimulus. Short descriptions about movie clips can be found from <xref ref-type="supplementary-material" rid="SD2">Table SI-1</xref>. Because this task was designed to map neural processing of naturalistic socioemotional events, the clips were not deliberately matched with respect to, for example, human motion or optic flow. The videos were presented in fixed order across the subjects without breaks. Subjects were instructed to view the movies similarly as if they were viewing a film at a cinema or at home and no specific task was assigned. Visual stimuli were presented with NordicNeuroLab VisualSystem binocular display. Sound was conveyed with Sensimetrics S14 insert earphones. Stimulation was controlled with Presentation software. Before the functional run, sound intensity was adjusted for each subject so that it could be heard over the gradient noise.</p></sec><sec id="S6"><title>Neuroimaging data acquisition and preprocessing</title><p id="P10">MR imaging was conducted at Turku PET Centre. The MRI data were acquired using a Phillips Ingenuity TF PET/MR 3-T whole-body scanner. High-resolution structural images were obtained with a T1-weighted (T1w) sequence (1 mm<sup>3</sup> resolution, TR 9.8 ms, TE 4.6 ms, flip angle 7◦, 250 mm FOV, 256 × 256 reconstruction matrix). A total of 467 functional volumes were acquired for the experiment with a T2∗-weighted echo-planar imaging sequence sensitive to the blood-oxygen-level-dependent (BOLD) signal contrast (TR 2600 ms, TE 30 ms, 75◦ flip angle, 240 mm FOV, 80 × 80 reconstruction matrix, 62.5 kHz bandwidth, 3.0 mm slice thickness, 45 interleaved axial slices acquired in ascending order without gaps).</p><p id="P11">The functional imaging data were preprocessed with FMRIPREP (<xref ref-type="bibr" rid="R21">Esteban et al., 2019</xref>) (v1.3.0), a Nipype (<xref ref-type="bibr" rid="R26">Gorgolewski et al., 2011</xref>) based tool that internally uses Nilearn (<xref ref-type="bibr" rid="R1">Abraham et al., 2014</xref>). During the preprocessing, each T1w volume was corrected for intensity non-uniformity using N4BiasFieldCorrection (v2.1.0) (<xref ref-type="bibr" rid="R76">Tustison et al., 2010</xref>) and skull-stripped using antsBrainExtraction.sh (v2.1.0) using the OASIS template. Brain surfaces were reconstructed using recon-all from FreeSurfer (v6.0.1) (<xref ref-type="bibr" rid="R16">Dale et al., 1999</xref>), and the brain mask estimated previously was refined with a custom variation of the method to reconcile ANTs-derived and FreeSurfer-derived segmentations of the cortical grey-matter of Mindboggle (<xref ref-type="bibr" rid="R44">Klein et al., 2017</xref>). Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="R23">Fonov et al., 2009</xref>) was performed through nonlinear registration with the antsRegistration (ANTs v2.1.0) (<xref ref-type="bibr" rid="R5">Avants et al., 2008</xref>), using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid, white-matter and grey-matter was performed on the brain-extracted T1w image using FAST (<xref ref-type="bibr" rid="R86">Zhang et al., 2001</xref>) (FSL v5.0.9).</p><p id="P12">Functional data were slice-time-corrected using 3dTshift from AFNI (<xref ref-type="bibr" rid="R13">Cox, 1996</xref>) (v16.2.07) and motion-corrected using MCFLIRT (<xref ref-type="bibr" rid="R40">Jenkinson et al., 2002</xref>) (FSL v5.0.9). These steps were followed by co-registration to the T1w image using boundary-based registration (<xref ref-type="bibr" rid="R27">Greve &amp; Fischl, 2009</xref>) with six degrees of freedom, using bbregister (FreeSurfer v6.0.1). The transformations from motion-correction, coregistration, and spatial normalization were concatenated and applied in a single step using antsApplyTransforms (ANTs v2.1.0) using Lanczos interpolation. Independent-component-analysis-based Automatic Removal Of Motion Artifacts (ICA-AROMA) was used to denoise the data nonaggressively after spatial smoothing with 6-mm Gaussian kernel (<xref ref-type="bibr" rid="R63">Pruim et al., 2015</xref>). The data were then detrended using 240-s-Savitzky–Golay filtering to remove the scanner drift (<xref ref-type="bibr" rid="R14">Cukur et al., 2013</xref>), and finally downsampled to original 3 mm isotropic voxel size. The BOLD signals were demeaned to make the regression coefficients comparable across different individuals (<xref ref-type="bibr" rid="R12">Chen et al., 2017</xref>). First and last two functional volumes were discarded to ensure equilibrium effects and to exclude the time points after the stimulus had ended.</p></sec><sec id="S7"><title>Stimulus features</title><p id="P13">Five individuals not participating in fMRI rated the 112 predefined social features (see <xref ref-type="supplementary-material" rid="SD2">Table SI-2</xref>) from the film clips. We selected a broad range of socioemotional features describing persons, social situations and behaviours from following categories: sensory input (e.g. smelling, tasting), basic body functions (e.g. facial expressions, walking, eating), person characteristics (e.g. pleasantness, trustworthiness) and person’s inner states (e.g. pleasant feeling, arousal), social interaction signals (e.g. talking, communicating with gestures) and social interaction characteristics (e.g. hostility, sexuality). Collecting perceptual ratings from a large set of individual social features enables reliable mapping of the whole social perceptual space that can be derived from the stimulus film clips and ensures that the data-driven dimensionality arises from the used stimulus. It was stressed to the observers that they should rate the perceived features of the social interaction rather than the observer’s own inner states (such as emotions evoked by the films). The ratings were collected separately for each video clip in short time intervals (median 4.0 sec, range: 3.1 – 7.3 seconds). Features were annotated in a continuous and abstract scale from “absent” to “extremely much”. For analyses the ratings were transformed to continuous scale from 0 (absent) to 100 (extremely much). Annotators watched the video clips altogether 12 times, rating an average of 10 features on each viewing to reduce the cognitive load. The ratings were done using an online rating platform Onni (<ext-link ext-link-type="uri" xlink:href="http://onni.utu.fi">http://onni.utu.fi</ext-link>) developed at Turku PET Centre (<xref ref-type="bibr" rid="R35">Heikkilä et al., 2020</xref>).</p></sec><sec id="S8"><title>Feature reliability</title><p id="P14">We first evaluated whether the <italic>a priori</italic> features were frequently and consistently perceived in the stimulus films. Features with low occurrence rate and/or inter-rater reliability were excluded, because i) high occurrence rate is needed to reliably estimate the stimulus-dependent variation in BOLD signal, and ii) high inter-rater reliability is necessary to study brain activity in a sample of participants independent from the raters. The occurrence rate was defined as the number of time points where the mean (minus standard error of the mean) of the rating exceeded 5 (on a scale ranging from 0 to 100). Features were included in the analyses if they occurred at least five times throughout the experiment; this was estimated to yield sufficient statistical power in the BOLD-fMRI GLM analyses. Inter-rater reliability of the features was assessed using intra-class correlation coefficient (ICC) as calculated in the R package psych (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/package=psych">https://cran.r-project.org/package=psych</ext-link>). ICC(A,1) was selected as appropriate model for ICC since it treats both video clips and raters as random effects and measures the absolute agreement between raters (<xref ref-type="bibr" rid="R53">McGraw &amp; Wong, 1996</xref>). ICCs below 0.5 are considered poor (<xref ref-type="bibr" rid="R47">Koo &amp; Li, 2016</xref>), and we thus only included features with ICC over 0.5. 45 features satisfied both criteria. The occurrence rate and interrater reliability of each feature are shown in <xref ref-type="supplementary-material" rid="SD1">Figure SI-1</xref>.</p></sec><sec id="S9"><title>Dimension reduction</title><p id="P15">The reliable 45 features were linearly correlated (<xref ref-type="fig" rid="F1">Figure 1</xref>) and it is unlikely that each social feature is processed in different brain regions or networks. We performed dimension reduction with hierarchical clustering on the correlation matrix of selected features to define the perceptual dimensions that characterize different aspects of social interaction. Clustering was chosen over principal component analysis for easier interpretation of the dimensions, and as this allowed us to model also features or their combinations which may be salient and reliable but not necessarily share a large proportion of variance with other variables. Initially we chose Pearson correlation as the similarity measure because the co-occurrence of features measured in abstract and possibly not strictly continuous scale is more interesting than the absolute distance between them (considered in PCA). Unweighted pair group method with arithmetic mean (UPGMA), as implemented in R, was used as the clustering algorithm (<ext-link ext-link-type="uri" xlink:href="https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust">https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/hclust</ext-link>). Other average linkage clustering methods implemented in the R package (WPGMA, WPGMC and UPGMC) yielded highly similar clustering hierarchy. Hierarchical clustering requires a desired number of resulting clusters as an input for automatic definition of cluster boundaries from hierarchical tree (<xref ref-type="supplementary-material" rid="SD1">Figure SI-2</xref>). To estimate the optimal number of clusters we chose three criteria that the clustering result should satisfy. These were cluster stability, theoretically meaningful clustering, and sufficient reduction in collinearity between the clusters. To assess the stability of clusters we conducted a consensus clustering analysis with ConsensusClusterPlus R package (<xref ref-type="bibr" rid="R80">Wilkerson &amp; Hayes, 2010</xref>). Theoretically meaningful clustering was then assessed, and collinearity was measured using Pearson correlation and variance inflating factor (VIF). Detailed information of the cluster analysis and consensus clustering results can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Materials</xref> (see also <xref ref-type="supplementary-material" rid="SD1">Figure SI-3</xref>). Cluster analysis grouped social features into six clusters and seven independent features not belonging to any cluster (<xref ref-type="fig" rid="F1">Figure 1</xref>) and these social dimensions formed the final model for social perception. The cluster regressors were created by averaging across the individual feature values in each cluster (<xref ref-type="supplementary-material" rid="SD1">Figure SI-4</xref>).</p></sec><sec id="S10"><title>Modelling low-level sensory features</title><p id="P16">Our goal was to map perceived social dimensions in the human brain. The stimulus film clips were not balanced by their low-level audiovisual properties thus these were controlled statistically when estimating the unique contribution of social dimensions to the BOLD signal. We extracted 14 different dynamic audiovisual properties from the stimulus film clips including six visual features (luminance, first derivative of luminance, optic flow, differential energy, and spatial energy with two different frequency filters) and eight auditory features (RMS energy, first derivative of RMS energy, zero crossing, spectral centroid, spectral entropy, high frequency energy and roughness). Optic flow was estimated with opticalFlowLK - function with basic options (<ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/vision/ref/opticalflowlk.html">https://www.mathworks.com/help/vision/ref/opticalflowlk.html</ext-link>). Custom functions were used for estimating other visual features (see <xref ref-type="sec" rid="S29">Code availability</xref>). Auditory features were extracted using MIRToolbox1.8.1 (<xref ref-type="bibr" rid="R58">Olivier Lartillot &amp; Toiviainen, 2007</xref>). First eight principal components (PCs) explaining over 90 % of the total variance were selected as regressors for low-level audiovisual features. As the stimulus film clips included control clips with no human interaction, we created a “nonsocial” block regressor by assigning a value of 1 to the time points where the stimulus did not contain people, human voice, or animals. A low-level model was formed by combining the eight audiovisual PCs, the nonsocial regressor and subjectwise mean signals from cerebrospinal fluid (CSF) and white matter (WM). See <xref ref-type="supplementary-material" rid="SD1">Figure SI-5</xref> for correlations between low-level features and social dimensions.</p></sec><sec id="S11"><title>Modelling brain responses to social perceptual dimensions</title><p id="P17">Ridge regression (<xref ref-type="bibr" rid="R36">Hoerl &amp; Kennard, 1970</xref>) was used to estimate the contributions of the low-level features and cluster-based composite social dimensions to the BOLD signals for each subject. Ridge regression was preferred over ordinary least squares (OLS) regression because even after dimension reduction, the social regressors were moderately correlated (range: -0.38 – 0.32) and we wanted to include all perceptual dimensions in the same model to estimate their unique contributions to the BOLD signals. We also wanted to avoid overfitting while retaining generalizability of the results. To conservatively control for low-level features, the demeaned BOLD signals were first predicted with the low-level model and the residual BOLD signals were then used as input in the following regression analysis with the social stimulus model. The low-level regressors were still included as nuisance covariates in the analysis of social dimensions for the possible interaction between the social dimensions and low-level features. In both consecutive analyses ridge parameter was optimized using leave-one-subject-out cross-validation. Prior to statistical modelling the regressor time series were convolved with the canonical HRF and the columns of the design matrices were standardised (μ = 0, σ = 1). Detailed description of ridge regression modelling is included in <xref ref-type="supplementary-material" rid="SD1">supplementary materials</xref> (<xref ref-type="supplementary-material" rid="SD1">Figure SI-6</xref>).</p><p id="P18">Ridge regularization parameter optimisation is computationally prohibitive when considering all the voxels in the brain since the leave-one-out cross-validation loads the complete voxelwise data into memory requiring constantly 70-80 Gb of memory even when considering 20 % of the voxels in the brain. Optimizing ridge penalty for each voxel separately could have yielded in large differences in the penalty parameters values throughout the brain thus making it more difficult to interpret the regional differences in the results. Thus, we selected an unbiased sample of grey matter voxels for the optimisation by randomly sampling 20 % of grey matter voxels uniformly throughout the brain. Only voxels within population level EPI mask where the population level probability of grey matter was over 0.5 were available for sampling. Consequently, a uniformly distributed sample of ~5000 voxels was selected for ridge parameter optimization.</p><p id="P19">For the social perceptual model, the regression analysis was run both at voxel-level and at region-of-interest (ROI) level. The population level EPI-mask was used in all analyses to include only voxels with BOLD signal from each subject and thus brain areas including parts of orbitofrontal, inferior temporal and occipital pole areas were not included in the analyses. In voxel-level analysis, subject-level β-coefficient-maps were subjected to group-level analysis to identify the brain regions where the association between intensity of each social dimension and haemodynamic activity was consistent across the subjects. Voxels outside the population level EPI mask were excluded from the analysis. Statistical significance was identified using the randomise function of FSL (<xref ref-type="bibr" rid="R82">Winkler et al., 2014</xref>). Voxel-level FDR with q-value of 0.05 was used to correct for multiple comparisons (<xref ref-type="bibr" rid="R6">Benjamini &amp; Hochberg, 1995</xref>). Anatomical ROIs were extracted from AAL2 atlas (<xref ref-type="bibr" rid="R65">Rolls et al., 2015</xref>). ROIs, where at least 50% of voxels were outside the population level EPI mask, were excluded from the analysis and only voxels within population level EPI mask were considered for the included ROIs. This resulted in inclusion of 41 bilateral ROIs into the analysis. A parametric T-test on the β-weights of a ROI was used to assess statistical inference across subjects. ROI-analysis results were considered significant with P-value threshold of 0.05 Bonferroni corrected for multiple comparisons. The results for ROI analyses are reported as union of bilateral ROIs.</p></sec><sec id="S12"><title>Multivariate pattern analysis of social perceptual dimensions</title><p id="P20">To reveal the regional specialization in processing of different social features, between-subject classification of 11 perceptual dimensions<sup><xref ref-type="fn" rid="FN3">1</xref></sup> was performed in Python using the PyMVPA toolbox (<xref ref-type="bibr" rid="R28">Hanke et al., 2009</xref>). The aim of the classification analysis was to complement univariate regression analysis by testing whether the human brain expressed regional specificity for distinct social dimensions. This approach was based on classification of discrete social dimensions from brain activity, rather than computationally more complex approach to predict actual values of multiple social predictors simultaneously based on brain activity. For this kind of classification, only one dimension label for each time point (each TR) could be given. In continuous signal, the choice of the best label for each time point was not always obvious, because more than one social feature could be present simultaneously and is likely that unusual or rarely present social information draws more attention (“Somebody starts crying”) than constantly present information (“People are talking”). To resolve this issue, we first normalized the dimension rating time series (μ = 0, σ = 1) and then, for each time point, chose the feature with the highest Z-score as the category label for that time point. To ensure that the included time points would be representative of the assigned categories, we chose only time points where Z-scores for the chosen dimension were positive. With this procedure we ensured that each time point holds a label of a representative category and that rarely present information is weighted more than constantly present information.</p><p id="P21">Classifying every time point separately is computationally prohibitive and single EPI scans are noisy. Moreover, it cannot be assumed that adjacent time points assigned with the same label would be independent from each other. Accordingly, we split the data into 29 time windows and all time points with the same label within a time window were considered as a single event of that class. The number of time windows was selected based on the fact that the response length of canonical HRF is approximately 30 seconds and therefor over 30 second time windows would be less dependent from each other than shorter time windows while the data would contain enough events for classification. The time window boundaries were adjusted so that adjacent time points with the same label would not be interspersed to different time windows because temporal autocorrelation of adjacent time points may yield in artificial increase of the classification accuracy. After adjustment, the average time window length was 39 seconds (range: 34 sec – 49 sec). The time windows were longer than the movie clips and therefor timepoint from different clips with similar social context could be judged as one event if they belong to same time window. Altogether the data consisted of 87 events for each subject. These events include Using an object: 16, Communication 15, Antisocial behaviour: 11, Feeding: 10, Walking: 9, Prosocial behaviour: 8, Body movement: 5, Crying: 4, Play: 4, Running: 3 and Searching: 2.</p><p id="P22">An ordinary least squares GLM was fit for the residual BOLD time series (after regressing out the low-level features) in each time window and social dimension and the resulting subjectwise β-images were used as input for the multivariate pattern analysis (MVPA). The input residual BOLD time series were normalized (μ = 0, σ = 1) before application of the GLM. A neural network (NN) model (<ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</ext-link>) was trained to classify the perceptual dimensions using leave-one-subject-out cross-validation, where the model was trained on the data from all except one subject and tested on the hold-out subject’s data; this procedure was repeated N times so that each subject was used once as the hold-out subject. Such leave-one-subject-out cross-validation tests the generalizability of the results across the sample of the subjects. The analysis was performed using whole brain data (with non-brain voxels masked out) and regional data using anatomical ROIs. In the whole-brain analysis, an ANOVA feature selection was applied to the training set within each cross-validation and 3000 voxels with the highest F-score were selected. The regional MVPA was first performed using data form all voxels within a region. To control for the effect of ROI size to the classification accuracy the regional MVPA was also performed with an ANOVA feature selection where the size of the smallest ROI (lateral orbitofrontal cortex, 119 voxels) was selected as the number of features for the feature selection.</p><p id="P23">Hyperparameters of the NN algorithm were optimized within a limited set of predefined hyperparameter values in the whole brain analysis. Hyperparameter values reflecting the best prediction accuracy with acceptable runtime were used in both full brain and ROI analyses (see <xref ref-type="supplementary-material" rid="SD2">Table SI-3</xref> for hyperparameter tuning). The optimized NN included two hidden layers with 100 nodes in each (alpha = 1.00, max_iter 500, other hyperparameters set to default). In the model learning process, the order of events was shuffled in each training iteration which minimized the model’s ability to learn the order of the events in the stimulus. A support vector machine (SVM) classifier had similar classification accuracy in the whole brain analysis, but NN model was chosen because the computation time was shorter and the variance of classification accuracies between subjects were lower with NN model compared to SVM classifier.</p><p id="P24">Classification accuracy was quantified by computing the proportion of correctly classified events relative to the total number of events (i.e., recall). To estimate the null distribution, the following procedure was repeated 500 times: we 1) randomly shuffled social class labels; 2) ran the whole-brain MVPA with 97 leave-one-subject-out cross-validations, where the classifier was trained on the data with shuffled labels from N-1 subjects and tested on data with correct labels from the remaining subject; and 3) calculated the classification accuracies on each of the 500 iterations. The null distribution estimation was computationally prohibitive as one iteration took approximately one hour, and we decided that 500 iterations would be sufficient to assess the statistical significance of our findings. If the true accuracy was larger than 99% of the accuracies obtained with the randomly shuffled labels, the true accuracy was considered significant with an alpha of 0.01. We cannot assume that the null distribution of classification accuracies for each class is equal and center around the naïve chance level because the number of events is unbalanced between classes. For this reason, we only report if the total accuracy of the classification is statistically significant. In the whole-brain classification we also report the precision of the classifications which is the number of correct predictions for a class divided by the total number of predictions into that class. In ROI analyses, the statistical differences between regional classification accuracies were tested using paired T-tests between subjectwise classification accuracies between each pair of regions.</p></sec><sec id="S13"><title>Inter-subject correlation analysis</title><p id="P25">Watching films synchronizes brain activity between different individuals particularly in the occipital, temporal, and parietal regions of the brain and the synchronization of brain activity can be measured with inter-subject correlation (ISC) analysis (<xref ref-type="bibr" rid="R32">Hasson et al., 2004</xref>). As the only variable factor in the experiment is the time-varying audiovisual stimulus, ISC analysis captures the shared stimulus-dependent activation in the brain. It is well known that ISC is greatest on the sensory cortices, but an important yet unresolved question is which variables drive the degree of synchronization of BOLD response. Some prior studies suggest that emotions and top-down perspectives play a role (<xref ref-type="bibr" rid="R48">Lahnakoski et al., 2014</xref>; <xref ref-type="bibr" rid="R57">Nummenmaa et al., 2014</xref>), but the role of social features remain unknown. As a <italic>post hoc</italic> analysis, we assessed whether the regional differences in brain response profiles for social dimensions relate to the inter-subject response reliability of BOLD response. To this end, we calculated the ISC across subjects over the whole experiment and compared the regional ISC with the results from regression and MVPA analyses. ISC-toolbox with default settings was used for ISC calculations (<xref ref-type="bibr" rid="R43">Kauppi et al., 2014</xref>).</p></sec></sec><sec id="S14" sec-type="results"><title>Results</title><sec id="S15"><title>How people perceive the social world?</title><p id="P26">A total of 45 out of the 112 social features had sufficient inter-rater reliability and occurrence rate (see <xref ref-type="supplementary-material" rid="SD1">Figure SI-1</xref>). Hierarchical clustering identified six clusters that were labelled as “Antisocial behaviour”, “Prosocial behaviour”, “Communication”, “Body movement”, “Feeding” and “Play”. Seven perceptual dimensions did not link with any cluster and were analysed separately. These dimensions were “Using an object”, “Crying”, “Male”, “Female”, “Running”, “Walking” and “Searching”. <xref ref-type="fig" rid="F1">Figure 1</xref> shows the clustering of the dimensions. Median pairwise correlation between any two of the 13 dimensions was 0.02 (range: -0.38 – 0.32) and the maximum variance inflation factor (VIF) in the design matrix excluding nuisance covariates was 3.3 (male regressor) and median VIF value was 1.3. These diagnostics indicate that regression coefficients for the dimensions will be stable in linear model estimations and, and they could thus be included in the same model. See <xref ref-type="supplementary-material" rid="SD1">Figure SI-4</xref> for visualized time series of social dimensions and <xref ref-type="supplementary-material" rid="SD1">Figure SI-5</xref> for correlations matrices for low-level features and social dimensions.</p></sec><sec id="S16"><title>Cerebral topography of social perception</title><p id="P27">Regularized ridge regression was used to establish the full-volume activation patterns for 13 perceptual social dimensions (<xref ref-type="fig" rid="F2">Figure 2</xref>). Social information processing engaged all brain lobes and both cortical and subcortical regions. Robust responses were observed in occipital, temporal, and parietal cortices (<xref ref-type="fig" rid="F3">Figure 3</xref>). There was a clear gradient in the responses, such that posterior temporal, occipital and parietal regions showed the strongest positive association with most of the social dimensions, with significantly less consistent activations in the frontal lobes and subcortical regions. Yet, frontal, and subcortical activations were also observed for some dimensions such as prosocial behaviour, antisocial behaviour and feeding.</p><p id="P28">In ROI analysis, broad responses for social dimensions were observed in STG and MTG with strongest responses for communication and antisocial behaviour, respectively. In parietal lobe, all regions except angular gyrus and paracentral lobule associated with a wide range of perceptual dimensions. In frontal regions the associations between social dimensions and haemodynamic activity were less consistent than in more posterior regions, yet still statistically significant in some of the regions including IFG, cingulate cortex and precentral gyrus. Most consistent frontal effects were found for prosocial and antisocial behaviour. For subcortical regions the observed associations were generally weak. Most notable subcortical associations with perceptual dimensions were seen in amygdala and thalamus. Consistent negative associations were restricted to occipital lobe and were observed for communication, crying, body movement and running.</p></sec><sec id="S17"><title>Brain gradient of social perception</title><p id="P29"><xref ref-type="fig" rid="F4">Figure 4a</xref> shows the cumulative brain activation maps for all 13 perceptual dimensions. There was a gradient in the regional selectivity for social dimensions. Posterior temporal and occipital cortices as well as parietal cortices responded to most social dimensions, while responses become more selective in the frontal cortex although IFG, precentral gyrus and the frontal part of the medial superior frontal gyrus (SFG) had some consitency in their response profiles. Because the same stimulus was used across the subjects, we hypothesized that the brain activation in the areas with the broadest response profiles would be temporally most synchronized across subjects. We thus calculated the ISC of brain activation over the whole experiment (<xref ref-type="fig" rid="F4">Figure 4b</xref>) and correlated the regional ISC values with corresponding response selectivity values (i.e. number of social features resulting in significant activations in each region). Scatterplot in <xref ref-type="fig" rid="F4">Figure 4c</xref> shows the association between ISC and corresponding brain response selectivity for perceptual dimensions (Pearson r = 0.86).</p></sec><sec id="S18"><title>Multivariate pattern analysis</title><p id="P30">Finally, we trained a between-subject neural network model to decode presence of perceptual social dimensions from the spatial haemodynamic activation patterns to reveal which social dimensions are consistently represented in each cerebral region. Whole brain classification was performed in 3000 voxels that passed through the ANOVA feature selection. Most of the selected voxels (<xref ref-type="fig" rid="F5">Figure 5a</xref>) localized into temporal (STG, MTG, Heschl gyrus and superior temporal pole), occipital (calcarine and lingual gyri, cuneus, FG, superior occipital gyrus (SOccG), middle occipital gyrus (MoccG) and inferior occipital gyrus (IoccG)) and parietal cortices (supramarginal, superior parietal gyrus (SPG) and inferior parietal gyrus (IPG)). The permuted chance level for the total classification accuracy in the whole brain analysis was 0.128 which is above naïve chance level <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:mfrac><mml:mo>≅</mml:mo><mml:mn>0.09</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> At the whole brain level, the NN model was able to classify all 11 social dimensions significantly above chance level with the total classification accuracy of 0.52 (p&lt;0.01). Classification accuracies/precisions for each social dimension were: walking: 0.49/0.51, using an object: 0.53/0.50, searching: 0.70/0.69, running 0.56/0.62, prosocial behaviour 0.45/0.48, play 0.53/0.51, feeding 0.46/0.48, crying 0.46/0.51, communication 0.55/0.55, body movement 0.52/0.50 and antisocial behaviour 0.55/0.53 (<xref ref-type="fig" rid="F5">Figure 5a</xref>).</p><p id="P31">The classification was also performed within anatomical ROIs (<xref ref-type="fig" rid="F5">Figure 5b</xref>). Most accurate classifier performance was observed in lingual gyrus (0.34, p&lt;0.01), calcarine gyrus (0.33, p&lt;0.01), cuneus (0.29, p&lt;0.01), SOccG (0.29, p&lt;0.01), MOccG (0.27, p&lt;0.01), STG (0.27, p&lt;0.01) and MTG (0.25, p&lt;0.01). Although the prediction accuracies were statistically significantly above permuted chance level for each ROI, the gradient in brain responses for social perception was also observed in the classification accuracies so that highest accuracy was observed in occipital and temporal areas, followed by parietal cortices and frontal and cingulate cortices. Lowest accuracies were found in the subcortical regions. (<xref ref-type="fig" rid="F5">Figure 5b</xref>). We also validated that this gradient was not an artefact stemming from the sizes of the ROIs, as similar gradient was observed in the regional classification with ANOVA feature selection limited to 119 voxels (<xref ref-type="fig" rid="F5">Figure 5c</xref>). <xref ref-type="supplementary-material" rid="SD1">Figure SI-8</xref> shows the statistical significance of the classification accuracies between all pairs of ROIs confirming the observed gradient in classification accuracies. Occipital and temporal areas (excluding Temporal pole) showed significantly higher classification accuracy than frontal and subcortical regions.</p></sec><sec id="S19"><title>Relationship between classification accuracy and ISC</title><p id="P32">Regional classification accuracy and ISC were positively correlated (Pearson r = 0.85, <xref ref-type="fig" rid="F6">Figure 6a</xref>). Most occipital regions, STG and MTG showed high synchrony (ISC &gt; 0.1) and high classification accuracy (acc &gt; 0.25). Most parietal regions showed average ISC and average classification accuracy while frontal and subcortical regions showed low ISC and low classification accuracies. The most notable exception to this pattern was Heschl gyrus which had high ISC (0.28) yet average classification accuracy (acc = 0.22). <xref ref-type="fig" rid="F6">Figure 6b</xref> summarizes the results from separate regression, ISC and classification analyses where the findings overlap most in temporal and occipital cortices.</p></sec></sec><sec id="S20" sec-type="discussion"><title>Discussion</title><p id="P33">Our findings provide the currently most detailed map of the social perceptual mechanisms in the human brain using naturalistic stimulus. The behavioral data established that 13 social dimensions reliably capture the social perceptual space contained in the video stimulus. The cerebral topography for social perception was organized along an axis, where posterior temporal and cortical regions served a central general-purpose role in social perception, while the regional selectivity for social dimensions increased towards frontal and subcortical regions. Multivariate pattern recognition established that particularly occipito-temporal and parietal regions carry detailed and spatially dimension-specific information regarding the social world, as evidenced by the highest classification accuracies in the multi-class classification approach. Both classification accuracy and consistency of the responses for specific social dimensions were the highest in the brain regions having most reliable (indicated by ISC) activation patterns throughout the experiment. These effects were observed although low-level sensory features were statistically controlled for. Altogether these results show that multiple brain regions are jointly involved in representing the social world and that different brain regions have variable specificity in their spatial response profiles towards social dimensions.</p><sec id="S21"><title>Dimensions of social perception</title><p id="P34">The behavioural experiment established that the observers used consistently a set of 45 descriptors when evaluating the social contents of the movies. Dimension reduction techniques further revealed that these 45 features could be adequately summarized in 13 social dimensions. The largest clusters were organized along the valence dimension of the social interaction containing prosocial (e.g., kissing, touching, sexuality) versus antisocial (hurting others, yelling) behaviors. Social communicative behaviors (e.g., eye contact, talking) and body movements (e.g., waving, moving a foot) also formed large clusters. Play-related behaviors (laughing, playfulness) as well as feeding-related actions (e.g., tasting, eating) were also represented into smaller clusters. Notably, some features such as presence of males versus females, walking, and using objects remained independent of any of the clusters. Average hierarchical clustering algorithm was used because it yields clearly interpretable clusters and because feature similarity could be measured with correlation instead of absolute distance. Further research could establish how behavioural clusters found with hierarchical clustering relate to, for example, principal components off the same data and how the clusters generalize to other naturalistic stimuli.</p><p id="P35">The stimulus film clips cannot portray all possible social scenarios and Hollywood films are only a proxy of real-life social interaction. Still, 99 of the predefined 112 social features had sufficient occurrence rate in the stimulus video clips (<xref ref-type="supplementary-material" rid="SD1">Figure SI-1</xref>) which indicate that the stimulus contains a broad range of social information. The average duration of film clips was ~10 seconds and we acknowledge that this timescale does not allow examination of social processes occurring at slower temporal frequencies such as pair bonding and long-term impression formation. However, social perception may be astonishingly fast. Semantic, social, and affective categorization may happen in few hundred milliseconds (<xref ref-type="bibr" rid="R55">Nummenmaa et al., 2010</xref>) and the judgements do not significantly change from the initial judgments after longer consideration (<xref ref-type="bibr" rid="R81">Willis &amp; Todorov, 2006</xref>). Electroencephalography (EEG) has also confirmed reliable associations between social perceptual features and brain response already 400ms after the stimulus (<xref ref-type="bibr" rid="R19">Dima et al., 2022</xref>) concluding that short video clips can capture some temporal scales of social perception. Data-driven models for characterising social perception (<xref ref-type="bibr" rid="R2">Adolphs et al., 2016</xref>) constitute an important, complementary alternative for the theory-based models for separate taxonomies of person, situation, and action perception since i) the found clusters are based on the actual perception of the social context, ii) the data-driven model does not separate persons, situations, and actions but is based on the subjects’ percept of the stimulus and iii) only dimensions actually present in the stimulus are considered. Importantly, this data-driven model for social perception has many similarities with previously proposed taxonomies. The largest observed clusters prosocial and antisocial behaviour closely relate to the emotional valence which is at the core of emotion theory (<xref ref-type="bibr" rid="R66">Russell, 1980</xref>) and is also considered in taxonomies describing persons (<xref ref-type="bibr" rid="R69">Simms, 2007</xref>) and situations (<xref ref-type="bibr" rid="R60">Parrigon et al., 2017</xref>; <xref ref-type="bibr" rid="R64">Rauthmann et al., 2014</xref>). Clusters play and feeding closely relate to dimensions Humor from situation taxonomy (<xref ref-type="bibr" rid="R60">Parrigon et al., 2017</xref>) and Food from action domain (<xref ref-type="bibr" rid="R73">Thornton &amp; Tamir, 2022</xref>), respectively. Mapping of the neural space for social perception requires the social features to be consistently rated among the independent set of annotators. 61 of the total 112 rated social features showed low between-rater agreement (ICC &lt; 0.5, <xref ref-type="supplementary-material" rid="SD1">Figure SI-1</xref>) which is itself an important finding regarding the consistency of the perceptual taxonomy individuals use for describing social events. The exclusion of these features had the effect that more abstract, or idiosyncratically judged dimensions cannot be addressed in this experiment and pushed the studied perceptual processes towards action and situation domains. Further research should nevertheless investigate the shared versus idiosyncratic social evaluations across individuals, as this would be informative regarding what are the core building blocks of the social environment that are shared across most observers.</p></sec><sec id="S22"><title>Cerebral gradient in social perception</title><p id="P36">The univariate BOLD-fMRI analysis based on social dimensions revealed that a widely distributed cortical and subcortical networks encode the social contents of the video stimuli. Most dimensions activated LOTC, STS, TPJ, as well as other occipitotemporal and parietal regions. There was a gradual change from these unselective social responses in occipitotemporal and parietal regions towards more selective responses in frontal and subcortical regions, suggesting that social perception is mainly processed in lateral and caudal parts of the brain. This effect was also confirmed by the ROI analysis. Most consistent responses were observed in all occipital regions and in temporal regions STG and MTG (which outline STS) and Heschl gyrus. In parietal cortex, most consistent responses were observed in supramarginal gyrus (a part of TPJ), SPG and precuneus. Frontally the responses were less consistent although brain activity in IFG, precentral gyrus and frontal part of medial SFG associated with a limited number of dimensions including “Prosocial behaviour”, “Antisocial behaviour”, “Feeding” and “Using an object”. These data are consistent with previous univariate studies addressing social functions for LOTC (<xref ref-type="bibr" rid="R20">Downing et al., 2001</xref>; <xref ref-type="bibr" rid="R52">Lingnau &amp; Downing, 2015</xref>; <xref ref-type="bibr" rid="R83">Wurm &amp; Caramazza, 2019</xref>; <xref ref-type="bibr" rid="R84">Wurm et al., 2017</xref>), STS (<xref ref-type="bibr" rid="R18">Deen et al., 2015</xref>; <xref ref-type="bibr" rid="R39">Isik et al., 2017</xref>; <xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>; <xref ref-type="bibr" rid="R78">Walbrin et al., 2018</xref>), TPJ (<xref ref-type="bibr" rid="R9">Carter &amp; Huettel, 2013</xref>; <xref ref-type="bibr" rid="R68">Saxe &amp; Kanwisher, 2003</xref>), and MFC (<xref ref-type="bibr" rid="R17">de la Vega et al., 2016</xref>). The results were controlled with an extensive set of PCA rotated audiovisual features. A non-social regressor was also built from the stimulus time points where no social interaction was present, and this feature was added to the low-level model. The fMRI data were collected in one scan, hence ruling out the possibility to control for low-level features by cross-validation. Therefor the separation of social perceptual features from all possible low and mid-level features is not possible. However, we did not find extensive associations between social dimensions and BOLD responses in V1 and higher-level information such as body parts and actions have already been shown to associate with BOLD response better than low-level visual features in occipital cortex outside V1 (<xref ref-type="bibr" rid="R72">Tarhan &amp; Konkle, 2020</xref>). Additionally, it has been shown that social features of actions explain more variance of EEG responses to videos than low-level visual features (<xref ref-type="bibr" rid="R19">Dima et al., 2022</xref>) further supporting the conclusion that the results reflect social information processing rather than low-level audiovisual perception.</p></sec><sec id="S23"><title>Decoding of perceptual social dimensions from brain activation patterns</title><p id="P37">The univariate analysis revealed the overall topography and regional brevity of the tuning for different social signals. However, this analysis cannot determine whether a single anatomical region activated by multiple social dimensions reflects responses to shared features across all the dimensions (such as biological motion perception or intentionality detection; <xref ref-type="bibr" rid="R3">Allison et al., 2000</xref>; <xref ref-type="bibr" rid="R54">Nummenmaa &amp; Calder, 2009</xref>), or spatially overlapping yet dimension-specific processing. Multivariate classification analysis revealed that the answer to this question depends on the region. The ANOVA feature selection for the whole-brain classification retrieved voxels from STS, LOTC, TPJ and FG (<xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="fig" rid="F6">Figure 6</xref>) yielding classification accuracy exceeding 50% for the multi-class classification. Regional classification confirmed that occipital, temporal and parietal regions showed average to high classification accuracies, whereas the classification accuracies diminished towards chance level in frontal and subcortical regions. These results show that even if the regional univariate responses for social dimensions were overlapping the specificity of the spatial activation patterns was different between regions. Interestingly, the ANOVA selected voxels found to best discriminate social features closely resemble the network proposed for social aspects of human actions in a recent study (<xref ref-type="bibr" rid="R72">Tarhan &amp; Konkle, 2020</xref>) with the exception that our results are more bilateral. Previous multivariate studies have shown how individual social features are represented in these regions. For example, specific response patterns to pictures of faces versus animals, houses or man-made objects can be found in FG and LOTC (<xref ref-type="bibr" rid="R33">Haxby et al., 2001</xref>) and semantic information from different human actions judged from static images are represented in LOTC (<xref ref-type="bibr" rid="R75">Tucciarelli et al., 2019</xref>). Subsequent classification studies have shown that, for example, different facial expressions can be classified from activation patterns in FG, and STS (<xref ref-type="bibr" rid="R67">Said et al., 2010</xref>; <xref ref-type="bibr" rid="R79">Wegrzyn et al., 2015</xref>) and goal-oriented actions in LOTC and interior parietal lobe (<xref ref-type="bibr" rid="R70">Smirnov et al., 2017</xref>; <xref ref-type="bibr" rid="R85">Wurm &amp; Lingnau, 2015</xref>). Importantly, our results show that BOLD-fMRI can be used for classification of multiple overlapping event categories from continuous naturalistic stimulation. Previous multivariate pattern analyses of social categories have used block designs and categorical stimuli matching the <italic>a priori</italic> category labels. In addition, these studies have only focused on a certain detailed aspect of socioemotional processing. The present results thus underline that even with high-dimensional naturalistic stimulus, the response properties of certain brain areas show high degree of category specificity.</p><p id="P38">The results from the classification analysis complement the results from the regression analysis with some limitations. The video clips were shown in the same order for all subjects, which may artificially boost classification accuracy, although the model should not learn the actual order of the events since the data were shuffled in each learning iteration. Regardless, the observed differences in classification accuracies in different brain regions should not be due to the order of the stimulus which is more interesting than the actual classification accuracies. It is likely that people focus attention in the most salient social details in the stimulus films instead of continuously monitoring for multiple sources of information with possibly low importance. Hence, we chose a classification approach where each time point was labelled with the social dimension of the highest relative intensity instead of trying to predict the values of all social features simultaneously. Future studies could try to predict multiple intensities for multiple categories in the stimulus set. Due to naturalistic and uncontrolled stimuli the classification dataset was unbalanced. Even in regions with near chance level total accuracy, some classes with large number of events were classified with relatively high accuracy (<xref ref-type="supplementary-material" rid="SD1">Figure SI-9</xref>) which may reflect the differences in the number of events in these classes and might not reflect the actual social information processing in the brain. Consequently, regional differences in the prediction accuracies to individual classes cannot be addressed.</p></sec><sec id="S24"><title>Reliability versus specificity of responses to social perceptual dimensions</title><p id="P39">We observed robust inter-subject correlation of brain activity in temporal and occipital regions while subjects viewed the video clips. Previous studies have found that the ISC is in general the strongest in sensory regions, and it progressively becomes weaker toward the polysensory and associative cortices (<xref ref-type="bibr" rid="R31">Hasson et al., 2010</xref>). Our data revealed that the strength of the ISC was contingent on the number of social features each region responded to in the univariate analysis (r = 0.86, <xref ref-type="fig" rid="F4">Figure 4c</xref>). Additionally, regional ISC was also associated with the corresponding regional classification accuracy (r = 0.85, <xref ref-type="fig" rid="F6">Figure 6a</xref>). These data highlight the relevance of the social domain to the cortical information processing, as the consistency of the regional neural responses was associated with the brevity of the tuning for social signals in each region. In other words, regions responding to multiple social signals also do so in a time-locked fashion across subjects, whereas the responses become more idiosyncratic as they become more selective. Importantly, this effect was not just an artefact of the consistency of sensory cortical responses to social signals but was also observed in higher-order associative areas including LOTC and STS.</p></sec><sec id="S25"><title>Functional organization of social perception in the human brain</title><p id="P40">The regional response profiles towards social signals can be summarized based on the combination of the regional response consistency (univariate regression analysis), the spatial response pattern specificity (MVPA) and the reliability of the BOLD signal across subjects (ISC). First, posterior temporal and occipital regions responded consistently to most social dimensions, while the presence of specific social dimensions could also be classified accurately from these regions. High classification accuracy suggests that these regions already hold dimension-specific and integrated information regarding the social world. Additionally, these regions responded consistently to the social stimuli (as indicated by high ISC) across subjects. LOTC, STS, TPJ, FG and occipital regions thus constitute the most fundamental hubs for social perception in the human brain and are likely involved in integration of the multisensory information and semantic representations regarding the social events (<xref ref-type="bibr" rid="R3">Allison et al., 2000</xref>; <xref ref-type="bibr" rid="R49">Lahnakoski et al., 2012</xref>).</p><p id="P41">Second, Heschl gyrus, the site of the auditory cortex (<xref ref-type="bibr" rid="R15">Da Costa et al., 2011</xref>) responded consistently to social dimensions but the classification accuracy was only moderate in that region while the ISC of the response was the highest of all regions. This suggests that Heschl gyrus processes domain-general social (most likely auditory) information but does not carry detailed information about the distinct social dimensions, as evidenced by the weak ability to classify specific dimensions from this region. Third, parietal regions especially precuneus, supramarginal gyrus and SPG showed consistent responses with numerous social dimensions and yet their ISC and classification accuracies were only moderate. Previously precuneus have been linked with attention and memory retrieval (<xref ref-type="bibr" rid="R10">Cavanna &amp; Trimble, 2006</xref>), supramarginal gyrus with phonological (<xref ref-type="bibr" rid="R30">Hartwigsen et al., 2010</xref>) and visual (<xref ref-type="bibr" rid="R71">Stoeckel et al., 2009</xref>) processing of words and SPG in visuospatial processing and working memory (<xref ref-type="bibr" rid="R45">Koenigs et al., 2009</xref>). These parietal regions thus likely respond to some general features of the social signals or idiosyncratic brain states associated with social dimensions.</p><p id="P42">Frontal and subcortical regions responded only to a limited number of social dimensions, and classification accuracy and ISC were remained low. The regression analysis showed some consistency in IFG, precentral gyrus, the frontal part of the medial SFG, amygdala and thalamus, yet the classification accuracies remained low. MFC have previously been associated with higher-level social and affective inference such as linking social processing with decision making, affective processing and theory of mind (<xref ref-type="bibr" rid="R4">Amodio &amp; Frith, 2006</xref>; <xref ref-type="bibr" rid="R17">de la Vega et al., 2016</xref>). But previous classification studies have not found specificity for responses to social perceptual dimensions in frontal cortex (<xref ref-type="bibr" rid="R33">Haxby et al., 2001</xref>; <xref ref-type="bibr" rid="R59">Oosterhof et al., 2012</xref>; <xref ref-type="bibr" rid="R79">Wegrzyn et al., 2015</xref>; <xref ref-type="bibr" rid="R85">Wurm &amp; Lingnau, 2015</xref>). Thus, frontal areas may subserve higher-order social process by linking low-level social perception into more complex and abstract cognitive processes such as making predictions of the next actions or linking perception with the brains affective system. Indeed, there is evidence that MFC could be responsible in giving a affective meaning for the ongoing experiences and that MFC processing is highly idiosyncratic (<xref ref-type="bibr" rid="R11">Chang et al., 2021</xref>). Finally, limbic regions such as amygdala and thalamus in turn have been linked with processing of (negative) emotions (<xref ref-type="bibr" rid="R42">Karjalainen et al., 2019</xref>) and accordingly they showed reliable responses primarily to perception of antisocial behaviours.</p></sec></sec><sec id="S26" sec-type="conclusions"><title>Conclusion</title><p id="P43">Using a combination of data-driven approaches and multivariate pattern recognition we established the perceptual space for social features and mapped the cerebral topography of social perception that can be adequately described with 13 perceptual dimensions. Social perceptual space included clusters of social features describing prosocial and antisocial behaviour, feeding, body movement, communication and playfulness, as well as individual dimensions male, female, running, walking, searching, crying and using an object. Clear gradient in response selectivity was observed from broad response profiles in temporal, occipital and parietal regions towards narrow and selective responses in frontal and subcortical regions. Perceptual social dimensions could be reliably decoded from regional activation patterns using multivariate pattern analysis. Both regression analysis and multivariate pattern analysis highlighted the importance of LOTC, STS, TPJ and FG and other occipitotemporal regions as dimension-specific social information processors, while parietal areas and Heschl’s gyrus process domain-general information from the social scenes. Additionally, regional response profiles for social perception closely related to the overall reliability of the BOLD responses. Altogether these results highlight the distributed nature of social processing in the brain as well as the spatial specificity of brain regions to social dimensions.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS157191-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d137aAdFbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Table SI-1</label><media xlink:href="EMS157191-supplement-Table_SI_1.xlsx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.spreadsheetml.sheet" id="d137aAdFcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S27"><title>Acknowledgements</title><p>The study was supported by grants to LN from European Research Council (#313000) and the Academy of Finland (#332225, #294897). The authors declare no competing financial or non-financial interests. We thank Tuulia Malen for the fruitful conversations around the analysis methods used in this article and Juha Lahnakoski for the help with low-level feature extraction.</p></ack><sec id="S28" sec-type="data-availability"><title>Data availability</title><p id="P44">The stimulus movie clips can be made available for review although copyrights preclude public redistribution of the stimulus set. Short descriptions of each film clip can be found from a supplementary excel sheet. According to Finnish legislation, the original (even anonymized) neuroimaging data used in the experiment cannot be released for public use. All necessary files can however be made available for reviewers. The brain activation patterns (unthresholded T-maps) for each GLM predictor are available in NeuroVault (<ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/IZWVFEYI/">https://neurovault.org/collections/IZWVFEYI/</ext-link>).</p></sec><sec id="S29" sec-type="data-availability"><title>Code availability</title><p id="P45">We developed in-house scripts for low-level audiovisual feature extraction and ridge regression optimization, and the scripts are freely available in GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/santavis/functionalorganization-of-social-perception">https://github.com/santavis/functionalorganization-of-social-perception</ext-link>). Other analyses involved using available R and Python packages and these scripts can be made available for review.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P46"><bold>Author contributions</bold></p><p id="P47">SS developed the analysis methods, analysed the data, and wrote the manuscript.</p><p id="P48">TK preprocessed the fMRI data, developed the regression analysis methods, and wrote and reviewed the manuscript.</p><p id="P49">SNF developed the MVPA analysis methods and reviewed the manuscript.</p><p id="P50">MH conceptualized the study design, collected the data, and reviewed the manuscript.</p><p id="P51">VP, KS, and LS collected the data and reviewed the manuscript.</p><p id="P52">EG developed the analysis and preprocessing methods and reviewed the manuscript.</p><p id="P53">JH reviewed the MR images and reviewed the manuscript and HK supervised the security of MRI data collection and reviewed the manuscript.</p><p id="P54">LN conceptualized the study design, acquired funding, supervised the project, developed analysis methods, and wrote and reviewed the manuscript.</p></fn><fn id="FN2" fn-type="conflict"><p id="P55"><bold>Conflict of interest statement</bold></p><p id="P56">The authors declare no conflicts of interest</p></fn><fn id="FN3"><label>1</label><p id="P57">Dimensions “Male” and “Female” were excluded from classification, because unlike the rest of the dimensions, they are genuinely binary features and thus not comparable with the other dimensions in the implemented classification framework (see <xref ref-type="supplementary-material" rid="SD1">Figure SI-4</xref>).</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><article-title>Machine learning for neuroimaging with scikit-learn [Methods]</article-title><source>Frontiers in Neuroinformatics</source><year>2014</year><month>February</month><day>21</day><volume>8</volume><issue>14</issue><comment>2014</comment><pub-id pub-id-type="pmcid">PMC3930868</pub-id><pub-id pub-id-type="pmid">24600388</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Todorov</surname><given-names>A</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Data-driven approaches in the investigation of social perception</article-title><source>Philos Trans R Soc Lond B Biol Sci</source><year>2016</year><month>May</month><day>5</day><volume>371</volume><issue>1693</issue><pub-id pub-id-type="pmcid">PMC4843606</pub-id><pub-id pub-id-type="pmid">27069045</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0367</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Social perception from visual cues: role of the STS region</article-title><source>Trends Cogn Sci</source><year>2000</year><month>Jul</month><volume>4</volume><issue>7</issue><fpage>267</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">10859571</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amodio</surname><given-names>DM</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><article-title>Meeting of minds: the medial frontal cortex and social cognition</article-title><source>Nat Rev Neurosci</source><year>2006</year><month>Apr</month><volume>7</volume><issue>4</issue><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="pmid">16552413</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><article-title>Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><year>2008</year><month>02</month><day>01</day><volume>12</volume><issue>1</issue><fpage>26</fpage><lpage>41</lpage><comment>2008</comment><pub-id pub-id-type="pmcid">PMC2276735</pub-id><pub-id pub-id-type="pmid">17659998</pub-id><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the False Discovery Rate - a Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society Series B-Statistical Methodology</source><year>1995</year><volume>57</volume><issue>1</issue><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brooks</surname><given-names>JA</given-names></name><name><surname>Stolier</surname><given-names>RM</given-names></name><name><surname>Freeman</surname><given-names>JB</given-names></name></person-group><article-title>Computational approaches to the neuroscience of social perception</article-title><source>Soc Cogn Affect Neurosci</source><year>2020</year><month>Sep</month><day>28</day><pub-id pub-id-type="pmcid">PMC8343569</pub-id><pub-id pub-id-type="pmid">32986115</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsaa127</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosch</surname><given-names>T</given-names></name><name><surname>Bar-David</surname><given-names>E</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><article-title>Implicit race bias decreases the similarity of neural representations of black and white faces</article-title><source>Psychol Sci</source><year>2013</year><month>Feb</month><day>1</day><volume>24</volume><issue>2</issue><fpage>160</fpage><lpage>166</lpage><pub-id pub-id-type="pmcid">PMC3864653</pub-id><pub-id pub-id-type="pmid">23300228</pub-id><pub-id pub-id-type="doi">10.1177/0956797612451465</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>RM</given-names></name><name><surname>Huettel</surname><given-names>SA</given-names></name></person-group><article-title>A nexus model of the temporal-parietal junction</article-title><source>Trends Cogn Sci</source><year>2013</year><month>Jul</month><volume>17</volume><issue>7</issue><fpage>328</fpage><lpage>336</lpage><pub-id pub-id-type="pmcid">PMC3750983</pub-id><pub-id pub-id-type="pmid">23790322</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.05.007</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanna</surname><given-names>AE</given-names></name><name><surname>Trimble</surname><given-names>MR</given-names></name></person-group><article-title>The precuneus: a review of its functional anatomy and behavioural correlates</article-title><source>Brain</source><year>2006</year><month>Mar</month><volume>129</volume><issue>Pt 3</issue><fpage>564</fpage><lpage>583</lpage><pub-id pub-id-type="pmid">16399806</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>LJ</given-names></name><name><surname>Jolly</surname><given-names>E</given-names></name><name><surname>Cheong</surname><given-names>JH</given-names></name><name><surname>Rapuano</surname><given-names>KM</given-names></name><name><surname>Greenstein</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>PA</given-names></name><name><surname>Manning</surname><given-names>JR</given-names></name></person-group><article-title>Endogenous variation in ventromedial prefrontal cortex state dynamics during naturalistic viewing reflects affective experience</article-title><source>Sci Adv</source><year>2021</year><month>Apr</month><volume>7</volume><issue>17</issue><pub-id pub-id-type="pmcid">PMC8064646</pub-id><pub-id pub-id-type="pmid">33893106</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abf7129</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Taylor</surname><given-names>PA</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>Is the statistic value all we should care about in neuroimaging?</article-title><source>Neuroimage</source><year>2017</year><month>02</month><day>15</day><volume>147</volume><fpage>952</fpage><lpage>959</lpage><comment>2017</comment><pub-id pub-id-type="pmcid">PMC6591724</pub-id><pub-id pub-id-type="pmid">27729277</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.09.066</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>AFNI: Software for Analysis and Visualization of Functional Magnetic Resonance Neuroimages</article-title><source>Computers and Biomedical Research</source><year>1996</year><month>06</month><day>01</day><volume>29</volume><issue>3</issue><fpage>162</fpage><lpage>173</lpage><comment>1996</comment><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cukur</surname><given-names>T</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Attention during natural vision warps semantic representation across the human brain</article-title><source>Nat Neurosci</source><year>2013</year><month>Jun</month><volume>16</volume><issue>6</issue><fpage>763</fpage><lpage>770</lpage><pub-id pub-id-type="pmcid">PMC3929490</pub-id><pub-id pub-id-type="pmid">23603707</pub-id><pub-id pub-id-type="doi">10.1038/nn.3381</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Da Costa</surname><given-names>S</given-names></name><name><surname>van der Zwaag</surname><given-names>W</given-names></name><name><surname>Marques</surname><given-names>JP</given-names></name><name><surname>Frackowiak</surname><given-names>RS</given-names></name><name><surname>Clarke</surname><given-names>S</given-names></name><name><surname>Saenz</surname><given-names>M</given-names></name></person-group><article-title>Human primary auditory cortex follows the shape of Heschl’s gyrus</article-title><source>J Neurosci</source><year>2011</year><month>Oct</month><day>5</day><volume>31</volume><issue>40</issue><fpage>14067</fpage><lpage>14075</lpage><pub-id pub-id-type="pmcid">PMC6623669</pub-id><pub-id pub-id-type="pmid">21976491</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2000-11.2011</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Sereno</surname><given-names>MI</given-names></name></person-group><article-title>Cortical Surface-Based Analysis: I. Segmentation and Surface Reconstruction</article-title><source>Neuroimage</source><year>1999</year><month>02</month><day>01</day><volume>9</volume><issue>2</issue><fpage>179</fpage><lpage>194</lpage><comment>1999</comment><pub-id pub-id-type="pmid">9931268</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de la Vega</surname><given-names>A</given-names></name><name><surname>Chang</surname><given-names>LJ</given-names></name><name><surname>Banich</surname><given-names>MT</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><article-title>Large-Scale Meta-Analysis of Human Medial Frontal Cortex Reveals Tripartite Functional Organization</article-title><source>J Neurosci</source><year>2016</year><month>Jun</month><day>15</day><volume>36</volume><issue>24</issue><fpage>6553</fpage><lpage>6562</lpage><pub-id pub-id-type="pmcid">PMC5015787</pub-id><pub-id pub-id-type="pmid">27307242</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4402-15.2016</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname><given-names>B</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Functional Organization of Social Perception and Cognition in the Superior Temporal Sulcus</article-title><source>Cereb Cortex</source><year>2015</year><month>Nov</month><volume>25</volume><issue>11</issue><fpage>4596</fpage><lpage>4609</lpage><pub-id pub-id-type="pmcid">PMC4816802</pub-id><pub-id pub-id-type="pmid">26048954</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhv111</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dima</surname><given-names>DC</given-names></name><name><surname>Tomita</surname><given-names>TM</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name></person-group><article-title>Social-affective features drive human representations of observed actions</article-title><source>Elife</source><year>2022</year><month>May</month><day>24</day><volume>11</volume><pub-id pub-id-type="pmcid">PMC9159752</pub-id><pub-id pub-id-type="pmid">35608254</pub-id><pub-id pub-id-type="doi">10.7554/eLife.75027</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A cortical area selective for visual processing of the human body</article-title><source>Science</source><year>2001</year><month>Sep</month><day>28</day><volume>293</volume><issue>5539</issue><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="pmid">11577239</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><etal/></person-group><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><year>2019</year><month>01</month><day>01</day><volume>16</volume><issue>1</issue><fpage>111</fpage><lpage>116</lpage><comment>2019</comment><pub-id pub-id-type="pmcid">PMC6319393</pub-id><pub-id pub-id-type="pmid">30532080</pub-id><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><article-title>A natural approach to studying vision [Article]</article-title><source>Nature Neuroscience</source><year>2005</year><month>Dec</month><volume>8</volume><issue>12</issue><fpage>1643</fpage><lpage>1646</lpage><pub-id pub-id-type="pmid">16306891</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>Neuroimage</source><year>2009</year><month>07</month><day>01</day><volume>47</volume><fpage>S102</fpage><comment>2009</comment><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funder</surname><given-names>DC</given-names></name></person-group><article-title>Towards a resolution of the personality triad: Persons, situations, and behaviors</article-title><source>Journal of Research in Personality</source><year>2006</year><volume>40</volume><issue>1</issue><fpage>21</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.jrp.2005.08.003</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberg</surname><given-names>LR</given-names></name></person-group><article-title>An alternative “description of personality”: The Big-Five factor structure</article-title><source>Journal of Personality and Social Psychology</source><year>1990</year><volume>59</volume><issue>6</issue><fpage>1216</fpage><lpage>1229</lpage><pub-id pub-id-type="pmid">2283588</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>Y</given-names></name><name><surname>Waskom</surname><given-names>M</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name></person-group><article-title>Nipype: A Flexible, Lightweight and Extensible Neuroimaging Data Processing Framework in Python [Original Research]</article-title><source>Frontiers in Neuroinformatics</source><year>2011</year><month>August</month><day>22</day><volume>5</volume><issue>13</issue><comment>2011</comment><pub-id pub-id-type="pmcid">PMC3159964</pub-id><pub-id pub-id-type="pmid">21897815</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>Neuroimage</source><year>2009</year><month>10</month><day>15</day><volume>48</volume><issue>1</issue><fpage>63</fpage><lpage>72</lpage><comment>2009</comment><pub-id pub-id-type="pmcid">PMC2733527</pub-id><pub-id pub-id-type="pmid">19573611</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Hanson</surname><given-names>SJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Pollmann</surname><given-names>S</given-names></name></person-group><article-title>PyMVPA: a Python Toolbox for Multivariate Pattern Analysis of fMRI Data</article-title><source>Neuroinformatics</source><year>2009</year><volume>7</volume><issue>1</issue><fpage>37</fpage><lpage>53</lpage><pub-id pub-id-type="pmcid">PMC2664559</pub-id><pub-id pub-id-type="pmid">19184561</pub-id><pub-id pub-id-type="doi">10.1007/s12021-008-9041-y</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harry</surname><given-names>B</given-names></name><name><surname>Williams</surname><given-names>MA</given-names></name><name><surname>Davis</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><article-title>Emotional expressions evoke a differential response in the fusiform face area</article-title><source>Front Hum Neurosci</source><year>2013</year><volume>7</volume><fpage>692</fpage><pub-id pub-id-type="pmcid">PMC3809557</pub-id><pub-id pub-id-type="pmid">24194707</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2013.00692</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Baumgaertner</surname><given-names>A</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Koehnke</surname><given-names>M</given-names></name><name><surname>Ulmer</surname><given-names>S</given-names></name><name><surname>Siebner</surname><given-names>HR</given-names></name></person-group><article-title>Phonological decisions require both the left and right supramarginal gyri</article-title><source>Proc Natl Acad Sci U S A</source><year>2010</year><month>Sep</month><day>21</day><volume>107</volume><issue>38</issue><fpage>16494</fpage><lpage>16499</lpage><pub-id pub-id-type="pmcid">PMC2944751</pub-id><pub-id pub-id-type="pmid">20807747</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1008121107</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name></person-group><article-title>Reliability of cortical activity during natural stimulation</article-title><source>Trends Cogn Sci</source><year>2010</year><month>Jan</month><volume>14</volume><issue>1</issue><fpage>40</fpage><lpage>48</lpage><pub-id pub-id-type="pmcid">PMC2818432</pub-id><pub-id pub-id-type="pmid">20004608</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2009.10.011</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>Intersubject synchronization of cortical activity during natural vision</article-title><source>Science</source><year>2004</year><month>Mar</month><day>12</day><volume>303</volume><issue>5664</issue><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="pmid">15016991</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><year>2001</year><month>Sep</month><day>28</day><volume>293</volume><issue>5539</issue><fpage>2425</fpage><lpage>2430</lpage><pub-id pub-id-type="pmid">11577229</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Hoffman</surname><given-names>EA</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><article-title>The distributed human neural system for face perception</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>6</issue><fpage>223</fpage><lpage>233</lpage><pub-id pub-id-type="pmid">10827445</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Heikkilä</surname><given-names>TT</given-names></name><name><surname>Laine</surname><given-names>O</given-names></name><name><surname>Savela</surname><given-names>J</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><source>Onni: An online experiment platform for research</source><publisher-name>Zenodo</publisher-name><year>2020</year><month>December</month><day>4</day><comment>Version 1.0</comment></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoerl</surname><given-names>AE</given-names></name><name><surname>Kennard</surname><given-names>RW</given-names></name></person-group><article-title>Ridge Regression: Biased Estimation for Nonorthogonal Problems</article-title><source>Technometrics</source><year>1970</year><month>02</month><day>01</day><volume>12</volume><issue>1</issue><fpage>55</fpage><lpage>67</lpage><comment>1970</comment><pub-id pub-id-type="doi">10.1080/00401706.1970.10488634</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>de Heer</surname><given-names>WA</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><year>2016</year><month>Apr</month><day>28</day><volume>532</volume><issue>7600</issue><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="pmcid">PMC4852309</pub-id><pub-id pub-id-type="pmid">27121839</pub-id><pub-id pub-id-type="doi">10.1038/nature17637</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><year>2012</year><month>Dec</month><day>20</day><volume>76</volume><issue>6</issue><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="pmcid">PMC3556488</pub-id><pub-id pub-id-type="pmid">23259955</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Beeler</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Perceiving social interactions in the posterior superior temporal sulcus</article-title><source>Proc Natl Acad Sci U S A</source><year>2017</year><month>Oct</month><day>24</day><volume>114</volume><issue>43</issue><fpage>E9145</fpage><lpage>E9152</lpage><pub-id pub-id-type="pmcid">PMC5664556</pub-id><pub-id pub-id-type="pmid">29073111</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1714471114</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images</article-title><source>Neuroimage</source><year>2002</year><month>10</month><day>01</day><volume>17</volume><issue>2</issue><fpage>825</fpage><lpage>841</lpage><comment>2002</comment><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karjalainen</surname><given-names>T</given-names></name><name><surname>Karlsson</surname><given-names>HK</given-names></name><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Nuutila</surname><given-names>P</given-names></name><name><surname>Jaaskelainen</surname><given-names>IP</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><article-title>Dissociable Roles of Cerebral mu-Opioid and Type 2 Dopamine Receptors in Vicarious Pain: A Combined PET-fMRI Study</article-title><source>Cereb Cortex</source><year>2017</year><month>Aug</month><day>1</day><volume>27</volume><issue>8</issue><fpage>4257</fpage><lpage>4266</lpage><pub-id pub-id-type="pmid">28541428</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karjalainen</surname><given-names>T</given-names></name><name><surname>Seppala</surname><given-names>K</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Karlsson</surname><given-names>HK</given-names></name><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Nuutila</surname><given-names>P</given-names></name><name><surname>Jaaskelainen</surname><given-names>IP</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><article-title>Opioidergic Regulation of Emotional Arousal: A Combined PET-fMRI Study</article-title><source>Cereb Cortex</source><year>2019</year><month>Aug</month><day>14</day><volume>29</volume><issue>9</issue><fpage>4006</fpage><lpage>4016</lpage><pub-id pub-id-type="pmid">30475982</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kauppi</surname><given-names>JP</given-names></name><name><surname>Pajula</surname><given-names>J</given-names></name><name><surname>Tohka</surname><given-names>J</given-names></name></person-group><article-title>A versatile software package for inter-subject correlation based analyses of fMRI</article-title><source>Front Neuzoin/Orm</source><year>2014</year><volume>8</volume><fpage>2</fpage><pub-id pub-id-type="pmcid">PMC3907702</pub-id><pub-id pub-id-type="pmid">24550818</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00002</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Bao</surname><given-names>FS</given-names></name><name><surname>Giard</surname><given-names>J</given-names></name><name><surname>Häme</surname><given-names>Y</given-names></name><name><surname>Stavsky</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>N</given-names></name><name><surname>Rossa</surname><given-names>B</given-names></name><name><surname>Reuter</surname><given-names>M</given-names></name><name><surname>Chaibub Neto</surname><given-names>E</given-names></name><name><surname>Keshavan</surname><given-names>A</given-names></name></person-group><article-title>Mindboggling morphometry of human brains</article-title><source>PLOS Computational Biology</source><year>2017</year><volume>13</volume><issue>2</issue><elocation-id>e1005350</elocation-id><pub-id pub-id-type="pmcid">PMC5322885</pub-id><pub-id pub-id-type="pmid">28231282</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005350</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenigs</surname><given-names>M</given-names></name><name><surname>Barbey</surname><given-names>AK</given-names></name><name><surname>Postle</surname><given-names>BR</given-names></name><name><surname>Grafman</surname><given-names>J</given-names></name></person-group><article-title>Superior parietal cortex is critical for the manipulation of information in working memory</article-title><source>J Neurosci</source><year>2009</year><month>Nov</month><day>25</day><volume>29</volume><issue>47</issue><fpage>14980</fpage><lpage>14986</lpage><pub-id pub-id-type="pmcid">PMC2799248</pub-id><pub-id pub-id-type="pmid">19940193</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3706-09.2009</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koide-Majima</surname><given-names>N</given-names></name><name><surname>Nakai</surname><given-names>T</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name></person-group><article-title>Distinct dimensions of emotion in the human brain and their representation on the cortical surface</article-title><source>Neuroimage</source><year>2020</year><month>Nov</month><day>15</day><volume>222</volume><elocation-id>117258</elocation-id><pub-id pub-id-type="pmid">32798681</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koo</surname><given-names>TK</given-names></name><name><surname>Li</surname><given-names>MY</given-names></name></person-group><article-title>A Guideline of Selecting and Reporting Intraclass Correlation Coefficients for Reliability Research</article-title><source>Journal of Chiropractic Medicine</source><year>2016</year><month>06</month><day>01</day><volume>15</volume><issue>2</issue><fpage>155</fpage><lpage>163</lpage><comment>2016</comment><pub-id pub-id-type="pmcid">PMC4913118</pub-id><pub-id pub-id-type="pmid">27330520</pub-id><pub-id pub-id-type="doi">10.1016/j.jcm.2016.02.012</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Jaaskelainen</surname><given-names>IP</given-names></name><name><surname>Hyona</surname><given-names>J</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><article-title>Synchronous brain activity across individuals underlies shared psychological perspectives</article-title><source>Neuroimage</source><year>2014</year><month>Oct</month><day>15</day><volume>100</volume><fpage>316</fpage><lpage>324</lpage><pub-id pub-id-type="pmcid">PMC4153812</pub-id><pub-id pub-id-type="pmid">24936687</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.06.022</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Salmi</surname><given-names>J</given-names></name><name><surname>Jaaskelainen</surname><given-names>IP</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><article-title>Naturalistic FMRI mapping reveals superior temporal sulcus as the hub for the distributed brain network for social perception</article-title><source>Front Hum Neurosci</source><year>2012</year><volume>6</volume><fpage>233</fpage><pub-id pub-id-type="pmcid">PMC3417167</pub-id><pub-id pub-id-type="pmid">22905026</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00233</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>K</given-names></name><name><surname>Ashton</surname><given-names>MC</given-names></name></person-group><article-title>Psychometric Properties of the HEXACO Personality Inventory</article-title><source>Multivariate Behav Res</source><year>2004</year><month>Apr</month><day>1</day><volume>39</volume><issue>2</issue><fpage>329</fpage><lpage>358</lpage><pub-id pub-id-type="pmid">26804579</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lewin</surname><given-names>K</given-names></name></person-group><source>Principles of topological psychology</source><publisher-name>McGraw-Hill</publisher-name><year>1936</year><pub-id pub-id-type="doi">10.1037/10019-000</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lingnau</surname><given-names>A</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>The lateral occipitotemporal cortex in action</article-title><source>Trends Cogn Sci</source><year>2015</year><month>May</month><volume>19</volume><issue>5</issue><fpage>268</fpage><lpage>277</lpage><pub-id pub-id-type="pmid">25843544</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGraw</surname><given-names>KO</given-names></name><name><surname>Wong</surname><given-names>SP</given-names></name></person-group><article-title>Forming inferences about some intraclass correlation coefficients</article-title><source>Psychological Methods</source><year>1996</year><volume>1</volume><issue>1</issue><fpage>30</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1037/1082-989x.1.1.30</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Calder</surname><given-names>AJ</given-names></name></person-group><article-title>Neural mechanisms of social attention [Review]</article-title><source>Trends in Cognitive Sciences</source><year>2009</year><month>Mar</month><volume>13</volume><issue>3</issue><fpage>135</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">19223221</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Hyönä</surname><given-names>J</given-names></name><name><surname>Calvo</surname><given-names>MG</given-names></name></person-group><article-title>Semantic categorization precedes affective evaluation of visual scenes</article-title><source>Journal of Experimental Psychology: General</source><year>2010</year><volume>139</volume><issue>2</issue><fpage>222</fpage><lpage>246</lpage><pub-id pub-id-type="pmid">20438250</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Lukkarinen</surname><given-names>L</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name><name><surname>Putkinen</surname><given-names>V</given-names></name><name><surname>Seppala</surname><given-names>K</given-names></name><name><surname>Karjalainen</surname><given-names>T</given-names></name><name><surname>Karlsson</surname><given-names>HK</given-names></name><name><surname>Hudson</surname><given-names>M</given-names></name><name><surname>Venetjoki</surname><given-names>N</given-names></name><name><surname>Salomaa</surname><given-names>M</given-names></name><name><surname>Rautio</surname><given-names>P</given-names></name><etal/></person-group><article-title>Brain Basis of Psychopathy in Criminal Offenders and General Population</article-title><source>Cereb Cortex</source><year>2021</year><month>Jul</month><day>29</day><volume>31</volume><issue>9</issue><fpage>4104</fpage><lpage>4114</lpage><pub-id pub-id-type="pmcid">PMC8328218</pub-id><pub-id pub-id-type="pmid">33834203</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab072</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Saarimaki</surname><given-names>H</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Gotsopoulos</surname><given-names>A</given-names></name><name><surname>Jaaskelainen</surname><given-names>IP</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name></person-group><article-title>Emotional speech synchronizes brains across listeners and engages large-scale dynamic brain networks</article-title><source>Neuroimage</source><year>2014</year><month>Nov</month><day>15</day><volume>102</volume><issue>Pt 2</issue><fpage>498</fpage><lpage>509</lpage><pub-id pub-id-type="pmcid">PMC4229500</pub-id><pub-id pub-id-type="pmid">25128711</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.063</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Olivier</surname><given-names>Lartillot</given-names></name><name><surname>Toiviainen</surname><given-names>P</given-names></name></person-group><source>A Matlab Toolbox for Musical Feature Extraction From Audio</source><conf-name>International Conference on Digital Audio Effects</conf-name><year>2007</year></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Tipper</surname><given-names>SP</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>Viewpoint (in)dependence of action representations: an MVPA study</article-title><source>J Cogn Neurosci</source><year>2012</year><month>Apr</month><volume>24</volume><issue>4</issue><fpage>975</fpage><lpage>989</lpage><pub-id pub-id-type="pmid">22264198</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parrigon</surname><given-names>S</given-names></name><name><surname>Woo</surname><given-names>SE</given-names></name><name><surname>Tay</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name></person-group><article-title>CAPTION-ing the situation: A lexically-derived taxonomy of psychological situation characteristics</article-title><source>J Pers Soc Psychol</source><year>2017</year><month>Apr</month><volume>112</volume><issue>4</issue><fpage>642</fpage><lpage>681</lpage><pub-id pub-id-type="pmid">27537274</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Popham</surname><given-names>SF</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Bilenko</surname><given-names>NY</given-names></name><name><surname>Deniz</surname><given-names>F</given-names></name><name><surname>Gao</surname><given-names>JS</given-names></name><name><surname>Nunez-Elizalde</surname><given-names>AO</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Visual and linguistic semantic representations are aligned at the border of human visual cortex</article-title><source>Nat Neurosi</source><year>2021</year><month>Nov</month><volume>24</volume><issue>11</issue><fpage>1628</fpage><lpage>1636</lpage><pub-id pub-id-type="pmid">34711960</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Price</surname><given-names>CJ</given-names></name></person-group><article-title>A review and synthesis of the first 20 years of PET and fMRI studies of heard speech, spoken language and reading</article-title><source>Neuroimage</source><year>2012</year><month>Aug</month><day>15</day><volume>62</volume><issue>2</issue><fpage>816</fpage><lpage>847</lpage><pub-id pub-id-type="pmcid">PMC3398395</pub-id><pub-id pub-id-type="pmid">22584224</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.04.062</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pruim</surname><given-names>RHR</given-names></name><name><surname>Mennes</surname><given-names>M</given-names></name><name><surname>van Rooij</surname><given-names>D</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Buitelaar</surname><given-names>JK</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><article-title>ICA-AROMA: A robust ICA-based strategy for removing motion artifacts from fMRI data</article-title><source>Neuroimage</source><year>2015</year><month>05</month><day>15</day><volume>112</volume><fpage>267</fpage><lpage>277</lpage><comment>2015</comment><pub-id pub-id-type="pmid">25770991</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauthmann</surname><given-names>JF</given-names></name><name><surname>Gallardo-Pujol</surname><given-names>D</given-names></name><name><surname>Guillaume</surname><given-names>EM</given-names></name><name><surname>Todd</surname><given-names>E</given-names></name><name><surname>Nave</surname><given-names>CS</given-names></name><name><surname>Sherman</surname><given-names>RA</given-names></name><name><surname>Ziegler</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>AB</given-names></name><name><surname>Funder</surname><given-names>DC</given-names></name></person-group><article-title>The Situational Eight DIAMONDS: a taxonomy of major dimensions of situation characteristics</article-title><source>J Pers Soc Psychol</source><year>2014</year><month>Oct</month><volume>107</volume><issue>4</issue><fpage>677</fpage><lpage>718</lpage><pub-id pub-id-type="pmid">25133715</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name></person-group><article-title>Implementation of a new parcellation of the orbitofrontal cortex in the automated anatomical labeling atlas</article-title><source>Neuroimage</source><year>2015</year><month>Nov</month><day>15</day><volume>122</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">26241684</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>JA</given-names></name></person-group><article-title>A Circumplex Model of Affect</article-title><source>Journal of Personality and Social Psychology</source><year>1980</year><volume>39</volume><issue>6</issue><fpage>1161</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1037/h0077714</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Said</surname><given-names>CP</given-names></name><name><surname>Moore</surname><given-names>CD</given-names></name><name><surname>Engell</surname><given-names>AD</given-names></name><name><surname>Todorov</surname><given-names>A</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Distributed representations of dynamic facial expressions in the superior temporal sulcus</article-title><source>J Vis</source><year>2010</year><month>May</month><day>1</day><volume>10</volume><issue>5</issue><fpage>11</fpage><pub-id pub-id-type="pmcid">PMC3061045</pub-id><pub-id pub-id-type="pmid">20616141</pub-id><pub-id pub-id-type="doi">10.1167/10.5.11</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>People thinking about thinking people. The role of the temporo-parietal junction in “theory of mind”</article-title><source>Neuroimage</source><year>2003</year><volume>19</volume><issue>4</issue><fpage>1835</fpage><lpage>1842</lpage><pub-id pub-id-type="pmid">12948738</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simms</surname><given-names>LJ</given-names></name></person-group><article-title>The big seven model of personality and its relevance to personality pathology</article-title><source>J Pers</source><year>2007</year><month>Feb</month><volume>75</volume><issue>1</issue><fpage>65</fpage><lpage>94</lpage><pub-id pub-id-type="pmid">17214592</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smirnov</surname><given-names>D</given-names></name><name><surname>Lachat</surname><given-names>F</given-names></name><name><surname>Peltola</surname><given-names>T</given-names></name><name><surname>Lahnakoski</surname><given-names>JM</given-names></name><name><surname>Koistinen</surname><given-names>O-P</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name><name><surname>Nummenmaa</surname><given-names>L</given-names></name></person-group><article-title>Brain-to-brain hyperclassification reveals action-specific motor mapping of observed actions in humans</article-title><source>Plos One</source><year>2017</year><volume>12</volume><issue>12</issue><elocation-id>e0189508</elocation-id><pub-id pub-id-type="pmcid">PMC5724834</pub-id><pub-id pub-id-type="pmid">29228054</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0189508</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoeckel</surname><given-names>C</given-names></name><name><surname>Gough</surname><given-names>PM</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Devlin</surname><given-names>JT</given-names></name></person-group><article-title>Supramarginal gyrus involvement in visual word recognition</article-title><source>Cortex</source><year>2009</year><month>Oct</month><volume>45</volume><issue>9</issue><fpage>1091</fpage><lpage>1096</lpage><pub-id pub-id-type="pmcid">PMC2726132</pub-id><pub-id pub-id-type="pmid">19232583</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2008.12.004</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarhan</surname><given-names>L</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Sociality and interaction envelope organize visual action representations</article-title><source>Nat Commun</source><year>2020</year><month>Jun</month><day>12</day><volume>11</volume><issue>1</issue><elocation-id>3002</elocation-id><pub-id pub-id-type="pmcid">PMC7293348</pub-id><pub-id pub-id-type="pmid">32532982</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-16846-w</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thornton</surname><given-names>MA</given-names></name><name><surname>Tamir</surname><given-names>DI</given-names></name></person-group><article-title>Six dimensions describe action understanding: The ACT-FASTaxonomy</article-title><source>Journal of Personality and Social Psychology</source><year>2022</year><volume>122</volume><issue>4</issue><fpage>577</fpage><lpage>605</lpage><pub-id pub-id-type="pmcid">PMC8901456</pub-id><pub-id pub-id-type="pmid">34591540</pub-id><pub-id pub-id-type="doi">10.1037/pspa0000286</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>Pratte</surname><given-names>MS</given-names></name></person-group><article-title>Decoding patterns of human brain activity</article-title><source>Annu Rev Psychol</source><year>2012</year><volume>63</volume><fpage>483</fpage><lpage>509</lpage><pub-id pub-id-type="pmcid">PMC7869795</pub-id><pub-id pub-id-type="pmid">21943172</pub-id><pub-id pub-id-type="doi">10.1146/annurev-psych-120710-100412</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tucciarelli</surname><given-names>R</given-names></name><name><surname>Wurm</surname><given-names>M</given-names></name><name><surname>Baccolo</surname><given-names>E</given-names></name><name><surname>Lingnau</surname><given-names>A</given-names></name></person-group><article-title>The representational space of observed actions</article-title><source>Elife</source><year>2019</year><month>Dec</month><day>5</day><volume>8</volume><pub-id pub-id-type="pmcid">PMC6894926</pub-id><pub-id pub-id-type="pmid">31804177</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47686</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><article-title>N4ITK: Improved N3 Bias Correction</article-title><source>IEEE Transactions on Medical Imaging</source><year>2010</year><volume>29</volume><issue>6</issue><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="pmcid">PMC3071855</pub-id><pub-id pub-id-type="pmid">20378467</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><article-title>Cortical cartography and Caret software</article-title><source>Neuroimage</source><year>2012</year><month>Aug</month><day>15</day><volume>62</volume><issue>2</issue><fpage>757</fpage><lpage>764</lpage><pub-id pub-id-type="pmcid">PMC3288593</pub-id><pub-id pub-id-type="pmid">22062192</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.10.077</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walbrin</surname><given-names>J</given-names></name><name><surname>Downing</surname><given-names>P</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name></person-group><article-title>Neural responses to visually observed social interactions</article-title><source>Neuropsychologia</source><year>2018</year><month>Apr</month><volume>112</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="pmcid">PMC5899757</pub-id><pub-id pub-id-type="pmid">29476765</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.02.023</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wegrzyn</surname><given-names>M</given-names></name><name><surname>Riehle</surname><given-names>M</given-names></name><name><surname>Labudda</surname><given-names>K</given-names></name><name><surname>Woermann</surname><given-names>F</given-names></name><name><surname>Baumgartner</surname><given-names>F</given-names></name><name><surname>Pollmann</surname><given-names>S</given-names></name><name><surname>Bien</surname><given-names>CG</given-names></name><name><surname>Kissler</surname><given-names>J</given-names></name></person-group><article-title>Investigating the brain basis of facial expression perception using multivoxel pattern analysis</article-title><source>Cortex</source><year>2015</year><month>Aug</month><volume>69</volume><fpage>131</fpage><lpage>140</lpage><pub-id pub-id-type="pmid">26046623</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkerson</surname><given-names>MD</given-names></name><name><surname>Hayes</surname><given-names>DN</given-names></name></person-group><article-title>ConsensusClusterPlus: a class discovery tool with confidence assessments and item tracking</article-title><source>Bioinformatics</source><year>2010</year><month>Jun</month><day>15</day><volume>26</volume><issue>12</issue><fpage>1572</fpage><lpage>1573</lpage><pub-id pub-id-type="pmcid">PMC2881355</pub-id><pub-id pub-id-type="pmid">20427518</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btq170</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname><given-names>J</given-names></name><name><surname>Todorov</surname><given-names>A</given-names></name></person-group><article-title>First impressions: making up your mind after a 100-ms exposure to a face</article-title><source>Psychol Sci</source><year>2006</year><month>Jul</month><volume>17</volume><issue>7</issue><fpage>592</fpage><lpage>598</lpage><pub-id pub-id-type="pmid">16866745</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Ridgway</surname><given-names>GR</given-names></name><name><surname>Webster</surname><given-names>MA</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><article-title>Permutation inference for the general linear model</article-title><source>Neuroimage</source><year>2014</year><month>05</month><day>15</day><volume>92</volume><fpage>381</fpage><lpage>397</lpage><comment>2014</comment><pub-id pub-id-type="pmcid">PMC4010955</pub-id><pub-id pub-id-type="pmid">24530839</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.01.060</pub-id></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Lateral occipitotemporal cortex encodes perceptual components of social actions rather than abstract representations of sociality</article-title><source>Neuroimage</source><year>2019</year><month>Nov</month><day>15</day><volume>202</volume><elocation-id>116153</elocation-id><pub-id pub-id-type="pmid">31491524</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Lingnau</surname><given-names>A</given-names></name></person-group><article-title>Action Categories in Lateral Occipitotemporal Cortex Are Organized Along Sociality and Transitivity</article-title><source>J Neurosci</source><year>2017</year><month>Jan</month><day>18</day><volume>37</volume><issue>3</issue><fpage>562</fpage><lpage>575</lpage><pub-id pub-id-type="pmcid">PMC6596756</pub-id><pub-id pub-id-type="pmid">28100739</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1717-16.2016</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurm</surname><given-names>MF</given-names></name><name><surname>Lingnau</surname><given-names>A</given-names></name></person-group><article-title>Decoding actions at different levels of abstraction</article-title><source>J Neurosci</source><year>2015</year><month>May</month><day>20</day><volume>35</volume><issue>20</issue><fpage>7727</fpage><lpage>7735</lpage><pub-id pub-id-type="pmcid">PMC6795187</pub-id><pub-id pub-id-type="pmid">25995462</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0188-15.2015</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><year>2001</year><volume>20</volume><issue>1</issue><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The results of hierarchical clustering of reliably rated social features.</title><p>The correlation matrix is ordered hierarchically, and clustering results (k=13) are shown. Hierarchical clustering analysis identified that social perceptual space of the stimulus can be reduced to six clusters and seven individual features.</p></caption><graphic xlink:href="EMS157191-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Brain regions showing increased BOLD activity for the social dimensions.</title><p>Results show the voxelwise T-values (FDR-corrected, q = 0.05) of increased BOLD activity for each social dimension from the multiple regression analysis. The results are also visualized as cortical inflation in <xref ref-type="supplementary-material" rid="SD1">Figure SI-7</xref>.</p></caption><graphic xlink:href="EMS157191-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Regional results from the multiple regression analysis.</title><p>A heatmap shows T-values for regression coefficients in each ROI and perceptual dimension. Statistically significant (p &lt; 0.05, Bonferroni-corrected for each dimension independently) ROIs are marked with an asterisk.</p></caption><graphic xlink:href="EMS157191-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>(a) Cumulative activation map for social dimensions. Voxel intensities indicate how many social dimensions (out of 13) activated the voxel statistically significantly (FDR-corrected, q = 0.05). White lines indicate the localizations of major gyri. (b) Significant ISC (FDR-corrected, q = 0.05) across subjects over the whole experiment (c) Scatterplot showing the association between ISC and tuning for social perceptual dimensions. ISC is plotted in Y-axis and the X-axis shows how many social dimensions (out of 13) associated significantly with BOLD response. Regional values are calculated as the average over all regional voxels. CARET software (<xref ref-type="bibr" rid="R77">Van Essen, 2012</xref>) was used for mapping results from ICBM 152 Nonlinear Asymmetrical template version 2009c space to the flatmap surface.</p></caption><graphic xlink:href="EMS157191-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Results from the multivariate pattern analysis of social dimensions.</title><p>(a) Whole brain classification accuracies and the localization of ANOVA selected voxels used in the whole brain classification analysis. (b) Regional classification accuracies compared with the whole brain classification accuracy (black). The permuted chance level accuracy (acc = 0.128) is shown as a horizontal line. The mean prediction accuracy was significantly (p&lt;0.01) above the chance level accuracy in the whole brain analysis and for each region-of-interest. (c) Regional classification accuracies using only 119 voxels (the size of the smallest region) with the highest F-scores as input for the classifier.</p></caption><graphic xlink:href="EMS157191-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>(a) Scatterplot showing the relationship between regional ISC and classification accuracy. (b) Additive RGB map summarizing the main findings. The overlap between activation patterns for perceptual dimensions in regression analysis is shown as blue (areas where at least 3 dimensions expressed FDR-corrected brain activation). Significant (FDR-corrected, q = 0.05) ISC across subjects is shown as red and the ANOVA selected voxels for the whole brain classification are shown as green.</p></caption><graphic xlink:href="EMS157191-f006"/></fig></floats-group></article>