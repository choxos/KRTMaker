<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS145926</article-id><article-id pub-id-type="doi">10.1101/2021.12.14.472546</article-id><article-id pub-id-type="archive">PPR432353</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>The contrasting shape representations that support object recognition in humans and CNNs</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Malhotra</surname><given-names>Gaurav</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Dujmovic</surname><given-names>Marin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hummel</surname><given-names>John</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bowers</surname><given-names>Jeff</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>School of Psychological Sciences, University of Bristol, Bristol, UK</aff><aff id="A2"><label>2</label>Department of Psychology, University of Illinois Urbana-Champaign, Champaign, USA</aff><author-notes><corresp id="CR1"><label>*</label> <email>gaurav.malhotra@bristol.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>10</day><month>06</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><label>1</label><p id="P1">Recently it has been argued that CNNs trained to perform object classification through supervised learning can emulate a key property of human vision, namely, they can classify objects based on their shape. However, it is unclear whether their learned shape representations are human-like. We explored this question in the context of a well-known observation from psychology showing that human shape representations encode the relations between object features. We ran a series of simulations where we trained CNNs on datasets of novel shapes and tested them on a set of controlled deformations of these shapes. We found that CNNs do not show any enhanced sensitivity to deformations which alter relations between features, even when explicitly trained on such deformations. This behaviour contrasted with human participants in previous studies as well as in a new experiment. We argue that these results are a consequence of a fundamental difference between how humans and CNNs learn to recognise objects: while CNNs select features that allow them to optimally classify the proximal stimulus, humans select features that they infer to be properties of the distal stimulus. This makes human representations more generalisable to novel contexts and tasks.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">A great deal of research into human vision is driven by the observation that visual perception is biased. For example, we prefer to group objects in a scene based on certain Gestalt principles – a bias to look for proximity, similarity, closure and continuity [<xref ref-type="bibr" rid="R10">10</xref>]. We also prefer to view objects from certain viewpoints – a bias for canonical-perspectives [<xref ref-type="bibr" rid="R39">39</xref>]. This paper is focused on one such bias – the <italic>shape-bias</italic> – the observation that humans, from a very young age, prefer to categorise objects based on their shape, rather than other prominent features such as colour, size or texture [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R6">6</xref>]. One manifestation of this bias is that we can identify most objects from line drawings as quickly and accurately as we can identify them from full-color photographs [<xref ref-type="bibr" rid="R6">6</xref>] and we can do this even if we have no previous experience with line drawings [<xref ref-type="bibr" rid="R18">18</xref>].</p><p id="P3">Two different explanations have been proposed regarding the origin of these biases. The first view proposes that these biases are an internalisation of the biases present in the environment relevant for classifying objects. According to this view, humans prefer to view objects from a canonical perspective because these perspectives are more frequent in the visual environment and they prefer to classify objects based on shape because shape is more diagnostic during object classification. In other words, biases are a consequence of performing statistical learning with the goal of optimising behaviour on a particular task. We will call this the optimisation-for-classification approach or, more briefly, the <italic>optimisation approach</italic>.</p><p id="P4">The second view, which we call the <italic>heuristic approach</italic>, proposes that biases originate because the visual system needs to transform the <italic>proximal stimulus</italic> – i.e., the retinal image – into a representation of the <italic>distal stimulus</italic> – i.e., a veridical representation of the cause of the stimulus. Of course, the simple act of transforming one representation to another should not necessarily lead to biases. But, in this case, mapping the retinal image to the distal stimulus is an ill-posed problem: there is not enough information in the proximal stimulus to unambiguously recover the properties of the distal stimulus [<xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R38">38</xref>]. To overcome this problem, the visual system makes assumptions (i.e., employs heuristics) to determine which properties of the proximal stimulus are used to build distal representations [<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R42">42</xref>]. The advantage of distal representations is that they are relevant for broad range of tasks – the same representation of an object can be used for recognition and visual reasoning [<xref ref-type="bibr" rid="R23">23</xref>] amongst other visual skills.</p><p id="P5">The goal of this study was to adjudicate which of these approaches provides a better explanation for shape-bias in human object recognition. We tested this by focusing on supervised Convolutional Neural Networks (CNNs) – which are machine learning models that recognise objects by learning statistical features of their proximal stimuli that can be used to optimally classify a each stimulus, given some training data. The learned representations that support object recognition are specialized for image classification. There is no pressure to learn representations of objects than can perform a range of tasks, let alone learn distal representations of objects. As such, CNNs trained using supervised learning to classify objects provide a concrete model to test the optimisation view. If human perceptual biases are acquired through internalising the statistics of the environment in order to classify objects, then training CNNs to perform classification on ecologically realistic datasets should lead to perceptual shape biases similar to the ones observed for humans.</p><p id="P6">Initial studies testing shape-bias in CNNs showed that CNNs trained in a supervised setting on large datasets of naturalistic images (e.g. <monospace>ImageNet</monospace>) frequently lacked a shape-bias, instead preferring to classify images based on texture [<xref ref-type="bibr" rid="R13">13</xref>] or other local features [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R35">35</xref>]. However, it has been argued that CNNs can also be trained to infer an object’s shape given the right type of training. For example, Geirhos et al. [<xref ref-type="bibr" rid="R13">13</xref>] trained standard CNNs on Style-Transfer image dataset that mixes the shape of images from one class with the texture from other classes so that only shape was diagnostic of category. CNNs trained on this dataset learned to classify objects by shape. In another study, Feinman and Lake [<xref ref-type="bibr" rid="R11">11</xref>] found CNNs were capable of learning a shape-bias based on a small set of images, as long as the training data was carefully controlled. Similarly, Hermann et al. [<xref ref-type="bibr" rid="R17">17</xref>] showed that more psychologically plausible forms of data augmentation, namely the introduction of color distortion, noise, and blur to input images, make standard CNNs rely more on shape when classifying images. Indeed, the authors found that data augmentation was more effective in inducing a shape bias than modifying the learning algorithms or architectures of networks, and concluded: “Our results indicate that apparent differences in the way humans and ImageNet-trained CNNs process images may arise not primarily from differences in their internal workings, but from differences in the data that they see”.</p><p id="P7">These results raise the possibility that human biases are indeed a consequence of internalising the statistical properties of the environment relevant to classifying objects rather than the product of heuristics involved in building distal representations of objects. But studies so far have focused on judging whether or not CNNs are able to develop a shape-bias, rather than examining the type of shape representations they acquire. If humans and CNNs indeed acquire a shape-bias through a similar process of statistical optimisation, then CNNs should not only show a shape-bias, but also develop shape representations that are similar to human shape representations.</p><p id="P8">A key finding about human shape representations is that humans do not give equal weight to all shape-related features. For example, it has been shown that human participants are more sensitive to distortions of shape that change relations between parts of objects than distortions that preserve these relations [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R26">26</xref>]. These observations have typically been taken to support a heuristic view according to which relations present in the proximal images are used to build distal representations of objects [<xref ref-type="bibr" rid="R20">20</xref>]. The question we ask is whether CNNs trained to classify objects learn to encode these relational features of shape. Across two sets of experiments we show that even though CNNs are able to categorise objects based on shape, they do this on the basis of qualitatively different shape representations to humans. In both experiments human behaviour is better explained by the heuristic approach than an optimisation approach.</p><p id="P9">The rest of the paper is divided into three sections. In the first section, we focus on objects that consist of multiple parts and, in the second section, on objects that consist of a single part. The deformations required to infer the shape representations of these two types of objects are different, but related. Therefore, we begin each section by describing these deformations and how these deformations are predicted to affect shape representations under the two (optimisation and heuristic) views. We then present results of experiments where humans and CNNs were trained on the same set of shapes and then presented these deformations. In the final section, we discuss how our findings pose a challenge for developing models of human vision.</p></sec><sec id="S2"><title>Experiment 1: multi-part objects</title><sec id="S3"><title>Proximal and distal encodings of multi-part objects</title><p id="P10">What sort of deformations of the proximal stimulus should allow us to contrast the optimisation and heuristic approaches? Specific hypotheses can be derived from the structural description theory [<xref ref-type="bibr" rid="R2">2</xref>], which assumes we build representations of distal stimuli during the process of identifying objects. On this theory, objects are represented as collections of convex parts in specific categorical spatial relations. For example, consider two objects – a bucket and a mug – both of which consist of the same parts: a curved cylinder (the handle) and a truncated cone (the body). The encoding of objects through parts and relations between parts makes it possible to support a range of visual skills. For example, it is possible to appreciate the similarity between a mug and a bucket because they both contain the same parts (curved cylinder and truncated code) as well as their differences (the different relations between the object parts). That is, the representational scheme supports visual reasoning. In addition, the parts themselves are coded so that they can be identified from a wide range of viewing conditions (e.g., invariance to scale, translation and viewing angle, as well as robustness to occlusion), allowing objects to be classified from novel poses and under degraded conditions.</p><p id="P11">Note that the reliance on categorical relations to build up distal representations of multi-part objects is a built-in assumption of the model (one of the model’s heuristics), and it leads to the first hypothesis we test, namely that image deformations that change a categorical relation between an object’s parts should have a larger impact on the object’s representation than metrically-equivalent deformations that leave the categorical relations intact (as might be produced by viewing a given object from different angles). By contrast, any model that relies only on the properties of the proximal stimulus might be expected to treat all metrically-equivalent deformations as equivalent. Such a model may learn that some distortions are more important than others in the context of specific objects, but it is unclear why they would show a general tendency to treat categorical deformations as different than metric ones since there is no heuristic that assumes that categorical relations between parts is central feature of object shape representations. (Indeed, there is no explicit encoding of parts at all.) Instead, all deformations are simply changes in the locations of features in the image.</p><p id="P12">Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] explored this question in the context of comparing structural description and view based models of human vision. They created a collection of shapes modeled on Tarr and Pinker’s (1989) simple “objects”. Each object consisted of a collection of lines connected at right angles (<xref ref-type="fig" rid="F1">Figure 1</xref>). Hummel and Stankiewicz then created two deformations of each of these <italic>Basis</italic> object. One deformation, the <italic>relational</italic> deformation (<monospace>Rel</monospace>), was identical to the Basis object from which it was created except that one line was moved so that its “above/below” relation to the line to which it was connected changed (from above to below or vice-versa). This deformation differed from the Basis object in the coordinates of one part and in the categorical relation of one part to another. The other deformation, the <italic>coordinates</italic> deformation (<monospace>Cood</monospace>), moved two lines in the Basis object in a way that preserved the categorical spatial relations between all the lines composing the object, but changed the coordinates of two lines. Note that both variants deformed the objects (proximal stimulus) but the relational variant changes categorical relations between parts of the object.</p><p id="P13">Across five experiments participants first learned to classify a set of base objects and then tested on their ability to distinguish them from their relational (<monospace>Rel</monospace> and coordinate (<monospace>Cood</monospace>) deformations. The experiments differed in the specific set of images used, the specific tasks, the duration of the stimuli, but across all experiments, participants found it easy to discriminate the <monospace>Rel</monospace> deformations from their corresponding basis object and difficult to distinguish the <monospace>Cood</monospace> deformations. The effects were not subtle. In Experiment 1 (that used the stimuli from <xref ref-type="fig" rid="F1">Figure 1</xref>) participants mistook the <monospace>Rel</monospace> and <monospace>Cood</monospace> images as the base approximately 10% and 90%, respectively, with similar findings observed across experiments. Hummel and Stankiewicz took these findings to support the claim that humans encode objects in terms of the categorical relations between their parts, consistent with the predictions of the structural description theories that propose a heuristic approach to human shape representation [<xref ref-type="bibr" rid="R20">20</xref>].</p></sec><sec id="S4"><title>Testing CNNs on the Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] stimuli</title><p id="P14">The findings of Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] provide a critical test for any model that claims to be a theory of human vision: if shape-bias in humans is a consequence of optimising over an object recognition task, then it should also lead to shape representations that are more sensitive to relational deformations than coordinate deformations. To test this hypothesis, we replicated the experimental setup of Hummel and Stankiewicz, replacing human participants with two well-known CNNs – <monospace>VGG-16</monospace> and <monospace>AlexNet</monospace> – that have been previously argued to capture human-like representations [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R31">31</xref>] and an ability to develop a shape-bias [<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R13">13</xref>].</p><p id="P15">We took CNNs that were either pre-trained on <monospace>ImageNet</monospace> (a large database of naturalistic images) or <monospace>Stylized-ImageNet</monospace> (a database specifically constructed to induce a shape-bias, see [<xref ref-type="bibr" rid="R13">13</xref>]). We then tested these networks under either a <italic>Zero-shot</italic> learning condition, where we observed the response of the pre-trained networks to the <monospace>Basis</monospace>, <monospace>Rel</monospace> and <monospace>Cood</monospace> images, or a <italic>Fine-tuned</italic> learning condition, where we further trained these pre-trained networks to categorise the <monospace>Basis</monospace> images (at variout rotations, translations and scales) and observed their response to the <monospace>Rel</monospace> and <monospace>Cood</monospace> variants (see <xref ref-type="sec" rid="S11">Methods</xref> for details).</p><p id="P16">We determined each network’s ability to distinguish between the basis shapes and their relational and coordinate deformations by computing the similarity between the internal representation for a basis shape and each type of deformation (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1.1</xref> for the networks’ classification performance). Results for <monospace>VGG-16</monospace> are shown in <xref ref-type="fig" rid="F2">Figure 2</xref> and for <monospace>AlexNet</monospace> in <xref ref-type="supplementary-material" rid="SD1">Appendix 1.4</xref>. Each panel in <xref ref-type="fig" rid="F2">Figure 2</xref> corresponds to a combination of pre-training and test conditions and shows the average cosine similarity between internal representations for a Basis image and it’s relational (<monospace>Ba-Rel</monospace>, solid red line) and coordinate (<monospace>Ba-Cood</monospace>, dashed blue line) deformations. The internal representations are computed at all convolutional and fully connected layers within the network. We compared these similarities to two baselines: the average similarity between two Basis images that belong to the same category and the average similarity between two Basis images that belong to different categories. These two baselines provide the upper and lower bounds on similarities (hatched yellow region).</p><p id="P17">We observed that in the <italic>Zero-shot</italic> condition (left-hand column in <xref ref-type="fig" rid="F2">Figure 2</xref>), the similarity between a basis image and it’s relational variant was statistically the same as the similarity between the basis image and it’s coordinate variant throughout the networks. That is, the networks failed to distinguish between the basis images and their relational and coordinate variants. In fact, networks also failed to distinguish between basis images from different categories (note the narrow hatched (yellow) region in the <italic>Zero shot</italic> condition in <xref ref-type="fig" rid="F2">Figure 2</xref> and the classification performance in <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). Thus pre-training on <monospace>ImageNet</monospace> or <monospace>Stylized-ImageNet</monospace> was not sufficient for networks to distinguish between the stimuli or their deformations used by Hummel and Stankiewicz – to these models, all line drawings are alike. In contrast, the networks successfully learned to distinguish between stimuli from different categories in the <italic>Fine-tuned</italic> condition (see classification performance in <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). Examining the internal representations showed that the networks represented all types of images in a similar manner in the early convolution layers (there is no statistical difference between similarities within or between categories in the early layers) but representations begin to separate in the deeper convolution and fully connected layers (the hatched (yellow) region increases in size as we move left to right because images from different categories have lower similarity than images from the same category). However, both types of pre-treained networks, the basis images were equally distant to their relational and coordinate deformations (see the overlapping <monospace>Ba-Rel</monospace> and <monospace>Ba-Cood</monospace> lines in <xref ref-type="fig" rid="F2">Figure 2</xref>). Note that this is not because the networks overfit to the training data. In fact, networks showed very good generalisation to both novel Basis images (in unseen combinations of rotations, translation and scale) and the two types of deformations, with the cosine distance between a basis shape and either deformation close to the upper bound of similarity (also see the high classification performance for both deformations in <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). In summary, we did not find any evidence that suggests that the CNN represents a relational change to an image in any privileged manner compared to a coordinate change.</p></sec><sec id="S5"><title>Teaching relational representations of multi-part objects</title><p id="P18">The above results suggest that training CNNs on <monospace>ImageNet</monospace> or even <monospace>Stylized-ImageNet</monospace> is not sufficient for these models to perceive the objects in terms of their categorical relations. But it could be argued that this is not because of a limitation of the optimisation approach, but due to the limitation of these datasets. It is possible that if the classification model is trained on a dataset where relational differences are important for classification, it may internalise this statistic and start perceiving objects in terms of their categorical relations, just like humans. We tested this hypothesis in the next set of simulations, where we created a training environment with a “relational bias”. We show next that when we do this, the network can learn specific changes to relations but it does not generalise this knowledge to novel (but highly similar) relational changes.</p><p id="P19">Consider the three augmented training sets shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. In each set the pre-trained networks are trained on the six Basis shapes (and their translation, rotation and scale transformations) just like in the <italic>Fine-tuned</italic> condition in the experiments above. In addition, they are also trained on five new shapes. These five shapes are the <monospace>Rel</monospace> deformations of the first five Basis shapes. In other words, the training set assigns different categories to a shape and it’s <monospace>Rel</monospace> deformation for five out of six figures. After training the networks on these eleven (5 + 5 + 1) shapes, we tested them on the <monospace>Rel</monospace> and <monospace>Cood</monospace> deformations of the final (unpaired) Basis shape.</p><p id="P20">The difference between the three datasets lies in the degree of novelty of test images. In all three datasets in <xref ref-type="fig" rid="F3">Figure 3</xref>, the same relation (dashed red circle) is changed between the unpaired Basis shape and it’s <monospace>Rel</monospace> deformation. However, in the first set, there were four other categories (two pairs, highlighted in red rectangles) in the training set where a similar change in relation occurred – that is, for all highlighted categories, there existed another category where the short red segment at the left end of the top bar flipped from “above” to “below” or vice-versa. In the second training set (Set 2 in <xref ref-type="fig" rid="F3">Figure 3</xref>) there were two categories in the training set where the tested relation changed. However, in this case, this relational change occurred in a different location (closer to central vertical line). In the third training set (Set 3 in <xref ref-type="fig" rid="F3">Figure 3</xref>), the tested relational change was the least similar to training (relational changes only occurred to the right of central vertical line for all other trained images).</p><p id="P21"><xref ref-type="fig" rid="F4">Figure 4</xref> shows the cosine similarity in internal representations for <monospace>VGG-16</monospace> trained on these three modified data sets (we obtained a similar pattern of results for <monospace>AlexNet</monospace> – see <xref ref-type="supplementary-material" rid="SD1">Figure S8</xref>). As in previous simulations, we tested networks that were either pre-trained on <monospace>ImageNet</monospace> (first row) or on <monospace>Stylized-ImageNet</monospace> (second row) and fine-tuned to each training set. We observed that when networks were trained on Set 1 (left column in <xref ref-type="fig" rid="F4">Figure 4</xref>), the cosine similarity <monospace>Ba-Rel</monospace> was lower than <monospace>Ba-Cood</monospace> in deeper layers of the CNN. That is, the networks treated the relational deformation as <italic>less</italic> similar to Basis figures than the coordinate deformations. This looks much more like the behaviour of human participants in Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]. But note that Set 1 contained two pairs of categories with the same relational change that distinguishes the tested <monospace>Rel</monospace> deformation from the corresponding Basis figure. A stronger test is provided by Set 2 that excludes the pair of categories distinguished by the critical relational change from the training set. Here, we observed that the this effect was significantly reduced (middle column in <xref ref-type="fig" rid="F4">Figure 4</xref>) – the cosine similarity <monospace>Ba-Rel</monospace> was slightly lower than <monospace>Ba-Cood</monospace> but by a much smaller degree and the difference only existed only for the networks pre-trained on <monospace>ImageNet</monospace> and only in the fully connected layers (also compare results in <xref ref-type="supplementary-material" rid="SD1">Figure S8 in Appendix</xref> for <monospace>AlexNet</monospace>, where this effect is slightly more pronounced but qualitatively similar). The strongest test for whether the network learns relational representations is provided by Set 3, where none of the categories in the training set changed the exact relation that distinguishes the <monospace>Rel</monospace> deformation from the Basis image in the test set. Here, we observed (<xref ref-type="fig" rid="F4">Figure 4</xref>, right-hand column) that the effect disappeared completely – the cosine similarity <monospace>Ba-Rel</monospace> was indistinguishable from <monospace>Ba-Cood</monospace> and both similarities were at the upper bound. All networks failed to learn that novel relational changes are more important for classification than coordinate changes even when the learning environment contained a “relational bias” – i.e., changing relations led to a change in an image’s category mapping.</p></sec></sec><sec id="S6"><title>Experiment 2: single-part objects</title><sec id="S7"><title>Deformations for testing single-part objects</title><p id="P22">As detailed above, structural description theories claim that the categorical relations between object parts are encoded in order to build distal representations of multi-part objects. But of course, in order to build distal representations of complex objects, it is also necessary to build distal representations of the parts themselves. This raises the question of what sorts of deformations of the proximal stimulus should allow us to contrast optimisation and heuristic approaches for identifying the component parts of complex objects or single-part objects? According to the structural description theory [<xref ref-type="bibr" rid="R2">2</xref>], certain shape properties of the proximal image are taken by the visual system as strong evidence that individual parts have those properties. For example, if there is a straight or parallel line in the image, the visual system infers that the part contains a straight edge or parallel edges. If the proximal stimulus is symmetrical, it is assumed that the part is symmetrical [see, for example, 43]. These (and other) shape features used to build a distal representation of the object part are called nonaccidental because they would only rarely be produced by accidental alignments of viewpoint. The visual system ignores the possibility that a given nonaccidential feature in the proximal stimulus (e.g., a straight line) is the product of an accidental alignment of eye and distal stimulus (e.g., a curved edge). That is, the human visual system uses nonaccidental proximal features as a heuristic to infer distal representations of object parts.</p><p id="P23">Critical for our purpose, many of the nonaccidental features described by Biederman [<xref ref-type="bibr" rid="R2">2</xref>] are relational features, and indeed, many of the features are associated with Gestalt rules of perceptual organization, such as good continuation, symmetry, and Pragnanz (simplicity). Accordingly, any deformations of the proximal stimulus that alter these nonaccidental features (such as disrupting symmetry) should have a larger impact on classifications than deformations that do not. By contrast, it is not clear that CNNs optimized to classify objects will encode symmetry or other relational features used to build distal representations. Accordingly, CNNs may be insensitive to deformations of symmetry or other relations present in the proximal stimulus.</p><p id="P24">With this in mind, we created set of seven symmetrical pentagons (<xref ref-type="fig" rid="F5">Figure 5(a)</xref>), and made deformations of these polygons by altering the locations of the vertices composing the polygons in a way that precisely controlled the metric change in the vertices’ locations (in the retinal image). Like Experiment 1, we created two types of deformations: (a) a coordinate deformation that parametrically varied the degree to which a polygon rotated in the visual image, vs. (b) a relational change that had an equivalent impact as the corresponding rotation, but instead introduced a shear that changed relative location of the polygon’s vertices. Although the specific manipulation is different from that used in Experiment 1, the general logic is the same: one of the deformations preserves the relations between object features while the other changes them. To a model that looks only at proximal stimulus, both deformations lead to an equivalent pixel-by-pixel change, while to a model that infers properties such as symmetry and solidity of the distal stimulus, the coordinate deformation preserves these properties while the relational deformation changes them.</p><p id="P25"><xref ref-type="fig" rid="F5">Figure 5(b)</xref> shows some examples of test images for one of the trained shapes. These test shapes are organised based on the degree and type of deformation. The degree of relational deformation (shear) of a test image increases as we move from left to right, while the degree of coordinate deformation (rotation) increases as we move from top to bottom. We can also construct test shapes that are a combination of these relational and coordinate deformations. Every shape in <xref ref-type="fig" rid="F5">Figure 5(b)</xref> is a combination of a rotation and a shear of the basis shape in the top-left corner. We have organised these test shapes based on their distance to the basis figure: all shapes along each diagonal have the same cosine distance to the basis shape.<sup><xref ref-type="fn" rid="FN1">1</xref></sup> and diagonals farther from the basis shape are at a larger distance. Thus, this method gives us a set of test shapes organised according to increasing relational and coordinate changes and matched based on the distance to the basis shape. We could now ask how accuracy degrades on this landscape of test shapes. If the visual system encodes shape as a set of diagnostic features of the proximal (retinal) image, accuracy should fall as one moves across (perpendicular to) the diagonals on the landscape. On the other hand, if the visual system encodes shape as a property of the distal stimulus, then changing internal relations should lead to a larger change in classification accuracy than an equivalent coordinate change – that is, the accuracy should fall sharply as one moves left to right along each diagonal. <xref ref-type="fig" rid="F5">Figure 5(c)</xref> shows predicted accuracy on this landscape for the two types of shape representations.</p></sec><sec id="S8"><title>Performance of CNNs</title><p id="P26">We again tested CNNs pre-trained on either <monospace>ImageNet</monospace> or <monospace>Stylized-ImageNet</monospace>. The performance of a typical CNN, here <monospace>VGG-16</monospace>, is shown in <xref ref-type="fig" rid="F6">Figure 6</xref> (we obtained a qualitatively similar pattern of results for <monospace>AlexNet</monospace>, see <xref ref-type="supplementary-material" rid="SD1">Appendix 1.4</xref>). For all networks, we observed that test accuracy was highest at the top-left corner (i.e., for the Basis shape) and reduced as the degree of relational and coordinate change was increased. Thus, unlike Experiment 1, where we were able to observe only ceiling performance for both deformations, the design of Experiment 2 allowed us to compare how performance degrades for the two types of deformations. Crucially, we observed that for most categories, accuracy decreased as a function of distance to the Basis shape (perpendicular to the diagonals), rather than relational change (left to right). In fact, for some categories accuracy <italic>improved</italic> as one moved from left to right along the diagonals. Occasionally, we observed high accuracy for large rotations on one category. This was generally due to false positives, where large rotations for all categories were classified as the same category by the network (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1.2</xref> for details). Overall, these results suggest that the network does not represent the shapes in this task in a relational manner. If it did, it’s performance on relational changes should have been a lot worse than it’s performance on relation-preserving rotations.</p><p id="P27">But classification accuracy only provides a indirect measure of internal representations. In order to get a more insight into the network’s internal representations for relational and coordinate deformations, we examined the cosine distance between internal representations for the Basis image and two test images that were equidistant from it. An example of these images is highlighted (dashed red squares) in <xref ref-type="fig" rid="F5">Figure 5(b)</xref>. These cosine similarities for two <monospace>VGG-16</monospace> networks are plotted in <xref ref-type="fig" rid="F7">Figure 7</xref> (we again obtained qualitatively similar results for <monospace>AlexNet</monospace> – see <xref ref-type="supplementary-material" rid="SD1">Figure S11 in Appendix</xref>). At all internal layers, we observed that the average similarity between a Basis image and it’s relational (shear) deformation was equal or higher than the average similarity between the Basis image and it’s coordinate (rotation) deformation (compare solid (red) and dashed (blue) lines in <xref ref-type="fig" rid="F7">Figure 7</xref>). In other words, relational deformation of an image was closer to the Basis image than it’s coordinate deformation and pre-training on the <monospace>Stylized-ImageNet</monospace> dataset to give the network a shape-bias did not change this pattern. This is the opposite of what one would expect if the network represented the stimuli in a relational manner. It may be argued that this difference between humans and CNNs may be due to a difference in experience, with humans experiences objects in different rotations. However, even when CNNs were trained to recognise objects in different rotations, we observed a qualitatively similar pattern – performance on deformations depended on the (cosine or Euclidean) distance to the corresponding Basis images, rather than selectively on the degree of relational change (see <xref ref-type="supplementary-material" rid="SD1">Appendix 1.3</xref>).</p></sec><sec id="S9"><title>Performance of human participants</title><p id="P28">The optimisation view (CNNs) and heuristic view (structural description theory) make contrasting predictions of how performance should degrade when a learned shape is deformed through rotation and shear transformations. In our next experiment, we examined which of these predictions holds for human participants.</p><p id="P29">We trained 23 participants on the same categorisation task used to train the CNNs above. Participants saw the polygons shown in <xref ref-type="fig" rid="F5">Figure 5(a)</xref> and had to learn to categorise them. Once they had learned this task, they were tested on four deformations of each Basis shape – two shears and two rotations. These deformations are marked as D1 and D2 in <xref ref-type="fig" rid="F5">Figure 5(b)</xref> (see <xref ref-type="sec" rid="S11">Methods</xref> for details).</p><p id="P30">The average accuracy of classification on each of these deformations is shown in <xref ref-type="fig" rid="F8">Figure 8</xref>. We can see that irrespective of training, <monospace>VGG-16</monospace> was more sensitive to rotation than to shear (see <xref ref-type="supplementary-material" rid="SD1">Figure S12</xref> for <monospace>AlexNet</monospace>). While performance decreases for both deformations, it decreases more rapidly for rotations. Human participants showed the opposite pattern (<xref ref-type="fig" rid="F8">Figure 8</xref>, right-hand panel). There was no significant difference in performance between the basis image and the two rotation deformations (both <italic>t</italic>(22) &lt; 3.48, <italic>p</italic> &gt; .28), while performance decreased significantly for each of shear deformations (both <italic>t</italic>(22) &gt; 14.10, <italic>p</italic> &lt; .001, <italic>d<sub>z</sub></italic> &gt; .83). The largest shear resulted in largest decrease in performance (<italic>M<sub>difference</sub></italic> = 25.87%). Thus, the behaviour of participants was in line with the prediction of structural description theories, where shape is encoded based on relations between internal parts, and in the opposite direction to the performance of the CNNs.</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P31">In two sets of experiments we have shown that CNNs and humans represent shape in qualitatively different ways as reflected in the biases they show in generalisation. In Experiment 1 we compared how CNNs and humans encode multi-part objects following deformations in the categorical relation between parts (Relational Variants) and deformations that maintained the categorical relations between parts (Coordinate Variants). Whereas humans are highly sensitive to deformations in the categorical relations between parts [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R20">20</xref>], we found that CNNs are entirely insensitive to these deformations, with performance only a function of the cosine distance between images in pixel space. Furthermore, we could not train CNNs to classify objects on the basis of the relations between parts. In Experiment 2, we compared CNNs and humans in the classification of single part objects when they were deformed by sheering (Relational) or by rotating (Coordinate) manipulations. Again, we found that humans are highly sensitive to relational deformations, whereas CNNs are only sensitive to coordinate distance, and once again, CNNs could not learn to be sensitive to relational manipulations.</p><p id="P32">These findings challenge the hypothesis that CNNs and humans perceive objects based on similar principles and that apparent differences arise due to “differences in the data that they see” [<xref ref-type="bibr" rid="R17">17</xref>]. The results of both experiments show that even CNNs that have been trained to classify objects on the basis of shape (trained on the <monospace>Stylized-ImageNet</monospace>) learn the wrong sort of shape representation. These findings add to other studies that also highlight the different types of shape representation used by CNNs and the human visual system. For example, Puebla and Bowers [<xref ref-type="bibr" rid="R44">44</xref>] have found that CNNs fail to support a simple relational judgement with shapes, namely, whether two shapes are the same or different. Again, this highlights how CNNs trained to process shape ignore relational information. In addition, Baker et al. [<xref ref-type="bibr" rid="R1">1</xref>] have shown that CNNs that classify objects based on shape focus on local features and ignore how local features relate to one another in order to encode the global structure of objects.</p><p id="P33">These failures may reflect a range of processes present in humans but absent in CNNs trained to recognise objects through supervised learning, such as figure-ground segregation, completing objects behind occluders, encoding border ownership, and inferring 3D properties about the object [<xref ref-type="bibr" rid="R43">43</xref>]. Consistent with this hypothesis, Jacob et al. [<xref ref-type="bibr" rid="R27">27</xref>] and Bowers et al. [<xref ref-type="bibr" rid="R7">7</xref>] have recently highlighted a number of these failures in CNNs, including a failure to represent 3D structure, occlusion, and parts of objects. More broadly, these results challenge the the claim that CNNs trained to recognise objects through supervised learning are good models of the ventral visual stream of human vision (see, for example, [<xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R37">37</xref>]).</p><p id="P34">One interesting study that provides some evidence to suggest that standard CNNs have similar shape representations to humans was reported by Kubilius et al. [<xref ref-type="bibr" rid="R33">33</xref>]. In one of their experiments (Experiment 3), they compared the similarity of representations in various CNNs in response to a change in metric and non-accidental features of single-part objects. For instance, they compared a base object that looked like a slightly curved brick to two objects: one object that was obtained by deforming the base object into a straight brick (a non-accidental change) and a second object that was obtained by deforming the base object into a greatly curved brick (a metric change). Kubilius et al. reported that, like humans, CNNs were more sensitive to non-accidental changes. However, it is unclear whether CNNs were more sensitive to one of their manipulations because of the non-accidental change or because of other confounds accompanying these manipulations. For example, when Kubilius et al. modified some of the base shapes to non-accidental deformations, it was accompanied by a change in local features (such as properties of vertices). Recent research [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R13">13</xref>] has shown that, unlike humans, CNNs are in fact highly sensitive to change in local and textural features and it is unclear whether it is these types changes that are driving the effects observed by Kubilius et al. [<xref ref-type="bibr" rid="R33">33</xref>]. More work is required to reconcile their findings with our own.</p><p id="P35">Of course, it is possible that training CNNs on a range of different tasks (especially tasks where the objective is to approximate the distal representation) or on tasks with different objectives rather than classification (e.g. generative modelling [<xref ref-type="bibr" rid="R29">29</xref>] or on a “self-supervised” task [<xref ref-type="bibr" rid="R16">16</xref>]) may lead to shape representations that are more similar to those formed in human visual cortex. However, here we wanted to focus on CNNs trained on recognising objects through supervised learning because of two reasons. Firstly, it has been argued that CNNs trained under these settings learn to classify objects based on human-like shape representations [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. Secondly, these models have had the largest success in predicting neural representations in human and primate visual system [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R46">46</xref>] and it has been argued that there is a “strong correlation between a model’s categorization performance and it’s ability to predict individual level IT neural unit response data” [<xref ref-type="bibr" rid="R52">52</xref>]. Our findings challenge the view that optimizing performance in a classification task can explain shape representations used during human shape perception. Instead, these findings are well predicted by the classic structural description theory of object recognition that builds a distal representation of objects using heuristics [e.g., 2].</p><p id="P36">It is also possible that a different Deep Learning architecture may be more successful than CNNs at encoding objects based on relations between their parts. Indeed, previous research indicates that relational reasoning may require a more powerful architecture that can explicitly and separately represent (i) parts and relations, and (ii) their bindings [e.g., to distinguish whether the brick is above the cone or vice-versa; <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>]. Other Deep Learning architectures such as Capsule Networks [<xref ref-type="bibr" rid="R45">45</xref>], Transformers [<xref ref-type="bibr" rid="R50">50</xref>], LSTMs [<xref ref-type="bibr" rid="R19">19</xref>] or Neural Turing machines [<xref ref-type="bibr" rid="R15">15</xref>] may also provide the representational power necessary to represent structural descriptions, but to date this has yet to be demonstrated.</p></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12"><title>Generating training and test sets</title><sec id="S13"><title>Training and test sets for Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]</title><p id="P37">We constructed six basis shapes that were identical to the shapes used by Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] in their Experiments 1–3. Each image was sized 196x196 pixels and consisted of five black line segments on a white background organised into different shapes. All images had one short (horizontal) segment at the bottom and one long (vertical) segment in the middle. This left three segments, two long, which were always horizontal, and one short, which was always vertical. The two horizontal segments could be either left-of or right-of the central vertical segment. Additionally, the short vertical segment could be attached to the left-of or the right-of the upper horizontal segment. This means that there were a total of 8 (2x2x2) possible Basis shapes. We selected six out of these to match the six shapes used by Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]. Following Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>], we constructed <monospace>Rel</monospace> (relational) deformations (called V1 variants by Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]) of each Basis shape by shifting the location of the top vertical segment, so that it’s categorical relation to the upper horizontal segment changed from “above” to “below”. Similarly, we constructed Cood (coordinate) deformations (called V2 variants by Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]) by shifting the location of <italic>both</italic> the top horizontal line and the short vertical segments together, so that the categorical relations between all the segments remained the same but the pixel distance (e.g. cosine distance) was at least as large as the pixel distance for the corresponding <monospace>Rel</monospace> deformation. Each training set contained 5000 images in each category. When no augmentation was used, all the images in each category were identical – i.e. the shape appeared at the identical location on the canvas for all images in a category. In addition, we constructed two augmented datasets, one in which the Basis image was translated to a random locations (in the range [−50, +50] pixels) on the canvas and another in which it was additionally randomly scaled <inline-formula><mml:math id="M1"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> or rotated ([−20°, +20°]).</p><p id="P38">As described above, we generated three additional datasets for teaching CNNs to recognise relational deformations on Hummel and Stankiewicz’s stimuli (see <xref ref-type="fig" rid="F3">Figure 3</xref>). Each of these training sets contained five pairs of Basis shape and one unpaired shape. Each pair consisted of a shape and it’s <monospace>Rel</monospace> deformation. The test set consisted of <monospace>Rel</monospace> and <monospace>Cood</monospace> deformations of the unpaired shape. The difference in the three datasets was the amount of overlap between the trained <monospace>Rel</monospace> deformations and tested deformation. In the first dataset, there were two pairs of Basis shapes where a <monospace>Rel</monospace> deformation was constructed by changing the same categorical relation as the one that differed between the unpaired shape and the tested <monospace>Rel</monospace> deformation. In the second set, there was only one such pair. And in the third set, none of the trained shapes differed in the tested deformation. Each training set again consisted of 5000 images, where each image was constructed by translating, scaling and rotating the Basis shape for that category. The test set consisted of 1000 images where each image was constructed by randomly translating, scaling and rotating the <monospace>Rel</monospace> and <monospace>Cood</monospace> deformations of the unpaired Basis shape.</p></sec><sec id="S14"><title>Training and test sets for polygons task</title><p id="P39">The training set for Experiment 2 consisted of seven symmetric filled pentagons, presented on a white canvas. Each category contained 5000 training images. The training set presented these polygons at different translations and scales, so it was not possible to classify them based on the position of a local feature or the area of the polygon. The difference between Basis shapes for two categories was the angles between the edges. The test set consisted of a grid of shapes that were obtained by deforming the Basis shape of the corresponding category. We used two deformations: rotation, which preserved the internal angles between edges, and shear, which changed internal angles. To shear a shape, it’s vertices were horizontally moved by a distance that depended on the vertical distance to the apex. For a vertex with coordinates (<italic>x<sub>old</sub></italic>, <italic>y<sub>old</sub></italic>), we obtained a new set of vertices, (<italic>x<sub>new</sub></italic>, <italic>y<sub>new</sub></italic>) = (<italic>x<sub>old</sub></italic> + λ(Δ<italic>y</italic>)<sup>2</sup>, <italic>y<sub>old</sub></italic>), where λ was the degree of shear and Δ<sub><italic>y</italic></sub> was the distance between <italic>y<sub>old</sub></italic> and <italic>y<sub>apex</sub></italic>, the y-coordinate of the vertex at the apex. Images could also be combination of rotations and shears. To do this, the Basis image was first sheared, then rotated. We measured the distance of a deformed image and test images were organised as shown in <xref ref-type="fig" rid="F5">Figure 5</xref>, where images in each column had the same degree of shear and images along each diagonal had the same (cosine or euclidean) distance to the Basis image. We then obtained twenty exemplars of each deformed image on the grid by randomly translating and scaling the image.</p></sec></sec><sec id="S15"><title>CNN Simulations</title><p id="P40">We evaluated two deep convolutional neural networks, <monospace>VGG-16</monospace> [<xref ref-type="bibr" rid="R47">47</xref>] and <monospace>AlexNet</monospace> [<xref ref-type="bibr" rid="R32">32</xref>] on the image classification tasks described in the Results section. We obtained qualitatively similar results for both architectures. Therefore, we focus on the results of <monospace>VGG-16</monospace> in the main text and describe the results of <monospace>AlexNet</monospace> in <xref ref-type="supplementary-material" rid="SD1">Appendix 1.4</xref>. Since human participants had a lifetime experience of classifying naturalistic objects prior to the experiment, we used network implementations that had been pre-trained on a set of naturalistic images. Two types of pre-training were used: networks were either pre-trained in the standard manner on <monospace>ImageNet</monospace>, or pre-trained on a set of images where shape was made more predictive than texture by using style-transfer [<xref ref-type="bibr" rid="R12">12</xref>]. We used networks pre-trained by Geirhos et al. [<xref ref-type="bibr" rid="R13">13</xref>], who have shown that networks trained in this manner have a greater shape-bias than networks trained on <monospace>ImageNet</monospace>. Networks were either tested in the <italic>Zero-shot</italic> condition, where no training was given on any of our datasets and we recorded the response of the pre-trained networks to the test images, or in the <italic>Fine-tuned</italic> condition, where the pre-trained network was fine-tuned to classify the 5000 images per category. Each of these images were obtained from the corresponding Basis image in the manner described above. This fine-tuning was performed in the standard manner [<xref ref-type="bibr" rid="R53">53</xref>] by replacing the last layer of the classifier to reflect the number of target classes in each dataset. The models learnt to minimise the cross-entropy error by using the Adam optimiser [<xref ref-type="bibr" rid="R28">28</xref>] with a small learning rate of 10<sup>−5</sup> and a weight-decay of 10<sup>−3</sup>. In all simulations, learning continued till the loss function had converged. To check for overfitting, we created cross-validation sets and ensured performance on training set was not higher than on the cross-validation sets. We also trained networks using standard regularization methods such as batch normalization and dropout and obtained qualitatively similar results. In most cases, the networks achieved nearly perfect classification on the training set. For the first set of experiments (Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] stimuli), the test set consisted of 1000 Basis, <monospace>Rel</monospace> and <monospace>Cood</monospace> deformations of each category. In the second set of experiments (polygons stimuli) the test set consisted of 100 exemplars (translation and scale variants) of each test image on the grid (see <xref ref-type="fig" rid="F5">Figure 5</xref>). All simulations were perfomed using the <monospace>Pytorch</monospace> framework [<xref ref-type="bibr" rid="R40">40</xref>] and we used <monospace>torchvision</monospace> implementation of all models.</p><p id="P41">To test the similarity of internal representations (<xref ref-type="fig" rid="F2">Figures 2</xref>, <xref ref-type="fig" rid="F4">4</xref> and <xref ref-type="fig" rid="F7">7</xref>), we obtained the embedding of an image at each convolution and fully connected layer of the CNN. For the first set of simulations (Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>] stimuli), we selected one (of the six) category and randomly chose 100 pairs of images from the Basis and <monospace>Rel</monospace> test set. We then computed the cosine similarity between embeddings of each pair. This gives the estimated average distance in the <monospace>Ba-Rel</monospace> condition (solid red line in <xref ref-type="fig" rid="F2">Figure 2</xref>). Similarly the cosine similarity between 100 pairs of Basis and <monospace>Cood</monospace> test images gives the <monospace>Ba-Cood</monospace> distance (dashed blue line). These distances are compared against two baseline conditions. The upper limit of similarity is given by the similarity of 100 pairs of Basis images from the same category (upper bound of the hatched yellow area in <xref ref-type="fig" rid="F2">Figure 2</xref>). The lower limit is given by the similarity of 100 pairs of Basis images from different category (in each pair, one of the images was from one category and the other from one of the other six categories). The similarity of internal reprsentations for the polygons stimuli is obtained in a similar manner. The similarity <monospace>Ba-Sh</monospace> (solid read line in <xref ref-type="fig" rid="F7">Figure 7</xref>) is estimated by measuring the average cosine similarity between embeddings of 100 pairs images from the Basis and sheared sets of the same category. Similarly, <monospace>Ba-Rot</monospace> is estimated by measuring the average cosine similarity between embeddings of 100 pairs of images from Basis and rotated sets of the same category.</p></sec><sec id="S16"><title>Behavioral experiment</title><sec id="S17" sec-type="subjects"><title>Participants</title><p id="P42">Participants (<italic>N</italic> = 37, <italic>M<sub>age</sub></italic> = 33, 70% female) with normal or corrected-to-normal vision were recruited via Prolific and the experiment was conducted on the Pavlovia platform. They were reimbursed a fixed 2 GBP and participants who proceeded to the testing phase (<italic>N</italic> = 23) had a chance to earn a bonus of up to another 2 GBP depending on performance during testing. The average payment was 8 GBP/hour. An written ethics approval for the study was obtained for the study from the University of Bristol Ethics board.</p></sec><sec id="S18"><title>Stimuli</title><p id="P43">Four categories were chosen from the total data set for the behavioral study. These are Cat 1, Cat 3, Cat 5 and Cat 7 from <xref ref-type="fig" rid="F5">Figure 5a</xref>. For the test data, we selected two deformations of each type that were matched according to the cosine distance from the basis (trained) image. For the relational deformation, these were the fifth (Deformation D1) and final (Deformation D2) shear in the top row of <xref ref-type="fig" rid="F5">Figure 5b</xref>. For the coordinate deformation, these were the fifth (D1) and final (D2) rotations in the left most column of <xref ref-type="fig" rid="F5">Figure 5b</xref>. This made up the 5 conditions in the experiment: Basis, D1 (Shear), D2 (Shear), D1 (Rotation) and D2 (Rotation). The original stimuli were 224x224 pixels but were re-scaled for each participant to 50% of the vertical resolution to account for the variability in screen size and resolution when running the study online.</p></sec><sec id="S19" sec-type="methods"><title>Procedure</title><p id="P44">Participants completed a supervised training phase in which they learned to categorize basis versions of the four categories. Each training block consisted of 40 stimuli for a total of 200 training trials (50 per category). Feedback on overall accuracy was given at the end of each block. Participants completed up to a maximum of 5 training blocks, or until they reached 85% categorization accuracy in a block. Participants who managed to reach 85% accuracy continued to the test block. The order of trials was randomised for each participant. Each trial started with a fixation cross (750 ms), then the stimulus was presented (500 ms) followed by four response buttons corresponding to the four categories (until response). After participants responded, feedback was given - CORRECT (1 s) if the response was correct, and INCORRECT with additional information about what the correct response should have been (1.5 s) if the response was incorrect.</p><p id="P45">The training phase was followed by a test phase consisting of five test blocks. Each block consisted of 20 trials for a total of 100 test trials (25 per condition). Like the training phase, the order of test trials was randomised for each participant. The procedure for each test trial was the same as in the training phase apart from the fact that participants were not given any feedback during testing.</p></sec><sec id="S20"><title>Analysis</title><p id="P46">Four planned comparisons (t-tests) were conducted in order to test whether accuracy rates in each of the shear and rotation conditions differed from accuracy in the basis condition.</p></sec></sec><sec id="S21"><title>Code and Data</title><p id="P47">All code for generating the datasets, simulating the model as well as participant data from Experiment 2 can be downloaded from: <ext-link ext-link-type="uri" xlink:href="https://github.com/gammagit/distal">https://github.com/gammagit/distal</ext-link></p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Appendix</label><media xlink:href="EMS145926-supplement-Supplementary_Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d62aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgments</title><p>This research was supported by the European Research Council Grant Generalization in Mind and Machine, ID number 741134.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P48">We obtained qualitatively similar results when deformations were organised based on their Euclidean distance to the Basis shape.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>Nicholas</given-names></name><name><surname>Lu</surname><given-names>Hongjing</given-names></name><name><surname>Erlikhman</surname><given-names>Gennady</given-names></name><name><surname>Kellman</surname><given-names>Philip J</given-names></name></person-group><article-title>Deep convolutional networks do not classify based on global object shape</article-title><source>PLoS computational biology</source><year>2018</year><volume>14</volume><issue>12</issue><elocation-id>e1006613</elocation-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>Irving</given-names></name><etal/></person-group><article-title>Recognition-by-components: a theory of human image understanding</article-title><source>Psychological review</source><year>1987</year><volume>94</volume><issue>2</issue><fpage>115</fpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>Irving</given-names></name><name><surname>Cooper</surname><given-names>Eric E</given-names></name></person-group><article-title>Evidence for complete translational and reflectional invariance in visual object priming</article-title><source>Perception</source><year>1991</year><volume>20</volume><issue>5</issue><fpage>585</fpage><lpage>593</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>Irving</given-names></name><name><surname>Cooper</surname><given-names>Eric E</given-names></name></person-group><article-title>Size invariance in visual object priming</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1992</year><volume>18</volume><issue>1</issue><fpage>121</fpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>Irving</given-names></name><name><surname>Gerhardstein</surname><given-names>Peter C</given-names></name></person-group><source>dependent mechanisms in visual object recognition: Reply to tarr and bülthoff</source><year>1995</year><issue>1995</issue></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biederman</surname><given-names>Irving</given-names></name><name><surname>Ju</surname><given-names>Ginny</given-names></name></person-group><article-title>Surface versus edge-based determinants of visual recognition</article-title><source>Cognitive psychology</source><year>1988</year><volume>20</volume><issue>1</issue><fpage>38</fpage><lpage>64</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowers</surname><given-names>Jeffrey S</given-names></name><name><surname>Malhotra</surname><given-names>Gaurav</given-names></name><name><surname>Dujmović</surname><given-names>Marin</given-names></name><name><surname>Milton Montero</surname><given-names>Llera</given-names></name><name><surname>Tsvetkov</surname><given-names>Christian</given-names></name><name><surname>Biscione</surname><given-names>Valerio</given-names></name><name><surname>Puebla</surname><given-names>Guillermo</given-names></name><name><surname>Adolfi</surname><given-names>Federico G</given-names></name><name><surname>Hummel</surname><given-names>John</given-names></name><name><surname>Rachel Heaton</surname><given-names>Flood</given-names></name><etal/></person-group><article-title>Deep problems with neural network models of human vision</article-title><year>2022</year></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadieu</surname><given-names>Charles F</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Daniel Yamins</surname><given-names>LK</given-names></name><name><surname>Pinto</surname><given-names>Nicolas</given-names></name><name><surname>Ardila</surname><given-names>Diego</given-names></name><name><surname>Solomon</surname><given-names>Ethan A</given-names></name><name><surname>Majaj</surname><given-names>Najib J</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Deep neural networks rival the representation of primate IT cortex for core visual object recognition</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><issue>12</issue><elocation-id>e1003963</elocation-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doumas</surname><given-names>LeonidasAA</given-names></name><name><surname>Puebla</surname><given-names>Guillermo</given-names></name><name><surname>Martin</surname><given-names>Andrea E</given-names></name><name><surname>Hummel</surname><given-names>John E</given-names></name></person-group><article-title>A theory of relation learning and cross-domain generalization</article-title><source>Psychological Review</source><comment>in press</comment></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>Willis D</given-names></name></person-group><source>A source book of Gestalt psychology</source><publisher-name>Routledge</publisher-name><year>2013</year></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feinman</surname><given-names>Reuben</given-names></name><name><surname>Lake</surname><given-names>Brenden M</given-names></name></person-group><article-title>Learning inductive biases with simple neural networks</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv:1802.02745</elocation-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gatys</surname><given-names>Leon A</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group><source>Image style transfer using convolutional neural networks</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2016</year><fpage>2414</fpage><lpage>2423</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geirhos</surname><given-names>Robert</given-names></name><name><surname>Rubisch</surname><given-names>Patricia</given-names></name><name><surname>Michaelis</surname><given-names>Claudio</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name><name><surname>Wichmann</surname><given-names>Felix A</given-names></name><name><surname>Brendel</surname><given-names>Wieland</given-names></name></person-group><article-title>Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</article-title><source>arXiv preprint</source><year>2018</year><elocation-id>arXiv:1811.12231</elocation-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name></person-group><source>Deep learning</source><publisher-name>MIT press</publisher-name><year>2016</year></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graves</surname><given-names>Alex</given-names></name><name><surname>Wayne</surname><given-names>Greg</given-names></name><name><surname>Danihelka</surname><given-names>Ivo</given-names></name></person-group><article-title>Neural turing machines</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>arXiv:1410.5401</elocation-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill</surname><given-names>Jean-Bastien</given-names></name><name><surname>Strub</surname><given-names>Florian</given-names></name><name><surname>Altché</surname><given-names>Florent</given-names></name><name><surname>Tallec</surname><given-names>Corentin</given-names></name><name><surname>Richemond</surname><given-names>Pierre</given-names></name><name><surname>Buchatskaya</surname><given-names>Elena</given-names></name><name><surname>Doersch</surname><given-names>Carl</given-names></name><name><surname>Bernardo Pires</surname><given-names>Avila</given-names></name><name><surname>Guo</surname><given-names>Zhaohan</given-names></name><name><surname>Mohammad Azar</surname><given-names>Gheshlaghi</given-names></name><etal/></person-group><article-title>Bootstrap your own latent-a new approach to self-supervised learning</article-title><source>Advances in Neural Systems Information Processing</source><year>2020</year><volume>33</volume><fpage>21271</fpage><lpage>21284</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermann</surname><given-names>Katherine</given-names></name><name><surname>Chen</surname><given-names>Ting</given-names></name><name><surname>Kornblith</surname><given-names>Simon</given-names></name></person-group><article-title>The origins and prevalence of texture bias in convolutional neural networks</article-title><source>Advances in Neural Systems Information Processing</source><year>2020</year><volume>33</volume></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochberg</surname><given-names>Julian</given-names></name><name><surname>Brooks</surname><given-names>Virginia</given-names></name></person-group><article-title>Pictorial recognition as an unlearned ability: A study of one child’s performance</article-title><source>The American Journal of Psychology</source><year>1962</year><volume>75</volume><issue>4</issue><fpage>624</fpage><lpage>628</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>Sepp</given-names></name><name><surname>Schmidhuber</surname><given-names>Jürgen</given-names></name></person-group><article-title>Long short-term memory</article-title><source>Neural computation</source><year>1997</year><volume>9</volume><issue>8</issue><fpage>1735</fpage><lpage>1780</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name></person-group><article-title>Reference frames and relations in computational models of ob ject recognition</article-title><source>Current Directions in Psychological Science</source><year>1994</year><volume>3</volume><issue>4</issue><fpage>111</fpage><lpage>116</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name></person-group><article-title>Getting symbols out of a neural architecture</article-title><source>Connection Science</source><year>2011</year><volume>23</volume><issue>2</issue><fpage>109</fpage><lpage>118</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name></person-group><article-title>Object recognition</article-title><source>Oxford handbook of cognitive psychology</source><year>2013</year><fpage>32</fpage><lpage>46</lpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name><name><surname>Biederman</surname><given-names>Irving</given-names></name></person-group><article-title>Dynamic binding in a neural network for shape recognition</article-title><source>Psychological review</source><year>1992</year><volume>99</volume><issue>3</issue><fpage>480</fpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name><name><surname>Holyoak</surname><given-names>Keith J</given-names></name></person-group><article-title>Distributed representations of structure: A theory of analogical access and mapping</article-title><source>Psychological review</source><year>1997</year><volume>104</volume><issue>3</issue><fpage>427</fpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name><name><surname>Holyoak</surname><given-names>Keith J</given-names></name></person-group><article-title>A symbolic-connectionist theory of relational inference and generalization</article-title><source>Psychological review</source><year>2003</year><volume>110</volume><issue>2</issue><fpage>220</fpage></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hummel</surname><given-names>John E</given-names></name><name><surname>Stankiewicz</surname><given-names>Brian J</given-names></name></person-group><article-title>Categorical relations in shape perception</article-title><source>Spatial vision</source><year>1996</year><volume>10</volume><issue>3</issue><fpage>201</fpage><lpage>236</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>Georgin</given-names></name><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Katti</surname><given-names>Harish</given-names></name><name><surname>Arun</surname><given-names>SP</given-names></name></person-group><article-title>Qualitative similarities and differences in visual object representations between brains and deep networks</article-title><source>Nature communications</source><year>2021</year><volume>12</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>arXiv:1412.6980</elocation-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Welling</surname><given-names>Max</given-names></name></person-group><article-title>Auto-encoding variational bayes</article-title><source>arXiv preprint</source><year>2013</year><elocation-id>arXiv:1312.6114</elocation-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knill</surname><given-names>David C</given-names></name></person-group><article-title>Perception of surface contours and surface shape: from computation to psychophysics</article-title><source>JOSA A</source><year>1992</year><volume>9</volume><issue>9</issue><fpage>1449</fpage><lpage>1464</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></person-group><article-title>Deep neural networks: a new framework for modeling biological vision and brain information processing</article-title><source>Annual review of vision science</source><year>2015</year><volume>1</volume><fpage>417</fpage><lpage>446</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>Alex</given-names></name><name><surname>Sutskever</surname><given-names>Ilya</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group><article-title>Imagenet classification with deep convolutional neural networks</article-title><source>Advances in neural information processing systems</source><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>Jonas</given-names></name><name><surname>Bracci</surname><given-names>Stefania</given-names></name><name><surname>Op de Beeck</surname><given-names>Hans P</given-names></name></person-group><article-title>Deep neural networks as a computational model for human shape sensitivity</article-title><source>PLoS computational biology</source><year>2016</year><volume>12</volume><issue>4</issue><elocation-id>e1004896</elocation-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landau</surname><given-names>Barbara</given-names></name><name><surname>Smith</surname><given-names>Linda B</given-names></name><name><surname>Jones</surname><given-names>Susan S</given-names></name></person-group><article-title>The importance of shape in early lexical learning</article-title><source>Cognitive development</source><year>1988</year><volume>3</volume><issue>3</issue><fpage>299</fpage><lpage>321</lpage></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>Gaurav</given-names></name><name><surname>Dujmovic</surname><given-names>Marin</given-names></name><name><surname>Bowers</surname><given-names>Jeffrey S</given-names></name></person-group><article-title>Feature blindness: a challenge for understanding and modelling visual object recognition</article-title><source>bioRxiv</source><year>2021</year></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mamassian</surname><given-names>Pascal</given-names></name><name><surname>Landy</surname><given-names>Michael S</given-names></name></person-group><article-title>Observer biases in the 3d interpretation of line drawings</article-title><source>Vision research</source><year>1998</year><volume>38</volume><issue>18</issue><fpage>2817</fpage><lpage>2832</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehrer</surname><given-names>Johannes</given-names></name><name><surname>Spoerer</surname><given-names>Courtney J</given-names></name><name><surname>Jones</surname><given-names>Emer C</given-names></name><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><name><surname>Kietzmann</surname><given-names>Tim C</given-names></name></person-group><article-title>An ecologically motivated image dataset for deep learning yields better models of human vision</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>8</issue></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakayama</surname><given-names>Ken</given-names></name><name><surname>He</surname><given-names>Zijiang J</given-names></name><name><surname>Shimojo</surname><given-names>Shinsuke</given-names></name></person-group><article-title>Visual surface representation: A critical link between lower-level and higher-level vision</article-title><source>Visual cognition: An invitation to cognitive science</source><year>1995</year><volume>2</volume><fpage>1</fpage><lpage>70</lpage></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>Stephen</given-names></name></person-group><article-title>Canonical perspective and the perception of objects</article-title><source>Attention and performance</source><year>1981</year><fpage>135</fpage><lpage>151</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><name><surname>Gross</surname><given-names>Sam</given-names></name><name><surname>Chintala</surname><given-names>Soumith</given-names></name><name><surname>Chanan</surname><given-names>Gregory</given-names></name><name><surname>Yang</surname><given-names>Edward</given-names></name><name><surname>DeVito</surname><given-names>Zachary</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Desmaison</surname><given-names>Alban</given-names></name><name><surname>Antiga</surname><given-names>Luca</given-names></name><name><surname>Lerer</surname><given-names>Adam</given-names></name></person-group><source>Automatic differentiation in pytorch</source><year>2017</year></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizlo</surname><given-names>Zygmunt</given-names></name></person-group><article-title>Perception viewed as an inverse problem</article-title><source>Vision research</source><year>2001</year><volume>41</volume><issue>24</issue><fpage>3145</fpage><lpage>3161</lpage></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizlo</surname><given-names>Zygmunt</given-names></name><name><surname>Stevenson</surname><given-names>Adam K</given-names></name></person-group><article-title>Shape constancy from novel views</article-title><source>Perception &amp; Psychophysics</source><year>1999</year><volume>61</volume><issue>7</issue><fpage>1299</fpage><lpage>1307</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pizlo</surname><given-names>Zygmunt</given-names></name><name><surname>Sawada</surname><given-names>Tadamasa</given-names></name><name><surname>Li</surname><given-names>Yunfeng</given-names></name><name><surname>Walter Kropatsch</surname><given-names>G</given-names></name><name><surname>Steinman</surname><given-names>Robert M</given-names></name></person-group><article-title>New approach to the perception of 3d shape based on 746 veridicality, complexity, symmetry and volume</article-title><source>Vision research</source><year>2010</year><volume>50</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Puebla</surname><given-names>Guillermo</given-names></name><name><surname>Bowers</surname><given-names>Jeffrey</given-names></name></person-group><article-title>Can deep convolutional neural networks support relational reasoning in the same-different task?</article-title><source>bioRxiv</source><year>2021</year></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sabour</surname><given-names>Sara</given-names></name><name><surname>Frosst</surname><given-names>Nicholas</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name></person-group><article-title>Dynamic routing between capsules</article-title><source>arXiv preprint</source><year>2017</year><elocation-id>arXiv:1710.09829</elocation-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><name><surname>Kubilius</surname><given-names>Jonas</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Na jibj</surname><given-names>J Maja</given-names></name><name><surname>Rajalingham</surname><given-names>Rishi</given-names></name><name><surname>Issa</surname><given-names>Elias B</given-names></name><name><surname>Kar</surname><given-names>Kohitij</given-names></name><name><surname>Bashivan</surname><given-names>Pouya</given-names></name><name><surname>Prescott-Roy</surname><given-names>Jonathan</given-names></name><name><surname>Geiger</surname><given-names>Franziska</given-names></name><etal/></person-group><article-title>Brain-score: Which artificial neural network for ob ject recognition is most brain-like?</article-title><source>BioRxiv</source><year>2020</year><elocation-id>407007</elocation-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group><article-title>Very deep convolutional networks for large-scale image recognition</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>arXiv:1409.1556</elocation-id></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>Kent A</given-names></name></person-group><article-title>The visual interpretation of surface contours</article-title><source>Artificial Intelligence</source><year>1981</year><volume>17</volume><issue>1-3</issue><fpage>47</fpage><lpage>73</lpage></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarr</surname><given-names>Michael J</given-names></name><name><surname>Pinker</surname><given-names>Steven</given-names></name></person-group><article-title>Mental rotation and orientation-dependence in shape recognition</article-title><source>Cognitive psychology</source><year>1989</year><volume>21</volume><issue>2</issue><fpage>233</fpage><lpage>282</lpage></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>Ashish</given-names></name><name><surname>Shazeer</surname><given-names>Noam</given-names></name><name><surname>Parmar</surname><given-names>Niki</given-names></name><name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name><name><surname>Jones</surname><given-names>Llion</given-names></name><name><surname>Aidan Gomez</surname><given-names>N</given-names></name><name><surname>Kaiser</surname><given-names>Lukasz</given-names></name><name><surname>Polosukhin</surname><given-names>Illia</given-names></name></person-group><article-title>Attention is all you need</article-title><source>arXiv preprint</source><year>2017</year><elocation-id>arXiv:1706.03762</elocation-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature neuroscience</source><year>2016</year><volume>19</volume><issue>3</issue><fpage>356</fpage><lpage>365</lpage></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>Daniel LK</given-names></name><name><surname>Hong</surname><given-names>Ha</given-names></name><name><surname>Cadieu</surname><given-names>Charles F</given-names></name><name><surname>Solomon</surname><given-names>Ethan A</given-names></name><name><surname>Seibert</surname><given-names>Darren</given-names></name><name><surname>DiCarlo</surname><given-names>James J</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the national academy of sciences</source><year>2014</year><volume>111</volume><issue>23</issue><fpage>8619</fpage><lpage>8624</lpage></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yosinski</surname><given-names>Jason</given-names></name><name><surname>Clune</surname><given-names>Jeff</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>Lipson</surname><given-names>Hod</given-names></name></person-group><article-title>How transferable are features in deep neural networks?</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>arXiv:1411.1792</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig 1</label><caption><p>Stimuli used by Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>]. The first column shows a set of six (<monospace>Basis</monospace>) shapes that participants were trained to recognise. Participants were then tested on shapes in the second and third columns, which were generated by deforming the Basis shape in the corresponding row. In the second column (<monospace>Rel</monospace> deformation) a shape is generated by changing one categorical relation (highlighted in red circle). In the third column (<monospace>Cood</monospace> deformation) all categorical relations are preserved but coordinates of some elements are shifted (highlighted in blue ellipse).</p></caption><graphic xlink:href="EMS145926-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><p>Cosine similarity between internal representations at convolutional (<monospace>Conv2d</monospace>) and fully connected (<monospace>Linear</monospace>) layers of <monospace>VGG-16</monospace> network that is either pre-trained on <monospace>ImageNet</monospace> or on <monospace>Stylized-ImageNet</monospace> – a dataset developed to induce a shape-bias in CNNs. In each panel, the solid (red) line plots the cosine similarity between the internal representations of a Basis shape and its <monospace>Rel</monospace> deformation, while the dashed (blue) line plots the cosine similarity between the internal representations of a Basis shape and its <monospace>Cood</monospace> deformation. The hatched (yellow) area shows the upper and lower bounds on cosine similarity (obtained by computing the cosine similarity of images from the same and different categories, respectively). Shaded regions around each line show 95% confidence interval. Based on the results of Hummel and Stankiewicz [<xref ref-type="bibr" rid="R26">26</xref>], we would expect the solid (red) line (<monospace>Ba-Rel</monospace>) to be closer to the lower, rather than upper bound. Instead we observe that it stays at the upper bound throughout the network and is statistically indistinguishable from the dashed (blue) line, showing that there is no significant difference between the Basis shape and either (relational or coordinate) deformation at any layer of the network.</p></caption><graphic xlink:href="EMS145926-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><p>Three training sets that try to teach the network to recognise relational changes. In each set, the first column shows a set of six unique Basis shapes, while the second column shows <monospace>Rel</monospace> deformations of the first five shapes (see red arrow). At the bottom are the two test shapes. These test shapes are identical to the eleventh (unpaired) training shape, except for one relational (dashed red circle) or coordinate (dashed blue ellipse) deformation. In Set 1, the difference between the untrained shape and the tested <monospace>Rel</monospace> deformation is exactly the same as the relational change distinguishing one pair of shapes and similar to another pair in the training set (both highlighted in solid red rectangles). In Set 2, the exact relational change is not trained, however there is similar relational change at a close location (pair again highlighted in solid red rectangle). Set 3 is the most challenging, where none of the diagnostic relational changes in the training set occur at similar locations to the tested relational deformation.</p></caption><graphic xlink:href="EMS145926-f003"/></fig><fig id="F4" position="float"><label>Fig 4</label><caption><p>Cosine similarity between Basis image and two types of deformations for a <monospace>VGG-16</monospace> network pre-trained on either <monospace>ImageNet</monospace> (first row) or <monospace>Stylized-ImageNet</monospace> (second row) and fine-tuned on Set 1 (left), Set 2 (middle) or Set 3 (right) shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. Like <xref ref-type="fig" rid="F2">Figure 2</xref>, the hatched (yellow) region shows the upper and lower bound on similarity, the solid (red) line shows similarity between Basis and the relational deformation while the dashed (blue) lines shows similarity between Basis and the coordinate deformation. We can see that the network fine-tuned on Set 1 represents relational deformations as significantly different from Basis images as well as coordinate deformations (solid red line is much lower than upper bound and dashed blue line for deeper layers in the network). However, this is not the case for networks fine-tuned on Set 2 or Set 3.</p></caption><graphic xlink:href="EMS145926-f004"/></fig><fig id="F5" position="float"><label>Fig 5</label><caption><title>Stimuli used to test shape representations in single-part objects.</title><p>(a) The shapes in the Basis set used for training. Each shape is presented at various translations and scales. (b) The test set for one of the categories (Cat 2) is obtained by deforming the Basis shape (in the top-left corner) through a combination of rotation and shear operations. Here we have organised these deformations in a matrix based on their coordinate distance (measured as cosine distance) and relational distance (measured as change in relative location of vertices) from the basis shape. All deformations on a diagonal of this matrix are at the same coordinate distance from the Basis shape and all deformations in a column are at the same relational distance from the Basis shape. Highlighted (red) squares show stimuli for computing cosine distance in <xref ref-type="fig" rid="F7">Figure 7</xref> below. Deformations marked D1 and D2 are used for testing human participants. (c) The predicted accuracy on the test set presented as heat-maps, assuming that accuracy is a function of coordinate distance (top), or relational distance (bottom).</p></caption><graphic xlink:href="EMS145926-f005"/></fig><fig id="F6" position="float"><label>Fig 6</label><caption><p>Test stimuli for each category (first row) and accuracy on the landscape of relational and coordinate deformations for <monospace>VGG-16</monospace> pre-trained on either <monospace>ImageNet</monospace> (middle row) or <monospace>Stylized-ImageNet</monospace> (bottom row). In each case, the network was fine-tuned on the set of seven polygons shown in <xref ref-type="fig" rid="F5">Figure 5(a)</xref>. Each heatmap (in middle and bottom rows) corresponds to a category and shows the percent of shapes (with a relational and coordinate deformation given by the position on the landscape) accurately classified as the category from which the stimulus was derived. For most categories, accuracy is highest for small deformations (top-left corner) and decreases as a function of the coordinate distance from the basis shape (perpendicular to diagonal). The relational distance (left-to-right) has no added effect on this decrease in accuracy.</p></caption><graphic xlink:href="EMS145926-f006"/></fig><fig id="F7" position="float"><label>Fig 7</label><caption><p>Cosine similarity in internal representations of VGG-16 in Experiment 2. The solid (red) and dashed (blue) lines show the average cosine similarity between Basis images and relational (shear) and coordinate (rotation) deformations, respectively. The hatched (yellow) region shows the bounds on this similarity, with the upper bound determined by the average similarity between Basis images from the same category and lower bound determined by the average similarity between Basis images of different categories. If relational (shear) deformation has a larger affect on internal representations than a coordinate (rotation) deformation, one would expect the solid (red) line to be below the dashed (blue) line.</p></caption><graphic xlink:href="EMS145926-f007"/></fig><fig id="F8" position="float"><label>Fig 8</label><caption><p>Comparison of how classification accuracy changes for <monospace>VGG-16</monospace> (left and middle) and human participants (right) with stimuli deformation. Each panel shows performance under three conditions: basis image, deformation D1 and deformation D2. For the shear deformation (solid, red line), D1 and D2 consists of images in the top row in the fourth and eighth column in <xref ref-type="fig" rid="F5">Figure 5</xref>. For the rotation deformation (dashed, blue line), D1 and D2 consist of images in the first column and fourth and eighth rows. Error bars show 95% confidence interval and dashed black line shows chance performance.</p></caption><graphic xlink:href="EMS145926-f008"/></fig></floats-group></article>