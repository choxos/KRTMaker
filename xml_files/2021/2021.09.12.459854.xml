<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS153648</article-id><article-id pub-id-type="doi">10.1101/2021.09.12.459854</article-id><article-id pub-id-type="archive">PPR394738</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Disentangling five dimensions of animacy in human brain and behaviour</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jozwik</surname><given-names>Kamila M</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">#</xref></contrib><contrib contrib-type="author"><name><surname>Najarro</surname><given-names>Elias</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>van den Bosch</surname><given-names>Jasper JF</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Charest</surname><given-names>Ian</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw M</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="fn" rid="FN1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>University of Cambridge, Department of Psychology, Cambridge, UK</aff><aff id="A2"><label>2</label>IT University of Copenhagen, Digital Design Department, Copenhagen, Denmark</aff><aff id="A3"><label>3</label>University of Birmingham, School of Psychology, Birmingham, UK</aff><aff id="A4"><label>4</label>Freie Universität Berlin, Department of Education and Psychology, Berlin, Germany</aff><aff id="A5"><label>5</label>Columbia University, Zuckerman Mind Brain Behavior Institute, Department of Psychology, Department of Neuroscience, Department of Electrical Engineering, New York, USA</aff><author-notes><corresp id="CR1">
<label>#</label>corresponding author: <email>jozwik.kamila@gmail.com</email></corresp><fn id="FN1" fn-type="equal"><label>*</label><p id="P1">contributed equally</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>31</day><month>08</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Distinguishing animate from inanimate things is of great behavioural importance. Despite distinct brain and behavioural responses to animate and inanimate things, it remains unclear which object properties drive these responses. Here, we investigate the importance of five object dimensions related to animacy (“being alive”, “looking like an animal”, “having agency”, “having mobility”, and “being unpredictable”) in brain (fMRI, EEG) and behaviour (property and similarity judgements) of 19 participants. We used a stimulus set of 128 images, optimized by a genetic algorithm to disentangle these five dimensions. The five dimensions explained much variance in the similarity judgments. Each dimension also explained a modest but significant amount of variance in the brain representations, except, surprisingly, “being alive”. Different brain regions sensitive to animacy may represent distinct dimensions, either as accessible perceptual stepping stones toward detecting whether something is alive or because they are of behavioural importance in their own right.</p></abstract><kwd-group><kwd>Animacy</kwd><kwd>Dimensions</kwd><kwd>Visual object recognition</kwd><kwd>EEG</kwd><kwd>fMRI</kwd><kwd>Similarity judgements</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">The perception of animate things is of great behavioural and evolutionary importance to humans and other animals. Recognizing animate things is essential for choosing appropriate actions as we engage the physical and social world, and can be a matter of life and death (e.g., quick recognition of a predator). Animacy is an important representational division in nonhuman and human higher ventral visual cortical areas in the inferior temporal cortex (<xref ref-type="bibr" rid="R23">Kriegeskorte et al., 2008</xref>) and the medial temporal lobe (<xref ref-type="bibr" rid="R7">Blumenthal et al., 2018</xref>) as measured by functional magnetic resonance imaging (fMRI). Consistent with the importance of animacy perception in the classical neuropsychological literature, lesion studies established that living things are represented in dedicated regions of the cortex (<xref ref-type="bibr" rid="R13">Funnell &amp; Sheridan, 1992</xref>; <xref ref-type="bibr" rid="R35">Ralph et al., 1998</xref>; <xref ref-type="bibr" rid="R44">Silveri et al., 1997</xref>). However, it is less clear which of the distinctive features of animate things are represented in the brain and reflected in judgments of animacy and of the similarity between things. An animal differs from an inanimate object in many respects, so animacy could be diagnosed by many different indicators. The dimensions of animacy that have been explored include “being alive” (<xref ref-type="bibr" rid="R10">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="R16">Gray et al., 2007</xref>; <xref ref-type="bibr" rid="R17">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="R26">Leib et al., 2016</xref>; <xref ref-type="bibr" rid="R28">Looser et al., 2013</xref>; <xref ref-type="bibr" rid="R37">Rogers et al., 2005</xref>; <xref ref-type="bibr" rid="R47">Wheatley et al., 2011</xref>), “looking like an animal” (<xref ref-type="bibr" rid="R8">Bracci et al., 2019</xref>; <xref ref-type="bibr" rid="R10">Connolly et al., 2012</xref>; <xref ref-type="bibr" rid="R17">Huth et al., 2012</xref>; <xref ref-type="bibr" rid="R37">Rogers et al., 2005</xref>; <xref ref-type="bibr" rid="R41">Sha et al., 2015</xref>; <xref ref-type="bibr" rid="R47">Wheatley et al., 2011</xref>), “having mobility” (<xref ref-type="bibr" rid="R5">Beauchamp et al., 2002</xref>, <xref ref-type="bibr" rid="R6">2003</xref>; <xref ref-type="bibr" rid="R42">Shultz &amp; McCarthy, 2014</xref>), “having agency” (<xref ref-type="bibr" rid="R15">Gobbini et al., 2007</xref>, <xref ref-type="bibr" rid="R14">2010</xref>; <xref ref-type="bibr" rid="R29">Lowder &amp; Gordon, 2015</xref>; <xref ref-type="bibr" rid="R43">Shultz et al., 2015</xref>; <xref ref-type="bibr" rid="R42">Shultz &amp; McCarthy, 2014</xref>; <xref ref-type="bibr" rid="R45">Thorat et al., 2019</xref>), and “being unpredictable” (<xref ref-type="bibr" rid="R29">Lowder &amp; Gordon, 2015</xref>). Each of these studies offers important insights on one particular dimension of animacy. However, to understand which of a number of confounded dimensions are represented, we need experiments designed to disentangle them.</p><p id="P4">Dimensions that define a concept depend on the chosen definition of the concept. Dictionary definitions of animate rely heavily on the dimension of being alive (animate: “living; having life”, Oxford Advanced Learner's Dictionary). However, being alive is a difficult-to-assess latent property of a thing. It seems plausible that a perceptual system might represent more accessible dimensions that are correlated with being alive, even if being alive were ultimately the behaviourally important property. In addition, a more accessible related property, such as <italic>looking like an animal</italic> or <italic>being unpredictable</italic>, may be of behavioural importance in its own right. We are not concerned here with the philosophical and semantic questions of animacy, but with the empirical question of which of several related and commonly conflated dimensions are represented in particular brain regions and in behavioural judgements.</p><p id="P5">Apart from the abovementioned dimensions of animacy, several other human-centred interpretations of animacy have been proposed. Recently reported animacy-related concepts that explain variance in the ventral visual stream fMRI measurements are human-likeness (<xref ref-type="bibr" rid="R39">Rosenthal-von der Pütten et al., 2019</xref>), humanness (<xref ref-type="bibr" rid="R12">Contini et al., 2020</xref>), resemblance to human faces and bodies (<xref ref-type="bibr" rid="R36">Ritchie et al., 2021</xref>), and capacity for self-movement and thought rather than face presence (<xref ref-type="bibr" rid="R32">Proklova &amp; Goodale, 2022</xref>). Another similar concept is the animacy continuum, where objects are perceived as more animate when they are more similar to humans (e.g., images of monkeys would be perceived as more animate than insects, even though both species belong to the animal category; <xref ref-type="bibr" rid="R10">Connolly et al., 2012</xref>). In addition to the high-level dimensions, low-level visual features correlate with animacy (<xref ref-type="bibr" rid="R27">Long et al., 2017</xref>; <xref ref-type="bibr" rid="R38">Rosenthal et al., 2018</xref>; <xref ref-type="bibr" rid="R40">Schmidt et al., 2017</xref>). Colour statistics (<xref ref-type="bibr" rid="R38">Rosenthal et al., 2018</xref>), curvature (<xref ref-type="bibr" rid="R27">Long et al., 2017</xref>), and mid-level shape features (<xref ref-type="bibr" rid="R40">Schmidt et al., 2017</xref>) can be used to classify whether an object is animate or inanimate.</p><p id="P6">The stimuli used in previous studies were mostly handpicked to investigate a chosen dimension without controlling for confounding variables. Previous studies often used unnatural stimuli (e.g., point-light displays in <xref ref-type="bibr" rid="R15">Gobbini et al., 2007</xref> or grey-scale simplified objects in <xref ref-type="bibr" rid="R33">Proklova et al., 2016</xref>) or stimuli from only one category of objects (e.g., animals in <xref ref-type="bibr" rid="R45">Thorat et al., 2019</xref>). The “being unpredictable” dimension has only been studied in the language domain and not in vision. Finally, previous studies mostly focused on one type of behaviour or one type of brain measurement but none of them has combined multiple measurements of behaviour and different brain measurement modalities, which would allow building a more comprehensive understanding. This approach has limited our progress toward a comprehensive understanding of the representation of the multiple dimensions of animacy. Thus, despite decades of research that has established the prominence of different dimensions of animacy in human brain and behaviour, it remains unclear whether any one of the five selected dimensions or a subset of them can explain the responses and how this depends on the brain region or behavioural measure.</p><p id="P7">Here, we comprehensively investigate the importance of the five selected dimensions of animacy: “being alive”, “looking like an animal”, “having agency”, “having mobility”, and “being unpredictable”. By using a larger number and diversity of stimuli than in the previous studies, we can disentangle the dimensions experimentally. We study responses to this stimulus set using two behavioural tasks (animacy ratings and similarity judgements) and two brain measurement modalities (fMRI and EEG). To disentangle the five selected dimensions of animacy, we optimized a stimulus set of 128 images using a genetic algorithm (GA).</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Stimulus selection procedure and stimulus set</title><p id="P8">Evaluating the contribution of individual dimensions of animacy (“being alive”, “looking like an animal”, “having mobility”, “having agency”, and “being unpredictable”) would be best performed on a stimulus set that is as decorrelated on these dimensions as possible. We created such a stimulus set by optimizing stimuli using a genetic algorithm. We selected images that were maximally decorrelated on dimensions of animacy using a four-step procedure (<xref ref-type="fig" rid="F1">Figure 1a</xref>, see <xref ref-type="sec" rid="S10">Methods</xref> for details). First, we created an animacy dimension grid where we asked participants to fill freely in the names of the objects fulfilling each animacy dimension combination to then find images that satisfy combinations of dimensions of animacy (29 out of 32 possible combinations). The object names provided by the subjects did not cover all 32 combinations, which is why 29 combinations out of 32 were included in step 1 of the stimulus selection procedure. Participants came up with 100 classes in total, and the participants were not given any object classes to use by the experimenters (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref>). The object categories were distinct (e.g., there were different types of robots and, therefore, two different object categories, “humanoid robot” and “animal robot”, were included). Second, we assembled object images based on object names from step one. Third, an independent set of participants rated object images (which were assembled based on object names from step one) on each of the five selected dimensions of animacy to generate animacy ratings. Fourth, we used a genetic algorithm to select a subset of images with the lowest maximum correlation between dimensions of animacy (10,000 generations). The maximum correlation between dimensions in the stimulus set was 0.36. This result was better than when randomly selecting the stimuli 10,000 times without optimization (maximum correlation between dimensions = 0.64), proving that our novel stimulus selection procedure was successful. The pairwise correlations between animacy dimensions for the 128 stimuli selected by GA and for the 128 stimuli selected randomly are represented in <xref ref-type="fig" rid="F1">Figure 1b</xref>. This stimulus set was used in subsequent behavioural and brain imaging experiments.</p><p id="P9">The stimulus set (<xref ref-type="fig" rid="F2">Figure 2a</xref>) consisted of 128 images spanning almost all animacy dimension combinations (26 out of 32 possible combinations). Among 29 dimension combinations for which participants provided object names, three were not selected by the GA. This is because the objective of the GA was to minimise the maximum correlations between dimensions, and some dimension combinations were not optimal to be chosen. A wide range of objects was present, such as humans, human fetuses, human organs, human and animal shadows, plants, corals, forces of nature, game items, toys, vehicles, and electronic equipment covering 68 categories. The GA did not choose some of the images representing object names from the initial 100 object names listed in step 1, as the dimensions of animacy ratings on these images were not optimal for the GA objective. The GA was allowed to choose a maximum of two different images representing different objects from a given category (e.g., two different animal robots) if this selection contributed to an optimal GA solution. A small percentage of stimuli can be considered as unusual (7.8 %, 10 out of 128 stimuli, considering that stimuli such as human fetus, disembodied eyeball, person on life support, heart can be considered as emotional triggers). As the unusual stimuli constitute only a small percentage of our stimulus set we do not think that they would affect the interpretation of our results. None of the participants mentioned that they found any of the stimuli upsetting after performing the experiment. However, future studies may benefit from including affect ratings alongside the dimensions of animacy ratings. This stimulus set was used for two behavioural studies: animacy ratings and similarity judgements, and two brain response measurement studies: EEG (to access temporal information) and fMRI (to access spatial information). Nineteen participants performed all the studies. Importantly, participants first performed EEG and fMRI studies, followed by similarity judgements and finally animacy ratings (<xref ref-type="fig" rid="F2">Figure 2b</xref>). This experimental order was to ensure that participants did not know about animacy dimensions tested until the final animacy ratings.</p></sec><sec id="S4"><title>Consistency in animacy ratings across participants</title><p id="P10">We first wanted to evaluate the contribution of each dimension of animacy when participants were asked to judge how animate an object image was. We first explored how consistent participants were in judging each dimension of animacy and each image. We used representational similarity analysis (<xref ref-type="bibr" rid="R31">Nili et al., 2014</xref>) to reveal which dimensions contribute most when participants judged animacy. We also examined which dimension(s) explained unique variance in the animacy ratings.</p><p id="P11">Participants judged each object image using a continuous scale from -10 to 10 for each dimension, e.g., -10 meant “inanimate” and 10 meant “animate” (<xref ref-type="fig" rid="F3">Figure 3a</xref>). The same image was judged in the same way on the five investigated dimensions of animacy using the same scale. The mean between-participant correlation was 0.6, which indicated that participants were generally consistent in their ratings. The mean within-participant consistency for thirty repeated stimuli was 0.89 meaning that participants consistently judged the same stimulus within a session. The raw data of animacy ratings shows that a given stimulus could be differently rated on each of the dimensions (<xref ref-type="fig" rid="F3">Figure 3b</xref>).</p><p id="P12">We wanted to know how consistent participants were in judging each dimension and each stimulus. This analysis had two purposes: to be able to determine whether the variability in animacy ratings is low enough to interpret the ratings at all and to test which dimensions and stimuli were particularly controversial for participants as reflected by higher variability.</p><p id="P13">We first asked about the participants’ consistency across stimuli in all dimensions of animacy. There is variability in the consistency of ratings as some of the object images, e.g., "bike", was judged very consistently as expected, whereas other more controversial ones, e.g., 'human fetus', less consistently, with "human shadow" having the lowest consistency among the object images tested (<xref ref-type="fig" rid="F3">Figure 3c</xref>). While participants were asked to judge what is represented on images (“human shadow”), it might not be apparent whether to judge the shadow itself or the person creating the shadow.</p><p id="P14">To get more insights into animacy dimension ratings, we explored the consistency of each stimulus per each animacy dimension. For "looking like an animal", which was judged most consistently among the dimensions, "hammer" was one of the object images that were most consistently judged, whereas an image of a "human" was not judged very consistently. The lower consistency of judging an image of a human may be related to the fact that some humans do and others do not consider themselves animals, even though from a biological point of view Homo sapiens belong to the animal category. Looking at the other side of the spectrum - "having agency" dimension was judged least consistently - we observed that an example image that had a high consistency of ratings was "eyeball", whereas an image of a "robot" was not very consistently judged (<xref ref-type="fig" rid="F3">Figure 3d</xref>).</p><p id="P15">Among all the dimensions participants judged "having agency" least consistently and "looking like an animal" most consistently (see <xref ref-type="fig" rid="F3">Figure 3e</xref> for the consistency of ratings for each dimension). What about the animacy ratings (“being animate”)? Would object animacy be judged consistently across participants or given the ambiguous definition of this term, would the consistency be lower than some of the more precisely defined dimensions of animacy? We found that the latter was the case - animacy ratings had lower consistency than more precisely defined dimensions of animacy, except for the 'having agency' dimension.</p></sec><sec id="S5"><title>Contribution of each animacy dimension to animacy ratings</title><p id="P16">To gain more intuition about what stimuli are considered to have the highest value on each animacy dimension, we visualized 10 object images with the overall minimum and 10 with the maximum rating on a given dimension (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Overall, images that had low values on animacy dimension ratings were similar among dimensions (e.g., plush toys, meat, washing machine). In contrast, images with high ratings did differ depending on a dimension tested (e.g., stimuli judged as the most unpredictable being humans and forces of nature, in contrast to humans judged as having the most agency), proving that indeed these dimensions capture different aspects of animacy perception (<xref ref-type="fig" rid="F4">Figure 4a</xref>, right panel).</p><p id="P17">To reveal which dimensions contribute most when participants judged animacy we used representational similarity analysis (RSA, see <xref ref-type="sec" rid="S10">Methods</xref>). RSA characterises representations in behavioural and brain data and models by representational dissimilarity matrices (RDMs) of the response patterns elicited by stimuli. RDMs capture the information represented by data or models by characterizing their representational geometry (<xref ref-type="bibr" rid="R24">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>; <xref ref-type="bibr" rid="R21">Kriegeskorte &amp; Kievit, 2013</xref>). The representational geometry reflects which stimulus information is emphasized and which is de-emphasized. Models (here dimensions of animacy) are tested by correlating their RDMs with data RDMs.</p><p id="P18">Using RSA, we found that among all the dimensions, "having agency" and "being alive" explained more variance than other dimensions (<xref ref-type="fig" rid="F4">Figure 4b</xref>) in animacy ratings (when participants were asked to judge how animate an object image was). This result means that when asked to judge animacy humans mostly think about whether an object is alive and whether it has agency. Even though "having agency" and "being alive" explain more variance than the other dimensions, it does not mean that they explain unique variance. To test that, we performed a unique variance analysis (see <xref ref-type="sec" rid="S10">Methods</xref> for details) and observed that only one dimension, "being alive", explains significantly more unique variance than "having agency" and "being unpredictable" dimensions (<xref ref-type="fig" rid="F4">Figure 4c</xref>). "Being alive" is one of the dimensions that explain the most variance in animacy ratings and also the only dimension that explains significantly more unique variance than some of the other dimensions.</p><p id="P19">In summary, when asked to rate object animacy, participants found "being alive" and "having agency" as dominant dimensions, and were most consistent when judging the "looking like an animal" dimension and least consistent in judging the "having agency" dimension.</p></sec><sec id="S6"><title>Contribution of each animacy dimension to similarity judgements</title><p id="P20">After having established the contribution of animacy dimensions to animacy ratings, we tested whether any of the dimensions would explain similarity judgements. The similarity judgements task allows participants to evaluate objects in a more natural way concentrating on general object similarity. Please note that participants performed the similarity judgements task before competing dimensions of animacy ratings, so they were unaware of the dimensions of animacy tested. As participants were asked to arrange object images based on their similarity and not asked about animacy, dimensions of animacy may not explain these representations. Rather than dimensions of animacy, either other categorical divisions or lower-level image features could be used for judging object similarity. Participants placed images of objects inside a circular arena according to how similar they judge them (<xref ref-type="fig" rid="F5">Figure 5a</xref>). The procedure was repeated with different numbers of objects that had to be arranged indefinitely until reaching a predefined arrangement consistency (see <xref ref-type="sec" rid="S10">Methods</xref>). We evaluated how well the dimensions of animacy explained the similarity judgements task using RSA and the unique variance analysis.</p><p id="P21">Visualizing the similarity judgements as a multidimensional scaling (MDS) plot helped us to determine which object images were grouped together (<xref ref-type="fig" rid="F5">Figure 5b</xref>). For example, object images of most robots, fetuses, and a human on life support were grouped, with human images separated but placed close to an image of a realistic humanoid robot. This grouping is related to the agency dimension as these images received the highest rating on “having agency” dimension (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Animal robots formed their own cluster together with other moving objects such as boomerangs, balls, and buses. Different vehicles were grouped nearby with images of cars being placed near comets, clouds, and dominos. Games and toys were arranged in proximity to the “animal-like robot” group and the “human” group. Some unpredictable objects were also grouped together: geysers and game machines, or volcanos and waves (<xref ref-type="fig" rid="F4">Figure 4a</xref>). As a sanity check, images that depict the same object were grouped together, for example, two pictures of flowers or wheels (<xref ref-type="fig" rid="F4">Figure 4a</xref>). To gain an intuition of how well the five dimensions of animacy studied here separated representations in the similarity judgements, we have displayed MDS plots colour-coded according to binary animacy dimensions (e.g., “being alive” with one colour of dots and “not being alive” with another colour of dots, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>). All dimensions separated the stimuli well, with each dimension revealing different divisions between stimuli (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>). Despite “having agency” dimension having positive values for three stimuli (images of two adult humans and a human fetus in the late stages of pregnancy, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>), this dimension of animacy explained significant variance in the similarity judgements (<xref ref-type="fig" rid="F5">Figure 5c</xref>).</p><p id="P22">If we assume that the similarity judgements are based only on the similarity between low-level visual features, the dimensions of animacy should not explain a large fraction of the variance. This assumption is not what we observed - all dimensions of animacy explained a significant amount of variance in the similarity judgements task (<xref ref-type="fig" rid="F5">Figure 5c</xref>). If the low-level features were the only ones that participants used in similarity judgements object arrangements, then we would see on the MDS that objects are arranged by, for example, colour or shape, but this is not what we observe (<xref ref-type="fig" rid="F5">Figure 5b</xref>). None of the dimensions fully explained the similarity judgements data but the "having agency" dimension was close to explaining the total explainable variance given the noise in the data. As a portion of the variance remained unexplained, other dimensions beyond the ones explored here are likely needed to capture the data fully. Overall, when judging object similarity, humans use all dimensions of animacy tested here.</p><p id="P23">Even when all dimensions of animacy explain similarity judgements, maybe one or more dimensions explain unique variance. After having performed the unique variance analysis, we observed that each dimension explained unique variance in the similarity judgements (<xref ref-type="fig" rid="F5">Figure 5d</xref>). However, almost no dimension explained more unique variance than the other dimensions. This finding suggests that each dimension not only explains variance in the data but also explains a unique portion of that variance.</p><p id="P24">It is important to consider the effect of low-level visual features on the interpretation of these results and test whether the five animacy dimensions studied here explain unique variance over and above low-level visual features in similarity judgements. We included the first convolutional layer of the deep neural network AlexNet (<xref ref-type="bibr" rid="R25">Krizhevsky et al., 2012</xref>) as a model of low-level visual features in this analysis. We observed that each of the animacy dimensions explained a significant amount of unique variance over and above the variance explained by low-level visual features and the other dimensions (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref>).</p><p id="P25">Overall, each animacy dimension explained a similar amount of variance in a behavioural task of similarity judgements, meaning that participants use higher-level dimensions of animacy when judging object similarity.</p></sec><sec id="S7"><title>Contribution of each animacy dimension to EEG time course</title><p id="P26">Having shown the contribution of each animacy dimension in explaining animacy ratings and in a behavioural task of similarity judgements, we asked whether dimensions of animacy explain the time course of object image processing in the brain using EEG. One possibility is that once an image is shown for only half a second, the brain performs only automated image processing, and higher-level dimensions of animacy do not contribute to this process at all. The other possibility is that beyond low-level features, higher-level dimensions of animacy do play a role in forming brain representations of images even with a short stimulus presentation. To arbitrate between those possibilities, we evaluated the amount of variance explained by each animacy dimension and tested whether any dimension(s) explains unique variance in the EEG signal.</p><p id="P27">We first performed multivariate pattern analysis (MVPA) to determine how well we could decode the pattern of activations evoked by each stimulus. We performed pairwise stimuli decoding using a support vector machine approach and we could decode images in the stimulus set to a high decoding accuracy (62%) in a long time window (between 43 and 1000 ms after stimulus onset, <xref ref-type="fig" rid="F6">Figure 6a</xref>). The peak decoding accuracy was at 197ms (+/- 7ms, standard error).</p><p id="P28">To explore the structure of the representations, we displayed MDS plots for the selected timepoints: 0 ms - when the stimulus was just displayed, 100 ms - when the decoding accuracy started to go up, 200 ms - peak decoding accuracy, and 300 ms - when the decoding accuracy started to drop. As expected, no structure was visible at the stimulus onset (0 ms, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5a</xref>). At 100 ms, human and humanoid and animal robot faces were grouped together, as well as forces of nature (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5b</xref>). At 200 ms, faces were still grouped together, however, faces of a human and robots were represented further away from each other (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5c</xref>). We have also displayed MDS plots colour-coded according to binary animacy dimensions for visualization purposes (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 6</xref>). At 300 ms, we observed similar clusters to those present at 100 ms (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5d</xref>).</p><p id="P29">Once we knew that we could decode our stimuli, we asked how much variance each animacy dimension explained in EEG recordings. Do any of the dimensions explain any variance at all? To answer this question, we correlated each animacy dimension with EEG representations at every time point in every participant using RSA. First, we determined whether each of the animacy dimension RDMs was significantly related to the EEG data RDMs at every timepoint using a participant-as-random-effect analysis (one-sided Wilcoxon signed-rank test). We subsequently tested for differences in animacy dimension performance between each pair of dimensions of animacy at each timepoint using a participant-as-random-effect analysis (twosided Wilcoxon signed-rank test). We accounted for multiple comparisons for each analysis by controlling the FDR at 0.05. We found that most animacy dimensions explained a significant amount of variance in EEG recordings (<xref ref-type="fig" rid="F6">Figure 6b</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>); however, some dimensions explained variance at slightly different times. Despite differences in the exact timing of when dimensions of animacy explained the variance, a very clear pattern that one dimension explains representations earlier than the other was not observed. However, 'being unpredictable' explained significantly more variance than most dimensions in early time points: specifically more than “looking like an animal” (89-130 ms), “having mobility” (89-113 ms), and ”having agency” (79-126 ms). While “looking like an animal” explained more variance than most other dimensions in later time points: more than “being alive” (209-302 ms), ”having agency” (230-266 ms), and “being unpredictable” (146-184 ms). Finally, “having agency” explained more variance than most of the dimensions even later in time: more than “being alive” (268-301 ms), “having mobility” (261-289 ms) and “being unpredictable” (293-315 ms). To investigate how the five dimensions of animacy studied here relate to the general animacy, we correlated the general animacy ratings alongside the dimensions of animacy with the EEG RDMs. We observed that the “being animate” ratings explained a significant amount of variance in the EEG responses, similar in magnitude and timing to the variance explained by the dimensions of animacy tested (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4</xref>).</p><p id="P30">Most animacy dimensions explained variance in EEG recordings. Is it the same or unique variance? Does one dimension explain more unique variance than the others, as in the case of animacy ratings, or is there no difference between the amount of unique variance explained by each dimension, as for the similarity judgements? We found that only one dimension - "looking like an animal"- explained the unique variance in EEG recordings between 237 and 301 ms. None of the other dimensions explained any significant unique variance (<xref ref-type="fig" rid="F6">Figure 6c</xref>). We wanted to check whether the unique variance explained by the “looking like an animal” dimension in the EEG data can be related to low-level visual features or whether this dimension explains unique variance over and above the variance explained by low-level features. We therefore included the first convolutional layer of AlexNet as a model of low-level visual features in the unique variance analysis in addition to the five dimensions of animacy studied here. We observed that our result of “looking like an animal” explaining a significant amount of the unique variance still holds. “Looking like an animal” explains the unique variance not explained by either the other four dimensions or the first convolutional layer of AlexNet (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 7</xref>).</p><p id="P31">Overall, most dimensions of animacy explained EEG recordings with subtle differences in timing, but only ‘looking like an animal’ explained unique variance. Even for the rapid object recognition time course, higher-level dimensions of animacy explained a significant amount of variance in brain representations.</p></sec><sec id="S8"><title>Contribution of each animacy dimension to fMRI representations</title><p id="P32">We asked where in the brain dimensions of animacy explain patterns of responses to images using fMRI. We performed both regions of interest (ROI) analysis along the ventral and dorsal visual streams and searchlight analysis. The ROI analysis was performed to evaluate the contribution of dimensions of animacy in the brain regions along the visual stream where we know object images are represented. The searchlight analysis complemented the ROI analysis testing whether other regions in the visual stream exist where dimensions of animacy explain variance that we may have missed when preselecting ROIs.</p><p id="P33">We first evaluated the contribution of each animacy dimension in ROIs across the ventral: visual area 1 (V1v), ventral occipital cortex 2 (VO2), parahippocampal cortex 2 (PHC2) and dorsal: visual area 1 (V1d), lateral occipital cortex 2 (LO2), TO2 visual streams (<xref ref-type="fig" rid="F7">Figure 7a</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 9</xref> - with displayed noise ceiling). To define ROIs, we used a Probabilistic brain atlas (<xref ref-type="bibr" rid="R46">Wang et al., 2015</xref>). The locations of the examined ROIs are presented in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8</xref>. In the ventral visual stream, “being unpredictable”, “having mobility”, and to a lesser extent “having agency” explained variance in V1v, in contrast to higher-level visual areas (VO2, PHC2) where additionally “looking like an animal” explained a significant amount of variance and “having agency” explained more variance than in V1v. In the dorsal visual stream, “being unpredictable” and “having mobility” explained variance in V1d, with “having agency” explaining variance in higher-level visual areas (LO2, TO2), and “looking like an animal explaining variance only in TO2. As the dorsal stream carries information related to movement it is intuitive that “having mobility” and “being unpredictable" explain variance in dorsal regions. It was not clear, however, whether “having agency” and “looking like an animal” would explain variance in dorsal regions but it is indeed what we observe. After examining the contribution of general “being animate” ratings to the fMRI ROI representations, we did not see that “being animate” explained a significant amount of variance in fMRI data (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 10</xref>). For completeness, we performed the same analysis for all regions in ventral (V1v, V2v, hV4, VO1, VO2, PHC1, PHC2) and dorsal (V1d, V2d, V3d, V3a, V3b, LO1, LO2, TO1, TO2) visual streams. We observed that more dimensions of animacy studied here explain variance in higher-level visual cortex than in early visual cortex (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figures 11</xref> and <xref ref-type="supplementary-material" rid="SD1">12</xref>).</p><p id="P34">To gain an intuition about the structure of the representations, we displayed MDS plots for the ROIs. As expected, the stimuli in early visual regions were grouped by shapes and colours, whereas we could see clusters of faces and forces of nature in higher-level visual regions (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 13</xref>). In addition, the colour-coded MDS plot for PHC2 based on the binary dimensions of animacy revealed mild categorical structure for some dimensions (e.g., “looking like an animal”, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 14</xref>).</p><p id="P35">The results of the unique variance analysis are included in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 15</xref>. “Being unpredictable”, “having mobility”, and “having agency” dimensions explained unique variance in early visual cortex. Except “being alive” dimension, four dimensions of animacy explained unique variance in higher-level visual areas. We also wanted to test whether the five dimensions of animacy studied here explained variance over and above low-level visual features in the fMRI ROI analysis. As for the EEG analysis, we therefore included the first convolutional layer of AlexNet along with the five dimensions of animacy in the unique variance analysis. Similarly to behavioural and EEG results, selected dimensions of animacy explained variance over and above low-level visual features of AlexNet’s first convolutional layer (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 16</xref>). As expected, the unique contribution of the first convolutional layer of AlexNet was greater in the early and intermediate visual cortex in comparison to the high-level visual cortex (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 16</xref>). These results suggest that the dimensions of animacy are important for both ventral and dorsal visual streams, however, a substantial variance remained unexplained.</p><p id="P36">To investigate the contribution of each animacy dimension in a spatially unbiased fashion we performed a searchlight analysis in the visual stream (<xref ref-type="fig" rid="F7">Figure 7b</xref>). Consistently with the ROI analysis “being alive” did not explain any significant amount of variance in the brain. “Looking like an animal” also did not explain any significant amount of variance in searchlight analysis but it is possible that with a larger amount of data we would see this dimension explaining some variance as the ROI analysis and EEG results pointed in that direction. “Having mobility” dimension explained variance in early visual cortex only (based on the correlation strength of the searchlight analysis). In contrast, “having agency” explained variance only in higher-level visual cortex, which is consistent with the ROI analysis where “having agency” explains more variance in higher-level visual areas and further suggests that this dimension best captures higher-level representations. One dimension that explained variance in both early and higher-level visual areas was “being unpredictable” suggesting that unpredictability is important for attention and is already detected in the early visual cortex. Despite the living non-living distinction being thought to be important for brain representations “being alive” dimension did not explain any significant amount of variance in brain responses based on the ROI and searchlight analyses. This suggests that the ventral and dorsal visual streams do not represent the results of a deeper cognitive inference process that would assess whether something is alive.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P37">We investigated the representation of the five selected facets of animacy in brain and behaviour (summary in <xref ref-type="fig" rid="F8">Figure 8</xref>) and concluded that different brain regions sensitive to animacy may actually represent distinct dimensions, either as accessible perceptual stepping stones toward detecting whether something is alive or because they are of behavioural importance in their own right. Including multiple dimensions in a linear model enabled us to disentangle their roles. To increase the power of these analyses, we decorrelated the five selected dimensions of animacy using an optimized set of natural stimuli. We managed to reduce the maximum pairwise correlation from 0.64 (for random selection) to 0.36 (optimized). It may not be possible to create a stimulus set that has no correlations between dimensions at all using natural images. However, as long as the predictors of the model do not form a linearly dependent set, they can be disentangled in analysis by considering the unique variance explained by each. Historically, our analysis is related to studying one predictor of interest at a time, e.g., face-selective regions (<xref ref-type="bibr" rid="R18">Kanwisher et al., 1997</xref>). More recently, studies focused on decorrelating two predictors e.g., shape and animacy (<xref ref-type="bibr" rid="R33">Proklova et al., 2016</xref>). However, to our knowledge, none of the studies tried to decorrelate as many as the five selected dimensions of animacy. More broadly, our novel stimulus selection procedure using a genetic algorithm could be adopted to disentangle other multidimensional concepts beyond animacy.</p><p id="P38">The brain activity patterns did not fall into a small number of clusters, such as the animate and inanimate clusters observed in <xref ref-type="bibr" rid="R23">Kriegeskorte et al. (2008)</xref>. This does not contradict previous findings, but rather reflects the design of the stimulus set, which was optimized to reveal the ambiguities at the boundaries between the categories, far from the prototypically animate and prototypically inanimate stimuli. The <xref ref-type="bibr" rid="R23">Kriegeskorte et al. (2008)</xref> study is a good example of what happens when a wide range of common things are selected to define the stimulus set: many of them are low on all of the animacy dimensions studied here and many others are high on all of them. Here, by contrast, the stimulus set was designed to evenly populate a 5-dimensional space. The prototypes of animate and inanimate things fall in diametrically opposed corners of this 5-dimensional space. Those two corners are populated by very few stimuli (those that are either low on all five dimensions or high on all five dimensions). Most of the stimuli sample the unknown territory close to the boundary, which has not been explored in previous studies.</p><p id="P39">When evaluating variance explained by different dimensions it is important to keep in mind that the dimensions may not be equally represented by the stimuli. This issue can be illustrated with a conceptual experiment that aims to compare the contribution of colour and orientation in explaining early visual responses. If the presented stimuli vary in orientation by a few degrees only, whereas they vary in colour by a wide range of hues, then we can’t directly compare the contribution of orientation and colour dimensions. Since the variance of orientation would be much smaller than the variance of colour, colour will most likely contribute more to explaining the representations. A more fair comparison would be if we used all 360 degrees for orientation and all possible colour hues for colour. Unlike in the example above we use the same scale for all the dimensions of animacy (values between -10 and 10), which make the dimensions comparable to each other. However, a similar issue exists in our study in that it is impossible to equalize the distribution of values for each dimension using a natural stimulus set. We already know that some of the dimension combinations are more sampled than others. This issue does not change the interpretation of our findings but rather points at the overall issue that arises when multidimensional concepts are compared. In future studies, it will be important to validate the generalizability of the results presented here with a larger stimulus set spanning a wide range of object categories.</p><p id="P40">Despite its prominence in the neuroscience literature (<xref ref-type="bibr" rid="R13">Funnell &amp; Sheridan, 1992</xref>; <xref ref-type="bibr" rid="R35">Ralph et al., 1998</xref>; <xref ref-type="bibr" rid="R44">Silveri et al., 1997</xref>), the living/non-living distinction (“being alive”) did not explain variance in brain representations. This finding suggests that the ventral and dorsal visual streams do not represent the results of a deeper cognitive inference process that would assess whether something is alive. Our EEG result is consistent with a magnetoencephalography (MEG) study where the “living” dimension did not explain much variance in MEG representations (<xref ref-type="bibr" rid="R11">Contini et al., 2019</xref>). The fact that “looking like an animal” was the only dimension that explained significant unique variance in the EEG data may be because this dimension provides an accessible visual correlate of animacy that can be computed by the visual system. For a stimulus set where dimensions of animacy were not decorrelated, we predict that “being alive” would explain substantial variance. Our decorrelated stimulus set revealed that other dimensions underlie the responses to living things. Likely “having agency” dimension has captured some variance of the “being alive” dimension in the brain responses as the “being alive” dimension was correlated with the “having agency” dimension (<xref ref-type="fig" rid="F1">Figure 1b</xref>). Studying the interactions between the dimensions will be an important future work direction. The “being alive” dimension did explain variance in animacy ratings and similarity judgements suggesting that this dimension is present in cognition despite its lack of prevalence in the brain responses. Our result of differential representations of dimensions of animacy in brain measurement modalities and in brain and behavioural responses is consistent with previous studies that investigated a given animacy dimension of interest. For example, MEG patterns do not seem to carry information about the animate vs inanimate object category, in contrast to fMRI (<xref ref-type="bibr" rid="R34">Proklova et al., 2019</xref>). Another study has shown that animate-looking (e.g., cow mug) and animate objects (e.g., cow) are dissociated in behaviour but not in the ventral visual stream (<xref ref-type="bibr" rid="R8">Bracci et al., 2019</xref>). Little agreement between behavioural similarity judgements and 7T fMRI responses has also been found in <xref ref-type="bibr" rid="R20">King et al., 2019</xref>.</p><p id="P41">Consistent with higher-cognitive contributions, the “having agency” dimension explained significant variance in higher-level visual areas, explained more variance than most other animacy dimensions later in time, and was prominent in the judgements. The observation of the “having agency” dimension explaining high-level visual representations is consistent with previous studies that showed agency representations in the fusiform gyrus (<xref ref-type="bibr" rid="R15">Gobbini et al., 2007</xref>, <xref ref-type="bibr" rid="R42">Shultz &amp; McCarthy, 2014</xref>) and the ventral visual cortex (<xref ref-type="bibr" rid="R45">Thorat et al., 2019</xref>). Gobbini et al. used point-light displays as stimuli rather than images of natural objects used in this study. Shultz &amp; McCarthy used two computer-animated avatar characters. Stimuli in Throat et al. were from 40 animal categories. The number and the diversity of stimuli in our study allowed us to disentangle the dimensions of animacy experimentally with higher precision. The “having agency” dimension was least consistently judged in behavioural animacy ratings suggesting that different people have different intuitions on whether something has agency; for example, our participants were divided as to whether the robots in our stimuli had agency or not. Seeing agency (or not) in robots mirrors an ongoing debate in society and may influence how humans interact with the increasing presence of robots in their environment.</p><p id="P42">The “being unpredictable” dimension captured representations in both lower and higher-level visual cortex and earlier in time. One interpretation of this finding is that unpredictable things require attention and need to be processed early on. Humans need to know what to attend to in the visual world and if something unpredictable happens it captures attention, which enables us to stay on top of what is happening around us. This dimension of animacy has been studied only in the language domain and only in the context of natural forces (<xref ref-type="bibr" rid="R29">Lowder &amp; Gordon, 2015</xref>), but we now show that “being unpredictable” also explains visual representations using a variety of stimuli. Lowder &amp; Gordon found that natural forces are processed like animate entities during online sentence processing based on an eye-tracking experiment. They propose an alternative explanation why the “being unpredictable” may be important in contrast to the attention-based explanation mentioned above. They claim that the “being unpredictable” dimension reflects a cognitive and linguistic focus on casual explanations that increase the predictability of events, which could offer an alternative theory to explain our results.</p><p id="P43">Our study did not include human-centred dimensions (human-likeness (<xref ref-type="bibr" rid="R39">Rosenthal-von der Pütten et al., 2019</xref>), humanness (<xref ref-type="bibr" rid="R12">Contini et al., 2020</xref>), resemblance to human faces and bodies (<xref ref-type="bibr" rid="R36">Ritchie et al., 2021</xref>), capacity for self-movement and thought rather than face presence (<xref ref-type="bibr" rid="R32">Proklova &amp; Goodale, 2022</xref>). It would be interesting to include these interpretations of animacy in a follow-up study when designing a larger stimulus set with a greater number of dimensions of animacy that we try to disentangle. Some of the dimensions tested in this study (e.g., "looking like an animal") may be correlated with the human-centred dimensions (e.g., has a face) and exploring the relationship between the five selected dimensions of animacy and the human-centred dimensions could be a focus of future studies. Previous observations that animate versus inanimate objects can be distinguished using classifiers based on colour information (<xref ref-type="bibr" rid="R38">Rosenthal et al., 2018</xref>), as well as curvature (<xref ref-type="bibr" rid="R27">Long et al., 2017</xref>) and mid-level shape features (<xref ref-type="bibr" rid="R40">Schmidt et al., 2017</xref>) inspired our control analyses that tested whether the five selected dimensions of animacy can explain unique variance over and above the low-level visual features and indeed they can. Future stimulus selection procedures using a genetic algorithm could benefit from accounting for the low-level visual features at the stimulus set selection stage.</p><p id="P44">Our study is the first one to our knowledge that disentangled the five selected dimensions of animacy and will pave the way for future studies. For example, more work is needed to understand how exactly something quite abstract like "agency" or “unpredictability” is computed from visual stimuli. Testing alternative theories of animacy dimension computations could be addressed by comparing different classes of models in their ability to explain the data. The subsequent acquisition of higher resolution fMRI data (7T) would provide insights into the finer-grained spatial organisation of animacy dimensions and their representations across cortical layers. Future studies may extend our approach to videos because some of the dimensions like "having mobility" may be better represented dynamically.</p><p id="P45">In summary, we disentangled the five selected dimensions of animacy using a novel approach for stimulus decorrelation and showed the contribution of each animacy dimension in explaining human brain representations and behavioural judgements. These dimensions captured behaviour well. A significant amount of variance in brain representations was also explained by most dimensions, with the surprising exception of “being alive”. Our results suggest that different brain regions sensitive to animacy may represent distinct dimensions, either as accessible perceptual stepping stones toward detecting whether something is alive or because they are of behavioural importance by themselves. Future studies may expand on the representation of each of the dimensions while avoiding their entanglement and may apply our dimension disentanglement approach to other multidimensional concepts.</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>Stimulus set generation</title><sec id="S12"><title>Filling animacy dimension grid combinations</title><p id="P46">We created a grid with all possible combinations of dimensions of animacy (2^5 = 32). We asked participants (S = 11, mean age = 33, 6 females) to write down object category names (e.g., “humanoid robot”) for each combination in the grid to obtain a list of object categories (<xref ref-type="fig" rid="F1">Figure 1a</xref> Step 1). Participants listed 100 categories, and we selected 3 images per category (total = 300 images, <xref ref-type="fig" rid="F1">Figure 1a</xref> Step 2), which formed the basis for the experiment to rate the dimensions of animacy.</p></sec><sec id="S13"><title>Ratings of dimensions of animacy to generate stimulus set</title><p id="P47">Twenty-six participants (mean age = 25, 21 females) performed animacy ratings of 300 object images through an on-line web-based interface. Participants judged each object image using a continuous scale from -10 to +10 for each dimension, e.g., -10 meant “dead” and +10 meant “alive” for the “being alive” dimension (<xref ref-type="fig" rid="F1">Figure 1a</xref> Step 3). Thirty images were repeated for a within-participant consistency measure.</p></sec><sec id="S14"><title>Stimuli subset selection using a genetic algorithm</title><p id="P48">To select a subset of 128 images for which ratings on the dimensions of animacy were maximally decorrelated we used a genetic algorithm. A genetic algorithm is an optimization method that mimics biological evolution through natural selection. Fitness was defined as minimising the maximum correlation between dimensions of animacy (<xref ref-type="fig" rid="F1">Figure 1a</xref> Step 4). We also introduced a penalty if the algorithm selected more than two stimuli from the same category (to ensure that stimuli were selected from a wide range of categories) and if the algorithm did not select at least one image of a human face and a human body (to have a reference point of object images that we know should have high ratings on the dimensions of animacy).</p></sec><sec id="S15"><title>Stimuli</title><p id="P49">All stimuli are displayed in <xref ref-type="fig" rid="F2">Figure 2a</xref>. Stimuli were 128 coloured images of real-world objects with natural backgrounds, selected from the Internet. The same set of stimuli was used in animacy ratings, similarity judgements, EEG, and fMRI experiments.</p></sec><sec id="S16" sec-type="subjects"><title>Participants</title><p id="P50">The same nineteen participants performed an on-line animacy ratings, similarity judgements, EEG, and fMRI experiments (mean age = 27, 13 females). Participants had normal or corrected-to-normal vision. All of them were right-handed. Before completing the experiment, participants received information about the procedure of the experiment and gave their written informed consent. All participants received monetary reimbursement or course credits for their participation. The experiment was conducted in accordance with the Ethics Committee of the Department of Education and Psychology at Free University, Berlin. Participants first completed the EEG and fMRI experiments, then similarity judgements experiment, and finally animacy ratings experiment so that they did not know about specific dimensions of animacy tested while performing EEG, fMRI, and similarity judgements experiments.</p></sec></sec><sec id="S17"><title>Animacy ratings</title><sec id="S18"><title>Experimental design and task</title><p id="P51">Participants judged each object image using a continuous scale from -10 to +10 for each animacy dimension, e.g., -10 meant “dead” and +10 meant “alive” for the “being alive” dimension (<xref ref-type="fig" rid="F3">Figure 3a</xref>). Additionally, participants performed a rating of "being animate" dimension in a similar fashion.</p></sec></sec><sec id="S19"><title>Similarity judgements</title><sec id="S20"><title>Experimental design and task</title><p id="P52">We acquired pairwise object-similarity judgements for all 128 images by asking participants to perform an on-line multi-arrangement task using Meadows platform (<ext-link ext-link-type="uri" xlink:href="http://www.meadows-research.com/">http://www.meadows-research.com/</ext-link>). During this task, object images were shown on a computer screen in a circular arena, and participants were asked to arrange the objects by their similarity, such that similar objects were placed close together and dissimilar objects were placed further apart (<xref ref-type="fig" rid="F5">Figure 5a</xref>). The multi-arrangement method uses an adaptive trial design, showing all object images on the first trial, and selecting subsets of objects with weak dissimilarity evidence for subsequent trials. To determine which stimuli to select for the next trial, the evidence weight of each stimulus had an evidence utility exponent (E=10) applied to it, to calculate its utility if the stimulus was picked. The similarity judgements task was completed if, among all pairs of stimuli, the pair with the lowest evidence had an evidence weight higher than 0.5. The multiarrangement method allows the efficient acquisition of a large number of pairwise similarities. We deliberately did not specify which object properties to focus on, to avoid biasing participants’ spontaneous mental representation of the similarities between objects. We aimed to obtain similarity judgements that reflect the natural representation of objects without forcing participants to rely on one given dimension. However, participants were asked after having performed the task, what dimension(s) they used in judging object similarity. All participants reported arranging the images according to categorical clusters. The reports suggest that participants used a consistent strategy throughout the experiment. The method of the object similarity judgements has been described in <xref ref-type="bibr" rid="R30">Mur et al., 2013</xref>, where further details can be found.</p></sec></sec><sec id="S21"><title>EEG</title><sec id="S22"><title>Experimental design and task</title><p id="P53">Stimuli were presented at the centre of the screen for 500 ms, while participants performed a paper clip detection task. Stimuli were overlaid with a light grey fixation cross and displayed at a width of 4° visual angle. Participants completed 15 trials. Each image was presented twice in every trial in random order with an inter-trial interval of 1–1.1 s. Participants were asked to press a button and blink their eyes in response to a paper clip image shown randomly every 3 to 5 trials (mean performance 99% (+/- 0.09, standard error)). These trials were excluded from the analysis.</p></sec><sec id="S23"><title>Acquisition</title><p id="P54">The electroencephalogram (EEG) signals were acquired using BrainVision actiCHamp EASYCAP 64 channel system at a sampling rate of 1,000 Hz. The arrangement of the electrodes followed the standard 10-20 system.</p></sec><sec id="S24"><title>Preprocessing</title><p id="P55">The time series were analysed with Brainstorm (<ext-link ext-link-type="uri" xlink:href="http://neuroimage.usc.edu/brainstorm/">http://neuroimage.usc.edu/brainstorm/</ext-link>). We extracted EEG patterns for each millisecond time point (from 100 ms before stimulus onset to 1000 ms after stimulus onset) for each trial. We filtered the responses between 0 and 50Hz.</p></sec><sec id="S25"><title>Decoding</title><p id="P56">We performed pairwise decoding across stimuli using a support vector machine (SVM) approach (<xref ref-type="bibr" rid="R9">Cichy et al., 2014</xref>). For each time point, EEG signals were arranged in 64-dimensional vectors (corresponding to the 64 EEG channels), yielding M = 30 pattern vectors per time point and condition. We sub-averaged the M vectors in groups of k = 5 with random assignment, obtaining L = M/k averaged pattern vectors. This procedure was performed to reduce computational load and improve the signal-to-noise ratio. Subsequently, for each pair of conditions, we assigned L-1 averaged pattern vectors to train a linear SVM using the LibSVM implementation (<ext-link ext-link-type="uri" xlink:href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</ext-link>). We used the trained SVM to predict the condition labels of the left-out testing data set consisting of the Lth averaged pattern vector. This process was repeated 100 times with random assignment of the M raw pattern vectors to L averaged pattern vectors. For every time point, we assigned the average decoding accuracy to a decoding accuracy matrix.</p></sec><sec id="S26"><title>Peak latency analysis</title><p id="P57">We defined peaks of the decoding accuracy as time points with the maximum decoding accuracy.</p></sec></sec><sec id="S27"><title>fMRI</title><sec id="S28"><title>Experimental design and task</title><p id="P58">Stimuli were presented using a rapid event-related design (stimulus duration, 500 ms) while participants performed a fixation-cross-brightness-change detection task, and their brain activity was measured with a 3T fMRI scanner. Stimuli were overlaid with a light grey fixation cross and displayed at a width of 4° visual angle. Each image was presented once per run in random order. Each run contained 32 randomly timed null trials (null trial duration, 500 ms) without the stimulus presentation (grey square background with a fixation cross). Participants had to report a short (100 ms) change in the luminance of the fixation cross via a button press (mean performance 97% (+/- 0.14, standard error)). On average, reaction times for fixation cross trials were 0.55 s (+/- 0.06 s, 2.45 s before the subsequent stimulus trial). The fixation cross was always present between stimuli or null trial presentations.</p></sec><sec id="S29"><title>Acquisition</title><p id="P59">Magnetic resonance imaging was acquired using Siemens 3T Trio with a 12-channel head coil. For structural images, we used a standard T1-weighted sequence (176 slices). The TR was 2 s, and the inter-trial-interval was 3 s. For fMRI, we conducted 9–13 runs in which 249 volumes were acquired for each participant. The number of runs varied per participant as different participants took a different number of breaks during the experiment and different amount of time was needed for them to become comfortable in the scanner. The average number of runs per participant was 10.9 (+/- 0.07, standard error). The acquisition volume covered the full brain.</p></sec><sec id="S30"><title>Estimation of single-image activity patterns</title><p id="P60">The fMRI data were preprocessed using SPM12 (<ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/">https://www.fil.ion.ucl.ac.uk/spm/</ext-link>). For each participant and session separately, functional data were spatially realigned, slice-time corrected, and coregistered to the participant-individual T1 structural image. We estimated the fMRI responses to the 128 image conditions with a general linear model (GLM), which included movement parameters as nuisance terms. To obtain a t-value for each voxel and condition, GLM parameter estimates for each condition/stimulus were contrasted against a baseline. To assess the degree of the general visual stimulation we contrasted the parameter estimates for all images against the baseline.</p></sec><sec id="S31"><title>Definition of regions of interest</title><p id="P61">For the ROI definition, we used a Probabilistic brain atlas (<xref ref-type="bibr" rid="R46">Wang et al., 2015</xref>). Anatomical masks were reverse-normalized from MNI-space to single-participant space. For each ROI, we extracted a multivoxel pattern of activity (t-values) for each of the 128 stimuli. We included 100 most strongly activated voxels in the ROI analysis.</p></sec><sec id="S32"><title>Searchlight analysis</title><p id="P62">To analyse fMRI data in a spatially unbiased approach, we performed a volume-based searchlight analysis (<xref ref-type="bibr" rid="R21">Kriegeskorte et al., 2006</xref>) in each participant (radius of 4 voxels) with each animacy dimension RDM. We restricted the voxels included in the significance testing to visual stream areas (one-sided Wilcoxon signed-rank test).</p></sec><sec id="S33"><title>Construction of representational dissimilarity matrix</title><p id="P63">We used the representational similarity analysis toolbox for animacy dimension comparison (<xref ref-type="bibr" rid="R31">Nili et al., 2014</xref>). We computed response patterns (across animacy ratings, similarity judgements, EEG, and fMRI signals) for each image. We then computed response-pattern dissimilarities between images (using Euclidean distance as a metric) and placed these in a representational dissimilarity matrix (RDM). An RDM captures which distinctions among stimuli are emphasized and which are de-emphasized.</p></sec><sec id="S34"><title>Inferential analysis of model performance</title><p id="P64">We estimated animacy dimension performance by correlating the animacy dimension and data RDMs using Spearman's correlation coefficient. We determined whether each of the animacy dimension RDMs was significantly related to the data RDMs using a participant-as-random-effect analysis (one-sided Wilcoxon signed-rank test). We subsequently tested for differences in animacy dimension performance between each pair of dimensions of animacy using a participant-as-random-effect analysis (two-sided Wilcoxon signed-rank test). For each analysis, we accounted for multiple comparisons by controlling the FDR at 0.05.</p></sec><sec id="S35"><title>Unique variance analysis</title><p id="P65">We used a hierarchical general linear model (GLM) to evaluate the unique variance explained by dimensions of animacy (<xref ref-type="bibr" rid="R19">Kietzmann et al., 2019</xref>). For each animacy dimension m, the unique variance was computed by subtracting the total variance explained by the reduced GLM (excluding the dimension of interest) from the total variance explained by the full GLM. Specifically, for dimension m, we fit GLM on X = "all dimensions but m" and Y = data, then we subtract the resulting R<sup>2</sup> from the total R<sup>2</sup> (fit GLM on X = "all dimensions" and Y = data). We performed this procedure for each participant and used non-negative least squares to find optimal weights. A constant term was included in the GLM model. We performed a one-sided Wilcoxon signed-rank test to evaluate the significance of the unique variance contributed by each dimension across participants controlling the expected false discovery rate at 0.05.</p></sec><sec id="S36"><title>Statistical analyses</title><p id="P66">We used non-parametric tests across the manuscript as detailed above.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS153648-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d100aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S37"><title>Acknowledgements</title><p>This research was supported by the Wellcome Trust grant [206521/Z/17/Z] awarded to KMJ; the Alexander von Humboldt Foundation postdoctoral fellowship awarded to KMJ; the German Research Council grants [CI241/1-1, CI241/3-1, CI241/7-1] awarded to RMC; and the European Research Council grant [ERC-StG-2018-803370] awarded to RMC. For the purpose of open access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p></ack><sec id="S38" sec-type="data-availability"><title>Data availability</title><p id="P67">The datasets generated during the current study are available from the corresponding author on request.</p></sec><sec id="S39" sec-type="data-availability"><title>Code availability</title><p id="P68">The code generated during the current study is available from the corresponding author on request.</p></sec><fn-group><fn id="FN2" fn-type="con"><p id="P69"><bold>Author contributions</bold></p><p id="P70">KMJ, IC, NK and RMC designed the experiments. KMJ and EN collected the data. JJFB helped to set up an on-line similarity judgements experiment. KMJ performed the analyses. KMJ, RMC, and NK wrote the paper. All authors edited the paper. NK and RMC supervised the work.</p></fn><fn id="FN3" fn-type="conflict"><p id="P71"><bold>Competing financial interests</bold></p><p id="P72">The authors declare that they have no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><article-title>Categories of knowledge? unfamiliar aspects of living and nonliving things</article-title><source>Cognitive Neuropsychology</source><volume>9</volume><issue>2</issue><fpage>135</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1080/02643299208252056</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><article-title>Are living and non-living category-specific deficits causally linked to impaired perceptual or associative knowledge? evidence from a category-specific double dissociation</article-title><source>Neurocase</source><volume>4</volume><issue>4-5</issue><fpage>311</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1080/13554799808410630</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><source>Journal of Cognitive Neuroscience</source><volume>17</volume><issue>3</issue><fpage>434</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1162/0898929053279531</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><article-title>Naming deficit for non-living items: Neuropsychological and PET study</article-title><source>Neuropsychologia</source><volume>35</volume><issue>3</issue><fpage>359</fpage><lpage>367</lpage><pub-id pub-id-type="pmid">9051684</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Lee</surname><given-names>KE</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>Parallel visual motion processing streams for manipulable objects and human movements</article-title><source>Neuron</source><year>2002</year><volume>34</volume><issue>1</issue><fpage>149</fpage><lpage>159</lpage><pub-id pub-id-type="pmid">11931749</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beauchamp</surname><given-names>MS</given-names></name><name><surname>Lee</surname><given-names>KE</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>FMRI responses to video and point-light displays of moving humans and manipulable objects</article-title><source>Journal of Cognitive Neuroscience</source><year>2003</year><volume>15</volume><issue>7</issue><fpage>991</fpage><lpage>1001</lpage><pub-id pub-id-type="pmid">14614810</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumenthal</surname><given-names>A</given-names></name><name><surname>Stojanoski</surname><given-names>B</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>Cusack</surname><given-names>R</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name></person-group><article-title>Animacy and real-world size shape object representations in the human medial temporal lobes</article-title><source>Human Brain Mapping</source><year>2018</year><volume>39</volume><issue>9</issue><fpage>3779</fpage><lpage>3792</lpage><pub-id pub-id-type="pmcid">PMC6866524</pub-id><pub-id pub-id-type="pmid">29947037</pub-id><pub-id pub-id-type="doi">10.1002/hbm.24212</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Kalfas</surname><given-names>I</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><article-title>The Ventral Visual Pathway Represents Animal Appearance over Animacy, Unlike Human Behavior and Deep Neural Networks</article-title><source>The Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>33</issue><fpage>6513</fpage><lpage>6525</lpage><pub-id pub-id-type="pmcid">PMC6697402</pub-id><pub-id pub-id-type="pmid">31196934</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1714-18.2019</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nature Neuroscience</source><year>2014</year><volume>17</volume><issue>3</issue><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="pmcid">PMC4261693</pub-id><pub-id pub-id-type="pmid">24464044</pub-id><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Gors</surname><given-names>J</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><source>The Representation of Biological Classes in the Human Brain</source><year>2012</year><volume>32</volume><issue>8</issue><fpage>2608</fpage><lpage>2618</lpage><pub-id pub-id-type="pmcid">PMC3532035</pub-id><pub-id pub-id-type="pmid">22357845</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5547-11.2012</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contini</surname><given-names>EW</given-names></name><name><surname>Goddard</surname><given-names>E</given-names></name><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Williams</surname><given-names>M</given-names></name><name><surname>Carlson</surname><given-names>T</given-names></name></person-group><article-title>A humanness dimension to visual object coding in the brain</article-title><source>Neuroscience</source><year>2019</year><comment>[Preprint]</comment><pub-id pub-id-type="pmid">32663643</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Contini</surname><given-names>EW</given-names></name><name><surname>Goddard</surname><given-names>E</given-names></name><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Williams</surname><given-names>M</given-names></name><name><surname>Carlson</surname><given-names>T</given-names></name></person-group><article-title>A humanness dimension to visual object coding in the brain</article-title><source>NeuroImage</source><year>2020</year><volume>221</volume><elocation-id>117139</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117139</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funnell</surname><given-names>E</given-names></name><name><surname>Sheridan</surname><given-names>J</given-names></name></person-group><source>Categories of knowledge? Unfamiliar aspects of living and nonliving things</source><volume>20</volume><comment>n.d</comment></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Gentili</surname><given-names>C</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Bellucci</surname><given-names>C</given-names></name><name><surname>Salvini</surname><given-names>P</given-names></name><name><surname>Laschi</surname><given-names>C</given-names></name><name><surname>Guazzelli</surname><given-names>M</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>Distinct Neural Systems Involved in Agency and Animacy Detection</article-title><source>Journal of Cognitive Neuroscience</source><year>2010</year><volume>23</volume><issue>8</issue><fpage>1911</fpage><lpage>1920</lpage><pub-id pub-id-type="pmid">20849234</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Koralek</surname><given-names>AC</given-names></name><name><surname>Bryan</surname><given-names>RE</given-names></name><name><surname>Montgomery</surname><given-names>KJ</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Two takes on the social brain: A comparison of theory of mind tasks</article-title><source>Journal of Cognitive Neuroscience</source><year>2007</year><volume>19</volume><issue>11</issue><fpage>1803</fpage><lpage>1814</lpage><pub-id pub-id-type="pmid">17958483</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gray</surname><given-names>HM</given-names></name><name><surname>Gray</surname><given-names>K</given-names></name><name><surname>Wegner</surname><given-names>DM</given-names></name></person-group><source>Supporting Online Material for Dimensions of Mind Perception</source><year>2007</year><month>February</month><volume>619</volume><fpage>10</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1126/science.1134475</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Nishimoto</surname><given-names>S</given-names></name><name><surname>Vu</surname><given-names>AT</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain</article-title><source>Neuron</source><year>2012</year><volume>76</volume><issue>6</issue><fpage>1210</fpage><lpage>1224</lpage><pub-id pub-id-type="pmcid">PMC3556488</pub-id><pub-id pub-id-type="pmid">23259955</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.014</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: A module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience: The Official Journal of the Society for Neuroscience</source><year>1997</year><volume>17</volume><issue>11</issue><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Recurrence is required to capture the representational dynamics of the human visual system</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><issue>43</issue><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="pmcid">PMC6815174</pub-id><pub-id pub-id-type="pmid">31591217</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>ML</given-names></name><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Steel</surname><given-names>A</given-names></name><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name></person-group><article-title>Similarity judgments and cortical visual responses reflect different properties of object and scene categories in naturalistic images</article-title><source>NeuroImage</source><year>2019</year><volume>197</volume><fpage>368</fpage><lpage>382</lpage><pub-id pub-id-type="pmcid">PMC6591094</pub-id><pub-id pub-id-type="pmid">31054350</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.079</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Information-based functional brain mapping</article-title><source>Proceedings of the National Academy of Sciences</source><year>2006</year><volume>103</volume><issue>10</issue><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="pmcid">PMC1383651</pub-id><pub-id pub-id-type="pmid">16537458</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: Integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><issue>8</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC3730178</pub-id><pub-id pub-id-type="pmid">23876494</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis— Connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><year>2008</year><month>November</month><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>Da</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>Pa</given-names></name></person-group><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><issue>6</issue><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title>ImageNet Classification with Deep Convolutional Neural Networks</article-title><source>Advances In Neural Information Processing Systems</source><year>2012</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.protcy.2014.09.007</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leib</surname><given-names>AY</given-names></name><name><surname>Kosovicheva</surname><given-names>A</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>Fast ensemble representations for abstract visual impressions</article-title><source>Nature Communications</source><year>2016</year><volume>7</volume><elocation-id>13186</elocation-id><pub-id pub-id-type="pmcid">PMC5116093</pub-id><pub-id pub-id-type="pmid">27848949</pub-id><pub-id pub-id-type="doi">10.1038/ncomms13186</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Störmer</surname><given-names>VS</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name></person-group><article-title>Mid-level perceptual features contain early cues to animacy</article-title><source>Journal of Vision</source><year>2017</year><volume>17</volume><issue>6</issue><fpage>20</fpage><pub-id pub-id-type="pmid">28654965</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Looser</surname><given-names>CE</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Wheatley</surname><given-names>T</given-names></name></person-group><article-title>Multivoxel patterns in face-sensitive temporal regions reveal an encoding schema based on detecting life in a face</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2013</year><volume>8</volume><issue>7</issue><fpage>799</fpage><lpage>805</lpage><pub-id pub-id-type="pmcid">PMC3791074</pub-id><pub-id pub-id-type="pmid">22798395</pub-id><pub-id pub-id-type="doi">10.1093/scan/nss078</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowder</surname><given-names>MW</given-names></name><name><surname>Gordon</surname><given-names>PC</given-names></name></person-group><article-title>Natural forces as agents: Reconceptualizing the animate-inanimate distinction</article-title><source>Cognition</source><year>2015</year><volume>136</volume><fpage>85</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC4308490</pub-id><pub-id pub-id-type="pmid">25497518</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2014.11.021</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Meys</surname><given-names>M</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>Pa</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Human Object-Similarity Judgments Reflect and Transcend the Primate-IT Object Representation</article-title><source>Frontiers in Psychology</source><year>2013</year><month>March</month><volume>4</volume><fpage>128</fpage><pub-id pub-id-type="pmcid">PMC3605517</pub-id><pub-id pub-id-type="pmid">23525516</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00128</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>A toolbox for representational similarity analysis</article-title><source>PLoS Computational Biology</source><year>2014</year><volume>10</volume><issue>4</issue><elocation-id>e1003553</elocation-id><pub-id pub-id-type="pmcid">PMC3990488</pub-id><pub-id pub-id-type="pmid">24743308</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name></person-group><article-title>The role of animal faces in the animate-inanimate distinction in the ventral temporal cortex</article-title><source>Neuropsychologia</source><year>2022</year><volume>169</volume><elocation-id>108192</elocation-id><pub-id pub-id-type="pmid">35245528</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate-Inanimate Distinction</article-title><source>Journal of Cognitive Neuroscience</source><year>2016</year><volume>28</volume><issue>5</issue><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>MEG sensor patterns reflect perceptual but not categorical similarity of animate and inanimate objects</article-title><source>NeuroImage</source><year>2019</year><volume>193</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="pmid">30885785</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Howard</surname><given-names>D</given-names></name><name><surname>Nightingale</surname><given-names>G</given-names></name><name><surname>Ellis</surname><given-names>AW</given-names></name></person-group><article-title>Are Living and Non-living Category-specific Deficits Causally Linked to Impaired Perceptual or Associative Knowledge?</article-title><source>Evidence From a Category-specific Double Dissociation</source><volume>28</volume><comment>n.d</comment></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Zeman</surname><given-names>AA</given-names></name><name><surname>Bosmans</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>S</given-names></name><name><surname>Verhaegen</surname><given-names>K</given-names></name><name><surname>Op de Beeck</surname><given-names>HP</given-names></name></person-group><article-title>Untangling the Animacy Organization of Occipitotemporal Cortex</article-title><source>The Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>33</issue><fpage>7103</fpage><lpage>7119</lpage><pub-id pub-id-type="pmcid">PMC8372013</pub-id><pub-id pub-id-type="pmid">34230104</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2628-20.2021</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Hocking</surname><given-names>J</given-names></name><name><surname>Mechelli</surname><given-names>A</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Price</surname><given-names>C</given-names></name></person-group><source>Fusiform Activation to Animals is Driven by the Process , Not the Stimulus</source><year>2005</year><fpage>434</fpage><lpage>445</lpage><pub-id pub-id-type="pmid">15814003</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal</surname><given-names>I</given-names></name><name><surname>Ratnasingam</surname><given-names>S</given-names></name><name><surname>Haile</surname><given-names>T</given-names></name><name><surname>Eastman</surname><given-names>S</given-names></name><name><surname>Fuller-Deets</surname><given-names>J</given-names></name><name><surname>Conway</surname><given-names>BR</given-names></name></person-group><article-title>Color statistics of objects, and color tuning of object cortex in macaque monkey</article-title><source>Journal of Vision</source><year>2018</year><volume>18</volume><issue>11</issue><fpage>1</fpage><pub-id pub-id-type="pmcid">PMC6168048</pub-id><pub-id pub-id-type="pmid">30285103</pub-id><pub-id pub-id-type="doi">10.1167/18.11.1</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenthal-von der Pütten</surname><given-names>AM</given-names></name><name><surname>Krämer</surname><given-names>NC</given-names></name><name><surname>Maderwald</surname><given-names>S</given-names></name><name><surname>Brand</surname><given-names>M</given-names></name><name><surname>Grabenhorst</surname><given-names>Y</given-names></name></person-group><article-title>Neural Mechanisms for Accepting and Rejecting Artificial Social Partners in the Uncanny Valley</article-title><source>The Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>33</issue><fpage>6555</fpage><lpage>6570</lpage><pub-id pub-id-type="pmcid">PMC6697392</pub-id><pub-id pub-id-type="pmid">31263064</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2956-18.2019</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Hegele</surname><given-names>M</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Perceiving animacy from shape</article-title><source>Journal of Vision</source><year>2017</year><volume>17</volume><issue>11</issue><fpage>10</fpage><pub-id pub-id-type="pmid">28973562</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sha</surname><given-names>L</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Abdi</surname><given-names>H</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name></person-group><article-title>The Animacy Continuum in the Human Ventral Vision Pathway</article-title><source>Journal of Cognitive Neuroscience</source><year>2015</year><volume>27</volume><issue>4</issue><fpage>665</fpage><lpage>678</lpage><pub-id pub-id-type="pmid">25269114</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shultz</surname><given-names>S</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Perceived animacy influences the processing of humanlike surface features in the fusiform gyrus</article-title><source>Neuropsychologia</source><year>2014</year><volume>60</volume><fpage>115</fpage><lpage>120</lpage><pub-id pub-id-type="pmcid">PMC4322763</pub-id><pub-id pub-id-type="pmid">24905285</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.05.019</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shultz</surname><given-names>S</given-names></name><name><surname>van den Honert</surname><given-names>RN</given-names></name><name><surname>Engell</surname><given-names>AD</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Stimulus-induced reversal of information flow through a cortical network for animacy perception</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2015</year><volume>10</volume><issue>1</issue><fpage>129</fpage><lpage>135</lpage><pub-id pub-id-type="pmcid">PMC4994845</pub-id><pub-id pub-id-type="pmid">24625785</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsu028</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silveri</surname><given-names>MC</given-names></name><name><surname>Gainotti</surname><given-names>G</given-names></name><name><surname>Perani</surname><given-names>D</given-names></name><name><surname>Cappelletti</surname><given-names>JY</given-names></name><name><surname>Carbone</surname><given-names>G</given-names></name><name><surname>Fazio</surname><given-names>F</given-names></name></person-group><source>Naming de_cit for non!living items] Neuropsychological and PET study</source><volume>9</volume><comment>n.d</comment></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>ELife</source><year>2019</year><volume>8</volume><elocation-id>e47142</elocation-id><pub-id pub-id-type="pmcid">PMC6733573</pub-id><pub-id pub-id-type="pmid">31496518</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>REB</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><article-title>Probabilistic maps of visual topography in human cortex</article-title><source>Cerebral Cortex</source><year>2015</year><volume>25</volume><issue>10</issue><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="pmcid">PMC4585523</pub-id><pub-id pub-id-type="pmid">25452571</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatley</surname><given-names>T</given-names></name><name><surname>Weinberg</surname><given-names>A</given-names></name><name><surname>Looser</surname><given-names>C</given-names></name><name><surname>Moran</surname><given-names>T</given-names></name><name><surname>Hajcak</surname><given-names>G</given-names></name></person-group><article-title>Mind perception: Real but not artificial faces sustain neural activity beyond the N170/VPP</article-title><source>PLoS ONE</source><year>2011</year><volume>6</volume><issue>3</issue><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3069036</pub-id><pub-id pub-id-type="pmid">21483856</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0017960</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Stimulus-selection procedure and pairwise correlation between animacy dimensions before and after stimulus selection by the genetic algorithm.</title><p><bold>a.</bold> First, we created an animacy grid with all dimensions of animacy combinations and asked 11 participants to fill in the names of objects that fulfilled these combinations. Second, we assembled object images based on object names from step one (images containing a human face are removed according to bioRxiv’s policy on images of individuals). Third, an independent set of 26 participants performed animacy ratings of 300 of these object images. Finally, we selected an optimal set of stimuli that had a low correlation between dimensions (as behaviourally rated) using a genetic algorithm. These stimuli were used in behavioural and brain representation experiments where a new set of participants was recruited to make sure that the stimulus generation and the actual experiments were independent.</p><p><bold>b.</bold> Pairwise correlation between animacy dimensions for the randomly selected 128 stimuli (left) and the 128 stimuli selected by the genetic algorithm (right) in behavioural ratings.</p></caption><graphic xlink:href="EMS153648-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Stimulus set and study overview.</title><p><bold>a.</bold> The genetic-algorithm driven stimulus set consisted of 128 images decorrelated on dimensions of animacy. The stimuli were coloured images of sport equipment, games, robots, dolls and puppets, plush toys, land vehicles, air vehicles, plants, forces of nature (water, air, fire, smoke), sea organisms, cells, organs and fetuses, humans, food, kitchen and office equipment, shadows (images containing a human face are removed according to bioRxiv’s policy on images of individuals).</p><p><bold>b.</bold> Study overview. All 19 participants performed two behavioural studies: animacy ratings and similarity judgements, and two brain response measurement studies: EEG (to access temporal information) and fMRI (to access spatial information). Importantly, participants first performed EEG and fMRI studies, then similarity judgements and finally animacy ratings. This experimental order was to ensure that participants did not know about animacy dimensions tested until the final animacy ratings.</p></caption><graphic xlink:href="EMS153648-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Animacy ratings and their consistency with examples of images judged consistently and not very consistently.</title><p><bold>a.</bold> Illustration of animacy ratings. Participants judged each object image using a continuous scale from -10 to +10 for each animacy dimension, e.g., -10 meant “dead” and +10 meant “alive” for the “being alive” dimension. Additionally, participants performed a rating of “being animate” dimension in a similar fashion.</p><p><bold>b</bold>. Mean ratings of each animacy dimension and stimulus across participants.</p><p><bold>c.</bold> Consistency of each stimulus in animacy ratings across participants (standard error of the mean) with examples of stimuli with varying values of standard error.</p><p><bold>d.</bold> Consistency of each animacy dimension and stimulus in animacy ratings across participants with examples of stimuli with varying values of standard error for the most consistently judged ("looking like an animal") and the least consistently judged ("having agency") dimensions.</p><p><bold>e.</bold> Consistency of each animacy dimension in animacy ratings across participants (standard error of the mean).</p></caption><graphic xlink:href="EMS153648-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Animacy ratings.</title><p><bold>a.</bold> Order of images with lowest and highest ratings on each animacy dimension. Out of 128 images, we show 10 lowest and 10 highest rated images on each animacy dimension (images containing a human face are removed according to bioRxiv’s policy on images of individuals).</p><p><bold>b.</bold> Animacy dimension representational dissimilarity matrices (RDMs) comparisons with animacy ratings (“being animate”) RDMs. Bars show the correlation between the animacy ratings RDMs and each animacy dimension RDM. A significant correlation is indicated by an asterisk (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected). Error bars show the standard error of the mean based on single-participant correlations, i.e., correlations between the single-participant animacy ratings RDMs and animacy dimension RDM. The grey bar represents the noise ceiling, which indicates the expected performance of the true model given the noise in the data. Horizontal lines show pairwise differences between model performance (p &lt; 0.05, FDR corrected across all comparisons).</p><p><bold>c.</bold> Unique variance of each animacy dimension in explaining animacy ratings computed using a general linear model (GLM). For each animacy dimension m, the unique variance was computed by subtracting the total variance explained by the reduced GLM (excluding the dimension of interest) from the total variance explained by the full GLM. Specifically, for dimension m, we fit GLM on X = "all dimensions but m" and Y = data, then we subtract the resulting R2 from the total R2 (fit GLM on X = "all dimensions" and Y = data). We used non-negative least squares to find optimal weights. A significant unique variance is indicated by an asterisk (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected). The error bars show the standard error of the mean based on single-participant unique variance. Horizontal lines show pairwise differences between model performance (p &lt; 0.05, FDR corrected across all comparisons).</p></caption><graphic xlink:href="EMS153648-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Dimensions of animacy and similarity judgements.</title><p><bold>a.</bold> Similarity judgements multiarrangement task. During this task, object images were shown on a computer screen in a circular arena, and participants were asked to arrange the objects according to their similarity, such that similar objects were placed close together and dissimilar objects were placed further apart. Participants performed multiple arrangements of subsets of the images, enabling us to estimate the underlying perceptual similarity space (see <xref ref-type="sec" rid="S10">Methods</xref> for details).</p><p><bold>b.</bold> Multidimensional scaling plot of similarity judgements (mean across participants, with metric stress criterion).</p><p><bold>c.</bold> Animacy dimension RDM comparisons with similarity judgements RDMs. Bars show the correlation between the similarity judgements RDMs and each animacy dimension RDM using the same conventions as in <xref ref-type="fig" rid="F4">Figure 4b</xref>.</p><p><bold>d.</bold> Unique variance of each animacy dimension in explaining similarity judgements computed using the same conventions as in <xref ref-type="fig" rid="F4">Figure 4c</xref>.</p></caption><graphic xlink:href="EMS153648-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Dimensions of animacy and EEG time course.</title><p><bold>a.</bold> Mean decoding curve across participants (pairwise stimuli decoding using a support vector machine approach). Significant decoding is indicated by a horizontal line above the graph (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected) and starts at 43ms (+/- 2ms, standard error) with a peak latency of 197ms (+/- 7ms, standard error, indicated by an arrow). The shaded area around the lines shows the standard error of the mean based on singleparticipant decoding. The grey horizontal bar on the x axis indicates the stimulus duration.</p><p><bold>b.</bold> Animacy dimension RDM comparison with EEG RDMs across time. Lines show the correlation between the EEG RDMs and each animacy dimension RDM. A significant correlation is indicated by a horizontal line above the graph (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected). The grey horizontal bar on the x axis indicates the stimulus duration.</p><p><bold>c.</bold> Unique variance of each animacy dimension in explaining EEG RDMs computed using a GLM. For each animacy dimension, the unique variance is computed by subtracting the total variance explained by the reduced GLM (excluding the animacy dimension of interest) from the total variance explained by the full GLM, using non-negative least squares to find optimal weights. A significant unique variance (between 237 and 301ms) is indicated by a horizontal line above the graph (one-sided Wilcoxon signed-rank test, p &lt; 0.05 corrected). The grey horizontal bar on the x axis indicates the stimulus duration.</p></caption><graphic xlink:href="EMS153648-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Dimensions of animacy and fMRI responses.</title><p><bold>a.</bold> Animacy dimension RDM comparisons with fMRI ROI RDMs. Bars show the correlation between each animacy dimension RDM with fMRI ROI RDMs using the same conventions as in <xref ref-type="fig" rid="F4">Figure 4b</xref>. We selected ROIs across the ventral (V1v, VO2, PHC2) and dorsal (V1d, LO2, TO2) visual streams.</p><p><bold>b.</bold> Searchlight analysis with each animacy dimension showing where in the brain animacy dimension explain image representations masked with the visual stream regions (Spearman’s ρ between animacy dimension and brain representations, one-sided Wilcoxon signed-rank test, FDR controlled at 0.05).</p></caption><graphic xlink:href="EMS153648-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Summary of findings.</title><p>We find that the five tested dimensions of animacy captured behaviour very well. Brain representations were also explained by most dimensions (surprisingly not “being alive”), however, to a lesser extent than behaviour. The living/non-living distinction (“being alive”) features prominently in both dictionary definitions of animacy and the neuroscience literature on brain representations. Consistent with this prominent role, “being alive” accounted for about half the explainable variance in our participants’ object similarity judgements. Surprisingly, however, “being alive” did not explain variance in brain representations. The other four dimensions of animacy explained variance in both brain and behaviour. The “looking like an animal” dimension was the only dimension that explained significant unique variance in the EEG data. One interpretation is that “looking like an animal” provides an accessible visual correlate of animacy that can be computed by the visual system. The “having agency” dimension explained more variance in higher-level visual areas, consistent with the cognitive demands of determining agency. The “being unpredictable” dimension was reflected in representations in both lower and higher-level visual cortex, possibly because unpredictable things require attention. In the fMRI data, three dimensions explained unique variance in early visual cortex (“having agency”, “having mobility”, and “being unpredictable”) and four dimensions in higher visual cortex (all except “being alive”). Our results reveal that different brain regions sensitive to animacy may actually represent distinct dimensions, either as accessible perceptual stepping stones toward detecting whether something is alive or because they are of behavioural importance in their own right.</p></caption><graphic xlink:href="EMS153648-f008"/></fig></floats-group></article>