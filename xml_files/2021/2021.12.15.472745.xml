<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="preprint">
<?all-math-mml yes?>
<?use-mml?>
<?origin ukpmcpa?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">bioRxiv</journal-id>
<journal-title-group>
<journal-title>bioRxiv : the preprint server for biology</journal-title>
</journal-title-group>
<issn pub-type="ppub"/>
</journal-meta>
<article-meta>
<article-id pub-id-type="manuscript">EMS141168</article-id>
<article-id pub-id-type="doi">10.1101/2021.12.15.472745</article-id>
<article-id pub-id-type="archive">PPR434758</article-id>
<article-version article-version-type="publisher-id">1</article-version>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Sentimental Tweets Classification of Symptomatic COVID-19</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Tharun</surname>
<given-names>P</given-names>
</name>
<aff id="A1">Computer Science and Engineering, <italic>St Joseph's College of Engineering</italic>, Chennai, India</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="CR1">
<email>tharunfutur@outlook.com</email>
</corresp>
</author-notes>
<pub-date pub-type="nihms-submitted">
<day>07</day>
<month>02</month>
<year>2022</year>
</pub-date>
<pub-date pub-type="preprint">
<day>21</day>
<month>12</month>
<year>2021</year>
</pub-date>
<permissions>
<ali:free_to_read/>
<license>
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p>
</license>
</permissions>
<abstract>
<p id="P1">The approach I described is straightforward, related to COVID-19 SARS based tweets and the symptoms, that people tweet about. Also, social media mining for health application reports was shared in many different tasks of 2021. The motto at the back of this observe is to analyses tweets of COVID-19 based symptoms. By performing BERT model and text classification with XLNET with which uses to classify text and purpose of the texts (i.e.) tweets. So that I can get a deep understanding of the texts. When developing the system, I used two models the XLNet and DistilBERT for the text sorting task, but the outcome was XLNET out-performs the given approach to the best accuracy achieved. Now I discover a whole lot vital for as it should be categorizing tweets as encompassing self-said COVID-19 indications. Whether or not a tweets associated with COVID-19 is a non-public report or an information point out to the virus. Which gives test accuracy to an F1 score of 96%.</p>
</abstract>
<kwd-group>
<kwd>Deep Learning</kwd>
<kwd>Sentimental analysis</kwd>
<kwd>XLNET</kwd>
<kwd>BERT</kwd>
<kwd>lockdown</kwd>
<kwd>COVID-19</kwd>
<kwd>Word cloud</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="S1" sec-type="intro">
<label>1</label>
<title>Introduction</title>
<p id="P2">The COVID-19 pandemic has encouraged an astounding damage of anthropological life universal and grants an unexpected food system, worldwide health, and the workplace are all under threat [<xref ref-type="bibr" rid="R1">1</xref>]. As this disease is highly contagious and rapidly changing, one of the best sources for the live information is on the social media [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R4">4</xref>]. There can be numerous sources of symptoms information on social media such as news and scientific articles (facts), other people’s account (second or third person statements) and self-report (first person statements). In this paper we will discuss our method as a team contributing in SMM4H [<xref ref-type="bibr" rid="R5">5</xref>] shared task 6 related to the sorting of such information from social media platform like twitter. We will be looking at using pre-trained NLU models like BERT and XLNET for this task.</p>
<p id="P3">So far, the world as we know it has changed, and today we live in a new and ever-changing atmosphere. The way we live, communicate and talk to others has changed forever [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R5">5</xref>]. In these special circumstances, virus risk plays a role in educating, displaying, and refusing to distribute data to the public. The coronavirus poses a real pandemic risk and poses a major challenge to the government to control, prepare, respond and improve. Health groups, stakeholders and the media [<xref ref-type="bibr" rid="R5">5</xref>].</p>
<p id="P4">The current catastrophe is due to COVID-19, the health community is being exposed to socially unconventional circumstances. Almost everything in our daily lives is becoming globalized, and people are relocating and travelling more often, Globalization means the interconnectivity between economies is growing, and tool development and knowledge are becoming more sophisticated.</p>
<p id="P5">From the sort of predicted class label, it is clear that in addition to semantics, contextual illustration also plays an imperative role. This task usually uses repetitive patterns, which are calculated along through the character positions of the input and output sequences. In the intention time step, they produce a classification of concealed circumstances that based on the input of the previous hidden state ht-1 and position t. This inherent chronological nature prevents the parallelization of training samples, which becomes decisive in longer classifications. Because memory limitations limit the group processing in the example. Initially, there are two strategies for contextual representation to apply previously trained language representations to future tasks: function-based and fine-tuning. However, both are limited by the fact that they are one-way language models and cannot learn universal language representations. In Recent developments in generalized autoregressive model (XLNET) views resolve these two problems since it captures bi-directional framework by means of a mechanism called “permutation language modeling” Determine the depth of unlabeled text by adjusting the left and right context together at all levels [<xref ref-type="bibr" rid="R6">6</xref>]. And encode to attain the vector illustration of the verdict, and fix the BERT for the ternary classification problem.</p>
<p id="P6">Similarly, due to the outbreak of this virus, false reports and criticisms of movements that disrupt communication between health authorities and cause social tension continue to appear [<xref ref-type="bibr" rid="R7">7</xref>]. Through the sentiment analysis of the new virus, five main problems have been identified, Positive to negative [<xref ref-type="bibr" rid="R8">8</xref>]. An extensive analysis of tweets from Indian users of social media was conducted in this article. To utilize deep learning for Text Data Analysis, we defined a generalized autoregressive model enhanced by transformer-XL model pre-trained (XLNET). The performance of numerous various Machine Learning algorithms is compared with BERT by using logistic regression, vector machine support, and single-layer long-term memory algorithms. Therefore, it is hoped that the following questions will be answered in this article.</p>
<p id="P7">These models are trained and retrained by XLNet to perform and come with best accuracy to achieved both from positive and negative to attain the accuracy level. <list list-type="bullet" id="L1">
<list-item>
<label>Q1:</label>
<p id="P8">What remain the most prevalent keywords in global tweets?</p>
</list-item>
<list-item>
<label>Q2:</label>
<p id="P9">What are the ramifications of these tweets on public health?</p>
</list-item>
<list-item>
<label>Q3:</label>
<p id="P10">Using machine learning algorithms, can emotions, feelings, and thoughts be analyzed?</p>
</list-item>
<list-item>
<label>Q4:</label>
<p id="P11">Are deep learning BERT models as effective as the other three traditional machine learning models?</p>
</list-item>
</list>
</p>
<sec id="S2">
<title>Fake News in Social Media Correlated with COVID-19 Studies</title>
<p id="P12">The rise of false information through Internet sites is becoming a global problem. Although fake news is not essential, it is currently worrying because online media is known for cooperating and spreading new ideas. [<xref ref-type="bibr" rid="R9">9</xref>] The COVID19 pandemic shows the valuable impact of the new data. Dissemination of false information will obviously weaken personal awareness and undermine government countermeasures are feasible. When misinformation on social media spreads, panic and fear can be caused among COVID19 patients, alerting authorities and prompting citizens to confirm the spread. [<xref ref-type="bibr" rid="R10">10</xref>] With the COVID19 pandemic [<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R14">14</xref>]. The spread of information about COVID19 through immense data analysis on large social media platforms can be personalized on a large scale to investigate rumors about the epidemic [<xref ref-type="bibr" rid="R15">15</xref>].</p>
<p id="P13">It is a well-known social media platform and Weibo system anywhere people can post and exchange communications called “tweets”. Twitter receives 500 million tweets a day and 200 billion tweets a year due to how it is structured turn out to be the main data access point for online media discussions with the public and global environments [<xref ref-type="bibr" rid="R16">16</xref>]. Unfortunately, due to the blowout of fake news, it is also a major source of global panic. It is described in 2020 that most tweets about COVID19 are in a positive mood, but users often contribute in the spread of negative tweets, and when manipulative the frequency of words in the tweets, they find that there are not many useful words. [<xref ref-type="bibr" rid="R17">17</xref>]</p>
<p id="P14">As with any virus, the current pandemic has spawned rapidly spreading rumors and conspiracy theories. My research has been centered on automatic detection and categorization of diseases and viruses using machine learning methods [<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref>] and identify the narrative framework that supports this false propaganda. The consequences of misleading information about COVID19 and its dogmatic philosophy eventually festering community well-being. [<xref ref-type="bibr" rid="R18">18</xref>]</p>
<p id="P15">In 2020 WHO reported that there are a lot of rumors and false stories about COVID19 circulating on social media. It is problematic to distinguish this fake news from real news, but its accuracy or reliability is no longer possible. [<xref ref-type="bibr" rid="R19">19</xref>] Counterfeiting on societal mass media sites can benefit individuals, health professionals, and administrations avoid unnecessary psychological stress. These programs provide general explanations for any non-exclusive events and can be easily customized based on conspiracy theories. In the works verified by tweets, I tested the deep learning type of machine learning model and verified the effectiveness of the model by comparing it with three other traditional models.</p>
</sec>
</sec>
<sec id="S3">
<label>2</label>
<title>Task and Data Description</title>
<sec id="S4">
<label>A</label>
<title>Task</title>
<p id="P16">Tasks of the Social Media Mining for Health Applications (SMM4H) 2021 overall tasks. [<xref ref-type="bibr" rid="R20">20</xref>] requires participants to develop an automated classification system to identify mentions of <list list-type="simple" id="L2">
<list-item>
<label>(1)</label>
<p id="P17">the self-report,</p>
</list-item>
<list-item>
<label>(2)</label>
<p id="P18">Non-personal reports, as well</p>
</list-item>
<list-item>
<label>(3)</label>
<p id="P19">Bibliography. COVID-19 SARS news, articles, and related topics are mentioned.</p>
</list-item>
</list>
</p>
<p id="P20">It is expressed by way of a multi-class sorting problem, in which the system must predict the label of each tweet in order to set up tweets.</p>
<p id="P21">In this study, I included the tweets data of twitter users during the COVID19 outage in the nation. The dataset of 4,010 tweets is taken and contains clean tweets on issues such by way of COVID19, Coronavirus, blocking, etc. For the analysis, I considered the tweets from the Kaggle platform Twitter to record the coronavirus and conclusions.</p>
<p id="P22">I used natural learning processing technology (NLP), which is a form of machine learning that helps process tweets. Generally, NLP includes various text mining methods, such as clatter reduction, halt words, and buzzword elimination. In direction to recover the recital of the BERT model, measurements, intangible factors, or text noise such as accents, math, and line breaks have been removed. [<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref>] Deleting these items will reduce the testing area for potential skills, thereby improving performance.</p>
<p id="P23">I divided the record of each task into each part of approximately 1500 and 1780 annotated examples, set up models for the 4 convolutions, and checked the remaining convolutions. For each fold, I optimize the model in more and more training examples, in addition, for these tasks, [<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref>] I tried to use the previously trained model for another task that I suspect may be useful, because these tasks seem to be related.</p>
<p id="P24">As mentioned earlier, in order to usage pre-trained text weights in the BERT model, we must adjust the input attributes of the encoding. I applied 600 tokens to the maximum length on this particular [<xref ref-type="bibr" rid="R25">25</xref>] model. In addition, I analyzed the token length of each tweet. A large number of tweets contain less than 600 tokens.</p>
<p id="P25">Based on the given multi-class classification problem, I used a dependent variable whose values were positive, extremely positive, neutral, negative, and extremely negative. In addition, I divided this question into two binary classifications: positive and negative. Towards improving accuracy, the classification is further separated into three categories: positive, negative, and neutral. The results of these algorithms will be compared in the evaluation phase.</p>
<p id="P26">To clean up, I deleted some of the sentences from “@” to the first space, links, numbers, “#” and extra spaces. Later in the preprocessing, I converted the data into a machine-readable format. I transformed the label to three discrete integers. And I use the tokenizer described in the model section to tweet the text in the token. Then I cropped the tweet text to reduce indentation. For BERT, I trimmed before applying the tokenizer and provide a proposal of length 66, while for XLNet, I trimmed after using the tokenizer to reach a sequence length of 160.</p>
</sec>
<sec id="S5">
<label>B</label>
<title>Data Description</title>
<p id="P27">The training data set contains 9,000 labeled tweets, the validation data set contains 5.76 labeled tweets, and the test data set contains 6,500 unlabeled tweets.</p>
<p id="P28">The classification model cannot be adapted to the data set for confirmation. I divided the data set into two parts: working out and challenging. Well-trained tweets help classify data patterns, reduce errors, and test data sets for analysis. 85% of tweets are used for educational purposes and 17% are used for testing determinations. To Provide demographic data by tweet multi-class classification [<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref>].</p>
</sec>
<sec id="S6">
<label>C</label>
<title>BERT Model</title>
<p id="P29">My first system for this task is BERT. BERT stands for Bidirectional Transformer Encoder Illustrations, which is trained by using the left and right contexts of the masked words to randomly predict the masking tags during pre-training. [<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref>] Token also has a second purpose-to predict whether two given sentences are continuous. In my experiments, I used uppercase and lowercase letters in this model. The basic version of BERT has 12 encoder levels, 768 hidden level measurements, and 12 attention levels. The head has 109 million parameters, while the big head has 24 encoder levels, 1024 hidden layer measurements and [<xref ref-type="bibr" rid="R30">30</xref>]16 attention heads, with 335 million parameters. I use their tokenizer.</p>
</sec>
<sec id="S7">
<label>D</label>
<title>XLNet Model</title>
<p id="P30">XLNet is an extension of the previously trained Transformer XL model that uses autoregression to learn bidirectional framework by exploiting the anticipated probability of entirely variations of the input sequence decomposition order.</p>
<p id="P31">It allows to use autoregressive and automatic pre-workout coding techniques to overcome inconsistencies in pre-workout fine-tuning.</p>
<p id="P32">This can be easily used for any task by loading a pre-trained model and setting it as a future task. In short, hugging face Transformers has provided several model classes to perform specific tasks for subsequent use of XLNet. I only need to download and configure them without writing our own model classes, which are additional layers on top of the XLNet model.</p>
<p id="P33">I used XLNet as the second system. XLNet is an autoregressive model, which is different from BERT in that it uses an autoregressive formula to learn two-way context. The output of the word tag is calculated captivating into interpretation the arrangement of all word tags in the sentence, which is contrary to the traditional method that is only used on the left or right side of the target tag. We experimented with the basic and large versions of this model. The basic version has 12 layers, 778 hidden layer sizes and 12 consideration heads, 110 million. We use their tokenizer.</p>
</sec>
</sec>
<sec id="S8">
<label>3</label>
<title>Model Training</title>
<p id="P34">We did some experiments on clean and unclean data. We checked the basic and major versions of BERT and XLNet, as well as numerous training methods, fine-tuning and retraining of the complete prototypical. The outcomes are made known in <xref ref-type="table" rid="T1">Table 1</xref>. Use the appropriate sorting level. For all these experiments, the loss function represents the loss of cross entropy, and the learning rate of the optimizer. I found that the larger version of BERT retrained with the original data performed better. The XLNet version of the tokenizer with the basic version of the model works best. Thus obtained the best results by retraining the model using the original data. The best system results of the test data for general problems are shown in <xref ref-type="table" rid="T2">Table 2</xref>.</p>
<p id="P35">A model of BERT identifies whether the second set of a pair of operators in the source file corresponds to the final result after collecting a pair of operators as input. During training, partial of the information sources are paired so that the reverse sentence is the result judgement in the first file, and the other half selects the incorrect sentence from the amount as the reverse sentence. Consider separating the incorrect sentence from the main sentence. BERT’s input source is a combination of inserted markers, segments, and positions. During training, the training data is processed as follows so that the model can distinguish between different topics.</p>
<p id="P36">Position insertion is added to each mark to indicate its status consistently. The BERT tokenizer performs word segmentation on the part of the word: the dictionary displays discrete language features, and again adds high-frequency word combinations. Use word cloud graphs to rank keywords in tweets. For training and verification purposes, there are two subsets of data. Use the accuracy calculation to see how accurate the model is. Retraining is also superior to refining according to my observations. BERT, the large version is better than the basic version, and on XLNet, the basic version is better than the large version. The test suite, and XLNet does a better job in the test suite.</p>
</sec>
<sec id="S9">
<label>4</label>
<title>System Overview and Implementations</title>
<sec id="S10">
<label>A</label>
<title>System Overview and Design</title>
<p id="P37">We studied the automatic coding and autoregressive models of classification problems, and selected BERT and XLNet. In our experiments, we used the previously trained hugging face model [<xref ref-type="bibr" rid="R30">30</xref>]. They use uppercase, case-sensitive versions. 103 This is necessary because capital letters are important for defining nouns and pronouns, which in turn are important for defining the first, second, and third persons in the sentence.</p>
<p id="P38">I also studied issues related to community sentimentality, shimmering deep concerns about the coronavirus and COVID19, foremost to increased anxiety and negative emotions. I also established the use of exploratory and eloquent text analysis and techniques. Visualize text data to discover early ideas. Group words by the level of specific non-text variables. Using those datasets to train to the best accuracy achieved using XLNet also outperforms the other model.</p>
<p id="P39">Many potential ethical issues have been identified regarding how professionals use Twitter data for research; these include the use of tweets by vulnerable groups in crisis situations [<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R32">32</xref>]. Research, as well as human beings, are no longer the responsibility of researchers, and researchers are no longer the responsibility of “data subjects” [<xref ref-type="bibr" rid="R33">33</xref>]. As a result, public data is valuable as a voluntary contribution, even though it does not violate ethical principles. From Twitter users, access public spaces. Moreover, the study shows how Twitter data was analyzed in connection with pandemics, including the 2009 swine flu [<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref>], demonstrating a mature mentality toward identifying and managing infections and crises by using social media.</p>
<p id="P40">The first layer and the second layer both combined to form a XLNet based framework by pre- processing to analyze the correct sequence model to the given based framework. Which utilizes the second layer to filter by using keras layer to sort out the dense and classifying the tweets by another encoder to predict what come next as positive or negative based on the situation it then again TensorFlow for sorting out the tweets again by repetition method this goes until by identifying the accuracy by training iteratively.</p>
<p id="P41">In addition to, BERT base, XLnet and Electra, we gained control of them all. FP16 calculations are used to reduce the size [<xref ref-type="bibr" rid="R36">36</xref>] of the model as controlled fine-tuning consumes a lot of GPU memory. To train at 0.00003, we selected the 0.00003 settings. Sequences can have a maximum length. 250 is the set value. We have set the number of sublots to two. The learning rate is set to 3e5 using Adam optimizer. Training is done over three epochs with gradient accumulation.</p>
</sec>
</sec>
<sec id="S11" sec-type="conclusions">
<label>7</label>
<title>Conclusion</title>
<p id="P42">So, as discussed, a simple way to adapt the XLNet model to 2021 social media mining common problems in healthcare applications. These results are not up-to-date, but they are competitive and demonstrate the benefits of using contextualized and pre-trained language models. On a large scale. The model has achieved F1 score of 93% accuracy (as shown in <xref ref-type="table" rid="T1">table 1</xref>). I also discussed the benefits of first training the model for the task at hand and determine when it might be useful.</p>
</sec>
</body>
<back>
<ref-list>
<ref id="R1">
<label>1</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chawla</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Mittal</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Chawla</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Goyal</surname>
<given-names>L</given-names>
</name>
</person-group>
<article-title>Corona Virus-SARS-CoV-2: An Insight to Another way of Natural Disaster</article-title>
<source>EAI Endorsed Trans Pervasive Health Technol</source>
<year>2020</year>
</element-citation>
</ref>
<ref id="R2">
<label>2</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yuan</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Wu</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Qin</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Qiao</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Huang</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Han</surname>
<given-names>N</given-names>
</name>
</person-group>
<article-title>An effective image classification method for shallow densely connected convolution networks through squeezing and splitting techniques</article-title>
<source>Appl Intell</source>
<year>2019</year>
<volume>49</volume>
<issue>10</issue>
<fpage>3570</fpage>
<lpage>3586</lpage>
</element-citation>
</ref>
<ref id="R3">
<label>3</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lin</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>Q</given-names>
</name>
<name>
<surname>Qi</surname>
<given-names>X</given-names>
</name>
</person-group>
<article-title>Deep reinforcement learning for imbalanced classification</article-title>
<source>Appl Intell</source>
<year>2020</year>
<fpage>1</fpage>
<lpage>15</lpage>
</element-citation>
</ref>
<ref id="R4">
<label>4</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Staszkiewicz</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Chomiak-Orsa</surname>
<given-names>I</given-names>
</name>
</person-group>
<article-title>Dynamics of the COVID-19 Contagion and Mortality: Country Factors, social media, and Market Response Evidence from a Global Panel Analysis</article-title>
<source>IEEE Access</source>
<year>2020</year>
<volume>8</volume>
<fpage>106009</fpage>
<lpage>106022</lpage>
</element-citation>
</ref>
<ref id="R5">
<label>5</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Guo</surname>
<given-names>Y-R</given-names>
</name>
<name>
<surname>Cao</surname>
<given-names>Q-D</given-names>
</name>
<name>
<surname>Hong</surname>
<given-names>Z-S</given-names>
</name>
<name>
<surname>Tan</surname>
<given-names>Y-Y</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>S-D</given-names>
</name>
<name>
<surname>Jin</surname>
<given-names>H-J</given-names>
</name>
<name>
<surname>Tan</surname>
<given-names>K-S</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>D-Y</given-names>
</name>
<name>
<surname>Yan</surname>
<given-names>Y</given-names>
</name>
</person-group>
<article-title>The origin, transmission and clinical therapies on coronavirus disease 2019 (COVID-19) outbreak—An update on the status</article-title>
<source>Mil Med Res</source>
<year>2020</year>
<volume>7</volume>
<fpage>1</fpage>
<lpage>10</lpage>
</element-citation>
</ref>
<ref id="R6">
<label>6</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mittal</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Battineni</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Goyal</surname>
<given-names>LM</given-names>
</name>
<name>
<surname>Chhetri</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Oberoi</surname>
<given-names>SV</given-names>
</name>
<name>
<surname>Chintalapudi</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Amenta</surname>
<given-names>F</given-names>
</name>
</person-group>
<article-title>Cloud-based framework to mitigate the impact of COVID-19 on seafarers’ mental health</article-title>
<source>Int Marit Health</source>
<year>2020</year>
<volume>71</volume>
<fpage>213</fpage>
<lpage>214</lpage>
</element-citation>
</ref>
<ref id="R7">
<label>7</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Eom</surname>
<given-names>KB</given-names>
</name>
</person-group>
<article-title>Active deep learning for classification of hyperspectral images</article-title>
<source>IEEE J Sel Top Appl Earth Obs Remote Sens</source>
<year>2016</year>
<volume>10</volume>
<issue>2</issue>
<fpage>712</fpage>
<lpage>724</lpage>
</element-citation>
</ref>
<ref id="R8">
<label>8</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Akande</surname>
<given-names>ON</given-names>
</name>
<name>
<surname>Badmus</surname>
<given-names>TA</given-names>
</name>
<name>
<surname>Akindele</surname>
<given-names>AT</given-names>
</name>
<name>
<surname>Arulogun</surname>
<given-names>OT</given-names>
</name>
</person-group>
<article-title>Dataset to support the adoption of social media and emerging technologies for students’ continuous engagement</article-title>
<source>Data Brief</source>
<year>2020</year>
<volume>31</volume>
<elocation-id>105926</elocation-id>
</element-citation>
</ref>
<ref id="R9">
<label>9</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gorriz</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Carlier</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Faure</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Giro-i-Nieto</surname>
<given-names>X</given-names>
</name>
</person-group>
<article-title>Cost-effective active learning for melanoma segmentation</article-title>
<source>arXiv:1711.091681711.09168</source>
<year>2017</year>
</element-citation>
</ref>
<ref id="R10">
<label>10</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Dang</surname>
<given-names>Huong</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>Kahyun</given-names>
</name>
<name>
<surname>Henry</surname>
<given-names>Sam</given-names>
</name>
<name>
<surname>Uzuner</surname>
<given-names>Özlem</given-names>
</name>
</person-group>
<source>Ensemble BERT for classifying medication-mentioning tweets</source>
<conf-name>Proceedings of the Fifth Social Media Mining for Health Applications Workshop &amp; Shared Task</conf-name>
<conf-sponsor>Association for Computational Linguistics</conf-sponsor>
<conf-loc>Barcelona, Spain</conf-loc>
<year>2020</year>
<fpage>37</fpage>
<lpage>41</lpage>
</element-citation>
</ref>
<ref id="R11">
<label>11</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Devlin</surname>
<given-names>Jacob</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>Ming-Wei</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>Kenton</given-names>
</name>
<name>
<surname>Toutanova</surname>
<given-names>Kristina</given-names>
</name>
</person-group>
<source>BERT: Pre-training of 140 deep bidirectional transformers for language understanding</source>
<conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</conf-name>
<conf-sponsor>Association for Computational Linguistics</conf-sponsor>
<conf-loc>Minneapolis, Minnesota</conf-loc>
<year>2019</year>
<volume>1</volume>
<fpage>4171</fpage>
<lpage>4186</lpage>
</element-citation>
</ref>
<ref id="R12">
<label>12</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Magge</surname>
<given-names>Arjun</given-names>
</name>
<name>
<surname>Klein</surname>
<given-names>Ari</given-names>
</name>
<name>
<surname>Flores</surname>
<given-names>Ivan</given-names>
</name>
<name>
<surname>Alimova</surname>
<given-names>Ilseyar</given-names>
</name>
<name>
<surname>Al-garadi</surname>
<given-names>Mohammed Ali</given-names>
</name>
<name>
<surname>Escalada</surname>
<given-names>Antonio Miranda</given-names>
</name>
<name>
<surname>Miftahutdinov</surname>
<given-names>Zulfat</given-names>
</name>
<name>
<surname>FarréMaduell</surname>
<given-names>Eulàlia</given-names>
</name>
<name>
<surname>López</surname>
<given-names>Salvador Lima</given-names>
</name>
<name>
<surname>Banda</surname>
<given-names>Juan M</given-names>
</name>
<name>
<surname>O’Connor</surname>
<given-names>Karen</given-names>
</name>
<etal/>
</person-group>
<source>Overview of the sixth social media mining for health applications (# smm4h) shared tasks at naacl 2021</source>
<conf-name>Proceedings of the Sixth Social Media Mining for Health Applications Workshop &amp; Shared Task</conf-name>
<year>2021</year>
</element-citation>
</ref>
<ref id="R13">
<label>13</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Tsai</surname>
<given-names>Henry</given-names>
</name>
<name>
<surname>Riesa</surname>
<given-names>Jason</given-names>
</name>
<name>
<surname>Johnson</surname>
<given-names>Melvin</given-names>
</name>
<name>
<surname>Arivazhagan</surname>
<given-names>Naveen</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>Xin</given-names>
</name>
<name>
<surname>Archer</surname>
<given-names>Amelia</given-names>
</name>
</person-group>
<article-title>Small and practical bert models for sequence labeling</article-title>
<source>arXiv preprint</source>
<year>2019</year>
<elocation-id>arXiv:1909.00100</elocation-id>
</element-citation>
</ref>
<ref id="R14">
<label>14</label>
<element-citation publication-type="other">
<person-group person-group-type="author">
<name>
<surname>Ruder</surname>
<given-names>Sebastian</given-names>
</name>
</person-group>
<source>Recent Advances in Language Model Fine-tuning</source>
<year>2021</year>
</element-citation>
</ref>
<ref id="R15">
<label>15</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sanh</surname>
<given-names>Victor</given-names>
</name>
<name>
<surname>Debut</surname>
<given-names>Lysandre</given-names>
</name>
<name>
<surname>Chaumond</surname>
<given-names>Julien</given-names>
</name>
<name>
<surname>Wolf</surname>
<given-names>Thomas</given-names>
</name>
</person-group>
<article-title>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</article-title>
<source>arXiv preprint</source>
<year>2019</year>
<elocation-id>arXiv:1910.01108</elocation-id>
</element-citation>
</ref>
<ref id="R16">
<label>16</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Kuo</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Häne</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Yuh</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Mukheηee</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Malik</surname>
<given-names>J</given-names>
</name>
</person-group>
<source>Cost-sensitive active learning for intracranial hemorrhage detection</source>
<conf-name>International conference on medical image computing and computer-assisted intervention</conf-name>
<publisher-name>Springer</publisher-name>
<publisher-loc>New York</publisher-loc>
<year>2018</year>
<fpage>715</fpage>
<lpage>723</lpage>
</element-citation>
</ref>
<ref id="R17">
<label>17</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lv</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Duan</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Jiang</surname>
<given-names>JJ</given-names>
</name>
<name>
<surname>Fu</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Gan</surname>
<given-names>L</given-names>
</name>
</person-group>
<article-title>Deep active learning for surface defect detection</article-title>
<source>Sensors</source>
<year>2020</year>
<volume>20</volume>
<issue>6</issue>
<elocation-id>1650</elocation-id>
</element-citation>
</ref>
<ref id="R18">
<label>18</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Yoo</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Kweon</surname>
<given-names>IS</given-names>
</name>
</person-group>
<source>Learning loss for active learning</source>
<conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name>
<year>2019</year>
<fpage>93</fpage>
<lpage>102</lpage>
</element-citation>
</ref>
<ref id="R19">
<label>19</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shi</surname>
<given-names>Zhan</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>Xinchi</given-names>
</name>
<name>
<surname>Qiu</surname>
<given-names>Xipeng</given-names>
</name>
<name>
<surname>Huang</surname>
<given-names>Xuanjing</given-names>
</name>
</person-group>
<article-title>Toward diverse text generation with inverse reinforcement learning</article-title>
<source>IJCAI</source>
<year>2018</year>
</element-citation>
</ref>
<ref id="R20">
<label>20</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhao</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Veeravalli</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Zeng</surname>
<given-names>Z</given-names>
</name>
</person-group>
<article-title>Deeply supervised active learning for finger bones segmentation</article-title>
<source>arXiv:2005 03225</source>
<year>2020</year>
</element-citation>
</ref>
<ref id="R21">
<label>21</label>
<element-citation publication-type="other">
<person-group person-group-type="author">
<name>
<surname>Novak</surname>
<given-names>Roman</given-names>
</name>
<name>
<surname>Auli</surname>
<given-names>Michael</given-names>
</name>
<name>
<surname>Grangier</surname>
<given-names>David</given-names>
</name>
</person-group>
<source>Iterative refinement for machine translation</source>
<year>2016</year>
</element-citation>
</ref>
<ref id="R22">
<label>22</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Puduppully</surname>
<given-names>Ratish</given-names>
</name>
<name>
<surname>Dong</surname>
<given-names>Li</given-names>
</name>
<name>
<surname>Lapata</surname>
<given-names>Mirella</given-names>
</name>
</person-group>
<article-title>Data-to-text generation with content selection and planning</article-title>
<source>AAAI</source>
<year>2019</year>
<volume>33</volume>
<fpage>6908</fpage>
<lpage>6915</lpage>
</element-citation>
</ref>
<ref id="R23">
<label>23</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Garcia</surname>
<given-names>LP</given-names>
</name>
<name>
<surname>Duarte</surname>
<given-names>E</given-names>
</name>
</person-group>
<article-title>Infodemic: Excess quantity to the detriment of quality of information about COVID-19</article-title>
<source>Epidemiol Serv Health</source>
<year>2020</year>
<volume>29</volume>
</element-citation>
</ref>
<ref id="R24">
<label>24</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hung</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Lauren</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Hon</surname>
<given-names>ES</given-names>
</name>
<name>
<surname>Birmingham</surname>
<given-names>WC</given-names>
</name>
<name>
<surname>Xu</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Su</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Hon</surname>
<given-names>SD</given-names>
</name>
<name>
<surname>Park</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Dang</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Lipsky</surname>
<given-names>MS</given-names>
</name>
</person-group>
<article-title>Social Network Analysis of COVID-19 Sentiments: Application of Artificial Intelligence</article-title>
<source>J Med Internet Res</source>
<year>2020</year>
<volume>22</volume>
<elocation-id>e22590</elocation-id>
</element-citation>
</ref>
<ref id="R25">
<label>25</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Di Domenico</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Sit</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ishizaka</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Nunan</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>Fake news, social media and marketing: A systematic review</article-title>
<source>J Bus Res</source>
<year>2021</year>
<volume>124</volume>
<fpage>329</fpage>
<lpage>341</lpage>
</element-citation>
</ref>
<ref id="R26">
<label>26</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Apuke</surname>
<given-names>OD</given-names>
</name>
<name>
<surname>Omar</surname>
<given-names>B</given-names>
</name>
</person-group>
<article-title>Fake news and COVID-19: Modelling the predictors of fake news sharing among social media users</article-title>
<source>Telemat Inform</source>
<year>2021</year>
<volume>56</volume>
<elocation-id>101475</elocation-id>
</element-citation>
</ref>
<ref id="R27">
<label>27</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zaman</surname>
<given-names>A</given-names>
</name>
</person-group>
<article-title>COVID-19-Related Social Media Fake News in India</article-title>
<source>J Media</source>
<year>2021</year>
<volume>2</volume>
<fpage>7</fpage>
</element-citation>
</ref>
<ref id="R28">
<label>28</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sohn</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Yan</surname>
<given-names>X</given-names>
</name>
</person-group>
<article-title>Learning structured output representation using deep conditional generative models</article-title>
<source>Advances in neural information processing systems</source>
<year>2015</year>
<fpage>3483</fpage>
<lpage>3491</lpage>
</element-citation>
</ref>
<ref id="R29">
<label>29</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Depoux</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Martin</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Karafillakis</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Preet</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Wilder-Smith</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Larson</surname>
<given-names>H</given-names>
</name>
</person-group>
<article-title>The pandemic of social media panic travels faster than the COVID-19 outbreak</article-title>
<source>J Travel Med</source>
<year>2020</year>
<volume>27</volume>
<elocation-id>taaa031</elocation-id>
</element-citation>
</ref>
<ref id="R30">
<label>30</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gao</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Zheng</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Jia</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Mao</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Fu</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Dai</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Mental health problems and social media exposure during COVID-19 outbreak</article-title>
<source>PLoS ONE</source>
<year>2020</year>
<volume>15</volume>
<elocation-id>e0231924</elocation-id>
</element-citation>
</ref>
<ref id="R31">
<label>31</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ahmad</surname>
<given-names>AR</given-names>
</name>
<name>
<surname>Murad</surname>
<given-names>HR</given-names>
</name>
</person-group>
<article-title>The Impact of Social Media on Panic During the COVID-19 Pandemic in Iraqi Kurdistan: Online Questionnaire Study</article-title>
<source>J Med Internet Res</source>
<year>2020</year>
<volume>22</volume>
<elocation-id>e19556</elocation-id>
</element-citation>
</ref>
<ref id="R32">
<label>32</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cinelli</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Quattrociocchi</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Galeazzi</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Valensise</surname>
<given-names>CM</given-names>
</name>
<name>
<surname>Brugnoli</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Schmidt</surname>
<given-names>AL</given-names>
</name>
<name>
<surname>Zola</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Zollo</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Scala</surname>
<given-names>A</given-names>
</name>
</person-group>
<article-title>The COVID-19 social media infodemic</article-title>
<source>Sci Rep</source>
<year>2020</year>
<volume>10</volume>
<fpage>1</fpage>
<lpage>10</lpage>
</element-citation>
</ref>
<ref id="R33">
<label>33</label>
<element-citation publication-type="web">
<collab>Twitter</collab>
<source>Twitter Usage Statistics—Internet Live Stats</source>
<date-in-citation>(accessed on 19 October 2020)</date-in-citation>
<comment>online:<ext-link ext-link-type="uri" xlink:href="https://www.internetlivestats.com/twitter-statistics/">https://www.internetlivestats.com/twitter-statistics/</ext-link>
</comment>
</element-citation>
</ref>
<ref id="R34">
<label>34</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chakraborty</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Bhatia</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Bhattacharyya</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Platos</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Bag</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Hassanien</surname>
<given-names>AE</given-names>
</name>
</person-group>
<article-title>Sentiment Analysis of COVID-19 tweets by Deep Learning Classifiers—A study to show how popularity is affecting accuracy in social media</article-title>
<source>Appl Soft Comput</source>
<year>2020</year>
<volume>97</volume>
<elocation-id>106754</elocation-id>
</element-citation>
</ref>
<ref id="R35">
<label>35</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shahsavari</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Holur</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Tangherlini</surname>
<given-names>TR</given-names>
</name>
<name>
<surname>Roychowdhury</surname>
<given-names>V</given-names>
</name>
</person-group>
<article-title>Conspiracy in the time of corona: Automatic detection of COVID-19 conspiracy theories in social media and the news</article-title>
<source>J Comput Soc Sci</source>
<year>2020</year>
<volume>3</volume>
<fpage>279</fpage>
<lpage>317</lpage>
</element-citation>
</ref>
<ref id="R36">
<label>36</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Havey</surname>
<given-names>NF</given-names>
</name>
</person-group>
<article-title>Partisan public health: How does political ideology influence support for COVID-19 related misinformation?</article-title>
<source>J Comput Soc Sci</source>
<year>2020</year>
<volume>3</volume>
<fpage>319</fpage>
<lpage>342</lpage>
</element-citation>
</ref>
<ref id="R37">
<label>37</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Obi-Ani</surname>
<given-names>NA</given-names>
</name>
<name>
<surname>Anikwenze</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Isiani</surname>
<given-names>MC</given-names>
</name>
</person-group>
<article-title>Social media and the COVID-19 pandemic: Observations from Nigeria</article-title>
<source>Cogent Arts Humanit</source>
<year>2020</year>
<volume>7</volume>
<elocation-id>1799483</elocation-id>
</element-citation>
</ref>
<ref id="R38">
<label>38</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barkur</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Vibha</surname>
</name>
<name>
<surname>Kamath</surname>
<given-names>GB</given-names>
</name>
</person-group>
<article-title>Sentiment analysis of nationwide lockdown due to COVID 19 outbreak: Evidence from India</article-title>
<source>Asian J Psychiatry</source>
<year>2020</year>
<volume>51</volume>
<elocation-id>102089</elocation-id>
</element-citation>
</ref>
<ref id="R39">
<label>39</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huynh</surname>
<given-names>TL</given-names>
</name>
</person-group>
<article-title>The COVID-19 risk perception: A survey on socioeconomics and media attention</article-title>
<source>Econ Bull</source>
<year>2020</year>
<volume>40</volume>
<fpage>758</fpage>
<lpage>764</lpage>
</element-citation>
</ref>
<ref id="R40">
<label>40</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chintalapudi</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Battineni</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Di Canio</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Sagaro</surname>
<given-names>GG</given-names>
</name>
<name>
<surname>Amenta</surname>
<given-names>F</given-names>
</name>
</person-group>
<article-title>Text mining with sentiment analysis on seafarers’ medical documents</article-title>
<source>Int J Inf Manag Data Insights</source>
<year>2021</year>
<volume>1</volume>
<elocation-id>100005</elocation-id>
</element-citation>
</ref>
<ref id="R41">
<label>41</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Devlin</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>MW</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Toutanova</surname>
<given-names>K</given-names>
</name>
</person-group>
<article-title>BERT: Pre-training of deep bidirectional transformers for language understanding</article-title>
<source>arXiv</source>
<year>2018</year>
<elocation-id>arXiv:1810.04805</elocation-id>
</element-citation>
</ref>
<ref id="R42">
<label>42</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chang</surname>
<given-names>YW</given-names>
</name>
<name>
<surname>Hsieh</surname>
<given-names>CJ</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>KW</given-names>
</name>
<name>
<surname>Ringgaard</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Lin</surname>
<given-names>CJ</given-names>
</name>
</person-group>
<article-title>Training and testing low-degree polynomial data mappings via linear SVM</article-title>
<source>J Mach Learn Res</source>
<year>2010</year>
<volume>11</volume>
<fpage>1471</fpage>
<lpage>1490</lpage>
</element-citation>
</ref>
<ref id="R43">
<label>43</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Melamud</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Goldberger</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Dagan</surname>
<given-names>I</given-names>
</name>
</person-group>
<source>Context2vec: Learning generic context embedding with bidirectional LSTM</source>
<conf-name>Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</conf-name>
<conf-loc>Berlin, Germany</conf-loc>
<conf-date>11–12 August 2016</conf-date>
<year>2016</year>
</element-citation>
</ref>
<ref id="R44">
<label>44</label>
<element-citation publication-type="web">
<source>India—COVID-19 Overview—Johns Hopkins</source>
<date-in-citation>(accessed on 23 March 2021)</date-in-citation>
<comment>online: <ext-link ext-link-type="uri" xlink:href="https://coronavirus.jhu.edu/region/india">https://coronavirus.jhu.edu/region/india</ext-link>
</comment>
</element-citation>
</ref>
<ref id="R45">
<label>45</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Samuel</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ali</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Rahman</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Esawi</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Samuel</surname>
<given-names>Y</given-names>
</name>
</person-group>
<article-title>COVID-19 Public Sentiment Insights and Machine Learning for Tweets Classification</article-title>
<source>Information</source>
<year>2020</year>
<volume>11</volume>
<fpage>314</fpage>
</element-citation>
</ref>
<ref id="R46">
<label>46</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Abd-Alrazaq</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Alhuwail</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Househ</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Hamdi</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Shah</surname>
<given-names>Z</given-names>
</name>
</person-group>
<article-title>Top Concerns of Tweeters During the COVID-19 Pandemic: Infoveillance Study</article-title>
<source>J Med Internet Res</source>
<year>2020</year>
<volume>22</volume>
<elocation-id>e19016</elocation-id>
</element-citation>
</ref>
<ref id="R47">
<label>47</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Xue</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Zhao</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Zhu</surname>
<given-names>T</given-names>
</name>
</person-group>
<article-title>The Impact of COVID-19 Epidemic Declaration on Psychological Consequences: A Study on Active Weibo Users</article-title>
<source>Int J Environ Res Public Health</source>
<year>2020</year>
<volume>17</volume>
<elocation-id>2032</elocation-id>
</element-citation>
</ref>
</ref-list>
</back>
<floats-group>
<fig id="F1" position="float">
<label>Fig 2.1</label>
<caption>
<title>The result of the total length of tweets</title>
</caption>
<graphic xlink:href="EMS141168-f001"/>
</fig>
<fig id="F2" position="float">
<label>Fig 3.1</label>
<caption>
<title>Model</title>
</caption>
<graphic xlink:href="EMS141168-f002"/>
</fig>
<fig id="F3" position="float">
<label>Figure 4.1</label>
<caption>
<title>This depicts COVID-19 is mainly used as a hashtag in tweets, people also want to talk about other events in tweets.</title>
</caption>
<graphic xlink:href="EMS141168-f003"/>
</fig>
<table-wrap id="T1" position="float" orientation="portrait">
<label>Table 1</label>
<caption>
<title>This gives the statistics of the dataset.</title>
</caption>
<table frame="box" rules="all">
<thead>
<tr>
<th align="center" valign="top">Dataset</th>
<th align="center" valign="top">LN</th>
<th align="center" valign="top">NP</th>
<th align="center" valign="top">Self</th>
<th align="center" valign="top">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">Train</td>
<td align="center" valign="top">4267</td>
<td align="center" valign="top">3404</td>
<td align="center" valign="top">1248</td>
<td align="center" valign="top">8919</td>
</tr>
<tr>
<td align="center" valign="top">Validation</td>
<td align="center" valign="top">250</td>
<td align="center" valign="top">177</td>
<td align="center" valign="top">88</td>
<td align="center" valign="top">515</td>
</tr>
<tr>
<td align="center" valign="top">Test</td>
<td align="center" valign="top">-</td>
<td align="center" valign="top">-</td>
<td align="center" valign="top">-</td>
<td align="center" valign="top">6500</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="T2" position="float" orientation="portrait">
<label>Table 2</label>
<caption>
<title>Demographic representation of tweets multi-class classification.</title>
</caption>
<table frame="box" rules="all">
<thead>
<tr>
<th align="center" valign="bottom" colspan="2">Multi-Class Classification</th>
</tr>
<tr>
<th align="center" valign="top">Model</th>
<th align="center" valign="top">Test accuracy (F1 score)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left" valign="top">Accuracy</td>
<td align="right" valign="top">62.0%</td>
</tr>
<tr>
<td align="left" valign="top">Neutral positive</td>
<td align="right" valign="top">61.8%</td>
</tr>
<tr>
<td align="left" valign="top">Neutral negative</td>
<td align="right" valign="top">60.7%</td>
</tr>
<tr>
<td align="left" valign="top">Weighted avg</td>
<td align="right" valign="top">57.3%</td>
</tr>
<tr>
<td align="left" valign="top">Macro avg</td>
<td align="right" valign="top">53.0%</td>
</tr>
<tr>
<td align="left" valign="top">Extremely positive</td>
<td align="right" valign="top">49.7%</td>
</tr>
<tr>
<td align="left" valign="top">Extremely negative</td>
<td align="right" valign="top">47.9%</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="T3" position="float" orientation="portrait">
<label>Table 1</label>
<caption>
<title>Results</title>
</caption>
<table frame="box" rules="all">
<thead>
<tr>
<th align="center" valign="top">Dataset</th>
<th align="left" valign="top">BERT</th>
<th align="left" valign="top">XLNet</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center" valign="top">Result</td>
<td align="left" valign="top">.941</td>
<td align="left" valign="top">.968</td>
</tr>
<tr>
<td align="center" valign="top">Test</td>
<td align="left" valign="top">.666</td>
<td align="left" valign="top">.784</td>
</tr>
</tbody>
</table>
</table-wrap>
</floats-group>
</article>
