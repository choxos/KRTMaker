<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS154990</article-id><article-id pub-id-type="doi">10.1101/2021.11.08.467745</article-id><article-id pub-id-type="archive">PPR418047</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Distinct replay signatures for prospective decision-making and memory preservation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Elliott Wimmer</surname><given-names>G.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Yunzhe</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>McNamee</surname><given-names>Daniel C.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Dolan</surname><given-names>Raymond J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Max Planck University College London Centre for Computational Psychiatry and Ageing Research, University College London, London, UK</aff><aff id="A2"><label>2</label>Wellcome Centre for Human Neuroimaging, University College London, London, UK</aff><aff id="A3"><label>3</label>State Key Laboratory of Cognitive Neuroscience and Learning, IDG/McGovern Institute for Brain Research, Beijing Normal University, Beijing, China</aff><aff id="A4"><label>4</label>Chinese Institute for Brain Research, Beijing, China</aff><aff id="A5"><label>5</label>Neuroscience Programme, Champalimaud Research, Lisbon, Portugal</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author. <email>e.wimmer@ucl.ac.uk</email> (G.E.W.)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>29</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>09</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Theories of neural replay propose that it supports a range of functions, most prominently planning and memory consolidation. Here, we test the hypothesis that distinct signatures of replay in the same task are related to model-based decision-making (‘planning’) and memory preservation. We designed a reward learning task wherein participants utilized structure knowledge for model-based evaluation, while at the same time had to maintain knowledge of two independent and randomly alternating task environments. Using magnetoencephalography (MEG) and multivariate analysis, we first identified temporally compressed sequential reactivation, or replay, both prior to choice and following reward feedback. Before choice, prospective replay strength was enhanced for the current task-relevant environment when a model-based planning strategy was beneficial. Following reward receipt, and consistent with a memory preservation role, replay for the alternative distal task environment was enhanced as a function of decreasing recency of experience with that environment. Critically, these planning and memory preservation relationships were selective to pre-choice and post-feedback periods. Our results provide new support for key theoretical proposals regarding the functional role of replay and demonstrate that the relative strength of planning and memory-related signals are modulated by on-going computational and task demands.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Humans have a remarkable ability to process information that extends beyond the immediately perceptible, including simulation of prospective plans and retrieval of past memories. It has been hypothesized that hippocampal replay contributes to both these abilities (<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R5">5</xref>). In rodents, replay is strongly linked to the hippocampus, where cells encoding distinct locations reactivate in a coordinated sequential manner, recapitulating past or simulating potential future trajectories (<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R6">6</xref>). A similar phenomenon of sequential reactivation has been identified in humans using decoding techniques in conjunction with high temporal resolution MEG data (<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R14">14</xref>).</p><p id="P3">Prominent theories of neural replay propose that it is important for planning future actions (<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R18">18</xref>) in addition to supporting memory preservation (<xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R25">25</xref>). One hypothesis is that task demands, operationalized as temporal proximity to action versus feedback, determine the contribution of replay to planning and memory, respectively (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R26">26</xref>). However, the contribution of awake on-line replay to these two functions has largely been addressed in the context of separate experiments (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R33">33</xref>). Here we directly address the contribution of replay to both these roles within a single task context.</p><p id="P4">Replay of trajectories leading to a goal has been proposed to underpin decision making that exploits structure knowledge of an environment, referred to as model-based decision-making (<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R34">34</xref>). A number of rodent studies indicate a link between hippocampal neural sequences and subsequent path choice selection, consistent with a role for replay in planning (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R35">35</xref>–<xref ref-type="bibr" rid="R37">37</xref>). However, an inconsistency in such findings raises the possibility that any relationship between replay and subsequent choice might differ across evaluation strategies and reward environments (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R39">39</xref>). Critically, and regardless of any relationship to choice identity, brain lesion studies highlight a necessary role for the hippocampus in model-based behavior (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>). This suggests that hippocampal replay may be enhanced when model-based decisionmaking (planning) is beneficial. Thus far, however, there is no clear evidence linking demands for model-based control and neural replay preceding choice (<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R13">13</xref>).</p><p id="P5">Beyond planning, replay is considered critical for memory preservation, where replay of previous experiences might serve to strengthen memory and prevent interference from newer experiences (‘catastrophic forgetting’) (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R43">43</xref>). Studies that have disrupted hippocampal activity support the idea that offline place cell reactivation is critical for learning, memory consolidation, or both (<xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R47">47</xref>). It has been conjectured that human rest-period replay subserves a similar function (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R49">49</xref>). Studies of replay in rodents navigating a single environmental context provide initial, but inconclusive, evidence for a link between recent experience with an environment and replay (<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R39">39</xref>).</p><p id="P6">Here, we address a role for replay in both planning and memory preservation in a context where participants needed to retain a memory of a ‘distal’ environment while at the same time learning within a local one. To do this we adapted a reward learning task originally designed to study model-based decision-making (<xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R52">52</xref>), where distinct start states converge upon shared paths. Critically, to study memory, we included two independent randomly alternating environments. Both the early convergence on shared paths and the alternation of environments strongly favors online planning, and these features distinguish the current paradigm from a recent related report (<xref ref-type="bibr" rid="R11">11</xref>). Using recently-developed MEG analytic methods (<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R53">53</xref>) we first identify sequential neural reactivation and then ask whether replay strength varies as a function of task demands and recent experience. We hypothesized replay would be boosted during prechoice path planning when model-based decision making was more beneficial (<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R50">50</xref>). By contrast, following receipt of choice feedback, we hypothesized replay for an alternative environment would relate to the infrequency of recent experience, consistent with a role in supporting memory preservation (<xref ref-type="bibr" rid="R1">1</xref>).</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Behavior</title><p id="P7">Participants navigated two separate, independent environments (‘worlds’) in order to earn reward points (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Each world contained two path options, where a top-level shape led deterministically to one of the sequential paths comprising 3 unique stimuli. Knowledge of these paths was tested after each scanning block. In a given world, each path led to a separate stream of reward feedback which drifted over time (<xref ref-type="supplementary-material" rid="SD1">Fig. S2a</xref>). Importantly, within a given world, each trial could start in one of two equivalent start states leading to the two paths. For an agent to perform well in the task, outcomes should have an equivalent influence on subsequent choices regardless of whether an agent starts in the same start state or the alternative, equivalent, start state. This design feature, combined with a sufficient drift in reward across trials (<xref ref-type="bibr" rid="R51">51</xref>), allows us to characterize how well participants use structure knowledge to guide model-based behavior (<xref ref-type="fig" rid="F1">Fig. 1c</xref>) (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>).</p><p id="P8">By way of example, imagine an agent is faced with a choice in world 1, start state A (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). The agent selects the diamond over the crown and receives an unexpected high reward. If next faced with a choice in start state B, a model-based agent will generalize this experience to promote the subsequent choice of the magenta pentagon to reach the same just-rewarded path. In contrast, a model-free agent does not have access to this recent relevant experience. Thus, only a model-based agent can exploit structure knowledge to allow generalization of reward feedback across equivalent start states. Such behaviour has been proposed to involve looking ahead to values associated with terminal states, possibly using prospective neural reactivation or replay (<xref ref-type="bibr" rid="R50">50</xref>). Thus, this design allows us to identify behavior reflective of model-based versus model-free learning (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>), analogous to variants of the paradigm that use probabilistic state transitions (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R52">52</xref>).</p><p id="P9">One feature of our deterministic task variant is that trials are divided between those where model-based behavior is beneficial (different start state) versus neutral (same start state) (<xref ref-type="bibr" rid="R50">50</xref>), allowing us to test for an association between neural replay and the benefits accruing from model-based behavior. Furthermore, a deterministic transition structure and an absence of branching paths increases our ability to detect evidence of sequential neural reactivation (<xref ref-type="bibr" rid="R11">11</xref>). With respect to planning, the fact that our design includes multiple worlds decreases the predictability of an upcoming trial, thus promoting deployment of planning-related processes at choice as opposed to after outcome feedback (<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R56">56</xref>). Critically, including multiple worlds also allows us to examine replay signatures of memory preservation for more distal (non-local) experiences.</p><p id="P10">After an initial scanned exploration period without reward feedback, participants performed the primary reward learning task. To ascertain the degree to which behavior was guided by model-based and model-free learning we used a regression approach in combination with a set of reinforcement learning models. Our regression analyses quantify a model-based influence on behavior by testing for an effect of generalization: whether a prior reward has a different effect on choice when starting in the same start state versus a different state than previous experience (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>). A model-based controller acts to generalize reward feedback across equivalent starting states, potentially using structure knowledge to look ahead and evaluate expected terminal rewards. Alternatively, updating the equivalent start state can occur after feedback (prior to choice), a form of non-local learning (<xref ref-type="bibr" rid="R11">11</xref>). In our analysis, if the effect of previous reward on choice is similar when starting in a different versus same state, then this indicates that participants’ behaviour reflects an exploitation of structure knowledge for a task environment (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R52">52</xref>). Qualitatively, we observed that receipt of high versus low reward enhanced the likelihood of choice repetition (<xref ref-type="fig" rid="F1">Fig. 1d</xref>) while a high degree of model-based behavior was evident in equivalent stay probabilities for choices starting at the same versus a different state (<xref ref-type="fig" rid="F1">Fig. 1d</xref>).</p><p id="P11">Using logistic regression we quantified the effect of reward and start state on choice (<xref ref-type="bibr" rid="R51">51</xref>), finding a robust overall effect of previous reward on stay choices (β (regression coefficient) = 0.536 [0.424 0.647]; z = 9.434, p &lt; 0.0001). Importantly, there was an equivalent influence of previous reward on stay choices in the same versus a different start state, consistent with the absence of a significant model-free contribution (interaction between reward and same start state, β = 0.0313 [-0.0365 0.0991]; z = 0.905, p = 0.365). As a non-significant effect does not provide evidence in support of the null hypothesis, we employed a two-one-sided test (TOST) equivalence procedure to enable us to reject the presence of a medium- or larger-sized effect (<xref ref-type="bibr" rid="R57">57</xref>). Indeed, based on this we can reject a medium- or larger-sized model-free interaction effect (TOST equivalence test p = 0.027). While reward-guided choice behaviour was unaffected by start state changes, we found that reaction times were overall slower for choices on start state change trials (different versus same start state trials β = 0.009 [0.0017 0.0163]; z = 2.423, p = 0.0155), even though the delayed choice limits reaction time variance. In a complementary regression approach, testing the effect of previous reward on the identity of option selection (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>) (<xref ref-type="bibr" rid="R50">50</xref>), we again found no interaction between reward and start state, consistent with a model-based learning signature (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R52">52</xref>). Overall, this allows us to infer that behavior is guided by reward to the same degree irrespective of generalization demands, likely reflecting participants’ behavior being strongly model-based.</p><p id="P12">Next, we compared fully model-based and model-free reinforcement learning models to a hybrid model commonly used to assess the relative strength of model-based and model-free learning (<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>). The hybrid model includes a weighting parameter <italic>w</italic> which controls the degree of model-based influence on choice. Overall, behavior was best explained by a model-based controller, which outperformed the hybrid model, while the fit of the model-free controller was poor (<xref ref-type="supplementary-material" rid="SD1">Table S1-S2</xref>). While the hybrid model exhibited a numerical benefit in raw fit, when penalized for model complexity the pure model-based controller provided an equivalent (measured via AIC) or better fit (measured via BIC; <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>). At the individual participant level, the model-based controller provided a better fit for more than 83% of participants (using either AIC or BIC). Finally, given that good performance requires model-based generalization throughout the task, as expected we found no evidence for a change in model-based behavior over time (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>). The high degree of model-based behavior provided a robust context for us to next examine the associated neural processes.</p><p id="P13">We also obtained memory tests for sequential path stimuli to confirm that participants learned the path structure. Our task was designed to achieve high memory performance by the start of reward learning, as this would serve to both boost model-based behaviour as well as our ability to detect sequential reactivation. Memory performance, already at a high level at the end of the incentivized structure learning phase (second half, 87.5% [78.2 96.8]), was maintained at this level across the reward learning phase (mean 95.0% [91.3 98.7, with all blocks above 92%; effect of block, p &gt; 0.64).</p></sec><sec id="S4"><title>Sequenceness identification</title><p id="P14">In our neural analyses, we first established reliable decoding from neural patterns evoked by the unique stimuli that indexed individual path states (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). A classifier trained during a pre-task localizer showed successful discrimination of all path stimuli, with peak decoding evident from 140-210 ms post stimulus onset. Based on this, and to maintain consistency with our prior studies, we selected a post-stimulus 200 ms time point for subsequent replay analyses (<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R11">11</xref>). The trained classifier generalized from the localizer to the actual presentation of path objects during reward learning, showing significant across-task classification (t<sub>(23)</sub> = 7.361, p &lt; 10<sup>-7</sup>; <xref ref-type="fig" rid="F2">Fig. 2</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S4</xref>). We also found evidence for significant reactivation of stimuli both during planning and after feedback (as compared to reactivation derived from permuted classifiers; p-values &lt; 0.01; see <xref ref-type="sec" rid="S10">Methods</xref>).</p><p id="P15">We next used the trained classifiers to seek evidence for time-compressed sequential reactivation of path elements. First, we applied the classifiers to reward learning task MEG data to derive measures of state reactivation in each trial separately for each state and at each time point, focusing on pre-choice planning and post-feedback rest periods (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). Next, we tested for time lagged cross-correlations between state reactivations within these periods, yielding a measure of ‘sequenceness’ in both forward and backward temporal directions at each lag (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R53">53</xref>) (<xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>). We use the term sequenceness (or ‘sequence strength’) to refer to the prediction strength of state <italic>j</italic> to state <italic>i</italic> at some time lag, while we operationally refer to any reactivation of state sequences here as replay. This sequence detection method, validated in previous work, quantifies the average predictivity of state <italic>j</italic> to state <italic>i</italic> within a period, reflecting both the frequency and fidelity of replay events (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R53">53</xref>) (<xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>).</p><p id="P16">Our a priori hypothesis is that signatures of replay are differentially modulated by task environment and recency of experience with an environment; the presence of forward or backward replay during planning and feedback (e.g. 2, 9) is a necessary precondition for testing this hypothesis. An initial temporal lag localization step independently identified lags of interest for subsequent examination of links between replay and behaviour. To increase power for localization, sequences included all possible paths, the (to-be) chosen and (to-be) nonchosen paths in the current trial, as well the two paths for the ‘other world’. We identified sequenceness time lags of interest by comparing evidence across lags for all valid sequences, using a significance threshold determined by a permutation of stimulus assignment to paths (following previous work; 8, 9, 11, 53). These analyses revealed that during planning there was significant forward sequenceness, with a state-to-state time lag of 70 and 80 ms and also of 190-200ms. For the shorter lags, this indicates that, on average across participants, a given state was followed by reactivation of an adjoining state within the same path at a delay of 70 to 80 ms (<xref ref-type="fig" rid="F2">Fig. 2c</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref>). We found no significant evidence for backward sequenceness during planning.</p><p id="P17">We then examined replay following outcome feedback, a period when the displayed reward points faded from the screen toward a brief inter-trial interval rest (<xref ref-type="fig" rid="F1">Fig. 1b</xref>; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a</xref>). This corresponds to a time when replay has previously been identified in rodents (e.g. 30). Here we identified significant sequenceness with a peak state-to-state time lag at 40 ms in a backward direction, and 60 to 70 ms in the forward direction (<xref ref-type="fig" rid="F2">Fig. 2c-d</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref>). To focus on a period with less cognitive demands arising from actual feedback processing and value updating, with similarities to a procedural step in a related rodent study (<xref ref-type="bibr" rid="R26">26</xref>), our analysis was focused on the latter 3.5 s of the 5 s postfeedback period; note, however, qualitatively similar results were found when using the full feedback period. In general, our finding of forward and backward replay events, intermixed across seconds, echoes results reported in rodent studies (<xref ref-type="bibr" rid="R38">38</xref>, <xref ref-type="bibr" rid="R58">58</xref>) as well as in a recent human study (<xref ref-type="bibr" rid="R11">11</xref>).</p><p id="P18">Based on these initial replay temporal lag localization analyses, we focused our primary analyses on forward sequenceness with a 70 ms lag between states identified in both the planning and feedback periods. At feedback, we selected the peak lag of 40 ms from the above-threshold lags for backward sequenceness analyses, informed by our previous work (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>). To examine links between replay and task experience, we estimated sequenceness separately for path transitions in the current world and other world, for each period on each trial.</p></sec><sec id="S5"><title>Replay during planning and model-based generalization</title><p id="P19">We first tested a link between planning and relative sequence strength, leveraging the fact that in our task model-based generalization was more beneficial when a start state changes relative to when it remains the same (<xref ref-type="fig" rid="F1">Fig. 1d</xref>) (<xref ref-type="bibr" rid="R50">50</xref>). A planning account predicts that when the start state changes the benefit of model-based generalization should be reflected in a boosting of replay relative to when the start state remains the same (<xref ref-type="fig" rid="F1">Fig 1c</xref>). To increase power, our sequenceness analyses combined evidence for the two current world paths.</p><p id="P20">In line with predictions, sequenceness for current world paths was significantly stronger when the start state changed versus remained the same (i.e. when generalization was likely to be more beneficial; multilevel regression β (regression coefficient) = 0.1411 [0.0625 0.2197]; z = 3.519, p = 0.0004; <xref ref-type="fig" rid="F3">Fig. 3a</xref>). There was no relationship between sequenceness for other world paths and generalization (β = -0.0201 [-0.0979 0.0577]; z = -0.506, p = 0.613; other world TOST equivalence test p = 0.012; current versus other difference z = 2.829, p = 0.0023, one-tailed). Analyses across different state-to-state lags indicated the presence of a selective effect of generalization centered on the independently selected lag of 70 ms (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). Further, the effect of generalization was stable across trials (positive interaction with trial, t = 1.055, p = 0.292; see <xref ref-type="supplementary-material" rid="SD1">Supp. Results and Table S3</xref> for additional control analyses). This relationship was also qualitatively similar for both the to-be chosen and the to-be non-chosen paths (p-values &lt; 0.033). In follow-up control analyses, we also tested whether higher pre-choice replay was better explained by our planning account versus an alternative memory retrieval account. The planning account predicts a stepwise increase in replay on trials when generalization was more beneficial, whereas an alternative memory retrieval account predicts a graded increase in replay related to how far in the past a start state was last seen. We found that a planning account provided a better fit to the data (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>). Overall, the link between replay and model-based generalization rests on the detection of evidence for the same sequential neural representations being triggered by two different sets of choice cues, demonstrating a form of neural generalization underlying behavioural generalization.</p><p id="P21">We next examined whether planning-related replay was modulated by option value, in light of previous imaging and electrophysiology studies in humans reporting correlations between non-sequential hippocampal activity and value (e.g. 59, 60-62). If replay is involved in deriving value estimates, then we would not necessarily expect a modulation of replay by value, though it is possible that replay might be biased by option value when values are directly informed by recent experience.</p><p id="P22">Examining the relationship between replay and mean state value, the average model-predicted value across the two options, we found that current world replay strength significantly correlated with mean state value (β (regression coefficient) = 0.0479 [0.0130 0.0829]; t = 2.688, p = 0.0072). However, this relationship was only found on trials where there were no generalization demands (where the start state remained the same; same start trial value β = 0.1128 [0.0484 0.1771]; t = 3.440, p = 0.006; different start trial value β = 0.0110 [-0.0310 0.0530]; t = 0.513, p = 0.608; interaction β = 0.0510 [0.0140 0.0880]; t = 2.702, p = 0.0069). These results were also selective to current world forward replay during planning (<xref ref-type="supplementary-material" rid="SD1">Supp. Results and Table S3</xref>). Thus, replay positively related to option value in conditions where start state options were susceptible to direct reinforcement on the preceding trial, where we speculate value inference is less demanding.</p></sec><sec id="S6"><title>Backward replay prioritization at feedback and memory preservation</title><p id="P23">We next tested a prediction that replay during periods of low cognitive demand, specifically following reward feedback, relates to automatic memory maintenance processes, which we refer to as ‘preservation. Here we focused our analyses on backward replay with a 40 ms time lag, a signal selective to the feedback period (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). Behaviorally, we found consistently high levels of path memory during reward learning (see above) and this precluded examining direct links between feedback replay and memory variability. However, in the preceding brief structure learning phase when participants first experienced the sequential paths mean memory performance was lower (78.6% [68.0 89.3]), allowing us to test for a link with backward replay within the inter-trial interval (as no reward feedback was presented). Backward replay exhibited a numerical, but non-significant, increase across time (40ms lag, second half – first half trials; 0.101 [-0.034 0.237]; p = 0.136). Notably, increased backward replay from early to late trials in this initial phase correlated significantly with individual differences in memory performance during this phase (r = 0.409, p = 0.0470; <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref>). Although the number of trials here is much lower than the primary reward learning phase, this provides initial evidence consistent with a link between replay and memory.</p><p id="P24">After initial experience, memory preservation can be considered to be an automatic process driven in part by the infrequency, or rarity, of recent experience for a given environment (<xref ref-type="bibr" rid="R43">43</xref>). In the primary reward learning phase, we operationalized rarity as an exponentially weighted average of past exposures to each environment. We found that backward replay of other world paths was greater when they had been experienced less frequently over recent trials (rarity effect for other world paths, all trials β = 0.0513 [0.0921 -0.0104]; t = 2.463, p = 0.0139; <xref ref-type="fig" rid="F4">Fig. 4a</xref>). We found no relationship between current world replay and rarity (current β = -0.0101 [-0.0307 0.0509]; t = 0.486, p = 0.627; TOST p = 0.011; other versus current difference z = -2.076, p = 0.0190, onetailed; <xref ref-type="fig" rid="F4">Fig. 4a</xref>). Moreover, the relationship with rarity was stable across trials (interaction with trial, t = 0.487, p = 0.627; see <xref ref-type="supplementary-material" rid="SD1">Supp. Results and Table S3</xref> for additional control analyses). We also confirmed this experience-replay relationship in a basic model that makes no assumptions about learning, finding that other world replay was stronger when the other world had been experienced more than one trial ago versus when experienced on the previous trial (p = 0.0469).</p><p id="P25">To further explore this putative memory preservation signal, we also examined a link between the rare experience replay effect and planning forward replay. Stronger replay of rare experiences after feedback might be expected to decrease the need for planning replay. The planning replay signal was extracted from trials where the world changed from trial-to-trial alone, as this captures where any preceding feedback period ‘other world’ replay effects may relate to planning. Consistent with this we found an inverse relationship between the strength of the modulation of backward replay by rarity and planning replay across participants (current world forward replay, world change trials r = -0.521, p = 0.009; <xref ref-type="fig" rid="F4">Fig. 4</xref>).</p><p id="P26">Next, we asked whether there was a link between this rare experience replay signature and choice behaviour. If feedback replay supports memories for more distant structure and value, we might expect the strength of this replay signal to positively influence choice in the other world. In an augmented reinforcement learning model, we tested this connection by allowing replay-related memory preservation to decrease choice uncertainty (or noisiness). The feedback replay measure was extracted from trials preceding a world change, as this is where preceding feedback period replay may relate to a following choice. The model included two additional softmax inverse temperature parameters that applied to world change trials with high versus low preceding feedback replay (<xref ref-type="sec" rid="S10">Methods</xref>). A higher inverse temperature parameter can reflect lower uncertainty such that choices are more strongly guided by estimated prospective values. We found a significantly higher inverse temperature when choices were preceded by high replay versus low feedback replay (world change trials; high replay median = 19.40; low replay = 14.27; z = 2.171, p = 0.015, one-tailed, Wilcoxon signed rank test). Control analyses demonstrated that the replay effect on choice was selective to backward replay of the relevant world (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>). Further, while backward replay related to the experienced rarity of a world, we found no modulation of choice noisiness by experienced rarity itself, consistent with the internal variability of backward replay underlying the observed effect. Thus, supporting a potential memory preservation mechanism, we found 1) that backward replay was positively modulated by rarity of experience, 2) that a stronger rarity replay effect was linked to lower planning replay strength, and 3) that the strength of backward replay on a trial-to-trial basis decreases uncertainty in subsequent choices.</p><p id="P27">We then compared the task and experience links to replay that we identified during planning and after feedback to determine if these signals were distinct. We found no correlation between planning period forward replay and rarity of recent experience effect (current world 70 ms lag, p = 0.815), with the feedback period significantly stronger than the planning period effect (difference, z = 1.778, p = 0.0378, one-tailed). Conversely, we found no significant correlation between feedback period backward replay and the benefit of generalization (other world 40 ms lag, p = 0.119), while the planning period effect was significantly stronger than the feedback period (difference, z = 2.891, p = 0.002, one-tailed). Together, these planning and feedback comparisons represent a double dissociation, with feedback period replay being selective for an expected signature of memory preservation.</p><p id="P28">In additional control analyses, we found no relationship between backward replay and reward feedback or reward prediction error (<xref ref-type="supplementary-material" rid="SD1">Supp. Results and Table S3</xref>). Further, during planning, we identified significant forward replay with a 190 and 200 ms time lag (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), but found no correlation between this signal and any variables of interest (<xref ref-type="supplementary-material" rid="SD1">Table S3</xref>). At feedback, we also identified significant forward replay with a 70 ms time lag (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), but found no correlation between current world forward replay with feedback or other variables of interest (<xref ref-type="supplementary-material" rid="SD1">Supp. Results and Table S3</xref>). Finally, in exploratory analyses of a longer 160 ms lag replay signal identified recently (<xref ref-type="bibr" rid="R11">11</xref>), we found no relationship with variables of interest at feedback or during planning (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>).</p></sec><sec id="S7"><title>Replay onset beamforming and time-frequency analyses</title><p id="P29">To explore the spatial source of sequenceness events we conducted supplemental beamforming source localization analyses to test whether replay onset is associated with increased power in the hippocampus, as previously found (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R63">63</xref>). As the interpretation of sources of MEG signal is complex, especially for putative deep regions such as the MTL, we emphasize the supplemental nature of these analyses. Candidate replay onsets were identified by locating sequential reactivation events at time lags of interest, applying a stringent threshold to these events, and conducting broad-band beamforming analyses (as in 7).</p><p id="P30">At both planning and feedback periods, we identified power increases associated with replay onset in clusters extending from peaks in the visual cortex into the hippocampus (<xref ref-type="fig" rid="F5">Fig. 5</xref>; planning forward replay peak MNI coordinates [x, y, z] 20, -81, -2; p &lt; 0.05 whole-brain permutation-based cluster correction; feedback backward replay peak; -10, -91, -7; <xref ref-type="supplementary-material" rid="SD1">Fig. S7; Table S4</xref>). Using a hippocampal ROI analysis, during the planning period there was a significant power increase at replay onset evident in the right hippocampus (30, -36, -4; p &lt; 0.05, ROI permutation-based cluster correction). Within the feedback period, we found bilateral hippocampal power increases (right 30, -21, -12 and 26, -36, 0; left -25, -16, -17; p &lt; 0.05, corrected). Similar whole brain and hippocampal ROI results were found when separately looking at planning period current world replay onsets and feedback period other world replay onsets (<xref ref-type="supplementary-material" rid="SD1">Table S4</xref>). These results, along with recent related reports (<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>), support an interpretation that sequential reactivation is tightly linked to enhanced hippocampal activity.</p><p id="P31">In separate time-frequency analyses of power changes associated with replay onset events, we found that replay onset for both the planning and feedback periods related to increased power centered at 5-40 Hz (p&lt; 0.05, permutation-based cluster-correction; <xref ref-type="supplementary-material" rid="SD1">Fig. S8</xref>). While we found no main effect of replay-related power increases in the ripple band (120-150 Hz) (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R12">12</xref>), at feedback, we found that individual differences in other world replay onset ripple power correlated with the strength of the modulation of backward replay by rarity (other world – current world; r=0.527, p=0.016 corrected for two comparisons; <xref ref-type="supplementary-material" rid="SD1">Fig. S8</xref>). A similar but non-significant relationship was found for the theta band (5-8Hz, r=0.449, p=0.0572, corrected).</p></sec><sec id="S8"><title>Generalized position representations across worlds</title><p id="P32">Finally, we asked whether learning led to changes in neural representations that reflected abstract, generalized, information about task structure (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R64">64</xref>). We predicted that neural representations of stimuli in the same path position (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, and <xref ref-type="bibr" rid="R3">3</xref>) would become more similar after learning (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R12">12</xref>), an abstraction which might aid planning. We used Representational Similarity Analysis (RSA) to index representation changes from the pre-task localizer to reward learning path navigation (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R65">65</xref>, <xref ref-type="bibr" rid="R66">66</xref>).</p><p id="P33">When path stimuli were presented during learning, we found evidence for a significant representation of position information (<xref ref-type="supplementary-material" rid="SD1">Fig. S9</xref>; p&lt; 0.05 FWE corrected). In particular, a position representation was evident for stimuli presented in different worlds (‘across-world’), consistent with position information generalizing across stimuli shown on separate trials. Thus, for example, stimuli occupying position 1 in world 1 showed greater similarity to stimuli in position 1 in world 2 than to stimuli in positions 2 or 3 in world 2. The effect was evident in an initial peak from 180–250ms (p &lt; 0.0001 uncorrected), where across-world position information was present for all three individual positions when examined separately (p-values &lt; 0.0012). Our design did not include a post-learning localizer, so these representations were necessarily assessed during reward learning. Because of this, it is possible that some position representation information during the learning task could arise from a preceding trial phase, or indeed from cognitive expectation effects. We note that while previous findings showed encoding of position information for stimuli interleaved during learning (<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R12">12</xref>), our results demonstrate that position information generalizes across structurally similar environments even where stimuli never overlapped.</p><p id="P34">We also investigated path identity representations. For decision-making, it can be helpful to differentiate between grouped stimuli so as to facilitate distinct reactivation during learning; alternatively, it may be helpful to increase similarity for grouped stimuli to allow for linking or chunking. Supporting a differentiation effect, path information emerged at 800ms after stimulus onset, as reflected in a significant decrease in similarity for stimuli in the same path (800-910ms, p &lt; 0.05 FWE corrected; peak 820ms, p = p = 0.00038 uncorrected; <xref ref-type="supplementary-material" rid="SD1">Fig. S9</xref>). We did not find any relationship between position or path representation strength and behavioural or other neural measures; these null effects could partly be due to noise added by cognitive expectation effects during learning.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P35">Two proposed roles for neural replay relate to prospective decision-making and memory preservation. We show that during decision-making, replay strength was related to the relative benefit of model-based, goal-directed, control of behavior. By contrast, after outcome feedback, replay of alternative environment paths positively related to the rarity in recent experience of a more distal environment. Furthermore, consistent with a putative role in memory preservation, stronger replay following feedback related to a subsequent decrease in behavioural choice uncertainty for the alternative environment. Thus, we find selective links between replay strength and the benefits of planning and the recency of experience, demonstrating distinct roles for replay within a single task.</p><p id="P36">By manipulating the benefit derived from model-based generalization of reward value across trials, we identified a relationship between planning replay and use of modelbased inference. Building on previous imaging work which linked future state reactivation and individual variability in model-based behavior (<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R50">50</xref>) we show here, on a trial-by-trial basis, a boosting of replay when making a choice in a different, but functionally equivalent, state as on the preceding trial, potentially supporting value inference. Further, planning-related replay onsets were associated with power increases in the MTL consistent with a localization to the hippocampus. While previous lesion studies have demonstrated an overall role for the hippocampus in model-based behaviour (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>), our results suggest that this contribution occurs during model-based planning.</p><p id="P37">Model-based generalization can be accomplished using different strategies in addition to planning, such as updating ‘non-local’ options following feedback (<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R56">56</xref>) or by forming overlapping representations through extensive experience. To illuminate planning, our design included two unique features. First, unlike a majority of learning experiments which repeat the same environment on each and every trial (e.g. 52, 56), we employed two separate environments, intermixed in an unpredictable manner across trials, thereby limiting the utility of planning for the next choice immediately after feedback. Furthermore, in our design, the two alternative start states in an environment converge upon shared paths at the very first step, potentially increasing the degree of inference required during planning to differentiate between trajectories. These two features, environment alternation and early path convergence, distinguish the current task from a recent report that focused on feedback linked replay signals (<xref ref-type="bibr" rid="R11">11</xref>), which revealed that generalization involved updating different non-local start states after feedback. Here, by contrast, we found no significant reward-or planning-related responses after feedback either for our replay signatures of interest, or for the signature identified in Liu et al. (<xref ref-type="bibr" rid="R11">11</xref>) (<xref ref-type="supplementary-material" rid="SD1">Supp. Results</xref>). Thus, our current results, and those of Liu et al. (<xref ref-type="bibr" rid="R11">11</xref>), both demonstrate a link between neural replay and model-based inference, but at different time periods. It is often the case that the decisions we face arise unpredictably, and our current results support the idea that replay is of benefit in such situations.</p><p id="P38">To successfully generalize reward feedback and perform well in our task, participants need a model-based strategy. However, similar to a design used in a recent related report (<xref ref-type="bibr" rid="R11">11</xref>), choices were only made at the first level, and thus evaluation of path steps was not strictly necessary for model-based behavior. Nevertheless, we found robust evidence for path replay, identifying a new link between replay and model-based decision-making (planning). Further, this relationship was specific to sequential replay and we found no effects related to the reactivation of individual states. We speculate that in many environments outside the lab, replay-assisted planning of trajectories is advantageous and that it may be a default strategy employed even when not strictly beneficial. Future experiments could usefully study these neural mechanisms in environments where sequential step-by-step choices are required, although a previous study in this domain did not identify connections to participants’ behavior (<xref ref-type="bibr" rid="R9">9</xref>). Similarly, to link sequential replay to value-guided choice, it will be important for future experiments to explore whether pre-choice replay events are linked to value estimates. Our task was not optimized for value decoding. However, in simpler contexts, replay has been linked to outcome representation during a post-learning rest period in humans (<xref ref-type="bibr" rid="R8">8</xref>), with related links reported between hippocampal activity and activity in the ventral striatum or ventral tegmental area in rodents (<xref ref-type="bibr" rid="R67">67</xref>, <xref ref-type="bibr" rid="R68">68</xref>).</p><p id="P39">In contrast to planning, a rest period after outcome feedback entails minimal cognitive demands with respect to the current environment, rendering it likely that activity at this time-point might support preservation of weaker memories (<xref ref-type="bibr" rid="R1">1</xref>). Our results, selective to the feedback period, are consistent with this. Previous studies of hippocampal replay in rodents have suggested a link to less recent experiences (<xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R39">39</xref>). However, in these studies experience was confounded with low value, and in one case, a shift in replay was observed even before a reduction in experience (<xref ref-type="bibr" rid="R39">39</xref>). By parametrically varying experience in two distinct environments and controlling for value, we provide a quantitative link between replay and infrequent experiences. Such a neural replay mechanism can act to reinforce memories that are at risk of becoming weaker, effectively serving as internally-generated interleaved training (<xref ref-type="bibr" rid="R22">22</xref>). While we found a link to memory preservation during ongoing behaviour, we speculate that such a preservation mechanism may also operate offline for memory consolidation.</p><p id="P40">Our experiment necessitated robust task structure knowledge and very high memory performance for the sequential paths. Behaviourally, previous research has shown that a strong understanding of task structure promotes model-based learning while, conversely, a poor understanding leads to idiosyncratic learning strategies (<xref ref-type="bibr" rid="R69">69</xref>). One potential limitation of high memory performance is that this precludes linking variability in memory performance and post-feedback replay. However, during reward learning, a memory preservation account does not necessarily predict a simple relationship between the two: if internal evidence of lower memory strength drives higher post-feedback replay, an effective replay mechanism may remediate any memory compromise before it can be observed. A feature of our design is that it allows us to identify a neural mechanism that may naturally assist in maintaining high memory levels for distant experiences. Further, our reinforcement learning models suggest a link between trial-to-trial strength in feedback replay and memory via lower noise in the following choice. In general, we speculate that replay-supported memory preservation supports adaptive behaviour in situations where memory is more variable, and these are avenues to explore in future experiments.</p><p id="P41">We suggest that there may be a trade-off between the two separate functions of planning and memory preservation that we identify. One idea is that the content and function of replay is modulated by task demands, which differ between planning versus resting after feedback (<xref ref-type="bibr" rid="R1">1</xref>). Consistent with this, it has been reported that replay content that is temporally proximal to active navigation is task-directed, while replay content during reward consumption is undirected, potentially related to preserving memory of the entire environment (<xref ref-type="bibr" rid="R26">26</xref>). Computationally, our results suggest the expression of replay may reflect an arbitration of resources as a function of a reliance on planning versus memory (<xref ref-type="bibr" rid="R70">70</xref>). In line with such a tradeoff, we found that across-participant strength of the memory replay effect related to a lower expression of planning period forward replay. Such arbitration between functions would also influence the degree to which replay supports updating of values, as in a recently proposed computational model of replay (<xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R34">34</xref>). We did not formally manipulate task demands such as cognitive load during planning, but our results provide potential pointers for future targeted studies (<xref ref-type="bibr" rid="R71">71</xref>).</p><p id="P42">In conclusion, we provide evidence that prospective replay is enhanced when model-based behavior is beneficial, while replay consistent with memory preservation is observed when demands are low, consistent with distinct signatures for key proposed functions of neural replay (<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R49">49</xref>). While we dissociate these functions, both planning and memory functions are necessary for adaptive behavior, not least because a stable memory of the one’s environment aids successful decision making (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R72">72</xref>). By identifying replay signatures for planning and recency of experience, our results have relevance for targeting an understanding of common or separable disruptions to these functions in psychiatric disorders and disorders that impact on memory, such as Alzheimer’s disease (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R73">73</xref>–<xref ref-type="bibr" rid="R77">77</xref>).</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><p id="P43">Twenty-seven healthy volunteers participated in the experiment. Participants were recruited from the UCL Psychology and Language Sciences SONA database and from a group of volunteers who had participated in previous MEG studies. Of this participant group, the MEG session was not conducted in three participants: one due to scheduling conflicts, one due to technical problems with the MEG scanner, and one due to poor performance in the behavioral training session (see below). This resulted in the inclusion of data from 24 participants for behavioral and MEG analyses (14 female; mean age 23.8 years; range 18-34). Participants were required to meet the following criteria: age between 18-35, fluent English speaker, normal or corrected-to-normal vision, without current neurological or psychiatric disorders, no non-removable metal, and no participation in an MRI scan in the two days preceding the MEG session. The study was approved by the University College London Research Ethics Committee (Approval ID Number: 9929/002). All participants provided written informed consent before the experiment. Participants were paid for their time, for their performance in the reward learning task as well as memory for the state-to-state sequences (up to £10 based on percent correct performance above chance), and a bonus for performance in the localizer phase target detection task (up to £2).</p><sec id="S11"><title>Experimental task</title><p id="P44">We designed our reward learning experiment to investigate the potential role of replay in prospective planning and memory preservation (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). We adapted a reward-based learning task used in previous experiments to target model-based decision making (<xref ref-type="bibr" rid="R50">50</xref>, <xref ref-type="bibr" rid="R51">51</xref>), itself an adaptation of a common ‘two-step’ task (<xref ref-type="bibr" rid="R52">52</xref>). In this version of the task, there is an equivalence between the two alternative top-level start states in a world. This equivalence provides us with the ability to dissociate model-based and model-free behavior. An additional condition is that potential reward points drift across trials at a sufficient rate that allows for model-free and model-based expectations to often differ. We refer to the decision process as ‘planning’. Closely related studies (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R71">71</xref>) have defined planning as the engagement of model-based decision-making, and our results are consistent with a strong role for model-based decision-making. Of note, and similar to the current design, Miller et al. (<xref ref-type="bibr" rid="R41">41</xref>) use the term ‘planning’ in a two-step task where a decision is made solely at the top level.</p><p id="P45">Additionally, this version utilizes deterministic transitions between states. The deterministic version of the task has been shown to both incentivize and increase model-based behavior (<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R54">54</xref>). A task using probabilistic transitions would have decreased our ability to detect clear evidence of sequential reactivation, given the increase in number of transitions to evaluate and a constrained number of total trials, as reasonable scanning duration was already maximized. This reasoning also led to using a task with a non-branching path structure (i.e. where states involved no forced choices). We also constrained choice points and branching paths in order to decrease the number of total states, which maintained our ability to sufficiently decode visual states using state-of-the-art imaging technology and analysis techniques (<xref ref-type="bibr" rid="R53">53</xref>). Importantly, this limitation of branching choices is similar to the environments in the majority of rodent studies of replay, where animals navigate linear tracks with few, if any, subsequent choices (e.g. 2, 27, 30, 33, 35, 36, 39, 44). While these situations do not strictly require sequential evaluation (<xref ref-type="bibr" rid="R11">11</xref>), such human and animal designs are able to take advantage of any default tendency to utilize such a neural mechanism. Finally, as measures of model-based behavior are dependent on start state alone, the current task omitted choice at the terminal state, allowing participants to better track rewards and thereby likely increase model-based behavior (<xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R54">54</xref>, following recent work; <xref ref-type="bibr" rid="R78">78</xref>).</p><p id="P46">We augmented this task by adding a second multi-step structure or ‘world’ in order to study the relationship between neural replay signatures and memory preservation (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Further, by employing two worlds we 1) decreased the predictability of the upcoming trial, which in turn increased the utility of planning at choice onset versus immediately after feedback (<xref ref-type="bibr" rid="R56">56</xref>), and 2) decreased the dependence of reward learning on short-term working memory for immediately preceding feedback (<xref ref-type="bibr" rid="R79">79</xref>, <xref ref-type="bibr" rid="R80">80</xref>). We ensured that participants understood the structure of the task in four ways, as poor understanding of a related paradigm has been reported to result in apparent model-free behavior (<xref ref-type="bibr" rid="R69">69</xref>). First, we provided extensive instructions to minimize misunderstanding. Second, we employed an initial non-reinforced structure exploration learning phase. Third, across all sessions, accurate knowledge of task structure itself was incentivized by periodic memory test trials that allowed participants to harvest additional payment. Fourth, participants learned the abstract task structure during an initial training session (with different stimuli). This session was followed by a break before the MEG session, allowing for robust learning of the structure prior to scanning and potentially allowing for memory consolidation. We conducted the first session on a preceding day for almost all participants (n = 22, range 1-13 days) or after a 2-hour break in two participants, resulting in a 2 day median separation between sessions. Overall, these features helped ensure that behavior was quite model-based, as our goal was to understand planning. Further, by studying behavior reliant on well-learned structure knowledge, our results may better connect to how memory is utilized outside the lab.</p></sec></sec><sec id="S12"><title>Session 1 behavioral procedure</title><sec id="S13"><title>Instructions</title><p id="P47">In the first experimental session, participants were first instructed in the two-world task, then completed a structure learning phase, and finally completed a brief reward learning phase. This session utilized the same abstract structure as the subsequent MEG session but the structures were populated with different visual stimuli.</p><p id="P48">Participants were given detailed instructions about the reward learning task and the underlying ‘world’ structure in order to maximize understanding. In brief, they were first instructed that on each trial they would face a choice between two different shape options. Each shape would lead to a different path made up of three sequentially-presented images. Paths ended in reward points, which ranged from 0-9, and these reward points would drift over time. Their goal was to choose the shape that led down the path to the currently greater number of reward points to earn more bonus money in the experiment. They were also told that along with the presentation of shapes, they would see either ‘1x’ or ‘5x’ above the shape options, which indicated whether the end reward points on this trial would be multiplied by 1 or by 5 (<xref ref-type="bibr" rid="R51">51</xref>).</p><p id="P49">Next, participants were told that there were actually two pairs of shape options that would converge to lead down the same two paths and reach the same source of rewards. The shapes in the two pairs were equivalent, in that if shape 1a in start state A led to the path with a snowflake, shape 1b in start state B would also lead to the path with the snowflake (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Importantly, participants were instructed that the rewards at the end of the paths would be reached by either of the potential starting shapes (e.g. 1a and 1b). Consequently, if they were first choosing between the shapes in start state A and found a high reward after choosing shape 1a, then if they were subsequently starting with pair B, they should choose shape 1b in order to get to the same source of reward as they had just experienced when they chose shape 1a. Participants can only accomplish this by leveraging their knowledge of the task structure, classifying this as model-based behavior.</p><p id="P50">Finally, participants were instructed that these shapes and paths made up 1 ‘world’ and that the real experiment would have two independent worlds, each with the same structure. Trials would start with a pair of shapes from one of the two worlds at random. In general, participants’ goal was to keep track of what paths led to the highest rewards at a given time and to choose the shape that led to those paths, while at the same time maintaining their memory for the paths in each world.</p></sec><sec id="S14"><title>Structure learning session 1</title><p id="P51">Before starting the full reward learning task with point feedback, participants were given the opportunity to learn the structure of the worlds. This phase was composed of two blocks of 20 trials, with learning incentivized by rewarding performance on memory questions about the structure of the worlds. Trials started pseudorandomly in one of the four potential start states across the two worlds.</p><p id="P52">Participants’ goal in this structure learning phase was to explore the different paths in order to learn the sequence of images that followed each shape. Trial events were the same as in reward learning phase trials (see below; <xref ref-type="fig" rid="F1">Fig. 1b</xref>; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a</xref>) with the exception that no reward points were shown at the end of the paths and no stakes information was shown during shape presentation. In each structure learning trial, after a planning period, participants made a choice between two shape options (shown randomly on the left and right of the screen). After this selection, the three following states in the path associated with that shape were presented sequentially. Participants were instructed to remember the complete sequence from the chosen shape through to the third path picture. Each path stimulus was randomly presented on the left or right side of the screen and participants needed to press the corresponding 1 or 2 button to indicate the stimulus location on the screen. In this phase, no reward feedback was presented. A fixation cross was presented during a 4-6 s inter-trial interval (ITI). (See detailed timing in the MEG reward learning phase description.)</p><p id="P53">After each of the two structure learning blocks of 20 trials, participants completed eight memory test probe trials (<xref ref-type="supplementary-material" rid="SD1">Fig. S1b</xref>). Each of the eight start shapes cued one probe trial. On a memory test trial, a single shape was presented on the left or right side of the screen. After the participant selected the shape, they were presented with four potential stimuli from the first state of each of the four paths. Participants selected the stimulus that came next using the 1-4 buttons. After framing the selected stimulus in blue for 0.25 s, this probe structure was repeated for the second and third states in the path. At each stage, one of the four stimuli was correct, while the other three stimuli came from the same state (first, second, or third) across the other three paths. After the response for the third path stimulus, participants were asked to rate their confidence in their set of answers according to the following scale and indicated button response: “Guess (1) Low (2) Medium (3) Certain (4)”. Correct performance was based on accurately selecting the correct picture at each of the three stages. Structure memory performance increased from 56.9% (range, 0-100) after the first 20 learning trials to 90.1% (range, 12.5-100) after the second 20 learning trials. Similarly, mean confidence ratings increased from 2.89 to 3.65.</p></sec><sec id="S15"><title>Reward learning session 1</title><p id="P54">Next, participants engaged in a short reward learning phase to provide experience in maximizing reward earnings. The reward learning phase was the same as the scanned reward learning phase in session 2 (below), with the exception that structure memory questions were pseudo-randomly interleaved with the regular reward learning trials instead of being segregated to breaks between blocks. Trials started pseudorandomly in one of the four potential start states across the two worlds. All trials proceeded in the same way as trials in the preceding structure learning phase, with the addition of reward feedback at the end of each path as well as cued stakes information during planning and choice (<xref ref-type="fig" rid="F1">Fig. 1b</xref>; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a</xref>). Reward feedback was presented after a 2 s inter-stimulus-interval (ISI). Reward points flickered in brightness for a period of 1.5 s and then a 3-5 s blank inter-trial interval followed, a slight difference in procedure from the subsequent MEG session. (See detailed timing in the MEG reward learning phase description.)</p><p id="P55">The reward learning phase length in session 1 was initially based on the free time remaining in the scheduled session but was then set to be a maximum of 40 trials, resulting in a mean of 50 trials across participants (range, 29-105 trials). The memory probe questions were made more difficult in the reward learning phase than the structure learning phase by randomizing the incorrect lure stimuli to be from any stage and any path. Performance on the interleaved structure memory probes was 92.2% (range 70-100%). As noted above, one participant was not invited for MEG scanning based on very poor session 1 memory probe performance (40% correct).</p></sec></sec><sec id="S16"><title>Session 2 MEG procedure</title><sec id="S17"><title>Localizer</title><p id="P56">After initial setup in the MEG room, participants were given instructions for the localizer phase. The purpose of the functional localizer was to derive participant-specific sensor patterns that discriminated each of the 12 stimuli that made up the world paths by presenting each stimulus many times. The localizer design was adapted from those used previously, where a picture name identification task followed the presentation of each picture stimulus (<xref ref-type="bibr" rid="R11">11</xref>). Participants were instructed to pay attention to a picture shown in the center of the screen and then after the picture disappeared, to select the correct name for the picture from two alternatives. For complete localizer phase details, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>.</p></sec><sec id="S18"><title>Structure learning session 2</title><p id="P57">Participants then engaged in a structure learning phase where the new stimuli from the preceding localizer populated the two worlds. This phase was the same as the no-reward structure learning phase in session 1 (above). Participants were reminded of the world structure. For analyses, accuracy focused on the response for the first transition. We found participants explored all of the eight potential shape-path combinations (most-explored path per-participant, mean 6.8 trials out of 40 total [range 6.0 – 8.4]; least-explored path per-participant, mean 3.3 trials [range 2.0 – 4.2]).</p></sec><sec id="S19"><title>Reward learning session 2</title><p id="P58">Participants then engaged in the primary reward learning phase. The design of this phase was the same as the reward phase in session 1 (<xref ref-type="fig" rid="F1">Fig. 1b</xref>; <xref ref-type="supplementary-material" rid="SD1">Fig. S1a</xref>). Participants aimed to earn as many points as possible in the two different worlds. This phase was composed of 6 blocks of 24 trials for MEG data collection, yielding 144 total reward learning trials. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref> for full details.</p></sec></sec><sec id="S20"><title>Behavioral analysis</title><p id="P59">Learning behavior was analyzed using computational models, following prior studies (e.g. 50, 51, 52). To verify learning and determine how reward influenced choice, we used computational models that seek to explain a series of choices in terms of previous events. First, we used logistic regression models, which test for local trial-to-trial adjustments in behavior guided by minimal assumptions about their form, yet at the same time qualitatively capture aspects of model-free and model-based behavior (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>). We then used Q-learning reinforcement learning models, which use a structured set of assumptions to capture longer-term coupling between events and choices and can capture model-free and model-based contributions (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>).</p></sec><sec id="S21"><title>Rarity of experience</title><p id="P60">To examine potential memory preservation effects after reward feedback, we computed a variable representing the relative inverse frequency (‘rarity’) of the alternative world. Frequency of experience was an exponentially-weighted measure computed for each world on a trial-by-trial basis. This measure was calculated by adapting the fractional updating in Equation 2 to track world frequency based on appearance (1) or non-appearance (0) of a world on a given trial. We expected the learning rate controlling the change in estimates of world frequency to be relatively slow based on related work (<xref ref-type="bibr" rid="R81">81</xref>), which led us to set the experience learning rate to 0.10. The resulting experience frequency values were then inverted to provide the infrequency or ‘rarity’ of each world at each trial. A subsequent control analysis made no assumptions about learning rate, simply testing whether other world replay was stronger when that world had been experienced on the last trial or not.</p></sec><sec id="S22"><title>MEG Pre-processing</title><p id="P61">For complete details on MEG acquisition and initial pre-processing, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>. For planning analyses, we used the 2.5 s pre-choice planning period after excluding the first 160 ms to allow for early visual stimulus processing, following a related previous experiment (<xref ref-type="bibr" rid="R7">7</xref>). For memory analyses in the post-feedback period, where our prediction was that memory processes would be engaged when other cognitive demands are relatively low, with similarities to a procedural step in a related rodents study (<xref ref-type="bibr" rid="R26">26</xref>), we focused on the time period following initial reward processing (the latter 3.5 s of the 5 s period). Here we expected that the demands of actual feedback processing and value updating would preclude the engagement of any memory preservation signal. However, we note that our results remain qualitatively the same even without this early time period exclusion step.</p></sec><sec id="S23"><title>MEG data decoding and cross-validation</title><p id="P62">Lasso-regularized logistic regression models were trained for each of the 12 stimuli from the paths. Methods followed previous studies (<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>); for additional details see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref>. Decoding models were trained on MEG data elicited by direct presentations of the visual stimuli. Our experimental task was not optimized to detect reactivation of expected value or reward point outcome representations, and as a consequence our analyses focus only on stimuli.</p></sec><sec id="S24"><title>Sequenceness measure</title><p id="P63">The decoding model described above allowed us to measure spontaneous sequential reactivation of the 12 states either during the planning or after feedback periods. We applied each of the 12 trained classifiers to the MEG data at each time point in each period. This yielded a [time x state] reactivation probability matrix for each period in each trial, containing twelve time series of reactivation probabilities each with the length of time samples included in the analysis window. Please see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref> for complete details.</p><p id="P64">We then used the Temporally Delayed Linear Modelling framework to quantify evidence of ‘sequenceness’ (<xref ref-type="bibr" rid="R53">53</xref>), which describes the degree to which these representations were reactivated in a task-defined sequential order (<xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R53">53</xref>). TDLM is a multiple linear regression approach that quantifies the degree to which a time-lagged reactivation timecourse of state <italic>j</italic>, (<italic>X</italic>(Δ<italic>t</italic>)<italic><sub>j</sub></italic>, where Δ<italic>t</italic> indicates lag time) predicts the reactivation timecourse of state <italic>i</italic>, (<italic>X<sub>i</sub></italic>). It involves two stages. At the first stage, we use a set of multiple linear regression models to generate the empirical state-to-state reactivation pattern, using each state’s (<italic>j</italic> ∈ [1: 12]) reactivation timecourse as a dependent variable, and the historical (i.e., time-lagged) reactivation timecourses of all states (<italic>i</italic> ∈ [1: 12]) as predictor variables.</p><p id="P65">At the second stage, we quantified the evidence that the empirical transition matrix can be predicted by the sequences of interest, i.e., the 4 paths across both worlds in the task. All transitions of interest were specified in model transition matrices, separately for a forward direction (<italic>T<sub>F</sub></italic>, the same as visual experience) and the inverse for a backward direction (<italic>T<sub>B</sub></italic>). As control variables, the regression included a constant matrix (<italic>T<sub>cons</sub></italic>) that captures the average of all transitions, ensuring that any identified effects were not due to background neural dynamics, and a matrix (<italic>T<sub>auto</sub></italic>) that models self-transitions to control for auto-correlation. Repeating this procedure at each time lag (Δ<italic>t</italic> = 10, 20, 30, …, 600 ms) results in timecourses of both forward and backward sequence strength as a function of time lag, where smaller lags indicate greater time-compression of replay.</p><p id="P66">Please see <xref ref-type="supplementary-material" rid="SD1">Supplementary Methods</xref> for the following sections: Identifying Replay Onsets, MEG Source Reconstruction, Time-frequency analyses, Non-sequential reactivation analyses, Representational similarity, Multilevel regression models, and Statistical correction and null effects.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS154990-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d3aAdLbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S25"><title>Acknowledgments</title><p>The authors thank Rani Moran for helpful discussions, Matt Nour for assistance with analysis, and Daniel Bates and the Imaging Support team at WCHN for assistance with scanning. This work was supported by a Wellcome Investigator Award (098362/Z/12/Z) to R.J.D. G.E.W is supported by a research fellowship from the Deutsche Forschungsgemeinschaft (DFG) and an MRC Career Development Award (MR/V032429/1). Y.L. is supported by the Open Research Fund of the State Key Laboratory of Cognitive Neuroscience and Learning. D.M is supported by a Sir Henry Wellcome Trust Postdoctoral Research Fellowship (110257/Z/15/Z). The Max Planck University College London Centre is a joint initiative supported by University College London and the Max Planck Society. The Wellcome Centre for Human Neuroimaging is supported by core funding from the Wellcome Trust (203147/Z/16/Z).</p><p>This research was funded in whole, or in part, by the Wellcome Trust 098362/Z/12/Z. For the purpose of Open Access, the author has applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</p></ack><sec id="S26" sec-type="data-availability"><title>Data availability</title><p id="P67">Complete behavioral data are publicly available on the Open Science Framework (<ext-link ext-link-type="uri" xlink:href="https://osf.io/szjxp/">https://osf.io/szjxp/</ext-link>). The full MEG dataset will be publicly available on openneuro.org.</p><sec id="S27" sec-type="data-availability"><title>Code availability</title><p id="P68">Example code for sequenceness analyses will be available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/gewimmer-neuro/multistep_replay">https://github.com/gewimmer-neuro/multistep_replay</ext-link>).</p></sec></sec><fn-group><fn id="FN1" fn-type="con"><p id="P69"><bold>Author contributions:</bold> G.E.W., Y.L., and D.C.M. designed the experiment. G.E.W. collected and analyzed the data. G.E.W and Y.L. wrote the analysis code. G.E.W, Y.L., D.C.M and R.D. contributed to data interpretation. G.E.W. wrote the paper with Y.L., D.C.M., and R.J.D.</p></fn><fn id="FN2" fn-type="conflict"><p id="P70"><bold>Competing interests:</bold> The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olafsdottir</surname><given-names>HF</given-names></name><name><surname>Bush</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><article-title>The Role of Hippocampal Replay in Memory and Planning</article-title><source>Current biology : CB</source><year>2018</year><volume>28</volume><fpage>R37</fpage><lpage>R50</lpage><pub-id pub-id-type="pmcid">PMC5847173</pub-id><pub-id pub-id-type="pmid">29316421</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2017.10.073</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzsaki</surname><given-names>G</given-names></name></person-group><article-title>Forward and reverse hippocampal place-cell sequences during ripples</article-title><source>Nat Neurosci</source><year>2007</year><volume>10</volume><fpage>1241</fpage><lpage>1242</lpage><pub-id pub-id-type="pmcid">PMC2039924</pub-id><pub-id pub-id-type="pmid">17828259</pub-id><pub-id pub-id-type="doi">10.1038/nn1961</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsaki</surname><given-names>G</given-names></name></person-group><article-title>Hippocampal sharp wave-ripple: A cognitive biomarker for episodic memory and planning</article-title><source>Hippocampus</source><year>2015</year><volume>25</volume><fpage>1073</fpage><lpage>1188</lpage><pub-id pub-id-type="pmcid">PMC4648295</pub-id><pub-id pub-id-type="pmid">26135716</pub-id><pub-id pub-id-type="doi">10.1002/hipo.22488</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joo</surname><given-names>HR</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>The hippocampal sharp wave-ripple in memory retrieval for immediate use and consolidation</article-title><source>Nat Rev Neurosci</source><year>2018</year><volume>19</volume><fpage>744</fpage><lpage>757</lpage><pub-id pub-id-type="pmcid">PMC6794196</pub-id><pub-id pub-id-type="pmid">30356103</pub-id><pub-id pub-id-type="doi">10.1038/s41583-018-0077-1</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><article-title>Replay Comes of Age</article-title><source>Annu Rev Neurosci</source><year>2017</year><volume>40</volume><fpage>581</fpage><lpage>602</lpage><pub-id pub-id-type="pmid">28772098</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname><given-names>AM</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Hippocampal theta sequences reflect current goals</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>289</fpage><lpage>294</lpage><pub-id pub-id-type="pmcid">PMC4428659</pub-id><pub-id pub-id-type="pmid">25559082</pub-id><pub-id pub-id-type="doi">10.1038/nn.3909</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Vehar</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TJ</given-names></name><name><surname>Dolan</surname><given-names>RD</given-names></name></person-group><article-title>Episodic memory retrieval success is related to rapid replay of episode content</article-title><source>Nat Neurosci</source><year>2020</year><pub-id pub-id-type="pmcid">PMC7610376</pub-id><pub-id pub-id-type="pmid">32514135</pub-id><pub-id pub-id-type="doi">10.1038/s41593-020-0649-z</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name></person-group><article-title>Human replay spontaneously reorganises experience</article-title><source>Cell</source><year>2019</year><volume>178</volume><fpage>640</fpage><lpage>652</lpage><elocation-id>e614</elocation-id><pub-id pub-id-type="pmcid">PMC6657653</pub-id><pub-id pub-id-type="pmid">31280961</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2019.06.012</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Economides</surname><given-names>M</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Fast Sequences of Non-spatial State Representations in Humans</article-title><source>Neuron</source><year>2016</year><volume>91</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="pmcid">PMC4942698</pub-id><pub-id pub-id-type="pmid">27321922</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.05.028</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Lievre</surname><given-names>G</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>The roles of online and offline replay in planning</article-title><source>Elife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7299337</pub-id><pub-id pub-id-type="pmid">32553110</pub-id><pub-id pub-id-type="doi">10.7554/eLife.56911</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Experience replay is associated with efficient nonlocal learning</article-title><source>Science</source><year>2021</year><volume>372</volume><pub-id pub-id-type="pmcid">PMC7610948</pub-id><pub-id pub-id-type="pmid">34016753</pub-id><pub-id pub-id-type="doi">10.1126/science.abf1357</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nour</surname><given-names>MM</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Arumuham</surname><given-names>A</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Impaired neural replay of inferred relationships in schizophrenia</article-title><source>Cell</source><year>2021</year><pub-id pub-id-type="pmcid">PMC8357425</pub-id><pub-id pub-id-type="pmid">34197734</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.06.012</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wise</surname><given-names>T</given-names></name><name><surname>Liu</surname><given-names>YZ</given-names></name><name><surname>Chowdhury</surname><given-names>F</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Model-based aversive learning in humans is supported by preferential task state reactivation</article-title><source>Science Advances</source><year>2021</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC8318377</pub-id><pub-id pub-id-type="pmid">34321205</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abf9616</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michelmann</surname><given-names>S</given-names></name><name><surname>Staresina</surname><given-names>BP</given-names></name><name><surname>Bowman</surname><given-names>H</given-names></name><name><surname>Hanslmayr</surname><given-names>S</given-names></name></person-group><article-title>Speed of time-compressed forward replay flexibly changes in human episodic memory</article-title><source>Nat Hum Behav</source><year>2019</year><volume>3</volume><fpage>143</fpage><lpage>154</lpage><pub-id pub-id-type="pmid">30944439</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Donnarumma</surname><given-names>F</given-names></name><name><surname>Maisto</surname><given-names>D</given-names></name><name><surname>Stoianov</surname><given-names>I</given-names></name></person-group><article-title>Planning at decision time and in the background during spatial navigation</article-title><source>Curr Opin Beh Sci</source><year>2019</year><volume>29</volume><fpage>69</fpage><lpage>76</lpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Goals and habits in the brain</article-title><source>Neuron</source><year>2013</year><volume>80</volume><fpage>312</fpage><lpage>325</lpage><pub-id pub-id-type="pmcid">PMC3807793</pub-id><pub-id pub-id-type="pmid">24139036</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.007</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pezzulo</surname><given-names>G</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name></person-group><article-title>Internally generated hippocampal sequences as a vantage point to probe future-oriented cognition</article-title><source>Ann N Y Acad Sci</source><year>2017</year><volume>1396</volume><fpage>144</fpage><lpage>165</lpage><pub-id pub-id-type="pmid">28548460</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><article-title>The role of the hippocampus in prediction and imagination</article-title><source>Annu Rev Psychol</source><year>2010</year><volume>61</volume><issue>27-48</issue><fpage>C21</fpage><lpage>28</lpage><pub-id pub-id-type="pmid">19958178</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>GM</given-names></name><name><surname>Siegelmann</surname><given-names>HT</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Brain-inspired replay for continual learning with artificial neural networks</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><elocation-id>4069</elocation-id><pub-id pub-id-type="pmcid">PMC7426273</pub-id><pub-id pub-id-type="pmid">32792531</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17866-2</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><article-title>Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychol Rev</source><year>1995</year><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carr</surname><given-names>MF</given-names></name><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Hippocampal replay in the awake state: a potential substrate for memory consolidation and retrieval</article-title><source>Nat Neurosci</source><year>2011</year><volume>14</volume><fpage>147</fpage><lpage>153</lpage><pub-id pub-id-type="pmcid">PMC3215304</pub-id><pub-id pub-id-type="pmid">21270783</pub-id><pub-id pub-id-type="doi">10.1038/nn.2732</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name></person-group><article-title>What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated</article-title><source>Trends Cogn Sci</source><year>2016</year><volume>20</volume><fpage>512</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">27315762</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>GM</given-names></name><name><surname>Trouche</surname><given-names>S</given-names></name><name><surname>McNamara</surname><given-names>CG</given-names></name><name><surname>Allen</surname><given-names>K</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name></person-group><article-title>Hippocampal Offline Reactivation Consolidates Recently Formed Cell Assembly Patterns during Sharp Wave-Ripples</article-title><source>Neuron</source><year>2016</year><volume>92</volume><fpage>968</fpage><lpage>974</lpage><pub-id pub-id-type="pmcid">PMC5158132</pub-id><pub-id pub-id-type="pmid">27840002</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.020</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamee</surname><given-names>DM</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Flexible modulation of sequence generation in the entorhinal-hippocampal system</article-title><source>Nat Neurosci</source><year>2021</year><pub-id pub-id-type="pmcid">PMC7610914</pub-id><pub-id pub-id-type="pmid">33846626</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-00831-7</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Reactivation of hippocampal ensemble memories during sleep</article-title><source>Science</source><year>1994</year><volume>265</volume><fpage>676</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">8036517</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olafsdottir</surname><given-names>HF</given-names></name><name><surname>Carpenter</surname><given-names>F</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name></person-group><article-title>Task Demands Predict a Dynamic Switch in the Content of Awake Hippocampal Replay</article-title><source>Neuron</source><year>2017</year><volume>96</volume><fpage>925</fpage><lpage>935</lpage><elocation-id>e926</elocation-id><pub-id pub-id-type="pmcid">PMC5697915</pub-id><pub-id pub-id-type="pmid">29056296</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.035</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papale</surname><given-names>AE</given-names></name><name><surname>Zielinski</surname><given-names>MC</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Interplay between Hippocampal Sharp-Wave-Ripple Events and Vicarious Trial and Error Behaviors in Decision Making</article-title><source>Neuron</source><year>2016</year><volume>92</volume><fpage>975</fpage><lpage>982</lpage><pub-id pub-id-type="pmcid">PMC5145752</pub-id><pub-id pub-id-type="pmid">27866796</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.028</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>AS</given-names></name><name><surname>van der Meer</surname><given-names>MA</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Hippocampal replay is not a simple function of experience</article-title><source>Neuron</source><year>2010</year><volume>65</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="pmcid">PMC4460981</pub-id><pub-id pub-id-type="pmid">20223204</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.034</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>A</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Neural ensembles in CA3 transiently encode paths forward of the animal at a decision point</article-title><source>J Neurosci</source><year>2007</year><volume>27</volume><fpage>12176</fpage><lpage>12189</lpage><pub-id pub-id-type="pmcid">PMC6673267</pub-id><pub-id pub-id-type="pmid">17989284</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3761-07.2007</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrose</surname><given-names>RE</given-names></name><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><article-title>Reverse Replay of Hippocampal Place Cells Is Uniquely Modulated by Changing Reward</article-title><source>Neuron</source><year>2016</year><volume>91</volume><fpage>1124</fpage><lpage>1136</lpage><pub-id pub-id-type="pmcid">PMC6013068</pub-id><pub-id pub-id-type="pmid">27568518</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.07.047</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfeiffer</surname><given-names>BE</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name></person-group><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><year>2013</year><volume>497</volume><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="pmcid">PMC3990408</pub-id><pub-id pub-id-type="pmid">23594744</pub-id><pub-id pub-id-type="doi">10.1038/nature12112</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olafsdottir</surname><given-names>HF</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Saleem</surname><given-names>AB</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>Hippocampal place cells construct reward related sequences through unexplored space</article-title><source>Elife</source><year>2015</year><volume>4</volume><elocation-id>e06063</elocation-id><pub-id pub-id-type="pmcid">PMC4479790</pub-id><pub-id pub-id-type="pmid">26112828</pub-id><pub-id pub-id-type="doi">10.7554/eLife.06063</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>JD</given-names></name><name><surname>Tang</surname><given-names>W</given-names></name><name><surname>Jadhav</surname><given-names>SP</given-names></name></person-group><article-title>Dynamics of Awake Hippocampal-Prefrontal Replay for Spatial Learning and Memory-Guided Decision Making</article-title><source>Neuron</source><year>2019</year><volume>104</volume><fpage>1110</fpage><lpage>1125</lpage><elocation-id>e1117</elocation-id><pub-id pub-id-type="pmcid">PMC6923537</pub-id><pub-id pub-id-type="pmid">31677957</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.012</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattar</surname><given-names>MG</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Prioritized memory access explains planning and hippocampal replay</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1609</fpage><lpage>1617</lpage><pub-id pub-id-type="pmcid">PMC6203620</pub-id><pub-id pub-id-type="pmid">30349103</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0232-z</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Baracskay</surname><given-names>P</given-names></name><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><article-title>Assembly Responses of Hippocampal CA1 Place Cells Predict Learned Behavior in Goal-Directed Spatial Tasks on the Radial Eight-Arm Maze</article-title><source>Neuron</source><year>2019</year><volume>101</volume><fpage>119</fpage><lpage>132</lpage><elocation-id>e114</elocation-id><pub-id pub-id-type="pmid">30503645</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>CT</given-names></name><name><surname>Haggerty</surname><given-names>D</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>Ji</surname><given-names>D</given-names></name></person-group><article-title>Hippocampal awake replay in fear memory retrieval</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>571</fpage><lpage>580</lpage><pub-id pub-id-type="pmcid">PMC5373994</pub-id><pub-id pub-id-type="pmid">28218916</pub-id><pub-id pub-id-type="doi">10.1038/nn.4507</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>AC</given-names></name><name><surname>Carr</surname><given-names>MF</given-names></name><name><surname>Karlsson</surname><given-names>MP</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Hippocampal SWR activity predicts correct decisions during the initial learning of an alternation task</article-title><source>Neuron</source><year>2013</year><volume>77</volume><fpage>1163</fpage><lpage>1173</lpage><pub-id pub-id-type="pmcid">PMC3751175</pub-id><pub-id pub-id-type="pmid">23522050</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.027</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>AK</given-names></name><etal/></person-group><article-title>Hippocampal replay reflects specific past experiences rather than a plan for subsequent choice</article-title><source>Neuron</source><year>2021</year><comment>10.1016/j.neuron.2021.07.029</comment><pub-id pub-id-type="pmcid">PMC8497431</pub-id><pub-id pub-id-type="pmid">34450026</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.029</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carey</surname><given-names>AA</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name><name><surname>van der Meer</surname><given-names>MAA</given-names></name></person-group><article-title>Reward revaluation biases hippocampal replay content away from the preferred outcome</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>1450</fpage><lpage>1459</lpage><pub-id pub-id-type="pmid">31427771</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vikbladh</surname><given-names>OM</given-names></name><etal/></person-group><article-title>Hippocampal contributions to model-based planning and spatial memory</article-title><source>Neuron</source><year>2019</year><volume>102</volume><fpage>683</fpage><lpage>693</lpage><elocation-id>e684</elocation-id><pub-id pub-id-type="pmcid">PMC6508991</pub-id><pub-id pub-id-type="pmid">30871859</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.02.014</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Dorsal hippocampus contributes to model-based planning</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>1269</fpage><lpage>1276</lpage><pub-id pub-id-type="pmcid">PMC5575950</pub-id><pub-id pub-id-type="pmid">28758995</pub-id><pub-id pub-id-type="doi">10.1038/nn.4613</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><article-title>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions</article-title><source>Psychol Rev</source><year>1990</year><volume>97</volume><fpage>285</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">2186426</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCloskey</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><article-title>Catastrophic interference in connectionist networks: the sequential learning problem</article-title><source>Psychol Learn Motiv</source><year>1989</year><volume>24</volume><fpage>109</fpage><lpage>165</lpage></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadhav</surname><given-names>SP</given-names></name><name><surname>Kemere</surname><given-names>C</given-names></name><name><surname>German</surname><given-names>PW</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Awake hippocampal sharp-wave ripples support spatial memory</article-title><source>Science</source><year>2012</year><volume>336</volume><fpage>1454</fpage><lpage>1458</lpage><pub-id pub-id-type="pmcid">PMC4441285</pub-id><pub-id pub-id-type="pmid">22555434</pub-id><pub-id pub-id-type="doi">10.1126/science.1217230</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girardeau</surname><given-names>G</given-names></name><name><surname>Benchenane</surname><given-names>K</given-names></name><name><surname>Wiener</surname><given-names>SI</given-names></name><name><surname>Buzsaki</surname><given-names>G</given-names></name><name><surname>Zugaro</surname><given-names>MB</given-names></name></person-group><article-title>Selective suppression of hippocampal ripples impairs spatial memory</article-title><source>Nat Neurosci</source><year>2009</year><volume>12</volume><fpage>1222</fpage><lpage>1223</lpage><pub-id pub-id-type="pmid">19749750</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ego-Stengel</surname><given-names>V</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>Disruption of ripple-associated hippocampal activity during rest impairs spatial learning in the rat</article-title><source>Hippocampus</source><year>2010</year><volume>20</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC2801761</pub-id><pub-id pub-id-type="pmid">19816984</pub-id><pub-id pub-id-type="doi">10.1002/hipo.20707</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernandez-Ruiz</surname><given-names>A</given-names></name><etal/></person-group><article-title>Long-duration hippocampal sharp wave ripples improve memory</article-title><source>Science</source><year>2019</year><volume>364</volume><fpage>1082</fpage><lpage>1086</lpage><pub-id pub-id-type="pmcid">PMC6693581</pub-id><pub-id pub-id-type="pmid">31197012</pub-id><pub-id pub-id-type="doi">10.1126/science.aax0758</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>Sequential replay of nonspatial task states in the human hippocampus</article-title><source>Science</source><year>2019</year><volume>364</volume><pub-id pub-id-type="pmcid">PMC7241311</pub-id><pub-id pub-id-type="pmid">31249030</pub-id><pub-id pub-id-type="doi">10.1126/science.aaw5181</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>McDevitt</surname><given-names>EA</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name><name><surname>Mednick</surname><given-names>SC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><article-title>Human hippocampal replay during rest prioritizes weakly learned information and predicts memory performance</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><elocation-id>3920</elocation-id><pub-id pub-id-type="pmcid">PMC6156217</pub-id><pub-id pub-id-type="pmid">30254219</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-06213-1</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doll</surname><given-names>BB</given-names></name><name><surname>Duncan</surname><given-names>KD</given-names></name><name><surname>Simon</surname><given-names>DA</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Model-based choices involve prospective neural activity</article-title><source>Nat Neurosci</source><year>2015</year><volume>18</volume><fpage>767</fpage><lpage>772</lpage><pub-id pub-id-type="pmcid">PMC4414826</pub-id><pub-id pub-id-type="pmid">25799041</pub-id><pub-id pub-id-type="doi">10.1038/nn.3981</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Cushman</surname><given-names>FA</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>When Does Model-Based Control Pay Off?</article-title><source>PLoS Comput Biol</source><year>2016</year><volume>12</volume><elocation-id>e1005090</elocation-id><pub-id pub-id-type="pmcid">PMC5001643</pub-id><pub-id pub-id-type="pmid">27564094</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005090</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Model-based influences on humans’ choices and striatal prediction errors</article-title><source>Neuron</source><year>2011</year><volume>69</volume><fpage>1204</fpage><lpage>1215</lpage><pub-id pub-id-type="pmcid">PMC3077926</pub-id><pub-id pub-id-type="pmid">21435563</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Temporally delayed linear modelling (TDLM) measures replay in both animals and humans</article-title><source>Elife</source><year>2021</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC8318595</pub-id><pub-id pub-id-type="pmid">34096501</pub-id><pub-id pub-id-type="doi">10.7554/eLife.66917</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Cushman</surname><given-names>FA</given-names></name></person-group><article-title>Cost-Benefit Arbitration Between Multiple Reinforcement-Learning Systems</article-title><source>Psychol Sci</source><year>2017</year><volume>28</volume><fpage>1321</fpage><lpage>1333</lpage><pub-id pub-id-type="pmid">28731839</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patzelt</surname><given-names>EH</given-names></name><name><surname>Kool</surname><given-names>W</given-names></name><name><surname>Millner</surname><given-names>AJ</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Incentives Boost Model-Based Control Across a Range of Severity on Several Psychiatric Constructs</article-title><source>Biol Psychiatry</source><year>2019</year><volume>85</volume><fpage>425</fpage><lpage>433</lpage><pub-id pub-id-type="pmcid">PMC6314918</pub-id><pub-id pub-id-type="pmid">30077331</pub-id><pub-id pub-id-type="doi">10.1016/j.biopsych.2018.06.018</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konovalov</surname><given-names>A</given-names></name><name><surname>Krajbich</surname><given-names>I</given-names></name></person-group><article-title>Gaze data reveal distinct choice processes underlying model-based and model-free reinforcement learning</article-title><source>Nat Commun</source><year>2016</year><volume>7</volume><elocation-id>12438</elocation-id><pub-id pub-id-type="pmcid">PMC4987535</pub-id><pub-id pub-id-type="pmid">27511383</pub-id><pub-id pub-id-type="doi">10.1038/ncomms12438</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><article-title>Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses</article-title><source>Soc Psychol Personal Sci</source><year>2017</year><volume>8</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="pmcid">PMC5502906</pub-id><pub-id pub-id-type="pmid">28736600</pub-id><pub-id pub-id-type="doi">10.1177/1948550617697177</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidson</surname><given-names>TJ</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>Hippocampal replay of extended experience</article-title><source>Neuron</source><year>2009</year><volume>63</volume><fpage>497</fpage><lpage>507</lpage><pub-id pub-id-type="pmcid">PMC4364032</pub-id><pub-id pub-id-type="pmid">19709631</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.027</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Shohamy</surname><given-names>D</given-names></name></person-group><article-title>Generalization of value in reinforcement learning by humans</article-title><source>Eur J Neurosci</source><year>2012</year><volume>35</volume><fpage>1092</fpage><lpage>1104</lpage><pub-id pub-id-type="pmcid">PMC3404618</pub-id><pub-id pub-id-type="pmid">22487039</pub-id><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08017.x</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebreton</surname><given-names>M</given-names></name><name><surname>Jorge</surname><given-names>S</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Pessiglione</surname><given-names>M</given-names></name></person-group><article-title>An automatic valuation system in the human brain: evidence from functional neuroimaging</article-title><source>Neuron</source><year>2009</year><volume>64</volume><fpage>431</fpage><lpage>439</lpage><pub-id pub-id-type="pmid">19914190</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>JJ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Tracking the emergence of conceptual knowledge during human decision making</article-title><source>Neuron</source><year>2009</year><volume>63</volume><fpage>889</fpage><lpage>901</lpage><pub-id pub-id-type="pmcid">PMC2791172</pub-id><pub-id pub-id-type="pmid">19778516</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.030</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-Persem</surname><given-names>A</given-names></name><etal/></person-group><article-title>Four core properties of the human brain valuation system demonstrated in intracranial signals</article-title><source>Nat Neurosci</source><year>2020</year><volume>23</volume><fpage>664</fpage><lpage>675</lpage><pub-id pub-id-type="pmid">32284605</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>C</given-names></name><etal/></person-group><article-title>Replay bursts in humans coincide with activation of the default mode and parietal alpha networks</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>882</fpage><lpage>893</lpage><elocation-id>e887</elocation-id><pub-id pub-id-type="pmcid">PMC7927915</pub-id><pub-id pub-id-type="pmid">33357412</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.007</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><article-title>Hippocampal neurons represent events as transferable units of experience</article-title><source>Nat Neurosci</source><year>2020</year><volume>23</volume><fpage>651</fpage><lpage>663</lpage><pub-id pub-id-type="pmid">32251386</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diedrichsen</surname><given-names>J</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational models: A common framework for understanding encoding, pattern-component, and representational-similarity analysis</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><elocation-id>e1005508</elocation-id><pub-id pub-id-type="pmcid">PMC5421820</pub-id><pub-id pub-id-type="pmid">28437426</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005508</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuker</surname><given-names>L</given-names></name><name><surname>Bellmund</surname><given-names>JL</given-names></name><name><surname>Schroder</surname><given-names>T Navarro</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>An event map of memory space in the hippocampus</article-title><source>Elife</source><year>2016</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC5053807</pub-id><pub-id pub-id-type="pmid">27710766</pub-id><pub-id pub-id-type="doi">10.7554/eLife.16534</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Joo</surname><given-names>HR</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Dorsal and Ventral Hippocampal Sharp-Wave Ripples Activate Distinct Nucleus Accumbens Networks</article-title><source>Neuron</source><year>2020</year><volume>105</volume><fpage>725</fpage><lpage>741</lpage><elocation-id>e728</elocation-id><pub-id pub-id-type="pmcid">PMC7035181</pub-id><pub-id pub-id-type="pmid">31864947</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.11.022</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomperts</surname><given-names>SN</given-names></name><name><surname>Kloosterman</surname><given-names>F</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>VTA neurons coordinate with the hippocampal reactivation of spatial experience</article-title><source>Elife</source><year>2015</year><volume>4</volume><pub-id pub-id-type="pmcid">PMC4695386</pub-id><pub-id pub-id-type="pmid">26465113</pub-id><pub-id pub-id-type="doi">10.7554/eLife.05360</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feher da Silva</surname><given-names>C</given-names></name><name><surname>Hare</surname><given-names>TA</given-names></name></person-group><article-title>Humans primarily use model-based inference in the two-stage task</article-title><source>Nat Hum Behav</source><year>2020</year><pub-id pub-id-type="pmid">32632333</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kali</surname><given-names>S</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions</article-title><source>Nat Neurosci</source><year>2004</year><volume>7</volume><fpage>286</fpage><lpage>294</lpage><pub-id pub-id-type="pmid">14983183</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Markman</surname><given-names>AB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>The curse of planning: dissecting multiple reinforcement-learning systems by taxing the central executive</article-title><source>Psychol Sci</source><year>2013</year><volume>24</volume><fpage>751</fpage><lpage>761</lpage><pub-id pub-id-type="pmcid">PMC3843765</pub-id><pub-id pub-id-type="pmid">23558545</pub-id><pub-id pub-id-type="doi">10.1177/0956797612463080</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><etal/></person-group><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brewin</surname><given-names>CR</given-names></name><name><surname>Gregory</surname><given-names>JD</given-names></name><name><surname>Lipton</surname><given-names>M</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Intrusive images in psychological disorders: characteristics, neural mechanisms, and treatment implications</article-title><source>Psychol Rev</source><year>2010</year><volume>117</volume><fpage>210</fpage><lpage>232</lpage><pub-id pub-id-type="pmcid">PMC2834572</pub-id><pub-id pub-id-type="pmid">20063969</pub-id><pub-id pub-id-type="doi">10.1037/a0018113</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suh</surname><given-names>J</given-names></name><name><surname>Foster</surname><given-names>DJ</given-names></name><name><surname>Davoudi</surname><given-names>H</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><article-title>Impaired hippocampal ripple-associated replay in a mouse model of schizophrenia</article-title><source>Neuron</source><year>2013</year><volume>80</volume><fpage>484</fpage><lpage>493</lpage><pub-id pub-id-type="pmcid">PMC3871857</pub-id><pub-id pub-id-type="pmid">24139046</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.014</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname><given-names>QJM</given-names></name><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Paulus</surname><given-names>MP</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><article-title>Advances in the computational understanding of mental illness</article-title><source>Neuropsychopharmacology</source><year>2021</year><volume>46</volume><fpage>3</fpage><lpage>19</lpage><pub-id pub-id-type="pmcid">PMC7688938</pub-id><pub-id pub-id-type="pmid">32620005</pub-id><pub-id pub-id-type="doi">10.1038/s41386-020-0746-4</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>EA</given-names></name><name><surname>Gillespie</surname><given-names>AK</given-names></name><name><surname>Yoon</surname><given-names>SY</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name></person-group><article-title>Early Hippocampal Sharp-Wave Ripple Deficits Predict Later Learning and Memory Impairments in an Alzheimer’s Disease Mouse Model</article-title><source>Cell Rep</source><year>2019</year><volume>29</volume><fpage>2123</fpage><lpage>2133</lpage><elocation-id>e2124</elocation-id><pub-id pub-id-type="pmcid">PMC7437815</pub-id><pub-id pub-id-type="pmid">31747587</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2019.10.056</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prince</surname><given-names>SM</given-names></name><etal/></person-group><article-title>Alzheimer’s pathology causes impaired inhibitory connections and reactivation of spatial codes during spatial navigation</article-title><source>Cell Rep</source><year>2021</year><volume>35</volume><elocation-id>109008</elocation-id><pub-id pub-id-type="pmcid">PMC8139125</pub-id><pub-id pub-id-type="pmid">33882308</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109008</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillan</surname><given-names>CM</given-names></name><name><surname>Otto</surname><given-names>AR</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Model-based learning protects against forming habits</article-title><source>Cogn Affect Behav Neurosci</source><year>2015</year><pub-id pub-id-type="pmcid">PMC4526597</pub-id><pub-id pub-id-type="pmid">25801925</pub-id><pub-id pub-id-type="doi">10.3758/s13415-015-0347-6</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AG</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><article-title>How much of reinforcement learning is working memory, not reinforcement learning? A behavioral, computational, and neurogenetic analysis</article-title><source>Eur J Neurosci</source><year>2012</year><volume>35</volume><fpage>1024</fpage><lpage>1035</lpage><pub-id pub-id-type="pmcid">PMC3390186</pub-id><pub-id pub-id-type="pmid">22487033</pub-id><pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07980.x</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname><given-names>GE</given-names></name><name><surname>Li</surname><given-names>JK</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><article-title>Reward learning over weeks versus minutes increases the neural representation of value in the human brain</article-title><source>J Neurosci</source><year>2018</year><volume>38</volume><fpage>7649</fpage><lpage>7666</lpage><pub-id pub-id-type="pmcid">PMC6113901</pub-id><pub-id pub-id-type="pmid">30061189</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0075-18.2018</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname><given-names>AM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Cortical and hippocampal correlates of deliberation during model-based decisions for rewards in humans</article-title><source>PLoS Comput Biol</source><year>2013</year><volume>9</volume><elocation-id>e1003387</elocation-id><pub-id pub-id-type="pmcid">PMC3854511</pub-id><pub-id pub-id-type="pmid">24339770</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003387</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance statement</title></caption><p>The sequential neural reactivation of prior experience, known as replay, is considered to be an important mechanism for both future planning and preserving memories of the past. Whether, and how, replay supports both of these functions remains unknown. Here, in humans, we found that prior to a choice, rapid replay of potential future paths was enhanced when planning was more beneficial. By contrast, after choice feedback, when no future actions are imminent, we found evidence for a memory preservation signal evident in enhanced replay of paths that had been visited less in the recent past. The results demonstrate that distinct replay signatures, expressed at different times, relate to two dissociable cognitive functions.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Two-environment reward learning task and generalization behavior.</title><p>(<bold>a</bold>) Task schematic showing two alternative ‘worlds’ and their two equivalent start states. Trials in each world start at one of two equivalent shape pair options; to illustrate these connections, arrows from different states differ in color saturation. The shape options then lead deterministically to the same paths and reward outcomes (0-9 points (pts)). To learn this general structure participants engaged in a pre-scanning training session. For the MEG scanning session, participants then learned two worlds populated with new images. Participants’ memory for the path sequences was at asymptote after an initial no-reward exploration period. (<bold>b</bold>) Key trial periods in the reward learning phase. Replay was measured prior to choice in a time window we subsequently refer to as the ‘planning period’. After the disappearance of a central cross, participants entered their response. Participants then sequentially viewed the state images corresponding to the chosen path. Finally, participants received reward feedback (0-9 points), the amount of which drifted across trials. Replay was again measured in the post-feedback time window. For interpretation of subsequent results, in this example World 1 is the ‘current world’ while World 2 is the non-presented ‘other world’. (See also <xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref>.) (<bold>c</bold>) Example trial sequence, highlighting two cases where a trial either has a different start state or the same start state as the previous trial in the same world. (<bold>d</bold>) Illustration of the dependence of repeated choices (stay) on previous rewards, conditional on whether the start state in the current world was the same as in the previous trial or not. The plot depicts the probability of a stay choice (when participants repeated a previous path selection in a given world) following above-average (high) versus below-average (low) reward. This difference was equivalent for same (purple) versus different (orange) starting states, consistent with behavior being model-based (‘n.s.’ represents non-significant effects in a regression model using continuous reward data). For display purposes, graded point feedback was binarized into high and low and trials with near-mean feedback were excluded; alternative procedures yield the same qualitative results. Grey dots represent individual participants. (See also <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>.)</p></caption><graphic xlink:href="EMS154990-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Training of state localizer and sequenceness time lag identification.</title><p>See also <xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref>. (<bold>a</bold>) Classifier performance for path state stimuli presented during a pre-task localizer phase, training and testing at all time points. This revealed good discrimination between the 12 path stimuli used in the learning task. Color bar indicates predicted probability. Note that start state shape stimuli were not included in the pre-task localizer and are not included in sequenceness analyses. (<bold>b</bold>) Peak classifier performance from 140-210 ms after stimulus onset in the localizer phase (depicting the diagonal extracted from panel a). (<bold>c</bold>) Forward sequenceness for all learned paths during planning and feedback periods was evident at a common state-to-state lag of 70 ms in both trial periods. Open dots indicate time points exceeding a permutation significance threshold, which differs for the two periods. (<bold>d</bold>) Backward sequenceness for all learned paths during planning and feedback periods was evident at state-to-state lags that spanned 10-50 ms in feedback period alone. Note that the x-axis in the sequenceness panels indicates the lag between reactivations, derived as a summary measure across seconds; the axis does not represent time within a trial period. Open dots indicate time points exceeding a permutation significance threshold, which differs for the two periods. Shaded error margins represent SEM. See <xref ref-type="supplementary-material" rid="SD1">Fig. S6</xref> for example sequenceness events and <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref> for extended time lags.</p></caption><graphic xlink:href="EMS154990-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Planning period replay increases and the benefit of model-based generalization.</title><p>(<bold>a</bold>) Stronger forward replay on trials where the start state is different from the previous trial, where there is a greater benefit in utilizing model-based knowledge (<bold>b</bold>) Timecourse of regression coefficients for variables of interest, showing effects at state-to-state lags from 10 to 130 ms. The light blue line highlights the 70 ms time lag of interest shown in panel a. Y-axes represent sequenceness regression coefficients for binary different versus same start state. See <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref> for extended time lags. Seq = sequenceness. ** p&lt;0.01; *** p&lt;0.001; + p&lt;0.01, corrected for multiple comparisons.</p></caption><graphic xlink:href="EMS154990-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Feedback period backward replay increases with the rarity of recent other world experience.</title><p>(<bold>a</bold>) Rarity (lower recent experience) of the other world correlated with greater backward replay of other world paths. (<bold>b</bold>) Timecourse of regression coefficients for the rarity effect of interest, showing effects at state-to-state lags from 10 to 130 ms. The light blue line highlights the 40 time lag of interest shown in panel a. Y-axes represent sequenceness regression coefficients for rarity of the other world. See <xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref> for extended time lags. Seq = sequenceness; * p &lt; 0.05. (<bold>c</bold>) Across-participant relationship between the replay-rarity effect and lower planning period forward replay (world change trials; p = 0.009).</p></caption><graphic xlink:href="EMS154990-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Exploratory replay onset beamforming analyses.</title><p>(<bold>a</bold>) In the planning period, beamforming analyses revealed power increases associated with replay onset in the right MTL, including the hippocampus. (<bold>b</bold>) After reward feedback, power increases associated with replay onset were found in the bilateral MTL, including the hippocampus. See also <xref ref-type="supplementary-material" rid="SD1">Fig. S7 and Table S4</xref>. The y coordinate refers to the Montreal Neurological Institute (MNI) atlas. For display, statistical maps were thresholded at P &lt; 0.01 uncorrected; clusters significant at P &lt; 0.05, whole-brain corrected using nonparametric permutation tests. For unthresholded statistical maps and results within the hippocampus ROI mask, see <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/11163/">https://neurovault.org/collections/11163/</ext-link>.</p></caption><graphic xlink:href="EMS154990-f005"/></fig></floats-group></article>