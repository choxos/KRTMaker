<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS153436</article-id><article-id pub-id-type="doi">10.1101/2021.12.01.470868</article-id><article-id pub-id-type="archive">PPR427861</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="europepmc-category"><subject>Covid-19</subject></subj-group></article-categories><title-group><article-title>DeepLPI: a novel deep learning-based model for protein-ligand interaction prediction for drug repurposing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wei</surname><given-names>Bomin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Yue</given-names></name><email>zhang.yue@hsc.utah.edu</email><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Gong</surname><given-names>Xiang</given-names></name><email>xiang.gong@prismsus.org</email><xref ref-type="aff" rid="A1">1</xref></contrib><aff id="A1"><label>1</label>Princeton International School of Mathematics and Science, 19 Lambert Drive, Princeton, New Jersey, 08540 USA</aff><aff id="A2"><label>2</label>IDEAS Center, VA Salt Lake City Healthcare System, Salt Lake City, Utah, 84132 USA</aff><aff id="A3"><label>3</label>Department of Medicine, University of Utah, Salt Lake City, Utah, 84132 USA</aff><aff id="A4"><label>4</label>Division of Epidemiology, University of Utah, Salt Lake City, Utah, 84132 USA</aff></contrib-group><author-notes><corresp id="CR1"><label>*</label>Bomin Wei, <email>david.wbm.2022@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>30</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>28</day><month>08</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The substantial cost of new drug research and development has consistently posed a huge burden for both pharmaceutical companies and patients. In order to lower the expenditure and development failure rate, repurposing existing and approved drugs by identifying interactions between drug molecules and target proteins based on computational methods have gained growing attention. Here, we propose the DeepLPI, a novel deep learning-based model that mainly consists of ResNet-based 1-dimensional convolutional neural network (1D CNN) and bi-directional long short term memory network (biLSTM), to establish an end-to-end framework for protein-ligand interaction prediction. We first encode the raw drug molecular sequences and target protein sequences into dense vector representations, which go through two ResNet-based 1D CNN modules to derive features, respectively. The extracted feature vectors are concatenated and further fed into the biLSTM network, followed by the MLP module to finally predict protein-ligand interaction. We downloaded the well-known BindingDB and Davis dataset for training and testing our DeepLPI model. We also applied DeepLPI on a COVID-19 dataset for externally evaluating the prediction ability of DeepLPI. To benchmark our model, we compared our DeepLPI with the state-of-the-art methods of DeepCDA and DeepDTA, and observed that our DeepLPI outperformed these methods, suggesting the high accuracy of the DeepLPI towards protein-ligand interaction prediction. The high prediction performance of DeepLPI on the different datasets displayed its high capability of protein-ligand interaction in generalization, demonstrating that the DeepLPI has the potential to pinpoint new drug-target interactions and to find better destinations for proven drugs.</p></abstract><kwd-group><kwd>drug repurposing</kwd><kwd>deep learning</kwd><kwd>protein-ligand binding interaction prediction</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Introducing a new drug to the market has been characterized to be risky, time-consuming, and costly[<xref ref-type="bibr" rid="R1">1</xref>][<xref ref-type="bibr" rid="R2">2</xref>]. Drug discovery is the first phase of drug research and development (R&amp;D) that starts with identifying targets of an unmet disease such as proteins, followed by creating and optimizing a promising compound that can interact with the targets efficiently and safely. This step usually involves hundreds and thousands of compounds, yet only about 8% of which as drug leads can enter the phase of the <italic>in vitro</italic> and <italic>in vivo</italic> preclinical research [<xref ref-type="bibr" rid="R3">3</xref>]. To shorten the duration and improve the success rate in the phase of drug discovery, drug repurposing has become a hotspot of new drug research and development over the past few years [<xref ref-type="bibr" rid="R1">1</xref>][<xref ref-type="bibr" rid="R4">4</xref>], which intends to find an effective cure for a disease from a large amount of existing and approved drugs that were developed for other purposes [<xref ref-type="bibr" rid="R1">1</xref>]. For example, prednisone was originally developed for the treatment of inflammatory diseases, but it is likely to be effective against Parkinson’s disease as well [<xref ref-type="bibr" rid="R5">5</xref>]. In midst of all the drug repurposing methods, <italic>in silico</italic> computational-based methods to screen pharmaceutical compound libraries and identify drug-target interactions (DTIs) or protein-ligand interactions (PLIs) have gained increasing attention and made significant breakthroughs due to the development in high performance computational architectures and advances in machine learning methods.</p><p id="P3">Over the last decade, a variety of machine learning-based models have been developed to identify PLIs from millions of ligands and proteins. One type of models utilized 3D structures of proteins and drug molecules aiming at capturing interaction details in predictions of the drug-target binding affinity[<xref ref-type="bibr" rid="R6">6</xref>], such as Atomnet[<xref ref-type="bibr" rid="R7">7</xref>] and SE-OnionNet[<xref ref-type="bibr" rid="R8">8</xref>], but insufficient 3D protein structure data limited the practicability, generalizability, and accuracy [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. To exploit the vastly available protein sequencing data, a new type of model calculates human-selected features and predicts drug-target interactions with conventional machine learning [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>]. The disadvantages of these methods are that they not only require much domain knowledge but also possibly lead to a loss of the information about raw protein-ligand interactions due to limited features. Deep learning-based models can automatically learn highly complex and abstract level of features from large-scale datasets without extensive manual creation. Yet the recent development considers only simple encoding of input letter information [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>]. Without the contextual information, this type of model may not capture the complex protein features and thus have limited accuracy and generalizability.</p><p id="P4">Here, we propose DeepLPI, an innovative deep learning-based model to predict protein-ligand interaction using the simple formats of raw protein 1D sequences and 1D ligands (i.e., drug molecular) SMILES (<bold>S</bold>implified <bold>M</bold>olecular <bold>I</bold>nput <bold>L</bold>ine <bold>E</bold>ntry <bold>S</bold>ystem) strings as inputs, rather than manual-generated features or complex 3D protein structures. To capture contextual information in the sequence data, we first respectively employ Natural Language Processing-inspired, pre-trained models of Mol2Vec[<xref ref-type="bibr" rid="R17">17</xref>] and ProSE[<xref ref-type="bibr" rid="R18">18</xref>] to embed drug SMILES strings and protein FASTA sequences as numeric vectors. These embedded numeric vectors are then fed into two blocks, each of them consisting of two modules termed head convolutional module and ResNet-based convolutional neural network (CNN) module, to encode proteins and drug sequences, respectively. The encoded representations are concatenated into a vector and further fed into a bi-directional long short-term memory (biLSTM) layer, followed by three fully connected layers. We download the BindingDB dataset [<xref ref-type="bibr" rid="R19">19</xref>] and Davis [<xref ref-type="bibr" rid="R20">20</xref>] dataset to train the DeepLPI model and adjusted the hyperparameters and internally independently evaluate its performance towards PLI prediction. We further transformed the model on a COVID-19 3CL Protease [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>] dataset for externally assessing the prediction ability of DeepLPI. To benchmark our model, we compared DeepLPI with the start-of-the-art methods of DeepDTA [<xref ref-type="bibr" rid="R15">15</xref>] and DeepCDA [<xref ref-type="bibr" rid="R16">16</xref>] towards protein-ligand interaction. The prediction performance is quantitively represented in terms of area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive (PPV), predictive value, and negative predictive value (NPV). The high performance of our DeepLPI towards protein-ligand interaction prediction suggests that our model has the potential to accurately identify protein-ligand interaction and hence, promote the new drug development.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Dataset and data preprocessing</title><p id="P5">We use the BindingDB[<xref ref-type="bibr" rid="R19">19</xref>] and Davis [<xref ref-type="bibr" rid="R20">20</xref>] datasets to train and evaluate our DeepLPI model. We also use the COVID-19 3C-like Protease dataset from Diamond Light Source [<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>] for further assessment. All datasets are publicly accessible. The BindingDB is a continually updating database that contains 2,278,226 experimentally identified binding affinities between 8,005 target proteins and 986,143 small drug molecules up to July 29, 2021. We first apply the following criteria to compile the dataset for the development of our model (<xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref>): (1) excluding binding interactions with multichain protein complexes because it is not capable of identifying which chain of the protein interacts with the molecular; (2) retaining binding interactions only represented by <italic>K<sub>d</sub></italic> value and it means that other measurements in the form of <italic>IC</italic>50 or <italic>K<sub>i</sub></italic> values are removed; (3) keeping common drug molecules and target proteins occurring in at least three and six interactions in the entire dataset [<xref ref-type="bibr" rid="R9">9</xref>], respectively; (4) removing data with invalid <italic>K<sub>d</sub></italic> values and removing duplicated data entries. For example, we notice that some data used “&gt;” and “&lt;” in the labeled values to indicate ranges, so directly exclude them for the subsequent analysis. Additionally, there are some zeros in the values which should not appear based on the definition of binding affinity measurement of <italic>K<sub>d</sub></italic>. Thus, we treat them as invalid values and simply removed them; As a result, a total of 36,111 interactions with 17,773 drug molecules and 1,915 protein targets are finally used in developing our model. (5) As a binary classification problem in this study, we use label 1 to represent a pair of protein and ligand being active if their corresponding <italic>K<sub>d</sub></italic> value is less than 100 nM and use label 0 to represent a pair being inactive if their <italic>K<sub>d</sub></italic> value is greater or equal to 100mN since a greater dissociation constant means weaker binding. In this case, 59.9% of data are labeled active, and 40.1% of data are labeled inactive (<xref ref-type="fig" rid="F1">Fig. 1a and 1b</xref>). The median (standard deviation, [minimum, maximum]) lengths of drug molecular SMILES strings and protein sequences are 52 (45.81, [1, 760]) and 445 (456.1, [9, 7096]) (<xref ref-type="fig" rid="F1">Fig. 1c and 1d</xref>).</p><p id="P6">We then randomly select 85% of the pre-processed BindingDB dataset as the training set and the remaining 15% as the internal independent testing set to train and evaluate our DeepLPI model. To optimize hyperparameters, we further allocate 10% of all data from the training set for validation during the training phase, and the rest are used as a training subset (i.e., 75% of all data). The AUROC and four classification metrics (sensitivity, specificity, PPV and NPV) are computed.</p><p id="P7">To characterize the model generalizability, different drugs and proteins were selected and reserved for the testing set (<xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). “Drug unseen” testing set consists of drugs not seen in the training set, “Protein unseen” testing set consists of proteins not seen in the training set, and “None seen” testing set consists of drugs and proteins neither seen in the training set.</p><p id="P8">The reason for choosing <italic>K<sub>d</sub></italic> rather than other binding measurements is for enabling our model performance to be reasonably compared on Davis dataset, which contains interactions of 442 unique proteins and 68 unique compounds. The Davis dataset only reports <italic>K<sub>d</sub></italic> values of the kinase protein family and the relevant inhibitors. We used the same protocol to obtain the class label as we did above. The Davis dataset was referenced from the Davis work [<xref ref-type="bibr" rid="R20">20</xref>] and downloaded from the URL therein. The dataset contained duplicated data entries where the drug-protein pairs are the same, but the binding affinity values are different, potentially due to the experiment conditions. We keep only one entry in each group of duplicates. This doesn’t affect the balance of the dataset because according to our binary threshold, all data entries in the same duplicate group in fact have the same binary label. After the treatment, there are 24,548 interaction data entries. We split them into training, validation, and testing sets according to the same method described above.</p><p id="P9">To find effective drugs for SARS-CoV-2, we applied our model on a COVID-19 dataset where 879 small molecule drugs were tested on the SARS-COV-2 3C-like protease. The experiment measured EC50 results. For classification, we label 1 to indicate drug-protease active if EC50 is less than 30 nM [<xref ref-type="bibr" rid="R23">23</xref>] or 0 representing inactivity. The data is retrieved from a large XChem crystallographic fragment screen against SARS-CoV-2 main protease at high resolution from MIT AiCures. [<xref ref-type="bibr" rid="R22">22</xref>] Among those data, 78 are active according to the threshold.</p></sec><sec id="S4"><title>Model Design</title><sec id="S5"><title>Overview of DeepLPI model</title><p id="P10">The proposed DeepLPI consists of eight modules (<xref ref-type="fig" rid="F2">Fig. 2</xref>), including two embedding modules, two head modules, two ResNet-based CNN modules, one bi-directional LSTM (biLSTM) module, and one multilayer perceptron module (MLP). DeepLPI employs raw molecular SMILES strings and protein sequences as inputs, which are first represented as numeric vectors using the pretrained models of Mol2Vec and ProSE, respectively. The embedded vectors for the drug SMILES and the protein sequences are then fed into the respective head module and ResNet-based CNN module to extract features. The feature vectors for the inputs of drug molecules and protein targets are concatenated, pooled (max-pooling operation), and then encoded by a biLSTM layer. Subsequently, the encoded vectors are finally fed into an MLP module, and the final output is activated through a sigmoid function for binary classification to predict active/inactive labels.</p><sec id="S6"><title>Embedding module</title><p id="P11">To utilize the raw drug molecular SMILES string and protein sequence as inputs to the DeepLPI model, we firstly encode them into numeric vector representations using the pre-trained embedding models Mol2Vec[<xref ref-type="bibr" rid="R18">18</xref>] and ProSE[<xref ref-type="bibr" rid="R19">19</xref>], respectively. Mol2Vec is an unsupervised deep learning-based approach to convert a molecule into a numeric vector representation. Inspired by natural language processing (NLP) techniques, Mol2Vec regards the molecular substructures obtained by the Morgan identifier [<xref ref-type="bibr" rid="R23">23</xref>] as “words” and the compound as “sentences”, and then encodes them into dense vector representations based on a so-called corpus of compounds. On the other hand, the ProSE is a deep learning-based method developed to represent protein sequences into numeric vectors that encode protein structural information. It first translates a protein sequence into a list of specific alphabets (as a “sentence”) which map similar amino acids (as “words”) into close numbers. Then, the ProSE model encodes the words into numeric vectors.</p><p id="P12">We utilize the pre-trained Mol2Vec (download link: <ext-link ext-link-type="uri" xlink:href="https://github.com/samoturk/mol2vec">https://github.com/samoturk/mol2vec</ext-link>) and ProSE (download link, <ext-link ext-link-type="uri" xlink:href="https://github.com/tbepler/prose">https://github.com/tbepler/prose</ext-link>) to obtain vector representations with a fixed length for the drug molecular compound and protein, respectively.</p></sec><sec id="S7"><title>Head module and ResNet-based CNN module</title><p id="P13">After the embedding, we separately feed the drug molecular SMILES string vector and protein sequence vector each into the head modules with the same network architecture. The head module contained the following layers: 1D convolutional, batch normalization, nonlinear transformation (with the rectified linear unit, i.e., ReLU activation), dropout, and max-pooling.</p><p id="P14">Subsequently, two ResNet-based CNN modules are connected to the corresponding head module to further encode the information of input. Suppose <italic>x</italic> is the input into a ResNet-based block, the output of stacked layers is called residual, denoted as <italic>F</italic>(<italic>x</italic>), we then calculated ResNet-based block output with equation <italic>H</italic>(<italic>x</italic>) = <italic>F</italic>(<italic>x</italic>) + <italic>x</italic> [<xref ref-type="bibr" rid="R24">24</xref>]. Similar to the head module, the two ResNet-based CNN modules had the same network architecture. Specifically, each ResNet-based CNN module consists of three consecutive ResNet-based blocks, and each block comprises two branches, where the right branch is known as “shortcut connection”; and the left branch is known as a residual network that contains several stacked layers, including a 1D convolutional layer, a batch normalization layer, a ReLU layer, a dropout layer, another 1D convolutional layer, and one more batch normalization layer in sequence.</p></sec><sec id="S8"><title>biLSTM module and MLP module</title><p id="P15">In the biLSTM module, we first concatenate the outputs of features extracted by the two ResNet-based CNN modules, following with an average-pooling layer. The biLSTM, which stands for <bold>bi</bold>directional <bold>l</bold>ong <bold>s</bold>hort-<bold>t</bold>erm memory, can learn long-term dependency from inputs. This network processes the input twice, once from starting to the end and once the reverse way and thus can balance the molecular information and the protein information. Finally, the outputs on both sides of biLSTM will be combined as the output vector.</p><p id="P16">In the MLP module, we flatten the output vector of biLSTM and fed it into three stacks of consecutive FC layers. Finally, the output is passed through a sigmoid function for binary classification to predict 1/0 labels.</p></sec></sec></sec><sec id="S9"><title>Loss function</title><p id="P17">We treat the prediction as a classification task, predicting whether the drug and protein has a strong binding or a weak binding. For the <italic>n</italic> pairs of molecular SMILES strings and protein sequences the loss function of the DeepLPI model was given by: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mtable><mml:mtr><mml:mtd><mml:mtext>Loss</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:mtext>log</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mrow><mml:mtext>BCE</mml:mtext><mml:mspace width="0.1em"/><mml:mtext>loss</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>α</mml:mi><mml:mo>∥</mml:mo><mml:mi>W</mml:mi><mml:msubsup><mml:mo>∥</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder><mml:mrow><mml:mtext>L2-norm</mml:mtext><mml:mspace width="0.1em"/><mml:mtext>regularization</mml:mtext></mml:mrow></mml:munder></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <italic>y<sub>i</sub></italic> ∈ {0,1} is class label representing whether or not binding interaction of a input pair of protein and ligand sequences <italic>i</italic>. <inline-formula><mml:math id="M2"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></inline-formula> is the probability of interaction prediction for the input pair <italic>i</italic> by our model, <inline-formula><mml:math id="M3"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></inline-formula>, <italic>x</italic> is the output of the MLP module of our model. <italic>W</italic> is the trainable weight matrix in our model. <italic>α</italic> is decay rate and we set it as 0.8 in this study.</p></sec><sec id="S10"><title>Evaluation metrics</title><p id="P18">We calculate five metrics including area under the receiver operating characteristic curve (AUROC), sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) to evaluate the performance of our model.</p><list list-type="order" id="L1"><list-item><p id="P19">Sensitivity: <disp-formula id="FD2"><mml:math id="M4"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where TP represents true positives, FP represents false positives, TN represents true negatives, FN represents false negatives.</p></list-item><list-item><p id="P20">Specificity: <disp-formula id="FD3"><mml:math id="M5"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p id="P21">Positive predictive value (PPV): <disp-formula id="FD4"><mml:math id="M6"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">P</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item><list-item><p id="P22">Negative predictive value (NPV): <disp-formula id="FD5"><mml:math id="M7"><mml:mtable><mml:mtr><mml:mtd><mml:mstyle displaystyle="true"><mml:mrow><mml:mi mathvariant="normal">N</mml:mi><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">V</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">N</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p></list-item></list></sec><sec id="S11"><title>Experiment setup</title><p id="P23">Model training was done in Aliyun Cloud Computing. The node CPU used Intel(R) Xeon(R) Platinum 8163 (2.50GHz). An Nvidia Tesla T4 GPU is supplied. The sources code is available on Github. The model is implemented using the PyTorch library (version 1.8.1). The source code of training and evaluating DeepLPI and the requirements are available on GitHub (<ext-link ext-link-type="uri" xlink:href="https://github.com/David-BominWei/DeepLPI">https://github.com/David-BominWei/DeepLPI</ext-link>).</p></sec><sec id="S12"><title>Parameters setting for training DeepLPI</title><p id="P24">We use Kaiming Initialization to initialize DeepLPI network weights [<xref ref-type="bibr" rid="R25">25</xref>]. The Adam optimizer[<xref ref-type="bibr" rid="R26">26</xref>] is also employed with default parameters of <italic>β</italic><sub>1</sub> = 0.9 and <italic>β</italic><sub>2</sub> = 0.999 as an optimization algorithm to train our model. Furthermore, we use a batch size of 256 and initialize the learning rate at 0.001 with a decay rate of 0.8 for every 10 epochs. The maximum number of epochs is 100 epochs for BindingDB training and 50 for Davis training. All settings for the parameters implemented in our DeepLPI model are demonstrated in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>. It should be noted that we use the default parameter values for the pre-trained Mol2Vec and ProSE, and we yield vector representations with a fixed length of 300 for the drug molecules, and two lengthes of 100 and 6,165 for the target proteins. The 6,165-element representation for protein were tested to outperform the 100-element representation, and thus in the article we report only the 6,165-element results. Generally, we manually tune and optimize the hyperparameters of the DeepLPI network and choose the number of blocks empirically in the ResNet-based module.</p></sec></sec><sec id="S13" sec-type="results"><title>Results</title><sec id="S14"><title>Training and evaluation results</title><sec id="S15"><title>Evaluation on BindingDB Dataset</title><p id="P25">We first report loss and metric progression during training on the training dataset from BindingDB database in <xref ref-type="fig" rid="F3">Figure 3</xref>. The model training is stopped after 40 epochs once the validation loss stopped decreasing. Training beyond this point would lead to apparent overfitting marked by increase of the validation loss. We calculated the AUROC metric values during the model training, which achieved 0.95 and 0.89 for the training and validation, respectively.</p><p id="P26">We applied the trained model on the internal independent testing set, achieving AUROC measured of 0.893. We used Youden’s J statistic to determine the optimal classification threshold instead of using default value of 0.5 (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). The optimal threshold was used for later calculations of confusion matrix metrics (<xref ref-type="fig" rid="F4">Fig. 4b-e</xref>) including the three testing sets “Molecule unseen” (<xref ref-type="fig" rid="F4">Fig. 4c</xref>), “Protein unseen” (<xref ref-type="fig" rid="F4">Fig. 4d</xref>) and “None seen” (<xref ref-type="fig" rid="F4">Fig. 4e</xref>) where the drugs and/or the proteins are of different types (“unseen”) from those in the training set.</p><p id="P27">We observed that the DeepLPI model obtained the highest AUROC of 0.857 when the training set had partial knowledge of the testing set. In addition, AUROC can reach 0.718 when the testing sets contains interaction pairs of molecules (<xref ref-type="fig" rid="F4">Fig. 4c</xref>) or protein (<xref ref-type="fig" rid="F4">Fig. 4d</xref>) that were “unseen” in the training set. Furthermore, AUROC can achieve 0.655 (<xref ref-type="fig" rid="F4">Fig. 4e</xref>) when the training set has no knowledge of the interaction pairs of molecule and protein that exist in the testing set.</p><p id="P28">In <xref ref-type="table" rid="T1">Table 1</xref>, we compare the performance metrics of our model with the recently published DeepCDA [<xref ref-type="bibr" rid="R16">16</xref>] model and the popular baseline model DeepDTA [<xref ref-type="bibr" rid="R15">15</xref>] on the independent testing set from the BindingDB data. The results demonstrated that DeepLPI scored higher AUROC by 0.011 and 0.004 than DeepCDA and DeepDTA, respectively. Given that all models’ AUROC are close to 0.9, it can be concluded that DeepLPI is able to predict on BindingDB dataset with very high accuracy. DeepLPI reached sensitivity of 0.832 and NPV of 0.875, which were higher than both DeepCDA and DeepDTA. The specificity and PPV are close to DeepCDA but lower than DeepDTA, due to higher false positive rates indicating the DeepLPI and DeepCDA tend to “over-bind” drug-target pairs. This might result from the more complex network structure with LSTM methods, which invites further investigations.</p><p id="P29">Overall prediction results on the combined “unseen” testing sets showed DeepLPI with AUROC of 0.790 is 76% better than the DeepCDA model. The DeepCDA with AUROC of 0.448 is non-predictive given that random prediction would lead to AUROC of 0.5, indicating DeepLPI has greater generalizability than the two models when applied on different kinds of drugs and proteins outside of the training domain.</p></sec><sec id="S16"><title>Evaluation on Davis dataset</title><p id="P30">Model training on Davis dataset is stopped after 40 epochs to avoid overfitting when the validation loss stopped decreasing. AUROC values during the model training achieved 0.99 and 0.91 on the training and validation sets, respectively (<xref ref-type="fig" rid="F5">Fig. 5</xref>).</p><p id="P31">Evaluating the trained model on the Davis independent testing set, we obtained AUROC of 0.925. We also used Youden’s J statistic to determine the optimal classification threshold (<xref ref-type="fig" rid="F6">Figure 6a</xref>), which was used for later calculation of confusion matrix metrics (<xref ref-type="fig" rid="F6">Figure 6b-e</xref>) including “Molecule unseen” (<xref ref-type="fig" rid="F6">Fig. 6c</xref>), “Protein unseen” (<xref ref-type="fig" rid="F6">Fig. 6d</xref>) and “None seen” (<xref ref-type="fig" rid="F6">Fig. 6e</xref>) testing sets. In these “unseen” testing set, DeepLPI model achieved higher accuracy in “Protein unseen” (<xref ref-type="fig" rid="F6">Fig. 6d</xref>) testing set with AUROC of 0.812, compared to in the “None seen” (<xref ref-type="fig" rid="F6">Fig. 6e</xref>) testing set with AUROC 0.692, and the lowest accuracy was in “Drug unseen” (<xref ref-type="fig" rid="F6">Fig. 6c</xref>) testing set with AUROC 0.618. “Drug unseen” was expected to result in higher AUROC because of the partial protein knowledge it includes, and the lower AUROC result might arise from the specific drug molecules collection because Davis dataset did not contain sufficient drug molecules. Larger dataset with more drug molecules could lead to better performance as in the BindingDB case.</p><p id="P32">In <xref ref-type="table" rid="T2">Table 2</xref>, we compared the performance metrics of our model with DeepCDA and DeepDTA on the independent testing set from the Davis dataset. DeepLPI with AUROC of 0.925 scored higher values by 0.013 and 0.006 than DeepCDA and DeepDTA, respectively. Given that all models’ AUROC are above 0.9, it can be concluded that DeepLPI is able to predict on Davis dataset with very high accuracy. DeepLPI reached sensitivity of 0.855, specificity of 0.862, NPV of 0.988 and PPV of 0.306, which were higher than or comparable to both DeepCDA and DeepDTA. The low value of PPV may be due to the unbalanced distribution of Davis dataset with less than 10% strong binding entries, and therefore leading to a much smaller true positive rate compared with the false positive. We argue that the result does not invalidate the three models but put into question the suitability of Davis dataset as a benchmark. Overall prediction results on the combined “unseen” testing sets show DeepLPI with AUROC of 0.791 is higher than the DeepCDA model, indicating DeepLPI has better generalizability.</p></sec><sec id="S17"><title>Evaluation on COVID-19</title><p id="P33">We further applied the trained models using BindingDB dataset directly on the COVID-19 dataset without any fine-tuning. DeepLPI outperforms DeepCDA with an AUROC of 0.610 (<xref ref-type="table" rid="T3">Table 3</xref>). The high prediction performance on COVID-19 dataset suggests that our DeepLPI may be the potential method to find effective drugs for SARS-CoV-2. The low PPV and specificity of DeepLPI arise from large false positive rates and are indicating the potential upgrades in our future works.</p></sec></sec></sec><sec id="S18" sec-type="discussion"><title>Discussion</title><p id="P34">In our work, we successfully build DeepLPI model to predict DTI in classification tasks using 1D sequence data from protein and drug molecules. We first utilize the pre-trained embedding methods called Mol2Vec and ProSE to encode the raw drug molecular SMILES strings and target protein sequences respectively into dense vector representations. Then, we feed the encoded dense vector representations separately into head modules and ResNet-based modules to extract features, where these modules are based on 1D CNN. The extracted feature vectors are concatenated and fed into the biLSTM network, further followed by the MLP module to finally predict binary active or inactive based on <italic>K<sub>d</sub></italic> affinity labeled data. We used three datasets of BindingDB, Davis and COVID-19 to evaluate our DeepLPI model, and the results demonstrate that our model has a high performance on the prediction.</p><p id="P35">Unlike the methods to pre-define features that are heavily relied on domain knowledge or to represent sequences simply using sparse encoding approach, our DeepLPI applied pre-trained embedding models of Mol2Vec[<xref ref-type="bibr" rid="R17">17</xref>] and ProSE[<xref ref-type="bibr" rid="R18">18</xref>] to encode the raw drug SMILES string and target protein sequences, respectively. These semantic context embedding models are trained using a huge dataset to represent sequence data in the form of dense vectors, with consideration of structure information of molecule and target proteins to ensure that they are highly informative and efficient for feature embeddings. It is admired that there exists a variety of embedding methods to encode drug compounds and protein sequences, we picked Mol2Vec and ProSE in our DeepLPI due to our empirical experience. We use 1D CNN in our DeepLPI model to retain the sequential correlation. We adopt a ResNet-based module in the DeepLPI. Traditional feed-forward CNN may lose useful information as the design grows deeper. Nevertheless, ResNet-based CNN can mitigate this drawback by developing a “shortcut connection” for the network. Consequently, data inputted into the ResNet-based CNN module can be added with the residual of the network to alleviate the loss of information. The biLSTM is employed in the DeepLPI model, which can capture long-term dependencies of the sequence, equally encode input sequence once from beginning to end and once from end to beginning. Compared to the classical LSTM, the biLSTM enables the use of the two hidden states in each LSTM memory block to preserve information from both past and future.</p><p id="P36">In our experiments, we notice that the performance of DeepLPI is not uniform on different proteins: there might exist some common biological features of those proteins such as the sequences or the spatial structures. Detailed analysis of the shared features of the proteins requires a deeper understanding of the protein-drug interaction and can potentially explain the reason that the model behaves well on some of the proteins. Such analysis would be useful to improve the model when we generalize the results later.</p><p id="P37">The DeepLPI model may help in speeding up the COVID-19 drug research. As of today, the pandemic is not showing any sign of slowing down and people are still searching for an effective and safe cure for COVID-19 patients. The current widely-used combination treatment with hydroxychloroquine and azithromycin has not been proven to be satisfactory, and there are some research efforts in using computational, especially deep neural network, techniques for searching the effective repurposed drugs. Our model can be useful in speeding up the drug search and potentially increase the success rate because the training data fed into the model is not limited to the protein structural information.</p><p id="P38">Even though we have successfully built a model that can predict active/inactive interaction with high accuracy, the model still suffers from some limitations. There is still room for improvement regarding the prediction accuracy, especially when the model is applied on external datasets. From a broader perspective, the study of repurposing drugs should not be limited only to the binding affinities. Researchers should also pay attention to the possibility of potential adverse effects of using the repurposed drug. This can be a result of new interactions between the drug and the proposed disease target, or because the drug is administered to a new group of population. Sometimes the repurposed drug could have interactions with traditional drugs on the new disease, and adverse effects might also arise from such unexpected interactions. Deep learning methods could also be used in studying on these aspects for better safety.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary table and figure</label><media xlink:href="EMS153436-supplement-Supplementary_table_and_figure.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d97aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S19"><title>Acknowledgements</title><p>B. W. would like to thank Dr. Yingce Xia for insightful discussions on the embedding algorithms and the testing methods for generalizability. B. W. would like to thank Dr. Yong Bai for his advice in deep learning algorithms.</p></ack><sec sec-type="data-availability" id="S20"><title>Data Availability</title><p id="P39">The datasets including drug information and protein information analyzed during the current study are publicly available online through the following links:</p><p id="P40">BindingDB data (<ext-link ext-link-type="uri" xlink:href="http://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes">http://www.bindingdb.org/bind/chemsearch/marvin/SDFdownload.jsp?all_download=yes</ext-link>),</p><p id="P41">Davis data (<ext-link ext-link-type="uri" xlink:href="http://staff.cs.utu.fi/~aatapa/data/DrugTarget/">http://staff.cs.utu.fi/~aatapa/data/DrugTarget/</ext-link>), and COVID-19 data (<ext-link ext-link-type="uri" xlink:href="https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html">https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html</ext-link>)</p><p id="P42">The code of the LPI model and the training and testing script notebooks are available open source through the following Github link (<ext-link ext-link-type="uri" xlink:href="https://github.com/David-BominWei/DeepLPI">https://github.com/David-BominWei/DeepLPI</ext-link>).</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P43"><bold>Author Contributions</bold></p><p id="P44">B. W. conceived the idea. All authors contributed to the proposal of the algorithm and selection of database and baseline methods. B. W. and X. G. contributed to the choice of metrics. B. W. and Y. Z. contributed to the construction of testing methods. B. W. wrote the manuscript and all authors contributed to the manuscript revisions.</p></fn><fn id="FN2" fn-type="conflict"><p id="P45"><bold>Competing Interests</bold></p><p id="P46">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pushpakom</surname><given-names>S</given-names></name><etal/></person-group><article-title>Drug repurposing: progress, challenges and recommendations</article-title><source>Nature Reviews Drug Discovery</source><year>2019</year><month>Jan</month><volume>18</volume><issue>1</issue><pub-id pub-id-type="pmid">30310233</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wouters</surname><given-names>OJ</given-names></name><name><surname>McKee</surname><given-names>M</given-names></name><name><surname>Luyten</surname><given-names>J</given-names></name></person-group><article-title>Estimated Research and Development Investment Needed to Bring a New Medicine to Market, 2009-2018</article-title><source>JAMA</source><year>2020</year><month>Mar</month><volume>323</volume><issue>9</issue><pub-id pub-id-type="pmcid">PMC7054832</pub-id><pub-id pub-id-type="pmid">32125404</pub-id><pub-id pub-id-type="doi">10.1001/jama.2020.1166</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahajan</surname><given-names>R</given-names></name><name><surname>Gupta</surname><given-names>K</given-names></name></person-group><article-title>Food and drug administration’s critical path initiative and innovations in drug development paradigm: Challenges, progress, and controversies</article-title><source>Journal of Pharmacy and Bioallied Sciences</source><year>2010</year><volume>2</volume><issue>4</issue><pub-id pub-id-type="pmcid">PMC2996064</pub-id><pub-id pub-id-type="pmid">21180462</pub-id><pub-id pub-id-type="doi">10.4103/0975-7406.72130</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>F</given-names></name><etal/></person-group><article-title>Identification of amitriptyline HCl, flavin adenine dinucleotide, azacitidine and calcitriol as repurposing drugs for influenza A H5N1 virus-induced lung injury</article-title><source>PLOS Pathogens</source><year>2020</year><month>Mar</month><volume>16</volume><issue>3</issue><pub-id pub-id-type="pmcid">PMC7075543</pub-id><pub-id pub-id-type="pmid">32176725</pub-id><pub-id pub-id-type="doi">10.1371/journal.ppat.1008341</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>P</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Winnenburg</surname><given-names>R</given-names></name><name><surname>Baumbach</surname><given-names>J</given-names></name></person-group><article-title>Drug repurposing by integrated literature mining and drug–gene–disease triangulation</article-title><source>Drug Discovery Today</source><year>2017</year><month>Apr</month><volume>22</volume><issue>4</issue><pub-id pub-id-type="pmid">27780789</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimber</surname><given-names>TB</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Volkamer</surname><given-names>A</given-names></name></person-group><article-title>Deep Learning in Virtual Screening: Recent Applications and Developments</article-title><source>International Journal of Molecular Sciences</source><year>2021</year><month>Apr</month><volume>22</volume><issue>9</issue><pub-id pub-id-type="pmcid">PMC8123040</pub-id><pub-id pub-id-type="pmid">33922714</pub-id><pub-id pub-id-type="doi">10.3390/ijms22094435</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallach</surname><given-names>I</given-names></name><name><surname>Dzamba</surname><given-names>M</given-names></name><name><surname>Heifets</surname><given-names>A</given-names></name></person-group><article-title>AtomNet: a deep convolutional neural network for bioactivity prediction in structure-based drug discovery</article-title><source>arXiv preprint</source><year>2015</year><elocation-id>arXiv:1510.02855</elocation-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><etal/></person-group><article-title>SE-OnionNet: A Convolution Neural Network for Protein–Ligand Binding Affinity Prediction</article-title><source>Frontiers in Genetics</source><year>2021</year><month>Feb</month><volume>11</volume><pub-id pub-id-type="pmcid">PMC7962986</pub-id><pub-id pub-id-type="pmid">33737946</pub-id><pub-id pub-id-type="doi">10.3389/fgene.2020.607824</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><etal/></person-group><article-title>PDB-wide collection of binding data: current status of the PDBbind database</article-title><source>Bioinformatics</source><year>2015</year><month>Feb</month><volume>31</volume><issue>3</issue><pub-id pub-id-type="pmid">25301850</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>R</given-names></name><name><surname>Evans</surname><given-names>J</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><etal/></person-group><article-title>Highly accurate protein structure prediction with AlphaFold</article-title><source>Nature</source><year>2021</year><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="pmcid">PMC8371605</pub-id><pub-id pub-id-type="pmid">34265844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayat</surname><given-names>A</given-names></name></person-group><article-title>Science, medicine, and the future: Bioinformatics</article-title><source>BMJ</source><year>2002</year><volume>324</volume><issue>7344</issue><pub-id pub-id-type="pmcid">PMC1122955</pub-id><pub-id pub-id-type="pmid">11976246</pub-id><pub-id pub-id-type="doi">10.1136/bmj.324.7344.1018</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ballester</surname><given-names>PJ</given-names></name><name><surname>Mitchell</surname><given-names>JBO</given-names></name></person-group><article-title>A machine learning approach to predicting protein–ligand binding affinity with applications to molecular docking</article-title><source>Bioinformatics</source><year>2010</year><month>May</month><volume>26</volume><issue>9</issue><pub-id pub-id-type="pmcid">PMC3524828</pub-id><pub-id pub-id-type="pmid">20236947</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btq112</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Leung</surname><given-names>K-S</given-names></name><name><surname>Wong</surname><given-names>M-H</given-names></name><name><surname>Ballester</surname><given-names>P</given-names></name></person-group><article-title>Low-Quality Structural and Interaction Data Improves Binding Affinity Prediction via Random Forest</article-title><source>Molecules</source><year>2015</year><month>Jun</month><volume>20</volume><issue>6</issue><pub-id pub-id-type="pmcid">PMC6272292</pub-id><pub-id pub-id-type="pmid">26076113</pub-id><pub-id pub-id-type="doi">10.3390/molecules200610947</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>P-W</given-names></name><name><surname>Chan</surname><given-names>KCC</given-names></name><name><surname>You</surname><given-names>Z-H</given-names></name></person-group><source>Large-scale prediction of drug-target interactions from deep representations</source><conf-name>2016 International Joint Conference on Neural Networks (IJCNN)</conf-name><year>2016</year><fpage>1236</fpage><lpage>1243</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2016.7727339</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Öztürk</surname><given-names>H</given-names></name><name><surname>Özgür</surname><given-names>A</given-names></name><name><surname>Ozkirimli</surname><given-names>E</given-names></name></person-group><article-title>DeepDTA: Deep drug-target binding affinity prediction</article-title><source>Bioinformatics</source><year>2018</year><month>Sep</month><volume>34</volume><issue>17</issue><fpage>i821</fpage><lpage>i829</lpage><pub-id pub-id-type="pmcid">PMC6129291</pub-id><pub-id pub-id-type="pmid">30423097</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/bty593</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbasi</surname><given-names>K</given-names></name><name><surname>Razzaghi</surname><given-names>P</given-names></name><name><surname>Poso</surname><given-names>A</given-names></name><name><surname>Amanlou</surname><given-names>M</given-names></name><name><surname>Ghasemi</surname><given-names>JB</given-names></name><name><surname>Masoudi-Nejad</surname><given-names>A</given-names></name></person-group><article-title>DeepCDA: deep cross-domain compound–protein affinity prediction through LSTM and convolutional neural networks</article-title><source>Bioinformatics</source><year>2020</year><month>Nov</month><volume>36</volume><issue>17</issue><pub-id pub-id-type="pmid">32462178</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>S</given-names></name><name><surname>Fulle</surname><given-names>S</given-names></name><name><surname>Turk</surname><given-names>S</given-names></name></person-group><article-title>Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition</article-title><source>J Chem Inf Model</source><year>2018</year><volume>58</volume><issue>1</issue><fpage>27</fpage><lpage>35</lpage><pub-id pub-id-type="pmid">29268609</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bepler</surname><given-names>T</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name></person-group><article-title>Learning the protein language: Evolution, structure, and function</article-title><source>Cell Systems</source><year>2021</year><month>Jun</month><volume>12</volume><issue>6</issue><pub-id pub-id-type="pmcid">PMC8238390</pub-id><pub-id pub-id-type="pmid">34139171</pub-id><pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Wen</surname><given-names>X</given-names></name><name><surname>Jorissen</surname><given-names>RN</given-names></name><name><surname>Gilson</surname><given-names>MK</given-names></name></person-group><article-title>BindingDB: a web-accessible database of experimentally determined protein-ligand binding affinities</article-title><source>Nucleic Acids Research</source><year>2007</year><volume>35</volume><issue>suppl_1</issue><fpage>D198</fpage><lpage>D201</lpage><pub-id pub-id-type="pmcid">PMC1751547</pub-id><pub-id pub-id-type="pmid">17145705</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkl999</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MI</given-names></name><etal/></person-group><article-title>Comprehensive analysis of kinase inhibitor selectivity</article-title><source>Nature Biotechnology</source><year>2011</year><month>Nov</month><volume>29</volume><issue>11</issue><pub-id pub-id-type="pmid">22037378</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="web"><collab>Diamond Light Source</collab><source>Main protease structure and XChem fragment screen</source><comment>Online data source. <ext-link ext-link-type="uri" xlink:href="https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html">https://www.diamond.ac.uk/covid-19/for-scientists/Main-protease-structure-and-XChem.html</ext-link></comment></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>K</given-names></name><etal/></person-group><article-title>Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development</article-title><source>arXiv preprints</source><year>2021</year><elocation-id>arXiv:2102.09548</elocation-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>M</given-names></name></person-group><article-title>Extended-Connectivity Fingerprints</article-title><source>Journal of Chemical Information and Modeling</source><year>2010</year><month>May</month><volume>50</volume><issue>5</issue><pub-id pub-id-type="pmid">20426451</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><source>Deep Residual Learning for Image Recognition</source><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><source>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</source><conf-name>2015 IEEE International Conference on Computer Vision (ICCV)</conf-name><year>2015</year><fpage>1026</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv preprint</source><year>2014</year><elocation-id>arXiv:1412.6980</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Distribution of BindingDB data used to develop the DeepLPI model.</title><p>(a) Distribution of the <italic>pK<sub>d</sub></italic> values and the threshold for determining active/inactive. (b) distribution interaction in binary classes (c) Distribution of lengths of drug molecular SMILES strings. (d) Distribution of lengths of protein sequences.</p></caption><graphic xlink:href="EMS153436-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The overview of the DeepLPI model.</title></caption><graphic xlink:href="EMS153436-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The loss and AUROC score during the DeepLPI training on the BindingDB Kd dataset.</title><p><bold>(a)</bold> Loss scores for training and validation. <bold>(b)</bold> AUROC scores for training and validation</p></caption><graphic xlink:href="EMS153436-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>The prediction performance of the final DeepLPI model on BindingDB dataset.</title><p><bold>(a)</bold> The ROC curve and the determined optimal threshold. <bold>(b)</bold> Confusion matrix based on the optimal threshold. <bold>(c)</bold> – <bold>(e)</bold> Confusion matrix and performance metrics on the three “unseen” drug/protein testsets: <bold>(c)</bold> Molecule unseen <bold>(d)</bold> Protein unseen and <bold>(e)</bold> None seen.</p></caption><graphic xlink:href="EMS153436-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>The loss and AUROC score during the DeepLPI training on Davis dataset <bold>(a)</bold> Loss scores for training and validation. <bold>(b)</bold> AUROC scores for training and validation</p></caption><graphic xlink:href="EMS153436-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>The prediction performance of the final DeepLPI model Davis dataset.</title><p><bold>(a)</bold> The ROC curve and the determined optimal threshold. <bold>(b)</bold> Confusion matrix based on the optimal threshold. <bold>(c) – (e)</bold> Confusion matrix and performance metrics on the three unseen drug/protein testsets: <bold>(c)</bold> Molecule unseen <bold>(d)</bold> Protein unseen and <bold>(e)</bold> None seen.</p></caption><graphic xlink:href="EMS153436-f006"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Comparing Performance of DeepLPI, DeepCDA and DeepDTA on the internal independent testing set from the BindingDB data.</title></caption><table frame="hsides" rules="rows"><thead><tr><th align="left" valign="middle">BindingDB</th><th align="left" valign="middle">AUROC</th><th align="left" valign="middle">Sensitivity</th><th align="left" valign="middle">Specificity</th><th align="left" valign="middle">PPV</th><th align="left" valign="middle">NPV</th></tr></thead><tbody><tr><td align="left" valign="middle">This work</td><td align="left" valign="middle"><bold>0.893</bold></td><td align="left" valign="middle"><bold>0.831</bold></td><td align="left" valign="middle">0.792</td><td align="left" valign="middle">0.728</td><td align="left" valign="middle"><bold>0.875</bold></td></tr><tr><td align="left" valign="middle">DeepCDA</td><td align="left" valign="middle">0.882</td><td align="left" valign="middle">0.792</td><td align="left" valign="middle">0.804</td><td align="left" valign="middle">0.730</td><td align="left" valign="middle">0.852</td></tr><tr><td align="left" valign="middle">DeepDTA</td><td align="left" valign="middle">0.889</td><td align="left" valign="middle">0.772</td><td align="left" valign="middle"><bold>0.862</bold></td><td align="left" valign="middle"><bold>0.790</bold></td><td align="left" valign="middle">0.849</td></tr><tr><td align="left" valign="middle" colspan="6"><bold>Unseen Testsets Combined</bold></td></tr><tr><td align="left" valign="middle">This work</td><td align="left" valign="middle"><bold>0.790</bold></td><td align="left" valign="middle"><bold>0.684</bold></td><td align="left" valign="middle">0.773</td><td align="left" valign="middle"><bold>0.671</bold></td><td align="left" valign="middle"><bold>0.783</bold></td></tr><tr><td align="left" valign="middle">DeepCDA</td><td align="left" valign="middle">0.448</td><td align="left" valign="middle">0.000</td><td align="left" valign="middle"><bold>1.0</bold></td><td align="left" valign="middle">Nan</td><td align="left" valign="middle">0.596</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Comparing Performance of DeepLPI, DeepCDA and DeepDTA on the internal independent testing set from the BindingDB data</title></caption><table frame="hsides" rules="rows"><thead><tr><th align="left" valign="middle"/><th align="left" valign="middle">AUROC</th><th align="left" valign="middle">Sensitivity</th><th align="left" valign="middle">Specificity</th><th align="left" valign="middle">PPV</th><th align="left" valign="middle">NPV</th></tr></thead><tbody><tr><td align="left" valign="middle">DeepLPI</td><td align="left" valign="middle"><bold>0.925</bold></td><td align="left" valign="middle">0.855</td><td align="left" valign="middle">0.862</td><td align="left" valign="middle">0.306</td><td align="left" valign="middle"><bold>0.988</bold></td></tr><tr><td align="left" valign="middle">DeepCDA</td><td align="left" valign="middle">0.912</td><td align="left" valign="middle">0.766</td><td align="left" valign="middle"><bold>0.896</bold></td><td align="left" valign="middle"><bold>0.342</bold></td><td align="left" valign="middle">0.982</td></tr><tr><td align="left" valign="middle">DeepDTA</td><td align="left" valign="middle">0.909</td><td align="left" valign="middle"><bold>0.865</bold></td><td align="left" valign="middle">0.795</td><td align="left" valign="middle">0.221</td><td align="left" valign="middle"><bold>0.989</bold></td></tr><tr><td align="left" valign="middle" colspan="6"><bold>Unseen Testsets Combined</bold></td></tr><tr><td align="left" valign="middle">This work</td><td align="left" valign="middle"><bold>0.791</bold></td><td align="left" valign="middle"><bold>0.661</bold></td><td align="left" valign="middle">0.789</td><td align="left" valign="middle">0.132</td><td align="left" valign="middle"><bold>0.980</bold></td></tr><tr><td align="left" valign="middle">DeepCDA</td><td align="left" valign="middle">0.741</td><td align="left" valign="middle">0.511</td><td align="left" valign="middle"><bold>0.813</bold></td><td align="left" valign="middle"><bold>0.495</bold></td><td align="left" valign="middle">0.823</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><title>Comparison of DeepLPI and DeepCDA on transferring BindingDB trained model to COVID-19.</title></caption><table frame="hsides" rules="rows"><thead><tr><th align="left" valign="middle"/><th align="left" valign="middle">AUROC</th><th align="left" valign="middle">Sensitivity</th><th align="left" valign="middle">Specificity</th><th align="left" valign="middle">PPV</th><th align="left" valign="middle">NPV</th></tr></thead><tbody><tr><td align="left" valign="middle">DeepLPI</td><td align="left" valign="middle"><bold>0.610</bold></td><td align="left" valign="middle">0.538</td><td align="left" valign="middle">0.576</td><td align="left" valign="middle"><bold>0.110</bold></td><td align="left" valign="middle"><bold>0.928</bold></td></tr><tr><td align="left" valign="middle">DeepCDA</td><td align="left" valign="middle">0.400</td><td align="left" valign="middle">0.000</td><td align="left" valign="middle"><bold>1.000</bold></td><td align="left" valign="middle">nan</td><td align="left" valign="middle">0.911</td></tr></tbody></table></table-wrap></floats-group></article>