<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS144460</article-id><article-id pub-id-type="doi">10.1101/2021.08.30.458179</article-id><article-id pub-id-type="archive">PPR389666</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Adaptive cognitive maps for curved surfaces in the 3D world</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Kim</surname><given-names>Misun</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Doeller</surname><given-names>Christian F.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Max Planck Institute for Human Cognitive and Brain Sciences, Leipzig, Germany</aff><aff id="A2"><label>2</label>Institute of Psychology, Leipzig University, Leipzig, Germany</aff><aff id="A3"><label>3</label>Kavli Institute for Systems Neuroscience, Trondheim, Norway</aff><author-notes><corresp id="CR1">Postal address: Stephanstr. 1a, Leipzig 04107, Germany. Corresponding: <email>mkim@cbs.mpg.de</email>, <email>doeller@cbs.mpg.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>04</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>04</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Terrains in a 3D world can be undulating. Yet, most prior research has exclusively investigated spatial representations on a flat surface, leaving a 2D cognitive map as the dominant model in the field. Here, we investigated whether humans represent a curved surface by building a dimension-reduced flattened 2D map or a full 3D map. Participants learned the location of objects positioned on a flat and curved surface in a virtual environment by driving on the concave side of the surface (Experiment 1), driving and looking vertically (Experiment 2), or flying (Experiment 3). Subsequently, they were asked to retrieve either the path distance or the 3D Euclidean distance between the objects. Path distance estimation was good overall, but we found a significant underestimation bias for the path distance on the curve, suggesting an influence of potential 3D shortcuts, even though participants were only driving on the surface. Euclidean distance estimation was better when participants were exposed more to the global 3D structure of the environment by looking and flying. These results suggest that the representation of the 2D manifold, embedded in a 3D world, is neither purely 2D nor 3D. Rather, it is flexible and dependent on the behavioral experience and demand.</p></abstract><kwd-group><kwd>curved</kwd><kwd>three-dimensional</kwd><kwd>Euclidean</kwd><kwd>path distance</kwd><kwd>cognitive map</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">People often rely on printed or electronic maps for everyday navigation. Due to the 2D nature of these formats, most maps are simplified 2D representations of the 3D world in which we live. The maps often lack information about the elevation of undulating terrain (except for the contour plot) and give us the impression that both the physical map as well as our internal representation of the world is mainly 2D. The vast majority of research on spatial navigation and cognitive mapping has also been conducted on horizontal, flat, 2D surfaces, despite real environments being more complex (<xref ref-type="bibr" rid="R6">Boccia et al., 2014</xref>; <xref ref-type="bibr" rid="R35">Moser et al., 2017</xref>). Whether it is sufficient to have a 2D surface-based map or whether humans build a full 3D model of the world is an important question that is not fully understood.</p><p id="P3">Some of the early behavioral and neurophysiological studies suggest that surface-dwelling animals such as rodents and humans have a 2D or at best 2.5D representation of the environment, as their movements are constrained to the earth's surface due to gravity (<xref ref-type="bibr" rid="R29">Jeffery et al., 2013</xref>). Humans and dogs showed better within-floor spatial memory than across-floor memory in a multi-level building (<xref ref-type="bibr" rid="R7">Brandt &amp; Dieterich, 2013</xref>; <xref ref-type="bibr" rid="R26">Hölscher et al., 2006</xref>; <xref ref-type="bibr" rid="R34">Montello &amp; Pick, 1993</xref>). In the brain, head direction cells serve as a compass system and these cells are mainly sensitive to the horizontal component of the heading (azimuth) but not to the vertical pitch (<xref ref-type="bibr" rid="R47">Stackman &amp; Taube, 1998</xref>). Head direction cells encoded the direction, relative to the local plane of locomotion, even when the locomotion plane was rotated in 3D space such as with a vertical wall or ceiling (<xref ref-type="bibr" rid="R9">Calton, 2005</xref>; <xref ref-type="bibr" rid="R43">Page et al., 2018</xref>; <xref ref-type="bibr" rid="R51">Taube et al., 2004</xref>, <xref ref-type="bibr" rid="R52">2013</xref>). Rather than showing a 3D volumetric receptive field, entorhinal grid cells have shown similar firing patterns on a horizontal plane and its connected slope, as if the slope was an extension of the horizontal surface (<xref ref-type="bibr" rid="R23">Hayman et al., 2015</xref>). Based on neuroscientific findings, Jeffery et al. proposed that 3D space is encoded as a mosaic of local planar maps (<xref ref-type="bibr" rid="R29">Jeffery et al., 2013</xref>).</p><p id="P4">On the other hand, it is conceivable that even surface-residing animals could build a more complete 3D model of the environment than a mere surface model. Those who navigate on undulating terrain must consider the elevation to minimize the energy cost of moving upwards. This has been shown in a study where people tried to avoid local hills, taking a detour when they were asked to take the shortest route while solving the travelling salesman problem in a non-flat environment (<xref ref-type="bibr" rid="R32">Layton et al., 2010</xref>). Furthermore, a slope can be utilized as a salient orientation cue for spatial memory tasks in humans (<xref ref-type="bibr" rid="R25">Holmes et al., 2015</xref>; <xref ref-type="bibr" rid="R38">Nardi et al., 2011</xref>, <xref ref-type="bibr" rid="R39">2021</xref>; <xref ref-type="bibr" rid="R48">Steck et al., 2003</xref>), rats (<xref ref-type="bibr" rid="R21">Grobéty &amp; Schenk, 1992</xref>; <xref ref-type="bibr" rid="R62">Wilson et al., 2015</xref>), and pigeons (<xref ref-type="bibr" rid="R37">Nardi &amp; Bingman, 2009</xref>). It is also reported that the hippocampus in both rodents and humans encodes horizontal as well as the vertical spatial information similarly well when subjects move around in semi-volumetric environment (<xref ref-type="bibr" rid="R19">Grieves et al., 2020</xref>; <xref ref-type="bibr" rid="R30">Kim et al., 2017</xref>). Similarly, grid cells with 3D ellipsoidal receptive fields were observed in rat and bat entorhinal cortex when these animals explored an open volumetric space (<xref ref-type="bibr" rid="R16">Ginosar et al., 2021</xref>; <xref ref-type="bibr" rid="R18">Grieves et al., 2021</xref>).</p><p id="P5">A simplified surface-based map enables efficient and compressed encoding of the world, whereas a fully volumetric 3D map would allow flexible route planning beyond the current plane of locomotion. It remains unknown which type of the representation is utilized by humans who navigate on a curved surface embedded within a 3D world. To tackle this question, we built a novel 3D virtual environment composed of a flat and curved surface and asked participants, with varying degrees of movement, to learn the location of objects on the surface in the series of online behavior experiments (Experiment 1: driving alone, Experiment 2: driving with additional vertical viewing, Experiment 3: flying). We then asked participants to retrieve either path distances on the surface or Euclidean distances in 3D from their memory. Distance estimates for the flat and curved parts were analyzed to probe which type of cognitive maps participants built.</p></sec><sec id="S2"><label>2</label><title>Experiment 1</title><sec id="S3" sec-type="intro"><label>2.1</label><title>Introduction</title><p id="P6">In this experiment, participants learned the location of objects on seamlessly connected flat and cylindrical surfaces by driving with their heads parallel to the tangent of the surface (<xref ref-type="fig" rid="F1">Fig. 1</xref>). Crucially, the objects were positioned in a way that between-object path distances were identical on the flat and curved surfaces (e.g. AB=AG), whereas the Euclidean distance was shorter for the curved surface (e.g. AB&lt;AG). Given that all objects and participant views and movements were restricted to the local surface, it could be sufficient to remember the object location relative to the boundary of the surface, treating the curved surface as an extension of the flat surface and disregarding the position of the surfaces within the global 3D world. Thus, one might develop a flattened 2D map as illustrated in the schematic figure <xref ref-type="fig" rid="F1">Fig. 1B</xref>. In this case, there should be no systematic bias in path distance estimation for the objects on the flat and curved areas (e.g. AB=AG). Further, participants should show difficulty in estimating 3D Euclidean distance if they are asked to do so later. On the other hand, participants might also automatically encode the 3D global layout of the environment and build a volumetric 3D map (<xref ref-type="fig" rid="F1">Fig. 1A</xref>), even if it was not strictly necessary for the object location learning task. In this case, participants would be good at the Euclidean distance estimation task and 3D map knowledge might even interfere when participants try to recall the path distance, resulting in an underestimation bias for the curved path (e.g. AB&lt;AG). We tested these predictions in an online virtual reality (VR) experiment.</p></sec><sec id="S4" sec-type="methods"><label>2.2</label><title>Method</title><sec id="S5" sec-type="subjects"><label>2.2.1</label><title>Participants</title><p id="P7">Participants were recruited from the online experiment platform (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co/">www.prolific.co</ext-link>). Forty-two participants (female = 12, mean age = 22.7 ±4.0 years) were in the path distance group and 45 participants (female = 16, mean age = 24.2 ±4.8 years) were in the Euclidean distance group. All participants self-reported having no current, or history of, neurological or psychiatric disorder. This study was approved by a local research ethics committee.</p></sec><sec id="S6"><label>2.2.2</label><title>Virtual environment and the movement within it</title><p id="P8">The virtual environment was composed of a flat square and a curved surface (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The curved section was continuous with the flat section, with an arc of 270°. The entire structure resembled a paper that was being rolled up or a half-pipe in a skate park (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The long arc was used to maximize the difference between the 2D path distance and the 3D Euclidean distance between the two extreme ends of the arc. The arc length (25 virtual units) was identical to the length of the flat surface. A green grass texture was applied to the whole surface and semi-transparent walls formed the boundary of the surface, preventing participants from moving beyond the surface. A snowy mountain and lake landscape was used as a background. We used Unity 2018.1.9f2 (Unity Technologies, CA) to implement the virtual environment and deploy the experiment into the web browser.</p><p id="P9">Participants explored the virtual environment from a first-person perspective using a desktop-based VR. Participants were told that they were driving a vehicle on the surface and that the vehicle could not fall off. They could move forward/backward and turn the vehicle right/left using the arrow keys. Participants' heading (i.e. viewing) direction was always parallel to the tangent of the surface. Their view was rather limited on the curved portion of the surface due to the concavity of the environment. The speed of the movement was constant throughout the environment (8 virtual units/sec). More specifically, there was no acceleration or deceleration due to gravity and participants could move similarly on the curved and flat parts of the environment. Snapshots of the environment from a participant's perspective are shown in <xref ref-type="fig" rid="F1">Fig. 1C</xref> and <xref ref-type="fig" rid="F2">Fig. 2A</xref>. An example trajectory is shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Video 1</xref>. Readers can also explore the virtual environment in the interactive webpage (<ext-link ext-link-type="uri" xlink:href="http://misunkim.github.io/curveSpace_demo">http://misunkim.github.io/curveSpace_demo</ext-link>).</p></sec><sec id="S7"><label>2.2.3</label><title>Tasks and analysis</title><p id="P10">Participants completed the tasks in the following order: familiarization, object-location learning and testing, distance estimation tasks, and debriefing. The whole experiment took about 30 minutes.</p><sec id="S8"><label>2.2.3.1</label><title>Familiarization</title><p id="P11">Participants first familiarized themselves in the virtual environment and practiced the movements. They were instructed to move to a traffic cone. Once they arrived at the cone, another cone would appear, at a new location, and participants had to find the new cone. To help participants to quickly find the cone, which could be invisible due to curvature of the environment, a guide arrow was shown on the ground throughout the whole familiarization period.</p></sec><sec id="S9"><label>2.2.3.2</label><title>Object location learning and test phase</title><p id="P12">Following the familiarization period, participants learned the locations of 8 objects in the environment (<xref ref-type="fig" rid="F2">Fig. 2</xref>). The objects were cubes with pictures of animals or fruit on their faces. The 8 locations were constant, while the assignment of each picture cube to each location was randomized across participants. Each picture cube appeared once sequentially and participants were guided to move to each picture cube.</p><p id="P13">At the beginning of each trial in the test phase (following the learning phase), a picture cue was presented at the center of the screen and the participant was teleported to a random starting location. They then moved to the remembered location of the cued picture cube and pressed the spacebar. There was a time limit of 60 seconds per trial. Very few trials (0.1 trial per participant on average) were aborted due to the timeout. After each trial, participants received feedback on their displacement error (5 scales from a frowning to a smiling face) and were shown the correct location of the picture cube. They had to move to the correct location for the next trial to begin. Each object was tested between 4 and 8 times until the participant reached the learning criteria (distance error of less than 25% of the short axis of the environment). For the distance estimation analysis, we only included the participants who remembered all 8 objects well (i.e. distance error in either of the last two trials was less than 25%).</p></sec><sec id="S10"><label>2.2.3.3</label><title>Distance estimation tasks</title><p id="P14">We then asked participants to estimate the between-object distances outside the virtual environment. The path distance group was instructed to estimate the distance by imagining how long it would take to move from one picture cube to the other picture cube (e.g. red lines on the surface in the schematic <xref ref-type="fig" rid="F1">Fig. 1A</xref>). In contrast, the Euclidean distance group was instructed to imagine a straight line between the object in 3D space (e.g. blue lines in <xref ref-type="fig" rid="F1">Fig. 1A</xref>). To ensure that the participants had understood the definition of a 3D Euclidean line, we showed a short video that visualized it from a first-person-perspective in the virtual environment (<xref ref-type="supplementary-material" rid="SD1">Supplementary Video 2</xref>). This led to the possibility that participants would gain additional knowledge of the 3D environment from this short video instruction, specifically in the Euclidean group, even though their movement and views were restricted to the surface beforehand. In order to minimize the additional exposure to the 3D structure via the instruction video, we made the video short (less than a minute) and didn't show the picture cube during this instruction, thus preventing participants from further learning the distance between the objects. Importantly, a bird's eye view of the 3D environment like <xref ref-type="fig" rid="F1">Fig. 1A</xref> was not shown to participants and we did not inform them about the upcoming distance estimation tasks while they were learning the object locations inside the virtual environment. Therefore, participants must have reconstructed a cognitive map of the virtual environment from their memory to estimate either path or Euclidean distance upon request.</p><p id="P15">The knowledge of distance was probed with two types of tasks: a comparison task and a slider task. In the comparison task, a triplet of objects was presented on the screen and participants had to decide whether the object on the top row was closer to the 1<sup>st</sup> or 2<sup>nd</sup> object on the bottom row (<xref ref-type="fig" rid="F3">Fig. 3A</xref>). Participants did not receive performance feedback. The key triplets were the comparison between the path on the flat and curved surface (AB vs. AG, or EC vs. EF, see the location naming in <xref ref-type="fig" rid="F1">Fig. 1A</xref>) with each being presented four times (8 trials in total). The true path distance was identical for the curve and linear sections and therefore neither path should be reported as shorter (i.e., closer) above chance level (50%) if participants had an accurate surface map and were able to retrieve distances on that map. On the other hand, if the knowledge of the 3D nature of the map and Euclidean distance interferes with the surface map knowledge, participants might show an underestimation bias for the curve path, because the Euclidean distance was shorter for the objects on the curve. As a manipulation check, we also included trivial triplets that could easily be solved by either path or Euclidean distance (e.g. FH vs. FG, BD vs. BA) and the triplets where both path and Euclidean distances were identical. After excluding a few outliers (see <xref ref-type="sec" rid="S14">Results</xref> section), whose accuracy was chance level for the abovementioned easy trials, we tested whether the response rate for choosing the curve distance as shorter than the linear distance was above chance using a Wilcoxon-signed rank test due to non-normality of the data.</p><p id="P16">In the slider task, participants rated the distance between pairs of objects by adjusting the slider bar with a range from "very close" to "very far" (<xref ref-type="fig" rid="F3">Fig. 3B</xref>). We scaled the slider bar between 0 ("very close") and 1 ("very far"). All possible pairs of objects were presented twice (8 objects, 28 unique pairs, 56 trials in total) and we averaged the two ratings for each unique pair of objects within participant. We first quantified the overall consistency between the subjective distance rating and the true path or Euclidean distances using Spearman correlation. We then focused our analysis on the key pairs where the curve and linear distances were maximally dissociated (curve: AB, EC vs linear: AG, EF). We tested whether the curve distance was rated shorter than the linear distance using a paired t-test.</p></sec><sec id="S11"><label>2.2.3.4</label><title>Debriefing</title><p id="P17">After the main tasks, participants completed the short debriefing forms. The questionnaire included the question on whether they had paid attention to the distance or size of the virtual environment during the object-location tests and what types of strategies they used during the task (e.g. first-person perspective or 3D bird's eye view or flattened map). We report the percentage of self-reported strategies for the distance estimation task from the participants who met the object-location learning criteria.</p></sec><sec id="S12"><label>2.2.3.5</label><title>Statistical analyses</title><p id="P18">All analyses were conducted in MATLAB and we report the group mean ± standard deviation, degrees of freedom, and p-values, unless stated otherwise.</p></sec></sec><sec id="S13"><label>2.2.4</label><title>Control experiments</title><p id="P19">When participants drive on the surface, the view point is above the surface with certain eye height. The path length at the eye height is shorter than the path length at the surface level in the concave surface like the one used in the current experiment. We asked participants to estimate the path length between the picture cubes which were lying on the surface, therefore participants should estimate the path length at the ground level, not at the eye height level. Nevertheless, to rule out the possibility that participants underestimated the path length on the curve section due to mistaking the definition of path length, we conducted a control experiment which manipulates the eye height. We increased the eye height from 1 virtual unit to 2.5 virtual unit in this control experiment (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>). If participants measured the distance between the objects at the eye level, we would expect a significantly stronger underestimation bias for this new experiment. Similar to the main experiment, a new group of 44 participants were recruited and estimated the path distance (female = 21, mean age = 22.8 ±3.7 years). On a side note, we also increased the size of the traffic cone and picture cubes so that these objects were easily visible at the new eye height.</p><p id="P20">In the second control experiment, we built a convex environment where participants drive on the outside of the cylindrical surface (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2, Supplementary Video 3</xref>). In the new environment, path length at the eye level is longer on the curved section. Importantly, the emotional factor and the accessibility of Euclidean shortcut also differ from the main experiment. People can feel a stronger sensation of being upside down and falling when they drive outside of the curve compared to when they drive inside of the curve. More crucially, Euclidean shortcut might become less imaginable in the convex setting because that would require a building of tunnel through the surface, in comparison to relatively easily imaginable bridge or flying route in the concave setting. We tested whether there is still an underestimation bias for the path along the curved section in the convex setting with a new group of 45 participants (female = 22, mean age = 23.0 ±3.8 years).</p></sec></sec><sec id="S14" sec-type="results"><label>2.3</label><title>Results</title><sec id="S15"><label>2.3.1</label><title>Object location memory result</title><p id="P21">Most participants remembered the location of all 8 objects well (mean repetitions per object = 5.2 ±0.9, mean distance error on the last two trials = 0.09 ±0.09 for all 87 participants. All distances are reported as relative size to the short axis of the environment, <xref ref-type="fig" rid="F2">Fig. 2B</xref>). We had to exclude 13 participants from further analysis who did not meet our learning criteria (see Method <xref ref-type="sec" rid="S9">2.2.3.2</xref>). The final distance errors for the objects on the curved part of the environment were not significantly different from those on the flat part (mean error for the objects B, C, and D on the curve = 0.06 ±0.02 vs. object F, G, and H on the flat = 0.06 ±0.02, t(73) = 0.7, p = 0.49). This suggests that participants had comparably good memory for the flat and curved surfaces.</p><p id="P22">Of note, we found a small bias in participants' memory for the objects at the intersection between the flat and curved sections (location A, E). The remembered locations for these middle objects were slightly skewed to the flat side of the environment, such that they were placed slightly closer to the object at the end of the flat part rather than the object at the end of the curved part. This was due to the tendency of participants to approach the middle object location from the flat side rather than the curved part, where the view was inherently limited due to the curvature. The movement trajectories are visualized in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3</xref>. The resulting difference in the between-object distance for the linear and curved parts was very small, but significant (curve = 0.93 ±0.06 vs. linear = 0.87 ±0.07, t(73) = 5.4, p &lt; 0.001).</p></sec><sec id="S16"><label>2.3.2</label><title>Distance estimation result for path group</title><p id="P23">Following the object-location test, the path group participants were asked to estimate the between-object path distances on the surface. Key pairs of objects had been placed at equal distances on the curved and flat sections, and we tested whether there was a bias in the distance estimation that could be explained by interference from 3D map knowledge.</p><p id="P24">During the comparison task, most participants (33 out of 37) passed the manipulation check (e.g. above chance level accuracy for very easy trials, see Method <xref ref-type="sec" rid="S10">2.2.3.3</xref>). Critically, in the key comparison trials, where participants had to choose between the equidistant options of the curved and linear paths, they showed a significant underestimation bias for the curved path. This is indicative of an interference effect from the 3D map knowledge (mean rate = 64 ±33%, Wilcoxon sign-rank test, n = 33, Z = 2.2, p = 0.013, 1-tailed, <xref ref-type="fig" rid="F3">Fig. 3C</xref>).</p><p id="P25">We then examined the distance ratings from the slider bar task where participants indicated the perceived distances of all object pairs using the continuous scale ranging from "very close" (0) to "very far" (1). Participants' path distance estimates were highly correlated with the true 2D distances (ρ = 0.69 ±0.26, <xref ref-type="fig" rid="F3">Fig. 3D</xref>), implying that most participants had an overall good path distance estimation ability. Importantly, the curve distance estimate was again slightly but significantly shorter than the linear distance (mean estimate for curve = 0.44 ±0.13, linear = 0.49 ±0.11, paired t-test, t(36) = 1.7, p = 0.045, one-tailed, <xref ref-type="fig" rid="F3">Fig. 3E</xref>). Thus, in both the slider bar and comparison tasks we found consistent evidence for an influence of the 3D curve on distance estimates.</p><p id="P26">At the end of the experiment participants were asked about their strategy during distance estimation. The option "I imagined myself moving from one picture to the other picture and estimating the distance (first-person perspective)" was selected by 24% (<xref ref-type="fig" rid="F1">Fig. 1C</xref>), 57% selected "I had the map of the 3D curved environment in my mind and measured the distance on that map (bird's-eye view)" (<xref ref-type="fig" rid="F1">Fig.1A</xref>), and only 19% selected "I had the map of the environment in my mind, but it was rather a simple, flat map." (<xref ref-type="fig" rid="F1">Fig.1B</xref>).</p><p id="P27">In the control experiment where the eye height was increased, we found a similar magnitude of the underestimation bias for the curved section (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>), implying that participants did correctly understand the definition of path length at the surface, rather than mistakenly reporting the path length at the eye height. In other control experiment where the convex surface was used, we no longer saw the underestimation bias for the curved section (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2</xref>). Similar to the main concave experiment, a small number of participants reported to use a flattened map strategy (c.f. first-person perspective = 38%, 3D map = 35%, flattened map = 27%).</p></sec><sec id="S17"><label>2.3.3</label><title>Distance estimation result for Euclidean group</title><p id="P28">The Euclidean group were asked to recall 3D Euclidean distances instead of path distances on the surface. In the comparison task, 33 out of 37 participants solved the easy trials with above chance accuracy and these participants correctly chose the curve distance as shorter than the linear distance (71 ±32%, sign-rank test, Z = 3.2, p = 0.001, <xref ref-type="fig" rid="F3">Fig. 3F</xref>). However, the result of the slider task was ambiguous. Participants' distance estimates were positively correlated with the true Euclidean distance (<xref ref-type="fig" rid="F3">Fig. 3G</xref>, ρ = 0.38 ±0.25). However, the key curve distance estimate was not significantly shorter than the linear distance (mean estimate for curve = 0.42 ±0.15, linear = 0.46 ±0.13, t(36) = 1.1, p = 0.15, one-tailed, <xref ref-type="fig" rid="F3">Fig. 3H</xref>).</p></sec></sec><sec id="S18" sec-type="discussion"><label>2.4</label><title>Discussion</title><p id="P29">Overall, participants showed good spatial memory for objects lying on the flat and curved surfaces and they could also later estimate between-object distances from memory well above chance. Crucially, we found that participants estimated the path distances along the curve as slightly shorter even though the true path distances were identical. This distance estimation bias cannot be explained by an imprecise memory for object-location or poor task comprehension because we only included those participants who passed not only the object-location learning criteria but also the manipulation check for the distance task. Moreover, the opposite pattern of result would be expected if a small bias in location memory error was taken into consideration. However, as we described in the object-location memory result section, participants placed the middle object slightly closer to the objects on the flat side than the objects on the curved side. Thus, if this would have influenced the distance estimation, they should have rather underestimated the linear path, not the curved path. The control experiment with eye height manipulation showed that the path at the eye level above the ground could not explain the underestimation bias. Reduced visibility in the curved part also is also unlikely to explain the underestimation bias for the curve path. It is known that people overestimate the distances between objects separated by barriers (<xref ref-type="bibr" rid="R40">Newcombe &amp; Liben, 1982</xref>). Our curved environment can be regarded as containing a barrier because participants could not directly view the objects at the other end of the curve, due to the curve itself. Therefore, one would rather expect an overestimation bias for the curved path if visibility matters. Ruling out the abovementioned factors, we interpret the underestimation bias of the curved path as an influence of knowledge of the 3D representation and potential Euclidean shortcut. Participants never saw these 3D shortcuts in the virtual environment, which were thus, behaviorally irrelevant. The self-reports during the debriefing, which showed the dominance of the 3D map over the 2D map, also supports the view that participants acquired 3D knowledge of the environment.</p><p id="P30">Of note, we didn't see the underestimation bias for the curved path in the control experiment with a convex surface. Compared to the concave surface where one can relatively easily imagine a Euclidean shortcut between the points on the surface as a form of a bridge or straight-line along the sight, such a shortcut can be more difficult to imagine in the convex setting where a tunnel through the earth should be built and this might be a reason for the difference in path perception between the convex and the concave setting. However, even in the convex setting, only a small number of participants reported to use a flattened 2D map strategy; rather they reported to use a 3D map or first-person-perspective strategy. Therefore, we believe that people have a level of 3D global knowledge in both concave and convex setting.</p><p id="P31">However, the influence of 3D shortcuts and/or the self-reported strategy for a 3D map does not imply that humans generally build a precise, metric, 3D map knowledge when they drive on a surface. This is rather unlikely because 3D Euclidean distance estimation was far from perfect when participants were explicitly asked to imagine the 3D distance. They correctly selected the curve distance as the shorter one when they were forced to choose between the curve and linear distances during the comparison task, but their distance estimates for the curve and linear distance were not significantly different during the slider task. This suggests that the perceived difference between the two distances was small and that the difference was only detectible when the two were directly compared against each other. The difference was not detectible when participants were asked to estimate each distance separately, across different trials in the slider task. The slider task was inherently more difficult than the direct comparison task. The range of distances that participants had to remember and estimate with the slider was large, from the nearest object pair (e.g. BD) to the farthest-away object pair (e.g. CG), and the key pairs in the middle of the range.</p><p id="P32">Taken together, the current experiment showed that participants did not build a strictly 2D map that was agnostic to the global 3D structure. There was an influence of the global 3D structure and Euclidean distance. But participants did not form elaborate 3D map knowledge, which also was not necessary for the behavioral demands (i.e. learning the object-location on the surface while driving). This raises the question of whether humans have a fundamental difficulty in constructing 3D maps of the world. We predicted that the way participants learn the spatial layout of the environment (e.g. how many 3D cues are available in the environment or whether their movement is constrained to the surface or not, etc.) would influence the type of map they would build. This hypothesis was tested in the next experiments.</p></sec></sec><sec id="S19"><label>3</label><title>Experiment 2</title><sec id="S20" sec-type="intro"><label>3.1</label><title>Introduction</title><p id="P33">In Experiment 1, participants were always facing parallel to the surface while moving around. Thereby, the vertical tilt was zero (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). With zero vertical tilt, participants can see everything in front of them when standing on the open flat environment, whereas they can see distal points on the curved surface only when they look up, because the curvature blocks the view. In the current experiment, we allowed participants to freely look up and down while learning the object locations to facilitate the learning of the global 3D structure (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). The object locations and the following distance estimation tasks were identical to the previous experiment. We tested 1) whether there is an underestimation bias for the curve path in the current setup, and 2) whether participants can better estimate the 3D distance.</p></sec><sec id="S21" sec-type="methods"><label>3.2</label><title>Method</title><sec id="S22" sec-type="subjects"><label>3.2.1</label><title>Participants</title><p id="P34">Participants were recruited from the online experiment platform (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.co/">www.prolific.co</ext-link>). Forty-four participants (female = 10, mean age = 23.9 ±4.7 years) were in the Path distance group and 44 participants (female = 13, mean age = 23.7 ±4.8 years) were in the Euclidean distance group. None of them took part in the previous experiment.</p></sec><sec id="S23"><label>3.2.2</label><title>Virtual environment</title><p id="P35">The identical environment was used as in Experiment 1.</p></sec><sec id="S24"><label>3.2.3</label><title>Task and analysis</title><p id="P36">As previously, the experiment was composed of a familiarization phase, object-location learning and test, distance estimation tasks, and debriefing. The only changes were that participants could additionally look up (away from the surface) or down (towards the surface) using the arrow keys, and the guide arrow on the ground, that showed the direction to the target, was removed to encourage active looking behavior. Most participants looked slightly upwards (mean tilt angle = -11 ±9°) when they were on the curved part of the environment and they looked rather straight or slightly downwards when they were on the flat part of the environment (mean tilt angle = 4±7°) during the object location test. An example viewing trajectory and distribution of tilt angle is shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 4 and Supplementary Video 4</xref>. The rest of the analysis was identical to Experiment 1.</p></sec></sec><sec id="S25" sec-type="results"><label>3.3</label><title>Result</title><sec id="S26"><label>3.3.1</label><title>Object location memory</title><p id="P37">Similar to Experiment 1, most participants showed good memory of object locations (mean repetition per object = 5.1 ±1.1, mean distance error = 0.09 ±0.08 for all 88 participants, all distances are reported as relative to the size of the short axis of the environment, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 5</xref>). Only those who met our object-location memory criteria were included in the distance estimation analysis, leaving us with a final sample of n = 38 for the path group and n = 35 for the Euclidean group. As before, we found that participants placed the center objects slightly towards the flat side of the environment as opposed to the curved side (distance to the flat side = 0.89 ±0.06 vs. distance to the curved side = 0.92 ±0.06, t(72) = 2.6. p = 0.01). Unlike the first experiment, the final distance error for the objects on the curved section was slightly larger than for the flat section (error for the objects on the curve = 0.06 ±0.02 vs. flat = 0.05 ±0.02, t(72) = 2.6, p=0.012).</p></sec><sec id="S27"><label>3.3.2</label><title>Distance estimation: path group</title><p id="P38">As before, a majority of participants showed good comprehension of the distance estimation task: 35 out of 38 participants showed above chance accuracy for easy trials in the comparison task. During the direct comparison between the curved and linear paths, we found a tendency for participants to choose the curve as shorter (62 ±41%, sign-rank test, p = 0.051, Z = 1.6, 1-tailed, <xref ref-type="fig" rid="F4">Fig. 4A</xref>), similar to the result of Experiment 1. This suggests an influence of the 3D representation during path retrieval. Overall, participants showed good path distance estimation during the slider task as previously (ρ = 0.65 ±0.34, <xref ref-type="fig" rid="F4">Fig. 4B</xref>). However, the curve path was not significantly underestimated during the slider task (mean estimate for curve = 0.43 ±0.14 vs. linear = 0.45 ±0.10, paired t-test, t(37) = 1.0, p = 0.15, one-tailed, <xref ref-type="fig" rid="F4">Fig. 4C</xref>).</p><p id="P39">For the debriefing question on the distance estimation strategy, 37% selected the option that "I imagined myself moving from one picture to another picture and estimating the distance (first-person perspective)" (<xref ref-type="fig" rid="F1">Fig. 1C</xref>), 45% selected "I had a map of the 3D curved environment in my mind and measured the distance on that map (bird's-eye view 3D)" (<xref ref-type="fig" rid="F1">Fig. 1A</xref>), and 18% selected "I had the surface map of the environment, rather 2D, and measured the distance on that map (bird's-eye view 2D)." (<xref ref-type="fig" rid="F1">Fig. 1B</xref>).</p></sec><sec id="S28"><label>3.3.3</label><title>Distance estimation: Euclidean group</title><p id="P40">Most participants performed the distance estimation task well. Out of 35 individuals, 33 showed above chance accuracy for easy trials in the comparison task (mean accuracy for all participants = 92 ±14%). These participants correctly reported the curve as shorter in the comparison task (78 ±31%, sign-rank test, p &lt; 0.001, Z = 3.5, 1-tailed, <xref ref-type="fig" rid="F4">Fig. 4D</xref>). Participants' distance estimates during the slider task were positively correlated with true Euclidean distance (ρ = 0.49 ±0.26, <xref ref-type="fig" rid="F4">Fig. 4E</xref>). The curve distance estimate was also significantly shorter than the linear estimate during the slider task (mean estimate for the curve = 0.35 ±0.18 vs. linear = 0.45 ±0.12, t(34)=2.3, p = 0.014, one-tailed, <xref ref-type="fig" rid="F4">Fig. 4F</xref>).</p></sec></sec><sec id="S29" sec-type="discussion"><label>3.4</label><title>Discussion</title><p id="P41">In the current experiment, participants learned the object locations on the surface while driving as in the previous experiment, but additionally they could look above or below. We tested whether this additional viewing behavior encouraged participants to build a more precise 3D map of the environment, even though they were still physically constrained to the surface. If that were to be the case, we would expect more precise estimates of Euclidean distance, when they were explicitly asked (Euclidean group). It might also be the case that their path distance estimation would be strongly influenced by the 3D map, leading to an underestimation bias for the curve path (Path group).</p><p id="P42">When participants estimated the path distance, we found a tendency for underestimation of the curved path, similar to what we already observed in the previous experiment when participants were driving on the surface without looking up or down. This suggests that participants' maps of the space were not completely flattened (schematic representation in <xref ref-type="fig" rid="F1">Fig.1B</xref>) although such a flattened or topographic map could be the most efficient and adequate for the path estimation task. Similar to experiment 1, only a minority of participants reported that they imagined a flattened map during the distance estimation strategy. However, the influence of the 3D map was weak, only being observable in the direct comparison task, and not the slider task.</p><p id="P43">When participants estimated the Euclidean distance, the curve was reliably reported as shorter than the linear distance in both the comparison and slider tasks. Previously, we only found a significant difference between the curved and linear surfaces in the direct comparison task, and not in the slider task, which might be less sensitive to detect small differences between distances. This result fits our prediction that viewing behavior helps participants to build a global 3D representation of the environment. However, a direct comparison between the Euclidean distance estimate between the previous experiment and the current experiment didn't yield a statistically significant difference (t(70)=1.1, p=0.3, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 7</xref>). Also, we note that the participants' Euclidean distance estimation was still not perfect. First, the estimated ratio between the curve and linear distance in the slider task (median curve/linear = 0.79) was noticeably larger than the true ratio (0.48). Although, we should bear in mind that the distance estimate reported in the slider task does not necessarily have a strict linear relationship to the true distance. Second, when all pairs of between-object distances were considered, the correlation between the estimated and true Euclidean distance (ρ = 0.49 ±0.26) was significant, but not as strongly as the path estimation group who showed a very strong correlation (ρ = 0.65 ±0.34). However, it should be considered that Euclidean distance estimation can be inherently more difficult to report than the path distance because the range of the true Euclidean distance was smaller than the range of path distance (e.g. the maximum path distance between the objects was 2.0 whereas the maximum Euclidean distance was 1.3). Further, there was an additional confounding factor of the curved surface acting as a natural physical barrier, rendering the straight Euclidean distance estimation difficult (e.g. participants could not directly see location B when they were standing at location G because the curved surface was not transparent, <xref ref-type="fig" rid="F1">Fig. 1A</xref>). As we already discussed in Experiment 1, barriers are known to deteriorate the distance estimation (<xref ref-type="bibr" rid="R40">Newcombe &amp; Liben, 1982</xref>).</p><p id="P44">In sum, we again found that a completely flattened map was not utilized, although only the location and distance on the surface was behaviorally relevant. Participants could reliably distinguish the Euclidean distance on the curve and linear section, but the Euclidean distance estimation performance still seemed suboptimal. Might this mean that humans are inherently poor at building a 3D model of the world and distance estimation? In the following experiment we asked whether participants would be better at estimating Euclidean distance if they could freely fly in the virtual environment.</p></sec></sec><sec id="S30"><label>4</label><title>Experiment 3</title><sec id="S31" sec-type="intro"><label>4.1</label><title>Introduction</title><p id="P45">In Experiment 3, we removed the constraints of driving on the surface for participants. The objects that participants had to learn were still lying on the surface as in the previous two experiments, however, participants explored the environment with a flying motion. In contrast to Experiment 1 and 2, participants could directly move between the two points in 3D space along the straight line, as long as there was no natural barrier in the way. For example, a participant could fly along the nearly shortest (Euclidean) route from A to D. They still needed to take a small detour around the edge of the curved surface if they wanted to fly from A to B (<xref ref-type="fig" rid="F5">Fig. 5</xref>). The flying route from A to B was still significantly shorter than the equivalent driving route on the surface. In this flying setup we wanted to test whether participants would build a more volumetric representation of the environment and show better Euclidean distance estimation. Therefore, we only tested Euclidean distance, not path distance on the surface, in this experiment.</p></sec><sec id="S32" sec-type="methods"><label>4.2</label><title>Method</title><sec id="S33" sec-type="subjects"><label>4.2.1</label><title>Participants</title><p id="P46">Participants were recruited from the same online platform (<ext-link ext-link-type="uri" xlink:href="https://www.prolific.com/">www.prolific.com</ext-link>). Forty-one participants (female = 11, mean age = 23.5 ±4.5 years) completed the experiment. None of them took part in the previous experiments.</p></sec><sec id="S34"><label>4.2.2</label><title>Virtual environment</title><p id="P47">The identical environment was used as in the Experiment 1 and 2.</p></sec><sec id="S35"><label>4.2.3</label><title>Task and analysis</title><p id="P48">Participants completed the familiarization, object-location learning and test, two types of the distance estimation tasks, and the debriefing. The object locations, starting location and heading of the participants, trial structure, and experimental sequences were all identical to the previous experiments. The key change from the previous experiment was that participants could rotate, not only horizontally around the longitudinal body axis (yaw rotation), but also vertically around the side-to-side axis (pitch rotation). They could then move forwards or backwards in the direction the were facing. For instance, if they looked 45° upwards and pressed the forward movement key, they would move 45° upwards while maintaining the tilted heading. An example trajectory and a participant's view in the flying condition can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Video 5 and Supplementary Fig. 6</xref>. Of note, there is one more degree of freedom for rotation in 3D, called the roll rotation, which allows agents to rotate around the front-to-back axis (e.g. left ear down, right ear up). However, controlling all 3 rotation axes using a keyboard or mouse with desktop-based VR is complicated and the roll rotation does not affect the movement direction. It is also known that humans tend to keep upright posture with zero roll (<xref ref-type="bibr" rid="R2">Barnett-Cowan &amp; Bulthoff, 2013</xref>) and even flying bats have only a small number of cells tuned to roll rotation (<xref ref-type="bibr" rid="R15">Finkelstein et al., 2015</xref>). Thus, we only included the yaw and pitch components of rotation.</p><p id="P49">Controlling 3D rotation and flying movement can be more difficult compared to controlling driving motion in the previous experiments, so we allowed more time for the familiarization phase and object-location learning and test phase in the virtual environment (the time limit was set to 120 seconds from 60 seconds). We also increased the maximal number of repetitions per object from 8 to 10 during the object-location memory test. We do not think that this modification in a maximum number of repetitions per objects or time limit provided an advantage in object-location learning in the current experiment compared to previous experiments, because as we describe in the following result section (Section <xref ref-type="sec" rid="S37">4.3.1</xref>), the mean distance error and the mean number of repetitions was comparable to previous experiments. The time required for movement practice in the familiarization phase (201 ±70 seconds) and the mean trial duration during the object-location memory test (12.1 ±5.9 seconds) was longer than the previous driving experiments (c.f. mean familiarization duration = 126 ±57 seconds, mean object-location trial duration = 8.5 ±3.1 seconds, Experiment 1 and 2 combined), probably due to difficulty of controlling flying motion.</p></sec></sec><sec id="S36" sec-type="results"><label>4.3</label><title>Result</title><sec id="S37"><label>4.3.1</label><title>Object location memory</title><p id="P50">Similar to the previous driving experiments, most participants placed all objects in the close vicinity of the correct location (mean repetition per object = 5.8 ±1.3, mean distance error = 0.10 ±0.05 for all 41 participants, all distances are reported as relative to the short axis of the environment, <xref ref-type="fig" rid="F5">Fig. 5</xref>). Further, only those who remembered each of the 8 objects within a specific learning criterion (Euclidean distance error of max. 0.25) were included in the analysis, leaving n = 35. The final displacement error was larger for objects at the curved part compared to objects on the flat part (error for the objects on the curve = 0.10 ±0.03 vs. flat = 0.07 ±0.02, t(34) = 5.2, p &lt; 0.001). This was not surprising because larger displacement error was possible along the curved surface, when participants were flying, due to their oblique trajectories toward the targets, whereas the objects on the flat surface could be still approached as if participants were driving with near zero height from the plane (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 6</xref>).</p></sec><sec id="S38"><label>4.3.2</label><title>Euclidean distance estimation</title><p id="P51">Similar to the previous experiments, most participants passed the manipulation check (easy trials) for the distance estimation task (34 out of 35). The accuracy for comparing the curve and linear trials was also high (82 ±28%, sign-rank test, Z = 4.4, p &lt; 0.001, <xref ref-type="fig" rid="F6">Fig. 6A</xref>). In the slider task, participants showed a good fit between the estimated and correct Euclidean distance (ρ = 0.49 ±0.26, <xref ref-type="fig" rid="F6">Fig. 6B</xref>). The curve distance was estimated as significantly shorter than the linear distance (curve estimate = 0.35 ±0.15 vs. linear = 0.57 ±0.16, t(34) = 5.2, p &lt; 0.001, <xref ref-type="fig" rid="F6">Fig. 6C</xref>). It is noteworthy that the magnitude of the difference between the curve and linear distance estimates in this experiment was significantly larger than the difference observed in the previous experiments with driving (1-way ANOVA, F(2,104)=5.3, p=0.007, post-hoc pairwise comparison, Exp1 vs. Exp3, t(70)=3.3, p=0.002 (uncorrected); Exp2 vs. Exp3, t(68)=2.0, p=0.04 (uncorrected), <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 7</xref>). Further, the curve/linear ratio was comparable to the true Euclidean distance ratio (true = 0.40, the current flying experiment = 0.57, previous driving/looking experiment = 0.79).</p></sec></sec><sec id="S39" sec-type="discussion"><label>4.4</label><title>Discussion</title><p id="P52">As we hypothesized, participants who explored the environment with flying motion were better at estimating the Euclidean distance, compared to the previous experiments where participants explored the environment with driving motion. We believe that the experience of 3D rotation and movement encouraged participants to build a more appropriate 3D model of the environment, enabling a better distance estimation. Distance estimation could also have directly benefited from temporal memory of the travel distance. Humans can estimate distance using both static information (e.g. visual, auditory depth cues) and dynamic information (e.g. optic flow, self-motion)(<xref ref-type="bibr" rid="R50">Sun et al., 2004</xref>). In the previous driving experiments, participants could estimate the Euclidean distance on the curved surface using only the static depth cue or abstract representation of the environment, whereas the flying group could also recall the temporal duration (of getting from point A to point B) while estimating the Euclidean spatial distance. It is known that some people rely more on an implicit, time-based method during spatial distance estimation tasks (<xref ref-type="bibr" rid="R36">Mossio et al., 2008</xref>).</p><p id="P53">Of note, we noticed that the overall correlation between the estimated and true Euclidean distance, for all pairs, was significantly positive but still not as strong as between the estimated and true path distances in the previous driving experiments. This implies that there is still room for improvement in the Euclidean distance estimation. We think that the transparent wall surrounding the surface and the curved surface itself rendered the Euclidean distance estimation more difficult for particular pairs of objects such as G and B or G and D. To move between these pairs, participants had to take a detour around the wall (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 6</xref>). It has been shown that physical or contextual boundaries can divide space into compartments, hindering Euclidean distance judgments (<xref ref-type="bibr" rid="R22">Han &amp; Becker, 2014</xref>; <xref ref-type="bibr" rid="R24">Hirtle &amp; Jonides, 1985</xref>).</p></sec></sec><sec id="S40"><label>5</label><title>General discussion</title><p id="P54">Humans and most mammals dwell and navigate on 2D surfaces embedded within the 3D world. Terrains can have rather complex 3D profiles with bumps and holes, but a vast majority of previous research on spatial cognition was focused on horizontal flat surfaces. The 2D Cartesian map has become the predominant cognitive map in neuroscience research. In the current study we used a novel 3D virtual environment containing a curved surface to test whether humans represent such an environment using a 2D flattened map or a more volumetric 3D map. While a few previous studies have investigated participants' sense of orientation in 3D space using a maze consisting of narrow vertical and horizontal tracks (<xref ref-type="bibr" rid="R28">Indovina et al., 2016</xref>; <xref ref-type="bibr" rid="R57">Vidal et al., 2004</xref>, <xref ref-type="bibr" rid="R58">2006</xref>), slanted surfaces (<xref ref-type="bibr" rid="R38">Nardi et al., 2011</xref>; <xref ref-type="bibr" rid="R46">Restat et al., 2004</xref>; <xref ref-type="bibr" rid="R48">Steck et al., 2003</xref>), or a multilevel building (<xref ref-type="bibr" rid="R8">Brandt et al., 2015</xref>; <xref ref-type="bibr" rid="R31">Kim &amp; Maguire, 2018</xref>; <xref ref-type="bibr" rid="R34">Montello &amp; Pick, 1993</xref>), spatial representations of large, navigable, curved surfaces remained scarce.</p><p id="P55">The first main finding of the current study was that participants were aware of the global 3D layout of the environment despite their movement being restricted to the surface and despite the fact that the object-location memory task could be solved with 2D coordinates on the surface (Experiment 1 and 2). This implies that humans do not necessarily extract the pure topographic, relational knowledge (e.g. links between the key object locations) and store them in a 2D Cartesian map. Rather, they also hold information about the 3D world. It might seem somewhat suboptimal from a computational perspective, considering that the brain can extract a low-dimensional structure in a complex high-dimensional world, e.g. a principal component of high-dimensional stimulus space (<xref ref-type="bibr" rid="R10">Chang &amp; Tsao, 2017</xref>; <xref ref-type="bibr" rid="R49">Summerfield et al., 2020</xref>). However, the automatic encoding of a 3D layout could be the natural behavior given the saliency of 3D cues and the potential benefit of acquiring 3D knowledge for future navigational problems. This might be related to the ability of animals to take 2D vector shortcuts after only following 1D routes (<xref ref-type="bibr" rid="R56">Tolman, 1948</xref>). Previous behavioral experiments have shown that people use the vertical axis of a slant surface as an orientation cue to facilitate their spatial memory (<xref ref-type="bibr" rid="R38">Nardi et al., 2011</xref>, <xref ref-type="bibr" rid="R39">2021</xref>; <xref ref-type="bibr" rid="R46">Restat et al., 2004</xref>; <xref ref-type="bibr" rid="R48">Steck et al., 2003</xref>). Importantly, participants in our studies did not have any vestibular inputs, nor did they experience energy cost along the curved surface because we let participants move with constant speed in our virtual environment as if there is no gravity, in contrast to abovementioned experiments where participants experienced the energy cost when moving upward along the slope in the real world. The absence of vestibular inputs and an artificial environment is a limitation of the current study, but we could learn that the visual cues alone (e.g. sky, mountain landscape) are sufficient to trigger automatic encoding of the vertical or orthogonal axis to the surface of locomotion.</p><p id="P56">If the participants did not build a completely flattened 2D map, did they instead build a metric 3D map, containing precise Euclidean distance and angle information between the locations? This is also unlikely given the suboptimal 3D Euclidean distance estimation performance, even in the flying condition (Experiment 3). We propose a mixture of a metric map and topographic knowledge as the most likely form of spatial representation. Originally, O'Keefe and Nadel made a strong claim that the cognitive map is Euclidean (<xref ref-type="bibr" rid="R42">O’Keefe &amp; Nadel, 1978</xref>). However, many studies have since challenged the notion of a strict metric or Euclidean map. Behaviorally, Warren and colleagues have shown that participants successfully navigate in a virtual environment where invisible wormholes break the rules of 2D Euclidean geometry (<xref ref-type="bibr" rid="R14">Ericson &amp; Warren, 2020</xref>; <xref ref-type="bibr" rid="R61">Warren et al., 2017</xref>). Interestingly, the participants did not even notice the peculiarity of the environment, ruling out the necessity for Euclidean map knowledge. Neurally, hippocampal place cells change their receptive fields when the environment changes its shape and size by stretching and shearing, but preserve the relative location or topological information about the environment (<xref ref-type="bibr" rid="R13">Dabaghian et al., 2014</xref>; <xref ref-type="bibr" rid="R41">O’Keefe &amp; Burgess, 1996</xref>). Again, these results imply that space is not encoded in the brain like a precise Cartesian coordinate system with exact distance and angle from an origin. Alternatively, the model of the environment can be graph-like, consisting of nodes and links, and metric information about distance and angle between the nodes or landmarks is locally embedded, rather than forming a globally coherent 3D metric map (<xref ref-type="bibr" rid="R45">Peer et al., 2020</xref>; <xref ref-type="bibr" rid="R60">Warren, 2019</xref>). Combining the graph knowledge and local metric information can be used in robotics and artificial agents (<xref ref-type="bibr" rid="R27">Hübner &amp; Mallot, 2007</xref>; <xref ref-type="bibr" rid="R33">Mallot &amp; Basten, 2009</xref>). On a related note, Glennerster proposed that an observer in a 3D world does not need to reconstruct a 3D scene, rather, they could use the representation somewhere in between the 3D reconstruction and a more 2D image-based representation, which is updated upon movement of the observer in space (<xref ref-type="bibr" rid="R17">Glennerster, 2016</xref>). For instance, translation and rotation of vantage points provides information on the distance, slant, and depth of surfaces without necessarily reconstructing the perfect 3D scene. In our experiment, the most likely encoding scenario would be to remember the fine-scale relative location on the surface while holding rather coarse information about the layout of the surface within a global 3D world.</p><p id="P57">When we study the mental model of the world used by people, we should not underestimate the importance of how they explore the environment. In the present study, participants explored the identical 3D environment with varying modes of exploration, from driving alone (Experiment 1) to driving with vertical viewing (Experiment 2), to flying (Experiment 3) and we observed the enhanced 3D Euclidean distance estimation performance with the increased degree of freedom in 3D movement. From an ecological psychology perspective, active interaction between the observer and the environment should be given more importance than a static and rigid representation of the external world (<xref ref-type="bibr" rid="R12">Costall, 1984</xref>). Previous research in 2D flat environments has shown that the accuracy of spatial memory and shortcut behavior is dependent on which perspective participants took while learning the environment (1st-person-perspective, birds eye view, or hybrid slanted perspective) (<xref ref-type="bibr" rid="R3">Barra et al., 2012</xref>). At the neural level, the firing pattern of place cells is not only determined by the location and physical environment but also by trajectories and goal locations (<xref ref-type="bibr" rid="R20">Grieves et al., 2016</xref>). Therefore, it is crucial to consider the complexity of exploratory behavior and its ecological validity when we want to understand the mental representation of the external environment, be it flat or curved, 2D or 3D.</p><p id="P58">In conclusion, our study provided novel insights on human spatial memory and mental map formation of curved surfaces embedded within a 3D world. The cognitive map is neither completely reduced to 2D nor is it fully 3D. Rather, it is somewhere in between. We believe that the representation of the environment is flexible and adapted to behavioral experience and demand, such as how participants are interacting with the environment (e.g. driving or flying) and which type of spatial information needs to be recalled (e.g. object location, path, or Euclidean distance). Furthermore, our study encourages investigation of more general cognitive maps beyond 2D Euclidean space. It has been proposed that the neural mechanisms encoding navigable space, such as place codes and grid codes, can serve as general coding principles for encoding abstract knowledge space (<xref ref-type="bibr" rid="R4">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="R5">Bellmund et al., 2018</xref>; <xref ref-type="bibr" rid="R54">Theves et al., 2019</xref>, <xref ref-type="bibr" rid="R55">2020</xref>). Despite non-physical space having many more dimensions, not all of which are independent from one another, previous literature has focused 2 dimensions. These 'abstract spaces' have consisted of 2 orthogonal feature dimensions such as two independent smells, lengths of visual features, or personal characteristics (<xref ref-type="bibr" rid="R1">Bao et al., 2019</xref>; <xref ref-type="bibr" rid="R11">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="R44">Park et al., 2020</xref>; <xref ref-type="bibr" rid="R53">Tavares et al., 2015</xref>; <xref ref-type="bibr" rid="R59">Viganò &amp; Piazza, 2020</xref>), or alternatively, the conceptionally relevant 2D representation within a 3D feature space (<xref ref-type="bibr" rid="R55">Theves et al., 2020</xref>), which is analogous to a 2D flat surface. In due course, we hope to gain a better understanding of how humans develop their internal representation of high-dimensional, non-physical space, which contains a low-dimensional structure, such as the cognitive map for a 2D curved surface within a 3D abstract world.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS144460-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d2aAeFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S41"><title>Acknowledgement</title><p>This work has been supported by the Max Planck Society. CFD's research is further supported by the European Research Council (ERC-CoG GEOCOG 724836), the Kavli Foundation, and the Jebsen Foundation.</p></ack><sec id="S42" sec-type="data-availability"><title>Data and code availability</title><p id="P59">All data and analysis scripts are available in OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/gsnyx/">https://osf.io/gsnyx/</ext-link>).</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P60"><bold>Author contributions</bold></p><p id="P61">MK and CFD developed the study concept and wrote the manuscript. MK programmed the experiment, collected, analyzed the data.</p></fn><fn id="FN2" fn-type="conflict"><p id="P62"><bold>Declaration of interest</bold></p><p id="P63">Authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>X</given-names></name><name><surname>Gjorgieva</surname><given-names>E</given-names></name><name><surname>Shanahan</surname><given-names>LK</given-names></name><name><surname>Howard</surname><given-names>JD</given-names></name><name><surname>Kahnt</surname><given-names>T</given-names></name><name><surname>Gottfried</surname><given-names>JA</given-names></name></person-group><article-title>Grid-like Neural Representations Support Olfactory Navigation of a Two-Dimensional Odor Space</article-title><source>Neuron</source><year>2019</year><volume>102</volume><issue>5</issue><fpage>1066</fpage><lpage>1075</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.03.034</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett-Cowan</surname><given-names>M</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><article-title>Human path navigation in a three-dimensional world</article-title><source>Behavioral and Brain Sciences</source><year>2013</year><volume>36</volume><issue>5</issue><fpage>544</fpage><lpage>545</lpage><pub-id pub-id-type="doi">10.1017/S0140525X13000319</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barra</surname><given-names>J</given-names></name><name><surname>Laou</surname><given-names>L</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Lebihan</surname><given-names>D</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><article-title>Does an Oblique/Slanted Perspective during Virtual Navigation Engage Both Egocentric and Allocentric Brain Strategies?</article-title><source>PLoS ONE</source><year>2012</year><volume>7</volume><issue>11</issue><pub-id pub-id-type="doi">10.1371/journal.pone.0049537</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><article-title>What Is a Cognitive Map? Organizing Knowledge for Flexible Behavior</article-title><source>Neuron</source><year>2018</year><volume>100</volume><issue>2</issue><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.002</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname><given-names>JLS</given-names></name><name><surname>Gärdenfors</surname><given-names>P</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>Navigating cognition: Spatial codes for human thinking</article-title><source>Science</source><year>2018</year><volume>362</volume><issue>6415</issue><pub-id pub-id-type="doi">10.1126/science.aat6766</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccia</surname><given-names>M</given-names></name><name><surname>Nemmi</surname><given-names>F</given-names></name><name><surname>Guariglia</surname><given-names>C</given-names></name></person-group><article-title>Neuropsychology of Environmental Navigation in Humans: Review and Meta-Analysis of fMRI Studies in Healthy Participants</article-title><source>Neuropsychology Review</source><year>2014</year><volume>24</volume><issue>2</issue><fpage>236</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/s11065-014-9247-8</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name></person-group><article-title>“A Right Door,” wrong floor: A canine deficiency in navigation</article-title><source>Hippocampus</source><year>2013</year><volume>23</volume><issue>4</issue><fpage>245</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1002/hipo.22091</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Huber</surname><given-names>M</given-names></name><name><surname>Schramm</surname><given-names>H</given-names></name><name><surname>Kugler</surname><given-names>G</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Glasauer</surname><given-names>S</given-names></name></person-group><article-title>“Taller and Shorter”: Human 3-D Spatial Memory Distorts Familiar Multilevel Buildings</article-title><source>PLOS ONE</source><year>2015</year><volume>10</volume><issue>10</issue><elocation-id>e0141257</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0141257</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calton</surname><given-names>JL</given-names></name></person-group><article-title>Degradation of Head Direction Cell Activity during Inverted Locomotion</article-title><source>Journal of Neuroscience</source><year>2005</year><volume>25</volume><issue>9</issue><fpage>2420</fpage><lpage>2428</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3511-04.2005</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>The Code for Facial Identity in the Primate Brain</article-title><source>Cell</source><year>2017</year><volume>169</volume><issue>6</issue><fpage>1013</fpage><lpage>1028</lpage><elocation-id>e14</elocation-id><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>AO</given-names></name><name><surname>OReilly</surname><given-names>JX</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><year>2016</year><volume>352</volume><issue>6292</issue><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costall</surname><given-names>AP</given-names></name></person-group><article-title>Are Theories of Perception Necessary? A Review of Gibson’s the Ecological Approach to Visual Perception</article-title><source>Journal of the Experimental Analysis of Behavior</source><year>1984</year><volume>41</volume><issue>1</issue><fpage>109</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1901/jeab.1984.41-109</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dabaghian</surname><given-names>Y</given-names></name><name><surname>Brandt</surname><given-names>VL</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Reconceiving the hippocampal map as a topological template</article-title><source>ELife</source><year>2014</year><volume>3</volume><elocation-id>e03476</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03476</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ericson</surname><given-names>JD</given-names></name><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><article-title>Probing the invariant structure of spatial knowledge: Support for the cognitive graph hypothesis</article-title><source>Cognition</source><year>2020</year><volume>200</volume><elocation-id>104276</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104276</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkelstein</surname><given-names>A</given-names></name><name><surname>Derdikman</surname><given-names>D</given-names></name><name><surname>Rubin</surname><given-names>A</given-names></name><name><surname>Foerster</surname><given-names>JN</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><article-title>Three-dimensional head-direction coding in the bat brain</article-title><source>Nature</source><year>2015</year><volume>517</volume><issue>7533</issue><fpage>159</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1038/nature14031</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ginosar</surname><given-names>G</given-names></name><name><surname>Aljadeff</surname><given-names>J</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Las</surname><given-names>L</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><article-title>Locally ordered representation of 3D space in the entorhinal cortex</article-title><source>Nature</source><year>2021</year><volume>596</volume><fpage>404</fpage><lpage>409</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03783-x</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glennerster</surname><given-names>A</given-names></name></person-group><article-title>A moving observer in a three-dimensional world</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2016</year><volume>371</volume><issue>1697</issue><elocation-id>20150265</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0265</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Jedidi-Ayoub</surname><given-names>S</given-names></name><name><surname>Mishchanchuk</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Renaudineau</surname><given-names>S</given-names></name><name><surname>Duvelle</surname><given-names>E</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>Irregular distribution of grid cell firing fields in rats exploring a 3D volumetric space</article-title><source>Nature Neuroscience</source><year>2021</year><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00907-4</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Jedidi-Ayoub</surname><given-names>S</given-names></name><name><surname>Mishchanchuk</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>A</given-names></name><name><surname>Renaudineau</surname><given-names>S</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>The place-cell representation of volumetric space in rats</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><issue>1</issue><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-14611-7</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grieves</surname><given-names>RM</given-names></name><name><surname>Wood</surname><given-names>ER</given-names></name><name><surname>Dudchenko</surname><given-names>PA</given-names></name></person-group><article-title>Place cells on a maze encode routes rather than destinations</article-title><source>ELife</source><year>2016</year><volume>5</volume><elocation-id>e15986</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.15986</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grobéty</surname><given-names>MC</given-names></name><name><surname>Schenk</surname><given-names>F</given-names></name></person-group><article-title>The influence of spatial irregularity upon radial-maze performance in the rat</article-title><source>Animal Learning &amp; Behavior</source><year>1992</year><volume>20</volume><issue>4</issue><fpage>393</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.3758/BF03197962</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name></person-group><article-title>One spatial map or many? Spatial coding of connected environments</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2014</year><volume>40</volume><issue>2</issue><fpage>511</fpage><lpage>531</lpage><pub-id pub-id-type="doi">10.1037/a0035259</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayman</surname><given-names>RMA</given-names></name><name><surname>Casali</surname><given-names>G</given-names></name><name><surname>Wilson</surname><given-names>JJ</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>Grid cells on steeply sloping terrain: Evidence for planar rather than volumetric encoding</article-title><source>Frontiers in Psychology</source><year>2015</year><volume>6</volume><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00925</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirtle</surname><given-names>SC</given-names></name><name><surname>Jonides</surname><given-names>J</given-names></name></person-group><article-title>Evidence of hierarchies in cognitive maps</article-title><source>Memory &amp; Cognition</source><year>1985</year><volume>13</volume><issue>3</issue><fpage>208</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.3758/BF03197683</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>CA</given-names></name><name><surname>Nardi</surname><given-names>D</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name><name><surname>Weisberg</surname><given-names>SM</given-names></name></person-group><article-title>Children’s Use of Slope to Guide Navigation: Sex Differences Relate to Spontaneous Slope Perception</article-title><source>Spatial Cognition &amp; Computation</source><year>2015</year><volume>15</volume><issue>3</issue><fpage>170</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1080/13875868.2015.1015131</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hölscher</surname><given-names>C</given-names></name><name><surname>Meilinger</surname><given-names>T</given-names></name><name><surname>Vrachliotis</surname><given-names>G</given-names></name><name><surname>Brösamle</surname><given-names>M</given-names></name><name><surname>Knauff</surname><given-names>M</given-names></name></person-group><article-title>Up the down staircase: Wayfinding strategies in multi-level buildings</article-title><source>Journal of Environmental Psychology</source><year>2006</year><volume>26</volume><issue>4</issue><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/j.jenvp.2006.09.002</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hübner</surname><given-names>W</given-names></name><name><surname>Mallot</surname><given-names>HA</given-names></name></person-group><article-title>Metric embedding of view-graphs: A vision and odometry-based approach to cognitive mapping</article-title><source>Autonomous Robots</source><year>2007</year><volume>23</volume><issue>3</issue><fpage>183</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/s10514-007-9040-0</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Indovina</surname><given-names>I</given-names></name><name><surname>Maffei</surname><given-names>V</given-names></name><name><surname>Mazzarella</surname><given-names>E</given-names></name><name><surname>Sulpizio</surname><given-names>V</given-names></name><name><surname>Galati</surname><given-names>G</given-names></name><name><surname>Lacquaniti</surname><given-names>F</given-names></name></person-group><article-title>Path integration in 3D from visual motion cues: A human fMRI study</article-title><source>NeuroImage</source><year>2016</year><volume>142</volume><fpage>512</fpage><lpage>521</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.008</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffery</surname><given-names>KJ</given-names></name><name><surname>Jovalekic</surname><given-names>A</given-names></name><name><surname>Verriotis</surname><given-names>M</given-names></name><name><surname>Hayman</surname><given-names>R</given-names></name></person-group><article-title>Navigating in a three-dimensional world</article-title><source>Behavioral and Brain Sciences</source><year>2013</year><volume>36</volume><issue>5</issue><fpage>523</fpage><lpage>543</lpage><pub-id pub-id-type="doi">10.1017/S0140525X12002476</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Multivoxel Pattern Analysis Reveals 3D Place Information in the Human Hippocampus</article-title><source>The Journal of Neuroscience</source><year>2017</year><volume>37</volume><issue>16</issue><fpage>4270</fpage><lpage>4279</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2703-16.2017</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><article-title>Hippocampus, Retrosplenial and Parahippocampal Cortices Encode Multicompartment 3D Space in a Hierarchical Manner</article-title><source>Cerebral Cortex</source><year>2018</year><volume>28</volume><issue>5</issue><fpage>1898</fpage><lpage>1909</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy054</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Layton</surname><given-names>OW</given-names></name><name><surname>O’Connell</surname><given-names>T</given-names></name><name><surname>Phillips</surname><given-names>F</given-names></name></person-group><article-title>The Traveling Salesman Problem in the Natural Environment</article-title><year>2010</year><volume>20</volume><pub-id pub-id-type="doi">10.1167/9.8.1145</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mallot</surname><given-names>HA</given-names></name><name><surname>Basten</surname><given-names>K</given-names></name></person-group><article-title>Embodied spatial cognition: Biological and artificial systems</article-title><source>Image and Vision Computing</source><year>2009</year><volume>27</volume><issue>11</issue><fpage>1658</fpage><lpage>1670</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2008.09.001</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montello</surname><given-names>DR</given-names></name><name><surname>Pick</surname><given-names>HL</given-names></name></person-group><article-title>Integrating Knowledge of Vertically Aligned Large-Scale Spaces</article-title><source>Environment and Behavior</source><year>1992</year><volume>35</volume><issue>3</issue><fpage>457</fpage><lpage>484</lpage><pub-id pub-id-type="doi">10.1177/0013916593253002</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><article-title>Spatial representation in the hippocampal formation: A history</article-title><source>Nature Neuroscience</source><year>2017</year><volume>20</volume><issue>11</issue><fpage>1448</fpage><lpage>1464</lpage><pub-id pub-id-type="doi">10.1038/nn.4653</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mossio</surname><given-names>M</given-names></name><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><article-title>Traveled distances: New insights into the role of optic flow</article-title><source>Vision Research</source><year>2008</year><volume>48</volume><issue>2</issue><fpage>289</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2007.11.015</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nardi</surname><given-names>D</given-names></name><name><surname>Bingman</surname><given-names>VP</given-names></name></person-group><article-title>Pigeon (Columba livia) encoding of a goal location: The relative importance of shape geometry and slope information</article-title><source>Journal of Comparative Psychology</source><year>2009</year><volume>123</volume><issue>2</issue><fpage>204</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1037/a0015093</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nardi</surname><given-names>D</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name><name><surname>Shipley</surname><given-names>TF</given-names></name></person-group><article-title>The world is not flat: Can people reorient using slope?</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2011</year><volume>37</volume><issue>2</issue><fpage>354</fpage><lpage>367</lpage><pub-id pub-id-type="doi">10.1037/a0021614</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nardi</surname><given-names>D</given-names></name><name><surname>Singer</surname><given-names>KJ</given-names></name><name><surname>Price</surname><given-names>KM</given-names></name><name><surname>Carpenter</surname><given-names>SE</given-names></name><name><surname>Bryant</surname><given-names>JA</given-names></name><name><surname>Hatheway</surname><given-names>MA</given-names></name><name><surname>Johnson</surname><given-names>JN</given-names></name><name><surname>Pairitz</surname><given-names>AK</given-names></name><name><surname>Young</surname><given-names>KL</given-names></name><name><surname>Newcombe</surname><given-names>NS</given-names></name></person-group><article-title>Navigating without vision: Spontaneous use of terrain slant in outdoor place learning</article-title><source>Spatial Cognition &amp; Computation</source><year>2021</year><volume>21</volume><issue>3</issue><fpage>235</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1080/13875868.2021.1916504</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newcombe</surname><given-names>N</given-names></name><name><surname>Liben</surname><given-names>LS</given-names></name></person-group><article-title>Barrier effects in the cognitive maps of children and adults</article-title><source>Journal of Experimental Child Psychology</source><year>1982</year><volume>34</volume><issue>1</issue><fpage>46</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/0022-0965(82)90030-3</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>Geometric determinants of the place fields of hippocampal neurons</article-title><source>Nature</source><year>1996</year><volume>381</volume><issue>6581</issue><fpage>425</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/381425a0</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><source>The hippocampus as a cognitive map</source><year>1978</year><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>HJI</given-names></name><name><surname>Wilson</surname><given-names>JJ</given-names></name><name><surname>Jeffery</surname><given-names>KJ</given-names></name></person-group><article-title>A dual-axis rotation rule for updating the head direction cell reference frame during movement in three dimensions</article-title><source>Journal of Neurophysiology</source><year>2018</year><volume>119</volume><issue>1</issue><fpage>192</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1152/jn.00501.2017</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>DS</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name></person-group><article-title>Map Making: Constructing, Combining, and Inferring on Abstract Cognitive Maps</article-title><source>Neuron</source><year>2020</year><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.030</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peer</surname><given-names>M</given-names></name><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Newcombe</surname><given-names>N</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><article-title>Structuring Knowledge with Cognitive Maps and Cognitive Graphs</article-title><source>Trends in Cognitive Science</source><year>2020</year><pub-id pub-id-type="doi">10.1016/j.tics.2020.10.004</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Restat</surname><given-names>JD</given-names></name><name><surname>Steck</surname><given-names>SD</given-names></name><name><surname>Mochnatzki</surname><given-names>HF</given-names></name><name><surname>Mallot</surname><given-names>HA</given-names></name></person-group><article-title>Geographical Slant Facilitates Navigation and Orientation in Virtual Environments</article-title><source>Perception</source><year>2004</year><volume>33</volume><issue>6</issue><fpage>667</fpage><lpage>687</lpage><pub-id pub-id-type="doi">10.1068/p5030</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stackman</surname><given-names>RW</given-names></name><name><surname>Taube</surname><given-names>JS</given-names></name></person-group><article-title>Firing Properties of Rat Lateral Mammillary Single Units: Head Direction, Head Pitch, and Angular Head Velocity</article-title><source>The Journal of Neuroscience</source><year>1998</year><volume>18</volume><issue>21</issue><fpage>9020</fpage><lpage>9037</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-21-09020.1998</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Steck</surname><given-names>SD</given-names></name><name><surname>Mochnatzki</surname><given-names>HF</given-names></name><name><surname>Mallot</surname><given-names>HA</given-names></name></person-group><chapter-title>The Role of Geographical Slant in Virtual Environment Navigation</chapter-title><person-group person-group-type="editor"><name><surname>Freksa</surname><given-names>C</given-names></name><name><surname>Brauer</surname><given-names>W</given-names></name><name><surname>Habel</surname><given-names>C</given-names></name><name><surname>Wender</surname><given-names>KF</given-names></name></person-group><source>Spatial Cognition III</source><year>2003</year><volume>2685</volume><fpage>62</fpage><lpage>76</lpage><publisher-name>Springer</publisher-name><publisher-loc>Berlin Heidelberg</publisher-loc><pub-id pub-id-type="doi">10.1007/3-540-45004-1_4</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Luyckx</surname><given-names>F</given-names></name><name><surname>Sheahan</surname><given-names>H</given-names></name></person-group><article-title>Structure learning and the posterior parietal cortex</article-title><source>Progress in Neurobiology</source><year>2020</year><volume>184</volume><pub-id pub-id-type="doi">10.1016/j.pneurobio.2019.101717</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>H-J</given-names></name><name><surname>Campos</surname><given-names>JL</given-names></name><name><surname>Young</surname><given-names>M</given-names></name><name><surname>Chan</surname><given-names>GSW</given-names></name><name><surname>Ellard</surname><given-names>CG</given-names></name></person-group><article-title>The Contributions of Static Visual Cues, Nonvisual Cues, and Optic Flow in Distance Estimation</article-title><source>Perception</source><year>2004</year><volume>33</volume><issue>1</issue><fpage>49</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1068/p5145</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name><name><surname>Stackman</surname><given-names>RW</given-names></name><name><surname>Calton</surname><given-names>JL</given-names></name><name><surname>Oman</surname><given-names>CM</given-names></name></person-group><article-title>Rat Head Direction Cell Responses in Zero-Gravity Parabolic Flight</article-title><source>Journal of Neurophysiology</source><year>2004</year><volume>92</volume><issue>5</issue><fpage>2887</fpage><lpage>2997</lpage><pub-id pub-id-type="doi">10.1152/jn.00887.2003</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taube</surname><given-names>JS</given-names></name><name><surname>Wang</surname><given-names>SS</given-names></name><name><surname>Kim</surname><given-names>SY</given-names></name><name><surname>Frohardt</surname><given-names>RJ</given-names></name></person-group><article-title>Updating of the spatial reference frame of head direction cells in response to locomotion in the vertical plane</article-title><source>Journal of Neurophysiology</source><year>2013</year><volume>109</volume><issue>3</issue><fpage>873</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1152/jn.00239.2012</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavares</surname><given-names>RM</given-names></name><name><surname>Mendelsohn</surname><given-names>A</given-names></name><name><surname>Grossman</surname><given-names>Y</given-names></name><name><surname>Williams</surname><given-names>CH</given-names></name><name><surname>Shapiro</surname><given-names>M</given-names></name><name><surname>Trope</surname><given-names>Y</given-names></name><name><surname>Schiller</surname><given-names>D</given-names></name></person-group><article-title>A Map for Social Navigation in the Human Brain</article-title><source>Neuron</source><year>2015</year><volume>87</volume><issue>1</issue><fpage>231</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.011</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theves</surname><given-names>S</given-names></name><name><surname>Fernández</surname><given-names>G</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>The Hippocampus Encodes Distances in Multidimensional Feature Space</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><issue>7</issue><pub-id pub-id-type="doi">10.1016/j.cub.2019.02.035</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theves</surname><given-names>S</given-names></name><name><surname>Fernandez</surname><given-names>G</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><article-title>The Hippocampus Maps Concept Space, Not Feature Space</article-title><source>Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>38</issue><fpage>7318</fpage><lpage>7325</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0494-20.2020</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><year>1948</year><volume>55</volume><issue>4</issue><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Amorim</surname><given-names>M-A</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><article-title>Navigating in a virtual three-dimensional maze: How do egocentric and allocentric reference frames interact?</article-title><source>Cognitive Brain Research</source><year>2004</year><volume>19</volume><issue>3</issue><fpage>244</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.cogbrainres.2003.12.006</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidal</surname><given-names>M</given-names></name><name><surname>Amorim</surname><given-names>M-A</given-names></name><name><surname>McIntyre</surname><given-names>J</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><article-title>The perception of visually presented yaw and pitch turns: Assessing the contribution of motion, static, and cognitive cues</article-title><source>Perception &amp; Psychophysics</source><year>2006</year><volume>68</volume><issue>8</issue><fpage>1338</fpage><lpage>1350</lpage><pub-id pub-id-type="doi">10.3758/BF03193732</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viganò</surname><given-names>S</given-names></name><name><surname>Piazza</surname><given-names>M</given-names></name></person-group><article-title>Distance and direction codes underlie navigation of a novel semantic space in the human brain</article-title><source>The Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>13</issue><fpage>1849</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1523/jneurosci.1849-19.2020</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name></person-group><article-title>Non-Euclidean navigation</article-title><source>The Journal of Experimental Biology</source><year>2019</year><volume>222</volume><issue>Suppl 1</issue><elocation-id>jeb187971</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.187971</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warren</surname><given-names>WH</given-names></name><name><surname>Rothman</surname><given-names>DB</given-names></name><name><surname>Schnapp</surname><given-names>BH</given-names></name><name><surname>Ericson</surname><given-names>JD</given-names></name></person-group><article-title>Wormholes in virtual space: From cognitive maps to cognitive graphs</article-title><source>Cognition</source><year>2017</year><volume>166</volume><fpage>152</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2017.05.020</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>JJ</given-names></name><name><surname>Harding</surname><given-names>E</given-names></name><name><surname>Fortier</surname><given-names>M</given-names></name><name><surname>James</surname><given-names>B</given-names></name><name><surname>Donnett</surname><given-names>M</given-names></name><name><surname>Kerslake</surname><given-names>A</given-names></name><name><surname>O’Leary</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>N</given-names></name><name><surname>Jeffery</surname><given-names>K</given-names></name></person-group><article-title>Spatial learning by mice in three dimensions</article-title><source>Behavioural Brain Research</source><year>2015</year><volume>289</volume><fpage>125</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2015.04.035</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The virtual environment and exploration methods used in the three experiments.</title><p><bold>A.</bold> The virtual environment consisted of a flat square and a curved surface with an arc length of 270 degrees. The length of the square was identical to the arc length of the curve, so that between-object path distances (red lines) were identical on the curved and flat surfaces (e.g. AB=AG). In contrast, 3D Euclidean distances between the objects (blue lines) were shorter on the curve (e.g. AB&lt;AG). The virtual environment also contained semi-transparent walls at the rim of the surface which prevented participants from moving beyond the surface (not shown here for visibility). <bold>B.</bold> The same environment can be represented in an abstract, flattened 2D map. <bold>C.</bold> Participants explored this environment from a first-person-perspective. <bold>D.</bold> Schematic figures illustrating the movement method. In Experiment 1 participants drove on the surface and always remained parallel to the tangent of the surface. In Experiment 2 participants could also look up and down while driving. In Experiment 3, participants could freely fly.</p></caption><graphic xlink:href="EMS144460-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The object-location memory task.</title><p><bold>A.</bold> In each trial, a picture cue was shown at the beginning and participants moved to the remembered location of the object (retrieval). After the feedback, participants collected the object, which reappeared at the correct location. <bold>B.</bold> Most participants remembered the location well at the end of the test phase (mean distance error &lt; 0.1) and the distance errors for the objects on the curve (B, C, D) and the flat (F, G, H) sections were not different. Colored dots indicate the remembered location of objects in the last trial for all participants; black circle indicate the true object location. Left, 3D view; right, flattened view. Distance was normalized to the length of the short axis of the surface.</p></caption><graphic xlink:href="EMS144460-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Distance estimation task in Experiment 1 (driving).</title><p><bold>A.</bold> In the comparison task, participants chose whether the first or second object was closer to the top object from object triplets. <bold>B.</bold> In the slider task, participants reported the relative distance of all object pairs using the continuous scale. <bold>C-E.</bold> Results from the path group. <bold>C.</bold> There was an underestimation bias for the curve path, as shown by over 50% of responses choosing the curve path as shorter than the linear path. Black dots = individual participants; red line = chance level. <bold>D.</bold> In the slider task, participants' estimated path distances were highly correlated with the true path distances. Each dot = group mean distance estimate for the unique object pair. <bold>E.</bold> The mean path estimate for the curve was slightly but significantly shorter than the linear one. Dots = individual participant's distance estimate between 0 ("very close") and 1 ("very far"). <bold>F-G.</bold> Results from the Euclidean group. <bold>F.</bold> Participants correctly reported the curve distance as shorter than the linear distance during the comparison task. <bold>G.</bold> In the slider task the estimated distances were positively correlated with true Euclidean distance. <bold>H.</bold> Curved distance estimate was not significantly different from the linear distance. All error bars are group SE.</p></caption><graphic xlink:href="EMS144460-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Distance estimate results in Experiment 2 (driving/looking).</title><p><bold>A.</bold> In the comparison task, participants showed a tendency for underestimating the path distance on the curve (p = 0.051). Black dots = individual participant; red line = chance level. <bold>B.</bold> Participant's path distance estimates were highly correlated with the true path distance. Each dot = group mean distance estimate for unique object pair. <bold>C.</bold> The mean path estimate for the curve was not significantly different from the linear one. Dots = individual participant's distance estimate between 0 ("very close") and 1 ("very far"). <bold>D.</bold> Participants correctly reported the curve distance as shorter than linear during the comparison task. <bold>E.</bold> In the slider task the estimated distances were positively correlated with true Euclidean distance. <bold>F.</bold> Curved distance estimate was significantly shorter than the linear distance. All error bars are group SE.</p></caption><graphic xlink:href="EMS144460-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>Object-location memory test result in Experiment 3. Remembered object locations in the last trial of all participants are shown in 3D view (left) and side view (right). Overall, remembered locations (color dots) were very close to the true location (black circles), but the errors were larger for the locations (B, C, D) on the curve side than the locations on the flat side (F, G, H).</p></caption><graphic xlink:href="EMS144460-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Distance estimation result in Experiment 3 (flying).</title><p><bold>A.</bold> Participants showed high accuracy for the comparison task. <bold>B.</bold> In the slider task, the estimated distances were positively correlated with true Euclidean distance. <bold>C.</bold> Curved distance estimates were significantly shorter than the linear distance. All error bars are group SE.</p></caption><graphic xlink:href="EMS144460-f006"/></fig></floats-group></article>