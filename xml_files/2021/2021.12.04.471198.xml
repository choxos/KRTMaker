<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="preprint">
<?all-math-mml yes?>
<?use-mml?>
<?origin ukpmcpa?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">bioRxiv</journal-id>
<journal-title-group>
<journal-title>bioRxiv : the preprint server for biology</journal-title>
</journal-title-group>
<issn pub-type="ppub"/>
</journal-meta>
<article-meta>
<article-id pub-id-type="manuscript">EMS141083</article-id>
<article-id pub-id-type="doi">10.1101/2021.12.04.471198</article-id>
<article-id pub-id-type="archive">PPR428757</article-id>
<article-version article-version-type="publisher-id">1</article-version>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Modeling the trajectory of SARS-CoV-2 spike protein evolution in continuous latent space using a neural network and Gaussian process</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>King</surname>
<given-names>Samuel</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Xinyi E.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ng</surname>
<given-names>Sarah W. S.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rostin</surname>
<given-names>Kimia</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Roberts</surname>
<given-names>Tylo</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hahn</surname>
<given-names>Samuel V.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Schwab</surname>
<given-names>Janella C.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sekhon</surname>
<given-names>Parneet</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kagieva</surname>
<given-names>Madina</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Reilly</surname>
<given-names>Taylor</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Qi</surname>
<given-names>Ruo Chen</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Salman</surname>
<given-names>Paarsa</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hong</surname>
<given-names>Ryan J.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ma</surname>
<given-names>Eric J.</given-names>
</name>
<xref ref-type="aff" rid="A2">b</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hallam</surname>
<given-names>Steven J.</given-names>
</name>
<xref ref-type="aff" rid="A1">a</xref>
<xref ref-type="aff" rid="A3">c</xref>
<xref ref-type="aff" rid="A4">d</xref>
<xref ref-type="aff" rid="A5">e</xref>
<xref ref-type="aff" rid="A6">f</xref>
<xref ref-type="corresp" rid="CR1">*</xref>
</contrib>
<aff id="A1">
<label>a</label>Department of Microbiology and Immunology, University of British Columbia, 2350 Health Sciences Mall, Vancouver, BC, V6T 1Z3, Canada</aff>
<aff id="A2">
<label>b</label>Independent Researcher, Cambridge, MA 02139, USA</aff>
<aff id="A3">
<label>c</label>Graduate Program in Bioinformatics, University of British Columbia, Genome Sciences Centre, 100-570 West 7th Avenue, Vancouver, British Columbia V5Z 4S6, Canada</aff>
<aff id="A4">
<label>d</label>Genome Science and Technology Program, University of British Columbia, 2329 West Mall, Vancouver, BC V6T 1Z4, Canada</aff>
<aff id="A5">
<label>e</label>Life Sciences Institute, University of British Columbia, Vancouver, British Columbia, Canada V6T 1Z3</aff>
<aff id="A6">
<label>f</label>ECOSCOPE Training Program, University of British Columbia, Vancouver, British Columbia, Canada V6T 1Z3</aff>
</contrib-group>
<author-notes>
<corresp id="CR1">
<label>*</label>Correspondence concerning this paper should be addressed to Steven J. Hallam. Electronic mail may be sent to <email>shallam@mail.ubc.ca</email>.</corresp>
</author-notes>
<pub-date pub-type="nihms-submitted">
<day>07</day>
<month>02</month>
<year>2022</year>
</pub-date>
<pub-date pub-type="preprint">
<day>06</day>
<month>12</month>
<year>2021</year>
</pub-date>
<permissions>
<ali:free_to_read/>
<license>
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p>
</license>
</permissions>
<abstract>
<p id="P1">Viral vaccines can lose their efficacy as the genomes of targeted viruses rapidly evolve, resulting in new variants that may evade vaccine-induced immunity. This process is apparent in the emergence of new SARS-CoV-2 variants which have the potential to undermine vaccination efforts and cause further outbreaks. Predictive vaccinology points to a future of pandemic preparedness in which vaccines can be developed preemptively based in part on predictive models of viral evolution. Thus, modeling the trajectory of SARS-CoV-2 spike protein evolution could have value for mRNA vaccine development. Traditionally, <italic>in silico</italic> sequence evolution has been modeled discretely, while there has been limited investigation into continuous models. Here we present the Viral Predictor for mRNA Evolution (VPRE), an open-source software tool which learns from mutational patterns in viral proteins and models their most statistically likely evolutionary trajectories. We trained a variational autoencoder with real-time and simulated SARS-CoV-2 genome data from Australia to encode discrete spike protein sequences into continuous numerical variables. To simulate evolution along a phylogenetic path, we trained a Gaussian process model with the numerical variables to project spike protein evolution up to five months in advance. Our predictions mapped primarily to a sequence that differed by a single amino acid from the most reported spike protein in Australia within the prediction timeframe, indicating the utility of deep learning and continuous latent spaces for modeling viral protein evolution. VPRE can be readily adapted to investigate and predict the evolution of viruses other than SARS-CoV-2 in temporal, geographic, and lineage-specific pathways.</p>
</abstract>
<kwd-group>
<kwd>deep learning</kwd>
<kwd>Gaussian process</kwd>
<kwd>protein evolution</kwd>
<kwd>SARS-CoV-2</kwd>
<kwd>spike protein</kwd>
<kwd>variational autoencoder</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="S1" sec-type="intro">
<label>1</label>
<title>Introduction</title>
<p id="P2">The onset of the COVID-19 pandemic, caused by the respiratory disease resulting from infection of the novel coronavirus SARS-CoV-2, fueled a surge in vaccine research and scientific effort across the globe. Despite this effort, it is unclear whether vaccines for SARS-CoV-2 will remain effective over time as the virus evolves into new variants. Currently, the prominent issue is high transmission rate, but over time, the virus may become endemic: which raises the concern of long-term mutational risk combined with waning immunity [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R2">2</xref>]. Moreover, amongst viral genome architectures, single-stranded RNA viruses such as SARS-CoV-2 have the highest rate of mutation per replication, which in part gives them a relatively higher chance of mutating with each infection [<xref ref-type="bibr" rid="R3">3</xref>]. This suggests that updated vaccines will need to be carefully designed to compensate for viral evolution between infectious seasons, as is currently the case for influenza vaccinations. Therefore, a key challenge is ensuring vaccine-generated immunity remains effective even after viruses undergo multiple mutation cycles [<xref ref-type="bibr" rid="R4">4</xref>]. To establish long-lasting immunity in this context, it is crucial to have predictive platforms in place to allow the preemptive study of variants, inform vaccine development, and increase pandemic preparedness.</p>
<p id="P3">Currently, mRNA vaccines for SARS-CoV-2 specifically target the spike protein [<xref ref-type="bibr" rid="R5">5</xref>]. The spike protein is the surface glycoprotein that enables SARS-CoV-2 entry into human cells through binding of the angiotensin-converting enzyme 2 (ACE2) receptor [<xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R8">8</xref>]. The importance of the spike protein for host cell entry makes it a logical target candidate for vaccines, and therefore modeling spike protein evolution could inform vaccine development. Attempts to model viral evolution <italic>in silico</italic> have typically been carried out using computational models that aim to simulate general mechanisms of natural evolution, such as genetic drift [<xref ref-type="bibr" rid="R9">9</xref>]. For example, the arrival of new influenza strains each year has encouraged the development of various predictive platforms [e.g., <xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R11">11</xref>], which are useful in the development of annual flu vaccines [<xref ref-type="bibr" rid="R12">12</xref>].</p>
<p id="P4">Newer models for predictive evolution, particularly those involving machine learning, tend to extrapolate patterns from recorded evolutionary events, rather than impose theorized patterns of natural processes [<xref ref-type="bibr" rid="R13">13</xref>]. Deep learning is a subfield of machine learning that uses neural networks to learn from large amounts of data [<xref ref-type="bibr" rid="R14">14</xref>]. Deep learning is emerging as a promising method for modeling biological processes and has already been used to model SARS-CoV-2 evolution in a discrete context [<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref>] and through natural language models [<xref ref-type="bibr" rid="R18">18</xref>]. Of the different types of neural networks, variational autoencoders (VAEs) are viable candidates for amino acid sequence processing due to their ability to reduce multidimensional data down to numerical values that still capture biologically relevant features [<xref ref-type="bibr" rid="R19">19</xref>]. VAEs are an unsupervised deep learning approach, meaning that their patterns and features are extracted from unlabeled training data. In the case of SARS-CoV-2 spike protein evolution, a VAE can compress amino acid sequences into continuous latent space, which is a continuum of numerical values where similar values are located closely together. As a type of generative model, VAEs are often used for applications such as image and text generation [<xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R21">21</xref>], but they have also been used to capture biological information such as sequence mutations and novel cancer biomarkers [<xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R23">23</xref>]. Through numerical encoding, VAEs can be used to reduce protein sequences into lower dimensions, where they can be further analyzed for evolutionary patterns that are otherwise difficult to detect [<xref ref-type="bibr" rid="R24">24</xref>].</p>
<p id="P5">When paired with probabilistic models, such as Gaussian processes (GPs), the continuous coordinates encoded by a VAE can be fitted into evolutionary trajectories and projected forward in time. GPs are a Bayesian learning technique that construct probability models of previously observed data, which they can make inferences from [<xref ref-type="bibr" rid="R25">25</xref>]. GPs have been used to model various protein properties (e.g., ligand-binding affinity or enzyme activity) with high quantitative accuracy [<xref ref-type="bibr" rid="R26">26</xref>]. Regression modeling and time-series forecasting are common applications of GPs. For time-series regression, a GP can fit functions to a given set of data and timepoints and generate regression functions with associated probability distributions that allow for modeling of temporal trends [<xref ref-type="bibr" rid="R27">27</xref>]. GPs present a significant advantage over discrete models because their predictions are not restricted to discrete data, and they can predict protein properties across a great diversity of sequences [<xref ref-type="bibr" rid="R26">26</xref>]. The ability of GPs to quantify uncertainty helps to determine the validity of the outputs, and utilizing them to model temporal trends also makes fewer assumptions on the shape of data distribution [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R28">28</xref>], thus providing a more reliable model. Previous studies have shown the utility of GPs and/or latent spaces to study phylogenetic relationships, model protein stability, design proteins, and in inferring chemical species involved in biochemical interaction networks, among other uses [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R32">32</xref>]. However, the use of GPs and latent spaces for predicting unseen evolution on labelled timelines has not been employed. As a whole, when combined with the generative learning of VAEs, the predictive feature of GPs makes for a synergistic framework for modeling viral protein evolution.</p>
<p id="P6">Here, we describe the Viral Predictor for mRNA Evolution (VPRE), a novel approach to protein sequence prediction which models spike protein evolution as a continuum of numerical coordinates, rather than as a discrete timeline of amino acid sequences (<xref ref-type="fig" rid="F1">Fig. 1</xref>). VPRE encodes SARS-CoV-2 spike protein sequences with a VAE and uses a GP to project the most statistically likely chronological trajectories of spike protein evolution. As a proof-of-concept, we made predictions one, two, and five months into the future using validation data from Australia. Our model predicted six putative spike proteins that closely resemble the composition of spike proteins that appeared in real time, differing by only 0 to 3 amino acids depending on the sequence. The nearest neighbor (or the most common sequence differing by 1 amino acid) of the most frequent prediction was also the most frequent spike protein in real sequences from the same time period. These data suggest that modeling protein evolution in continuous latent space holds promise for predictive vaccinology.</p>
</sec>
<sec id="S2" sec-type="materials | methods">
<label>2</label>
<title>Materials and methods</title>
<sec id="S3" sec-type="data-availability">
<label>2.1</label>
<title>Data and code availability</title>
<p id="P7">This project was carried out by the 2020 University of British Columbia International Genetically Engineered Machine (iGEM) team, during the period of May 2020 to June 2021. Code associated with this work can be found in our GitHub repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/UBC-iGEM/VPRE">https://github.com/UBC-iGEM/VPRE</ext-link>.</p>
</sec>
<sec id="S4">
<label>2.2</label>
<title>Data acquisition and construction</title>
<p id="P8">The VPRE training dataset consisted of 9534 SARS-CoV-2 spike protein sequences from around the world at varying time points throughout the pandemic (Suppl. File 1). These were downloaded along with their corresponding metadata from NCBI Genbank on August 13, 2020. The five-month VPRE validation dataset from Australia consisted of 8488 spike protein sequences, which were downloaded on January 15, 2021. Incomplete sequences containing gaps (dashes and asterisks) and sequences with ambiguous amino acids (null, B, Z, X) were removed to ensure high quality data.</p>
<p id="P9">A training set of 20,000 semi-random mutated spike protein sequences was generated algorithmically by ensuring equal representation in the dataset of any amino acid substitution that occurred at least one time in the spike protein dataset. The algorithm started with the consensus sequence of our analyzed sequences, and went through each position stepwise, presenting with equal likelihood any point mutation observed in the data. This was repeated until 20,000 unique sequences had been created, with the effect of amplifying the presence of infrequent point mutations so that they could be more accurately decoded from predictions made by the GP. No change was made to the analyzed dataset, which remained as sequenced, and only underwent a multiple alignment prior to processing by the GP. The training dataset maintained all the same conserved regions as the real-world data.</p>
</sec>
<sec id="S5">
<label>2.3</label>
<title>Encoding and decoding amino acid sequences through a VAE</title>
<p id="P10">As a preprocessing step, training set sequences were aligned by progressive alignment via the Multialign function in the MATLAB bioinformatic toolbox [<xref ref-type="bibr" rid="R33">33</xref>]. The aligned sequences were padded with asterisk (*) characters to maximal length and one-hot encoded in order to yield binary matrix representations of the SARS-CoV-2 spike protein sequences that could be inputted into our deep learning model.</p>
<p id="P11">The VAE consisted of an encoder and decoder network, where the encoder compressed the sequence data (one-hot encoded amino acid sequences) to its latent embedding and the decoder decompressed the sequence data from its latent embedding. The VAE was implemented in Keras (version 2.4.0) [<xref ref-type="bibr" rid="R34">34</xref>] using a TensorFlow backend (version 1.4.0) [<xref ref-type="bibr" rid="R35">35</xref>]. In the encoder, the number of latent dimensions was set to three to allow for easier visualization, and thus each sequence was compressed to three numerical coordinates. The latent space distribution was defined with a latent mean and logarithmic variance. Both were calculated with Keras Dense with one-hot encoded training and with an input dimension of three. A standard sampling layer and a Dense layer were created in the encoder. The sampling layer randomly sampled data from latent space following a normal distribution with a mean of zero and a standard deviation of one. The Dense layer mapped the sampled data points to the latent distribution. The decoder was constructed using the encoded data as input and the last layer of the autoencoder as output.</p>
<p id="P12">The VAE model was compiled with an Adam optimizer and custom-built loss function. The loss function was the sum of a reconstruction term and a regularization term (expressed as the Kullback-Leibler divergence between the distribution returned by the encoder and a standard normal distribution): <disp-formula id="FD1">
<mml:math id="M1">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mi>L</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>r</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>n</mml:mi>
<mml:mspace width="0.2em"/>
<mml:mi>l</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>+</mml:mo>
<mml:mi>K</mml:mi>
<mml:mi>L</mml:mi>
<mml:mspace width="0.2em"/>
<mml:mi>d</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>v</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>c</mml:mi>
<mml:mi>e</mml:mi>
<mml:mspace width="0.2em"/>
<mml:mi>r</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>g</mml:mi>
<mml:mi>u</mml:mi>
<mml:mi>l</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>z</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>n</mml:mi>
<mml:mspace width="0.2em"/>
<mml:mi>t</mml:mi>
<mml:mi>e</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>m</mml:mi>
<mml:mo>,</mml:mo>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mi>L</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>s</mml:mi>
<mml:mo>=</mml:mo>
<mml:mi>B</mml:mi>
<mml:mi>i</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>y</mml:mi>
<mml:mspace width="0.2em"/>
<mml:mi>C</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>s</mml:mi>
<mml:mi>E</mml:mi>
<mml:mi>n</mml:mi>
<mml:mi>t</mml:mi>
<mml:mi>r</mml:mi>
<mml:mi>o</mml:mi>
<mml:mi>p</mml:mi>
<mml:mi>y</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:mover accent="true">
<mml:mi>x</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>+</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:mfrac>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo>+</mml:mo>
<mml:mi>log</mml:mi>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>z</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msubsup>
<mml:mi>μ</mml:mi>
<mml:mi>z</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo>−</mml:mo>
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>z</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula> where <italic>x</italic> represents the input data, <inline-formula>
<mml:math id="M2">
<mml:mover accent="true">
<mml:mi>x</mml:mi>
<mml:mo>^</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> represents the reconstructed data, <inline-formula>
<mml:math id="M3">
<mml:msubsup>
<mml:mi>σ</mml:mi>
<mml:mi>z</mml:mi>
<mml:mn>2</mml:mn>
</mml:msubsup>
</mml:math>
</inline-formula> represents the variance of the latent distribution, and <italic>μ<sub>z</sub>
</italic> represents the mean of the latent distribution. The reconstruction loss served as a measure of the efficacy of the encoder-decoder, as it represented the difference between the reconstructed (decoded) sequences and the input sequences (calculated using binary cross-entropy). Over training, the reconstruction loss was ultimately minimized. The regularization term helped in learning well-formed latent spaces and reducing overfitting during the training process [<xref ref-type="bibr" rid="R36">36</xref>]. An early stopping function was also applied with a patience parameter of two in order to stop training once the validation loss metric had stopped improving for two consecutive epochs, thus avoiding overfitting.</p>
</sec>
<sec id="S6">
<label>2.4</label>
<title>Modeling the trajectory of spike protein evolution with GP regressions</title>
<p id="P13">A GP was used to model temporal trajectories of each coordinate of Australian sequences encoded by the VAE. After removing duplicated sequences from each day to simplify the model, 185 sequences were obtained (<xref ref-type="supplementary-material" rid="SD1">Suppl. File 3</xref>). Data from sequences that were collected up to May 31, 2020 (n = 104) were used as a training dataset for the GP, and the rest from June 1 to July 31 (n = 81) were used for validation.</p>
<p id="P14">The PyMC3 package (version 3.11) [<xref ref-type="bibr" rid="R37">37</xref>] was used to construct the GP model. To model a temporal axis in the GP, an array was constructed to represent the number of days since the first sequence collection in Australia. The other axis in the GP consisted of the coordinate values from the VAE. <disp-formula id="FD2">
<mml:math id="M4">
<mml:mtext>The</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>GP</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>was</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>defined</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>as</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mi>Y</mml:mi>
<mml:mo>~</mml:mo>
<mml:mi>G</mml:mi>
<mml:mi>P</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:msup>
<mml:mi>x</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mtext>μ(</mml:mtext>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>adapted</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>from</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>Eric</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>Ma's</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>Flu</mml:mtext>
</mml:math>
</disp-formula>
</p>
<p id="P15">Forecaster (<ext-link ext-link-type="uri" xlink:href="https://github.com/ericmjl/flu-sequence-predictor/blob/master/flu-forecaster.ipynb">https://github.com/ericmjl/flu-sequence-predictor/blob/master/flu-forecaster.ipynb</ext-link>) with a GP latent variable implementation sample [<xref ref-type="bibr" rid="R37">37</xref>].</p>
<p id="P16">First, the covariance function was defined as an exponentiated quadratic function. The exponentiated quadratic kernel is a popular kernel used in GP modeling, thus it was chosen as a starting point for modeling the data. Because the VAE coordinates were modeled individually, an input dimension of 1 was used for the exponentiated quadratic kernel. The GP model was computed as follows: <disp-formula id="FD3">
<mml:math id="M5">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>,</mml:mo>
<mml:msup>
<mml:mi>x</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>=</mml:mo>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>×</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mfrac>
<mml:mrow>
<mml:msup>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo>−</mml:mo>
<mml:msup>
<mml:mi>x</mml:mi>
<mml:mo>′</mml:mo>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
<mml:mrow>
<mml:mn>2</mml:mn>
<mml:msup>
<mml:mi>l</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:mrow>
</mml:mfrac>
<mml:mo stretchy="false">)</mml:mo>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mi>s</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo>~</mml:mo>
<mml:mtext>Uniform</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>(-10,5)</mml:mtext>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mi>l</mml:mi>
<mml:mo>~</mml:mo>
<mml:mtext>Uniform</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>(0,30)</mml:mtext>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mtext>μ(</mml:mtext>
<mml:mi>x</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo>~</mml:mo>
<mml:mn>0</mml:mn>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula>
</p>
<p id="P17">The Uniform function in PyMC3 was used to construct the exponent and Theano was used to construct the exponentiation, followed by a deterministic transformation by using the Deterministic function from PyMC3 [<xref ref-type="bibr" rid="R37">37</xref>–<xref ref-type="bibr" rid="R38">38</xref>].</p>
<p id="P18">Second, a Student’s T log-likelihood distribution was defined to model uncertainties in the covariance function and the input data, adapted from a PyMC3 tutorial (<ext-link ext-link-type="uri" xlink:href="https://docs.pymc.io/notebooks/GP-Latent.html">https://docs.pymc.io/notebooks/GP-Latent.html</ext-link>): <disp-formula id="FD4">
<mml:math id="M6">
<mml:mtable columnalign="left">
<mml:mtr>
<mml:mtd>
<mml:mtext>df</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mo>~</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>Gamma</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mo>(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo>,</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mn>1</mml:mn>
<mml:mo>)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>where</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>df</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mo>=</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>degrees</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>of</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>freedom</mml:mtext>
</mml:mtd>
</mml:mtr>
<mml:mtr>
<mml:mtd>
<mml:mfrac>
<mml:mn>1</mml:mn>
<mml:mrow>
<mml:mi>l</mml:mi>
<mml:mi>a</mml:mi>
<mml:mi>m</mml:mi>
</mml:mrow>
</mml:mfrac>
<mml:mo>~</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>HalfCauchy</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mo>(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo>,</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mn>5</mml:mn>
<mml:mo>)</mml:mo>
<mml:mo>,</mml:mo>
<mml:mspace width="0.2em"/>
<mml:mtext>where</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>lam</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>is</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>a</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>scale</mml:mtext>
<mml:mspace width="0.2em"/>
<mml:mtext>parameter</mml:mtext>
</mml:mtd>
</mml:mtr>
</mml:mtable>
</mml:math>
</disp-formula>
</p>
<p id="P19">Lastly, the covariance and the mean function were assembled in a Latent GP model. The Exponentiated Quadratic covariance function and the time array were defined for a Latent GP. A Uniform log-likelihood distribution was applied to describe length-scale, as well as a HalfCauchy distribution and a Gamma distribution to define the uncertainty in the covariance function and to model the noise. The VAE coordinates were inputted as observed prior.</p>
</sec>
<sec id="S7">
<label>2.5</label>
<title>Extrapolating the trajectory of GP models</title>
<p id="P20">To extrapolate the trajectories the GP predicted, a new time array was set from 0 to 120 + x, where 120 was the number of training days and x represented the number of days into the future to predict. The variable x was chosen as x = 30, 60, and 150 to predict one, two, and five months into the future. The new time array was applied on the GP and a conditional distribution of the predicted functions was obtained with the new input time values using the <italic>conditional</italic> function. 1000 samples were drawn from the GP posterior for each of the three VAE coordinates and merged into 1000 triplets to represent the predicted numerical representations of spike proteins. The triplets were decoded by the decoder of the VAE to obtain predicted spike proteins. The likelihood of each sequence existing in the predicted timeframe was estimated based on the fraction of the 1000 GP predictions that translated exactly to the sequence. Additional packages used in the pipeline include numpy (version 1.19.5) [<xref ref-type="bibr" rid="R39">39</xref>], SciPy (version 1.4.1) [<xref ref-type="bibr" rid="R40">40</xref>], and pandas (version 1.1.5) [<xref ref-type="bibr" rid="R41">41</xref>].</p>
</sec>
</sec>
<sec id="S8" sec-type="results">
<label>3</label>
<title>Results</title>
<sec id="S9">
<label>3.1</label>
<title>Amplifying spike protein dataset variation and training the VAE</title>
<p id="P21">We used a VAE to compress spike protein sequences and create a biologically relevant latent space. Our dataset included 9534 spike protein sequences collected between January 25 and June 30, 2020. An initial obstacle when training the VAE was that the limited diversity in our relatively small dataset made it difficult for the neural network to identify patterns due to class imbalance (<xref ref-type="fig" rid="F2">Fig. 2a, b</xref>). Across all sequences, the average mutation frequency per amino acid variant was 1.5 × 10<sup>-4</sup> (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). As expected, we found that the spike protein is quite conserved, especially in its receptor binding domain, where mean variant frequency per amino acid was 6 × 10<sup>-5</sup> (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). To improve the VAE training, we simulated 20,000 spike proteins with an amplified but equal chance of mutation at any of the mutation sites seen in the original protein sequences, while the conserved regions were maintained (<xref ref-type="fig" rid="F2">Fig. 2e</xref>). The majority of variant frequencies at each position consequently rose to approximately 0.3 or 0.5, given that there were many amino acid positions with two or three variants. This approach increased the VAE’s ability to encode and decode rare variants in the dataset, which ensured that low-frequency variants were represented in the GP input and could be decoded accurately if predicted.</p>
<p id="P22">We trained the VAE for 41 epochs, determined by an early stopping function, where a single epoch included one round of encoding and decoding. This was followed by the calculation of a difference score between the input and output sequences, which represented the loss or error of the model (<xref ref-type="fig" rid="F3">Fig. 3a,c</xref>). As a simple means to verify whether or not the sequence encodings outputted by the VAE accurately captured differences in the spike protein sequences, the Euclidean distances between the numerical latent coordinates were compared to the Levenshtein distances between the amino acid sequences (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). A Euclidean distance is the length of a line segment between two data points in geometric space, while a Levenshtein distance is a metric for measuring the number of differences between two strings [<xref ref-type="bibr" rid="R42">42</xref>]. The two variables were strongly correlated (r = 0.79), suggesting that sequences became increasingly different as their distance in latent space increased, and that variation within amino acid sequences was well-captured in the VAE-encoded numerical coordinates.</p>
</sec>
<sec id="S10">
<label>3.2</label>
<title>Encoding spike proteins into continuous latent space using the trained VAE</title>
<p id="P23">After training the VAE on 20,000 simulated sequences, we encoded 7620 spike protein sequences collected before May 30, 2020, into three latent dimensions, or numerical coordinates (<xref ref-type="fig" rid="F3">Fig. 3a,b</xref>). The continuous latent space representation created by our sequence encodings separated into two major populations: one proximal to the sequences collected near December 2019, and the other proximal to those from May 2020 (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). We used the remaining 1914 sequences from the dataset collected after May 30, 2020, to validate the latent space generated by the first sequence encodings. As expected, the validation data appeared in the latent space within the population of sequences proximal to May 2020. This suggests that the VAE could learn a latent variable model and the parameters of the probability distribution modeling the input data.</p>
</sec>
<sec id="S11">
<label>3.3</label>
<title>Modeling and predicting evolutionary trajectories using GPs</title>
<p id="P24">To forecast novel predictions, we inputted each coordinate of the encoded sequences from the VAE into individual GPs. Each GP performed a regression analysis on the input coordinates from the VAE to find the best fitting functions to the data points in chronological order. After this training period, the functions were projected into the future to predict the sequences of the most statistically likely spike proteins that might evolve based on previous evolutionary patterns. Because GP predictions are continuous coordinates and amino acid sequences are discrete, multiple coordinate triplets can represent the same amino acid sequence. As a result, a frequency index can be calculated for each predicted sequence, which we used to estimate their likelihood.</p>
<p id="P25">As a proof-of-concept, we tested the ability of GPs to predict spike protein evolution by training on sequences from Australia collected prior to May 30, 2020 (n = 104), and projecting the trajectories of 1000 sequences one, two, and five months into the future (<xref ref-type="fig" rid="F4">Fig. 4</xref>). We chose Australia with the assumption that it would allow us to simulate an isolated and simplified phylogenetic pathway for SARS-CoV-2 spike proteins, under the hypothesis that Australia as an island continent was more isolated and therefore, less subject to external factors contributing to variant emergence. Moreover, at the onset of this work Australia had the most sufficient spike protein data available for our analysis when compared to other island nations. Within the training period, the functions of all three coordinates tightly fitted with the training sequences. In the first two months of predictions, the range of coordinate values expanded, but then stabilized throughout the five-month prediction period. This can also be seen in the frequency distributions for each coordinate and their respective prediction periods, where the predicted values for each coordinate generally followed a normal distribution, and months two and five appeared to have similar value distributions. Clear clustering of the training data points is seen in Coordinates 1 and 2, suggesting the presence of two dominant spike protein sequences within the Australian dataset.</p>
<p id="P26">The VAE decoded the 1000 coordinate triplets predicted by the GP at the end of five months into 17 amino acid sequences (<xref ref-type="fig" rid="F5">Fig. 5a</xref>). Upon a BLAST analysis, we found that the top two predictions were existing spike proteins and the other 15 were novel sequences. To investigate whether the model produced unseen amino acid variants, we compared the 17 predicted sequences against all training sequences (<xref ref-type="fig" rid="F5">Fig. 5b</xref>). We found five novel amino acid substitutions and four deletions that were only seen in the predicted sequences and not in the training data. Most of the unseen amino acid variants occurred at variable regions in the training set. Interestingly, there were three predicted amino acid variants at conserved regions within the training data. These results indicate that the model was able to produce sequences other than those that it was trained on.</p>
<p id="P27">Across the 17 predicted sequences, six persisted across all five months of predictions. We performed further analysis on the top three predictions due to their high frequency out of the 1000 GP predictions (<xref ref-type="fig" rid="F5">Fig. 5c</xref>; <xref ref-type="supplementary-material" rid="SD1">Suppl. Fig. 2</xref>). We did not consider the remaining 14 sequences as strong prediction candidates. However, for these low-frequency predictions, the top matches from BLAST analysis showed that they still had over 99% identity and query coverage to SARS-CoV-2 spike proteins.</p>
<p id="P28">As a control, we compared the top three predictions to the sequences collected in Australia one month after our training data cut-off date of May 30, 2020 (n = 81, <xref ref-type="fig" rid="F5">Fig. 5c</xref>). The top prediction comprised over 70% of the validation dataset, while the second-and third-most probable predictions were identical to less than 25% and 5% of the validation sequences, respectively. When comparing the top prediction to our GP training sequences (n = 104), around 55% were identical to our top prediction (<xref ref-type="fig" rid="F5">Fig. 5d</xref>). This suggests that our GP worked as expected even when trained on a small dataset, given that it predicted the most dominant sequence it was trained on, which was also the most dominant sequence present in Australia during the prediction period. The top two predictions being identical to existing spike proteins in Australia also suggests that the VAE could reproduce accurate spike protein sequences.</p>
<p id="P29">We retrieved 8407 additional Australian spike protein sequences collected between the training cut-off date of May 30, 2020, and November 30, 2020 (the five-month prediction period) and compared them against our three most frequent predictions (<xref ref-type="fig" rid="F6">Fig. 6a</xref>). Over 85% of the newly retrieved sequences differed from our most frequent prediction by only one amino acid (N477S), and over 90% of these nearest neighbors were identical to each other. The most common nearest neighbors of the second- and third-most frequent predictions differed by only two amino acids from the predictions. When tracing the frequency patterns of the two sequences in our Australian dataset between January and November 2020, we found that the top prediction was the most prevalent spike protein up until May (<xref ref-type="fig" rid="F6">Fig. 6b</xref>). After May, the top prediction’s nearest neighbor became the most prevalent spike protein in Australia, emerging in April, reaching a frequency of around 90% by July and outcompeting our predicted sequence. Taken together, the GP’s top prediction was off by a single amino acid when extrapolating the most dominant spike protein five months into the future. This suggests that our model could make meaningful predictions even when trained on only 104 amino acid sequences.</p>
</sec>
</sec>
<sec id="S12" sec-type="discussion">
<label>4</label>
<title>Discussion</title>
<sec id="S13">
<label>4.1</label>
<title>Analyzing VPRE’s predictions</title>
<p id="P30">VPRE leverages deep learning and GP regression to predict future variants of the SARS-CoV-2 spike protein. We analyzed the first seven months of available SARS-CoV-2 sequences by converting them into continuous latent dimension representations using a VAE, and by performing statistical regression on the continuous data using a GP. Predictions were made by extrapolating the GP model into the future, followed by decoding with the VAE decoder. As a proof-of-concept, we predicted sequences up to five months into the future after our training data cut-off of May 30, 2020, in Australia. The two most frequent predicted variants, N477S and D614G, were seen in the data that was collected during and one month after the training period, suggesting that no major evolutionary events happened in those 30 days. This finding corresponds with the evolutionary trends we observed in Australia over this period, where no variants rose to dominance within a one-month period. Interestingly, the mutation at position 477 is not a mutational hotspot and may result in lower binding affinity to ACE2 [<xref ref-type="bibr" rid="R43">43</xref>], but no data has suggested that this mutation lowers the fitness of the spike protein, so further <italic>in vitro</italic> testing is required to validate the prediction. The other mutation at position 614 is a mutational hotspot that has been previously recorded [<xref ref-type="bibr" rid="R44">44</xref>]. In our dataset, D614G is the most prevalent strain. Since our most likely prediction also contained D614G, this suggests that predictions from our model were realistic, at least in recapitulating the evolution of dominant SARS-CoV-2 strains.</p>
<p id="P31">Through its continuous numerical approach, VPRE showed potential in modeling viral protein evolution. The VAE was able to translate accurately between numerical coordinates and spike protein sequences, suggesting that it is possible to capture the complexity of amino acid sequences with only three continuous numerical values. The nearest neighbor to our most frequent prediction was only one amino acid different, even though our GP was trained on only 104 amino acid sequences within a timespan of 5 months. Given that our dataset spanned a relatively short timeframe and had limited diversity, it is expected that it was not entirely accurate, especially with the other 16 predictions it generated. Hence, it is reasonable that VPRE predicted variation from the dominant sequences at a very low frequency. It should also be noted that VPRE had no direct measure of antigenic shift or fitness other than the extent of these variables captured in the character of spike protein sequences over time. Therefore, predictions based solely on patterns of amino acid sequence evolution may not be sufficient for a well-informed <italic>in silico</italic> platform. We expect the performance of VPRE to be improved with a more diverse sequence dataset over a longer timeframe.</p>
<p id="P32">Further computational and/or experimental validation of VPRE’s predictions would increase the robustness of our model. Because the SARS-CoV-2 spike protein enables host cell entry by interacting with the ACE2 receptor, it would be informative to test the binding affinity of the predicted spike sequences to ACE2 [<xref ref-type="bibr" rid="R43">43</xref>]. Quantifying the expression and binding affinity of VPRE’s predicted spike proteins relative to the wildtype SARS-CoV-2 would inform on its fitness, which is one factor that influences the likelihood of the predicted variant emerging and becoming dominant.</p>
</sec>
<sec id="S14">
<label>4.2</label>
<title>Limitations of the model</title>
<p id="P33">Although our approach provides a novel way to analyze viral evolution, VPRE’s performance is impacted by limitations in our training dataset. Data-dependent models, especially those under a short time series, are more prone to a high risk of bias [<xref ref-type="bibr" rid="R45">45</xref>] and are constrained in their accuracy to the specific protein family they are trained on [<xref ref-type="bibr" rid="R26">26</xref>]. Additionally, the dataset was imbalanced across geographical regions and time. Data from North America dominated the training data, which likely biased the VAE’s decoding ability. Most sequences were collected after March 2020, when the virus had already circulated the globe, which posed a challenge in considering the early epidemiology of SARS-CoV-2 evolution. We expect the performance of the model to improve as the scale of SARS-CoV-2 data nears that of endemic viruses such as influenza. Indeed, more coronavirus data is becoming available on a daily basis, and the re-application of existing VAEs to scale to large datasets and predict the effects of mutations on spike protein function would prove useful [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R46">46</xref>].</p>
<p id="P34">We intended to analyze sequences belonging to the same evolutionary clade by first building a phylogenetic tree of spike proteins. However, because the variation between spike protein sequences is limited to a few substitutions or a single amino acid deletion, and most phylogenetic analysis software is optimized for larger genomic variation, we were not able to build an informative phylogenetic tree. To overcome this limitation, we hypothesized that sequences collected from the same geographical region should be subjected to similar evolutionary constraints, and thus be more likely to belong to the same evolutionary path. Hence, the GP model was trained with sequences collected only from Australia. Future attempts to build a phylogenetic tree of spike proteins will likely require whole genome sequencing data.</p>
<p id="P35">Working with a small dataset of less than 10,000 sequences which had low levels of variation also posed a challenge for the neural network training. We overcame this issue by synthesizing a dataset of 20,000 spike protein sequences with equally amplified mutation frequencies. This functionally limited the appearance of truly random point mutations in the predictions. While novel sequences and point mutations were predicted, the VAE might have had have a decreased ability to decode point mutations that it had not already observed [<xref ref-type="bibr" rid="R47">47</xref>]. Amplifying the mutational variation might have also introduced unnatural features in spike proteins or biased the network’s ability to model variants better than conserved amino acids. Although it is difficult to discern the magnitude of this effect, it appears that the impact was minimal, given that VPRE produced sequences corresponding to real world spike protein evolution in Australia.</p>
<p id="P36">The nature of the algorithms we used are such that each variable created by the VAE is fed into an independent Gaussian process. Ideally, all three variables generated by the VAE would be combined in a single multivariate Gaussian interpretation, given their strong interdependence. Interestingly, we found no libraries utilizing integrated trivariate Gaussian models in open-source repositories or available literature. As such, it seems future work in this stream is needed to construct a more robust multivariate model.</p>
</sec>
<sec id="S15" sec-type="conclusions">
<label>4.3</label>
<title>Conclusions</title>
<p id="P37">VPRE represents a novel approach to modeling protein evolution using continuous numerical values to encode protein sequences. The software was designed to show the utility of modeling evolution in continuous latent spaces, with the aim of progressing computational sequence prediction and bringing bioinformatics closer to accurate <italic>in silico</italic> protein evolution. Synthetic biology could play a critical role in this process: encoding the predicted spike protein sequences on plasmids would allow them to be readily transferred between model systems and laboratories. For example, a VPRE-predicted spike protein could be expressed in a yeast surface display system to assay ACE2 binding ability. The same predicted spike protein could then be used to pseudotype a non-pathogenic carrier virus and tested for antibody evasion using blood serum samples from vaccinated individuals.</p>
<p id="P38">In the process of developing VPRE, we identified areas for continued development, including the need for more powerful phylogenetic reconstruction algorithms and multivariate Gaussian processes. Overall, the implementation of predictive tools such as VPRE opens a window for further investigation into continuous evolutionary models that seek to improve epidemiological modeling, public health intervention systems, and vaccine development.</p>
</sec>
</sec>
<sec sec-type="supplementary-material" id="SM">
<title>Supplementary Material</title>
<supplementary-material content-type="local-data" id="SD1">
<label>Supplementary Materials</label>
<media xlink:href="EMS141083-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="N67058" position="anchor"/>
</supplementary-material>
</sec>
</body>
<back>
<ack id="S16">
<title>Acknowledgments</title>
<p>We thank Sibyl Drissler, Avery Noonan, Kristina Gagalova, Carmen Bayly, Arjun Baghela, Alina Kunitskaya, and Evan Gibbard for their feedback and support during the development of VPRE. Katrina Zaraska, Shira Agam, Daniel McClement, Ahmed Abdelmoneim, Kalen Dofher, Katherine Bessai, Mona Golmohammadzadeh, and Morris Huang provided helping hands in the early stages of our research, as part of UBC’s International Genetically Engineered Machine (iGEM) team. This work was funded by the Ecosystem Services, Commercialization Platforms and Entrepreneurship (ECOSCOPE) program and the Professional Activities Fund (PAF) at the University of British Columbia. The funding for ECOSCOPE was provided by the National Sciences and Engineering Research Council of Canada (NSERC) Collaborative Research and Training Experience (CREATE) program.</p>
</ack>
<glossary>
<title>Abbreviations</title>
<def-list>
<def-item>
<term>ACE2</term>
<def>
<p>angiotensin-converting enzyme 2</p>
</def>
</def-item>
<def-item>
<term>COVID-19</term>
<def>
<p>coronavirus disease 2019</p>
</def>
</def-item>
<def-item>
<term>GP</term>
<def>
<p>Gaussian process</p>
</def>
</def-item>
<def-item>
<term>SARS-CoV-2</term>
<def>
<p>severe acute respiratory syndrome coronavirus 2</p>
</def>
</def-item>
<def-item>
<term>VAE</term>
<def>
<p>variational autoencoder</p>
</def>
</def-item>
<def-item>
<term>VPRE</term>
<def>
<p>Viral Predictor for mRNA Evolution</p>
</def>
</def-item>
</def-list>
</glossary>
<fn-group>
<fn id="FN1" fn-type="conflict">
<p id="P39">Declaration of competing interest: SJH is a co-founder of Koonkie Inc., a bioinformatics consulting company that designs and provides scalable algorithmic and data analytics solutions in the cloud.</p>
</fn>
</fn-group>
<ref-list>
<ref id="R1">
<label>[1]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lavine</surname>
<given-names>JS</given-names>
</name>
<name>
<surname>Bjornstad</surname>
<given-names>ON</given-names>
</name>
<name>
<surname>Antia</surname>
<given-names>R</given-names>
</name>
</person-group>
<article-title>Immunological characteristics govern the transition of COVID-19 to endemicity</article-title>
<source>Science</source>
<year>2021</year>
<volume>371</volume>
<issue>6530</issue>
<fpage>741</fpage>
<lpage>745</lpage>
</element-citation>
</ref>
<ref id="R2">
<label>[2]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shaman</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Galanti</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>Will SARS-CoV-2 become endemic?</article-title>
<source>Science</source>
<year>2020</year>
<volume>370</volume>
<issue>6516</issue>
<fpage>527</fpage>
<lpage>529</lpage>
</element-citation>
</ref>
<ref id="R3">
<label>[3]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Duffy</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Shackelton</surname>
<given-names>LA</given-names>
</name>
<name>
<surname>Holmes</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Rates of evolutionary change in viruses: patterns and determinants</article-title>
<source>Nature Reviews Genetics</source>
<year>2008</year>
<volume>9</volume>
<issue>4</issue>
<fpage>267</fpage>
<lpage>276</lpage>
</element-citation>
</ref>
<ref id="R4">
<label>[4]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Poland</surname>
<given-names>GA</given-names>
</name>
<name>
<surname>Whitaker</surname>
<given-names>JA</given-names>
</name>
<name>
<surname>Poland</surname>
<given-names>CM</given-names>
</name>
<name>
<surname>Ovsyannikova</surname>
<given-names>IG</given-names>
</name>
<name>
<surname>Kennedy</surname>
<given-names>RB</given-names>
</name>
</person-group>
<article-title>Vaccinology in the third millennium: scientific and social challenges</article-title>
<source>Current Opinion in Virology</source>
<year>2016</year>
<volume>17</volume>
<fpage>116</fpage>
<lpage>125</lpage>
</element-citation>
</ref>
<ref id="R5">
<label>[5]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Le</surname>
<given-names>TT</given-names>
</name>
<name>
<surname>Andreadakis</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Kumar</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Roman</surname>
<given-names>RG</given-names>
</name>
<name>
<surname>Tollefsen</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Saville</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Mayhew</surname>
<given-names>S</given-names>
</name>
</person-group>
<article-title>The COVID-19 vaccine development landscape</article-title>
<source>Nature Reviews Drug Discovery</source>
<year>2020</year>
<volume>19</volume>
<issue>5</issue>
<fpage>305</fpage>
<lpage>306</lpage>
</element-citation>
</ref>
<ref id="R6">
<label>[6]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kim</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>JY</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>JS</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>JW</given-names>
</name>
<name>
<surname>Kim</surname>
<given-names>VN</given-names>
</name>
<name>
<surname>Chang</surname>
<given-names>H</given-names>
</name>
</person-group>
<article-title>The architecture of SARS-CoV-2 transcriptome</article-title>
<source>Cell</source>
<year>2020</year>
</element-citation>
</ref>
<ref id="R7">
<label>[7]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ou</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Liu</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Lei</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Mi</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Ren</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Qian</surname>
<given-names>Z</given-names>
</name>
</person-group>
<article-title>Characterization of spike glycoprotein of SARS-CoV-2 on virus entry and its immune cross-reactivity with SARS-CoV</article-title>
<source>Nature Communications</source>
<year>2020</year>
<volume>11</volume>
<issue>1</issue>
<fpage>1</fpage>
<lpage>12</lpage>
</element-citation>
</ref>
<ref id="R8">
<label>[8]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Zhang</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Penninger</surname>
<given-names>JM</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Zhong</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Slutsky</surname>
<given-names>AS</given-names>
</name>
</person-group>
<article-title>Angiotensin-converting enzyme 2 (ACE2) as a SARS-CoV-2 receptor: molecular mechanisms and potential therapeutic target</article-title>
<source>Intensive Care Medicine</source>
<year>2020</year>
<volume>46</volume>
<issue>4</issue>
<fpage>586</fpage>
<lpage>590</lpage>
</element-citation>
</ref>
<ref id="R9">
<label>[9]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>McCall</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Genetic algorithms for modelling and optimisation</article-title>
<source>Journal of Computational and Applied Mathematics</source>
<year>2005</year>
<volume>184</volume>
<issue>1</issue>
<fpage>205</fpage>
<lpage>222</lpage>
</element-citation>
</ref>
<ref id="R10">
<label>[10]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Łuksza</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Lässig</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>A predictive fitness model for influenza</article-title>
<source>Nature</source>
<year>2014</year>
<volume>507</volume>
<issue>7490</issue>
<fpage>57</fpage>
<lpage>61</lpage>
</element-citation>
</ref>
<ref id="R11">
<label>[11]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Steinbrück</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Klingen</surname>
<given-names>TR</given-names>
</name>
<name>
<surname>McHardy</surname>
<given-names>AC</given-names>
</name>
</person-group>
<article-title>Computational prediction of vaccine strains for human influenza A (H3N2) viruses</article-title>
<source>Journal of Virology</source>
<year>2014</year>
<volume>88</volume>
<issue>20</issue>
<fpage>12123</fpage>
<lpage>12132</lpage>
</element-citation>
</ref>
<ref id="R12">
<label>[12]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Morris</surname>
<given-names>DH</given-names>
</name>
<name>
<surname>Gostic</surname>
<given-names>KM</given-names>
</name>
<name>
<surname>Pompei</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Bedford</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Łuksza</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Neher</surname>
<given-names>RA</given-names>
</name>
<name>
<surname>McCauley</surname>
<given-names>JW</given-names>
</name>
</person-group>
<article-title>Predictive modeling of influenza shows the promise of applied evolutionary biology</article-title>
<source>Trends in Microbiology</source>
<year>2018</year>
<volume>26</volume>
<issue>2</issue>
<fpage>102</fpage>
<lpage>118</lpage>
</element-citation>
</ref>
<ref id="R13">
<label>[13]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lee</surname>
<given-names>EK</given-names>
</name>
<name>
<surname>Nakaya</surname>
<given-names>HI</given-names>
</name>
<name>
<surname>Yuan</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Querec</surname>
<given-names>TD</given-names>
</name>
<name>
<surname>Burel</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Pietz</surname>
<given-names>FH</given-names>
</name>
<name>
<surname>Pulendran</surname>
<given-names>B</given-names>
</name>
</person-group>
<article-title>Machine learning for predicting vaccine immunogenicity</article-title>
<source>Interfaces</source>
<year>2016</year>
<volume>46</volume>
<issue>5</issue>
<fpage>368</fpage>
<lpage>390</lpage>
<pub-id pub-id-type="doi">10.1287/inte.2016.0862</pub-id>
</element-citation>
</ref>
<ref id="R14">
<label>[14]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Eraslan</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Avsec</surname>
<given-names>Ž</given-names>
</name>
<name>
<surname>Gagneur</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Theis</surname>
<given-names>FJ</given-names>
</name>
</person-group>
<article-title>Deep learning: new computational modelling techniques for genomics</article-title>
<source>Nature Reviews Genetics</source>
<year>2019</year>
<volume>20</volume>
<issue>7</issue>
<fpage>389</fpage>
<lpage>403</lpage>
</element-citation>
</ref>
<ref id="R15">
<label>[15]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crossman</surname>
<given-names>LC</given-names>
</name>
</person-group>
<article-title>Leveraging deep learning to simulate coronavirus spike proteins has the potential to predict future zoonotic sequences</article-title>
<source>bioRxiv</source>
<year>2020</year>
</element-citation>
</ref>
<ref id="R16">
<label>[16]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sawmya</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Saha</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Tasnim</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Anjum</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Toufikuzzaman</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Rafid</surname>
<given-names>AHM</given-names>
</name>
<name>
<surname>Rahman</surname>
<given-names>MS</given-names>
</name>
</person-group>
<article-title>Analyzing hCov genome sequences: Applying machine intelligence and beyond</article-title>
<source>bioRxiv</source>
<year>2020</year>
</element-citation>
</ref>
<ref id="R17">
<label>[17]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Younis</surname>
<given-names>MC</given-names>
</name>
</person-group>
<article-title>Evaluation of deep learning approaches for identification of different corona-virus species and time series prediction</article-title>
<source>Computerized Medical Imaging and Graphics</source>
<year>2021</year>
<elocation-id>101921</elocation-id>
</element-citation>
</ref>
<ref id="R18">
<label>[18]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hie</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Zhong</surname>
<given-names>ED</given-names>
</name>
<name>
<surname>Berger</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Bryson</surname>
<given-names>B</given-names>
</name>
</person-group>
<article-title>Learning the language of viral evolution and escape</article-title>
<source>Science</source>
<year>2021</year>
<volume>371</volume>
<issue>6526</issue>
<fpage>284</fpage>
<lpage>288</lpage>
</element-citation>
</ref>
<ref id="R19">
<label>[19]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pocrnic</surname>
<given-names>I</given-names>
</name>
<name>
<surname>Lourenco</surname>
<given-names>DA</given-names>
</name>
<name>
<surname>Masuda</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Legarra</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Misztal</surname>
<given-names>I</given-names>
</name>
</person-group>
<article-title>The dimensionality of genomic information and its effect on genomic prediction</article-title>
<source>Genetics</source>
<year>2016</year>
<volume>203</volume>
<issue>1</issue>
<fpage>573</fpage>
<lpage>581</lpage>
</element-citation>
</ref>
<ref id="R20">
<label>[20]</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Hou</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Shen</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Sun</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Qiu</surname>
<given-names>G</given-names>
</name>
</person-group>
<source>Deep feature consistent variational autoencoder</source>
<conf-name>2017 IEEE Winter Conference on Applications of Computer Vision (WACV)</conf-name>
<year>2017</year>
</element-citation>
</ref>
<ref id="R21">
<label>[21]</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Bowman</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Vilnis</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Vinyals</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Dai</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Jozefowicz</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Bengio</surname>
<given-names>S</given-names>
</name>
</person-group>
<source>Generating Sentences from a Continuous Space</source>
<conf-name>Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</conf-name>
<year>2016</year>
<fpage>10</fpage>
<lpage>21</lpage>
</element-citation>
</ref>
<ref id="R22">
<label>[22]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Riesselman</surname>
<given-names>AJ</given-names>
</name>
<name>
<surname>Ingraham</surname>
<given-names>JB</given-names>
</name>
<name>
<surname>Marks</surname>
<given-names>DS</given-names>
</name>
</person-group>
<article-title>Deep generative models of genetic variation capture the effects of mutations</article-title>
<source>Nature Methods</source>
<year>2018</year>
<volume>15</volume>
<issue>10</issue>
<fpage>816</fpage>
<lpage>822</lpage>
</element-citation>
</ref>
<ref id="R23">
<label>[23]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Simidjievski</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Bodnar</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Tariq</surname>
<given-names>I</given-names>
</name>
<name>
<surname>Scherer</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Andres Terre</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Shams</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Jamnik</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Liò</surname>
<given-names>P</given-names>
</name>
</person-group>
<article-title>Variational autoencoders for cancer data integration: design principles and computational practice</article-title>
<source>Frontiers in Genetics</source>
<year>2019</year>
<volume>10</volume>
<fpage>1205</fpage>
</element-citation>
</ref>
<ref id="R24">
<label>[24]</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>Way</surname>
<given-names>GP</given-names>
</name>
<name>
<surname>Greene</surname>
<given-names>CS</given-names>
</name>
</person-group>
<source>Extracting a biologically relevant latent space from cancer transcriptomes with variational autoencoders</source>
<conf-name>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</conf-name>
<year>2018</year>
<volume>23</volume>
<fpage>80</fpage>
<lpage>91</lpage>
</element-citation>
</ref>
<ref id="R25">
<label>[25]</label>
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Rasmussen</surname>
<given-names>CE</given-names>
</name>
<name>
<surname>Williams</surname>
<given-names>CK</given-names>
</name>
</person-group>
<source>Gaussian processes for machine learning</source>
<publisher-loc>Cambridge, MA</publisher-loc>
<publisher-name>MIT Press</publisher-name>
<year>2008</year>
</element-citation>
</ref>
<ref id="R26">
<label>[26]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Romero</surname>
<given-names>PA</given-names>
</name>
<name>
<surname>Krause</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Arnold</surname>
<given-names>FH</given-names>
</name>
</person-group>
<article-title>Navigating the protein fitness landscape with Gaussian processes</article-title>
<source>Proceedings of the National Academy of Sciences</source>
<year>2013</year>
<volume>110</volume>
<issue>3</issue>
<fpage>E193</fpage>
<lpage>E201</lpage>
</element-citation>
</ref>
<ref id="R27">
<label>[27]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cheng</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Ramchandran</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Vatanen</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Lietzén</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Lahesmaa</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Vehtari</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Lähdesmäki</surname>
<given-names>H</given-names>
</name>
</person-group>
<article-title>An additive Gaussian process regression model for interpretable non-parametric analysis of longitudinal data</article-title>
<source>Nature Communications</source>
<year>2019</year>
<volume>10</volume>
<issue>1</issue>
<fpage>1</fpage>
<lpage>11</lpage>
</element-citation>
</ref>
<ref id="R28">
<label>[28]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Roberts</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Osborne</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Ebden</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Reece</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Gibson</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Aigrain</surname>
<given-names>S</given-names>
</name>
</person-group>
<article-title>Gaussian processes for time-series modelling</article-title>
<source>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences</source>
<year>2013</year>
<volume>371</volume>
<issue>1984</issue>
<elocation-id>20110550</elocation-id>
</element-citation>
</ref>
<ref id="R29">
<label>[29]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Zou</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Brooks III</surname>
<given-names>CL</given-names>
</name>
</person-group>
<article-title>Deciphering protein evolution and fitness landscapes with latent space models</article-title>
<source>Nature Communications</source>
<year>2019</year>
<volume>10</volume>
<issue>1</issue>
<fpage>1</fpage>
<lpage>13</lpage>
</element-citation>
</ref>
<ref id="R30">
<label>[30]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gao</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Honkela</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Rattray</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Lawrence</surname>
<given-names>ND</given-names>
</name>
</person-group>
<article-title>Gaussian process modelling of latent chemical species: applications to inferring transcription factoractivities</article-title>
<source>Bioinformatics</source>
<year>2008</year>
<volume>24</volume>
<issue>16</issue>
<fpage>i70</fpage>
<lpage>i75</lpage>
</element-citation>
</ref>
<ref id="R31">
<label>[31]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Greener</surname>
<given-names>JG</given-names>
</name>
<name>
<surname>Moffat</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>DT</given-names>
</name>
</person-group>
<article-title>Design of metalloproteins and novel protein folds using variational autoencoders</article-title>
<source>Scientific Reports</source>
<year>2018</year>
<volume>8</volume>
<issue>1</issue>
<fpage>1</fpage>
<lpage>12</lpage>
</element-citation>
</ref>
<ref id="R32">
<label>[32]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jones</surname>
<given-names>NS</given-names>
</name>
<name>
<surname>Moriarty</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Evolutionary inference for function-valued traits: Gaussian process regression on phylogenies</article-title>
<source>Journal of the Royal Society Interface</source>
<year>2013</year>
<volume>10</volume>
<issue>78</issue>
<elocation-id>20120616</elocation-id>
</element-citation>
</ref>
<ref id="R33">
<label>[33]</label>
<element-citation publication-type="web">
<collab>Mathworks</collab>
<source>Global Optimization Toolbox: User’s Guide (r2021a)</source>
<year>2021</year>
<comment>Retrieved from: <ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/help/bioinfo/ref/multialign.html">https://www.mathworks.com/help/bioinfo/ref/multialign.html</ext-link>
</comment>
</element-citation>
</ref>
<ref id="R34">
<label>[34]</label>
<element-citation publication-type="web">
<person-group person-group-type="author">
<name>
<surname>Chollet</surname>
<given-names>F</given-names>
</name>
<etal/>
</person-group>
<source>Keras GitHub</source>
<year>2015</year>
<comment>
<ext-link ext-link-type="uri" xlink:href="https://keras.io">https://keras.io</ext-link>
</comment>
</element-citation>
</ref>
<ref id="R35">
<label>[35]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Abadi</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Agarwal</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Barham</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Brevdo</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Chen</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Citro</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Zheng</surname>
<given-names>X</given-names>
</name>
</person-group>
<article-title>Tensorflow: Large-scale machine learning on heterogeneous distributed systems</article-title>
<source>arXiv preprint</source>
<year>2016</year>
<elocation-id>arXiv:1603.04467</elocation-id>
</element-citation>
</ref>
<ref id="R36">
<label>[36]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Phan</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Mikkelsen</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Chén</surname>
<given-names>OY</given-names>
</name>
<name>
<surname>Koch</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Mertins</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Kidmose</surname>
<given-names>P</given-names>
</name>
<name>
<surname>De Vos</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>Personalized automatic sleep staging with single-night data: a pilot study with Kullback-Leibler divergence regularization</article-title>
<source>Physiological Measurement</source>
<year>2020</year>
<volume>41</volume>
<issue>6</issue>
<elocation-id>064004</elocation-id>
</element-citation>
</ref>
<ref id="R37">
<label>[37]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Salvatier</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Wiecki</surname>
<given-names>TV</given-names>
</name>
<name>
<surname>Fonnesbeck</surname>
<given-names>C</given-names>
</name>
</person-group>
<article-title>Probabilistic programming in Python using PyMC3</article-title>
<source>PeerJ Computer Science</source>
<year>2016</year>
<volume>2</volume>
<elocation-id>e55</elocation-id>
</element-citation>
</ref>
<ref id="R38">
<label>[38]</label>
<element-citation publication-type="journal">
<collab>Theano Development Team</collab>
<article-title>Theano: A Python framework for fast computation of mathematical expressions</article-title>
<source>arXiv e-prints</source>
<year>2016</year>
<elocation-id>arXiv:1605.02688</elocation-id>
</element-citation>
</ref>
<ref id="R39">
<label>[39]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Harris</surname>
<given-names>CR</given-names>
</name>
<name>
<surname>Millman</surname>
<given-names>KJ</given-names>
</name>
<name>
<surname>van der Walt</surname>
<given-names>SJ</given-names>
</name>
<name>
<surname>Gommers</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Virtanen</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Cournapeau</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Oliphant</surname>
<given-names>TE</given-names>
</name>
</person-group>
<article-title>Array programming with NumPy</article-title>
<source>Nature</source>
<year>2020</year>
<volume>585</volume>
<issue>7825</issue>
<fpage>357</fpage>
<lpage>362</lpage>
</element-citation>
</ref>
<ref id="R40">
<label>[40]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Virtanen</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Gommers</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Oliphant</surname>
<given-names>TE</given-names>
</name>
<name>
<surname>Haberland</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Reddy</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Cournapeau</surname>
<given-names>D</given-names>
</name>
<name>
<surname>van Mulbregt</surname>
<given-names>P</given-names>
</name>
</person-group>
<article-title>SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title>
<source>Nature Methods</source>
<year>2020</year>
<volume>17</volume>
<fpage>261</fpage>
<lpage>72</lpage>
</element-citation>
</ref>
<ref id="R41">
<label>[41]</label>
<element-citation publication-type="confproc">
<person-group person-group-type="author">
<name>
<surname>McKinney</surname>
<given-names>W</given-names>
</name>
</person-group>
<source>Data structures for statistical computing in python</source>
<conf-name>Proceedings of the 9th Python in Science Conference</conf-name>
<year>2010</year>
<volume>445</volume>
<fpage>51</fpage>
<lpage>56</lpage>
</element-citation>
</ref>
<ref id="R42">
<label>[42]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Levenshtein</surname>
<given-names>V</given-names>
</name>
</person-group>
<article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title>
<source>Soviet Physics Doklady</source>
<year>1966</year>
<volume>10</volume>
<issue>8</issue>
<fpage>707</fpage>
<lpage>710</lpage>
</element-citation>
</ref>
<ref id="R43">
<label>[43]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Starr</surname>
<given-names>TN</given-names>
</name>
<name>
<surname>Greaney</surname>
<given-names>AJ</given-names>
</name>
<name>
<surname>Hilton</surname>
<given-names>SK</given-names>
</name>
<name>
<surname>Ellis</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Crawford</surname>
<given-names>KH</given-names>
</name>
<name>
<surname>Dingens</surname>
<given-names>AS</given-names>
</name>
<name>
<surname>Bloom</surname>
<given-names>JD</given-names>
</name>
</person-group>
<article-title>Deep mutational scanning of SARS-CoV-2 receptor binding domain reveals constraints on folding and ACE2 binding</article-title>
<source>Cell</source>
<year>2020</year>
<volume>182</volume>
<issue>5</issue>
<fpage>1295</fpage>
<lpage>1310</lpage>
</element-citation>
</ref>
<ref id="R44">
<label>[44]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Korber</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Fischer</surname>
<given-names>WM</given-names>
</name>
<name>
<surname>Gnanakaran</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Yoon</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Theiler</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Abfalterer</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Montefiori</surname>
<given-names>DC</given-names>
</name>
</person-group>
<article-title>Tracking changes in SARS-CoV-2 spike: evidence that D614G increases infectivity of the COVID-19 virus</article-title>
<source>Cell</source>
<year>2020</year>
<volume>182</volume>
<issue>4</issue>
<fpage>812</fpage>
<lpage>827</lpage>
</element-citation>
</ref>
<ref id="R45">
<label>[45]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wynants</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Van Calster</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Collins</surname>
<given-names>GS</given-names>
</name>
<name>
<surname>Riley</surname>
<given-names>RD</given-names>
</name>
<name>
<surname>Heinze</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Schuit</surname>
<given-names>E</given-names>
</name>
<name>
<surname>van Smeden</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>Prediction models for diagnosis and prognosis of COVID-19: systematic review and critical appraisal</article-title>
<source>BMJ</source>
<year>2020</year>
<volume>369</volume>
</element-citation>
</ref>
<ref id="R46">
<label>[46]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kingma</surname>
<given-names>DP</given-names>
</name>
<name>
<surname>Welling</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>Auto-encoding variational bayes</article-title>
<source>arXiv preprint</source>
<year>2013</year>
<elocation-id>arXiv:1312.6114</elocation-id>
</element-citation>
</ref>
<ref id="R47">
<label>[47]</label>
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Li</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Huang</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Ding</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Gao</surname>
<given-names>X</given-names>
</name>
</person-group>
<article-title>Deep learning in bioinformatics: Introduction, application, and perspective in the big data era</article-title>
<source>Methods</source>
<year>2019</year>
<volume>166</volume>
<fpage>4</fpage>
<lpage>21</lpage>
</element-citation>
</ref>
</ref-list>
</back>
<floats-group>
<fig id="F1" position="float">
<label>Figure 1</label>
<caption>
<title>Overview of the VPRE workflow.</title>
<p>(a) Step 1: Profile and diversity analysis of over 9000 SARS-CoV-2 spike protein sequences. Based on the diversity profile, the VAE trains on a set of 20,000 simulated spike sequences. (b) Step 2: each sequence in the data set is encoded into three continuous numerical variables, or coordinates, that represent spike proteins from the training data. (c) Step 3: The GP regression finds the best statistical fit of the coordinates over time, and projects probable future coordinates. (d) Step 4: Projected coordinates are decoded into putative sequences resulting from SARS-CoV-2 evolution.</p>
</caption>
<graphic xlink:href="EMS141083-f001"/>
</fig>
<fig id="F2" position="float">
<label>Figure 2</label>
<caption>
<title>The collected and simulated datasets for neural network training and encoding.</title>
<p>(a) Cumulative distribution of sequences over time from the dataset downloaded from NCBI GenBank on August 16th, 2020 (n = 9534). (b) Continental distribution of sequences from the GenBank dataset. (c) Distribution of amino acid variations in the spike receptor binding domain (white = at least one variant observed from the GenBank dataset; purple = no variant detected in the GenBank dataset). NTD, N-terminal domain; RBD, receptor binding domain; S2, spike protein S2 domain. (d) Variant frequency at each amino acid position on spike proteins observed from the NCBI dataset. (e) Comparison of variant frequency at each amino acid position on spike proteins in the GenBank dataset (purple) and the simulated dataset (green).</p>
</caption>
<graphic xlink:href="EMS141083-f002"/>
</fig>
<fig id="F3" position="float">
<label>Figure 3</label>
<caption>
<title>The architecture and unsupervised learning of the VAE.</title>
<p>(a) Illustration of the VAE architecture. Three latent dimensions, or coordinates, were set for VAE-translated variables. (b) Overview of latent representations of the viral spike protein sequences. Sequences collected prior to May 30, 2020, are grouped as a testing dataset and are represented by circles (n = 7620). Sequences collected after May 30, 2020, are grouped into a validation dataset and are represented by triangles (n = 1914). (c) VAE training loss curves. The dashed line indicates the number of epochs used in training the final model. (c) Correlation of Levenshtein distances of each sequence pair in the NCBI dataset and Euclidean distances of corresponding latent representations from the VAE. The black dotted line is the fitted line. The significance threshold was adjusted by Bonferroni correction. n = 7620.</p>
</caption>
<graphic xlink:href="EMS141083-f003"/>
</fig>
<fig id="F4" position="float">
<label>Figure 4</label>
<caption>
<title>Spike protein sequences from Australia projected up to five months into the future in the GP regression.</title>
<p>Trajectories of latent representations of sequences analyzed by the GP (purple lines; n = 1000) with the training coordinates overlaid on the training period (green dots; n = 104). After training the GP on sequences up until May 30, 2020 (n = 104), predictions were made one month (blue dotted line), two months (red dotted line), and five months (yellow dotted line) into the future. The corresponding frequency distributions of each prediction period are shown on the right.</p>
</caption>
<graphic xlink:href="EMS141083-f004"/>
</fig>
<fig id="F5" position="float">
<label>Figure 5</label>
<caption>
<title>Spike protein variant predictions from the GP.</title>
<p>(a) Predicted amino acid sequences decoded from three GP regressions performed on coordinates 0, 1, and 2. The frequency of predictions is ordered from highest to lowest from A to Q. (b) Unique variant positions produced by the VAE that were unseen in the training dataset, indicated by purple lollipops. Arrowheads indicate conserved positions in the training dataset. Novel variants include deletions. (c) Number of amino acid differences between the top three predictions and validation sequences collected in Australia up to one month after May 30, 2020 (n = 81). (d) Number of amino acid differences between the most frequent prediction (Seq A) and the GP training sequences (n = 104) and validation sequences (n = 81).</p>
</caption>
<graphic xlink:href="EMS141083-f005"/>
</fig>
<fig id="F6" position="float">
<label>Figure 6</label>
<caption>
<title>Nearest neighbor analysis of the three most frequent spike protein predictions.</title>
<p>(a) Number of amino acid differences between the three most frequent GP predictions and Australian sequences collected within the five-month prediction period (between May 30 and November 30, 2020). Arrowheads indicate the sequence’s nearest neighbors. (b) Monthly frequency of Seq A and its nearest neighbor across the entire Australian spike protein dataset.</p>
</caption>
<graphic xlink:href="EMS141083-f006"/>
</fig>
</floats-group>
</article>
