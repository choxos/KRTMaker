<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS154005</article-id><article-id pub-id-type="doi">10.1101/2021.11.28.470264</article-id><article-id pub-id-type="archive">PPR425763</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Joint Learning of Full-structure Noise in Hierarchical Bayesian Regression Models</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Hashemi</surname><given-names>Ali</given-names></name><email>hashemi@tu-berlin.de</email><aff id="A1">Inverse Modeling and Machine Learning Group, Technische Universitat Berlin, Germany and also with Machine Learning Group, Faculty IV Electrical Engineering and Computer Science, Institute of Software Engineering and Theoretical Computer Science, Technische Universitat Berlin, Sekretariat MAR 4-1 Marchstr. 23, 10587 Berlin, Germany</aff></contrib><contrib contrib-type="author"><name><surname>Cai</surname><given-names>Chang</given-names></name><aff id="A2">Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA, USA</aff></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Yijing</given-names></name><aff id="A3">Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA, USA</aff></contrib><contrib contrib-type="author"><name><surname>Ghosh</surname><given-names>Sanjay</given-names></name><aff id="A4">Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA, USA</aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Müller</surname><given-names>Klaus-Robert</given-names></name><role content-type="author">Member, IEEE</role><email>klaus-robert.mueller@tu-berlin.de</email><aff id="A5">Klaus-Robert Müller is with Machine Learning Group, Technische Universitat Berlin, Germany, BIFOLD – Berlin Institute for the Foundations of Learning and Data, Berlin, Germany, Department of Artificial Intelligence, Korea University, Seoul, South Korea, and also with Max Planck Institute for Informatics, Saarbrucken, Germany</aff></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Nagarajan</surname><given-names>Srikantan S.</given-names></name><role content-type="author">Fellow, IEEE</role><aff id="A6">Department of Radiology and Biomedical Imaging, University of California, San Francisco, CA, USA</aff><email>sri@ucsf.edu</email></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Haufe</surname><given-names>Stefan</given-names></name><email>haufe@tu-berlin.de</email><aff id="A7">Inverse Modeling and Machine Learning Group, Technische Universitat Berlin, Germany, Physikalisch-Technische Bundesanstalt Braunschweig und Berlin, Germany, and Charite – Universitatsmedizin Berlin, Germany</aff></contrib></contrib-group><author-notes><corresp id="CR1">Ali Hashemi, Klaus-Robert Müller, Srikantan S. Nagarajan, and Stefan Haufe are corresponding authors.</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>09</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>08</day><month>09</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">We consider the reconstruction of brain activity from electroencephalography (EEG). This inverse problem can be formulated as a linear regression with independent Gaussian scale mixture priors for both the source and noise components. Crucial factors influencing the accuracy of the source estimation are not only the noise level but also its correlation structure, but existing approaches have not addressed the estimation of noise covariance matrices with full structure. To address this shortcoming, we develop hierarchical Bayesian (type-II maximum likelihood) models for observations with latent variables for source and noise, which are estimated jointly from data. As an extension to classical sparse Bayesian learning (SBL), where across-sensor observations are assumed to be independent and identically distributed, we consider Gaussian noise with full covariance structure. Using the majorization-maximization framework and Riemannian geometry, we derive an efficient algorithm for updating the noise covariance along the manifold of positive definite matrices. We demonstrate that our algorithm has guaranteed and fast convergence and validate it in simulations and with real MEG data. Our results demonstrate that the novel framework significantly improves upon state-of-the-art techniques in the real-world scenario where the noise is indeed non-diagonal and full-structured. Our method has applications in many domains beyond biomagnetic inverse problems.</p></abstract><kwd-group><title>Index Terms</title><kwd>EEG/MEG</kwd><kwd>Brain Source Imaging</kwd><kwd>Hierarchical Bayesian Learning</kwd><kwd>Majorization Minimization</kwd><kwd>Sparse Bayesian Learning</kwd><kwd>Type-II Maximum-Likelihood</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>I</label><title>Introduction</title><p id="P2">PRECISE knowledge of the noise distribution is a fundamental requirement for obtaining accurate solutions in many regression problems [<xref ref-type="bibr" rid="R1">1</xref>], including biomedical imaging applications such as neural encoding models for task-based fMRI analyses [<xref ref-type="bibr" rid="R2">2</xref>], [<xref ref-type="bibr" rid="R3">3</xref>], electrical impedance tomography (EIT) [<xref ref-type="bibr" rid="R4">4</xref>]–[<xref ref-type="bibr" rid="R6">6</xref>] or magneto- or electroetoencephalography (M/EEG) inverse problems [<xref ref-type="bibr" rid="R7">7</xref>]–[<xref ref-type="bibr" rid="R9">9</xref>]. In some of these biomedical imaging applications, however, it is impossible to separately estimate this noise distribution, as distinct “noise-only” (baseline) measurements are not feasible. An alternative is to jointly estimate the regression coefficients and parameters of the noise distribution. This has been pursued both in a (penalized) maximum-likelihood setting (here referred to as <italic>Type-I</italic> approaches) [<xref ref-type="bibr" rid="R7">7</xref>] as well as in hierarchical Bayesian settings (referred to as <italic>Type-II</italic>) [<xref ref-type="bibr" rid="R8">8</xref>]–[<xref ref-type="bibr" rid="R11">11</xref>]. Most contributions in the literature, however, consider only a scalar noise level (homoscedastic noise) or a diagonal noise covariance (i.e., independent between different measurements, heteroscedastic noise) [<xref ref-type="bibr" rid="R12">12</xref>]–[<xref ref-type="bibr" rid="R14">14</xref>]. These are limiting assumptions in practice as noise may be highly correlated across measurements in many realistic scenarios and, thus, have non-trivial off-diagonal elements.</p><p id="P3">In this paper, we focus on M/EEG based brain source imaging (BSI), although the proposed algorithm can be used in general regression settings including sparse signal recovery. [<xref ref-type="bibr" rid="R15">15</xref>]–[<xref ref-type="bibr" rid="R17">17</xref>]. The goal of BSI is to reconstruct brain activity from M/EEG data which can be formulated as a sparse Bayesian learning (SBL) problem. Specifically, we cast it as a linear Bayesian regression model with independent Gaussian scale mixture priors on the parameters and noise. Extending classical SBL approaches, we here consider Gaussian noise with full covariance structure. Prominent sources of correlated noise in M/EEG data are, for example, artifacts caused by eye blinks and the heart beat, muscular artifacts and line noise. Other domains that would benefit from modeling full-structure noise include array processing [<xref ref-type="bibr" rid="R18">18</xref>], direction of arrival (DOA) estimation [<xref ref-type="bibr" rid="R19">19</xref>], geophysical inverse models [<xref ref-type="bibr" rid="R20">20</xref>], and electrical impedance tomography (EIT) [<xref ref-type="bibr" rid="R4">4</xref>]–[<xref ref-type="bibr" rid="R6">6</xref>].</p><p id="P4">Algorithms that can accurately estimate noise with full covariance structure in these domains can be expected to achieve more accurate regression models and predictions. This motivates us to present a model and to develop an efficient optimization algorithm for jointly estimating the posterior of regression parameters as well as the noise distribution. More specifically, our contribution in this paper is three-fold: <list list-type="simple" id="L1"><list-item><label>1)</label><p id="P5">We consider linear regression with Gaussian scale mixture priors on the parameters and <italic>full-structure multivariate Gaussian noise</italic> as opposed to classical SBL approaches that only consider noise distributions with scalar or diagonal structures.</p></list-item><list-item><label>2)</label><p id="P6">We formulate the problem as a hierarchical Bayesian (Type-II maximum-likelihood) regression problem, in which the <italic>source variance hyperparameters</italic> and <italic>a fullstructure noise covariance matrix</italic> are <italic>jointly</italic> estimated by maximizing the Bayesian evidence of the model.</p></list-item><list-item><label>3)</label><p id="P7">We derive an efficient algorithm based on the majorization-minimization (MM) framework for jointly estimating the source variances and noise covariance along the Riemannian manifold of positive definite (PD) matrices.</p></list-item></list></p><p id="P8">The paper is organized as follows: In <xref ref-type="sec" rid="S2">Section II</xref>, we review the necessary background on Type-II Bayesian learning. We then introduce our proposed algorithm in <xref ref-type="sec" rid="S3">Section III</xref>. Simulation studies and real data analysis demonstrating significant improvement in source localization for EEG/MEG brain source imaging are presented in <xref ref-type="sec" rid="S17">Sections IV</xref> and <xref ref-type="sec" rid="S25">V</xref>, respectively. Finally, <xref ref-type="sec" rid="S28">Section VI</xref> concludes the paper.</p></sec><sec id="S2"><label>II</label><title>Type-II Bayesian Regression</title><p id="P9">We consider the linear model <bold>Y</bold> = <bold>LX</bold> + <bold>E</bold>, where a set of coefficients or source components, <bold>X</bold> , is mapped to the measurements, <bold>Y</bold>, by forward or design matrix, <bold>L</bold> ∈ ℝ<sup><italic>M × N</italic></sup>. Depending on the setting, the problem of estimating <bold>X</bold> given <bold>L</bold> and <bold>Y</bold> is called an inverse problem in physics, a multi-task regression problem in machine learning, or a multiple measurement vector (MMV) recovery problem in signal processing [<xref ref-type="bibr" rid="R21">21</xref>]. Adopting a signal processing terminology, the <italic>measurement matrix</italic> <bold>Y</bold> ∈ ℝ<sup><italic>M ×T</italic></sup> captures the activity of <italic>M</italic> sensors at <italic>T</italic> time instants, <bold>y</bold>(<italic>t</italic>) ∈ ℝ<sup><italic>M</italic> × 1</sup>, <italic>t</italic> = 1, …, <italic>T</italic>, while the <italic>source matrix</italic>, <bold>X</bold> ∈ ℝ<sup><italic>N</italic>×<italic>T</italic></sup>, consists of the unknown activity of <italic>N</italic> sources at the same time instants, <bold>x</bold>(<italic>t</italic>) ∈ ℝ<sup><italic>N</italic>×1</sup>,<italic>t</italic> = 1, …, <italic>T</italic>. The matrix <bold>E</bold> = [<bold>e</bold>(1), …, <bold>e</bold>(<italic>T</italic>)] ∈ ℝ<sup><italic>M×T</italic></sup> represents <italic>T</italic> time instances of zero-mean Gaussian noise with full covariance <bold>Λ</bold>, <inline-formula><mml:math id="M1"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>e</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> which is assumed to be independent of the source activations.</p><p id="P10">The goal of BSI is to infer the underlying brain activity <bold>X</bold> from the EEG/MEG measurement <bold>Y</bold> given a known forward operator, called lead field matrix <bold>L</bold>. In practice, <bold>L</bold> can be computed using discretization methods such as the finite element method (FEM) for a given head geometry and known electrical conductivities [<xref ref-type="bibr" rid="R22">22</xref>]. As the number of sensors is typically much smaller than the number of locations of potential brain sources, this inverse problem is highly ill-posed. This problem is addressed by imposing prior distributions on the model parameters and adopting a Bayesian treatment through Maximum-a-Posteriori (MAP) estimation (<italic>Type-I Bayesian learning</italic>) [<xref ref-type="bibr" rid="R23">23</xref>]–[<xref ref-type="bibr" rid="R27">27</xref>] or, when the model has unknown hyperparameters, through Type-II Maximum-Likelihood estimation (<italic>Type-II Bayesian learning</italic>) [<xref ref-type="bibr" rid="R28">28</xref>]–[<xref ref-type="bibr" rid="R30">30</xref>]. In this paper, we focus on Type-II Bayesian learning, which assumes a family of prior distributions <italic>p</italic>(<bold>X</bold>|<bold>Θ</bold>) parameterized by a set of hyperparameters <bold>Θ</bold>. These hyper-parameters can be learned from the data along with the model parameters using a hierarchical Bayesian approach [<xref ref-type="bibr" rid="R31">31</xref>] through the maximumlikelihood principle: <disp-formula id="FD1"><mml:math id="M2"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle></mml:munder><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle></mml:munder><mml:mspace width="0.2em"/><mml:mstyle displaystyle="true"><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Θ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle></mml:mrow></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P11">Here we assume a zero-mean Gaussian prior with diagonal covariance <bold>Γ</bold> = diag(<italic>γ</italic>) for the underlying source distribution. That is, <inline-formula><mml:math id="M3"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <italic>γ</italic> = [<italic>γ</italic><sub>1</sub>, …, <italic>γ<sub>N</sub></italic>]<sup>⊤</sup> contains <italic>N</italic> distinct unknown variances associated to <italic>N</italic> modeled brain sources. In the Type-II Bayesian learning framework, modeling independent sources through a diagonal covariance matrix leads to sparsity of the resulting source distributions, i.e., at the optimum, many of the estimated source variances are zero. This mechanism is known as <italic>sparse Bayesian learning</italic> (SBL) [<xref ref-type="bibr" rid="R31">31</xref>] and is also closely related to the concept of <italic>automatic relevance determination</italic> (ARD) [<xref ref-type="bibr" rid="R32">32</xref>] and <italic>kernel Fisher discriminant</italic> (KFD) [<xref ref-type="bibr" rid="R33">33</xref>]. Just as most other approaches, SBL makes the simplifying assumption of statistical independence between time samples. This leads to the following expression for the distribution of the sources and measurements: <disp-formula id="FD2"><label>(1)</label><mml:math id="M4"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD3"><label>(2)</label><mml:math id="M5"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>x</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P12">The parameters of the Type-II model are the unknown source variances and the noise covariance, i.e., Θ = {<bold>Γ, Λ</bold>} which are optimized based on the current estimates of the source variances and noise covariance in an alternating iterative process. Given initial estimates of <bold>Γ</bold> and <bold>Λ</bold>, the posterior distribution of the sources is a Gaussian of the form [<xref ref-type="bibr" rid="R34">34</xref>] <disp-formula id="FD4"><label>(3)</label><mml:math id="M6"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>X</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi mathvariant="script">N</mml:mi><mml:mspace width="0.2em"/><mml:mtext>(</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>x</mml:mi></mml:mstyle></mml:msub><mml:mtext>)</mml:mtext><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>where</mml:mtext></mml:mrow></mml:math></disp-formula> <disp-formula id="FD5"><label>(4)</label><mml:math id="M7"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="FD6"><label>(5)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mtext>x</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula> <disp-formula id="FD7"><label>(6)</label><mml:math id="M9"><mml:mrow><mml:msub><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:msub><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mtext>Λ</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P13">The estimated posterior parameters <inline-formula><mml:math id="M10"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <bold>Σ</bold><sub>x</sub> are then in turn used to update <bold>Γ</bold> and <bold>Λ</bold> as the minimizers of the negative (marginal) log-likelihood – log<italic>p</italic>(<bold>Y</bold>|<bold>Γ, Λ</bold>) given by [<xref ref-type="bibr" rid="R35">35</xref>]: <disp-formula id="FD8"><label>(7)</label><mml:math id="M11"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width="0.2em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width="0.2em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P14">Given the final solution of hyperparameters Θ<sup>II</sup> = {<bold>Γ<sup>II</sup>, Λ<sup>II</sup></bold>}, the posterior source distribution is obtained by plugging these estimates into (<xref ref-type="disp-formula" rid="FD3">2</xref>)–(<xref ref-type="disp-formula" rid="FD4">5</xref>).</p></sec><sec id="S3"><label>III</label><title>Proposed Method: Full-structure noise (FUN) Learning</title><p id="P15">Here we propose a novel and efficient algorithm, fullstructure noise (<italic>FUN</italic>) learning, which is able to learn the full covariance structure of the noise jointly within the Bayesian Type-II regression framework. We adopt the SBL assumption for the sources, leading to <bold>Γ</bold>-updates previously described in the BSI literature under the name Champagne [<xref ref-type="bibr" rid="R28">28</xref>]. As a novelty and main focus of this paper, we here equip the SBL framework with the capability to jointly learn full noise covariances by invoking efficient methods from Riemannian geometry, in particular the geometric mean.</p><p id="P16">Note that the Type-II cost function in (<xref ref-type="disp-formula" rid="FD8">7</xref>) is non-convex and thus non-trivial to optimize. A number of iterative algorithms such as <italic>majorization-minimization</italic> (MM) approaches [<xref ref-type="bibr" rid="R36">36</xref>] have been proposed to address this challenge. Following the MM scheme, we here first construct convex surrogate functions that <italic>majorizes</italic> <inline-formula><mml:math id="M12"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>II</mml:mtext></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> in each iteration of the optimization algorithm. Then, we show the minimization equivalence between the constructed majoring functions and (<xref ref-type="disp-formula" rid="FD8">7</xref>). This result is presented in the following theorem:</p><sec id="S4"><title>Theorem 1</title><p id="P17"><italic>Let</italic> <bold>Λ</bold><italic><sup>k</sup> and</italic> <inline-formula><mml:math id="M13"><mml:mrow><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> <italic>be fixed values obtained in the</italic> (<italic>k</italic>)-<italic>th iteration of the optimization algorithm minimizing</italic> <inline-formula><mml:math id="M14"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> <italic>Then, optimizing the non-convex Type-II ML cost function in</italic> (<xref ref-type="disp-formula" rid="FD8">7</xref>), <inline-formula><mml:math id="M15"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> <italic>with respect to</italic> <bold>Γ</bold> <italic>is equivalent to optimizing the following convex function, which</italic> majorizes (<xref ref-type="disp-formula" rid="FD8">7</xref>): <disp-formula id="FD9"><label>(8)</label><mml:math id="M16"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>source</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>S</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><italic>where</italic> <inline-formula><mml:math id="M17"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>S</mml:mtext><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> <italic>is defined as:</italic> <disp-formula id="FD10"><label>(9)</label><mml:math id="M18"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>S</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width="0.2em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P18"><italic>Similarly, optimizing</italic> <inline-formula><mml:math id="M19"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> <italic>with respect to</italic> <bold>Λ</bold> <italic>is equivalent to optimizing the following convex majorizing function:</italic> <disp-formula id="FD11"><label>(10)</label><mml:math id="M20"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <italic>where</italic> <inline-formula><mml:math id="M21"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> <italic>is defined as:</italic> <disp-formula id="FD12"><label>(11)</label><mml:math id="M22"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mspace width="0.2em"/><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><sec id="S5"><title>Proof</title><p id="P19">The proof is presented in <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>. □</p><p id="P20">We continue by considering the optimization of the cost functions <inline-formula><mml:math id="M23"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mtext>(</mml:mtext><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mtext>)</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>source</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with respect to <bold>Λ</bold> and <bold>Γ</bold>, respectively. Note that in case of noise covariances with full structure, the solution of <inline-formula><mml:math id="M24"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> with respect to <bold>Λ</bold> lies within the (<italic>M</italic><sup>2</sup> + <italic>M</italic>)/2 Riemannian manifold of PD matrices of size <italic>M</italic> × <italic>M</italic>. This enables us to invoke efficient methods from Riemannian geometry (see [<xref ref-type="bibr" rid="R37">37</xref>]), which ensure that the solution at each step of the optimization is contained within the lower-dimensional solution space. Specifically, in order to optimize for the noise covariance, the algorithm calculates the geometric mean between the previously obtained statistical model covariance, <inline-formula><mml:math id="M25"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and the empirical sensorspace residuals, <inline-formula><mml:math id="M26"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> in each iteration. Regarding the solution of <inline-formula><mml:math id="M27"><mml:mrow><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>source</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> note that we adopt the SBL assumption for the sources by imposing a diagonal structure on the source covariance matrix, <bold>Γ</bold> = diag(<italic>γ</italic>), where <italic>γ</italic> = [<italic>γ</italic><sub>1</sub>,… ,<italic>γ<sub>N</sub></italic>]<sup>⊤</sup>. The update rules obtained from this algorithm are presented in the following theorems:</p></sec></sec><sec id="S6"><title>Theorem 2</title><p id="P21"><italic>The cost function</italic> <inline-formula><mml:math id="M28"><mml:mrow><mml:mspace width="0.2em"/><mml:msubsup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>noise</mml:mtext><mml:mspace width="0.2em"/></mml:mrow><mml:mrow><mml:mtext>conv</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mtext>Γ</mml:mtext><mml:mi>k</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> <italic>is strictly geodesically convex with respect to the PD manifold, and its minimum with respect to</italic> <bold>Λ</bold> <italic>can be attained according to the following update rule:</italic> <disp-formula id="FD13"><label>(12)</label><mml:math id="M29"><mml:mrow><mml:msup><mml:mtext>Λ</mml:mtext><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>←</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><sec id="S7"><title>Proof</title><p id="P22">A detailed proof can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>. Moreover, a geometric representation of the geodesic path between the pair of matrices <inline-formula><mml:math id="M30"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>N</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> on the PD manifold and the geometric mean between them, representing the update for <bold>Λ</bold><sup><italic>k</italic>+1</sup>, is provided in <xref ref-type="fig" rid="F1">Fig. 1</xref>. □</p></sec></sec><sec id="S8"><title>Remark 1</title><p id="P23">Note that the obtained update rule is a closed-form solution for the surrogate cost function, (<xref ref-type="disp-formula" rid="FD11">10</xref>), which stands in contrast to conventional majorization minimization algorithms (see <xref ref-type="supplementary-material" rid="SD1">Section D</xref> in the <xref ref-type="supplementary-material" rid="SD1">appendix</xref>), which require iterative procedures in each step of the optimization.</p></sec><sec id="S9"><title>Theorem 3</title><p id="P24"><italic>Constraining</italic> <bold>Γ</bold> <italic>in</italic> (<xref ref-type="disp-formula" rid="FD9">8</xref>) <italic>to the set of diagonal matrices with nonnegative elements</italic> <inline-formula><mml:math id="M31"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> <italic>i.e.</italic>, <inline-formula><mml:math id="M32"><mml:mrow><mml:mi mathvariant="script">S</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>∣</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:msub><mml:mi>γ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>}</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula></p><disp-formula id="FD14"><mml:math id="M33"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.2em"/><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>Γ</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>,</mml:mo><mml:mi>Λ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>Λ</mml:mi><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mtext>tr(</mml:mtext><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi><mml:mi>Γ</mml:mi></mml:mstyle><mml:mtext>)</mml:mtext><mml:mo>+</mml:mo><mml:mtext>tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>S</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><p id="P25"><italic>leads to the following update rule for the source variances: <inline-formula><mml:math id="M34"><mml:mrow><mml:msup><mml:mi>Γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mtext>diag</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where,</italic> <disp-formula id="FD15"><label>(13)</label><mml:math id="M35"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>for</mml:mtext><mml:mspace width="0.2em"/><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> <italic>and where</italic> <bold>L</bold><italic><sub>.n</sub> denotes the n-th column of the lead field matrix.</italic></p><sec id="S10"><title>Proof</title><p id="P26">A detailed proof can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>. □</p><p id="P27">Convergence of the resulting algorithm is shown in the following theorem:</p></sec></sec><sec id="S11"><title>Theorem 4</title><p id="P28"><italic>Optimizing the non-convex Type-II ML cost function in</italic> (<xref ref-type="disp-formula" rid="FD8">7</xref>), <inline-formula><mml:math id="M36"><mml:mrow><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> <italic>with alternating update rules for</italic> <bold>Λ</bold> <italic>and</italic> <bold>Γ</bold> <italic>in</italic> (<xref ref-type="disp-formula" rid="FD13">12</xref>) <italic>and</italic> (<xref ref-type="disp-formula" rid="FD15">13</xref>) <italic>leads to an MM algorithm with convergence guarantees.</italic></p></sec><sec id="S12"><title>Proof</title><p id="P29">A detailed proof can be found in <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>. □</p></sec><sec id="S13"><title>Remark 2</title><p id="P30"><italic>Note that</italic> (<xref ref-type="disp-formula" rid="FD15">13</xref>) <italic>is identical to the update rule of the</italic> Champagne <italic>algorithm</italic> [<xref ref-type="bibr" rid="R28">28</xref>]. <italic>Moreover, various recent Type-II schemes for learning diagonal noise covariance matrices that are rooted in the concept of SBL</italic> [<xref ref-type="bibr" rid="R8">8</xref>], [<xref ref-type="bibr" rid="R9">9</xref>] <italic>can also be derived as special cases of FUN learning. Specifically, imposing diagonal structure on the noise covariance matrix for the FUN algorithm, i.e.,</italic> <inline-formula><mml:math id="M37"><mml:mrow><mml:mi>Λ</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">S</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> <italic>results in the noise variance update rules derived in</italic> [<xref ref-type="bibr" rid="R9">9</xref>] <italic>for</italic> heteroscedastic, <italic>and in</italic> [<xref ref-type="bibr" rid="R8">8</xref>] <italic>for</italic> homoscedastic <italic>noise. We explicitly demonstrate the connection between FUN learning and heteroscedastic noise learning in <xref ref-type="supplementary-material" rid="SD1">Appendix E</xref></italic>.</p></sec><sec id="S14"><title>Remark 3</title><p id="P31"><italic>Although FUN is limited to estimating a diagonal source covariance matrix, e.g.</italic> <bold>Γ</bold> = diag(γ), <italic>this assumption can be relaxed in certain settings. One such setting is when the inverse of</italic> <inline-formula><mml:math id="M38"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> <italic>is well-defined. This is the case whenever the rank of the lead field matrix</italic> <bold>L</bold> <italic>is less than the number of sensors. In the context of BSI, this scenario, for example, occurs when a region-level lead field – instead of a voxel-level lead field – is used. Under this condition, an update rule similar to</italic> (<xref ref-type="disp-formula" rid="FD13">12</xref>) <italic>can be obtained for the fullstructure source covariance matrix:</italic> <disp-formula id="FD16"><label>(14)</label><mml:math id="M39"><mml:mrow><mml:msup><mml:mo>Γ</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>←</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>C</mml:mi></mml:mstyle><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <italic>where</italic> <inline-formula><mml:math id="M40"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo>:</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> <italic>For additional extensions to other scenarios, please see the discussion section.</italic></p><p id="P32">Summarizing, similar to Champagne and other SBL algorithms, the FUN learning approach also assumes independent Gaussian distributed sources with diagonal source covariances, which are updated through (<xref ref-type="disp-formula" rid="FD15">13</xref>). As an extension to the classical SBL setting, which assumes the noise distribution to be known, FUN models noise with full covariance structure, which is updated using (<xref ref-type="disp-formula" rid="FD13">12</xref>). We summarize the algorithm in <xref ref-type="boxed-text" rid="BX1">Algorithm 1</xref>.</p></sec><sec id="S15"><title>Remark 4</title><p id="P33"><italic>The theoretical results presented in <xref ref-type="sec" rid="S3">Section III</xref> have been obtained for the scalar setting of voxels, where the orientations of the dipolar brain source are assumed to be perpendicular to the surface of the cortex and, hence, only the scalar deflection of each source along the fixed orientation needs to be estimated. In real data, surface normals are hard to estimate or even undefined in case of volumetric reconstructions. Consequently, we model each source here as a full 3-dimensional current vector. This is achieved by introducing three variance parameters for each source within the source covariance matrix,</italic> <inline-formula><mml:math id="M41"><mml:mrow><mml:msup><mml:mi>Γ</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mi>D</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mi>z</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>N</mml:mi><mml:mi>x</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>N</mml:mi><mml:mi>y</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>N</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> <italic>As all Type-II algorithms considered here model the source covariance matrix</italic> <bold>Γ</bold> <italic>to be diagonal, the proposed extension to 3D sources with free orientation is readily applicable. Correspondingly, a full 3D leadfield matrix,</italic> <bold>L</bold><sup>3<italic>D</italic></sup> ∈ ℝ<sup><italic>M</italic>×3<italic>N</italic></sup>, <italic>is used, where we define</italic> <bold>L</bold><sup>3<italic>D</italic></sup> = [<bold>L</bold><sub>1</sub>, …, <bold>L</bold><sub><italic>N</italic></sub>], <italic>and where N is the number of voxels under consideration and</italic> <inline-formula><mml:math id="M42"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mo>⋯</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo stretchy="false">]</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> <italic>is the leadfield matrix for n-th voxel with d<sub>c</sub> orientations. The k-th column of</italic> <bold>L</bold><sub><italic>n</italic></sub>, <italic>i.e.</italic> <inline-formula><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> <italic>for k</italic> = 1, …, <italic>d<sub>c</sub>, represents the signal vector that would be observed at the scalp given a unit current source or dipole at the n-th voxel with a fixed orientation in the</italic> k<italic>-th direction. The voxel dimension d<sub>c</sub> is commonly set to 3 for EEG, and MEG with realistic volume conductor models, and 2 for MEG with single spherical shell models. The update rule in</italic> (<xref ref-type="disp-formula" rid="FD15">13</xref>) <italic>can then be reformulated as follows:</italic> <disp-formula id="FD17"><mml:math id="M44"><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>←</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>n</mml:mi><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>tr</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>Σ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><boxed-text id="BX1" position="anchor" content-type="below"><label>Algorithm 1</label><caption><title>Full-structure noise (FUN) learning</title></caption><p id="P34"><bold>Input</bold>: Lead field matrix <bold>L</bold> ∈ ℝ<sup><italic>M</italic> × <italic>N</italic></sup>; the measurement vectors</p><p id="P35"><bold>y</bold>(<italic>t</italic>) ∈ ℝ<sup><italic>M</italic> × 1</sup>, <italic>t</italic> = 1 and the parameters of stopping condition criteria: <italic>ϵ</italic> and <italic>k</italic><sub>max</sub>.</p><p id="P36"><bold>Result</bold>: Estimated prior source variances [<italic>γ</italic><sub>1</sub>, …, <italic>γ<sub>N</sub></italic>]<sup>⊤</sup>, noise covariance <bold>Λ</bold>, posterior mean <inline-formula><mml:math id="M45"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and sources covariance ∑<sub>x</sub></p><p id="P37">1 Random initialization of <bold>Λ</bold> that satisfies PD condition.</p><p id="P38">2 Initialization of <bold>Γ</bold> = diag([<italic>γ</italic><sub>1</sub>, …, <italic>γ<sub>N</sub></italic>]<sup>Τ</sup>) with ordinary least square solution as <inline-formula><mml:math id="M46"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Γ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>=</mml:mo><mml:mi>Cov</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="M47"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>†</mml:mo></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P39">3 Calculate the statistical covariance: ∑<sub><bold>y</bold></sub> = <bold>Λ</bold> + <bold>LΓL</bold><sup>Τ</sup>.</p><p id="P40">4 Initialize <italic>k</italic> ← 1</p><p id="P41">Repeat</p><p id="P42">5 Calculate the posterior mean as</p><p id="P43"><inline-formula><mml:math id="M48"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Γ</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>⊤</mml:mo></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P44">6 Calculate</p><p id="P45"><inline-formula><mml:math id="M49"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and update <bold>Λ</bold> using (<xref ref-type="disp-formula" rid="FD12">12</xref>).</p><p id="P46">7 Calculate <inline-formula><mml:math id="M50"><mml:mrow><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>S</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mstyle><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and update <bold>Λ</bold> and <italic>γ<sub>n</sub></italic> for <italic>n</italic> = 1, …, <italic>N</italic> based on (<xref ref-type="disp-formula" rid="FD13">13</xref>).</p><p id="P47">8 <italic>k</italic> ← <italic>k</italic> + 1.</p><p id="P48"><bold>Until</bold> <italic>stopping condition is satisfied:</italic></p><p id="P49"><inline-formula><mml:math id="M51"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>|</mml:mo><mml:msubsup><mml:mo>|</mml:mo><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>≤</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> <italic>or k = k<sub>max</sub></italic>;</p><p id="P50">9 Calculate the posterior covariance as <bold>∑<sub>x</sub></bold> = <bold>Λ - ΛL</bold><sup>Τ</sup>(<bold>∑<sub>y</sub></bold>)<sup>-1</sup> <bold>LΛ</bold>.</p></boxed-text></sec><sec id="S16"><title>Complexity Analysis</title><p id="P51">Suppose FUN takes <italic>K</italic> iterations to converge. The key steps within each iteration of FUN include matrix multiplications of different dimensions, additions of matrices, and a matrix inversion. Of note, <bold>Γ</bold> is diagonal matrix in our setting; which significantly reduces the computational burden. Finally, by retaining only dominating factors and using that <italic>T</italic> ≪ <italic>N, M</italic> ≪ <italic>N</italic>, and log(<italic>N</italic>) &lt; <italic>M</italic> in typical BSI settings, we obtain the overall complexity as <inline-formula><mml:math id="M52"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>M</mml:mi><mml:mi>N</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Note that since the model used in FUN learning better captures the structure of the noise in most settings, it converges faster than a less accurate diagonal noise model (please see convergence plots in <xref ref-type="fig" rid="F2">Fig. 2</xref>). Therefore, this observation can be interpreted as a trade-off in which even though per-iteration complexity is more significant for FUN compared to heteroscedastic or homoscedastic noise learning variants, fewer iterations are required for FUN to meet the convergence criteria. This behavior can reduce the overall computational complexity of FUN learning and result in a competitive or only slightly increased total computational time compared to diagonal heteroscedastic or homoscedastic noise learning.</p></sec></sec><sec id="S17"><label>IV</label><title>Numerical Simulations</title><p id="P52">In this section, we compare the performance of the proposed algorithm to variants employing simpler (home- and heteroscedastic) noise models through an extensive set of simulations. We consider a standard EEG inverse problem, where brain activity is reconstructed from simulated pseudo-EEG data [<xref ref-type="bibr" rid="R38">38</xref>]. Our MATLAB codes are publicly accessible at: <ext-link ext-link-type="uri" xlink:href="https://github.com/AliHashemi-ai/FUN-Learning">https://github.com/AliHashemi-ai/FUN-Learning</ext-link>.</p><sec id="S18"><label>A</label><title>Pseudo-EEG Signal Generation</title><sec id="S19"><title>Forward Modeling</title><p id="P53">We used a realistic volume conductor model (of human head) which exhibits a linear relationship between primary electrical source currents generated within the populations of pyramidal neurons in the cortical gray matter [<xref ref-type="bibr" rid="R22">22</xref>] and the resulting scalp surface potentials captured by EEG electrodes. The lead field matrix <bold>L</bold> ∈ ℝ<sup>58×2004</sup> consists of 2004 dipolar current sources and 58 sensors was generated using New York Head model [<xref ref-type="bibr" rid="R39">39</xref>]. The orientation of all source currents was fixed to be perpendicular to the cortical surface, so that only scalar source amplitudes needed to be estimated.</p></sec><sec id="S20"><title>Source and Noise Model</title><p id="P54">We simulated a sparse set of <italic>N</italic><sub>0</sub> = 5 active sources placed at random locations on the cortex. Neural activity of these sources <bold>X</bold> = [x(1), ..., x(<italic>T</italic>)], <italic>T</italic> = 200 were simulated by sampled from an identically and independently distributed (i.i.d) Gaussian distribution. Gaussian additive noise was randomly sampled from a multivariate zero-mean Gaussian distribution with <italic>full</italic> covariance matrix <inline-formula><mml:math id="M53"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>:</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>×</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> This setting is further referred to as <italic>full-structure noise</italic>. To further investigate the effect of model violation, we generated noise with diagonal covariance matrix, referred as <italic>heteroscedastic noise</italic>. The noise matrix <bold>E</bold> = [e(1), …,e(<italic>T</italic>)] ∈ ℝ<sup><italic>M × T</italic></sup> is normalized and added to the signal matrix <bold>Y</bold><sup>signal</sup> = <bold>LX</bold> as follows: <disp-formula id="FD18"><label>(15)</label><mml:math id="M54"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Y</mml:mi></mml:mstyle><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>Y</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>E</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msub><mml:mo>|</mml:mo><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>E</mml:mi></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>α</italic> determines signal-to-noise ratio (SNR) in sensor space defined as SNR = 20log<sub>10</sub> (<italic>α</italic>/1−<italic>α</italic>). The following SNR (dB) values were used in our experiments: {−12, −7.4, −5.4, −3.5, −1.7,0,1.7, 3.5, 5.4, 7.4,12}.</p></sec><sec id="S21"><title>Parameter Initialization</title><p id="P55">The variances of all voxels were initialized randomly by sampling from a standard normal distribution. The optimization programs were terminated either after reaching convergence (defined by a relative change of the Frobenius-norm of the reconstructed sources between subsequent iterations of less than 10<sup>−8</sup>), or after reaching a maximum of <italic>k</italic><sub>max</sub> = 1000 iterations.</p></sec><sec id="S22"><title>Performance Metrics</title><p id="P56">We applied the proposed FUN method on the aforementioned synthetic data to recover the locations and time courses of active brain sources. In addition, two further Type-II Bayesian learning schemes, namely homoscedastic and heteroscedastic Champagne [<xref ref-type="bibr" rid="R8">8</xref>], [<xref ref-type="bibr" rid="R9">9</xref>], were also included as benchmarks with respect to source reconstruction performance and noise covariance estimation accuracy.</p><p id="P57">Source reconstruction performance was evaluated according to the following metrics. First, <italic>earth mover’s distance</italic> (EMD) [<xref ref-type="bibr" rid="R25">25</xref>], [<xref ref-type="bibr" rid="R40">40</xref>], normalized to [0, 1], was used to quantify the spatial localization accuracy. The EMD measures the cost needed to transform two probability distributions defined on the same metric domain (in this case, distributions of the true and estimated sources defined in 3D Euclidean brain space) into each other. Second, the reconstruction error was measured using Pearson correlation between all pairs of simulated and reconstructed (i.e., those with non-zero activations) source time courses. To evaluate the localization error, we also report average Euclidean distance (EUCL) between each simulated source and the best (in terms of absolute correlations) matching reconstructed source.</p><p id="P58">To assess the recovery of the true support, we computed the F<sub>1</sub> measure [<xref ref-type="bibr" rid="R41">41</xref>]: F<sub>1</sub> = 2×<italic>TP</italic>/(<italic>P</italic>+<italic>TP</italic>+<italic>FP</italic>), where P denotes the number of true active sources, while TP and FP are the numbers of true and false positive predictions. Note that F<sub>1</sub> = 1 represents the perfect recovery of the true support.</p><p id="P59">The performance of the noise covariance estimation was evaluated using tree metrics: Pearson correlation (<bold>Λ</bold><sup>sim</sup>), the normalized mean squared error (NMSE), which is defined as <inline-formula><mml:math id="M55"><mml:mrow><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msubsup><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msubsup><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where <bold>Λ</bold> and <inline-formula><mml:math id="M56"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle></mml:math></inline-formula> denote true and reconstructed noise covariances, respectively, and finally the log-det Bregman matrix divergence – also known as Stein’s loss – between original and reconstructed noise covariance matrices, denoted by <inline-formula><mml:math id="M57"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mtext>log-det</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></inline-formula> An introduction to log-det Bregman matrix divergence in the context of BSI methods can be found in [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>]. Note that NMSE measures the reconstruction of the true scale of the noise covariance matrix, while <bold>Λ</bold><sup>sim</sup> is scale-invariant and hence only quantifies the overall structural similarity between simulated and estimated noise covariance matrices.</p><p id="P60">Each simulation was carried out 100 times using different instances of <bold>X</bold> and <bold>E</bold>, and the mean and standard error of the mean (SEM) of each performance measure across repetitions was calculated.</p></sec></sec><sec id="S23" sec-type="results"><label>B</label><title>Results</title><p id="P61"><xref ref-type="fig" rid="F2">Fig. 2</xref> shows two simulated datasets with five active sources in presence of full-structure noise (upper panel) and heteroscedastic noise (lower panel) at 0 dB SNR. Topographic maps depict the locations of the ground-truth active brain sources (first column) along with the source reconstruction result of three noise learning schemes –noise with homoscedastic, heteroscedastic, and full structure. For each algorithm, the estimated noise covariance matrix is also plotted above the topographic map. Source reconstruction performance was measured in terms of EMD and time course correlation (Corr); and are summarized in the table next to each panel. Besides, the accuracy of the noise covariance matrix reconstruction was measured in terms of <bold>Λ</bold><sup>sim</sup> and NMSE.</p><p id="P62"><xref ref-type="fig" rid="F2">Fig. 2</xref> (upper panel) allows for a direct comparison of the estimated noise covariance matrices obtained from the three different noise learning schemes. It can be seen that FUN learning can better capture the overall structure of ground truth full-structure noise as evidenced by lower NMSE and similarity errors compared to the heteroscedastic and homoscedastic algorithm variants that are only able to recover a diagonal matrix while enforcing the off-diagonal elements to zero. This results in higher spatial and temporal accuracy (lower EMD and time course error) for FUN learning compared to competing algorithms assuming diagonal noise covariance. This advantage is also visible in the topographic maps.</p><p id="P63">The lower-panel of <xref ref-type="fig" rid="F2">Fig. 2</xref> presents analogous results for the setting where the noise covariance is generated according to a heteroscedastic model. Note that the superior spatial and temporal reconstruction performance of the heteroscedastic noise learning algorithm compared to the full-structure scheme is expected here because the simulated ground truth noise is indeed heteroscedastic. The full-structure noise learning approach, however, provides fairly reasonable performance in terms of EMD, time course correlation (corr), and <bold>Λ</bold><sup>sim</sup>, although it is designed to estimate a full-structure noise covariance matrix. The convergence behaviour of all three noise learning variants is also illustrated in <xref ref-type="fig" rid="F2">Fig. 2</xref>. Note that the full-structure noise learning approach eventually reaches lower negative log-likelihood values in both scenarios, namely full-structure and heteroscedastic noise.</p><p id="P64"><xref ref-type="fig" rid="F3">Fig. 3</xref> shows the EMD, the time course reconstruction error, the EUCL and the F1 measure score incurred by three different noise learning approaches assuming homoscedastic (red), heteroscedastic (green) and full-structure (blue) noise covariances for a range of SNR values. The upper panel represents the evaluation metrics for the setting where the noise covariance is full-structure model, while the lower-panel depicts the same metric for simulated noise with heteroscedastic diagonal covariance. Concerning the first setting, FUN learning consistently outperforms its homoscedastic and heteroscedastic counterparts according to all evaluation metrics in particular at low-SNR. Consequently, as the SNR decreases, the gap between FUN learning and the two other variants increases. Conversely, heteroscedastic noise learning shows an improvement over FUN learning according to all evaluation metrics when the simulated noise is indeed heteroscedastic. However, note that the magnitude of this improvement is not as large as observed for the setting where the noise covariance is generated according to a full-structure model and then is estimated using the FUN approach.</p><p id="P65"><xref ref-type="fig" rid="F4">Fig. 4</xref> depicts the accuracy if the estimated noise covariance matrix reconstructed by three different noise learning approaches assuming noise with homoscedastic (red), heteroscedastic (green) and full (blue) structure. The ground truth noise covariance matrix either had full (upper row) or heteroscedastic (lower row) structure. Performance was measured in terms of similarity, NMSE, and <inline-formula><mml:math id="M58"><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mtext>log-det</mml:mtext></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math></inline-formula> To be consistent with NMSE, we report “similarity error”, defined as 1 — <bold>Λ</bold><sup>sim</sup>, instead of similarity, <bold>Λ</bold><sup>sim</sup>. Similar to the trend observed in <xref ref-type="fig" rid="F3">Fig. 3</xref>, full-structure noise learning leads to better noise covariance estimation accuracy (lower NMSE and similarity error) for the full-structure noise model, while superior reconstruction performance is achieved for heteroscedastic noise learning when true noise covariance is heteroscedastic.</p><p id="P66">The last column of <xref ref-type="fig" rid="F4">Fig. 4</xref> depicts the performance of FUN learning as well as heteroscedastic and homoscedastic noise learning approaches in terms of the Pearson correlation error, 1 — <bold>Λ</bold><sup>sim</sup>, for different numbers of time samples. For this experiment, the SNR is set to —3.5 dB and the following number of time samples are used: <italic>T</italic> = {10, 20, 50, 70,100,150, 250, 500,1000,1500}. The rest of the parameters are set to the values explained in <xref ref-type="sec" rid="S18">Section. IV-A</xref>.</p><p id="P67">Note that all model inference relies on the robustness of the estimated sample covariance matrix. According to the observed results, we, therefore, conclude that, when the number of samples is small, the sample covariance estimate becomes unreliable and correspondingly will negatively impact the performance of all algorithms. The inference quality, however, can be significantly improved for FUN learning by increasing the number of time samples.</p><sec id="S24"><title>Remark 5</title><p id="P68">When the true noise is heteroscedastic, the inference algorithm with a generative model that matches the true scenario, in this case, heteroscedastic noise learning, outperforms a more complex full-structure noise learning model that has many more parameters to be estimated to become zero, i.e. more degrees of freedom (DoF). Thus, the full-structure noise (FUN) learning model requires more data to converge to the true model. This behavior is confirmed in the last column of <xref ref-type="fig" rid="F4">Fig. 4</xref>, where we observe that both models, namely heteroscedastic noise learning and FUN, converge at large data lengths when the true noise is heteroscedastic. Notably, while the FUN and heteroscedastic noise learning solutions converge when the true noise is heteroscedastic, the same is not true when the true noise has full-structure. As only FUN learning is able to deal with full structure, its performance is dramatically better than that of heteroscedastic (and homoscedastic) noise learning across all sample sizes in this setting.</p></sec></sec></sec><sec id="S25"><label>V</label><title>Analysis of Real MEG Data</title><sec id="S26"><label>A</label><title>Auditory and Visual Evoked Fields (AEF and VEF)</title><p id="P69">All MEG data used here were acquired in the Biomagnetic Imaging Laboratory at the University of California San Francisco (UCSF) with an Omega 2000 whole-head MEG system from CTF Inc. (Coquitlam, BC, Canada) at a sampling rate of 1200 Hz. All human participants provided informed written consent prior to study participation and received monetary compensation for their participation. The studies were approved by the University of California, San Francisco Committee on Human Research.</p><p id="P70">Lead-fields for each subject were calculated using NUTMEG [<xref ref-type="bibr" rid="R42">42</xref>] assuming a single spherical shell volume conductor model resulting in only two spherical orientations. Lead-fields were constructed at a voxel resolution of 8 mm. Furthermore, each lead-field column was normalized. Neural responses to auditory evoked fields (AEF) and visual evoked fields (VEF) stimulus were localized using the FUN algorithm and other benchmarks. The AEF response was elicited during passive listening to binaural tones (600 ms duration, carrier frequency of 1 kHz, 40 dB SL). The VEF response was elicited while subjects were viewing pictures of objects projected onto a screen and subjects were instructed to overtly name the objects [<xref ref-type="bibr" rid="R43">43</xref>], [<xref ref-type="bibr" rid="R44">44</xref>]. Up to 120 AEF and 100 VEF trials were collected. For both AEF and VEF data, trials with clear artifacts or visible noise in the MEG sensors that exceeded 10 pT fluctuations were excluded prior to source localization analysis.</p><p id="P71">Both AEF and VEF data were digitally filtered to a passband of 1 to 70 Hz to remove artifacts and DC offset, and time-aligned to the stimulus onset. Averaging was then performed across sets of trials of increasing size: {10, 20, 40, 60, 100} trials for AEF, and {10, 20, 40} trials for VEF analyses. The pre-stimulus window was selected to be 100 ms prior to stimulus onset. The post-stimulus time window for AEF was selected to be +50 ms to +150 ms. For VEF data, we focused on source reconstruction in two time-windows – an early window ranging from +100 ms to +150 ms around the traditional M100 response, and a later time window ranging from +150 ms to +225 ms around the traditional M170 responses [<xref ref-type="bibr" rid="R35">35</xref>], [<xref ref-type="bibr" rid="R45">45</xref>]–[<xref ref-type="bibr" rid="R47">47</xref>].</p><p id="P72"><xref ref-type="fig" rid="F5">Fig. 5</xref> shows the reconstruction of the AEF for different number of trial averages for a representative subject using FUN learning along with Type-I and Type-II BSI benchmark methods. In addition to heteroscedastic Champagne, two classical non-SBL source reconstruction schemes were included for comparison. The minimum-current estimate (MCE) algorithm [<xref ref-type="bibr" rid="R48">48</xref>] shown here is an example of a sparse Type-I method based on <italic>ℓ</italic><sub>1</sub>-norm minimization. Additionally, eLORETA [<xref ref-type="bibr" rid="R49">49</xref>], represents a smooth inverse solution based on <inline-formula><mml:math id="M59"><mml:msubsup><mml:mi mathvariant="script">l</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>-</mml:mo><mml:mtext>norm</mml:mtext></mml:math></inline-formula> minimization.</p><p id="P73">Reconstruction performance of all algorithms for different trial averaging with 10, 20, 40, 60, and 100 trials are shown. All trials were selected randomly prior to averaging. As the subplots for different numbers of trial averages demonstrate, FUN learning can accurately localize bilateral auditory activity to Heschel’s gyrus, the characteristic location of the primary auditory cortex, even with as few as 10 trials. In this challenging setting, FUN outperforms all competing methods.</p><p id="P74">Regarding the comparison between FUN and the heteroscedastic noise learning approach on real data as demonstrated in <xref ref-type="fig" rid="F5">Fig. 5</xref>, it is not straightforward to evaluate the performance of BSI approaches quantitatively due to the absence of the ground truth. Therefore, the quality of the reconstructions is commonly assessed based on prior neurophysiological knowledge. In <xref ref-type="fig" rid="F5">Fig. 5</xref>, we observed an involvement of both bilateral Heschl’s gyri, which is expected for localization of auditory cortex. Indeed, qualitatively, FUN is able to localize both bilateral auditory activities even when the number of trials is limited to 10. For this setting, the heteroscedastic noise learning approach was only able to locate the left Heschl’s gyrus auditory activity. These results highlight the importance of accurate noise covariance estimation on the fidelity of source reconstructions.</p><p id="P75"><xref ref-type="fig" rid="F6">Fig. 6</xref> shows the localization and time series reconstruction of VEF activity for a single subject using FUN and heteroscedastic noise learning Champagne, eLORETA and MCE. Reconstruction performance is again shown for the number of trials used for averaging ranging from 10 to 40. Trials were randomly chosen from the full dataset without replacement prior to averaging. Within each panel, the top shows the source localization of the M100 (1<sup>st</sup> peak) and M170 (2<sup>nd</sup> peak) responses, respectively. The time course of the most prominent source (indicated by the intersecting green lines) across a +25 ms to +275 ms window is presented below the source localization results. Blue lines represent the voxel power with arbitrary units averaged across ten independent experiments (that is, ten random selections of trials for trial averaging). Blue shades represent the standard error of the mean (SEM) across different trial averaging experiments. We also included three additional benchmark algorithms, sLORETA [<xref ref-type="bibr" rid="R50">50</xref>], S-FLEX [<xref ref-type="bibr" rid="R25">25</xref>] and the LCMV beamformer [<xref ref-type="bibr" rid="R51">51</xref>] in <xref ref-type="fig" rid="F7">Fig. 7</xref>. In comparison to MCE and eLORETA, FUN shows accurate localization capability, while the former benchmarks did not yield reliable results for averages of only ten trials. Even when the number of trials used for averaging was increased to 20, these benchmarks yielded neither good spatial localization of the two visual cortical peaks, nor were the expected time courses of activation reconstructed. Furthermore, FUN detects two salient and clear peaks in each time window in contrast to other benchmarks, where the salience of the early and late peaks are less prominent. Results obtained from FUN are also robust across different SNRs/numbers of trial averages. For more benchmark results, please see <xref ref-type="fig" rid="F7">Fig. 7</xref>.</p></sec><sec id="S27"><label>B</label><title>Resting-state data</title><p id="P76">Resting-state data are particularly suited for the FUN algorithm because of the lack of baseline data on which the noise distribution could be estimated. Here, we show that FUN is able to learn the underlying noise distribution and consistently recover brain activity. For this analysis, three subjects were instructed simply to keep their eyes closed and remain awake. We collected four trials per subject, where each trial was one minute long. We randomly chose 30 seconds or equivalently 36000 time samples for brain source reconstruction from one trial of each subject. These resting-state MEG data were digitally filtered using a pass-band ranging from 8 to 12 Hz (alpha band) to remove artifacts and DC offset.</p><p id="P77">Localization of resting state alpha band activity from the three subjects are shown in <xref ref-type="fig" rid="F8">Fig. 8</xref>. The first three columns show the estimated source covariance patterns (with the application of a threshold of 10% the peak value) for the three noise learning variants of Champagne. Each row represents one subject. The corresponding loss function values across 1000 iterations are shown in the last column. FUN consistently localizes all subjects’ brain activity predominantly near the midline occipital lobe or posterior cingulate gyrus consistent with expected locations of alpha generators known to dominate resting-state activity.</p></sec></sec><sec id="S28" sec-type="discussion"><label>VI</label><title>Discussion</title><p id="P78">In this paper, we focused on sparse regression within the hierarchical Bayesian regression framework and its application in EEG/MEG brain source imaging. We proposed an efficient optimization algorithm for jointly estimating Gaussian regression parameter distributions as well as Gaussian noise distributions with full covariance structure within a hierarchical Bayesian framework. Using the Riemannian geometry of positive definite matrices, we derived an efficient algorithm for jointly estimating brain source variances and noise covariance. The benefits of our proposed framework were evaluated within an extensive set of experiments in the context of the electromagnetic brain source imaging inverse problem and showed significant improvement upon state-of-the-art techniques in the realistic scenario where the noise has full covariance structure. The practical performance of our method is further assessed through analyses of real auditory evoked fields (AEF), visual evoked fields (VEF) and resting-state MEG data.</p><p id="P79">In the context of BSI, [<xref ref-type="bibr" rid="R52">52</xref>] proposed a method for selecting a single regularization parameter based on cross-validation and maximum-likelihood estimation, while [<xref ref-type="bibr" rid="R53">53</xref>]–[<xref ref-type="bibr" rid="R57">57</xref>] assume more complex spatio-temporal noise covariance structures. A common limitation of these works is, however, that the noise level is not estimated as part of the source reconstruction problem on task-related data but from separate noise recordings. Our proposed algorithm substantially differs in this respect, as it learns the noise covariance jointly with the brain source distribution from the same data. This joint estimation perspective is opposed to a step-wise independent estimation process that can cause to error accumulation. The idea of joint estimation of brain source activity and noise covariance has been previously proposed for Type-I learning methods in [<xref ref-type="bibr" rid="R7">7</xref>], [<xref ref-type="bibr" rid="R58">58</xref>]. Bertrand et al. [<xref ref-type="bibr" rid="R7">7</xref>] proposed a method to extend the group Lasso class of algorithms to multi-task learning, where the noise covariance is estimated using an eigenvalue fit to the empirical sensor space residuals defined as <inline-formula><mml:math id="M60"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> in Theorem 1. In contrast, FUN learning uses Riemannian geometry principles, e.g., the geometric mean between the sensor space residuals <inline-formula><mml:math id="M61"><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mi>N</mml:mi><mml:mi>k</mml:mi></mml:msubsup></mml:math></inline-formula> and the previously obtained statistical model covariance, <inline-formula><mml:math id="M62"><mml:msubsup><mml:mi>Σ</mml:mi><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:math></inline-formula> This enables us to robustly estimate the noise covariance as part of the model, in contrast to the method proposed in [<xref ref-type="bibr" rid="R7">7</xref>], which estimates the noise covariance solely based on the eigenvalues of the observed sensor space residuals. Furthermore, in contrast to these Type-I likelihood estimation methods, FUN is a Type-II method, which learns the prior source distribution as part of the model fitting. Type-II methods have been reported to yield results that are consistently superior to those of Type-I methods [<xref ref-type="bibr" rid="R8">8</xref>], [<xref ref-type="bibr" rid="R9">9</xref>], [<xref ref-type="bibr" rid="R46">46</xref>], [<xref ref-type="bibr" rid="R47">47</xref>], [<xref ref-type="bibr" rid="R59">59</xref>]. Our numerical results show that the same holds also for FUN learning, which performs on par or better than existing variants from the Type-II family (including conventional Champagne) in this study.</p><p id="P80">The question of which noise model to use on real data can be addressed through well-known model selection techniques from the machine learning literature. One such strategy is to evaluate the Type-II negative log-likelihood loss of both models and pick the model that achieves the lowest loss, i.e. choose models that maximize the Bayesian evidence. This was the objective of our analysis in <xref ref-type="fig" rid="F8">Fig. 8</xref>, where we demonstrated that the localization of resting-state brain activity using FUN learning converges to a lower negative log-likelihood loss, i.e., better Bayesian model evidence, than heteroscedastic noise learning, which indicates the superiority of FUN learning and the necessity to model full-structure noise. Furthermore, it is also possible to evaluate the Type-II likelihood, or, the Bayesian model evidence, out-of-sample in order to perform model selection in real data analyses. This approach may be suitable when parameters of the Type-II likelihood are being optimized as is the case here for all approaches. Using this technique, which was successfully employed in [<xref ref-type="bibr" rid="R8">8</xref>], the data samples are first split into two parts, namely the training set and the testing set, i.e., hold-out data. For real data analysis, the data can be split among different trials or sensor subsets. The model parameters are fitted to the training set and the Type-II (Bregman) or Type-I likelihoods of the fitted model are then evaluated on the hold-out data (see [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="supplementary-material" rid="SD1">Eqs. 31 and 32</xref>] for related formulations). Note that since the hold-out data are not used during model fitting, the likelihood evaluation on this data is called <italic>out-of-sample</italic> likelihood. The BSI method that achieves better out-of-sample likelihood with respect to the evaluation metric can be considered superior in terms of performance for real data analysis. Formal comparisons of the performance of these model selection techniques on different real data sets are interesting explorations and are considered one of the directions of our future work.</p><p id="P81">Noise learning has also attracted attention in functional magnetic resonance imaging (fMRI) [<xref ref-type="bibr" rid="R2">2</xref>], [<xref ref-type="bibr" rid="R3">3</xref>], [<xref ref-type="bibr" rid="R60">60</xref>], where various models like matrix-normal (MN), factor analysis (FA), and Gaussian-process (GP) regression have been proposed. The majority of the noise learning algorithms in the fMRI literature rely on the EM framework, which is quite slow in practice [<xref ref-type="bibr" rid="R8">8</xref>] and has convergence guarantees only under certain restrictive conditions [<xref ref-type="bibr" rid="R36">36</xref>], [<xref ref-type="bibr" rid="R61">61</xref>]–[<xref ref-type="bibr" rid="R63">63</xref>]. In contrast to these existing approaches, our proposed framework not only applies to the models considered in these papers, but also benefits from theoretically proven convergence guarantees. To be more specific, we showed in this paper that FUN learning is an instance of the wider class of majorization-minimization (MM) framework, for which provable fast convergence is guaranteed. It is worth emphasizing our contribution within the MM optimization context as well. Unlike many other MM implementations, where surrogate functions are minimized using an iterative approach, our proposed algorithm is more efficient because it obtains a closed-form solution for the minimum of the surrogate function in each step.</p><p id="P82">As pointed out in the introduction, electrical impedance tomography (EIT) is another practical example in which the noise interference is highly correlated across measurements; and thus, indeed has full covariance structure. The authors in [<xref ref-type="bibr" rid="R4">4</xref>], [<xref ref-type="bibr" rid="R5">5</xref>] addressed this problem using SBL techniques for multiple measurement vector (MMV) models. Since the noise in these works is restricted to scalar or diagonal covariance structure, FUN learning could be used to model more realistic full-structural noise also in EIT problems.</p><p id="P83">While being broadly applicable (see [<xref ref-type="bibr" rid="R64">64</xref>, <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>] for a comprehensive list of potential applications), our approach is nevertheless limited by a number of factors. Although Gaussian noise distributions are commonly justified, it would be interesting to include more robust non-Gaussian noise distributions in our framework. Besides, signals in real-world scenarios often lie in a lower-dimensional space compared to the original high-dimensional ambient space due to the correlations that exist in the data. Therefore, imposing physiologically plausible constraints on the noise model, e.g., low-rank, Toeplitz, or Kronecker structure [<xref ref-type="bibr" rid="R65">65</xref>], [<xref ref-type="bibr" rid="R66">66</xref>], not only provides side information that can be leveraged for the reconstruction but also reduces the computational cost in two ways: a) by reducing the number of parameters and b) by taking advantage of efficient implementations using circular embeddings and the fast Fourier transform [<xref ref-type="bibr" rid="R67">67</xref>], [<xref ref-type="bibr" rid="R68">68</xref>]. In our recent work [<xref ref-type="bibr" rid="R68">68</xref>], we employed separable Gaussian distributions using Kronecker products of temporal and spatial covariance matrices. The proposed efficient algorithms exploit the intrinsic Riemannian geometry of temporal autocovariance matrices. For stationary dynamics described by Toeplitz matrices, the theory of circulant embeddings was employed.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS154005-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d80aAdGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S29"><title>Acknowledgment</title><p>This result is part of a project that has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement No. 758985).</p><p>AH acknowledges scholarship support from the Machine Learning/Intelligent Data Analysis research group at Technische Universitat Berlin. He further wishes to thank the Charite – Universitatsmedizin Berlin, the Berlin Mathematical School (BMS), and the Berlin Mathematics Research Center MATH+ for partial support. CC was supported by National Natural Science Foundation of China under Grants 62277023 and 62007013, as well as Hubei Provincial Natural Foundation of China under Grant 2021CFB384. KRM was partly funded by the German Ministry for Education and Research (under refs 01IS14013A-E, 01GQ1115, 01GQ0850, 01IS18056A, 01IS18025A and 01IS18037A), the German Research Foundation (DFG) as Math+: Berlin Mathematics Research Center (EXC 2046/1, project-ID: 390685689). Furthermore, KRM was partly supported by the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) grants funded by the Korea Government (No. 2019-0-00079, Artificial Intelligence Graduate School Program, Korea University). SSN was funded in part by National Institutes of Health grants (R01DC004855, R01EB022717, R01DC176960, R01DC010145, R01NS100440, R01AG062196, and R01DC013979), University of California MRPI MRP- 17–454755, the US Department of Defense grant (W81XWH- 13-1-0494) and a research contract with Ricoh MEG USA Inc.</p></ack><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rakitsch</surname><given-names>B</given-names></name><name><surname>Lippert</surname><given-names>C</given-names></name><name><surname>Borgwardt</surname><given-names>K</given-names></name><name><surname>Stegle</surname><given-names>O</given-names></name></person-group><source>It is all in the noise: Efficient multi-task gaussian process inference with structured residuals</source><conf-name>Proceedings of the 26th International Conference on Neural Information Processing Systems</conf-name><year>2013</year><volume>1</volume><fpage>1466</fpage><lpage>1474</lpage></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>M</given-names></name><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>A Bayesian method for reducing bias in neural representational similarity analysis</article-title><source>Advances in Neural Information Processing Systems</source><year>2016</year><fpage>4951</fpage><lpage>4959</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Shvartsman</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title>Incorporating structured assumptions with probabilistic graphical models in fMRI data analysis</article-title><source>Neuropsychologia</source><year>2020</year><elocation-id>107500</elocation-id><pub-id pub-id-type="pmcid">PMC7387580</pub-id><pub-id pub-id-type="pmid">32433952</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2020.107500</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Jia</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>YD</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name></person-group><article-title>Image reconstruction in electrical impedance tomography based on structure-aware sparse Bayesian learning</article-title><source>IEEE Transactions on Medical Imaging</source><year>2018</year><volume>37</volume><issue>9</issue><fpage>2090</fpage><lpage>2102</lpage><pub-id pub-id-type="pmid">29994084</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Tan</surname><given-names>C</given-names></name><name><surname>Jia</surname><given-names>J</given-names></name></person-group><article-title>Efficient multitask structure-aware sparse Bayesian learning for frequency-difference electrical impedance tomography</article-title><source>IEEE Transactions on Industrial Informatics</source><year>2020</year><volume>17</volume><issue>1</issue><fpage>463</fpage><lpage>472</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hannan</surname><given-names>S</given-names></name><name><surname>Faulkner</surname><given-names>M</given-names></name><name><surname>Aristovich</surname><given-names>K</given-names></name><name><surname>Avery</surname><given-names>J</given-names></name><name><surname>Walker</surname><given-names>MC</given-names></name><name><surname>Holder</surname><given-names>DS</given-names></name></person-group><article-title>In vivo imaging of deep neural activity from the cortical surface during hippocampal epileptiform events in the rat brain using electrical impedance tomography</article-title><source>NeuroImage</source><year>2020</year><volume>209</volume><elocation-id>116525</elocation-id><pub-id pub-id-type="pmid">31923606</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertrand</surname><given-names>Q</given-names></name><name><surname>Massias</surname><given-names>M</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Salmon</surname><given-names>J</given-names></name></person-group><article-title>Handling correlated and repeated measurements with the smoothed multivariate squareroot Lasso</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><fpage>3959</fpage><lpage>3970</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashemi</surname><given-names>A</given-names></name><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Kutyniok</surname><given-names>G</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name></person-group><article-title>Unification of sparse Bayesian learning algorithms for electromagnetic brain imaging with the majorization minimization framework</article-title><source>NeuroImage</source><year>2021</year><volume>239</volume><elocation-id>118309</elocation-id><pub-id pub-id-type="pmcid">PMC8433122</pub-id><pub-id pub-id-type="pmid">34182100</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118309</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Hashemi</surname><given-names>A</given-names></name><name><surname>Diwakar</surname><given-names>M</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>Robust estimation of noise for electromagnetic brain imaging with the Champagne algorithm</article-title><source>NeuroImage</source><year>2021</year><volume>225</volume><elocation-id>117411</elocation-id><pub-id pub-id-type="pmcid">PMC8451305</pub-id><pub-id pub-id-type="pmid">33039615</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117411</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wipf</surname><given-names>DP</given-names></name><name><surname>Rao</surname><given-names>BD</given-names></name></person-group><article-title>An empirical Bayesian strategy for solving the simultaneous sparse approximation problem</article-title><source>IEEE Transactions on Signal Processing</source><year>2007</year><volume>55</volume><issue>7</issue><fpage>3704</fpage><lpage>3716</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Rao</surname><given-names>BD</given-names></name></person-group><article-title>Sparse signal recovery with temporally correlated source vectors using sparse Bayesian learning</article-title><source>IEEE Journal of Selected Topics in Signal Processing</source><year>2011</year><volume>5</volume><issue>5</issue><fpage>912</fpage><lpage>926</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van de Geer</surname><given-names>S</given-names></name><name><surname>Lederer</surname><given-names>J</given-names></name><etal/></person-group><chapter-title>The Lasso, correlated design, and improved oracle inequalities</chapter-title><source>From Probability to Statistics and Back: High-Dimensional Models and Processes–A Festschrift in Honor of Jon A Wellner</source><publisher-name>Institute of Mathematical Statistics</publisher-name><year>2013</year><fpage>303</fpage><lpage>316</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dalalyan</surname><given-names>A</given-names></name><name><surname>Hebiri</surname><given-names>M</given-names></name><name><surname>Meziani</surname><given-names>K</given-names></name><name><surname>Salmon</surname><given-names>J</given-names></name></person-group><source>Learning het-eroscedastic models by convex programming under group sparsity</source><conf-name>International Conference on Machine Learning</conf-name><year>2013</year><fpage>379</fpage><lpage>387</lpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lederer</surname><given-names>J</given-names></name><name><surname>Muller</surname><given-names>CL</given-names></name></person-group><source>Don’t fall for tuning parameters: tuning-free variable selection in high dimensions with the TREX</source><conf-name>Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</conf-name><year>2015</year><fpage>2729</fpage><lpage>2735</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candès</surname><given-names>EJ</given-names></name><name><surname>Romberg</surname><given-names>JK</given-names></name><name><surname>Tao</surname><given-names>T</given-names></name></person-group><article-title>Stable signal recovery from incomplete and inaccurate measurements</article-title><source>Communications on pure and applied mathematics</source><year>2006</year><volume>59</volume><issue>8</issue><fpage>1207</fpage><lpage>1223</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoho</surname><given-names>DL</given-names></name></person-group><article-title>Compressed sensing</article-title><source>IEEE Transactions on Information Theory</source><year>2006</year><volume>52</volume><issue>4</issue><fpage>1289</fpage><lpage>1306</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malioutov</surname><given-names>D</given-names></name><name><surname>Cetin</surname><given-names>M</given-names></name><name><surname>Willsky</surname><given-names>AS</given-names></name></person-group><article-title>A sparse signal reconstruction perspective for source localization with sensor arrays</article-title><source>IEEE Transactions on Signal Processing</source><year>2005</year><volume>53</volume><issue>8</issue><fpage>3010</fpage><lpage>3022</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>T</given-names></name><name><surname>Nehorai</surname><given-names>A</given-names></name></person-group><article-title>Maximum likelihood direction finding in spatially colored noise fields using sparse sensor arrays</article-title><source>IEEE Transactions on Signal Processing</source><year>2010</year><volume>59</volume><issue>3</issue><fpage>1048</fpage><lpage>1062</lpage></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>CE</given-names></name><name><surname>Lorenzelli</surname><given-names>F</given-names></name><name><surname>Hudson</surname><given-names>RE</given-names></name><name><surname>Yao</surname><given-names>K</given-names></name></person-group><article-title>Stochastic maximum-likelihood DOA estimation in the presence of unknown nonuniform noise</article-title><source>IEEE Transactions on Signal Processing</source><year>2008</year><volume>56</volume><issue>7</issue><fpage>3038</fpage><lpage>3044</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhdanov</surname><given-names>MS</given-names></name></person-group><source>Inverse theory and applications in geophysics</source><publisher-name>Elsevier</publisher-name><year>2015</year><volume>36</volume></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cotter</surname><given-names>SF</given-names></name><name><surname>Rao</surname><given-names>BD</given-names></name><name><surname>Engan</surname><given-names>K</given-names></name><name><surname>Kreutz-Delgado</surname><given-names>K</given-names></name></person-group><article-title>Sparse solutions to linear inverse problems with multiple measurement vectors</article-title><source>IEEE Transactions on Signal Processing</source><year>2005</year><volume>53</volume><issue>7</issue><fpage>2477</fpage><lpage>2488</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname><given-names>M</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Knuutila</surname><given-names>J</given-names></name><name><surname>Lounasmaa</surname><given-names>OV</given-names></name></person-group><article-title>Magnetoencephalography—theory, instrumentation, and applications to noninvasive studies of the working human brain</article-title><source>Reviews of modern Physics</source><year>1993</year><volume>65</volume><issue>2</issue><fpage>413</fpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname><given-names>RD</given-names></name><name><surname>Michel</surname><given-names>CM</given-names></name><name><surname>Lehmann</surname><given-names>D</given-names></name></person-group><article-title>Low resolution electromagnetic tomography: a new method for localizing electrical activity in the brain</article-title><source>International Journal of psychophysiology</source><year>1994</year><volume>18</volume><issue>1</issue><fpage>49</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">7876038</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorodnitsky</surname><given-names>IF</given-names></name><name><surname>George</surname><given-names>JS</given-names></name><name><surname>Rao</surname><given-names>BD</given-names></name></person-group><article-title>Neuromagnetic source imaging with FOCUSS: a recursive weighted minimum norm algorithm</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1995</year><volume>95</volume><issue>4</issue><fpage>231</fpage><lpage>251</lpage><pub-id pub-id-type="pmid">8529554</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Nikulin</surname><given-names>VV</given-names></name><name><surname>Ziehe</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name><name><surname>Nolte</surname><given-names>G</given-names></name></person-group><article-title>Combining sparsity and rotational invariance in EEG/MEG source reconstruction</article-title><source>NeuroImage</source><year>2008</year><volume>42</volume><issue>2</issue><fpage>726</fpage><lpage>738</lpage><pub-id pub-id-type="pmid">18583157</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Kowalski</surname><given-names>M</given-names></name><name><surname>Hämäläinen</surname><given-names>M</given-names></name></person-group><article-title>Mixed-norm estimates for the M/EEG inverse problem using accelerated gradient methods</article-title><source>Physics in Medicine and Biology</source><year>2012</year><volume>57</volume><issue>7</issue><fpage>1937</fpage><pub-id pub-id-type="pmcid">PMC3566429</pub-id><pub-id pub-id-type="pmid">22421459</pub-id><pub-id pub-id-type="doi">10.1088/0031-9155/57/7/1937</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castaño-Candamil</surname><given-names>S</given-names></name><name><surname>Höhne</surname><given-names>J</given-names></name><name><surname>Martínez-Vargas</surname><given-names>J-D</given-names></name><name><surname>An</surname><given-names>X-W</given-names></name><name><surname>Castellanos-Domínguez</surname><given-names>G</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name></person-group><article-title>Solving the EEG inverse problem based on space–time–frequency structured sparsity constraints</article-title><source>NeuroImage</source><year>2015</year><volume>118</volume><fpage>598</fpage><lpage>612</lpage><pub-id pub-id-type="pmid">26048621</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wipf</surname><given-names>D</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name></person-group><article-title>A unified Bayesian framework for MEG/EEG source imaging</article-title><source>NeuroImage</source><year>2009</year><volume>44</volume><issue>3</issue><fpage>947</fpage><lpage>966</lpage><pub-id pub-id-type="pmcid">PMC4096355</pub-id><pub-id pub-id-type="pmid">18602278</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.02.059</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeger</surname><given-names>MW</given-names></name><name><surname>Wipf</surname><given-names>DP</given-names></name></person-group><article-title>Variational Bayesian inference techniques</article-title><source>IEEE Signal Processing Magazine</source><year>2010</year><volume>27</volume><issue>6</issue><fpage>81</fpage><lpage>91</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>W</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><article-title>Bayesian machine learning: EEG\MEG signal processing measurements</article-title><source>IEEE Signal Processing Magazine</source><year>2016</year><volume>33</volume><issue>1</issue><fpage>14</fpage><lpage>36</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wipf</surname><given-names>DP</given-names></name><name><surname>Rao</surname><given-names>BD</given-names></name></person-group><article-title>Sparse Bayesian learning for basis selection</article-title><source>IEEE Transactions on Signal Processing</source><year>2004</year><volume>52</volume><issue>8</issue><fpage>2153</fpage><lpage>2164</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tipping</surname><given-names>ME</given-names></name></person-group><article-title>Sparse Bayesian learning and the relevance vector machine</article-title><source>Journal of Machine Learning Research</source><year>2001</year><volume>1</volume><issue>Jun</issue><fpage>211</fpage><lpage>244</lpage></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mika</surname><given-names>S</given-names></name><name><surname>Rätsch</surname><given-names>G</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name></person-group><article-title>A mathematical programming approach to the kernel fisher algorithm</article-title><source>Advances in Neural Information Processing Systems</source><year>2001</year><volume>13</volume><fpage>591</fpage><lpage>597</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><source>Electromagnetic brain imaging: a Bayesian perspective</source><publisher-name>Springer</publisher-name><year>2015</year></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wipf</surname><given-names>DP</given-names></name><name><surname>Owen</surname><given-names>JP</given-names></name><name><surname>Attias</surname><given-names>HT</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>Robust Bayesian estimation of the location, orientation, and time course of multiple correlated neural sources using MEG</article-title><source>NeuroImage</source><year>2010</year><volume>49</volume><issue>1</issue><fpage>641</fpage><lpage>655</lpage><pub-id pub-id-type="pmcid">PMC4083006</pub-id><pub-id pub-id-type="pmid">19596072</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.083</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Babu</surname><given-names>P</given-names></name><name><surname>Palomar</surname><given-names>DP</given-names></name></person-group><article-title>Majorization-minimization algorithms in signal processing, communications, and machine learning</article-title><source>IEEE Transactions on Signal Processing</source><year>2017</year><volume>65</volume><issue>3</issue><fpage>794</fpage><lpage>816</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>P</given-names></name><name><surname>Axler</surname><given-names>S</given-names></name><name><surname>Ribet</surname><given-names>K</given-names></name></person-group><source>Riemannian geometry</source><publisher-name>Springer</publisher-name><year>2006</year><volume>171</volume></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Ewald</surname><given-names>A</given-names></name></person-group><article-title>A simulation framework for benchmarking EEG-based brain connectivity estimation methodologies</article-title><source>Brain topography</source><year>2016</year><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="pmid">27255482</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name></person-group><article-title>The New York head — a precise standardized volume conductor model for EEG source localization and tES targeting</article-title><source>NeuroImage</source><year>2016</year><volume>140</volume><fpage>150</fpage><lpage>162</lpage><pub-id pub-id-type="pmcid">PMC5778879</pub-id><pub-id pub-id-type="pmid">26706450</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.019</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubner</surname><given-names>Y</given-names></name><name><surname>Tomasi</surname><given-names>C</given-names></name><name><surname>Guibas</surname><given-names>LJ</given-names></name></person-group><article-title>The earth mover’s distance as a metric for image retrieval</article-title><source>International Journal of Computer Vision</source><year>2000</year><volume>40</volume><issue>2</issue><fpage>99</fpage><lpage>121</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chinchor</surname><given-names>N</given-names></name><name><surname>Sundheim</surname><given-names>BM</given-names></name></person-group><source>Muc-5 evaluation metrics</source><conf-name>Fifth Message Understanding Conference (MUC-5)</conf-name><year>1993</year></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Zumer</surname><given-names>J</given-names></name><name><surname>Agrawal</surname><given-names>V</given-names></name><name><surname>Hild</surname><given-names>K</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name></person-group><article-title>NUTMEG: a neuromagnetic source reconstruction toolbox</article-title><source>Neurology &amp; Clinical Neurophysiology: NCN</source><year>2004</year><volume>2004</volume><fpage>52</fpage><pub-id pub-id-type="pmcid">PMC1360185</pub-id><pub-id pub-id-type="pmid">16012626</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinkley</surname><given-names>LB</given-names></name><name><surname>Dale</surname><given-names>CL</given-names></name><name><surname>Luks</surname><given-names>TL</given-names></name><name><surname>Findlay</surname><given-names>AM</given-names></name><name><surname>Bukshpun</surname><given-names>P</given-names></name><name><surname>Pojman</surname><given-names>N</given-names></name><name><surname>Thieu</surname><given-names>T</given-names></name><name><surname>Chung</surname><given-names>WK</given-names></name><name><surname>Berman</surname><given-names>J</given-names></name><name><surname>Roberts</surname><given-names>TP</given-names></name><etal/></person-group><article-title>Sensorimotor cortical oscillations during movement preparation in 16p11. 2 deletion carriers</article-title><source>Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>37</issue><fpage>7321</fpage><lpage>7331</lpage><pub-id pub-id-type="pmcid">PMC6759026</pub-id><pub-id pub-id-type="pmid">31270155</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3001-17.2019</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinkley</surname><given-names>LB</given-names></name><name><surname>De Witte</surname><given-names>E</given-names></name><name><surname>Cahill-Thompson</surname><given-names>M</given-names></name><name><surname>Mizuiri</surname><given-names>D</given-names></name><name><surname>Garrett</surname><given-names>C</given-names></name><name><surname>Honma</surname><given-names>S</given-names></name><name><surname>Findlay</surname><given-names>A</given-names></name><name><surname>Gorno-Tempini</surname><given-names>ML</given-names></name><name><surname>Tarapore</surname><given-names>P</given-names></name><name><surname>Kirsch</surname><given-names>HE</given-names></name><etal/></person-group><article-title>Optimizing magnetoencephalographic imaging estimation of language lateralization for simpler language tasks</article-title><source>Frontiers in Human Neuroscience</source><year>2020</year><volume>14</volume><fpage>105</fpage><pub-id pub-id-type="pmcid">PMC7242765</pub-id><pub-id pub-id-type="pmid">32499685</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2020.00105</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Zumer</surname><given-names>JM</given-names></name><name><surname>Guggisberg</surname><given-names>AG</given-names></name><name><surname>Trumpis</surname><given-names>M</given-names></name><name><surname>Wong</surname><given-names>DD</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>MEG/EEG source reconstruction, statistical evaluation, and visualization with NUTMEG</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><pub-id pub-id-type="pmcid">PMC3061455</pub-id><pub-id pub-id-type="pmid">21437174</pub-id><pub-id pub-id-type="doi">10.1155/2011/758973</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owen</surname><given-names>JP</given-names></name><name><surname>Wipf</surname><given-names>DP</given-names></name><name><surname>Attias</surname><given-names>HT</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>Performance evaluation of the Champagne source reconstruction algorithm on simulated and real M/EEG data</article-title><source>Neuroimage</source><year>2012</year><volume>60</volume><issue>1</issue><fpage>305</fpage><lpage>323</lpage><pub-id pub-id-type="pmcid">PMC4096349</pub-id><pub-id pub-id-type="pmid">22209808</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.12.027</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Diwakar</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>Robust empirical Bayesian reconstruction of distributed sources for electromagnetic brain imaging</article-title><source>IEEE Transactions on Medical Imaging</source><year>2019</year><volume>39</volume><issue>3</issue><fpage>567</fpage><lpage>577</lpage><pub-id pub-id-type="pmcid">PMC7446954</pub-id><pub-id pub-id-type="pmid">31380750</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2019.2932290</pub-id></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsuura</surname><given-names>K</given-names></name><name><surname>Okabe</surname><given-names>Y</given-names></name></person-group><article-title>Selective minimum-norm solution of the biomagnetic inverse problem</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>1995</year><volume>42</volume><issue>6</issue><fpage>608</fpage><lpage>615</lpage><pub-id pub-id-type="pmid">7790017</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname><given-names>RD</given-names></name></person-group><source>Discrete, 3D distributed, linear imaging methods of electric neuronal activity. Part 1: exact, zero error localization</source><year>2007</year></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascual-Marqui</surname><given-names>RD</given-names></name><etal/></person-group><article-title>Standardized low-resolution brain electromagnetic tomography (sLORETA): technical details</article-title><source>Methods Find Exp Clin Pharmacol</source><year>2002</year><volume>24</volume><issue>Suppl D</issue><fpage>5</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">12575463</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Veen</surname><given-names>BD</given-names></name><name><surname>Van Drongelen</surname><given-names>W</given-names></name><name><surname>Yuchtman</surname><given-names>M</given-names></name><name><surname>Suzuki</surname><given-names>A</given-names></name></person-group><article-title>Localization of brain electrical activity via linearly constrained minimum variance spatial filtering</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>1997</year><volume>44</volume><issue>9</issue><fpage>867</fpage><lpage>880</lpage><pub-id pub-id-type="pmid">9282479</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><article-title>Automated model selection in covariance estimation and spatial whitening of MEG and EEG signals</article-title><source>NeuroImage</source><year>2015</year><volume>108</volume><fpage>328</fpage><lpage>342</lpage><pub-id pub-id-type="pmid">25541187</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huizenga</surname><given-names>HM</given-names></name><name><surname>De Munck</surname><given-names>JC</given-names></name><name><surname>Waldorp</surname><given-names>LJ</given-names></name><name><surname>Grasman</surname><given-names>RP</given-names></name></person-group><article-title>Spatiotemporal EEG/MEG source analysis based on a parametric noise covariance model</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>2002</year><volume>49</volume><issue>6</issue><fpage>533</fpage><lpage>539</lpage><pub-id pub-id-type="pmid">12046698</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Munck</surname><given-names>JC</given-names></name><name><surname>Huizenga</surname><given-names>HM</given-names></name><name><surname>Waldorp</surname><given-names>LJ</given-names></name><name><surname>Heethaar</surname><given-names>R</given-names></name></person-group><article-title>Estimating stationary dipoles from MEG/EEG data contaminated with spatially and temporally correlated background noise</article-title><source>IEEE Transactions on Signal Processing</source><year>2002</year><volume>50</volume><issue>7</issue><fpage>1565</fpage><lpage>1572</lpage></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bijma</surname><given-names>F</given-names></name><name><surname>De Munck</surname><given-names>JC</given-names></name><name><surname>Huizenga</surname><given-names>HM</given-names></name><name><surname>Heethaar</surname><given-names>RM</given-names></name></person-group><article-title>A mathematical approach to the temporal stationarity of background noise in MEG/EEG measurements</article-title><source>NeuroImage</source><year>2003</year><volume>20</volume><issue>1</issue><fpage>233</fpage><lpage>243</lpage><pub-id pub-id-type="pmid">14527584</pub-id></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Munck</surname><given-names>JC</given-names></name><name><surname>Bijma</surname><given-names>F</given-names></name><name><surname>Gaura</surname><given-names>P</given-names></name><name><surname>Sieluzycki</surname><given-names>CA</given-names></name><name><surname>Branco</surname><given-names>MI</given-names></name><name><surname>Heethaar</surname><given-names>RM</given-names></name></person-group><article-title>A maximum-likelihood estimator for trial-to-trial variations in noisy MEG/EEG data sets</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>2004</year><volume>51</volume><issue>12</issue><fpage>2123</fpage><lpage>2128</lpage><pub-id pub-id-type="pmid">15605859</pub-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>SC</given-names></name><name><surname>Plis</surname><given-names>SM</given-names></name><name><surname>Ranken</surname><given-names>DM</given-names></name><name><surname>Schmidt</surname><given-names>DM</given-names></name></person-group><article-title>Spatiotemporal noise covariance estimation from limited empirical magnetoencephalo-graphic data</article-title><source>Physics in Medicine &amp; Biology</source><year>2006</year><volume>51</volume><issue>21</issue><fpage>5549</fpage><pub-id pub-id-type="pmid">17047269</pub-id></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Massias</surname><given-names>M</given-names></name><name><surname>Fercoq</surname><given-names>O</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Salmon</surname><given-names>J</given-names></name></person-group><source>Generalized concomitant multi-task lasso for sparse multimodal regression</source><conf-name>International Conference on Artificial Intelligence and Statistics</conf-name><year>2018</year><fpage>998</fpage><lpage>1007</lpage></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Hinkley</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Hashemi</surname><given-names>A</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name><name><surname>Sekihara</surname><given-names>K</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name></person-group><article-title>Empirical bayesian localization of event-related timefrequency neural activity dynamics</article-title><source>NeuroImage</source><year>2022</year><volume>258</volume><elocation-id>119369</elocation-id><pub-id pub-id-type="pmid">35700943</pub-id></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shvartsman</surname><given-names>M</given-names></name><name><surname>Sundaram</surname><given-names>N</given-names></name><name><surname>Aoi</surname><given-names>M</given-names></name><name><surname>Charles</surname><given-names>A</given-names></name><name><surname>Willke</surname><given-names>T</given-names></name><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><source>Matrix-normal models for fMRI analysis</source><conf-name>International Conference on Artificial Intelligence and Statistics</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2018</year><fpage>1914</fpage><lpage>1923</lpage></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>TT</given-names></name><name><surname>Lange</surname><given-names>K</given-names></name><etal/></person-group><article-title>The MM alternative to EM</article-title><source>Statistical Science</source><year>2010</year><volume>25</volume><issue>4</issue><fpage>492</fpage><lpage>505</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobson</surname><given-names>MW</given-names></name><name><surname>Fessler</surname><given-names>JA</given-names></name></person-group><article-title>An expanded theoretical treatment of iteration-dependent majorize-minimize algorithms</article-title><source>IEEE Transactions on Image Processing</source><year>2007</year><volume>16</volume><issue>10</issue><fpage>2411</fpage><lpage>2422</lpage><pub-id pub-id-type="pmcid">PMC2750827</pub-id><pub-id pub-id-type="pmid">17926925</pub-id><pub-id pub-id-type="doi">10.1109/tip.2007.904387</pub-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razaviyayn</surname><given-names>M</given-names></name><name><surname>Hong</surname><given-names>M</given-names></name><name><surname>Luo</surname><given-names>Z-Q</given-names></name></person-group><article-title>A unified convergence analysis of block successive minimization methods for nonsmoothoptimization</article-title><source>SIAM Journal on Optimization</source><year>2013</year><volume>23</volume><issue>2</issue><fpage>1126</fpage><lpage>1153</lpage></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hashemi</surname><given-names>A</given-names></name><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name></person-group><article-title>Joint learning of full-structure noise in hierarchical Bayesian regression models</article-title><source>bioRxiv</source><year>2022</year><pub-id pub-id-type="pmid">36423312</pub-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Breloy</surname><given-names>A</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Babu</surname><given-names>P</given-names></name><name><surname>Ginolhac</surname><given-names>G</given-names></name><name><surname>Palomar</surname><given-names>DP</given-names></name></person-group><source>Robust rank constrained kronecker covariance matrix estimation</source><conf-name>2016 50th Asilomar Conference on Signals, Systems and Computers</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2016</year><fpage>810</fpage><lpage>814</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xin</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Wipf</surname><given-names>D</given-names></name></person-group><article-title>Building invariances into sparse subspace clustering</article-title><source>IEEE Transactions on Signal Processing</source><year>2017</year><volume>66</volume><issue>2</issue><fpage>449</fpage><lpage>462</lpage></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babu</surname><given-names>P</given-names></name></person-group><article-title>MELT—maximum-likelihood estimation of low-rank Toeplitz covariance matrix</article-title><source>IEEE Signal Processing Letters</source><year>2016</year><volume>23</volume><issue>11</issue><fpage>1587</fpage><lpage>1591</lpage></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hashemi</surname><given-names>A</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Cai</surname><given-names>C</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Haufe</surname><given-names>S</given-names></name></person-group><source>Efficient hierarchical Bayesian inference for spatiotemporal regression models in neuroimaging</source><conf-name>Thirty-Fifth Conference on Neural Information Processing Systems</conf-name><year>2021</year></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>R</given-names></name></person-group><source>Positive definite matrices</source><publisher-name>Princeton University Press</publisher-name><year>2009</year><volume>24</volume></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zadeh</surname><given-names>P</given-names></name><name><surname>Hosseini</surname><given-names>R</given-names></name><name><surname>Sra</surname><given-names>S</given-names></name></person-group><source>Geometric mean metric learning</source><conf-name>International Conference on Machine Learning</conf-name><year>2016</year><fpage>2464</fpage><lpage>2471</lpage></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Niculescu</surname><given-names>C</given-names></name><name><surname>Persson</surname><given-names>L-E</given-names></name></person-group><source>Convex functions and their applications</source><publisher-name>Springer</publisher-name><year>2006</year></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>JV</given-names></name><name><surname>Kulis</surname><given-names>B</given-names></name><name><surname>Jain</surname><given-names>P</given-names></name><name><surname>Sra</surname><given-names>S</given-names></name><name><surname>Dhillon</surname><given-names>IS</given-names></name></person-group><source>Information-theoretic metric learning</source><conf-name>Proceedings of the 24th International Conference on Machine Learning</conf-name><year>2007</year><fpage>209</fpage><lpage>216</lpage></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnabel</surname><given-names>S</given-names></name><name><surname>Sepulchre</surname><given-names>R</given-names></name></person-group><article-title>Riemannian metric and geometric mean for positive semidefinite matrices of fixed rank</article-title><source>SIAM Journal on Matrix Analysis and Applications</source><year>2009</year><volume>31</volume><issue>3</issue><fpage>1055</fpage><lpage>1070</lpage></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>De Klerk</surname><given-names>E</given-names></name></person-group><source>Aspects of semidefinite programming: interior point algorithms and selected applications</source><publisher-name>Springer Science &amp; Business Media</publisher-name><year>2006</year><volume>65</volume></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunter</surname><given-names>DR</given-names></name><name><surname>Lange</surname><given-names>K</given-names></name></person-group><article-title>A tutorial on MM algorithms</article-title><source>The American Statistician</source><year>2004</year><volume>58</volume><issue>1</issue><fpage>30</fpage><lpage>37</lpage></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rapcsak</surname><given-names>T</given-names></name></person-group><article-title>Geodesic convexity in nonlinear optimization</article-title><source>Journal of Optimization Theory and Applications</source><year>1991</year><volume>69</volume><issue>1</issue><fpage>169</fpage><lpage>183</lpage></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>SP</given-names></name><name><surname>Vandenberghe</surname><given-names>L</given-names></name></person-group><source>Convex optimization</source><publisher-name>Cambridge university press</publisher-name><year>2004</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><p>Geometric representation of the geodesic path between the pair of matrices <inline-formula><mml:math id="M63"><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mtext>Σ</mml:mtext><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>y</mml:mi></mml:mstyle><mml:mi>k</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>M</mml:mi></mml:mstyle><mml:mtext>N</mml:mtext><mml:mi>k</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> on the PD manifold and the geometric mean between them, which is used to update <bold>Λ</bold><sup><italic>k</italic>+1</sup>.</p></caption><graphic xlink:href="EMS154005-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>Two examples of the simulated data with five active sources in presence of full-structure noise (upper panel) as well as heteroscedastic noise (lower panel) at 0 dB SNR. Topographic maps depict the locations of the ground-truth active brain sources (first column) along with the source reconstruction results of three noise learning schemes assuming noise with homoscedastic (second column), heteroscedastic (third column), or full structure (fourth column). For each algorithm, the estimated noise covariance matrix is also plotted above the topographic maps. The source reconstruction performance of these examples in terms of EMD and time course correlation (Corr) is summarized in the associated table next to each panel. Beside these two source reconstruction metrics, we also report the accuracy with which the ground-truth noise covariance was estimated in terms of the <bold>Λ</bold><sup>sim</sup> and NMSE metrics. The convergence behaviour of all three noise estimation approaches is also shown. Note that the full-structure noise learning approach converges to better minima of the negative log-likelihood than competing approaches regardless of whether the ground-truth noise covariance has full or heteroscedastic structure. However, an advantage in terms of reconstruction is only observed in the former case.</p></caption><graphic xlink:href="EMS154005-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>Source reconstruction performance (mean ± SEM) of the three different noise learning schemes for data generated by a realistic lead field matrix. Generated sensor signals were superimposed by either full-structure or heteroscedastic noise covering a wide range of SNRs. Performance was measured in terms of the earth mover’s distance (EMD), time-course correlation error, F1-measure and Euclidean distance (EUCL) in (mm) between each simulated source and the reconstructed source with highest maximum absolute correlation.</p></caption><graphic xlink:href="EMS154005-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>Accuracy of the noise covariance matrix reconstruction incurred by three different noise learning approaches assuming homoscedastic (red), heteroscedastic (green) and full-structure (blue) noise covariances. The ground-truth noise covariance matrix is either full-structure (upper row) or heteroscedastic diagonal (lower row). Performance was assessed in terms of the Pearson correlation between the entries of the original and reconstructed noise covariance matrices, <bold>Λ</bold> and <inline-formula><mml:math id="M64"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> denoted by <bold>Λ</bold><sup>sim</sup> (first column). Shown is the similarity error 1 – <bold>Λ</bold><sup>sim</sup>. Further, the normalized mean squared error (NMSE) between <bold>Λ</bold> and <inline-formula><mml:math id="M65"><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> defined as <inline-formula><mml:math id="M66"><mml:mrow><mml:mtext>NMSE</mml:mtext><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mover accent="true"><mml:mi>Λ</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msubsup><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mo>|</mml:mo><mml:mo>|</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Λ</mml:mi></mml:mstyle><mml:mo>|</mml:mo><mml:msubsup><mml:mo>|</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> and the log-det Bregman matrix divergence between original and reconstructed noise covariance matrices, denoted by <inline-formula><mml:math id="M67"><mml:mrow><mml:msub><mml:mi mathvariant="script">D</mml:mi><mml:mrow><mml:mtext>log-det</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> are reported (second and third column). The last column depicts the performance of FUN learning as well as heteroscedastic and homoscedastic noise learning for different numbers of time samples as measured by Pearson correlation error between true and reconstructed noise covariance matrices.</p></caption><graphic xlink:href="EMS154005-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><p>Auditory evoked field (AEF) localization results from one representative subject for different numbers of trial averages using FUN learning, heteroscedastic Champagne, MCE and eLORETA. All reconstructions of FUN learning algorithm show focal sources at the expected locations of the auditory cortex. Even when limiting the number of trials to as few as 10 reconstruction result of FUN learning is accurate, it severely affects the reconstruction performance of competing benchmark methods.</p></caption><graphic xlink:href="EMS154005-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><p>Localization and time series results of visual evoked field (VEF) activity for a single subject using FUN and benchmarks. Comparing with MCE and eLORETA, FUN shows accurate localization capability. Furthermore, FUN detects sharper 2<sup>nd</sup> peaks when compared to the heteroscedastic noise-learning Champagne, which is consistent with the sharp response of the VEF. The results obtained by FUN are robust across different SNRs/numbers of trial averages. For additional benchmark results, please see <xref ref-type="fig" rid="F7">Fig. 7</xref>.</p></caption><graphic xlink:href="EMS154005-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><p>Localization and reconstructed time series of visual evoked field (VEF) activity for a single subject using another four benchmark algorithms. FUN outperforms LCMV beamformer and sLORETA in terms of localization. Moreover, the activation time courses derived from homoscedastic noise learning Champagne and S-FLEX do not exhibit as sharp responses as observed for FUN. The noise level used for S-FLEX reconstructions was set to values learnt from classical Champagne algorithm with noise learning.</p></caption><graphic xlink:href="EMS154005-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><p>Localization of resting-state brain activity for three subjects using FUN and the heteroscedastic and homoscedastice noise learning variants of Champagne. The source variance patterns estimated by each algorithm are projected onto the cortical surface. The convergence behaviour of all three noise estimation approaches is also shown in terms of the negative log-likelihood cost function. FUN converges to better minima when compared to these benchmarks.</p></caption><graphic xlink:href="EMS154005-f008"/></fig></floats-group></article>