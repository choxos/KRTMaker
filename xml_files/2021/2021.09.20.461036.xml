<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS145919</article-id><article-id pub-id-type="doi">10.1101/2021.09.20.461036</article-id><article-id pub-id-type="archive">PPR398468</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Learning about threat from friends and strangers is equally effective: an fMRI study on observational fear conditioning</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Kaźmierowska</surname><given-names>Anna M.</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Szczepanik</surname><given-names>Michał</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref></contrib><contrib contrib-type="author"><name><surname>Wypych</surname><given-names>Marek</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Droździel</surname><given-names>Dawid</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Marchewka</surname><given-names>Artur</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><name><surname>Michałowski</surname><given-names>Jarosław M.</given-names></name><xref ref-type="aff" rid="A3">c</xref></contrib><contrib contrib-type="author"><name><surname>Olsson</surname><given-names>Andreas</given-names></name><xref ref-type="aff" rid="A4">d</xref></contrib><contrib contrib-type="author"><name><surname>Knapska</surname><given-names>Ewelina</given-names></name><xref ref-type="aff" rid="A2">b</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>a</label>Laboratory of Brain Imaging, Nencki Institute of Experimental Biology of Polish Academy of Sciences, 3 Pasteur Str., 02-093 Warsaw, Poland</aff><aff id="A2"><label>b</label>Laboratory of Emotions Neurobiology, BRAINCITY - Centre of Excellence for Neural Plasticity and Brain Disorders, Nencki Institute of Experimental Biology of Polish Academy of Sciences, 3 Pasteur Str., 02-093 Warsaw, Poland</aff><aff id="A3"><label>c</label>Laboratory of Affective Neuroscience, SWPS University of Social Sciences and Humanities, 10 Kutrzeby Str., 61-719 Poznań, Poland</aff><aff id="A4"><label>d</label>Department of Clinical Neuroscience, Division of Psychology, Karolinska Institutet, Stockholm, Sweden</aff><author-notes><corresp id="CR1">
<label>*</label><email>e.knapska@nencki.edu.pl</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>08</day><month>06</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Humans often benefit from social cues when learning about the world. For instance, learning about threats from others can save the individual from dangerous first-hand experiences. Familiarity is believed to increase the effectiveness of social learning, but it is not clear whether it plays a role in learning about threats. Using functional magnetic resonance imaging, we undertook a naturalistic approach and investigated whether there was a difference between observational fear learning from friends and strangers. Participants (observers) witnessed either their friends or strangers (demonstrators) receiving aversive (shock) stimuli paired with colored squares (observational learning stage). Subsequently, participants watched the same squares, but without receiving any shocks (direct-expression stage). We observed a similar pattern of brain activity in both groups of observers. Regions related to threat responses (amygdala, anterior insula, anterior cingulate cortex) and social perception (fusiform gyrus, posterior superior temporal sulcus) were activated during the observational phase, reflecting the fear contagion process. The anterior insula and anterior cingulate cortex were also activated during the subsequent stage, indicating the expression of learned threat. Because there were no differences between participants observing friends and strangers, we argue that social threat learning is independent of the level of familiarity with the demonstrator.</p></abstract><kwd-group><kwd>fear contagion</kwd><kwd>observational fear learning</kwd><kwd>familiarity</kwd><kwd>social learning</kwd><kwd>ecological validity</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Detecting potential threats in the environment is vital for survival. To learn about danger, social species, including humans, use direct and vicarious cues (<xref ref-type="bibr" rid="R42">Olsson et al., 2020</xref>). Using vicarious cues is beneficial from the evolutionary perspective as it might eliminate the risk of first-hand encounters with a threat. One of the mechanisms underlying vicarious learning about dangers is emotional contagion, an automatic, unconscious tuning into the affective state of others. Due to such emotional mimicry, the observers can ‘feel’ the threat while enjoying relative safety themselves. Through exposure to the observed danger, they can learn about the potential risks within the environment.</p><p id="P3">According to the well-established Russian doll model of empathy, the strength of the relationship between the ‘demonstrator’ and the ‘observer’ modulates emotional contagion (<xref ref-type="bibr" rid="R47">Preston &amp; de Waal, 2002</xref>). The model predicts that emotional contagion is more robust among individuals in close social relationships as their perception-action coupling is more powerful, and empathy is higher. Empirical studies have confirmed that in general, familiarity improves the social transfer of emotions (<xref ref-type="bibr" rid="R8">Bruder et al., 2012</xref>; <xref ref-type="bibr" rid="R13">de Vignemont &amp; Singer, 2006</xref>; <xref ref-type="bibr" rid="R22">Gonzalez-Liencres et al., 2014</xref>; <xref ref-type="bibr" rid="R28">Hatfield et al., 2014</xref>). Importantly, however, previous research suggests that a close social relationship does not facilitate the emotional convergence of fear (<xref ref-type="bibr" rid="R8">Bruder et al., 2012</xref>). The only report comparing the neural correlates of a threat (an electric shock) to self, a friend, and a stranger gave no conclusive results (<xref ref-type="bibr" rid="R6">Beckes et al., 2013</xref>). Besides no significant difference found in a direct friend-stranger comparison, the within-subject design employed in this study did not allow for inferring about the individual effect of different experimental conditions. Thus, the impact of familiarity on fear contagion and observational fear learning remains unclear.</p><p id="P4">To address this open question, we used a modified observational fear learning protocol (<xref ref-type="bibr" rid="R26">Haaker, Golkar, et al., 2017</xref>) improved in terms of ecological validity (<xref ref-type="bibr" rid="R55">Szczepanik, Kazmierowska, et al., 2020</xref>). We directly tested how the demonstrator’s familiarity influenced both psychophysiological (skin conductance response, SCR) and brain (functional magnetic resonance imaging, fMRI) correlates of observational fear learning in the observer. We measured SCR to include a modality used in previous studies on observational fear learning (<xref ref-type="bibr" rid="R20">Golkar et al. 2015</xref>; <xref ref-type="bibr" rid="R21">Golkar and Olsson 2017</xref>; <xref ref-type="bibr" rid="R26">Haaker et al. 2017</xref>) and validate our paradigm (<xref ref-type="bibr" rid="R55">Szczepanik, Kazmierowska, et al., 2020</xref>), as well as to enhance the power of our inferences. The experimental design (<xref ref-type="fig" rid="F1">Figure 1</xref>) involved two main stages. During the observational learning stage, two groups of participants (observers) witnessed their friends or strangers (demonstrators) undergoing a differential fear conditioning procedure. In this task, uncomfortable but not painful electric shocks were applied to the demonstrator’s forearm during the presentation of emotionally neutral visual stimuli. Subsequently, during the direct-expression stage, we tested the observers’ brain responses to previously watched stimuli. No electrical stimulation was applied to the observers throughout the whole experiment. Thus, the observers derived the knowledge about the aversive value of stimuli only from social learning. As we wanted our experimental situation to be as naturalistic as possible, no apparent ingroup-outgroup division was introduced (we did not emphasize the demonstrator’s affiliation). Also, unlike most previous studies that have used an actor serving as the demonstrator, real-time videos were transmitted to the observers in the friend-observation condition. In the stranger-observation condition, we used recordings from the friend-observation condition. Developing such an ecologically valid paradigm enabled investigating social learning of fear in a setting relevant for naturally occurring behaviors.</p><p id="P5">Based on the Russian doll model of empathy, we hypothesized that a more powerful perception-action coupling in friends sharing past experiences and/or higher level of empathy in the group of friends than strangers would lead to enhanced fear contagion. Thus, observing a friend’s fear will result in more robust activation in the fear network (including the amygdala, anterior insula, and anterior mid-cingulate cortex) than observing a stranger’s fear. We also predicted that the friend-observers would have a higher observational fear learning efficacy (indicated by more robust threat responses during the direct-expression stage) compared to the stranger-observers. We expected that the group differences in brain activations would be accompanied by the analogous differences in physiological responses (SCR).</p></sec><sec id="S2" sec-type="methods"><label>2</label><title>Methods</title><sec id="S3" sec-type="subjects"><label>2.1</label><title>Participants</title><p id="P6">48 pairs of friends (‘friend’ group) and 47 individuals (‘stranger’ group) participated in the experiment. Pairs of friends took part in the experiment together, with one participant being a demonstrator and another an observer. The demonstrators undergoing fear conditioning were live video streamed in the ‘friend’ group, and their video recordings were used in the ‘stranger’ group. Only the observers underwent fMRI. To estimate groups’ sizes that provide sufficient statistical power, we considered a similar fMRI experiment with the between-group design (n=21 and n=22 (<xref ref-type="bibr" rid="R26">Haaker et al., 2017</xref>)). We increased the number of recruited individuals as the rate of contingency-aware participants is lower in a real-time demonstrator-observer interaction (<xref ref-type="bibr" rid="R55">Szczepanik, Kazmierowska, et al., 2020</xref>). The eligibility criteria included being heterosexual male, aged between 18 and 30 years, right-handed, and native or fluent Polish speaker. Only heterosexual participants (based on self-declaration) were recruited to restrict the relationships to non-romantic male friendship to reduce sample variability. Handedness was assessed on declarative level, both in the recruitment and fMRI safety-screening forms. We excluded students and graduates of either psychology or cognitive science, participants with neurological disorders or other medical conditions precluding MR scanning or electrical stimulation, and participants taking psychoactive drugs. Additionally, in the ‘friend’ group, the participants had to have known each other for at least three years and score at least 30 out of 60 points in the McGill Friendship Questionnaire - Respondent’s Affection (<xref ref-type="bibr" rid="R40">Mendelson &amp; Aboud, 1999</xref>), see <xref ref-type="sec" rid="S17">section 2.4.1</xref>.</p><p id="P7">We excluded four subjects from the ‘stranger’ group: three subjects who experienced technical problems with the video playback, and one subject who showed excessive head motion during one of the tasks (more than 25% volumes classified as motion outliers, see <xref ref-type="sec" rid="S24">fMRI Data Preprocessing</xref> section). We included only the contingency-aware participants in the analyses; please refer to <xref ref-type="sec" rid="S20">section 2.4.4</xref>. The final group sizes were n = 35 (‘friend’ group) and n = 34 (‘stranger’ group). In the analyzed sample, the mean age of all observers was 22.9 years (SD = 2.87), the mean length of the observer-demonstrator friendship was 7.7 years (SD = 4.22), and the mean observers’ score in the McGill Friendship Questionnaire was 52 (SD = 7.74), see <xref ref-type="supplementary-material" rid="SD1">Table S2 in Appendix</xref> for the detailed statistics. All participants received financial remuneration of 100 PLN (~23 EUR) for their participation.</p></sec><sec id="S4"><label>2.2</label><title>Tasks and Stimuli</title><sec id="S5"><label>2.2.1</label><title>Experimental Setup</title><p id="P8">In the MRI scanner, the observer watched a video (either a streaming or a replay, both without sound) or picture stimuli on an MR-compatible monitor through a mirror box placed on the head coil. The demonstrator sat in a small room adjacent to the MR room (in the ‘friend’ group). A GoPro Hero7 camera provided video transmission and recording. To ensure reliable reproduction of stimuli’ colors, we lighted the room with an LCD panel with adjustable color temperature, set the computer screen brightness to low, and adjusted the camera’s white balance. The room walls were covered with gray acoustic foam to minimize distractors.</p></sec><sec id="S6"><label>2.2.2</label><title>Stimuli</title><p id="P9">The conditioned stimuli were two squares, blue and yellow, displayed on a gray background. The squares were presented centrally and covered either a half (during the observation stage) or a quarter (during the direct-expression stage) of screen height (<xref ref-type="fig" rid="F2">Figure 2</xref>). The assignment of color to CS+ and CS- was counterbalanced across participants.</p><p id="P10">Cutaneous electrical stimulation, applied to the ventral part of a forearm of the demonstrator, was used as the unconditioned stimulus. Stimulating electrodes were placed above the <italic>flexor carpi radialis</italic> muscle so that even low-intensity stimulation caused involuntary muscle flexion, visible to the observer. The stimulation consisted of five unipolar pulses of 1 ms duration applied in 200 ms intervals. The demonstrators individually adjusted shock intensity to be unpleasant but not painful (see <xref ref-type="sec" rid="S12">section 2.3.3</xref>).</p></sec><sec id="S7"><label>2.2.3</label><title>Observational Learning Stage</title><p id="P11">The observer watched the demonstrator performing a differential conditioning task. The demonstrator watched 24 CS+, and 24 CS- displayed on the screen in pseudo-random order, with a given CS repeated at most twice in a row. Each CS lasted 9 seconds, half of the CS+ reinforced with the US. The US reinforced the first and the last presentation of CS+. The US started 7.5 seconds after the CS onset to allow the demonstrator’s reaction to co-terminate with the CS. The CS- was never reinforced. The intertrial intervals (ITIs) lasted randomly between 10 and 15 seconds, with a fixation symbol (+) displayed centrally on the screen (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p></sec><sec id="S8"><label>2.2.4</label><title>Direct-expression Stage</title><p id="P12">The observer watched 12 CS+ and 12 CS-. The stimuli and the stimuli’ order, timing, and ITI were the same as during the observational learning stage. None of the CS was reinforced.</p></sec></sec><sec id="S9" sec-type="methods"><label>2.3</label><title>Procedure</title><p id="P13">To investigate observational fear learning, we adapted the protocol of <xref ref-type="bibr" rid="R26">Haaker, Golkar, et al. (2017)</xref> for live observation of demonstrator-observer interaction (<xref ref-type="bibr" rid="R55">Szczepanik, Kazmierowska, et al., 2020</xref>). The protocol was approved by the Ethics Committee of the Faculty of Psychology at the University of Warsaw (decision from 28 November 2017). The procedure complied with the Ethical Principles of Psychologists and Code of Conduct published by the Polish and the American Psychological Associations.</p><sec id="S10"><label>2.3.1</label><title>Before the Experiment</title><p id="P14">The participants received brief information about the experimental procedure, including the possibility of receiving aversive electrical stimulation. Next, the participants gave informed consent (including optional permission for recording and recording’s reuse) and filled in safety screening forms. The roles of a demonstrator and an observer were then randomly assigned to the participants by giving two color-coded envelopes. Then the participants were isolated; the demonstrators went to a room adjacent to the MR room.</p></sec><sec id="S11"><label>2.3.2</label><title>Observer’s Preparation and Instructions</title><p id="P15">First, the observers filled in the questionnaires (see <xref ref-type="sec" rid="S16">section 2.4</xref> for details). Next, they opened the envelope with the role assignment and received instructions. The instruction included information on the two-stages procedure in the fMRI scanner. The instruction said that, firstly, the participants would watch their friends perform a task involving the presentation of colored symbols and the administration of unpleasant electrical stimulation. Then, they would perform an identical task themselves. Importantly, no information about the stimuli contingency was provided. After receiving the instruction, the observers had stimulation and skin conductance electrodes attached and went to the MR room. In the scanner, the subjects had sham leads connected to the stimulation electrodes attached to make receiving electrical stimulation believable.</p></sec><sec id="S12"><label>2.3.3</label><title>Demonstrator’s Preparation and Instructions</title><p id="P16">First, the demonstrators filled in the questionnaires (see <xref ref-type="sec" rid="S16">section 2.4</xref> for details). Next, they opened the envelope with the role assignment and received instructions. The instruction said they would perform a task involving the presentation of colored symbols and administering unpleasant, but not painful, electrical stimulation. Unlike the observers, the demonstrators received information on the stimuli contingency and reinforcement rules. We asked the demonstrators to react to the stimulation in a natural yet noticeable manner. The demonstrators watched a recording with a model reaction. After receiving the instruction, the demonstrators had stimulation and sham skin conductance electrodes attached. Afterward, the demonstrators adjusted stimulation intensity. The experimenter increased it stepwise, and the participants rated the intensity using a scale from 1 (<italic>imperceptible</italic>) to 8 (<italic>painful</italic>). The target level was 6 (<italic>very unpleasant but not painful</italic>).</p></sec><sec id="S13"><label>2.3.4</label><title>MRI Procedure</title><p id="P17">The MRI scanning started with the acquisition of an anatomical image. Then, the observational learning stage followed (see <xref ref-type="sec" rid="S7">section 2.2.3</xref>). The experimenter adjusted the camera’s position to ensure that the observer could see the demonstrator’s face, hand, and computer screen and turned on video transmission and recording. The observer received a brief reminder: ‘in this part of the study, you will observe your friend performing a certain task’. After completion of the observational learning stage, the observer received a brief reminder: ‘in this part of the study, you will perform the same task that you just watched’, and the direct-expression stage ensued (see <xref ref-type="sec" rid="S8">section 2.2.4</xref>).</p></sec><sec id="S14"><label>2.3.5</label><title>Conclusion of the Experiment</title><p id="P18">While the observer took part in the direct-expression stage, the demonstrator completed the post-experimental questionnaires (see <xref ref-type="sec" rid="S16">section 2.4</xref>). After the MRI session ended, the observer also completed their set of post-experimental questionnaires, and both participants were debriefed about the study.</p></sec><sec id="S15"><label>2.3.6</label><title>‘Stranger’ Group</title><p id="P19">In the ‘stranger’ group, only one subject (the observer) participated in the experimental session. Instead of a live stream, we used 45 recordings made in the ‘friend’ group. Otherwise, the procedure in the ‘stranger’ group was very similar to that involving a pair of friends and differed only in the necessary aspects. There was no role assignment, and instructions given to the observers referred to another person rather than a friend.</p></sec></sec><sec id="S16"><label>2.4</label><title>Behavioral Measures</title><sec id="S17"><label>2.4.1</label><title>McGill Friendship Questionnaire - Respondent’s Affection</title><p id="P20">To recruit pairs of friends, we used the McGill Friendship Questionnaire - Respondent’s Affection (<xref ref-type="bibr" rid="R40">Mendelson &amp; Aboud, 1999</xref>); translated by A. Kazmierowska, P. Pączek, and A. Schudy for online screening in this study. It consists of 16 positive statements describing feelings for a friend and satisfaction with the friendship, rated along a 9-point scale ranging from -4 (<italic>very much disagree</italic>) to 4 (<italic>very much agree</italic>). One item (no. 9) was omitted from the questionnaire due to human error. A score of 30 points was a threshold for inclusion in the study.</p></sec><sec id="S18"><label>2.4.2</label><title>Empathic Sensitiveness Scale</title><p id="P21">To measure the participants’ empathy, we used the Empathic Sensitiveness Scale (original title: Skala Wrazliwosci Empatycznej, SWE (<xref ref-type="bibr" rid="R35">Kaźmierczak et al., 2007</xref>)). It is a self-report questionnaire based on the Interpersonal Reactivity Index (<xref ref-type="bibr" rid="R11">Davis, 1983</xref>), but with far-reaching modifications developed in the Polish language. It contains 28 statements, which reflect three components of empathy: two emotional (empathic concern, personal distress) and one cognitive (perspective taking). The answers are given on a 5-point scale. The demonstrators and observers filled in the questionnaire at the end of the experimental procedure.</p></sec><sec id="S19"><label>2.4.3</label><title>State and Trait Anxiety Inventory</title><p id="P22">To measure participants’ anxiety, we used the State and Trait Anxiety Inventory (<xref ref-type="bibr" rid="R52">Spielberger et al., 1983</xref>, polish adaptation 2012). It is a self-report questionnaire consisting of two 20-item subscales, STAI-State and STAI-Trait. Participants rate statements related to how they feel either at a given moment (state) or usually (trait) using 4-point Likert scales. Each participant completed the STAI-state twice (at the beginning and the end of the experiment) and STAI-trait once (at the end of the experiment).</p></sec><sec id="S20"><label>2.4.4</label><title>Assessment of Stimulus Contingency Awareness</title><p id="P23">To assess the observers’ declarative knowledge of the CS/US contingency, we used an online questionnaire adapted from (<xref ref-type="bibr" rid="R59">Weidemann et al., 2016</xref>). The questionnaire referred to the observational learning stage and was divided into several parts to eliminate guessing. Firstly, it asked whether the participant could predict shocks to the demonstrator. Then, if answered positively, an open-ended question ‘how’ followed. Next, participants gave percentage ratings of shock occurrence for each stimulus. Finally, they made a forced choice of one stimulus paired with the shock. CS+, CS- and a fixation symbol were all presented as stimuli in the latter two questions. Observers completed this questionnaire after the direct-expression stage to avoid interfering with the learning process (<xref ref-type="bibr" rid="R26">Haaker, Golkar, et al., 2017</xref>).</p></sec><sec id="S21"><label>2.4.5</label><title>Evaluation of the Demonstrator’s Expression (the Observational US)</title><p id="P24">To control how the observers perceived the demonstrators’ behavior, we used a set of questions suggested by <xref ref-type="bibr" rid="R26">Haaker, Golkar, et al. (2017)</xref>. At the end of the experiment, we asked the observers to rate: how much discomfort the demonstrator experienced when receiving the electrical stimulation, how expressive the demonstrator was, how natural the demonstrator’s reactions were, and how much empathy they felt for the demonstrator. Additionally, we asked the observers to rate the degree of unpleasantness attributed to the demonstrators. Finally, in the ‘friend’ group, the observer scored the degree to which they identified with their friend. This question was not asked in the ‘stranger’ group due to the difficulties in understanding the concept of identification with strangers that we observed during the pilot study. All ratings used a 10-point Likert scale, ranging from 0 (<italic>not at all</italic>) to 9 (<italic>very much)</italic>, except for the unpleasantness rating, which used a 5-point Likert scale ranging from 1 (<italic>very unpleasant</italic>) to 5 (<italic>very pleasant</italic>).</p></sec></sec><sec id="S22"><label>2.5</label><title>Skin Conductance Response Recordings</title><p id="P25">During fMRI sessions, we registered skin conductance responses as a secondary measure. It was recorded using BrainVision BrainAmp ExG MR amplifier with GSR MR sensor, sampled at 250 Hz.</p></sec><sec id="S23"><label>2.6</label><title>fMRI Data Acquisition</title><p id="P26">The MRI data were acquired on a 3T Siemens Magnetom Trio scanner equipped with a 12-channel head coil. At the beginning of a session, a T1-weighted anatomical image was acquired using an MPRAGE sequence with 1 × 1 × 1 mm resolution and the following parameters: inversion time TI = 1100 ms, GRAPPA parallel imaging with acceleration factor PE = 2, acquisition time TA = 6 minutes and 3 seconds. After acquiring the anatomical scans, two functional imaging runs followed (observational learning and direct-expression tasks). The first run contained 362 volumes in the ‘friend’ group and 380 volumes in the ‘stranger’ group. The scanning started earlier in the ‘stranger’ group, but the leading volumes were removed from the analysis. The second run of scanning contained 184 volumes. Each functional volume comprised 47 axial slices (2.3 mm thick, with 2.3 × 2.3 mm in-plane resolution and 30% distance factor) that were acquired using a T2*-sensitive gradient echo-planar imaging (EPI) sequence with the following parameters: repetition time TR = 2870 ms, echo time TE = 30 ms, flip angle FA = 90 degrees, field of view FoV = 212 mm, matrix size: 92 × 92, interleaved acquisition order, GRAPPA acceleration factor PA = 2.</p></sec><sec id="S24"><label>2.7</label><title>fMRI Data Preprocessing</title><p id="P27">The fMRI data were preprocessed using fMRIPrep 1.4.0 (<xref ref-type="bibr" rid="R16">Esteban, Blair, et al., 2019</xref>; <xref ref-type="bibr" rid="R16">Esteban, Markiewicz, et al., 2019</xref>), based on Nipype 1.2.0 (<xref ref-type="bibr" rid="R23">Gorgolewski et al., 2011</xref>; <xref ref-type="bibr" rid="R24">Gorgolewski et al., 2019</xref>) and Nilearn 0.5.2 (<xref ref-type="bibr" rid="R1">Abraham et al., 2014</xref>). We followed the fMRIprep preprocessing with smoothing in SPM (SPM 12 v7487, Wellcome Centre for Human Neuroimaging, London, UK). At the beginning of the fMRIprep pipeline, the anatomical images were corrected for intensity non-uniformity with N4BiasFieldCorrection (<xref ref-type="bibr" rid="R57">Tustison et al., 2010</xref>), distributed with ANTs 2.2.0 (<xref ref-type="bibr" rid="R4">Avants et al., 2008</xref>), and used as an anatomical reference. The anatomical reference was then skull-stripped with antsBrainExtraction (from ANTs) and segmented using fast from FSL 5.0.9 (<xref ref-type="bibr" rid="R62">Zhang et al., 2001</xref>). Finally, the anatomical images were normalized to the MNI space through nonlinear registration with antsRegistration (ANTs 2.2.0). The ICBM 152 Nonlinear Asymmetrical template version 2009c was used (<xref ref-type="bibr" rid="R18">Fonov et al., 2009</xref>).</p><p id="P28">The functional images from the observational learning and direct-expression stages were preprocessed in the following manner. First, a reference volume was generated using a custom methodology of fMRIPrep. This reference was co-registered to the anatomical reference using flirt from FSL 5.0.9 (<xref ref-type="bibr" rid="R33">Jenkinson &amp; Smith, 2001</xref>) with the boundary-based registration cost-function (<xref ref-type="bibr" rid="R25">Greve &amp; Fischl, 2009</xref>). Head-motion parameters with respect to the functional reference volume (transformation matrices and six corresponding rotation and translation parameters) were estimated before any spatiotemporal filtering using mcflirt from FSL 5.0.9 (<xref ref-type="bibr" rid="R32">Jenkinson et al., 2002</xref>). The functional scanning runs were slice-time corrected using 3dTshift from AFNI 20160207 (<xref ref-type="bibr" rid="R10">Cox &amp; Hyde, 1997</xref>). Next, the functional images were resampled into the MNI space. All resamplings were performed in a single interpolation step (composing head-motion transform matrices and co-registrations to anatomical and output spaces) using antsApplyTransforms (ANTs). Finally, we smoothed the functional images with a 6 mm FWHM 3D Gaussian kernel using spm_smooth (SPM 12). Framewise displacement (FD) and the derivative of root mean square variance over voxels (DVARS) were calculated by fMRIPrep for each functional scan, both using their implementations in Nipype and following the definitions by (<xref ref-type="bibr" rid="R46">Power et al., 2014</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers. For more details on the fMRIprep pipeline, see fMRIPrep’s documentation at <ext-link ext-link-type="uri" xlink:href="https://fmriprep.org/en/latest/workflows.html">https://fmriprep.org/en/latest/workflows.html</ext-link>.</p><sec id="S25"><title>Physiological Data Analysis</title><p id="P29">To analyze skin conductance data we used PsPM 4.3.0 software (<ext-link ext-link-type="uri" xlink:href="https://bachlab.github.io/PsPM/">https://bachlab.github.io/PsPM/</ext-link>) running under MATLAB 2018b (MathWorks, Natick, MA, USA). We used a non-linear model for event-related SCR. In this analysis, three parameters of sudomotor nerve response function (its amplitude, latency and dispersion) are estimated to best match the observed skin conductance data. This model is suitable for anticipatory responses in fear conditioning and assumes that the response onset following CS presentation is not precisely known (<xref ref-type="bibr" rid="R5">Bach et al., 2010</xref>; <xref ref-type="bibr" rid="R54">Staib et al., 2015</xref>). Before the analysis, we visually inspected the signals for artifacts and manually marked missing epochs. We excluded subjects with missing data or excessive presence of artifacts precluding further analysis. As a result, we analyzed SCR data for 27 subjects per group. We used the default settings for preprocessing: signals were filtered using bi-directional 1st order Butterworth filters, with 5 Hz low-pass and 0.0159 Hz high-pass cut-off frequencies, and resampled to 10 Hz. We performed no response normalization. We accelerated the processing of multiple subjects using GNU Parallel v.20161222 (<xref ref-type="bibr" rid="R56">Tange, 2011</xref>).</p></sec></sec><sec id="S26"><label>2.8</label><title>fMRI Data Analysis</title><sec id="S27"><label>2.8.1</label><title>Activation Analysis</title><p id="P30">To analyze data we used a mass univariate approach based on a general linear model. We used SPM 12 software (SPM 12 v7487, Wellcome Centre for Human Neuroimaging, London, UK) running under MATLAB 2020a (MathWorks, Natick, MA, USA). First-level models contained four types of events in the observational learning stage: CS+, CS-, US, and no US (i.e., lack of US during 50% of CS+). The observational CS were modeled as instantaneous events (i.e., CS onset), while the observational US/no US events were 1.5 seconds (i.e., from US onset to CS offset). There were only two types of events in the direct-expression stage, CS+ and CS-, modeled as 9 seconds (entire presentation of CS). Additionally, we included temporal modulation of the 1st order to capture effects of extinction in the direct-expression stage. In addition to the event regressors described above, we had six motion parameters (translation and rotation) as regressors of no interest. We added one delta regressor for each volume annotated by fMRIPrep as a motion outlier (up to 60 such volumes per subject in the observational learning stage, up to 38 in the direct-expression stage, median 8 and 2, respectively).</p><p id="P31">The US &gt; no US was the primary contrast of interest in the observational learning stage and the CS+ &gt; CS- in the direct-expression stage. We calculated the contrasts within-subject and used the parameter estimates in second-level analyses. We performed two types of second-level analyses in SPM: one-sample t-test designs for data pooled across groups (the ‘friend’ and ‘stranger’ groups) and two-sample t-tests for between-groups comparisons (the ‘friend’ vs. the ‘stranger’ group). Additionally, we used a temporal modulation regressor (CS+ × t) for both groups analyzed together and for between-group comparisons. We thresholded the second-level statistical maps using family-wise error (FWE) correction, with a p = 0.05 threshold. We used the voxel-level correction for the primary contrasts and cluster size correction with p = .001 cluster defining threshold for the results of temporal modulation and psychophysiological interactions.</p></sec><sec id="S28"><label>2.8.2</label><title>Region-of-interest - Definitions and Analysis</title><p id="P32">To perform the ROI analysis (parameter estimate extraction), the psychophysiological interaction analysis (time course extraction), and, in the case of the amygdala, the small volume correction (SVC) analysis, we defined anatomical regions of interest (ROI). We used the following brain structures as ROIs: the anterior insula (AI), anterior mid-cingulate cortex (aMCC), amygdala, right fusiform face area (rFFA), right posterior superior temporal sulcus (rpSTS), and right temporo-parietal junction (rTPJ). We chose regional definitions for these structures based on a meta-analytic connectivity mapping study focusing on social processes (<xref ref-type="bibr" rid="R2">Alcalá-López et al. 2018</xref>) and the Neurovault collection (<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/neurovault.collection:2462">https://identifiers.org/neurovault.collection:2462</ext-link>).</p><p id="P33">In the case of AI and amygdala, we combined anatomical masks from the left and right hemispheres and treated them as single ROIs. For the FFA, TPJ and pSTS, the regions considered functionally lateralized (<xref ref-type="bibr" rid="R2">Alcalá-López et al., 2018</xref>; <xref ref-type="bibr" rid="R7">Boccadoro et al., 2019</xref>; <xref ref-type="bibr" rid="R51">Sliwinska &amp; Pitcher, 2018</xref>; <xref ref-type="bibr" rid="R61">Yovel et al., 2008</xref>), we used only the right-hemispheric masks. For the aMCC, which is a centrally-located structure, we used no hemispheric division. We used the same definitions across the ROI and PPI analyses. For small volume correction, which was applied only for the amygdala, we used an alternative bilateral definition based on the Harvard - Oxford atlas (<xref ref-type="bibr" rid="R12">Desikan et al., 2006</xref>) thresholded at 0.7. This definition was consistent with a previous study on observational fear conditioning (<xref ref-type="bibr" rid="R39">Lindstörm et al., 2018</xref>) and provided better structure coverage than the more restrictive meta-analytic definition. For the ROI analysis, the first-level parameter estimates were extracted and averaged within a given region for each participant using the spm_summarise function from SPM and compared using the BayesFactor package in R (<xref ref-type="bibr" rid="R41">Morey &amp; Rouder, 2018</xref>).</p></sec><sec id="S29"><label>2.8.3</label><title>Psychophysiological Interactions</title><p id="P34">To perform the psychophysiological interaction analysis (PPI), we extracted the functional time series (1st eigenvariate) from the ROIs defined above and multiplied with the psychological variable (observation stage: US &gt; no US; direct-expression stage: CS+ &gt; CS-) using SPM VOI and PPI modules. We entered the PPI time series into a first-level model as the regressor of interest. We also included the functional time series and the psychological variable in the model as regressors and the motion parameters and motion outliers (the same as in the models used for activation analysis). Then we subjected parameter estimates obtained for the PPI regressor to second level analyses, using one-sample (both groups analyzed together) and two-sample (between-group) designs. We thresholded the statistical maps using cluster-based FWE correction, with cluster-defining threshold of p = 0.001 and corrected p-value of 0.05. Additionally, we applied a small volume correction to the amygdala.</p></sec><sec id="S30"><label>2.8.4</label><title>Statistical Analysis</title><p id="P35">To perform most statistical analyses, we used R with <italic>afex</italic>, <italic>emmeans</italic>, and <italic>BayesFactor</italic> libraries, with plots made using <italic>the ggpubr</italic> library. We calculated Bayesian ANOVA using JASP. We used repeated-measures ANOVA (with type III errors) and Bayesian repeated-measures ANOVA to analyze STAI-state scores (with the group as a between-subject factor and the questionnaire filling time, before and after the experiment, as a within-subject factor) and skin conductance response amplitudes (with the group and stimulus factors). We used the Wilcoxon-Mann-Whitney test to compare the discrete ratings describing the demonstrator’s expression (observational US) between the groups and a chi-squared test to compare the distribution of contingency-aware participants. We used t-test (with Welch’s correction for unequal variance) and Bayesian t-test (using default effect size priors, i.e., Cauchy scale 0.707) to compare the results of STAI-trait and subscales of SWE, and region-of-interest parameter estimates from fMRI.</p><p id="P36">For the classical tests, we applied a significance level of p = 0.05. For Bayesian t-tests, we used BF<sub>10</sub> to denote evidence for the alternative hypothesis and BF<sub>01</sub> for the null hypothesis. In Bayesian ANOVA, we reported the effects as the Bayes factor for the inclusion of a given effect (BF<sub>incl</sub>), calculated as a ratio between the models’ average likelihood, including the factor (given the data) vs. that of the models without the factor. When interpreting these results, we followed the convention suggested by <xref ref-type="bibr" rid="R36">Keysers et al. (2020)</xref> and considered ⅓ &lt; BF &lt; 3 as absence of evidence, 1/10 &lt; BF &lt; ⅓ or 3 &lt; BF &lt; 10 as moderate evidence and BF &lt; 1/10 or BF &gt; 10 as strong evidence.</p></sec></sec></sec><sec id="S31" sec-type="results"><label>3</label><title>Results</title><sec id="S32"><label>3.1</label><title>Behavioral Results</title><sec id="S33"><label>3.1.1</label><title>Stimulus Contingency Awareness</title><p id="P37">After completing both stages of the experiment, the observers answered a set of questions about the relationship between the electrical stimulation and the visual cues in the observational learning stage (see <xref ref-type="sec" rid="S20">section 2.4.4</xref>). 35 out of 48 participants in the ‘friend’ group and 35 out of 44 participants in the ‘stranger’ group correctly identified the CS+/US contingency. While the previous studies have reported rare cases of contingency-unaware participants (<xref ref-type="bibr" rid="R20">Golkar et al. 2015</xref>; Haaker et al. 2021), our ratio of the CS+/US contingency unaware subjects is higher. The high ratio is most likely a consequence of a real-time demonstrator-observer interaction, contributing to the less structured and thus more demanding learning context. A similar effect was observed in our previous study (<xref ref-type="bibr" rid="R55">Szczepanik, Kaźmierowska, et al., 2020</xref>). The proportion of contingency-aware and non-aware participants was not significantly different between groups (chi-squared = 0.554, df = 1, p = 0.46). We included only the contingency-aware participants in further analyses as we found that the learning efficacy depends on the contingency knowledge (<xref ref-type="bibr" rid="R55">Szczepanik, Kaźmierowska, et al., 2020</xref>).</p></sec><sec id="S34"><label>3.1.2</label><title>Evaluation of the Demonstrator’s Expression (the Observational US)</title><p id="P38">Next, the observers evaluated the demonstrator’s expression (the observational US) by answering a set of questions scored on 0 - 9 scales. The questions concerned the demonstrator’s reactions to electrical stimulation (how much discomfort they experienced, how expressive they were, and how natural their reactions were) and their attitude (how much empathy they felt for the demonstrator). Additionally, in the pairs of friends, the observers assessed how well they could identify with the demonstrator (this question was not asked in the ‘stranger’ group, see <xref ref-type="sec" rid="S21">section 2.4.5</xref>.). Median ratings in both the ‘friend’ and ‘stranger’ groups were between 5 and 7 (see <xref ref-type="fig" rid="F3">Figure 3</xref>). Additionally, we asked the observers to rate the degree of unpleasantness attributed to the demonstrators on a 1 - 5 scale (from very unpleasant to very pleasant). The median ratings were between 1 (very unpleasant) and 2 (rather unpleasant). The Wilcoxon-Mann-Whitney test showed no significant differences between the groups in any category. See <xref ref-type="supplementary-material" rid="SD1">Table S3 in the Appendix</xref> for detailed statistics.</p></sec><sec id="S35"><label>3.1.3</label><title>Empathy and Anxiety - Questionnaire Results</title><p id="P39">The state anxiety was tested twice, before and after the experiment. The results were analyzed for the contingency-aware observers, who were included into the fMRI analysis. Repeated-measures ANOVA and analogous Bayesian repeated measures ANOVA with the group (friend, stranger) as a between-subject factor and measurement (before, after) as a within-subject factor showed no significant differences: main effect of the group: F(1, 67) = 0.76, η<sub>g</sub><sup>2</sup> = .009, <italic>p</italic> = .39, BF<sub>incl</sub> = 0.35; main effect of the measurement: F(1,67) = 1.92, η<sub>g</sub><sup>2</sup> = .007, p = .17, BF<sub>incl</sub> = 0.36; group × measurement interaction F(1,67) = 1.29, η<sub>g</sub><sup>2</sup> = .004, p = .259, BF<sub>incl</sub> = 0.14.</p><p id="P40">The trait anxiety index, and three subscales of the empathic sensitiveness scale were treated independently, and compared between friend- and stranger-observers groups using two sample t-tests. Again, none of the comparisons was statistically significant and indicated either absence of evidence or moderate evidence against the difference: trait anxiety: t = 1.07, df = 63.5, p = 0.29, BF = 0.40; empathic concern: t = 0.20, df = 59.0, p = 0.85, BF = 0.25; personal distress t = 1.11, df = 64.1, p = 0.27, BF = 0.42; perspective taking: t = 0.12, df = 63.2, p = 0.91, BF = 0.25.</p></sec></sec><sec id="S36"><label>3.2</label><title>Skin Conductance Responses</title><p id="P41">Amplitudes of the observers’ skin conductance responses (SCR) were compared (separately for each stage of the experiment) using classical and Bayesian repeated measures ANOVA, with group (friend, stranger) as a between subjects factor. In the observational learning stage, ANOVA revealed a significant main effect of the stimulus (US / no US), F(1, 52) = 29.19, η<sub>g</sub><sup>2</sup> = .089, p &lt; .001, BF<sub>incl</sub> = 6976 where participants showed stronger reactions to the US compared to no US, t(52) = 5.40, p &lt; .001, BF = 11341 (<xref ref-type="fig" rid="F4">Figure 4 A</xref>). The main effect of the group, F(1,52) = 2.21, η<sub>g</sub><sup>2</sup> = .034, p = .14, BF<sub>incl</sub> = 0.75, and the stimulus × group interaction, F(1,52) = 0.55, η<sub>g</sub><sup>2</sup> = .002, p = .46, BF<sub>incl</sub> = 0.63, were not significant. Similarly, in the direct-expression stage, we found a significant effect of the stimulus (CS+/CS-), F(1, 52) = 5.73, η<sub>g</sub><sup>2</sup> = .018, p = .02, BF<sub>incl</sub> = 1.79, with stronger reactions to CS+ than CS-, t(52) = 2.39, p = .02 , BF = 2.02 (<xref ref-type="fig" rid="F4">Figure 4 B</xref>), while the main effect of the group, F(1, 52) = 1.89, η<sub>g</sub><sup>2</sup> = .029, p = .18, BF<sub>incl</sub> = 0.62, and the stimulus * group interaction, F(1,52) = 0.83, η<sub>g</sub><sup>2</sup> = .003, p = .37, BF<sub>incl</sub> = 0.464, were not significant.</p></sec><sec id="S37"><label>3.3</label><title>Imaging Results</title><sec id="S38"><label>3.3.1</label><title>Brain Activation Analysis - Observational Learning Stage</title><p id="P42">In the observational learning stage, we analyzed the whole-brain activity of the observers witnessing the demonstrator’s reaction to the aversive stimulation. We compared it to the corresponding periods when the CS appeared without aversive stimulus (US &gt; no US). To test the main effect of the task, we first evaluated the US &gt; no US contrast for both groups. It revealed a robust and extensive activation (p &lt; 0.05, FWE peak-level correction) of multiple brain regions, including the bilateral amygdala, bilateral anterior insula (AI), and anterior mid-cingulate cortex (aMCC), the bilateral posterior superior temporal sulcus (pSTS) and bilateral fusiform gyrus, including the fusiform face area (FFA).</p><p id="P43">Next, we compared the ‘friend’ and ‘stranger’ groups directly, i.e., with the (friend US &gt; no US) vs. (stranger US &gt; no US) using t contrasts (see <xref ref-type="fig" rid="F5">Figure 5 A</xref>). The whole-brain analysis yielded no significant between-group differences (with either peak- or cluster-level FWE corrections). To further compare the groups, we performed region-of-interest analysis (averaging parameter estimates within a region) for the six pre-selected areas, defined independently of the functional data (AI, aMCC, amygdala, rFFA, rpSTS, rTPJ). We found no significant differences between the ‘friend’ and ‘stranger’ groups; Bayes Factor analysis indicated moderate evidence for the absence of effects (BF<sub>01</sub> &gt; 3) in all regions except the rFFA and rTPJ, where the evidence was inconclusive (BF<sub>01</sub> = 2.5 and 2.07 respectively), see <xref ref-type="table" rid="T1">Table 1</xref> and <xref ref-type="fig" rid="F5">Figure 5 B</xref>.</p><p id="P44">An additional between-group comparison, based on the observational US reactions, i.e., friend US &gt; stranger US t contrast, also did not yield significant differences on a whole-brain level. Region-of-interest analysis for this contrast (using the same set of regions as above) confirmed the lack of significant between-group differences. At the same time, Bayes factors indicated evidence that was either inconclusive or moderately favored the null hypothesis (all BF<sub>01</sub> between 1.36 and 3.65), see <xref ref-type="table" rid="T1">Table 1</xref> and <xref ref-type="supplementary-material" rid="SD2">Inline Supplementary Figure 5</xref>.</p></sec><sec id="S39"><label>3.3.2</label><title>Brain Activation Analysis - Direct-expression Stage</title><p id="P45">To measure observational fear learning, i.e., the association formed between the CS and the observational US, in the direct-expression stage, we analyzed the brain activity of the observers in response to the conditioned stimuli (CS+ &gt; CS-). To evaluate the main effect of the task, the CS+ &gt; CS- contrast was first analyzed for both groups together and subsequently compared between the groups.</p><p id="P46">Analyzing both groups together, we observed significant activations in the bilateral AI and the aMCC (<xref ref-type="fig" rid="F6">Figure 6</xref>). Next, we performed a direct between-group comparison, i.e., friend (CS+ &gt; CS-) vs. stranger (CS+ &gt; CS-), which yielded no significant differences on the whole-brain level. We performed a region-of-interest analysis with the same set of regions as in the observational learning stage to further compare the groups. We found no significant differences, and Bayes Factor analysis indicated moderate evidence for the absence of effects (BF<sub>01</sub> &gt; 3) in all regions except the rFFA and rpSTS where the evidence was inconclusive (BF<sub>01</sub> = 2.47 and 2.32 respectively); see <xref ref-type="table" rid="T1">Table 1</xref> and <xref ref-type="fig" rid="F8">Figure 8</xref>.</p><p id="P47">To capture the potential effects of extinction during the direct-expression stage, we included 1st order temporal modulation in the statistical model. Analyzing both groups together, we observed a significant decrease in the brain responses to CS+ across several regions, most notably in the aMCC. To test whether the groups differed concerning the temporal dynamics of brain responses, we evaluated the (friend CS+ × t) vs. (stranger CS+ × t) contrast. We observed the differences in some brain regions, including the left inferior frontal gyrus, left middle temporal gyrus, right lingual gyrus, and left putamen. Strangers showed a stronger linear decrease in activity than friends (see <xref ref-type="fig" rid="F7">Figure 7</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S6</xref>). Importantly, however, we found no differences in the structures activated in the observational and direct-expression stages.</p></sec><sec id="S40"><label>3.3.3</label><title>Psychophysiological Interactions</title><p id="P48">We performed a psychophysiological interaction (PPI) analysis for several selected ROIs to investigate the coupling between the activated structures. First, we analyzed the observational learning stage, focusing on the US &gt; no US contrast. As the seed structures, we used the AI, rpSTS, aMCC, amygdala, rFFA, and rTPJ. Analyzing both groups together, we observed increased coupling of the AI with several regions, including the rpSTS (<xref ref-type="fig" rid="F8">Figure 8 A</xref>), and stronger coupling of the rpSTS with the AI, right fusiform gyrus (<xref ref-type="fig" rid="F8">Figure 8 B</xref>), and amygdala (<xref ref-type="fig" rid="F8">Figure 8 C</xref>). However, there were no significant differences between the groups. Similarly, we did not find the group differences using the aMCC, amygdala, rFFA, and rTPJ as the seed structures. Further, we applied the PPI analysis to the activations observed during the direct-expression stage, using the AI, aMCC, and amygdala as the seeds. We did not find significant differences for the groups analyzed together or between the friend and stranger groups.</p></sec><sec id="S41"><label>3.3.4</label><title>Multivariate pattern analysis</title><p id="P49">To explore if possible group differences could be encoded in the patterns of activations rather than in the activation strength in specific regions, we have tried a multivariate approach. We used classification (decoding), one of the most popular multivariate pattern analysis (MVPA) methods (<xref ref-type="bibr" rid="R29">Haynes 2015</xref>; <xref ref-type="bibr" rid="R58">Valente et al. 2021</xref>), and employed The Decoding Toolbox (<xref ref-type="bibr" rid="R30">Hebart et al. 2014</xref>). Due to the limitations of our experimental design (which was not planned for the MVPA analysis), we treat this analysis as exploratory and report it in the <xref ref-type="supplementary-material" rid="SD1">Appendix</xref> (see <xref ref-type="sec" rid="S1">section 1</xref>). We found no statistically significant group differences in the classifier’s accuracy, neither in the observational fear learning nor in the direct-expression stage, which is consistent with the results of other analyses described in the manuscript.</p></sec></sec></sec><sec id="S42" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P50">Here, we investigated the relevance of the demonstrator’s familiarity in observational fear learning using physiological and neural measures. We focused on the US &gt; no US contrast in the observational learning and the CS+ &gt; CS- contrast in the direct-expression stage. We found no significant differences between participants observing friends (‘friend’ group) and subjects watching strangers (‘stranger’ group), neither in fear contagion nor in observational fear learning. Our study is the first to directly examine the difference between neural correlates of observational fear learning from friends and strangers. Using the ecologically valid paradigm that has been developed and tailored for this study, we replicated the previously reported brain activations underlying fear conditioning in humans. However, brain activity did not differ between participants observing friends and strangers. This clarifies the role of familiarity in fear contagion in humans and is in line with the previous rodent reports (<xref ref-type="bibr" rid="R31">Hernandez-Lallement et al., 2020</xref>). Overall, our results show that humans learn fear equally from friends and strangers, which might have been evolutionarily beneficial.</p><sec id="S43"><label>4.1</label><title>Main Physiological and fMRI Effects - Validation of the Protocol</title><p id="P51">We improved the original protocol of <xref ref-type="bibr" rid="R21">Haaker, Golkar et al. (2017)</xref> to study a real-time demonstrator-observer interaction. To validate the modified protocol, we firstly examined the data regardless of the level of familiarity. We analyzed both physiological and neural correlates of observational fear learning. The majority of observers correctly identified the CS+/US contingency and the contingency-aware to non-aware participants ratio, around three-quarters, was similar across groups. As our previous study showed that the psychophysiological effects of observational fear learning were present only in the contingency-aware participants (<xref ref-type="bibr" rid="R55">Szczepanik, Kaźmierowska, et al., 2020</xref>), we included only those in the analyses.</p><p id="P52">We observed enhanced observers’ skin conductance response (SCR) to the shock-associated stimuli during both observational learning and direct-expression stages, which is in line with earlier reports (<xref ref-type="bibr" rid="R50">Sevenster et al., 2014</xref>; <xref ref-type="bibr" rid="R55">Szczepanik, Kaźmierowska, et al., 2020</xref>). Neuroimaging analysis of data collected during the observational fear learning stage (US &gt; no US contrast) revealed brain activity in multiple regions, including those crucial for fear-related processing (amygdala, AI, aMCC), which is consistent with previous studies (<xref ref-type="bibr" rid="R39">Lindström et al., 2018</xref>). Additionally, we observed activation of brain areas involved in dynamic social perception (fusiform gyrus, pSTS, and supplementary motor area), which may indicate processing of visual body cues (<xref ref-type="bibr" rid="R3">Allison et al., 2000</xref>; <xref ref-type="bibr" rid="R60">Yang et al., 2015</xref>). PPI analysis revealed coupling of the AI with the rpSTS and of the rpSTS with the AI, right fusiform gyrus, and amygdala, suggesting that these brain regions are jointly involved in the social transfer of fear. Brain activity during the direct-expression stage (CS+ &gt; CS- contrast) involved the bilateral insular cortex and aMCC, which is in line with previous findings (<xref ref-type="bibr" rid="R39">Lindström et al., 2018</xref>; <xref ref-type="bibr" rid="R43">Olsson et al., 2007</xref>). We did not observe significant amygdala activations. In the direct fear conditioning, the amygdala is considered a region where the CS-US association is stored (<xref ref-type="bibr" rid="R43">Olsson &amp; Phelps, 2007</xref>; <xref ref-type="bibr" rid="R45">Phelps &amp; LeDoux, 2005</xref>). However, none of the previous studies has provided consistent evidence for the amygdala involvement during the direct-expression stage. Additionally, a meta-analysis on the neural signatures of the human fear conditioning has shown that the amygdala is not reliably activated when fMRI fear conditioning tasks are employed (<xref ref-type="bibr" rid="R19">Fullana et al., 2016</xref>). Taken together, using the ecologically valid paradigm, we replicated most of the previously reported brain activations underlying observational fear conditioning in humans.</p></sec><sec id="S44"><label>4.2</label><title>Familiarity Effects</title><p id="P53">We employed the experimental design in which familiarity with the demonstrator was the only between-group difference. We also made sure that the observers learned about the threat through social means only - they did not directly experience any electrical stimulation at any point of the experiment. The questionnaire results indicated that the observers from both groups did not differ in their general level of anxiety and empathy-related traits. Also, their initial state of anxiety (measured at the beginning of the experiment) was similar. Notably, the expressions of the demonstrators (the observational US) were rated as natural and did not differ between the groups. This lack of differences indicates that the experimental conditions were similar across groups, which is essential considering the naturalistic paradigm employed and potential confounds related to individual differences in the demonstrators’ fear expression. We found no significant differences in the brain activations of observers learning about the threat from their friends or strangers. The only previous report directly comparing the neural correlates of threat-to-self, threat-to-friend, and threat-to-stranger experience (<xref ref-type="bibr" rid="R6">Beckes et al., 2013</xref>) used a within-subject design. The within-subject protocol makes it difficult to disentangle the impact of direct vs. social sources of information. Our results show that familiarity between the participants does not modulate social fear learning when a demonstrator is a sole source of information.</p><p id="P54">In contrast to our results, the social learning of pain activated the pain network components (ACC and AI) depending on whether the participants imagined a loved one or a stranger (<xref ref-type="bibr" rid="R9">Cheng et al., 2010</xref>). This result seems to be at odds with our findings. However, the fear and pain paradigms differ. Although both paradigms use aversive stimulation (often involving electric shocks), the former emphasizes that it should be uncomfortable but not painful. In our study, we adjusted the electric shock’s intensity by asking every demonstrator to choose the level of electric stimulation and instructing him to react ‘in a natural, yet noticeable manner’. Based on the observers’ post-experimental ratings (on average describing the degree of unpleasantness attributed to the demonstrators as ‘rather unpleasant’), we may suppose they did not interpret the demonstrators’ condition in terms of pain.</p><p id="P55">Animal studies provide somewhat disparate results on the role of familiarity in threat contagion (<xref ref-type="bibr" rid="R22">Gonzalez-Liencres et al., 2014</xref>; <xref ref-type="bibr" rid="R34">Jeon et al., 2010</xref>; <xref ref-type="bibr" rid="R38">Knapska et al., 2010</xref>; <xref ref-type="bibr" rid="R49">Sanders et al., 2013</xref>). The studies carried out in a well-known environment, in which the presence of a partner attenuates threat response (the effect known as social buffering (<xref ref-type="bibr" rid="R37">Kiyokawa et al., 2014</xref>) more consistently report familiarity effect. Thus, familiarity may play a role in social attenuation rather than the social enhancement of fear. What is more, a recent meta-analysis of rodent studies indicated no familiarity effect on emotional contagion of threat (<xref ref-type="bibr" rid="R31">Hernandez-Lallement et al., 2020</xref>). The lack of familiarity effect in threat contagion may stem from the fact that social learning about danger, especially in a novel environment, is equally effective and might have been evolutionary beneficial regardless of the demonstrator’s level of familiarity.</p><p id="P56">Emotional contagion is considered a primary form of empathy, which results from the basic mechanism of the Perception-Action Model (<xref ref-type="bibr" rid="R14">De Waal, 2012</xref>; <xref ref-type="bibr" rid="R15">de Waal &amp; Preston, 2017</xref>). The PAM involves automatic matching between the target’s and the observer’s neural responses and constitutes a foundation for more complex phenomena such as sympathetic concern and perspective-taking. The model predicts that emotional contagion is more robust among individuals in close social relationships who share past experiences as their perception-action coupling is stronger (<xref ref-type="bibr" rid="R47">Preston &amp; de Waal, 2002</xref>). However, our data show that fear contagion does not depend on the level of familiarity, which may stem from its crucial informative role. Preston and de Waal’s model also postulates that emotional contagion is stronger when empathy is high. The reported level of experienced empathy toward the demonstrator did not differ between the groups, which could explain the lack of between-group differences in observational fear learning. Although surprising, a similar (and not exceptionally high) level of experienced empathy in both groups suggests the priority of threat processing over empathy involvement in our experimental situation. In the face of threat, observing the emotional responses of others can facilitate life-saving responses (<xref ref-type="bibr" rid="R42">Olsson et al., 2020</xref>). It is noteworthy that the actors serving as demonstrators in previous observational fear learning studies were all unfamiliar to the observers and still remained an effective source of learning (<xref ref-type="bibr" rid="R20">Golkar et al., 2015</xref>; <xref ref-type="bibr" rid="R21">Golkar &amp; Olsson, 2017</xref>). Thus, perceiving the emotions of others not only provides information about their source, i.e., the emotional state of the ‘demonstrator’ but also about the environment. Friends and strangers are equally good sources of information about threats.</p></sec><sec id="S45"><label>4.3</label><title>Limitations</title><p id="P57">Since the participants came together to the laboratory in the ‘friend’ group, the situation was more natural and socially engaging than in the ‘stranger’ group, where the observers had no interaction with the demonstrators before the experiment. However, if this was an essential factor, we could expect stronger activations of the social brain network in the ‘friend’ group, which was not the case. Moreover, we did not directly measure observers’ empathy toward the demonstrators. However, we assessed observers’ general level of empathy, situational empathy experienced during the observation and degree of unpleasantness attributed to the demonstrator and did not find differences between the groups. Finally, we studied only males; thus, it is unclear whether the results extrapolate to the female population. Considering the sex differences in emotional processing and empathy (<xref ref-type="bibr" rid="R48">Proverbio, 2021</xref>) and a pioneering character of the study, limiting the probe to one sex only enabled us to watch relatively strong and statistically powered effects. However, this issue certainly requires future investigations.</p></sec><sec id="S46" sec-type="conclusions"><label>4.4</label><title>Conclusions</title><p id="P58">We describe here the neural correlates of observational fear learning in humans using a naturalistic approach. Using this paradigm, we replicated most of the previously reported brain activation underlying fear conditioning in humans. Importantly, our findings constitute the first neuroimaging evidence for the lack of relationship between familiarity and fear contagion and observational fear learning in humans. We argue that fear contagion is an automatic, unconscious process independent of the level of the demonstrator’s familiarity. We claim that fear is learned equally from friends and strangers in humans, which is evolutionarily beneficial. These results resonate with ongoing debates on the questions such as the components of empathy, the boundaries between unconscious threat processing and conscious fear experience, and the role of the amygdala in the fear conditioning process. A question of the demonstrator’s characteristics modulating the observational fear learning process remains open, creating a space for further investigations.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS145919-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d10aAdEbB" position="anchor"/></supplementary-material><supplementary-material content-type="local-data" id="SD2"><label>Inline Supplementary Figure 5</label><media xlink:href="EMS145919-supplement-Inline_Supplementary_Figure_5.pdf" mimetype="application" mime-subtype="pdf" id="d10aAdEcB" position="anchor"/></supplementary-material></sec></body><back><ack id="S47"><title>Acknowledgements</title><p>We thank Jan Haaker for his valuable advice regarding the protocol’s modifications. We also acknowledge Urszula Baranowska and Małgorzata Dąbkowska for their support in collecting data, as well as Michał Kaźmierowski, Monika Riegel and Małgorzata Wierzba for their helpful insights regarding data analysis. Data collection and analysis were sponsored by National Science Centre grant 2015/19/B/HS6/02209. Ewelina Knapska was supported by European Research Council Starting Grant (H 415148).</p></ack><sec id="S48" sec-type="data-availability"><title>Data Availability</title><p id="P59">Relevant data are stored in an OSF repository and are available at <ext-link ext-link-type="uri" xlink:href="https://osf.io/g3wkq/">https://osf.io/g3wkq/</ext-link>. Unthresholded statistical maps from the reported comparisons are also available at Neurovault, <ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/RSLLSFTQ/">https://neurovault.org/collections/RSLLSFTQ/</ext-link>. Code replicating analyses reported here is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/nencki-lobi/emocon-mri">https://github.com/nencki-lobi/emocon-mri</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P60"><bold>Declaration of Competing Interests</bold></p><p id="P61">The authors declare no competing interests.</p></fn><fn id="FN2" fn-type="con"><p id="P62"><bold>Author Contributions</bold></p><p id="P63">Anna M. Kaźmierowska - Conceptualization, Methodology, Data curation, Formal Analysis, Investigation, Software, Visualization, Writing - original draft, Writing - review &amp; editing</p><p id="P64">Michał Szczepanik - Conceptualization, Methodology, Data curation, Formal Analysis, Investigation, Software, Visualization, Writing - original draft, Writing - review &amp; editing Marek Wypych - Conceptualization, Methodology, Supervision, Writing - review &amp; editing Dawid Droździel - Investigation, Resources</p><p id="P65">Artur Marchewka - Conceptualization, Methodology, Resources, Supervision, Writing - review &amp; editing</p><p id="P66">Jarosław M. Michałowski - Conceptualization, Methodology, Supervision, Writing - review &amp; editing</p><p id="P67">Andreas Olsson - Conceptualization, Supervision, Writing - review &amp; editing</p><p id="P68">Ewelina Knapska - Conceptualization, Methodology, Funding acquisition, Project administration, Supervision, Writing - review &amp; editing</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><year>2014</year><volume>8</volume><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alcalá-López</surname><given-names>D</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Van Overwalle</surname><given-names>F</given-names></name><name><surname>Vogeley</surname><given-names>K</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Turetsky</surname><given-names>BI</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Bzdok</surname><given-names>D</given-names></name></person-group><article-title>Computing the Social Brain Connectome Across Systems and States</article-title><source>Cerebral Cortex</source><year>2018</year><volume>28</volume><issue>7</issue><fpage>2207</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx121</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allison</surname><given-names>T</given-names></name><name><surname>Puc</surname><given-names>A</given-names></name><name><surname>McCarthy</surname><given-names>G</given-names></name></person-group><article-title>Social perception from visual cues: role of the STS region</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>7</issue><fpage>267</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(00)01501-1</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><article-title>Symmetric diffeomorphic image registration with cross-correlation: Evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><year>2008</year><volume>12</volume><issue>1</issue><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Dynamic causal modelling of anticipatory skin conductance responses</article-title><source>Biological Psychology</source><year>2010</year><volume>85</volume><issue>1</issue><fpage>163</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1016/j.biopsycho.2010.06.007</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckes</surname><given-names>L</given-names></name><name><surname>Coan</surname><given-names>JA</given-names></name><name><surname>Hasselmo</surname><given-names>K</given-names></name></person-group><article-title>Familiarity promotes the blurring of self and other in the neural representation of threat</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2013</year><volume>8</volume><issue>6</issue><fpage>670</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1093/scan/nss046</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boccadoro</surname><given-names>S</given-names></name><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Hudson</surname><given-names>AR</given-names></name><name><surname>Bardi</surname><given-names>L</given-names></name><name><surname>Nijhof</surname><given-names>AD</given-names></name><name><surname>Wiersema</surname><given-names>JR</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name><name><surname>Mueller</surname><given-names>SC</given-names></name></person-group><article-title>Defining the neural correlates of spontaneous theory of mind (ToM): An fMRI multi-study investigation</article-title><source>NeuroImage</source><year>2019</year><volume>203</volume><elocation-id>116193</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116193</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruder</surname><given-names>M</given-names></name><name><surname>Dosmukhambetova</surname><given-names>D</given-names></name><name><surname>Nerb</surname><given-names>J</given-names></name><name><surname>Manstead</surname><given-names>ASR</given-names></name></person-group><article-title>Emotional signals in nonverbal interaction: dyadic facilitation and convergence in expressions, appraisals, and feelings</article-title><source>Cognition &amp; Emotion</source><year>2012</year><volume>26</volume><issue>3</issue><fpage>480</fpage><lpage>502</lpage><pub-id pub-id-type="doi">10.1080/02699931.2011.645280</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Lin</surname><given-names>CP</given-names></name><name><surname>Chou</surname><given-names>KH</given-names></name><name><surname>Decety</surname><given-names>J</given-names></name></person-group><article-title>Love hurts: an fMRI study</article-title><source>NeuroImage</source><year>2010</year><volume>51</volume><issue>2</issue><fpage>923</fpage><lpage>929</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.02.047</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><year>1997</year><volume>10</volume><issue>4–5</issue><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1099-1492(199706/08)10:4/5&lt;171::AID-NBM453&gt;3.0.CO;2-L</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Measuring individual differences in empathy: Evidence for a multidimensional approach</article-title><source>Journal of Personality and Social Psychology</source><year>1983</year><volume>44</volume><issue>1</issue><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.44.1.113</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Segonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><etal/></person-group><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><year>2006</year><volume>31</volume><issue>3</issue><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vignemont</surname><given-names>F</given-names></name><name><surname>Singer</surname><given-names>T</given-names></name></person-group><article-title>The empathic brain: how when and why?</article-title><source>Trends in Cognitive Sciences</source><year>2006</year><volume>10</volume><issue>10</issue><fpage>435</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.08.008</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Waal</surname><given-names>FBM</given-names></name></person-group><article-title>The antiquity of empathy</article-title><source>Science</source><year>2012</year><volume>336</volume><issue>6083</issue><fpage>874</fpage><lpage>876</lpage><pub-id pub-id-type="doi">10.1126/science.1220999</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Waal</surname><given-names>FBM</given-names></name><name><surname>Preston</surname><given-names>SD</given-names></name></person-group><article-title>Mammalian empathy: behavioural manifestations and neural basis Nature Reviews</article-title><source>Neuroscience</source><year>2017</year><volume>18</volume><issue>8</issue><fpage>498</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1038/nrn.2017.72</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Berleant</surname><given-names>SL</given-names></name><name><surname>Moodie</surname><given-names>C</given-names></name><name><surname>Ma</surname><given-names>F</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><etal/></person-group><source>fMRIPrep: a robust preprocessing pipeline for functional MRI (Version 1.4.0) [Computer software]</source><year>2019</year><pub-id pub-id-type="doi">10.5281/zenodo.2851559</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><etal/></person-group><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><year>2019</year><volume>16</volume><issue>1</issue><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><year>2009</year><volume>47</volume><issue>Supplement 1</issue><fpage>S102</fpage><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fullana</surname><given-names>MA</given-names></name><name><surname>Harrison</surname><given-names>BJ</given-names></name><name><surname>Soriano-Mas</surname><given-names>C</given-names></name><name><surname>Vervliet</surname><given-names>B</given-names></name><name><surname>Cardoner</surname><given-names>N</given-names></name><name><surname>Avila-Parcet</surname><given-names>A</given-names></name><name><surname>Radua</surname><given-names>J</given-names></name></person-group><article-title>Neural signatures of human fear conditioning: an updated and extended meta-analysis of fMRI studies</article-title><source>Molecular Psychiatry</source><year>2016</year><volume>21</volume><issue>4</issue><fpage>500</fpage><lpage>508</lpage><pub-id pub-id-type="doi">10.1038/mp.2015.88</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golkar</surname><given-names>A</given-names></name><name><surname>Castro</surname><given-names>V</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><article-title>Social learning of fear and safety is determined by the demonstrator’s racial group</article-title><source>Biology Letters</source><year>2015</year><volume>11</volume><issue>1</issue><elocation-id>20140817</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2014.0817</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golkar</surname><given-names>A</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><article-title>The interplay of social group biases in social threat learning</article-title><source>Scientific Reports</source><year>2017</year><volume>7</volume><issue>1</issue><fpage>7685</fpage><pub-id pub-id-type="doi">10.1038/s41598-017-07522-z</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gonzalez-Liencres</surname><given-names>C</given-names></name><name><surname>Juckel</surname><given-names>G</given-names></name><name><surname>Tas</surname><given-names>C</given-names></name><name><surname>Friebe</surname><given-names>A</given-names></name><name><surname>Brüne</surname><given-names>M</given-names></name></person-group><article-title>Emotional contagion in mice: the role of familiarity</article-title><source>Behavioural Brain Research</source><year>2014</year><volume>263</volume><fpage>16</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.01.020</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><year>2011</year><volume>5</volume><fpage>13</fpage><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Ziegler</surname><given-names>E</given-names></name><name><surname>Ellis</surname><given-names>DG</given-names></name><name><surname>Jarecka</surname><given-names>D</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Johnson</surname><given-names>H</given-names></name><name><surname>Burns</surname><given-names>C</given-names></name><name><surname>Manhães-Savio</surname><given-names>A</given-names></name><name><surname>Hamalainen</surname><given-names>C</given-names></name><etal/></person-group><source>nipy/nipype: 1.2.0</source><year>2019</year><pub-id pub-id-type="doi">10.5281/zenodo.2685428</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>Neurolmage</source><year>2009</year><volume>48</volume><issue>1</issue><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaker</surname><given-names>J</given-names></name><name><surname>Golkar</surname><given-names>A</given-names></name><name><surname>Selbing</surname><given-names>I</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><article-title>Assessment of social transmission of threats in humans using observational fear conditioning</article-title><source>Nature Protocols</source><year>2017</year><volume>12</volume><issue>7</issue><fpage>1378</fpage><lpage>1386</lpage><pub-id pub-id-type="doi">10.1038/nprot.2017.027</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaker</surname><given-names>J</given-names></name><name><surname>Yi</surname><given-names>J</given-names></name><name><surname>Petrovic</surname><given-names>P</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><article-title>Endogenous opioids regulate social threat learning in humans</article-title><source>Nature Communication</source><year>2017</year><volume>8</volume><elocation-id>15495</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms15495</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatfield</surname><given-names>E</given-names></name><name><surname>Bensman</surname><given-names>L</given-names></name><name><surname>Thornton</surname><given-names>PD</given-names></name><name><surname>Rapson</surname><given-names>RL</given-names></name></person-group><article-title>New perspectives on emotional contagion: A review of classic and recent research on facial mimicry and contagion</article-title><source>Interpersona: An International Journal of Personal Relationships</source><year>2014</year><volume>8</volume><issue>2</issue><fpage>159</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.5964/ijpr.v8i2.162</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><article-title>A Primer on Pattern-Based Approaches to fMRI: Principles, Pitfalls, and Perspectives</article-title><source>Neuron</source><year>2015</year><volume>87</volume><issue>2</issue><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.025</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><year>2014</year><volume>8</volume><fpage>88</fpage><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernandez-Lallement</surname><given-names>J</given-names></name><name><surname>Gomez-Sotres</surname><given-names>P</given-names></name><name><surname>Carrillo</surname><given-names>M</given-names></name></person-group><article-title>Towards a unified theory of emotional contagion in rodents—A meta-analysis</article-title><source>Neuroscience and Biobehavioral Reviews</source><year>2020</year><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.09.010</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Improved Optimization for the Robust and Accurate Linear Registration and Motion Correction of Brain Images</article-title><source>NeuroImage</source><year>2002</year><volume>17</volume><issue>2</issue><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Medical Image Analysis</source><year>2001</year><volume>5</volume><issue>2</issue><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/S1361-8415(01)00036-6</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeon</surname><given-names>D</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Chetana</surname><given-names>M</given-names></name><name><surname>Jo</surname><given-names>D</given-names></name><name><surname>Ruley</surname><given-names>HE</given-names></name><name><surname>Lin</surname><given-names>S-Y</given-names></name><name><surname>Rabah</surname><given-names>D</given-names></name><name><surname>Kinet</surname><given-names>J-P</given-names></name><name><surname>Shin</surname><given-names>H-S</given-names></name></person-group><article-title>Observational fear learning involves affective pain system and Cav1.2 Ca2+ channels in ACC</article-title><source>Nature Neuroscience</source><year>2010</year><volume>13</volume><issue>4</issue><fpage>482</fpage><lpage>488</lpage><pub-id pub-id-type="doi">10.1038/nn.2504</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaźmierczak</surname><given-names>M</given-names></name><name><surname>Plopa</surname><given-names>M</given-names></name><name><surname>Retowski</surname><given-names>S</given-names></name></person-group><article-title>Skala wrażliwości empatycznej</article-title><source>Przegląd Psychologiczny</source><year>2007</year><volume>50</volume><issue>1</issue><fpage>9</fpage><lpage>24</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.kul.pl/files/714/nowy_folder/1.50.2007_art.1.pdf">https://www.kul.pl/files/714/nowy_folder/1.50.2007_art.1.pdf</ext-link></comment></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keysers</surname><given-names>C</given-names></name><name><surname>Gazzola</surname><given-names>V</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name></person-group><article-title>Using Bayes factor hypothesis testing in neuroscience to establish evidence of absence</article-title><source>Nature Neuroscience</source><year>2020</year><volume>23</volume><issue>7</issue><fpage>788</fpage><lpage>799</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0660-4</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiyokawa</surname><given-names>Y</given-names></name><name><surname>Honda</surname><given-names>A</given-names></name><name><surname>Takeuchi</surname><given-names>Y</given-names></name><name><surname>Mori</surname><given-names>Y</given-names></name></person-group><article-title>A familiar conspecific is more effective than an unfamiliar conspecific for social buffering of conditioned fear responses in male rats</article-title><source>Behavioural Brain Research</source><year>2014</year><volume>267</volume><fpage>189</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.bbr.2014.03.043</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knapska</surname><given-names>E</given-names></name><name><surname>Mikosz</surname><given-names>M</given-names></name><name><surname>Werka</surname><given-names>T</given-names></name><name><surname>Maren</surname><given-names>S</given-names></name></person-group><article-title>Social modulation of learning in rats</article-title><source>Learning &amp; Memory</source><year>2010</year><volume>17</volume><issue>1</issue><fpage>35</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1101/lm.1670910</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindström</surname><given-names>B</given-names></name><name><surname>Haaker</surname><given-names>J</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name></person-group><article-title>A common neural network differentially mediates direct and social fear learning</article-title><source>NeuroImage</source><year>2018</year><volume>167</volume><fpage>121</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.11.039</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendelson</surname><given-names>MJ</given-names></name><name><surname>Aboud</surname><given-names>FE</given-names></name></person-group><article-title>Measuring friendship quality in late adolescents and young adults: McGill Friendship Questionnaires</article-title><source>Canadian Journal of Behavioural Science/Revue Canadienne Des Sciences Du Comportement</source><year>1999</year><volume>31</volume><issue>2</issue><fpage>130</fpage><comment><ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/record/1999-13541-007">https://psycnet.apa.org/record/1999-13541-007</ext-link></comment></element-citation></ref><ref id="R41"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Morey</surname><given-names>RD</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><source>BayesFactor: Computation of Bayes Factors for Common Designs (Version 0.9.12-4.2)</source><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/packages/BayesFactor/index.html">https://CRAN.R-project.org/package=BayesFactor</ext-link></comment></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsson</surname><given-names>A</given-names></name><name><surname>Knapska</surname><given-names>E</given-names></name><name><surname>Lindström</surname><given-names>B</given-names></name></person-group><article-title>The neural and computational systems of social learning</article-title><source>Nature Reviews. Neuroscience</source><year>2020</year><volume>21</volume><issue>4</issue><fpage>197</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0276-4</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsson</surname><given-names>A</given-names></name><name><surname>Nearing</surname><given-names>KI</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><article-title>Learning fears by observing others: the neural systems of social fear transmission</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2007</year><volume>2</volume><issue>1</issue><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1093/scan/nsm005</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsson</surname><given-names>A</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><article-title>Social learning of fear</article-title><source>Nature Neuroscience</source><year>2007</year><volume>10</volume><issue>9</issue><fpage>1095</fpage><lpage>1102</lpage><pub-id pub-id-type="doi">10.1038/nn1968</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phelps</surname><given-names>EA</given-names></name><name><surname>LeDoux</surname><given-names>JE</given-names></name></person-group><article-title>Contributions of the amygdala to emotion processing: from animal models to human behavior</article-title><source>Neuron</source><year>2005</year><volume>48</volume><issue>2</issue><fpage>175</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.09.025</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><year>2014</year><volume>84</volume><issue>Supplement C</issue><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preston</surname><given-names>SD</given-names></name><name><surname>de Waal</surname><given-names>FBM</given-names></name></person-group><article-title>Empathy: Its ultimate and proximate bases</article-title><source>The Behavioral and Brain Sciences</source><year>2002</year><volume>25</volume><issue>1</issue><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1017/s0140525x02000018</pub-id><comment>discussion 20-71</comment></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proverbio</surname><given-names>AM</given-names></name></person-group><article-title>Sex differences in the social brain and in social cognition</article-title><source>Journal of Neuroscience Research</source><year>2021</year><pub-id pub-id-type="doi">10.1002/jnr.24787</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>J</given-names></name><name><surname>Mayford</surname><given-names>M</given-names></name><name><surname>Jeste</surname><given-names>D</given-names></name></person-group><article-title>Empathic fear responses in mice are triggered by recognition of a shared experience</article-title><source>PloS One</source><year>2013</year><volume>8</volume><issue>9</issue><elocation-id>e74609</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0074609</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sevenster</surname><given-names>D</given-names></name><name><surname>Beckers</surname><given-names>T</given-names></name><name><surname>Kindt</surname><given-names>M</given-names></name></person-group><article-title>Fear conditioning of SCR but not the startle reflex requires conscious discrimination of threat and safety</article-title><source>Frontiers in Behavioral Neuroscience</source><year>2014</year><volume>8</volume><fpage>32</fpage><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00032</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sliwinska</surname><given-names>MW</given-names></name><name><surname>Pitcher</surname><given-names>D</given-names></name></person-group><article-title>TMS demonstrates that both right and left superior temporal sulci are important for facial expression recognition</article-title><source>NeuroImage</source><year>2018</year><volume>183</volume><fpage>394</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.08.025</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname><given-names>CD</given-names></name><name><surname>Gorsuch</surname><given-names>RL</given-names></name><name><surname>Lushene</surname><given-names>R</given-names></name><name><surname>Vagg</surname><given-names>PR</given-names></name><name><surname>Jacobs</surname><given-names>GA</given-names></name></person-group><source>Manual for the State-Trait Anxiety Inventory</source><publisher-name>Consulting Psychologists Press</publisher-name><year>1983</year></element-citation></ref><ref id="R53"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Spielberger</surname><given-names>CD</given-names></name><name><surname>Strelau</surname><given-names>J</given-names></name><name><surname>Tysarczyk</surname><given-names>M</given-names></name><name><surname>Wrześniewski</surname><given-names>K</given-names></name></person-group><source>STAI - Inwentarz Stanu i Cechy Lęku</source><publisher-name>Pracownia Testów Psychologicznych Polskiego Towarzystwa Psychologicznego</publisher-name><year>2012</year></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staib</surname><given-names>M</given-names></name><name><surname>Castegnetti</surname><given-names>G</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name></person-group><article-title>Optimising a model-based approach to inferring fear learning from skin conductance responses</article-title><source>Journal of Neuroscience Methods</source><year>2015</year><volume>255</volume><fpage>131</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.08.009</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szczepanik</surname><given-names>M</given-names></name><name><surname>Kaźmierowska</surname><given-names>AM</given-names></name><name><surname>Michałowski</surname><given-names>JM</given-names></name><name><surname>Wypych</surname><given-names>M</given-names></name><name><surname>Olsson</surname><given-names>A</given-names></name><name><surname>Knapska</surname><given-names>E</given-names></name></person-group><article-title>Observational learning of fear in real time procedure</article-title><source>Scientific Reports</source><year>2020</year><volume>10</volume><issue>1</issue><elocation-id>16960</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-74113-w</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tange</surname><given-names>O</given-names></name></person-group><article-title>GNU Parallel - The Command-Line Power Tool</article-title><source>;login: The USENIX Magazine</source><year>2011</year><volume>36</volume><issue>1</issue><fpage>42</fpage><lpage>47</lpage><comment><ext-link ext-link-type="uri" xlink:href="http://www.gnu.org/software/parallel/">http://www.gnu.org/s/parallel</ext-link></comment></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><article-title>N4ITK: Improved N3 Bias Correction</article-title><source>IEEE Transactions on Medical Imaging</source><year>2010</year><volume>29</volume><issue>6</issue><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>G</given-names></name><name><surname>Castellanos</surname><given-names>AL</given-names></name><name><surname>Hausfeld</surname><given-names>L</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><article-title>Cross-validation and permutations in MVPA: Validity of permutation strategies and power of cross-validation schemes</article-title><source>NeuroImage</source><year>2021</year><volume>238</volume><elocation-id>118145</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118145</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weidemann</surname><given-names>G</given-names></name><name><surname>Satkunarajah</surname><given-names>M</given-names></name><name><surname>Lovibond</surname><given-names>PF</given-names></name></person-group><article-title>I Think, Therefore Eyeblink: The Importance of Contingency Awareness in Conditioning</article-title><source>Psychological Science</source><year>2016</year><volume>27</volume><issue>4</issue><fpage>467</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1177/0956797615625973</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>DY-J</given-names></name><name><surname>Rosenblau</surname><given-names>G</given-names></name><name><surname>Keifer</surname><given-names>C</given-names></name><name><surname>Pelphrey</surname><given-names>KA</given-names></name></person-group><article-title>An integrative neural model of social perception, action observation, and theory of mind</article-title><source>Neuroscience and Biobehavioral Reviews</source><year>2015</year><volume>51</volume><fpage>263</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.01.020</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yovel</surname><given-names>G</given-names></name><name><surname>Tambini</surname><given-names>A</given-names></name><name><surname>Brandman</surname><given-names>T</given-names></name></person-group><article-title>The asymmetry of the fusiform face area is a stable individual characteristic that underlies the left-visual-field superiority for faces</article-title><source>Neuropsychologia</source><year>2008</year><volume>46</volume><issue>13</issue><fpage>3061</fpage><lpage>3068</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2008.06.017</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><year>2001</year><volume>20</volume><issue>1</issue><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Highlights</title></caption><list id="L1" list-type="bullet"><list-item><p>We compared observational learning of fear from friends and strangers</p></list-item><list-item><p>Familiarity does not enhance social learning of fear in humans</p></list-item><list-item><p>Bayesian statistics confirm absence of differences between friends and strangers</p></list-item><list-item><p>Observational fear learning activates social and fear networks including amygdala</p></list-item><list-item><p>Amygdala activations are absent when learned fear is recalled</p></list-item></list></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental Design</title><p><italic>Note.</italic> (A) Depending on the group, observers arrived at the laboratory with their friend (upper panel, the ‘friend’ group, orange) or alone (lower panel, the ‘stranger’ group, blue). (B) During the observational learning stage, observers were lying in the fMRI scanner and watching the demonstrator (a friend or a stranger) performing the differential fear conditioning task. In the task, one stimulus (conditioned stimulus +; CS+) was sometimes paired with an uncomfortable electric shock (unconditioned stimulus, US), while the other one (conditioned stimulus -; CS-) was always safe. (C) During the direct-expression stage, observers remained in the scanner and were told that they would perform the same task as previously watched. They watched an identical set of visual stimuli, but no electrical stimulation was applied.</p></caption><graphic xlink:href="EMS145919-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Tasks and Stimuli Overview</title><p><italic>Note.</italic> Schematic representation of the task design. (A) The observational learning stage: participants watched another person (demonstrator) who performed a differential conditioning task. The US (electrical stimulation) reinforced half of the visual CS+, CS- were never reinforced. (B) The direct-expression stage: participants, informed that they would undergo an identical task as the demonstrators, watched the same visual stimuli presentation. However, the electric stimulation was not applied.</p></caption><graphic xlink:href="EMS145919-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Evaluation of the Demonstrator’s Expression (the observational US)</title><p><italic>Note.</italic> Error bars extend to data points placed no further than 1.5*IQR (interquartile range) beyond the 1<sup>st</sup> quartile and above the 3<sup>rd</sup> quartile. The identification rating was done in the ‘friend’ group only.</p></caption><graphic xlink:href="EMS145919-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Skin Conductance Responses</title><p><italic>Note.</italic> (A) observational learning stage, (B) direct-expression stage. Error bars extend to data points placed no further than 1.5*IQR beyond the 1<sup>st</sup> quartile and above the 3<sup>rd</sup> quartile.</p></caption><graphic xlink:href="EMS145919-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Activation in the Observational Learning Stage, US &gt; no US Contrast</title><p><italic>Note.</italic> We found no between-group differences. (A) Statistical maps show whole-brain activations for both groups analyzed together, p &lt; 0.05, FWE peak-level correction. (B) Region-of-interest statistics, showing group averages and 95% confidence intervals. Abbreviations: AI - anterior insula, aMCC - anterior mid-cingulate cortex, Amy - amygdala, rFFA - right fusiform face area, rpSTS - right posterior superior temporal sulcus, rTPJ - right temporo-parietal junction.</p></caption><graphic xlink:href="EMS145919-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Activation in the Direct-expression Stage, CS+ &gt; CS- Contrast</title><p><italic>Note.</italic> (A) Statistical map showing whole-brain activations for the ‘friend’ and ‘stranger’ groups analyzed together, FWE corrected, p &lt; 0.05. (B) Region-of-interest statistics, showing group averages and 95% confidence intervals. Abbreviations: AI - anterior insula, aMCC - anterior mid-cingulate cortex, Amy - amygdala, rFFA - right fusiform face area, rpSTS - right posterior superior temporal sulcus, rTPJ - right temporo-parietal junction.</p></caption><graphic xlink:href="EMS145919-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Temporal Modulation of the CS+ Response in the Direct-expression Stage</title><p><italic>Note.</italic> (A) Statistical map showing a significant effect for the ‘friend’ and ‘stranger’ groups analyzed together. (B) Statistical map showing the (stranger CS+ × time decrease) &gt; (friend CS+ × time decrease) contrast.</p></caption><graphic xlink:href="EMS145919-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><title>Psychophysiological Interaction Maps, US &gt; no US Contrast</title><p><italic>Note.</italic> AI and pSTS seeds were analyzed for both groups together during the observational learning stage. (A) the AI exhibits interaction with several regions, including the rpSTS (FWEc), (B) the rpSTS exhibits interaction with several areas, including the right fusiform gyrus and bilateral AI (FWEc), (C) psychophysiological interaction of the rpSTS with the amygdala (SVC).</p></caption><graphic xlink:href="EMS145919-f008"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Region-of-interest Analysis of Between-group Differences</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top" rowspan="2">ROI</th><th align="center" valign="top" colspan="3">Observational Learning Stage (US &gt; noUS)</th><th align="center" valign="top" colspan="3">Observational Learning Stage (US)</th><th align="center" valign="top" colspan="3">Direct-expression Stage (CS+ &gt; CS-)</th></tr><tr style="border-top: solid thin"><th align="center" valign="top">t</th><th align="center" valign="top">p</th><th align="center" valign="top">BF<sub>01</sub></th><th align="center" valign="top">t</th><th align="center" valign="top">p</th><th align="center" valign="top">BF<sub>01</sub></th><th align="center" valign="top">t</th><th align="center" valign="top">p</th><th align="center" valign="top">BF<sub>01</sub></th></tr></thead><tbody><tr><td align="left" valign="top">AI</td><td align="center" valign="top">-0.64</td><td align="center" valign="top">.52</td><td align="center" valign="top">3.39</td><td align="center" valign="top">1.06</td><td align="center" valign="top">.29</td><td align="center" valign="top">2.50</td><td align="center" valign="top">0.03</td><td align="center" valign="top">.97</td><td align="center" valign="top">4.04</td></tr><tr><td align="left" valign="top">aMCC</td><td align="center" valign="top">-0.77</td><td align="center" valign="top">.45</td><td align="center" valign="top">3.13</td><td align="center" valign="top">1.60</td><td align="center" valign="top">.11</td><td align="center" valign="top">1.36</td><td align="center" valign="top">-0.08</td><td align="center" valign="top">.93</td><td align="center" valign="top">4.03</td></tr><tr><td align="left" valign="top">Amy</td><td align="center" valign="top">0.01</td><td align="center" valign="top">.99</td><td align="center" valign="top">4.04</td><td align="center" valign="top">1.38</td><td align="center" valign="top">.17</td><td align="center" valign="top">1.79</td><td align="center" valign="top">0.40</td><td align="center" valign="top">.68</td><td align="center" valign="top">3.76</td></tr><tr><td align="left" valign="top">rFFA</td><td align="center" valign="top">-1.06</td><td align="center" valign="top">.29</td><td align="center" valign="top">2.50</td><td align="center" valign="top">-0.49</td><td align="center" valign="top">.62</td><td align="center" valign="top">3.65</td><td align="center" valign="top">-1.08</td><td align="center" valign="top">0.29</td><td align="center" valign="top">2.47</td></tr><tr><td align="left" valign="top">rpSTS</td><td align="center" valign="top">0.24</td><td align="center" valign="top">.81</td><td align="center" valign="top">3.94</td><td align="center" valign="top">1.08</td><td align="center" valign="top">.29</td><td align="center" valign="top">2.47</td><td align="center" valign="top">1.14</td><td align="center" valign="top">.26</td><td align="center" valign="top">2.32</td></tr><tr><td align="left" valign="top">rTPJ</td><td align="center" valign="top">-1.25</td><td align="center" valign="top">.21</td><td align="center" valign="top">2.07</td><td align="center" valign="top">-0.40</td><td align="center" valign="top">.69</td><td align="center" valign="top">3.78</td><td align="center" valign="top">-0.13</td><td align="center" valign="top">.90</td><td align="center" valign="top">4.01</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P69"><italic>Note.</italic> We performed two comparisons for the observational learning stage based on within-subject parameter estimates for either the (US &gt; no US) or the US alone. We compared the (CS+ &gt; CS-) contrast estimate between the groups for the direct-expression stage. The statistical results: positive t values represent friend &gt; stranger; t-tests with Welch correction to the degrees of freedom; p values not corrected for the number of comparisons; Bayes factors reported in favor of the null hypothesis. AI - anterior insula, aMCC - anterior mid-cingulate cortex, Amy - amygdala, rFFA - right fusiform face area, rpSTS - right posterior superior temporal sulcus, rTPJ - right temporo-parietal junction.</p></fn></table-wrap-foot></table-wrap></floats-group></article>