<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS202191</article-id><article-id pub-id-type="doi">10.1101/2021.12.01.470818</article-id><article-id pub-id-type="archive">PPR427854</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Foundation Model Enables Interpretable Open and Error-Tolerant Searching for Mass Spectrometry-Based Proteomics</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Altenburg</surname><given-names>Tom</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Muth</surname><given-names>Thilo</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>van Zalm</surname><given-names>Patrick</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Steen</surname><given-names>Hanno</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Renard</surname><given-names>Bernhard Y.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/058rn5r42</institution-id><institution>Hasso Plattner Institute</institution></institution-wrap>, Digital Engineering Faculty, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03bnmw459</institution-id><institution>University of Potsdam</institution></institution-wrap>, <city>Potsdam</city>, <country country="DE">Germany</country></aff><aff id="A2"><label>2</label>Department of Pathology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00dvg7y05</institution-id><institution>Boston Children’s Hospital</institution></institution-wrap> and Harvard Medical School, <city>Boston</city>, <state>MA</state>, <country country="US">USA</country></aff><aff id="A3"><label>3</label>Data Competence Center (MF 2), <institution-wrap><institution-id institution-id-type="ror">https://ror.org/01k5qnb77</institution-id><institution>Robert Koch Institute</institution></institution-wrap>, <city>Berlin</city>, <country country="DE">Germany</country></aff><aff id="A4"><label>4</label>Windreich Deptartment of Artificial Intelligence &amp; Human Health, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap>, <city>New York</city>, <country country="US">USA</country></aff><aff id="A5"><label>5</label>Hasso Plattner Institute for Digital Health at Mount Sinai, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/04a9tmd77</institution-id><institution>Icahn School of Medicine at Mount Sinai</institution></institution-wrap>, <city>New York</city>, <country country="US">USA</country></aff><author-notes><corresp id="CR1"><label>*</label><bold>Corresponding Author:</bold> <email>bernhard.renard@hpi.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>15</day><month>01</month><year>2025</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>01</month><year>2025</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Mass spectrometry-based proteomics allows to study all proteins of a sample on a molecular level. However, mass spectra are noisy and contain complex patterns, making them inherently challenging to analyze with purely algorithmic approaches. In terms of the protein sequence landscape, most recent bottom-up MS-based proteomics studies consider either a diverse pool of post-translational modifications, employ large databases – as in metaproteomics or proteogenomics, study multiple isoforms of proteins, include unspecific cleavage sites or even combinations thereof. All this makes peptide and protein identifications challenging due to sheer size of the search space. To cope with this two-sided challenge, i.e. the complexity of real spectra and the search space size, we present a foundation model, called yHydra, that jointly embeds spectra and peptides. This allows us to implement various downstream tasks and search modes in Euclidean space. In particular, we implement an open search which allows to query multiple ten-thousands of spectra against millions of peptides. Furthermore, we implement an error-tolerant search for identifying additional proteoforms that are not included in off-the-shelf reference proteomes. Our foundation model provides meaningful embeddings, as we interpret learned peptide embeddings in comparison to the peptide’s physico-chemical properties. yHydra’s open search, assigns delta masses to each identification which allows to unrestrictedly characterize post-translational modifications. The error-tolerant mode of yHydra can be used as post-processing to existing search engines or as a standalone. yHydra is evaluated on several real life data sets for the identification of modified protein sequences and shows up to 25% increase in protein identification at constant false discovery rate compared to the current state-of-the-art.</p></abstract><kwd-group><kwd>Deep Learning</kwd><kwd>Proteomics</kwd><kwd>Open Search</kwd><kwd>Error-tolerant Search</kwd><kwd>Post-translational Modifications</kwd><kwd>Joint Embedding</kwd><kwd>Foundation Model</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Peptide sequences and their corresponding fragmentation mass spectra have been the two pivotal peptide representations in mass spectrometry (MS)-based proteomics up to date [<xref ref-type="bibr" rid="R1">1</xref>]. A single MS experiment can produce up to one terabyte of data, mostly peptide information represented as tandem mass spectra when using recent instrumentation. To identify actual peptide sequences from spectra a conventional proteomics search engine is used in most cases [<xref ref-type="bibr" rid="R2">2</xref>]. Such a search engine constructs theoretical spectra derived from a context-matched reference proteome to identify the actual peptide sequence by performing comparisons in spectrum space. Each comparison is effortful as it involves the construction of spectra, peak matching, intensity summation, tie-breaking, score calculation and post-processing. The construction of spectra itself is challenging as real spectra have missing peaks, noise peaks (i.e. other than peptide-originating peaks), depict complex patterns due to the overlay of isotopic distributions of fragments, and peak heights are non-trivially characteristic to the underlying peptide. Here, we present a foundation model, called yHydra, that jointly embeds peptides and tandem mass spectra as vectors into the same Euclidean space. This way comparisons can be performed efficiently using Euclidean (L2-)distances between spectrum embeddings and peptide embeddings. yHydra is a foundation model and enables us to tackle various downstream tasks and implement multiple search modes. Specifically, we implement three search modes including closed, open and error-tolerant search modes.</p><p id="P3">Recently, various machine learning-based approaches have been introduced to improve the downstream data analysis in MS-based proteomics [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>]. Most of these approaches improve specific aspects of the aforementioned conventional database search. For example, fragment intensity prediction substantially improves search outcomes by using recent deep learning techniques [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>]. Post-processing, so-called rescoring, has been very successful by using semi-supervised learning [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>]. Auxiliary peptide properties such as ion mobility or retention time, acquired during the MS-experiment, can be predicted from deep learning models to further improve rescoring [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. Still, all these methods operate within the boundaries of a conventional database search and are conceptually limited by operating in spectrum space.</p><p id="P4">A major challenge in MS-based proteomics are proteoforms. These protein variants arise from mutations, splicing, processing, and/or post-translational modifications (PTMs). All these deviations from the reported primary sequence are relevant for physiological processes but also involved in human diseases [<xref ref-type="bibr" rid="R11">11</xref>]. Taking proteoforms into account means searching against more complex and larger search spaces. This can be achieved via an open search where a considerably wide mass window allows for a more flexible and larger search context per spectrum. A collection of algorithmic approaches for open searches exist, including index- [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>], tag- [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>] or spectral librarybased methods [<xref ref-type="bibr" rid="R16">16</xref>]. All these approaches are purely algorithmic. In contrast, yHydra as a foundation model was trained on real spectra, providing a more nuanced spectrum representation, which results in improved peptide identification. The above mentioned algorithms fall short as they only reflect limited feature sets from manual engineering. Our approach, allows to study proteoforms from three angles. First, by implementing an open search capturing the heuristic nature of real spectra through the embeddings. Second, by constructing a delta mass profile, which summarizes differences between proteoform masses and canonical peptide masses, indicative of PTMs or mutations. Third, by investigating particular regions of the embedding space for potential enrichment of PTMs.</p><p id="P5">There exist deep learning approaches for specific tasks in MS-based proteomics, including the detection of modified peptides [<xref ref-type="bibr" rid="R17">17</xref>], spectrum-to-peptide translation (so-called de novo sequencing) [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>] and a pairwise spectrum embedding for clustering [<xref ref-type="bibr" rid="R23">23</xref>]. There exists a cross-modal embedding for learning a similarity metric between tandem mass spectra and peptides [<xref ref-type="bibr" rid="R24">24</xref>]. Such learned similarity metric allows heuristic scoring of peptide spectrum matches (PSMs) but is limited to closed searches only.</p><p id="P6">Altogether, all the above methods are dedicated but limited to single tasks. None of these methods aims to provide a universal basis of how to compare spectra and peptides in a unified and interpretable manner. To mitigate these shortcomings, we provide a foundation model via learned joint embeddings of spectra and peptides. Having a unified view on both representations allows to implement various search modes and types of downstream analyses in embeddings space, of which we implemented three search modes and additionally fine-tuned our model to cope with the two most commonly used instruments in MS-based proteomics and also cover tryptic and non-tryptic peptides.</p><p id="P7">Here, we present our foundation model yHydra, consisting of a Spectrum Transformer and a Peptide Transformer. The Transformer model architecture has been successful in various domains due to its generalization capabilities [<xref ref-type="bibr" rid="R25">25</xref>]. Our approach is inspired by one of the first true foundation models, called CLIP, originally embedding images and text [<xref ref-type="bibr" rid="R26">26</xref>]. For training, we used 67 different proteomics repositories containing nearly 20 million identified peptide spectrum matches (PSMs). We show that the high resolution of modern mass spectrometers is well covered by our peak encodings, allowing to capture all information of real spectra. Next, we verify our approach by interpreting our embeddings using Uniform Manifold Approximation and Projection (UMAP). This demonstrates that learned embeddings are meaningful, as the learned manifold reflects physico-chemical properties of the underlying peptides. Once trained, we use the foundation model to implement specific sub-tasks. We start by implementing a closed and open search using a GPU-accelerated k-NN search library as backend [<xref ref-type="bibr" rid="R27">27</xref>]. Using our open search we were able to build a delta mass profile for characterizing PTMs in the sample. As an additional sub-task, we implement an error-tolerant search mode by implementing a dedicated beam search in our embedding space. Visualizing the reward matrix based on the gradient towards the decoded peptide sequence during beam search allowed further model interpretability. Having an error-tolerant mode enabled us to identify new proteoforms due to genetic variations. In particular, we searched samples of a monoclonal antibody and chimpanzee plasma.</p><p id="P8">Protein sequencing of monoclonal antibodies is challenging because of their variable regions [<xref ref-type="bibr" rid="R28">28</xref>]. These regions result from immune adaptations and are unique per individuum. Here, we use our error-tolerant search to identify peptides of a tryptic digest of a monoclonal antibody and evaluate our approach by searching against publicly available antibody sequences while comparing peptide identifications to the actual fully assembled antibody sequence as a ground truth.</p><p id="P9">Proteomes of many non-human primates, such as chimpanzee (Pan troglodytes), are not yet fully annotated. A better understanding of their proteomes can aid drug development and research. However, their genomes have been sequenced and corresponding proteins have been suggested [<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>]. Here, we perform a cross-species error-tolerant search of tryptic peptides from a chimpanzee plasma sample. Therefore, we evaluate our approach by searching against the canonical human proteome and comparing to the genome-derived isoforms of the predicted chimpanzee proteome. Thereby, we capture potential proteoforms due to genetic variations between the proteomes of both species.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>yHydra learns to jointly embed peptides and MS/MS spectra</title><p id="P10">To gain a foundation model we trained a deep learning model that jointly embeds peptides and fragmentation spectra. In particular, yHydra embeds both representations into a joint embedding space of real-valued vectors in an Euclidean space (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). Having such a metric between fixed-sized vectors allows computationally inexpensive comparisons between embeddings. To gain embeddings, we jointly trained dedicated Transformer models (one for each of the two domains) by providing a large collection of previously identified peptide spectrum matches (see <xref ref-type="sec" rid="S10">Methods</xref> section). During training, yHydra learns an embedding such that the Euclidean distance between both embeddings of a PSM is small compared to the distances of any mismatched paired embeddings (i.e. by swapping peptides and spectra of PSMs within each mini-batch). Or in other words, to find embeddings such that the diagonal of the pairwise Euclidean distance matrix (<xref ref-type="fig" rid="F1">Fig. 1A</xref>, bottom right) is minimized while the off-diagonal of this matrix is maximized. Note, after both embedders (respectively, Spectrum Transformer and Peptide Transformer) have been trained they can be used separately and independently while both embed into the same joint space. In particular, we implement an open search by embedding MS/MS spectra to search them against embeddings of digested peptides from a proteome database (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). Because both embeddings are real-valued vectors of the same fixed size that live in the same joint embedding space we can make use of highly optimized algorithms, such as the GPU-accelerated k-nearest neighbor (k-NN) search [<xref ref-type="bibr" rid="R27">27</xref>], as we demonstrate below.</p><p id="P11">As mentioned above, comparing spectra with one another typically requires peak matching. Instead, we use wavelet encoding (originally developed alongside the Transformer architecture) to encode the m/z location of each peak. This serves two purposes. First, it allows to present a spectrum to our Spectrum Transformer and second, it retains the ability to separate peaks in close proximity (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). To demonstrate this we offset two hypothetical peaks shown by the delta m/z on the x-axis (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). The change of each dimension of the 64 dimensional wavelet encoding is visualized (<xref ref-type="fig" rid="F1">Fig. 1C</xref>, right panel) in comparison to applying L1- or L2- between the two peaks (<xref ref-type="fig" rid="F1">Fig. 1C</xref>, left panel). This shows the information content and responsiveness of the peak encoding, giving the Spectrum Transformer its ability to input each peak and resolve its location.</p><p id="P12">Finally, because the learned joint embedding yields realvalued vectors of fixed size the entire toolbox of machine learning and statistical tools is open to be used in conjunction with our embeddings. For example, we used UMAP to visualize the manifold of the embeddings from identified PSMs (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). The two-dimensional manifold of both peptide and spectrum embeddings of identified PSMs is colored according to charge, precursor mass, and modifications. The embeddings are ordered from smaller to larger charges. They are also sorted by precursor masses. This indicates that representations retain information about those properties. In contrast, the PTMs are largely uniformly scattered, which makes the embedding suitable for an open search.</p></sec><sec id="S4"><title>yHydra enables ultra-fast open searching</title><p id="P13">MS-based proteomics is able to characterize proteoforms due to PTMs. However, accounting for PTMs increases the search space and thus an open search is needed to cope with the increased search complexity. Here, we implement an open search as a sub-task for our foundation model. Our open search allows for a delta mass that is a mass difference between the unmodified peptide from the reference proteome and any modified version of that same peptide. This delta mass can give rise to the underlying PTMs, specifically we observe oxidations (+16 Da) and carbamidomethylations (+57 Da) and combinations thereof (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). For the open search, yHydra starts by generating a set of tryptic peptides for a selected protein database (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). For each peptide, the trained Peptide Transformer infers a peptide embedding. Similarly, for each MS/MS spectrum in a run, the spectrum embedder infers a spectrum embedding. Subsequently, these spectrum embeddings are queried against the entire set of peptide embeddings using a k-nearest neighbor (k-NN) search. Note that all spectrum embeddings of an entire run (typically multiple ten-thousands) are queried simultaneously against the entire database in a single call to achieve lowest possible search times. To be able to search spectra against dedicated mass buckets (e.g. to select between a close, narrow or open search) while performing a single query per run we developed a multiplexed k-NN search (<xref ref-type="sec" rid="S10">Methods</xref>). As a result, the yHydra search only takes seconds when using the GPU-acceleration, and thus being faster than MSFragger (<xref ref-type="table" rid="T1">Table 1</xref>). In this experiment we chose a k of 50 (k is a user-defined parameter, see Discussion) and only the 50 closest peptides are then scored by constructing a theoretical spectrum, matching peaks, and subsequent false discovery rate estimation using a target-decoy approach (<xref ref-type="fig" rid="F2">Fig. 2E</xref>).</p><p id="P14">Here, we search a sample from cyanobacteria (PXD007963) and compare open searches by yHydra and MSFragger (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Both methods identify peptides resulting in similar peptide length and charge distributions (<xref ref-type="fig" rid="F2">Fig. 2B,C</xref>). Our yHydra score (<xref ref-type="fig" rid="F2">Fig. 2E</xref>) resembles a bimodal distribution and thus separates targets and decoys better than MSFragger’s hyperscore (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). Due to the open search mode each PSM is assigned with a delta mass (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). Finally, yHydra identified 10,810 peptides at 1% FDR (<xref ref-type="fig" rid="F2">Fig. 2D</xref>). In comparison, MSFragger identified 10,524 peptides at 1% FDR while both search engines share 5,954 of these peptides (<xref ref-type="fig" rid="F2">Fig. 2G</xref>).</p></sec><sec id="S5"><title>yHydra enables error-tolerant search via gradient descent</title><p id="P15">As an additional sub-task for our foundation model we implemented an error-tolerant search by using a gradient descent in the learned embedding space of yHydra. An error-tolerant search identifies peptides that deviate from the given reference proteome. A proteome is context-matched with a finite collection of protein sequences. However, real protein sequences can have sequence variations. An error-tolerant search uses the reference proteome as guidance while being able to deviate from the contained protein sequences and thus accounts for mutations. The gradient descent inputs PSMs with non-zero delta mass and a minimum score, being an user-adjustable parameter. Delta masses (mass difference between precursor and the candidate peptide) come from the output of our open search, see previous section. In particular, each descent starts with a candidate peptide (<xref ref-type="fig" rid="F3">Fig.3A</xref>, indicated step-0). Along the gradient descent, peptides are updated based on the gradient information (<xref ref-type="fig" rid="F3">Fig.3A</xref>, green boxes) when gradient steps are made towards the MS/MS spectrum embedding (<xref ref-type="fig" rid="F3">Fig.3A</xref>, blue box). Based the L2-gradient we update the peptide embedding and subsequently also the peptide sequence (<xref ref-type="fig" rid="F3">Fig.3A</xref>). The search terminates if the delta mass is zero or maximum number of steps is reached. The search was successful if the updated peptide has an improved score (compared to the initial candidate peptide) (<xref ref-type="fig" rid="F3">Fig.3A</xref>, dark green box).</p><p id="P16">To explore a wide range of peptide decodings by considering multiple alternative gradient descents (due to possible multiple local minima in embedding space, (<xref ref-type="fig" rid="F3">Fig.3A</xref>)) we implemented a beam search using yHydra (<xref ref-type="fig" rid="F3">Fig.3B</xref>). For the beam search, at each updating step and looking at the reward matrix (<xref ref-type="fig" rid="F3">Fig.3B</xref>, purple matrix) we follow multiple potential AA-changes, so-called beams. In contrast to a simple greedy search where only the single best change per step is kept. Two beams are illustrated, the correct beam (<xref ref-type="fig" rid="F3">Fig.3B</xref>, green arrows) and one alternative beam (<xref ref-type="fig" rid="F3">Fig.3B</xref>, red arrows). Both beams are specific realizations of AA-changes (<xref ref-type="fig" rid="F3">Fig.3B</xref>, indicated steps 0-3). All beams per PSM are kept and if the resulting peptide has a non-zero delta mass its score appears in the score-sorted list of beams (<xref ref-type="fig" rid="F3">Fig.3B</xref>, list of beams). Finally, per PSM the top scoring peptide with zero delta mass is reported in the results of the error-tolerant search (<xref ref-type="fig" rid="F3">Fig.3B</xref>, list of beams). The beam search explores multiple potential solutions (beams) and we report the overall best solution as the identified peptidoform (<xref ref-type="fig" rid="F3">Fig.3B</xref>, top-scoring peptide in list of beams) per PSM.</p></sec><sec id="S6"><title>yHydra is interpretable by looking at the AA-change reward matrices of the beam search</title><p id="P17">To interpret yHydra we take a closer look at the reward matrix of the gradient descent search (<xref ref-type="fig" rid="F3">Fig.3B</xref>, purple matrix) described in the previous section. yHydra is comprised of two embedding blocks, the Spectrum Transformer and the Peptide Transformer (<xref ref-type="fig" rid="F1">Fig.1A</xref>). Both are differentiable with respect to their inputs. This is essential to train both models but also provides means to interpret them. To interpret the peptide embedding we calculate the gradient of a given peptide embedding relative to its matching spectrum embedding (i.e. the partial derivative for each AA-position). Using the learned AA-encoding matrix we can further project this partial derivative onto a matrix that has the shape of the AA-alphabet times the peptide length (<xref ref-type="fig" rid="F3">Fig.3F</xref>), see <xref ref-type="sec" rid="S10">Methods</xref>.</p><p id="P18">As baselines we compare additional types of reward matrices that express AA-changes (<xref ref-type="fig" rid="F3">Fig.3D-H</xref>). Therefore, we computed the Point Accepted Mutation (PAM) matrix (<xref ref-type="fig" rid="F3">Fig.3G</xref>), which generally describes mutation probabilities of protein AA-changes, i.e. independent from our model. Also, we employ a binary delta mass matrix which constraints all possible AA-changes given a certain delta mass (<xref ref-type="fig" rid="F3">Fig.3H</xref>). Lastly, we compute reward matrices based on the L2-distance and the hyperscore (<xref ref-type="fig" rid="F3">Fig.3D+E</xref>). These two require exhaustively changing each AA at each position and store the changes of either L2 (<xref ref-type="fig" rid="F3">Fig.3D</xref>) or the hyperscore (<xref ref-type="fig" rid="F3">Fig.3E</xref>) as matrices. Note, these latter two matrices are more expensive to compute than our L2-gradient based matrix. For the beam search above we rely on the combined reward matrix (<xref ref-type="fig" rid="F3">Fig.3C</xref>) which is a combination of the gradient matrices (<xref ref-type="fig" rid="F3">Fig.3F-H</xref>), see <xref ref-type="sec" rid="S10">Methods</xref>.</p><p id="P19">We can interpret yHydra by looking at the combined reward matrix (<xref ref-type="fig" rid="F3">Fig.3C</xref>). This matrix is a visualization of how a current peptide, in this case TSEILTVNSIGQLK, should be changed. For this particular candidate peptide the single highest reward change is changing S (at position 2) into P, which results in the correct peptide TPEILTVNSIGQLK (<xref ref-type="fig" rid="F3">Fig.3C</xref>).</p></sec><sec id="S7"><title>yHydra boosts peptide identifications for a monoclonal antibody via error-tolerant searching</title><p id="P20">Our gradient descent (gd) error-tolerant search can be used either as standalone or as post-processing to existing open search search engines. Here, we demonstrate both and compare i) yHydra open search with boosting via error-tolerant search via gradient descent ’yHydra gd’ and ii) MSFragger open search and subsequent post-processing of MS-Fragger results via our gradient descent strategy, which we annotate as ’MSFragger gd’ subsequently (<xref ref-type="fig" rid="F4">Fig.4</xref>). As described above (<xref ref-type="fig" rid="F3">Fig.3A</xref>) the error-tolerant search is initialized from candidate PSMs that have a substantial delta mass (<xref ref-type="fig" rid="F3">Fig.3A</xref>) and a minimum score (<xref ref-type="fig" rid="F3">Fig.3B</xref>) both are parameters of the error-tolerant search, see <xref ref-type="sec" rid="S10">Methods</xref>. The gd error-tolerant search is formulated as an optimization algorithm that aims to reduce the absolute delta mass while simultaneously maximizing the score for the PSMs with their updated peptides. The gd error-tolerant search is formulated as an optimization algorithm that aims to maximize the score for the PSMs while removing the delta mass (i.e. delta mass being zero) by updating the peptide sequence based on the gradient information. This –explaining-away of delta mass for a better score– can be seen (orange dashed box in <xref ref-type="fig" rid="F4">Fig.4A</xref>), while most PSMs before the gd search are outside of orange box they enrich after the gd search (MS-Fragger gd and yHydra gd) in that box. Simultaneously these same PSMs receive a better scoring, shifting the score distribution of initial candidate PSMs to higher scores for both gradient descent searches MSFragger gd and yHydra gd (<xref ref-type="fig" rid="F4">Fig.4B</xref>). Here, we searched a tryptic digest of a monoclonal antibody (from MSV000079801) against heavy and light chains of human immunoglobulin sequences that are publicly available in UniProt [<xref ref-type="bibr" rid="R31">31</xref>]. To control for the FDR we kept decoy PSMs (<xref ref-type="fig" rid="F4">Fig.4A-B</xref>) throughout the workflow and evaluated q-values (<xref ref-type="fig" rid="F4">Fig.4C</xref>) based on these decoys that underwent gradient descent search similar to true peptides. Actual hits are peptides contained in the complete assembly of the monoclonal antibody MSV000079801 [<xref ref-type="bibr" rid="R28">28</xref>] serving as ground truth here. As a result, the peptide identifications from the monoclonal antibody are boosted, from 39 to 43 peptides for MSFragger to MSFragger gd and from 50 to 55 peptides for yHydra to yHydra gd (<xref ref-type="fig" rid="F3">Fig.3C+D</xref>), each at 5% peptide FDR.</p></sec><sec id="S8"><title>yHydra error-tolerant search identifies chimpanzee peptides in a cross-species search</title><p id="P21">Here, we searched a tryptic digest of a chimpanzee blood serum sample against the human proteome in an error-tolerant manner (<xref ref-type="fig" rid="F4">Fig.4E-H</xref>). Gentic differences between chimpanzee and human are well understood on the genome level, but experimental evidence for the majority of the predicted chimpanzee proteins is missing [<xref ref-type="bibr" rid="R30">30</xref>]. In a first pass, we performed open searches using yHydra or MSFragger and the resulting PSMs served as candidates for the gradient descent strategy. These second passes are called yHydra gd and MSFragger gd respectively (<xref ref-type="fig" rid="F4">Fig.4E+F</xref>). As ground truth we used predicted protein sequences from genome translation of protein-coding genes of the chimpanzee genome [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R29">29</xref>]. Specifically, an identified peptide is counted as hit when it is contained in the aforementioned predicted chimpanzee proteome. Similarly, we keep decoy hits from the reversed human protein sequences (open search candidates) throughout the gradient descent workflow and they are counted as decoy hits if they appear in the reversed chimpanzee protein sequences. This allows us to control for the FDR (q-value) (<xref ref-type="fig" rid="F4">Fig.4G+H</xref>). As a result, MSFragger identified 1658 peptides and yHydra found 1715 peptides (<xref ref-type="fig" rid="F4">Fig.4H</xref>). For the error-tolerant searches, MSFragger gd finds 1678 peptides while yHydra gd yielded 1717 peptides at 1% FDR.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P22">We propose a new approach of how to embed peptides and spectra of MS-based proteomics jointly and thus provide a foundation model to solve various sub-tasks. Using our model, we implemented an open search to unrestrictedly characterize post-translational modified peptides. Besides, we implemented an error-tolerant mode, that we evaluated as a standalone but also as post-processing to an existing algorithmic search engine. Our foundation model serves as a platform to implement further sub-task either by implementation or fine-tuning. In particular, we provide trained model weights for the two instruments Q Exactive and timsTOF, as well as for tryptic and non-tryptic peptides (Supplementary Material). We evaluated our model on relevant proteomics datasets, identifying proteoforms in a monoclonal antibody digest and a chimpanzee serum sample.</p><p id="P23">Training of yHydra makes use of historical data, consisting of previously identified peptide spectrum matches (PSMs). We directly used PSMs for the contrastive loss function because each mini-batch consists a set of matching PSMs (diagonal of L2-matrix in <xref ref-type="fig" rid="F1">Fig. 1A</xref>) and non-matching PSMs as negative training examples. We found that a random set of PSMs per mini-batch is sufficient. Additional MS1-precursor matching per mini-batch is not improving the model performance any further. For the open search we use a GPU-accelerated k-NN search (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). The number of neighbours k is a parameter, configurable by the user. Generally, a smaller k tends improves runtime, while a larger k tends to improve sensitivity. However, as a heuristic and depending on the dataset, a larger k may also result in low-scoring PSMs in the final target-decoy scoring, which effectively may reduce the number of identified peptides. Also, for an open search a larger k (larger than 100) is advisable since not only peptides but also their peptide species with various delta masses need to be considered.</p><p id="P24">To present spectra to the Spectrum Transformer we use wavelet encoding (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). We could show, that the wavelet encoding is able to resolve two peaks in close proximity. In particular, we show that the L2 distance applied directly on a peak encoding (before the Transformer) is approximating the actual distance between the two peaks and is highly responsive especially at closer distances for better resolution (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). An open search by yHydra is possible because our embeddings are largely indifferent across PTMs, i.e. a peptide sequence can be identified regardless of specific PTMs (however the reported delta masses can give rise to PTMs). Specifically, this can be seen from the respective UMAP visualizations (<xref ref-type="fig" rid="F1">Fig. 1D</xref>) and in the search results themselves and ultimately from the resulting delta mass profile of identified peptides (<xref ref-type="fig" rid="F2">Fig. 2A</xref>).</p><p id="P25">For the open search we evaluated yHydra against MSFragger, searching a sample of tryptic peptides from cyanobacteria. Despite the two methods employ different approaches, both show comparable results with more identified peptides in case of yHydra. MSFragger is an algorithmic approach based on a fragment-index, whereas ours is making use of the joint embeddings in Euclidean space to identify k peptide candidates in a k-NN search. These k candidates are subsequently scored (<xref ref-type="fig" rid="F2">Fig. 2E</xref>). When comparing the two score distributions of yHydra (<xref ref-type="fig" rid="F2">Fig. 2E</xref>) and MSFragger (<xref ref-type="fig" rid="F2">Fig. 2A</xref>) we see that yHydra has a better separation between target and decoy PSMs. Using yHydra for open searching additionally results in PSMs with delta masses. Each delta mass quantifies the difference between the MS1 precursor mass and the identified peptide mass. This delta mass is the aggregated mass of potential PTMs or due to sequence variants. A summary of all delta masses in the cyanobacteria sample is summarized as a delta mass profile (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). Here, we observe PTMs like an oxidation (Ox) +16 Da, cysteine carbamidomethylation (CAM) +57 Da, or combinations thereof. For example, 73 Da is Ox+CAM and two CAMs result in +114 Da (<xref ref-type="fig" rid="F2">Fig. 2F</xref>).</p><p id="P26">We implemented an error-tolerant search using our foundation model yHydra. Using our beam search we explored alternative AA-changes (<xref ref-type="fig" rid="F3">Fig.3A+B</xref>) due to the 2nd-, 3rd- and xth-best value in the combined reward matrix (<xref ref-type="fig" rid="F3">Fig.3C</xref>) for a given peptide candidate. Taking into account these alternative updates (i.e. alternative beams) increases our chances to decode the correct peptide (<xref ref-type="fig" rid="F3">Fig.3A</xref>, correct descent 1b). The final peptide is identified by scoring each decoded peptide with zero delta mass (<xref ref-type="fig" rid="F3">Fig.3B</xref>). The number of alternative beams is a sensitivity-to-runtime tradeoff, selectable by the user. For the two error-tolerant experiments (monoclonal antibody and chimpanzee plasma) we were able to decode peptides that score higher than their database-derived counterparts (<xref ref-type="fig" rid="F4">Fig.4B+F</xref>). While these decoded peptides also result in an enrichment of zero delta masses after they have been decoded (<xref ref-type="fig" rid="F4">Fig.4A+E</xref>, orange box). The increase of scores and vanishing of delta masses is predominantly true for hits against the ground truth sequences while less pronounced for decoy peptides. Ultimately, this suggests that our error-tolerant search using a gradient descent is a capable strategy to explore peptido-forms in such challenging datasets.</p><p id="P27">Altogether, our foundation model provides a powerful framework for MS-based proteomics. In our experiments for the various sub-tasks it surpasses other algorithmic approaches and results in robust peptide identification. We foresee, that our foundation model may also be extended to additional sub-tasks, as it is not only limited to the cases shown in this work. For example, de novo sequencing may be possible within the joint embedding space, as it would be related to our gradient descent approach and as dedicated Transformer models exist for de novo sequencing [<xref ref-type="bibr" rid="R21">21</xref>]. Further sub-tasks may be multi-omics integration by employing the embeddings and passing them to a downstream machine learning model, similar to the UMAP of our embeddings. Lastly, an additionally trained gene sequence Transformer could be a promising approach for tackling the six-frame translation problem in proteogenomics [<xref ref-type="bibr" rid="R14">14</xref>].</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>Preprocessing and encoding of spectra and peptides</title><p id="P28">The intensities are normalized such that the intensities (<italic>I</italic><sub>1</sub>, …, <italic>I</italic><sub>l</sub>, …, <italic>I</italic><sub>N</sub>) are a unity-length vector (i.e. scaled by <inline-formula><mml:math id="M1"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:msubsup><mml:mi>I</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>. For each peaks list, up to 500 peaks are kept. Otherwise, if the list contains more than the maximum number of peaks, we keep peaks according to the top-500 highest intensities or append tuples of (0.0,0.0) if the peaks list contained less peaks.</p><p id="P29">Both spectra and peptides are fed to Transformer architectures, hence we adapt the original positional encoding (also called wavelet encoding) and extend it to suit the context of spectra and peptides.</p><p id="P30">For the positional encoding of the peaks list of the peaks list (<italic>mz</italic><sub>1</sub>, …, <italic>mz<sub>l</sub></italic>, …, mz<sub>500</sub>), each <italic>mz<sub>l</sub></italic> is multiplied by the radiant rate <italic>r<sub>i</sub></italic> of the <italic>i</italic>-th dimension of the encoding vector <italic>s</italic>: <italic>s<sub>li</sub></italic> = <italic>mz<sub>l</sub></italic> · <italic>r<sub>i</sub></italic>, where <italic>r<sub>i</sub></italic> := 10000.0<sup>−<italic>i/d</italic></sup>. Furthermore, <inline-formula><mml:math id="M2"><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> if <italic>i</italic> is an even integer and <inline-formula><mml:math id="M3"><mml:msubsup><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> if <italic>i</italic> is odd. For the final spectrum encoding <italic>S</italic>, each positional encoded <italic>mz<sub>l</sub></italic> is scaled by its respective intensity: <inline-formula><mml:math id="M4"><mml:msub><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula>. Hence <italic>S</italic> is a tensor of size (number of peaks, <italic>d</italic>) = (500,64).</p><p id="P31">For each peptide sequence, we enumerate each amino acid (starting from zero, up to peptide length n - 1) these integer positions <italic>p<sub>n</sub></italic> are then multiplied by a radiant rate <italic>r<sub>i</sub></italic> (same definition as above) of the <italic>i</italic>-th dimension of the peptide positional encoding vector <italic>p</italic>: <italic>p<sub>li</sub></italic> = <italic>p<sub>l</sub></italic> · <italic>r<sub>i</sub></italic>. Up to n=42 amino acids are allowed, if the peptide is shorter it is padded with zeros. Hence, the peptide positional encoding vector has a size of (42,64). Furthermore, the amino acids are replaced by unique indices (starting from 0 up to 25, with zero being a padding character) and function as the index into a lookup table of size (alphabet size, d) = (26, 64), where the parameters of the lookup table are trainable and trained together with the model. For the final peptide sequence encoding P, each positional encoded <italic>p<sub>i</sub></italic> is multiplied by its amino acids embedding (from the lookup table) : <italic>P<sub>li</sub></italic> = <italic>p<sub>li</sub></italic> · <italic>aa<sub>i</sub></italic>. Hence <italic>P</italic> is a tensor of size (maximum peptide length, d) = (42,64).</p></sec><sec id="S12"><title>Training of yHydra</title><sec id="S13"><title>Training data</title><p id="P32">The Transformer models of yHydra were trained on 19,991,263 PSMs (retrieved as USIs) from the following 67 repositories: PXD000702, PXD001072, PXD001344, PXD001351, PXD002054, PXD002147, PXD003094, PXD003261, PXD003364, PXD003556, PXD003718, PXD003779, PXD003916, PXD003976, PXD004398, PXD004825, PXD005009, PXD005117, PXD005196, PXD005306, PXD005341, PXD005654, PXD005744, PXD006033, PXD006084, PXD006316, PXD006375, PXD006389, PXD006645, PXD006823, PXD006836, PXD008592, PXD008602, PXD008622, PXD008647, PXD008667, PXD008895, PXD009387, PXD009665, PXD009698, PXD009713, PXD010000, PXD010641, PXD010827, PXD011042, PXD011583, PXD011712, PXD011714, PXD011984, PXD012827, PXD013274, PXD013304, PXD013684, PXD013711, PXD013712, PXD013724, PXD013890, PXD013897, PXD015153, PXD015296, PXD015698, PXD016833, PXD016846, PXD017308, PXD018714, PXD019095, PXD019134. We specifically selected repositories of non-model organisms (excluding the top-10 most commonly studied organisms in terms of counts of repositories) to reduce the bias towards certain proteome-specific sequence patterns. Furthermore, we only included data acquiered on Q Exactive.</p></sec><sec id="S14"><title>Pairwise contrastive loss of yHydra</title><p id="P33">The loss of yHydra is inspired by the recent approach of CLIP [<xref ref-type="bibr" rid="R26">26</xref>] and is based on the idea to directly calculate a contrastive loss based on the pairwise distances within each mini-batch (<xref ref-type="table" rid="T2">table 2</xref> and <xref ref-type="fig" rid="F1">Fig.1A</xref>). We found this approach has major advantages over previous types of contrastive-losses or triplet-losses while virtually having none of their shortcomings. Most importantly the pairwise contrastive loss does not not require the creation of artificial negatives as they naturally occur due to the mixed pairs of distances (i.e. off-diagonal elements in <xref ref-type="fig" rid="F1">Fig.1A</xref>). This is not only more elegant than previous contrastive loss formulations but also makes the network learn at anytime, whereas for older types of contrastive losses typically hard-negative mining was essential to get decent training results. Furthermore, we extended this idea by adding label smoothing which should allow the model to also learn from the specific but small distances that mixed negatives naturally have. Label smoothing allows the model to also learn from this regime (i.e. off-diagonal elements in <xref ref-type="fig" rid="F1">Fig.1A</xref>).</p></sec></sec><sec id="S15"><title>yHydra algorithm</title><sec id="S16"><title>Multiplexed k-NN search of mass buckets for closed, narrow and open searches</title><p id="P34">Our multiplexed k-NN search allows us to search all spectra embeddings against all peptide embeddings within the same search call while at the same time only spectra against theoretically possible peptides (e.g. determined by certain combinations of precursor mass and respective peptide masses) are searched. Therefore we divide the peptides in the database into buckets according to their theoretical mass. Hence, for a closed search we could have a thousand of small buckets (of +/-1 Da width) and, in contrast, for the open search we have a few but wide buckets (e.g. +/- 500 Da width). Each bucket gets a unique vector assigned, which is appended to the peptide embeddings in that bucket (i.e. similar to an ’address’ vector). Subsequently, the query embeddings, which are supposed to be searched against a specific bucket also gets the respective address vector appended. Effectively, the L2-norm between the embeddings is dictated by their common ’address’-vector because only those with a common address-vector have meaningful intrabuckets L2-distances but comparatively high inter-bucket L2-distances. Ultimately, this allows us to achieve multiple mass-compliant search calls while really only performing a single search.</p></sec><sec id="S17"><title>GPU-accelerated peak matching and PSM scoring</title><p id="P35">The core algorithms of yHydra are GPU-accelerated (i.e. neural networks and k-NN search by faiss [<xref ref-type="bibr" rid="R27">27</xref>]). To further speed up the runtime of yHydra we developed a GPU-accelerated peak-matching and PSM scoring (<xref ref-type="table" rid="T3">table 3</xref>). The idea is to simultaneously score a batch of 64 spectra against their respective k-candidates, i.e. k=50, which means for 3,200 PSMs peaks are matched and scored in parallel.</p></sec><sec id="S18"><title>yHydra error-tolerant search via gradient descent</title><p id="P36">To update a candidate peptide we used the gradient in the Euclidean space. The gradient is the partial derivative of the difference between the peptide embedding and the spectrum embedding: <inline-formula><mml:math id="M5"><mml:mo>∇</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>L</mml:mi><mml:mn>2</mml:mn><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mi>Q</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula>, along each dimension e of the embedding space. By multiplying that gradient with the pairwise difference matrix of learned aa-encodings, which has a shape of (alphabet-size,alphabet-size,embedding-size) we gain the final reward matrix of gradient-based aa-changes: <inline-formula><mml:math id="M6"><mml:mo>∇</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>δ</mml:mi><mml:mi>a</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>. The reward matrix ▽<italic>E<sub>reward</sub></italic> has the shape of maximum peptide length times alphabet-size. A particular reward matrix evaluated for the peptide TSEILTVNSIGQLK is displayed in the main text.</p></sec><sec id="S19"><title>Beam search</title><p id="P37">The beam search helped us to explore many possible updates by also accounting for secondary alternative gradient directions. Each beam starts from the L2-gradient based reward matrix ▽<italic>E<sub>reward</sub></italic> as it provides alphabet-size times peptide length potential updates in a single step. This reward matrix can be seen as a ranking of updates. We set the beam size as percentage of the size of the reward matrix. For example, a peptide of length 14 and considering the alphabet-size 20, using beam size of 10% results in 28 updates for the next step. We tested different beam sizes and selected a beam size of 20% with three update steps for our experiments.</p></sec></sec><sec id="S20"><title>Open search parameters for yHydra and MSFragger</title><p id="P38">For both search open engines (yHydra and MSFragger) the raw files qe2_03132014_1WT-1, qe2_03132014_5WT-2, and qe2_03132014_13WT-3 were searched against the Syn-PCC7002_Cbase.fasta by considering tryptic peptides with up to 1 miscleavage of length between 7 and 42 amino acids. In both searches a minimum delta mass of -150.0 Da and a maximum delta mass of 500.0 Da is considered.</p><p id="P39">For yHydra, the matching tolerance was set to 0.01 Da and a mimimum of 4 matching peaks for a PSM was required. For scoring, the globally highest top-100 peaks per spectrum are considered. For each pepetide, b- and y-ions are calculated and up to 200 fragment ions are considered (any excess ions are discarded starting from higher charge states). For MSFragger, we used the standard search parameters of MSFragger version 3.3, the maximum peptide length of 42 and allowing up to 1 miscleavage. For both methods, the PSM-level FDR was set to 1%.</p></sec></sec></body><back><ack id="S21"><title>Acknowledgements</title><p>This work is supported by a European Research Council (ERC) grant (eXplAInProt, 101124385) to BYR.</p></ack><sec id="S22" sec-type="data-availability"><title>Data Availability</title><p id="P40">Proteomic data were downloaded from public repositories PXD000702, PXD001072, PXD001344, PXD001351, PXD002054, PXD002147, PXD003094, PXD003261, PXD003364, PXD003556, PXD003718, PXD003779, PXD003916, PXD003976, PXD004398, PXD004825, PXD005009, PXD005117, PXD005196, PXD005306, PXD005341, PXD005654, PXD005744, PXD006033, PXD006084, PXD006316, PXD006375, PXD006389, PXD006645, PXD006823, PXD006836, PXD008592, PXD008602, PXD008622, PXD008647, PXD008667, PXD008895, PXD009387, PXD009665, PXD009698, PXD009713, PXD010000, PXD010641, PXD010827, PXD011042, PXD011583, PXD011712, PXD011714, PXD011984, PXD012827, PXD013274, PXD013304, PXD013684, PXD013711, PXD013712, PXD013724, PXD013890, PXD013897, PXD015153, PXD015296, PXD015698, PXD016833, PXD016846, PXD017308, PXD018714, PXD019095, PXD019134 to train yHydra. Furthermore, public data from PXD007963 was downloaded to evaluate yHydra. For the monoclonal antibody benchmark, data was downloaded from MSV000079801 via MassiveKB. The Chimpanzee plasma data is available as a Zenodo dataset [<xref ref-type="bibr" rid="R32">32</xref>].</p></sec><sec id="S23" sec-type="data-availability"><title>Code Availability</title><p id="P41">An open source implementation with command-line instructions is publicly available (under MIT license) at <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/dacs-hpi/yHydra">https://gitlab.com/dacs-hpi/yHydra</ext-link>. A separate open source repository for training yHydra is available at <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/dacs-hpi/yHydra_train">https://gitlab.com/dacs-hpi/yHydra_train</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="conflict"><p id="P42"><bold>Competing interests</bold></p><p id="P43">TA is employed by Johnson &amp; Johnson Innovative Medicine.</p></fn><fn id="FN2"><p id="P44"><bold>Availability</bold>: (under MIT license) <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/dacs-hpi/yHydra">https://gitlab.com/dacs-hpi/yHydra</ext-link></p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aebersold</surname><given-names>Ruedi</given-names></name><name><surname>Mann</surname><given-names>Matthias</given-names></name></person-group><article-title>Mass-spectrometric exploration of proteome structure and function</article-title><source>Nature</source><year>2016</year><month>9</month><volume>537</volume><fpage>347</fpage><lpage>355</lpage><pub-id pub-id-type="pmid">27629641</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nesvizhskii</surname><given-names>Alexey I</given-names></name><name><surname>Vitek</surname><given-names>Olga</given-names></name><name><surname>Aebersold</surname><given-names>Ruedi</given-names></name></person-group><article-title>Analysis and validation of proteomic data generated by tandem mass spectrometry</article-title><source>Nature Methods</source><year>2007</year><month>10</month><volume>4</volume><fpage>787</fpage><lpage>797</lpage><pub-id pub-id-type="pmid">17901868</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>Matthias</given-names></name><name><surname>Kumar</surname><given-names>Chanchal</given-names></name><name><surname>Zeng</surname><given-names>Wen Feng</given-names></name><name><surname>Strauss</surname><given-names>Maximilian T</given-names></name></person-group><article-title>Artificial intelligence for proteomics and biomarker discovery</article-title><source>Cell Systems</source><year>2021</year><volume>12</volume><fpage>759</fpage><lpage>770</lpage><pub-id pub-id-type="pmid">34411543</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neely</surname><given-names>Benjamin A</given-names></name><name><surname>Dorfer</surname><given-names>Viktoria</given-names></name><name><surname>Martens</surname><given-names>Lennart</given-names></name><name><surname>Bludau</surname><given-names>Isabell</given-names></name><name><surname>Bouwmeester</surname><given-names>Robbin</given-names></name><name><surname>Degroeve</surname><given-names>Sven</given-names></name><name><surname>Deutsch</surname><given-names>Eric W</given-names></name><name><surname>Gessulat</surname><given-names>Siegfried</given-names></name><name><surname>Käll</surname><given-names>Lukas</given-names></name><name><surname>Palczynski</surname><given-names>Pawel</given-names></name><name><surname>Payne</surname><given-names>Samuel H</given-names></name><etal/></person-group><article-title>Toward an integrated machine learning model of a proteomics experiment</article-title><source>Journal of Proteome Research</source><year>2023</year><month>3</month><volume>22</volume><fpage>681</fpage><lpage>696</lpage><pub-id pub-id-type="pmcid">PMC9990124</pub-id><pub-id pub-id-type="pmid">36744821</pub-id><pub-id pub-id-type="doi">10.1021/acs.jproteome.2c00711</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gessulat</surname><given-names>Siegfried</given-names></name><name><surname>Schmidt</surname><given-names>Tobias</given-names></name><name><surname>Zolg</surname><given-names>Daniel Paul</given-names></name><name><surname>Samaras</surname><given-names>Patroklos</given-names></name><name><surname>Schnatbaum</surname><given-names>Karsten</given-names></name><name><surname>Zerweck</surname><given-names>Johannes</given-names></name><name><surname>Knaute</surname><given-names>Tobias</given-names></name><name><surname>Rechenberger</surname><given-names>Julia</given-names></name><name><surname>Delanghe</surname><given-names>Bernard</given-names></name><name><surname>Huhmer</surname><given-names>Andreas</given-names></name><name><surname>Reimer</surname><given-names>Ulf</given-names></name><etal/></person-group><article-title>Prosit: proteome-wide prediction of peptide tandem mass spectra by deep learning</article-title><source>Nature Methods</source><year>2019</year><volume>16</volume><fpage>509</fpage><lpage>518</lpage><pub-id pub-id-type="pmid">31133760</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarn</surname><given-names>Ching</given-names></name><name><surname>Zeng</surname><given-names>Wen-Feng</given-names></name></person-group><article-title>pdeep3: Toward more accurate spectrum prediction with fast few-shot learning</article-title><source>Analytical Chemistry</source><year>2021</year><volume>93</volume><fpage>5815</fpage><lpage>5822</lpage><pub-id pub-id-type="pmid">33797898</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Käll</surname><given-names>Lukas</given-names></name><name><surname>Canterbury</surname><given-names>Jesse D</given-names></name><name><surname>Weston</surname><given-names>Jason</given-names></name><name><surname>Noble</surname><given-names>William Stafford</given-names></name><name><surname>MacCoss</surname><given-names>Michael J</given-names></name></person-group><article-title>Semi-supervised learning for peptide identification from shotgun proteomics datasets</article-title><source>Nature Methods</source><year>2007</year><month>10</month><volume>4</volume><fpage>923</fpage><pub-id pub-id-type="pmid">17952086</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fondrie</surname><given-names>William E</given-names></name><name><surname>Noble</surname><given-names>William S</given-names></name></person-group><article-title>mokapot: Fast and flexible semisupervised learning for peptide detection</article-title><source>Journal of Proteome Research</source><year>2021</year><month>4</month><volume>20</volume><fpage>1966</fpage><lpage>1971</lpage><pub-id pub-id-type="pmcid">PMC8022319</pub-id><pub-id pub-id-type="pmid">33596079</pub-id><pub-id pub-id-type="doi">10.1021/acs.jproteome.0c01010</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouwmeester</surname><given-names>Robbin</given-names></name><name><surname>Gabriels</surname><given-names>Ralf</given-names></name><name><surname>Hulstaert</surname><given-names>Niels</given-names></name><name><surname>Martens</surname><given-names>Lennart</given-names></name><name><surname>Degroeve</surname><given-names>Sven</given-names></name></person-group><article-title>Deeplc can predict retention times for peptides that carry as-yet unseen modifications</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><fpage>1363</fpage><lpage>1369</lpage><pub-id pub-id-type="pmid">34711972</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>Florian</given-names></name><name><surname>Köhler</surname><given-names>Niklas D</given-names></name><name><surname>Brunner</surname><given-names>Andreas-David</given-names></name><name><surname>Wanka</surname><given-names>Jean-Marc H</given-names></name><name><surname>Voytik</surname><given-names>Eugenia</given-names></name><name><surname>Strauss</surname><given-names>Maximilian T</given-names></name><name><surname>Theis</surname><given-names>Fabian J</given-names></name><name><surname>Mann</surname><given-names>Matthias</given-names></name></person-group><article-title>Deep learning the collisional cross sections of the peptide universe from a million experimental values</article-title><source>Nature Communications</source><year>2021</year><month>12</month><volume>12</volume><fpage>1185</fpage><pub-id pub-id-type="pmcid">PMC7896072</pub-id><pub-id pub-id-type="pmid">33608539</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-21352-8</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogaert</surname><given-names>Annelies</given-names></name><name><surname>Fernandez</surname><given-names>Esperanza</given-names></name><name><surname>Gevaert</surname><given-names>Kris</given-names></name></person-group><article-title>N-terminal proteoforms in human disease</article-title><source>Trends in Biochemical Sciences</source><year>2020</year><month>4</month><volume>45</volume><fpage>308</fpage><lpage>320</lpage><pub-id pub-id-type="pmid">32001092</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Andy T</given-names></name><name><surname>Leprevost</surname><given-names>Felipe V</given-names></name><name><surname>Avtonomov</surname><given-names>Dmitry M</given-names></name><name><surname>Mellacheruvu</surname><given-names>Dattatreya</given-names></name><name><surname>Nesvizhskii</surname><given-names>Alexey I</given-names></name></person-group><article-title>Msfragger: Ultrafast and comprehensive peptide identification in mass spectrometry-based proteomics</article-title><source>Nature Methods</source><year>2017</year><volume>14</volume><fpage>513</fpage><lpage>520</lpage><pub-id pub-id-type="pmcid">PMC5409104</pub-id><pub-id pub-id-type="pmid">28394336</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.4256</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyter</surname><given-names>Doruk</given-names></name><name><surname>Lin</surname><given-names>Miin S</given-names></name><name><surname>Yu</surname><given-names>Yanbao</given-names></name><name><surname>Pieper</surname><given-names>Rembert</given-names></name><name><surname>Bafna</surname><given-names>Vineet</given-names></name></person-group><article-title>Proteostorm: An ultrafast metaproteomics database search framework</article-title><source>Cell Systems</source><year>2018</year><month>10</month><volume>7</volume><fpage>463</fpage><lpage>467</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC6231400</pub-id><pub-id pub-id-type="pmid">30268435</pub-id><pub-id pub-id-type="doi">10.1016/j.cels.2018.08.009</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renard</surname><given-names>Bernhard Y</given-names></name><name><surname>Xu</surname><given-names>Buote</given-names></name><name><surname>Kirchner</surname><given-names>Marc</given-names></name><name><surname>Zickmann</surname><given-names>Franziska</given-names></name><name><surname>Winter</surname><given-names>Dominic</given-names></name><name><surname>Korten</surname><given-names>Simone</given-names></name><name><surname>Brattig</surname><given-names>Norbert W</given-names></name><name><surname>Tzur</surname><given-names>Amit</given-names></name><name><surname>Hamprecht</surname><given-names>Fred A</given-names></name><name><surname>Steen</surname><given-names>Hanno</given-names></name></person-group><article-title>Overcoming species boundaries in peptide identification with bayesian information criterion-driven error-tolerant peptide search (biceps)</article-title><source>Molecular &amp; Cellular Proteomics</source><year>2012</year><month>7</month><volume>11</volume><comment>M111.014167–1–M111.014167–12</comment><pub-id pub-id-type="pmcid">PMC3394943</pub-id><pub-id pub-id-type="pmid">22493179</pub-id><pub-id pub-id-type="doi">10.1074/mcp.M111.014167</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devabhaktuni</surname><given-names>Arun</given-names></name><name><surname>Lin</surname><given-names>Sarah</given-names></name><name><surname>Zhang</surname><given-names>Lichao</given-names></name><name><surname>Swaminathan</surname><given-names>Kavya</given-names></name><name><surname>Gonzalez</surname><given-names>Carlos G</given-names></name><name><surname>Olsson</surname><given-names>Niclas</given-names></name><name><surname>Pearlman</surname><given-names>Samuel M</given-names></name><name><surname>Rawson</surname><given-names>Keith</given-names></name><name><surname>Elias</surname><given-names>Joshua E</given-names></name></person-group><article-title>Taggraph reveals vast protein modification landscapes from large tandem mass spectrometry datasets</article-title><source>Nature Biotechnology</source><year>2019</year><month>4</month><volume>37</volume><fpage>469</fpage><lpage>479</lpage><pub-id pub-id-type="pmcid">PMC6447449</pub-id><pub-id pub-id-type="pmid">30936560</pub-id><pub-id pub-id-type="doi">10.1038/s41587-019-0067-5</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittremieux</surname><given-names>Wout</given-names></name><name><surname>Meysman</surname><given-names>Pieter</given-names></name><name><surname>Noble</surname><given-names>William Stafford</given-names></name><name><surname>Laukens</surname><given-names>Kris</given-names></name></person-group><article-title>Fast open modification spectral library searching through approximate nearest neighbor indexing</article-title><source>bioRxiv</source><year>2018</year><volume>1</volume><pub-id pub-id-type="pmcid">PMC6173621</pub-id><pub-id pub-id-type="pmid">30184435</pub-id><pub-id pub-id-type="doi">10.1021/acs.jproteome.8b00359</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altenburg</surname><given-names>Tom</given-names></name><name><surname>Giese</surname><given-names>Sven</given-names></name><name><surname>Wang</surname><given-names>Shengbo</given-names></name><name><surname>Muth</surname><given-names>Thilo</given-names></name><name><surname>Renard</surname><given-names>Bernhard Y</given-names></name></person-group><article-title>Ahlf: ad hoc learning of peptide fragmentation from mass spectra enables an interpretable detection of phosphorylated and cross-linked peptides</article-title><source>bioRxiv</source><year>2021</year><elocation-id>2020.05.19.101345</elocation-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>Ngoc Hieu</given-names></name><name><surname>Zhang</surname><given-names>Xianglilan</given-names></name><name><surname>Xin</surname><given-names>Lei</given-names></name><name><surname>Shan</surname><given-names>Baozhen</given-names></name><name><surname>Li</surname><given-names>Ming</given-names></name></person-group><article-title>De novo peptide sequencing by deep learning</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><fpage>8247</fpage><lpage>8252</lpage><pub-id pub-id-type="pmcid">PMC5547637</pub-id><pub-id pub-id-type="pmid">28720701</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1705691114</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiao</surname><given-names>Rui</given-names></name><name><surname>Tran</surname><given-names>Ngoc Hieu</given-names></name><name><surname>Xin</surname><given-names>Lei</given-names></name><name><surname>Chen</surname><given-names>Xin</given-names></name><name><surname>Li</surname><given-names>Ming</given-names></name><name><surname>Shan</surname><given-names>Baozhen</given-names></name><name><surname>Ghodsi</surname><given-names>Ali</given-names></name></person-group><article-title>Computationally instrument-resolution-independent de novo peptide sequencing for high-resolution devices</article-title><source>Nature Machine Intelligence</source><year>2021</year><volume>3</volume></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>Melih</given-names></name><name><surname>Fondrie</surname><given-names>William</given-names></name><name><surname>Bittremieux</surname><given-names>Wout</given-names></name><name><surname>Oh</surname><given-names>Sewoong</given-names></name><name><surname>Noble</surname><given-names>William S</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Chaudhuri</surname><given-names>Kamalika</given-names></name><name><surname>Jegelka</surname><given-names>Stefanie</given-names></name><name><surname>Song</surname><given-names>Le</given-names></name><name><surname>Szepesvari</surname><given-names>Csaba</given-names></name><name><surname>Niu</surname><given-names>Gang</given-names></name><name><surname>Sabato</surname><given-names>Sivan</given-names></name></person-group><source>De novo mass spectrometry peptide sequencing with a transformer model</source><conf-name>Proceedings of the 39th International Conference on Machine Learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2022</year><month>5</month><volume>162</volume><fpage>25514</fpage><lpage>25522</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yilmaz</surname><given-names>Melih</given-names></name><name><surname>Fondrie</surname><given-names>William E</given-names></name><name><surname>Bittremieux</surname><given-names>Wout</given-names></name><name><surname>Melendez</surname><given-names>Carlo F</given-names></name><name><surname>Nelson</surname><given-names>Rowan</given-names></name><name><surname>Ananth</surname><given-names>Varun</given-names></name><name><surname>Oh</surname><given-names>Sewoong</given-names></name><name><surname>Noble</surname><given-names>William Stafford</given-names></name></person-group><article-title>Sequence-to-sequence translation from mass spectra to peptides with a transformer model</article-title><source>Nature Communications</source><year>2024</year><month>7</month><volume>15</volume><fpage>6427</fpage><pub-id pub-id-type="pmcid">PMC11289372</pub-id><pub-id pub-id-type="pmid">39080256</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-49731-x</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klaproth-Andrade</surname><given-names>Daniela</given-names></name><name><surname>Hingerl</surname><given-names>Johannes</given-names></name><name><surname>Bruns</surname><given-names>Yanik</given-names></name><name><surname>Smith</surname><given-names>Nicholas H</given-names></name><name><surname>Träuble</surname><given-names>Jakob</given-names></name><name><surname>Wilhelm</surname><given-names>Mathias</given-names></name><name><surname>Gagneur</surname><given-names>Julien</given-names></name></person-group><article-title>Deep learning-driven fragment ion series classification enables highly precise and sensitive de novo peptide sequencing</article-title><source>Nature Communications</source><year>2024</year><month>1</month><volume>15</volume><fpage>151</fpage><pub-id pub-id-type="pmcid">PMC10762064</pub-id><pub-id pub-id-type="pmid">38167372</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-44323-7</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bittremieux</surname><given-names>Wout</given-names></name><name><surname>May</surname><given-names>Damon H</given-names></name><name><surname>Bilmes</surname><given-names>Jeffrey</given-names></name><name><surname>Noble</surname><given-names>William Stafford</given-names></name></person-group><article-title>A learned embedding for efficient joint analysis of millions of mass spectra</article-title><source>Nature Methods</source><year>2022</year><month>6</month><volume>19</volume><fpage>675</fpage><lpage>678</lpage><pub-id pub-id-type="pmcid">PMC9189069</pub-id><pub-id pub-id-type="pmid">35637305</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01496-1</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tariq</surname><given-names>Muhammad Usman</given-names></name><name><surname>Saeed</surname><given-names>Fahad</given-names></name></person-group><article-title>Specollate: Deep cross-modal similarity network for mass spectrometry data based peptide deductions</article-title><source>PLOS ONE</source><year>2021</year><month>10</month><volume>16</volume><elocation-id>e0259349</elocation-id><pub-id pub-id-type="pmcid">PMC8555789</pub-id><pub-id pub-id-type="pmid">34714871</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0259349</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaswani</surname><given-names>Ashish</given-names></name><name><surname>Shazeer</surname><given-names>Noam</given-names></name><name><surname>Parmar</surname><given-names>Niki</given-names></name><name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name><name><surname>Jones</surname><given-names>Llion</given-names></name><name><surname>Gomez</surname><given-names>Aidan N</given-names></name><name><surname>Kaiser</surname><given-names>Lukasz</given-names></name><name><surname>Polosukhin</surname><given-names>Illia</given-names></name></person-group><source>Attention is all you need</source><year>2017</year><volume>6</volume></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>Alec</given-names></name><name><surname>Kim</surname><given-names>Jong Wook</given-names></name><name><surname>Hallacy</surname><given-names>Chris</given-names></name><name><surname>Ramesh</surname><given-names>Aditya</given-names></name><name><surname>Goh</surname><given-names>Gabriel</given-names></name><name><surname>Agarwal</surname><given-names>Sandhini</given-names></name><name><surname>Sastry</surname><given-names>Girish</given-names></name><name><surname>Askell</surname><given-names>Amanda</given-names></name><name><surname>Mishkin</surname><given-names>Pamela</given-names></name><name><surname>Clark</surname><given-names>Jack</given-names></name><name><surname>Krueger</surname><given-names>Gretchen</given-names></name><etal/></person-group><source>Learning transferable visual models from natural language supervision</source><year>2021</year><volume>2</volume></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>Jeff</given-names></name><name><surname>Douze</surname><given-names>Matthijs</given-names></name><name><surname>Jegou</surname><given-names>Herve</given-names></name></person-group><article-title>Billion-scale similarity search with gpus</article-title><source>IEEE Transactions on Big Data</source><year>2019</year><fpage>1</fpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tran</surname><given-names>Ngoc Hieu</given-names></name><name><surname>Rahman</surname><given-names>M Ziaur</given-names></name><name><surname>He</surname><given-names>Lin</given-names></name><name><surname>Xin</surname><given-names>Lei</given-names></name><name><surname>Shan</surname><given-names>Baozhen</given-names></name><name><surname>Li</surname><given-names>Ming</given-names></name></person-group><article-title>Complete de novo assembly of monoclonal antibody sequences</article-title><source>Scientific Reports</source><year>2016</year><month>8</month><volume>6</volume><elocation-id>31730</elocation-id><pub-id pub-id-type="pmcid">PMC4999880</pub-id><pub-id pub-id-type="pmid">27562653</pub-id><pub-id pub-id-type="doi">10.1038/srep31730</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="other"><source>Uniprotkb gene model predicted proteome of chimpanzee (taxid:9598)</source></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><collab>The Chimpanzee Sequencing and Analysis Consortium</collab><article-title>Initial sequence of the chimpanzee genome and comparison with the human genome</article-title><source>Nature</source><year>2005</year><month>9</month><volume>437</volume><fpage>69</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">16136131</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="other"><source>Uniprotkb (swiss-prot) immunoglobulin light and heavy chain of human (taxid:9606)</source></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altenburg</surname><given-names>Tom</given-names></name><name><surname>Steen</surname><given-names>Hanno</given-names></name><name><surname>Renard</surname><given-names>Bernhard</given-names></name></person-group><source>Chimpanzee plasma sample timstof</source><year>2024</year><volume>11</volume><pub-id pub-id-type="doi">10.5281/zenodo.14231220</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>yHydra jointly embeds spectra and peptides.</title><p><bold>A</bold>: Illustration of the yHydra architecture: Spectrum Transformer for spectra (grey) and Peptide Transformer for peptide sequences (purple), each embed to their respective embeddings (yellow, blue, pink, green) for which pairwise Euclidean distances (L2-metric) serve as loss during training. The pairwise distance metrics allows to update model parameters such that the diagonal minimized and the off-diagonal is maximized. <bold>B</bold>: Illustration of the open search of yHydra, using a trained Spectrum Transformer and trained Peptide Transformer. Both Transformers embed batches of their respective inputs into the joint embedding space (center) in which a k-nearest neighbor search finds peptide candidates within an entire protein database. An individual spectrum and its correctly matching peptide is highlighted (green). <bold>C</bold>: Left, comparison of the behavior of distances between the wavelet encoding of the m/z location of a single peak with respect to deviations from this location (-10 m/z to 10 m/z). Right, visualization of the change for each of the i-th dimension of the wavelet encoding of a single peak depending on m/z deviations. <bold>D</bold>: UMAP visualization of the manifold of joint embeddings for identified peptide spectrum matches. The manifold appears as two clusters, spectra (left) and peptides (right). Each manifold is colored according to either charge, precursor mass. or post-translational modification. While charge and precursor mass show a clear structure in the UMAP, this is not the case for PTMs, supporting the hypotheses that the identification of modified peptides is feasible by exploring neighborhoods in embedding space.</p></caption><graphic xlink:href="EMS202191-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>yHydra enables ultra-fast open searching, allowing an unrestrictive characterization of post-translational modifications in cyanobacteria.</title><p><bold>A</bold>: Target-decoy distribution of MSFragger hyperscores. <bold>B</bold>: Peptide lengths distribution according to MSFragger (blue) and yHydra (green) identifications. <bold>C</bold>: Precursor charge distribution according to MSFragger (blue) and yHydra (green) identifications. <bold>D</bold>: q-values estimate the FDR (x-axis) in comparison to number of identifications (y-axis) for yHydra (green) and MSFragger (blue) <bold>E</bold>: Target-decoy distribution of yHydra score. <bold>F</bold>: yHydra matches spectra to peptides in a wide mass range, the resulting delta masses between precursor mass and peptide mass are shown here. The five most common delta masses are labeled, including oxidation (Ox) +16 Da, cysteine carbamidomethylation (CAM) +57 Da, and their combinations, +73 Da is Ox+CAM and two CAMs is +114 Da. <bold>G</bold>: Venn diagram representing a unique set peptides identified by MSFragger (blue, 1% FDR), the intersection of identified peptides from both methods (turquoise; 1% FDR) and a unique set of peptides only identified by yHydra (green, 1% FDR).</p></caption><graphic xlink:href="EMS202191-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>yHydra facilitates an interpretable error-tolerant search via gradient descent.</title><p><bold>A</bold>: Illustration of the gradient descent updating a candidate peptide EGIKSPEMVIS (0-step) towards the correct peptide TGIVMPEMVIS (dark green box) via AA-changes informed by the L2-gradient descent towards the MS/MS-embedding (blue box). A correct descent (1a-step) and alternative descent (1b-step) illustrate two alternative local minima. <bold>B</bold>: Overview of the beam search exploring two alternative beams: the correct beam (green arrows) and one alternative beam (red arrows). Beams are based on the reward matrix (purple) in each decoding step. The search initializes with a peptide candidate (0-step) and performs AA-changes (1a,2a,3a-steps) decoding the correct peptide TGIVMPEMVIS, ranked as top-scoring peptide in the list of beams. Exploration of an alternative beam (1b to Xb-steps) terminates with a low scoring peptide appearing at the bottom of the list of beams. <bold>C</bold>: Combined reward matrix, showing the probability of changing one AA of the peptide (x-axis) into any other AA from the AA-alphabet (y-axis), beams of beam-size 10 are indicated (blue boxes), where the correct change (S→P at position 2) has the highest probability. <bold>D-G</bold>: for interpretability, comparing the four AA-change reward matrices: L2-based-, hyperscore-based-, L2 gradient-based- and PAM-based probabilities that we are able to compute using yHydra. <bold>H</bold>: delta mass binary mask showing allowed AA-changes based on possible AA-changes restricted by the delta mass (between candidate peptide and precursor). For <bold>C-H</bold>, the true peptide is shown as panel title, whereas the current candidate peptide AAs are labels on the x-axis.</p></caption><graphic xlink:href="EMS202191-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>yHydra enables error-tolerant searching of a monoclonal antibody and chimpanzee plasma.</title><p><bold>A-D</bold>: Comparisons of the antibody searches by MSFragger and yHydra with additional gradient descent mode (gd). <bold>A</bold>: Absolute delta masses show a better enrichment (orange box) of hits (black) at around zero compared to decoys (pink), comparing MSFragger, yHydra with and without gradient descent (gd). <bold>B</bold>: hyperscores of gd-candidates before and after gradient descent (gd). <bold>C</bold>: q-value curve showing the number of peptide identifications for MSFragger and yHydra before and after gradient descent (gd). Dashed line indicates the number of peptide identifications at 5%FDR peptide level (q-value), shown in <bold>D</bold>. <bold>E-H</bold>: same as <bold>A-D</bold> but for the chimpanzee plasma sample.</p></caption><graphic xlink:href="EMS202191-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Runtimes of yHydra and MSFragger on PXD007963 using a compute node with an A100 (NVIDIA) GPU and a EPYC (AMD) CPU.</title></caption><table frame="below" rules="rows"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">yHydra</th><th align="left" valign="top">MSFragger</th></tr></thead><tbody><tr><td align="right" valign="top">per run [seconds]</td><td align="center" valign="top"><bold>113.3</bold></td><td align="center" valign="top">183.0</td></tr><tr><td align="right" valign="top">total runtime [seconds]</td><td align="center" valign="top"><bold>340.0</bold></td><td align="center" valign="top">549.0</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Tensorflow-/Numpy-like pseudocode of the calculation of the pairwise loss between spectra and peptides. The Transformer models <italic>T<sub>SPEC</sub></italic> and <italic>T<sub>SEQ</sub></italic> yield embeddings <italic>E<sub>SPEC</sub></italic> and <italic>E<sub>SEQ</sub></italic> each of size (batch-size=64, embeddings-size=64). The spectrum encoding SP ECENC and peptide sequence <italic>SEQ<sub>ENC</sub></italic> are described in the main text. Parameters are set to eps=0.001, smoothing=0.1 and T=3.0 and tf.sce is the sparse cross entropy between targets and logits according to the the distance matrix D (illustrated in <xref ref-type="fig" rid="F1">Fig. 1A</xref>).</p></caption><table frame="below" rules="groups"><tbody><tr><td align="left" valign="top">IN</td><td align="left" valign="top">:</td><td align="left" valign="top">spectrum embedding <italic>T<sub>SPEC</sub></italic>(<italic>SPEC<sub>ENC</sub></italic>) peptide embedding <italic>T<sub>SEQ</sub></italic>(<italic>SEQ<sub>ENC</sub></italic>)</td></tr><tr style="border-top: solid thin"><td align="left" valign="top"><italic>E<sub>SPEC</sub></italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>L2N<sub>orm</sub>T<sub>SPEC</sub>(SPEC<sub>ENC</sub>))</italic></td></tr><tr><td align="left" valign="top"><italic>E<sub>SEQ</sub></italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>L2N<sub>orm</sub>(T<sub>SEQ</sub>(SEQ<sub>ENC</sub>))</italic></td></tr><tr><td align="left" valign="top"><italic>D</italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>L2Dist<sub>pairwise</sub>(E<sub>SPEC</sub>, E<sub>SEQ</sub>.T)</italic></td></tr><tr><td align="left" valign="top"><italic>logits</italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>−tf.log(D + eps) · tf.exp(T)</italic></td></tr><tr><td align="left" valign="top"><italic>targets</italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>tf.range(N)</italic></td></tr><tr><td align="left" valign="top"><italic>L<sub>SPEC</sub></italic></td><td align="center" valign="top">=</td><td align="left" valign="top">tf.sce(targets,logits,smoothing,axis=0)</td></tr><tr><td align="left" valign="top"><italic>L<sub>SEQ</sub></italic></td><td align="center" valign="top">=</td><td align="left" valign="top">tf.sce(targets,logits,smoothing,axis=1)</td></tr><tr><td align="left" valign="top"><italic>loss</italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>(L<sub>SPEC</sub> + L<sub>SEQ</sub>)</italic>/2</td></tr><tr style="border-top: solid thin; border-bottom: hidden"><td align="left" valign="top">OUT</td><td align="center" valign="top">:</td><td align="left" valign="top"><italic>loss</italic></td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Tensorflow-/Numpy-like pseudocode of peak matching and scoring for PSMs. The inputs are a minibatch of b spectra, with n peaks considering their mz-values <italic>mz<sub>acq</sub></italic>. and intensities <italic>I<sub>acq.</sub></italic>. Furthermore, list of k candidate peptide (result of the k-NN search) is considered as theoretical ions with up to <italic>l</italic> mz-locations mztheor <italic>mz<sub>theor</sub></italic>, see <xref ref-type="sec" rid="S10">Methods</xref> for details on parameters.</p></caption><table frame="below" rules="groups"><tbody><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">tensor size</td></tr><tr><td align="left" valign="top">IN</td><td align="center" valign="top">:</td><td align="left" valign="top">queries <italic>q</italic> = <italic>mz<sub>theor</sub></italic></td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k,l]</styled-content></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">keys <italic>k</italic> = <italic>mz<sub>acq</sub></italic>.</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,n]</styled-content></td></tr><tr><td align="left" valign="top"/><td align="left" valign="top"/><td align="left" valign="top">values <italic>v</italic> = <italic>I<sub>acq</sub></italic>.</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,n]</styled-content></td></tr><tr style="border-top: solid thin"><td align="left" valign="top">M</td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>L2Dist<sub>pairwise</sub> (q, k.T)</italic></td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k,l,n]</styled-content></td></tr><tr><td align="left" valign="top"><italic>M</italic></td><td align="center" valign="top">=</td><td align="left" valign="top">tf.where(<italic>M</italic> &lt; tolerance, 1/(<italic>M</italic>), 0.0)</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k,l,n]</styled-content></td></tr><tr><td align="left" valign="top">M</td><td align="center" valign="top">=</td><td align="left" valign="top">tf.reduce_maximum(<italic>M</italic>,axis=2)</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k,n]</styled-content></td></tr><tr><td align="left" valign="top"><italic>M</italic></td><td align="center" valign="top">=</td><td align="left" valign="top">tf.where(<italic>M</italic> &gt; 0.0, 1.0, 0.0)</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k,n]</styled-content></td></tr><tr><td align="left" valign="top"><italic>S</italic></td><td align="center" valign="top">=</td><td align="left" valign="top"><italic>M</italic> · <italic>v</italic></td><td align="left" valign="top"><styled-content style="color:#0000FF">[b,k]</styled-content></td></tr><tr><td align="left" valign="top"><italic>S</italic></td><td align="center" valign="top">=</td><td align="left" valign="top">tf.reduce_maximum(<italic>S</italic>,axis=-1)</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b]</styled-content></td></tr><tr><td align="left" valign="top">I</td><td align="center" valign="top">=</td><td align="left" valign="top">tf.argmax(<italic>S</italic>,axis=-1)</td><td align="left" valign="top"><styled-content style="color:#0000FF">[b]</styled-content></td></tr><tr style="border-top: solid thin; border-bottom: hidden"><td align="left" valign="top">OUT</td><td align="center" valign="top">:</td><td align="left" valign="top">indices (I) and scores (S) of the best matching peptide for each spectra in the mini-batch of size b</td><td align="center" valign="top"/></tr></tbody></table></table-wrap></floats-group></article>