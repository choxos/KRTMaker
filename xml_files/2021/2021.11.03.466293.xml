<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS156772</article-id><article-id pub-id-type="doi">10.1101/2021.11.03.466293</article-id><article-id pub-id-type="archive">PPR415525</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Visual category representations in the infant brain</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Siying</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Hoehl</surname><given-names>Stefanie</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Moeskops</surname><given-names>Merle</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kayhan</surname><given-names>Ezgi</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Kliesch</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Turtleton</surname><given-names>Bert</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Köster</surname><given-names>Moritz</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Cichy</surname><given-names>Radoslaw M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="fn" rid="FN1">10</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Education and Psychology, Freie Universität Berlin, Habelschwerdter Allee, Berlin 14195, Germany</aff><aff id="A2"><label>2</label>Faculty of Psychology, Department of Developmental and Educational Psychology, University of Vienna, Liebiggasse, Wien 1010, Austria</aff><aff id="A3"><label>3</label>Max Planck Institute for Human Cognitive and Brain Sciences, Stephanstraße, Leipzig 04103, Germany</aff><aff id="A4"><label>4</label>Department of Developmental Psychology, University of Potsdam, Karl-Liebknecht-Straße, Potsdam 14476, Germany</aff><aff id="A5"><label>5</label>Institute of Psychology, University of Regensburg, Sedanstraße, Regensburg 93055, Germany</aff><aff id="A6"><label>6</label>Berlin School of Mind and Brain, Humboldt-Universität zu Berlin, Unter den Linden, Berlin 10099, Germany</aff><aff id="A7"><label>7</label>Einstein Center for Neurosciences Berlin, Charité-Universitätsmedizin Berlin, Charitéplatz, Berlin 10117, Germany</aff><aff id="A8"><label>8</label>Bernstein Center for Computational Neuroscience Berlin, Humboldt-Universität zu Berlin, Unter den Linden, Berlin 10099, Germany</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author. <email>siying.xie@outlook.com</email> (SX); <email>rmcichy@zedat.fu-berlin.de</email> (RMC)</corresp><fn id="FN1"><label>10</label><p id="P1">Lead Contact</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>08</day><month>11</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>07</day><month>11</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P2">Visual categorization is a human core cognitive capacity <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup> that depends on the development of visual category representations in the infant brain <sup><xref ref-type="bibr" rid="R3">3</xref>–<xref ref-type="bibr" rid="R7">7</xref></sup> However, the exact nature of infant visual category representations and their relationship to the corresponding adult form remains unknown <sup><xref ref-type="bibr" rid="R8">8</xref></sup>. Our results clarify the nature of visual category representations from electroencephalography (EEG) data in 6- to 8-month-old infants and their developmental trajectory towards adult maturity in the key characteristics of temporal dynamics <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>, representational format <sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>, and spectral properties <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup> Temporal dynamics change from slowly emerging, developing representations in infants to quickly emerging, complex representations in adults. Despite those differences, infants and adults already partly share visual category representations. The format of infants’ representations is visual features of low to intermediate complexity, whereas adults’ representations also encode high complexity features. Theta band activity contributes to visual category representations in infants, and these representations are shifted to the alpha/beta band in adults. Together, we reveal the developmental neural basis of visual categorization in humans, show how information transmission channels change in development, and demonstrate the power of advanced multivariate analysis techniques in infant EEG research for theory building in developmental cognitive science.</p></abstract><kwd-group><kwd>Infant cognition</kwd><kwd>cognitive development</kwd><kwd>visual perception</kwd><kwd>object recognition</kwd><kwd>spectral characterization</kwd><kwd>multivariate analysis</kwd><kwd>deep learning</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="results | discussion"><title>Results &amp; Discussion</title><p id="P3">The ability to recognize and categorize visual objects effortlessly and within the blink of an eye is a core human cognitive capacity <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R2">2</xref></sup> that develops through learning and interaction with the environment. Behavioral research in infants using looking times <sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> and neural markers of attention provides evidence for visual category processing <sup><xref ref-type="bibr" rid="R16">16</xref></sup> and learning <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup> already within the first year of life.</p><p id="P4">In adults fundamental research in human and non-human primates has described the nature of the neural representations underlying mature visual categorization abilities, revealing their temporal dynamics <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup> what features they encode <sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>, their cortical locus <sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>, and how they relate to neural oscillations <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup> In contrast, these key characteristics of visual category representations <sup><xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R23">23</xref></sup> are less well understood in infants due to strong methodological challenges in human and non-human infant neuroimaging research <sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup> In particular, research using EEG – the workhorse of infant neuroimaging for decades – has yielded insights that are principally limited in two ways. One research approach focused on assessing the successful outcome of visual categorization rather than the underlying representations themselves <sup><xref ref-type="bibr" rid="R26">26</xref></sup>. Thus, the insights gained about representations are indirect. Another research approach did assess underlying representations directly but was limited to the category of faces <sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> for which known neural markers exist. Thus, the generalizability from the unique and small stimulus subset to the broad set of visual categories of the visual world remains unclear.</p><p id="P5">Here we overcome this double impasse to reveal the nature of general visual category representations for various object categories in 6- to 8-month-old infants from EEG data. We do so by leveraging an integrated multivariate analysis framework of multivariate classification <sup><xref ref-type="bibr" rid="R9">9</xref></sup> and direct quantitative comparison <sup><xref ref-type="bibr" rid="R28">28</xref></sup> of the infant to adult EEG data and deep learning models of vision <sup><xref ref-type="bibr" rid="R12">12</xref></sup>.</p><sec id="S2"><title>The temporal dynamics of visual category representations</title><p id="P6">Infant participants (<italic>n</italic> = 40) viewed 128 images of real-world objects from four categories (i.e., toys, bodies, houses and faces, see <xref ref-type="fig" rid="F1">Figure 1A</xref>; for rationale of category choice see <xref ref-type="sec" rid="S15">Method Details</xref>) while we acquired EEG data. The age group was chosen based on extensive work showing that by this age infants discriminate between basic level categories reliably <sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. Images were presented for 2s every 2.7–2.9s. For direct comparison we acquired EEG data in adult participants (<italic>n</italic> = 20) viewing the same stimulus set with an adapted experimental design (<xref ref-type="supplementary-material" rid="SD1">Figure S1A,B</xref>). We consider the epoch of –100ms to +1,000ms with respect to stimulus onset in our analyses.</p><p id="P7">To reveal the time course with which visual category is discriminated by visual representations we used time-resolved multivariate pattern analysis <sup><xref ref-type="bibr" rid="R9">9</xref></sup> (<xref ref-type="fig" rid="F1">Figure 1B</xref>). We report peak latency (95% confidence intervals (CIs) in brackets) as the time point during neural processing when category information was most explicit, as well as onset and offset of significance for each group.</p><p id="P8">In infants (<xref ref-type="fig" rid="F1">Figure 1C</xref>), the classification curve rose gradually from 100ms onwards, reaching significance at 252ms (250–254ms), followed by a broad peak at 416ms (268–462ms) and a gradual decline. This pattern of result did not depend on any particular object or category (<xref ref-type="supplementary-material" rid="SD1">Figure S1E,F</xref>), held equally for classifications within and across the animacy division (<xref ref-type="supplementary-material" rid="SD1">Figure S1G,H</xref>), and emerged equivalently for alternative common analysis schemes (<xref ref-type="supplementary-material" rid="SD1">Figure S1I–K</xref>). In contrast, in adults, the classification curve had a different shape (<xref ref-type="fig" rid="F1">Figure 1D</xref>). It emerged earlier (significant at 72ms (72–74ms)) and faster, peaking at 154ms (144–176ms) than in infants (P &lt; .001, bootstrap test, <xref ref-type="supplementary-material" rid="SD1">Table S1A</xref>). The observed delay is not only due to longer latencies already at the early cortical processing stages: the P1 component peak in infants was delayed by 22–68ms in infants (<xref ref-type="supplementary-material" rid="SD1">Figure S1C,D</xref>, <xref ref-type="supplementary-material" rid="SD1">Table S1F</xref>), consistent with previous studies <sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R34">34</xref></sup> Instead, the grand average ERP peak was much stronger, delayed by 98–242ms. This suggests that the observed peak latency differences with which category representations emerge reflect a mixture of processing delays at early and late processing stages.</p><p id="P9">Searchlight analysis in EEG channel space revealed that information about visual category representations was highest in EEG channels overlying occipitoparietal cortex in both infants (<xref ref-type="fig" rid="F1">Figure 1E</xref>) and adults (<xref ref-type="fig" rid="F1">Figure 1F</xref>), tentatively suggesting partly similar cortical sources in the posterior cortex.</p><p id="P10">This multivariate approach constitutes a novel analytical access point to visual category representations in infants from EEG data. Noteworthy, there is no simple mapping function of our results to the results of classical univariate results, as the approaches differ in many aspects. Univariate analyses focus on single electrodes or averages, while multivariate analyses focus on patterns across electrodes, potentially increasing sensitivity <sup><xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>. In adults, univariate and multivariate analyses also do not directly agree <sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup> However, the multivariate results carry meaning, as they can be meaningfully related to behavior <sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R38">38</xref></sup>. Further research combining multivariate analyses with behavioral measures <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> in infants is needed.</p><p id="P11">The rise and fall of the classification curves in a few hundred milliseconds might indicate rapid changes in the underlying visual representations <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup> slow ramping up of persistent representations or a combination of both. To investigate this, we assessed the temporal stability of visual representations using time-generalization analysis <sup><xref ref-type="bibr" rid="R39">39</xref></sup>. We determined how well classifiers trained on predicting visual category from EEG at one time point perform when tested at other time points. Lack of generalization across time indicates transience of the underlying visual representation, whereas generalization across time indicates persistence.</p><p id="P12">In both infants (<xref ref-type="fig" rid="F1">Figure 1G</xref>) and adults (<xref ref-type="fig" rid="F1">Figure 1H</xref>), classification accuracy was highest along the diagonal (i.e., similar time points for training and testing) with a broadening over time (white dotted ellipse). This result suggests common neural mechanisms of a rapid sequence of processing steps that result in an outcome held online for further use, indicated by rapidly changing transient representations at earlier time points and more slowly changing persistent representations at later time points, respectively.</p><p id="P13">In addition to this general similarity between infants and adults, two notable differences were indicative of incomplete development of feedforward and feedback information processing in infants. For one, early after stimulus onset, when neural processing is dominantly feedforward, in adults, we observed high classification accuracy trailing the diagonal narrowly (<xref ref-type="fig" rid="F1">Figure 1H</xref>, 50–200ms, dotted square), indicating rapid changing representations. Infants did not exhibit such signals. This pattern suggests incomplete development of feedforward visual information processing mechanisms in infants. Secondly, in adults, the classifier generalized well for the time point combination of 100–200ms and 200–1,000ms (<xref ref-type="fig" rid="F1">Figure 1H</xref>, white striped rectangle). This suggests highly persistent representations, likely emerging in the early visual cortex <sup><xref ref-type="bibr" rid="R9">9</xref></sup>. There were no such signals in infants. This indicates incomplete neural structures for recurrent processing that maintain visual information online for long stretches of time.</p><p id="P14">While the overall pattern of results did not depend on any particular category (<xref ref-type="supplementary-material" rid="SD1">Figure S1L,M</xref>), we cannot exclude that differences between age groups could also be due to differences in experimental task or signal-to-noise ratio (SNR). We boost SNR in adults compared to infants by design to increase the chance of identifying similarities at the cost of interpretative difficulties for differences. These difficulties are, however, alleviated by focusing on peaks as core measures for interpretation whose size, but not latency depends on SNR.</p></sec><sec id="S3"><title>Shared visual category representations between infants and adults</title><p id="P15">The results so far show that we identified visual category representations in both infants and adults and that their time courses have both similar and different aspects. However, we have not tested whether infants and adults have similar category representations. An alternative hypothesis is that we observe time courses of category classification for infants and adults, but those are unrelated rather than shared representations. Direct identification of shared representations between infants and adults is challenging due to differences in the time course over which the representations emerge and the EEG channel spaces differ. We used a time-generalization variant of representational similarity analysis (RSA) <sup><xref ref-type="bibr" rid="R28">28</xref></sup> (<xref ref-type="fig" rid="F2">Figure 2A</xref>) to overcome these hurdles. In short, we abstracted multivariate signals from the incommensurate infant and adult EEG channel spaces to a common representational dissimilarity space, and we compared the signals across all time point combinations.</p><p id="P16">We observed similarity in visual category representations between infants and adults at the time point combinations of 160–540ms in infants, and 100–1,000ms in adults (<xref ref-type="fig" rid="F2">Figure 2B</xref>, peak latency in infants: 200ms (200–360ms); in adults: 120ms (120–1,000ms)). This result was similarly achieved for alternative processing and data aggregation choices (<xref ref-type="supplementary-material" rid="SD1">Figure S2A,B</xref>), and did not depend on any particular category except on toys (<xref ref-type="supplementary-material" rid="SD1">Figure S2C</xref>). Our findings establish quantitatively and directly that infants and adults share visual category representations.</p><p id="P17">In sum, the emerging picture is one of not yet fully developed dynamics of adult-like visual category representations in infants. Representations in infants emerged later, slower, and lacked particular components of feedforward and recurrent processing, possibly related to immature myelination <sup><xref ref-type="bibr" rid="R40">40</xref></sup> and synaptic connectivity <sup><xref ref-type="bibr" rid="R41">41</xref></sup>. Nevertheless, representations in infants and adults shared large-scale temporal dynamics that encoded visual category information similarly, consistent with previous showing partly adult-like behavioral <sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R15">15</xref></sup> and neural <sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup> category sensitivity in the first year of age.</p><p id="P18">Our approach goes beyond previous EEG work in developmental visual neuroscience in three ways. First, rather than relying on indirect inference from attentional markers indicating successful categorization <sup><xref ref-type="bibr" rid="R26">26</xref></sup>, our approach assessed representations directly as they emerge with millisecond resolution. Second, our approach is not limited to the face category and face-specific EEG components <sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>, but allows the study of potentially any visual category. Third, our approach enabled a new quantitative comparison <sup><xref ref-type="bibr" rid="R28">28</xref></sup> of infant and adult visual category representations.</p><p id="P19">Our results make direct predictions for the detailed developmental trajectory of visual category representations <sup><xref ref-type="bibr" rid="R42">42</xref></sup>. We expect category representations to emerge increasingly earlier and with faster temporal dynamics with increasing age, with additional feedforward and feedback components appearing at critical stages until a mature adult-like system emerges. Our approach makes these predictions immediately testable in future studies using other age groups between early infancy and adulthood.</p><p id="P20">More broadly, our multivariate EEG analysis approach demonstrates a novel access point to largely unmapped neural representations in the infant brain, with strong potential to inform theories of cognitive development for cognitive capacities that emerge in the first year of life, such as object learning <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup> speech processing <sup><xref ref-type="bibr" rid="R43">43</xref></sup>, and core knowledge systems <sup><xref ref-type="bibr" rid="R44">44</xref></sup> Combined with human infant fMRI <sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup> and behavioral assessment <sup><xref ref-type="bibr" rid="R15">15</xref></sup> in a common framework <sup><xref ref-type="bibr" rid="R45">45</xref></sup>, this promises to reveal the unknown spatiotemporal neural dynamics underlying cognitive functions in infants in the future.</p></sec><sec id="S4"><title>The format of visual category representations</title><p id="P21">The time-resolved multivariate pattern analysis revealed the presence and dynamics of visual category representations in the infant and adult brain. However, by itself, it is unable to specify their format, i.e., what type of visual features they encode. We hypothesized that adults would encode visual features represented at all levels of the visual processing hierarchy from low- to high complexity <sup><xref ref-type="bibr" rid="R46">46</xref></sup>. Instead, infants would encode visual features rather of low- and mid-complexity, as predicted from visual behavior <sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup> and anatomical development patterns <sup><xref ref-type="bibr" rid="R42">42</xref></sup> of the infant visual brain.</p><p id="P22">To determine the format of category representations, we related them to computational models of vision (<xref ref-type="fig" rid="F3">Figure 3A</xref>). We probed two types of models: a Gabor wavelet pyramid model as a model of simple visual features <sup><xref ref-type="bibr" rid="R47">47</xref></sup>, and the deep neural network VGG-19 model <sup><xref ref-type="bibr" rid="R48">48</xref></sup> trained on object categorization, which exhibits a hierarchy of low-to-high complexity features along with its layers, and predicts activity along the visual processing hierarchy of the adult human brain well <sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup>.</p><p id="P23">Assessing first the Gabor model and an aggregated summary of the VGG model across layers, we found similar representations between both models and infant and adult visual representations (<xref ref-type="fig" rid="F3">Figure 3B</xref>). This suggested that features ranging from low to high complexity might contribute, and invited further in-depth analysis.</p><p id="P24">Turning to the VGG model first we conducted a finer investigation of VGG at the level of layers. Considering each layer separately, we found that in infants, middle layers predicted brain activity best, with layer groups 3 and 4 being significant (<xref ref-type="fig" rid="F3">Figure 3C</xref>, left). In contrast, layers at all stages were significantly predictive in adults (<xref ref-type="fig" rid="F3">Figure 3C</xref>, right). This pattern of results was also achieved for other types of deep neural network architectures (<xref ref-type="supplementary-material" rid="SD1">Figure S3A,B</xref>) and independent of data selection choices (<xref ref-type="supplementary-material" rid="SD1">Figure S3C,D</xref>), demonstrating the robustness of the result. This pattern of results suggests that infants and adults share similar visual features with the VGG model at intermediate complexity. We ascertained this in two ways. First, using partial correlation, we related the VGG model to each age group while partialling out the effect of the other age group. This abolished all effects in infants (<xref ref-type="fig" rid="F3">Figure 3D</xref>, left) while leaving the resulting pattern in adults unchanged (<xref ref-type="fig" rid="F3">Figure 3D</xref>, right), suggesting that the features underlying visual category representation in infants are a subset of the features in adults. Second, we conducted a variance partitioning analysis between the VGG model and infant and adult visual representations at layers 3 and 4, revealing shared variance (both R<sup>2</sup> = 0.16; <italic>P</italic> &lt; .05, FDR-corrected). This reveals that in infants category representations are in the format of low to intermediate complexity features and form a subset of the representations seen in adults, while in adults category is discriminated by features at all levels of complexity.</p><p id="P25">The prediction by the Gabor filter model and by the early layers of VGG in adults suggests that in both age groups, the category is discriminated by representations encoding features not only of low- and intermediate, but also low complexity, albeit to a different degree or in different ways. This is consistent with observations that low-level visual features are represented in high-level ventral visual cortex alongside features of higher complexity <sup><xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R52">52</xref></sup>, and that categories are systematically related to category through differences in spatial frequency content, thus support classification <sup><xref ref-type="bibr" rid="R53">53</xref></sup>.</p><p id="P26">We thus investigated the role of low-level features at different spatial frequencies in visual category representations in infants and adults. We filtered the stimulus material in spatial frequency in 100 bins spaced logarithmically between 0.1 and 30 cycle per degree (cpd) visual angle (for an example see <xref ref-type="fig" rid="F3">Figure 3E</xref>). As expected, visual object categories were associated with different spatial frequency content in the images (<xref ref-type="supplementary-material" rid="SD1">Figure S3E</xref>) that allows category to be determined directly from the images (<xref ref-type="supplementary-material" rid="SD1">Figure S3F</xref>). Using RSA, we assessed the similarity between category representations and the spatially filtered images. We observed a significant relationship across all spatial frequencies (except at 0.18 cpd) in both infants and adults (<xref ref-type="fig" rid="F3">Figure 3G</xref>), with stronger relationships for adults than infants above 1 cpd. This shows that category representations in infants and adults are differentiated by features across the spatial frequency spectrum, with a stronger role of higher spatial frequencies in adults.</p><p id="P27">Based on this result, we refined the deep neural network model based analysis with respect to spatial frequency. We compared the VGG model’s representation of the filtered images with infant and adult category representations (<xref ref-type="fig" rid="F3">Figure 3H</xref>). For adults, the result revealed similar representations across all spatial frequencies as expected, with a peak at 3.06–5.42 cpd. In contrast, for infants, the similarity was restricted to spatial frequencies from 0.31 to 1.73 cpd, with a peak at 0.55–0.98 cpd. This is consistent with the shift in peaks in spatial sensitivity from low spatial frequency up to 1 cpd <sup><xref ref-type="bibr" rid="R54">54</xref>,<xref ref-type="bibr" rid="R55">55</xref></sup> to higher spatial frequency at 2–6 cpd <sup><xref ref-type="bibr" rid="R56">56</xref></sup>. As expected from the previous analysis, we find significantly stronger correlations for higher spatial frequencies in adults than in infants.</p><p id="P28">Taken together, this reveals that in infants, category representations are in the format of low to intermediate complexity features at low spatial frequency and form a subset of the representations seen in adults. In contrast, in adults, category is discriminated by features at all levels of complexity and all spatial frequencies, with a higher reliance on high spatial frequencies.</p><p id="P29">Which processes may contribute to the emergence of high-complexity features in the developmental trajectory from infancy to adulthood? At this moment, we can only speculate. The results of the time-generalization analysis in conjunction suggest that local and far-reaching feedback processes from the frontal cortex might be involved <sup><xref ref-type="bibr" rid="R57">57</xref>–<xref ref-type="bibr" rid="R59">59</xref></sup>. In more cognitive terms, linguistic and semantic processing is known to modulate visual processing in adults and might modulate visual representations <sup><xref ref-type="bibr" rid="R60">60</xref></sup>.</p><p id="P30">Previous research investigating the format of infant visual category representations tested hypotheses one by one through experimental manipulation, for example, determining whether infants are sensitive to stimulus inversion <sup><xref ref-type="bibr" rid="R27">27</xref></sup> or tolerant to changes in viewing conditions <sup><xref ref-type="bibr" rid="R61">61</xref></sup>. Instead, our approach allows the comparison of any number of hypotheses as captured in explicit, image-computable computational models to capture infant visual category representations in the future increasingly accurately. To speed up this process, we make the data publicly available.</p><p id="P31">Our findings further suggest constraints for artificial intelligence research. The biological brain inspired the engineering of deep learning models, but the models’ learning has remained biologically unrealistic <sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup> and is perceived as a major impediment to building better models. We suggest that models of human visual categorization striving for increased biological realism should follow a similar developmental trajectory of representations as described here.</p></sec><sec id="S5"><title>Spectral properties of visual category representations</title><p id="P32">Neural oscillations underlie the formation and communication of visual representations <sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R63">63</xref>–<xref ref-type="bibr" rid="R65">65</xref></sup>. Here we determined the spectral signature of visual category representations in infants as a first step toward describing their relationship to neural oscillations. For this, we resolved EEG data in distinct frequency bins from 2 to 30Hz and performed time-resolved visual category classification on each bin separately (<xref ref-type="fig" rid="F4">Figure 4A</xref>). In infants (<xref ref-type="fig" rid="F4">Figure 4B</xref>), we observed significant category classification accuracy in a specific cluster in the theta band with a peak at 4.63Hz (2.91–6.73Hz) and 400ms (160–580ms). This result reveals activity in the theta band as the spectral signature of visual category representations in infants. In contrast, in adults (<xref ref-type="fig" rid="F4">Figure 4C</xref>), the cluster extended across the whole frequency range and time course investigated. It shows that the spectral signature of visual category representations in adults is broadband. The patterns of results did not depend on any particular category except on faces in infants (<xref ref-type="supplementary-material" rid="SD1">Figure S4E,F</xref>). Note that the observed differences in infants and adults are not a trivial consequence of differences in EEG power spectra pattern or category, as those were similarly broadband in both infants and adults and for all categories (<xref ref-type="supplementary-material" rid="SD1">Figure S4A,B</xref>). Further, the classification peaks do not map onto the power spectra peaks in terms of frequency and latency (<xref ref-type="supplementary-material" rid="SD1">Figure S4C,D</xref>). They are thus not a function of signal-to-noise ratio in the power spectrum. Finally, the difference between infants and adults is not due to higher inter-subject variability in spectral power patterns in adults, as different measures of variability were lower in adults than in infants (<xref ref-type="supplementary-material" rid="SD1">Table S4D</xref>).</p><p id="P33">The observed pattern of results is consistent with two alternative hypotheses about the relationship between the oscillatory basis of visual category representations in infants and adults. One hypothesis is that there is a direct match in frequency, suggesting that peak classification in infants and adults is at similar frequencies (i.e., at 4.63Hz and 5.59Hz, respectively). Another hypothesis is an upward shift across age, made plausible by the observations that brain rhythms increase in frequency during infant development <sup><xref ref-type="bibr" rid="R66">66</xref></sup>.</p><p id="P34">To arbitrate between those hypotheses, we determined which frequency band and time points category representations in adults were similar to infant category representations identified in the theta band (<xref ref-type="fig" rid="F4">Figure 4D</xref>). We extracted RDMs from the infant data at the cluster in the theta range. We used their average as a search template, comparing it to RMDs from the adult data for all time-point and frequency combinations. We found a cluster of significant correlations with a peak at 17.13Hz (9.78–20.65 Hz) at 120ms (120–360ms) (<xref ref-type="fig" rid="F4">Figure 4E</xref>). This pattern of results was partly independent of category (except faces, <xref ref-type="supplementary-material" rid="SD1">Figure S4G</xref>), held across different data aggregation schemes and ways to assess the theta cluster (<xref ref-type="supplementary-material" rid="SD1">Figure S4H</xref>). Further, there was no relationship between signals at the (non-significant) peak in infant alpha/beta at 100ms and adult signals at any time-frequency combination (<xref ref-type="supplementary-material" rid="SD1">Figure S4I,J</xref>). It directly and specifically quantitatively demonstrates an upward shift in the spectral signature of neural activity supporting visual category representations from the theta range in infants to the alpha/beta range in adults. As expected from the investigation of the representational format above, the shared representations which shifted relied on low spatial frequency features (<xref ref-type="supplementary-material" rid="SD1">Figure S3G,H</xref>).</p><p id="P35">The shift observed is not a trivial consequence of differences in the peak latency and frequency of the power spectrum, which are similar in infants and adults (<xref ref-type="supplementary-material" rid="SD1">Figure S4A–D</xref>). Closer inspection of the classification results (<xref ref-type="fig" rid="F4">Figure 4B,C</xref>) at peak latency reveals a similar peak around 4.63Hz–5.59Hz (<xref ref-type="fig" rid="F4">Figure 4B,C</xref>, line profiles), leaving open the possibility that to some degree the profile observed in infants might be a down-scaled and noisier version of the situation in adults, and predicting shared representations across age group in the theta band. Instead, the shared representations are present only in the alpha/beta band (<xref ref-type="fig" rid="F4">Figure 4E</xref>, line profile), demonstrating a clear dissociation from overall signal strength.</p><p id="P36">One interpretation of these findings is that in infants, neural networks for learning and memory associated with the theta rhythm contribute to the formation of category representations <sup><xref ref-type="bibr" rid="R67">67</xref>,<xref ref-type="bibr" rid="R68">68</xref></sup>, whereas in adults, equivalent category representations are processed quickly in fully developed semantic networks associated with the alpha/beta rhythms <sup><xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R70">70</xref></sup>. Alternatively, the frequency shift might be due to more efficient axonal transmission as a result of improved myelination <sup><xref ref-type="bibr" rid="R40">40</xref></sup> that may enable higher neural oscillations to emerge in the same neural circuits, consistent with increases in the prevalent frequency in the EEG across development <sup><xref ref-type="bibr" rid="R66">66</xref></sup>. On this account, our finding suggests a novel general developmental trajectory for neural communication channels in the human brain: specific information-processing mechanisms working at low frequency in infants are shifted up in frequency gradually across development, and the size of this shift depends on the differences in neural circuit myelination. However, we note that power in a frequency does by itself indicate oscillations in that frequency, and further research is needed to establish this firmly, e.g., by distinguishing aperiodic from periodic components <sup><xref ref-type="bibr" rid="R71">71</xref>,<xref ref-type="bibr" rid="R72">72</xref></sup>. Similarly, we do not observe a one-to-one mapping between shared representations revealed by time-resolved analysis (<xref ref-type="fig" rid="F2">Figure 2B</xref>) and time-frequency resolved analysis (<xref ref-type="fig" rid="F4">Figure 4E</xref>). Instead, we expect the relationship to be akin to the complex relationship between neural oscillations and evoked responses <sup><xref ref-type="bibr" rid="R73">73</xref>,<xref ref-type="bibr" rid="R74">74</xref></sup> Further research is needed to resolve to which degree they depend on distinct or shared neural phenomena.</p></sec><sec id="S6"><title>The nature and developmental trajectory of infant to adult visual category representations</title><p id="P37">In sum, our results reveal the nature and developmental trajectory of the infant to adult visual category representations, from infancy to adulthood. Temporal dynamics change from slowly to quickly emerging in time, the format from visual features up to intermediate complexity to features of high complexity, and the oscillatory signature from the theta to the alpha/beta frequency. These results provide insight into visual category representations that underlie the development of fast and efficient visual categorizations skills in humans. They also further reveal how cortical information transmission channels change in human development and demonstrate the power of advanced multivariate analysis techniques in infant EEG research for developmental cognitive science.</p></sec></sec><sec sec-type="methods" id="S7" specific-use="web-only"><title>Star methods text</title><sec id="S8"><title>Key resources table</title><table-wrap id="T1" position="anchor" orientation="portrait"><table frame="box" rules="all"><thead><tr><th align="left">REAGENT or RESOURCE</th><th align="left">SOURCE</th><th align="left">IDENTIFIER</th></tr></thead><tbody><tr><td align="left" colspan="2" valign="top"><bold>Deposited Data</bold></td></tr><tr><td align="left" valign="top">Raw and analyzed data</td><td align="left" valign="top">This paper</td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://osf.io/ruxfg/">https://osf.io/ruxfg/</ext-link></td></tr><tr><td align="left" colspan="2" valign="top"><bold>Software and Algorithms</bold></td></tr><tr><td align="left" valign="top">Customized code</td><td align="left" valign="top">This paper</td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/siyingxie/VCR_infant">https://github.com/siyingxie/VCR_infant</ext-link></td></tr><tr><td align="left" valign="top">MATLAB</td><td align="left" valign="top">Mathworks Inc.:<break/><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/">https://www.mathworks.com/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/matlab.html;">https://www.mathworks.com/products/matlab.html;</ext-link><break/>RRID:SCR_001622</td></tr><tr><td align="left" valign="top">Psychtoolbox-3</td><td align="left" valign="top">Psychtoolbox:<break/><ext-link ext-link-type="uri" xlink:href="https://www.psychtoolbox.net">https://www.psychtoolbox.net/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link>;<break/>RRID:SCR_002881</td></tr><tr><td align="left" valign="top">Fieldtrip Toolbox</td><td align="left" valign="top">FieldTrip:<break/><ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/">https://www.fieldtriptoolbox.org/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.fieldtriptoolbox.org/">https://www.fieldtriptoolbox.org/</ext-link>;<break/>RRID:SCR_004849</td></tr><tr><td align="left" valign="top">Brainstorm3</td><td align="left" valign="top">Brainstorm:<break/><ext-link ext-link-type="uri" xlink:href="https://neuroimage.usc.edu/brainstorm/">https://neuroimage.usc.edu/brainstorm/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://neuroimage.usc.edu/brainstorm/">https://neuroimage.usc.edu/brainstorm/</ext-link>;<break/>RRID:SCR_001761</td></tr><tr><td align="left" valign="top">LIBSVM: A library for Support Vector Machines</td><td align="left" valign="top">LIBSVM:<break/><ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/">https://www.csie.ntu.edu.tw/~cjlin/libsvm/</ext-link>;<break/>RRID:SCR_010243</td></tr><tr><td align="left" valign="top">MatConvNet: CNNs for MATLAB</td><td align="left" valign="top">MatConvNet:<break/><ext-link ext-link-type="uri" xlink:href="https://www.vlfeat.org/matconvnet/">https://www.vlfeat.org/matconvnet/</ext-link></td><td align="left" valign="top"><ext-link ext-link-type="uri" xlink:href="https://github.com/vlfeat/matconvnet">https://github.com/vlfeat/matconvnet</ext-link></td></tr></tbody></table></table-wrap></sec><sec id="S9"><title>Resource availability</title><sec id="S10"><title>Lead contact</title><p id="P38">Further information and requests for the resources should be directed to and will be fulfilled by the lead contact, Radoslaw M. Cichy (<email>rmcichy@zedat.fu-berlin.de</email>).</p></sec><sec id="S11"><title>Materials availability</title><p id="P39">This study did not generate new unique reagents.</p></sec></sec><sec id="S12"><title>Experimental model and subject details</title><p id="P40">Two independent pools of participants took part in this study: 6–8 months old infants and young adults. We chose this infant age group based on extensive evidence from behavioral and electrophysiological work showing that infants discriminate between various basic level visual categories by this age, whereas in younger infants, category discrimination is less stable and relies more on the chosen paradigm and the specific categories assessed <sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. The infant sample was assessed at the Max Planck Institute for Human Cognitive and Brain Sciences in Leipzig, Germany. It comprised 48 participants, of which 8 were excluded due to insufficient data, yielding a final sample analyzed of 40 infant participants (gender: 19 female, age: mean ± SD: 214.9 ± 14.76 days). The adult sample was assessed at the Freie Universität Berlin, Germany. It comprised 20 participants of which none was excluded (gender: 11 female, age: mean ± SD: 26.1 ± 3.81 years). Caregivers of all infants and all adult participants gave written informed consent. The study was conducted according to the Declaration of Helsinki and the infant and adult protocols were approved by the respective local ethic committees.</p></sec><sec id="S13"><title>Method details</title><sec id="S14"><title>Stimuli</title><p id="P41">The stimulus set consisted of 32 object images in each of the four categories included: houses, toys, faces, and bodies. We chose those categories for four reasons: (1) they are highly familiar to infants, and infants encounter them in everyday life; (2) faces <sup><xref ref-type="bibr" rid="R75">75</xref>–<xref ref-type="bibr" rid="R78">78</xref></sup>, toys <sup><xref ref-type="bibr" rid="R3">3</xref></sup>, bodies <sup><xref ref-type="bibr" rid="R79">79</xref>,<xref ref-type="bibr" rid="R80">80</xref></sup>, and houses <sup><xref ref-type="bibr" rid="R81">81</xref>,<xref ref-type="bibr" rid="R82">82</xref></sup> (as large objects that define scenes) have been used in previous infant research making our results in principle comparable; (3) they have well-described and distinct neural signatures in adults <sup><xref ref-type="bibr" rid="R83">83</xref></sup>; and (4) a recent fMRI study showed distinct neural signatures for faces, objects, and scenes in infants, too <sup><xref ref-type="bibr" rid="R20">20</xref></sup>. This yielded a total set of 4 × 32 = 128 object images. All object images were cut-out from color photographs. All images are available in the <bold>OSF repository</bold>. We analyzed the data at the level of category.</p></sec><sec id="S15"><title>Experimental procedure in the infant sample</title><p id="P42">In the infant experiment, participants were presented with 272 trials divided into four blocks. Each block had the same basic structure. At the beginning of each block four stimuli (one stimulus per object category) were separately presented three times in randomized order. Thereafter participants were presented with a random sequence of images comprising the same four stimuli seven more times, intermixed with 28 other images (seven images per category) presented only once. This experimental design was chosen because it allows assessing the effect of object image repetition of infant brain responses, but this question is orthogonal to the ones pursued here and will be reported separately.</p><p id="P43">Each trial consisted of a fixation dot presented for a variable duration of 700–900ms, followed by a stimulus presented for 2,000ms at the center of the screen (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). To capture the attention of the infants and direct their gaze to the screen we implemented two measures. First, we presented a yellow duck image and duck sound for 1,000ms at the beginning of each block and thereafter every 10 trials. Second, each stimulus was presented together with one of ten arbitrary sounds that were assigned randomly at each trial.</p><p id="P44">During the assessment, infants sat on their care giver’s lap at a viewing distance of about 80 cm from a 17-in. CRT screen. The object images were presented at the center of the screen, subtending a visual angle of approximately 5.0°. To monitor infants’ gaze, we recorded videos of infants’ faces throughout the experiment.</p></sec><sec id="S16"><title>Experimental procedure in the adult sample</title><p id="P45">We adapted the experimental design for the adult sample. In short, all images were shown equally often, with higher number of repetitions, at shorter presentation times and higher presentation rates that in the infant study (<xref ref-type="supplementary-material" rid="SD1">Figure S1B</xref>).</p><p id="P46">The first 3 participants were presented with 1,280 trials divided into 5 runs. In each run each object image was presented twice. The other 17 participants were presented with 3,840 trials divided into 10 runs. In each run each object image was presented three times. In each run, images were presented in random order, and runs were separated by breaks that were self-paced by the participants.</p><p id="P47">Each trial consisted of the presentation of a fixation cross with a variable duration of 600–800ms, followed by a stimulus presentation for 500ms. Stimuli were presented at the center of the screen at a visual angle of approximately 7.0°.</p><p id="P48">Participants were instructed to keep fixation on the center of the screen throughout the experiment. To ensure that participants attended to the stimuli and to avoid contamination of the relevant recording times with blink artefacts, participants were instructed to press a button and blink their eyes in response to a paper clip image that was shown randomly every 4 to 6 trials (average 5 trials). Paper clip trials were excluded from all further analysis.</p></sec><sec id="S17"><title>EEG acquisition and preprocessing</title><sec id="S18"><title>Infant sample</title><p id="P49">EEG data for the infant sample were recorded in a shielded room using 30 Ag/AgCl ring electrodes and a TMSi 32-channel REFA amplifier at a sampling rate of 500 Hz. Electrodes were placed according to the standard 10-20 system. Electrodes V+Fp2 and V– recorded the vertical electrooculogram (VEOG), and electrodes H-F9 and HF+10 recorded the horizontal electrooculogram (HEOG), Cz served as the online reference. We conducted preprocessing using the Fieldtrip toolbox <sup><xref ref-type="bibr" rid="R84">84</xref></sup>. The continuous EEG data was segmented for each trial into epochs. For subsequent time-resolved multivariate analysis we extracted the epoch from –200ms to +1,000ms with respect to image onset. For analysis that was additionally resolved in frequency we used longer epochs to allow better estimation at lower frequencies from –500ms to +1,000ms.</p><p id="P50">We removed all trials during which participants did not gaze at the screen for 1,000ms after stimulus onset as assessed by visual inspection of the video recordings. In this reduced trial set (mean ± SD: 139.6 ± 47.76 trials) we removed noisy channels (mean ± SD: 1.25 ± 1.32) and replaced them by interpolated data from adjacent electrodes. We further conducted independent component analysis (ICA) and removed components related to eye-movement and muscle artifacts as identified by visual inspection.</p></sec><sec id="S19"><title>Adult sample</title><p id="P51">EEG data for the adult sample were recorded using an EASYCAP 64-channel system and a Brainvision actiCHamp amplifier at a sampling rate of 1,000Hz. Data were filtered online between 0.3 and 100 Hz. Electrodes were placed according to the standard 10-10 system. Electrode Fz served as the online reference. We conducted preprocessing using the Brainstorm 3 toolbox <sup><xref ref-type="bibr" rid="R85">85</xref></sup>. Up to two noisy channels were removed for each participant as identified by visual inspection. We conducted ICA to identify and remove eye-movement and muscle artifact components by visual inspection of independent components. The continuous EEG data were then segmented for each trial into epochs from –200ms to +1,000ms (for time-resolved analysis) and from –500ms to +1,000ms (for time- and frequency resolved analysis).</p></sec></sec><sec id="S20"><title>EEG time-frequency decomposition</title><p id="P52">We decomposed the EEG time series into frequency-specific components by convolving the data with complex <italic>Morlet</italic> wavelets separately for each trial and sensor. We performed decomposition based on single trials so that the decomposed activity reflects stimulus-locked evoked responses and induced responses <sup><xref ref-type="bibr" rid="R73">73</xref></sup>. The wavelets had a constant length of 2,600ms and were logarithmically spaced in 30 frequency bins between 2Hz and 30Hz. We obtained the absolute power values for each time point and frequency bin by taking the square root of the resulting time-frequency coefficients. We normalized these power values to reflect relative changes (expressed in dB) with respect to the pre-stimulus baseline (–300ms to –100ms with respect to stimulus onset). We downsampled the time-frequency representations to a temporal resolution of 50 Hz (by averaging data in 20ms-bins) to increase the signal-to-noise ratio of subsequent analyses. This yielded for each trial a power value for each time point and frequency bin.</p></sec><sec id="S21"><title>Multivariate classification of visual category from EEG data</title><p id="P53">To characterize the temporal dynamics with which visual category representations emerge in infant and adult brains we conducted multivariate EEG classification using linear support vector machines (SVMs). We analyzed the infant and the adult data set separately and equivalently.</p><p id="P54">We conducted two common variants of multivariate EEG classification: time-resolved EEG analysis <sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup> and time-generalization analysis <sup><xref ref-type="bibr" rid="R39">39</xref></sup>. We conducted the analysis separately on the adult and infant sample, and separately for each participant. All analyses employed binary c-support vector classification (C-SVC) with a linear kernel as implemented in the LIBSVM toolbox <sup><xref ref-type="bibr" rid="R86">86</xref></sup>. The details of the time-resolved and the time-generalization analysis are as follows.</p><sec id="S22"><title>Time-resolved classification</title><p id="P55">We used time-resolved multivariate pattern analysis on EEG data (<xref ref-type="fig" rid="F1">Figure 1B</xref>) to determine the time course with which visual category representations emerge in infant and adult brains. For each time point of the EEG epoch (from –200ms to +1,000ms), we extracted trial-specific EEG channel activations (i.e., 25 in infants and 63 in adults) and arranged them into pattern vectors for each of the four category conditions (i.e., face, house, body, and toy) of the stimulus set. To increase the signal-to-noise ratio (SNR), we randomly assigned raw trials into four bins of approximately equal size each and averaged them into four pseudo-trials. We used a leave-one-pseudo-trial-out cross validated classification approach. We trained the SVM classifier to pairwise decode any two conditions using three of the four pseudo-trials for training. We used the fourth left-out pseudo-trial for testing, yielding classification accuracy (chance level 50%) as a result. The procedure was repeated 100 times, each time with a new random assignment of trials to pseudo-trials. The resulting decoding accuracy was averaged across repetitions and assigned to a decoding accuracy matrix of size 4 × 4, with rows and columns indexed by the conditions classified. The matrix is symmetric across the diagonal, with the diagonal undefined. This procedure yielded one decoding matrix for every time point.</p></sec><sec id="S23"><title>Time-frequency resolved classification</title><p id="P56">In addition to classifying visual category from broadband responses (i.e., single trial raw unfiltered waveforms), we classified object categories from oscillatory responses. This analysis followed the same rationale as the classification analysis described above, with the only difference that classification was conducted on power value patterns instead of raw activation value patterns. The analysis was conducted separately for each frequency bin separately. This resulted in a decoding accuracy matrix of size 4 × 4 as defined above for every time point and every frequency bin.</p></sec><sec id="S24"><title>Time generalization analysis</title><p id="P57">We used time-generalization classification analysis <sup><xref ref-type="bibr" rid="R39">39</xref></sup> to determine how visual representations emerging at different time points during the dynamics of visual perception relate to each other. For time and memory efficiency, we down-sampled the EEG data to a sampling rate of 50 Hz by averaging the raw EEG data in 20ms bins. The procedure was equivalent to the time-resolved classification analysis with the only difference that classifiers trained on data from a particular time point were not only tested on left out data from the same time point, but iteratively on data from the same and all other time points. The idea is that successful classifier generalization across time points indicates similarity of visual representations over time. This analysis yielded thus a size 4 × 4 decoding accuracy matrix indexed in rows and columns by the conditions compared for all time point combinations from –200 to +1,000ms. We averaged the entries of the decoding accuracy matrix at each time point, yielding a temporal generalization matrix indexed in rows and columns by training and testing time.</p></sec><sec id="S25"><title>Sensor-space searchlight analysis</title><p id="P58">We performed a sensor-space searchlight analysis <sup><xref ref-type="bibr" rid="R87">87</xref>,<xref ref-type="bibr" rid="R88">88</xref></sup> to localize in EEG channel space which channels contributed to the classification of category. For each EEG channel we defined a neighborhood as a sphere of the 10 (for adults) or 5 (for infants) closest EEG channels. For each EEG channel we then performed time-resolved category classification analysis, limiting data entering the analysis to its neighboring channels. Averaging across all pairwise category classifications yielded one decoding accuracy for each time point and for each EEG channel. We further averaged the results in 200ms bins, yielding a single EEG channel searchlight map of grand average decoding accuracy for each time bin.</p></sec></sec><sec id="S26"><title>Computing spatial-frequency specific versions of the stimulus set</title><p id="P59">To assess the role of spatial frequency on visual object categorizations, we decomposed the stimulus set in terms of spatial frequency. For this, we first used the Fourier transform to transform each image into the frequency domain. We then defined a set of 100 Butterworth band-pass filters (complex higher order filters with a roll-off response rate of 5) logarithmically spaced between 0.1 and 30 cycle per degree (cpd) visual angle. We applied each band-pass filter to the frequency representation of each image, yielding 100 band-pass filtered versions of each image in the frequency domain. We combined the resulting power values of each image in the frequency domain together with the image’s original phase information to compute the corresponding frequency-filtered images using the inverse Fourier transform. This procedure resulted in 100 sets of the stimulus set, band-pass filtered between 0.1 and 30 cpd.</p></sec><sec id="S27"><title>Comparing visual representations in infants and adults</title><p id="P60">We determine whether infants and adults have similar visual category representations using representational similarity analysis (RSA) <sup><xref ref-type="bibr" rid="R89">89</xref>,<xref ref-type="bibr" rid="R90">90</xref></sup>. The idea is that infants and adults share representations of category if they treat the same categories as similar or dissimilar. We determined this in a two-step process. In a first step, for each age group independently condition-specific multivariate activity patterns (adults: 63 electrodes; infants: 25 electrodes) were compared for dissimilarity. Dissimilarity was determined for all pairwise combinations of conditions, and dissimilarity values were aggregated in so-called representational dissimilarity matrices (RDMs) indexed in rows and columns by the conditions compared (here: 4 × 4 RDMs indexed by the 4 object categories). RDMs thus provide a statistical summary of the similarity and thus representational relations between visual category representations. The RDMs gained from the infant and adult sensor space separately have the same definition and dimensionality and are thus directly comparable. Thus, in a second step, the infant RDM and the adult RDMs are related to each other by determining their similarity.</p><p id="P61">We applied RSA to two different types of data: evoked responses (i.e., recorded voltage signals) and oscillatory responses (i.e., spectral power). In both cases we re-used the results of the classification analysis described above for the definition of RDMs. Classification accuracy can be interpreted as a dissimilarity measure on the assumption that the more dissimilar activation patterns are for two conditions, the easier they are to classify <sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R91">91</xref></sup>. We detail the different RSA procedures below. To reduce visual complexity of the analysis we subsampled the results of the classification analysis by binning them in 10ms bins.</p><sec id="S28"><title>Relating visual category representations in infants and adults based on raw broadband time courses</title><p id="P62">We investigated whether infants and adults share common visual representations based on broadband responses. As visual representations in adults and infants likely emerge with different time courses, we related their visual representations in a representational similarity time-generalization analysis. As RDMs we used time-point specific decoding accuracy matrices (<xref ref-type="fig" rid="F2">Figure 2A</xref>). We first averaged infant RDMs across all participants to increase SNR, resulting in one average infant RDM per time point. We then correlated (Spearman’s <italic>R</italic>) the average infant RDM to each adult (<italic>n</italic> = 20) RDM across all time point combinations. This yielded 20 correlation matrices, indexed in rows and columns by the time points compared (rows: infant time; columns: adult time), indicating when infants and adults share category representations.</p></sec><sec id="S29"><title>Relating visual category representations in infants and adults based on frequency-specific power time courses</title><p id="P63">We investigated whether infants and adults share visual representations in particular frequency bands. As RDMs we used decoding accuracy matrices from the classification analysis based on time-frequency resolved power values. As in this analysis we could neither assume similar time courses, nor similar roles for particular frequencies across infants and adults, we related infant and adult representations in a time-and-frequency-generalization analysis. To do this, we first defined a single aggregate infant RDM by averaging decoding accuracy matrices based on the extent (time and frequency) of the significant cluster in the infant data (<xref ref-type="fig" rid="F4">Figure 4B</xref>) alone. To increase signal-to-noise we only included RDMs whose average across entries in single participants was greater than or equal to 50% decoding accuracy. Note that this criterion is orthogonal to the hypotheses tested and thus does not bias the analysis. We compared (Spearman’s <italic>R</italic>) this single aggregate infant RDM to time- and frequency- resolved RDMs for each participant of the adult sample (<italic>n</italic> = 20), separately for each frequency and time point. This yielded 20 correlation matrices, with rows representing time points and columns representing frequency bins, indicating when and at which frequency infants and adults share category representations.</p></sec><sec id="S30"><title>Relating visual representations in infants and adults to computational models</title><p id="P64">To characterize the format of visual category we related neural representations in infants and results to different computational models using RSA. We constructed model RDMs from computational models (<xref ref-type="fig" rid="F3">Figure 3A</xref>) that represent visual information in different formats. We considered two types of visual computational models: a Gabor wavelet pyramid as a model of low-level feature representations <sup><xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R92">92</xref></sup>, and the VGG-19 <sup><xref ref-type="bibr" rid="R48">48</xref></sup> deep convolutional neural network (DNN) trained to categorize object images. Deep neural networks process visual information along a hierarchy of increasing complexity from low to high <sup><xref ref-type="bibr" rid="R93">93</xref>,<xref ref-type="bibr" rid="R94">94</xref></sup> that has been shown to match the processing hierarchy of the human brain <sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R95">95</xref>,<xref ref-type="bibr" rid="R96">96</xref></sup> and predict human and non-human primate brain activity better than other model class <sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R97">97</xref>–<xref ref-type="bibr" rid="R99">99</xref></sup>.</p><p id="P65">To construct model RDMs we first ran all visual stimuli in the study (i.e., 128 object images) through the models and extracted their activation values. More specifically, for the Gabor model we extracted a single set of model responses for Gabor wavelets differing in size, position, orientation, spatial frequency and phase. For the DNN we used the MatConvNet toolbox <sup><xref ref-type="bibr" rid="R100">100</xref></sup> to extract model neuron activation values from the rectified linear units (Relu) for each layer. We z-transformed activation values across stimuli for each stage/layer separately and averaged the transformed values across the 32 stimuli belonging to each of the four categories (i.e., face, body, house and toy), resulting in four category-specific activation values. We formed the patterns into vectors and computed the dissimilarity (1 – Pearson’s <italic>R</italic>) between all pairwise combinations of the four category activation vectors, resulting in a 4 × 4 RDM for each DNN layer of each DNN separately, and one model RDM for the Gabor feature model.</p><p id="P66">To construct spatial frequency-specific DNN model RDMs, we used an equivalent procedure with the difference that we ran band-pass filtered images through the DNN model separately for each band-pass defined. This resulted in 4 × 4 RDM for each spatial frequency band and DNN layer of the DNN separately.</p><p id="P67">To construct spatial-frequency specific image-based RDMs, we did not run the images through a model, but the procedure was otherwise equivalent. We directly averaged the pixel values of the filtered images across the 32 stimuli belonging to each of the four categories (i.e., face, body, house, and toy), resulting in four category-specific activation values. We formed the patterns into vectors and computed the dissimilarity (1 – Pearson’s <italic>R</italic>) between all four category activation vectors pairwise combinations, resulting in a 4 × 4 RDM for each frequency band.</p><p id="P68">To construct neural RDMs that capture category representations well we averaged decoding accuracy matrices from time-resolved category classification (<xref ref-type="fig" rid="F1">Figure 1C,D</xref>) in the 95% confidence intervals around peak latency in time-resolved category classification. Our rationale was that peak latency is the time point when categories were linearly best separable and thus their representations most explicit <sup><xref ref-type="bibr" rid="R101">101</xref></sup>. To increase signal-to-noise we only included RDMs whose average across entries in single participants was greater than or equal to 50% decoding accuracy. Note that this criterion is orthogonal to the hypotheses tested and thus unbiased. This yielded a single neural RDM for every infant and adult participant. We then related infant and adult neural RDMs to model RDMs using Spearman’s <italic>R</italic>, yielding a single correlation value for each model RDM and participant.</p><p id="P69">To allow assessing the models’ predictivity with respect to the noise in the data we calculated an upper and lower bound for the noise ceiling <sup><xref ref-type="bibr" rid="R102">102</xref></sup>, that is the predictions a perfect model may reach given the noise in the data. This procedure was conducted separately for the infant and the adult sample. To estimate the upper bound we correlated (Spearman’s <italic>R</italic>) each participant’s neural RDM with the mean neural RDM across all participants. To estimate the lower bound we correlated (Spearman’s <italic>R</italic>) each participant’s neural RDM with the mean neural RDM excluding that participant iteratively for all participants. We averaged the results, yielding estimates of the lower and upper noise ceiling for infants and adults.</p><p id="P70">To reveal whether infants, adults, and the DNN share common representations, we applied variance partitioning using a general linear model (GLM). The procedure was as follows. We first computed two GLMs between the DNN model RDM (i.e., observation) and the infant and adult RDMs (i.e., main regressor), respectively. This revealed the total variance that the model and each age group shared. We then computed two additional GLMs, adding the other age group’s average RDM to the model (i.e., there are two main regressors). From the additional RDMs, we obtained the unique variance of infant and adult RDMs, which was the difference in explained variance after infant and adult RDMs were reduced from the models. From the results of those two types of GLMs, we computed the shared variance that resulted from subtracting the unique variance from the total variance. We applied this analysis for network layers to which infants and adults showed a significant relationship in the correlation analysis, which are layers 3 and 4.</p></sec></sec></sec><sec id="S31"><title>Quantification and statistical analysis</title><p id="P71">We used non-parametric statistical inference for random-effects inference to avoid assumptions about the distribution of the data <sup><xref ref-type="bibr" rid="R103">103</xref>,<xref ref-type="bibr" rid="R104">104</xref></sup>. We used permutation tests for cluster-size inference, and bootstrap tests for confidence intervals on maxima, cluster onset/offset, and peak-to-peak latency differences. The sample size (<italic>n</italic>) for infants was 40 and for adults 20. Tests were either two- or right-tailed and are indicated for each result separately.</p><sec id="S32"><title>Permutation tests</title><p id="P72">We tested the statistic of interest (i.e., mean decoding accuracy or correlation coefficient in RSA across participants) using sign permutation tests. The null hypothesis was that the statistic of interest was equal to chance (i.e., 50% decoding accuracy, a Spearman’s <italic>R</italic> of 0). Under the null hypothesis, we could permute the category labels of the EEG data, which effectively corresponds to a sign permutation test that randomly multiplies participant-specific data with +1 or –1. For each permutation sample, we recomputed the statistic of interest. Repeating this permutation procedure 10,000 times, we obtained an empirical distribution of the data. We converted the original statistic (i.e., correlation coefficient, the decoding time courses, time-time matrices of correlation coefficients or decoding accuracies, and time-frequency decoding matrices) into <italic>P</italic>-values (correlation coefficients), 1-dimensional (time courses), or 2-dimensional (time-generalization or time-frequency) <italic>P</italic>-value matrices.</p><p id="P73">We controlled the family-wise error rate using cluster-size inference. We first thresholded <italic>P</italic>-value time courses or maps at <italic>P</italic> &lt; .005 (cluster-definition threshold) to define supra-threshold clusters by contiguity. These supra-threshold clusters were reported significant only if the size exceeded a threshold, estimated as follows: the previously computed permutation samples were also converted to <italic>P</italic>-value time courses/matrices and also thresholded to define resampled versions of supra-threshold clusters. These clusters were used to construct an empirical distribution of maximum cluster size and estimate a threshold of 5% of the right tail of this distribution (i.e., the corrected <italic>P</italic>-values is <italic>P</italic> &lt; .05).</p></sec><sec id="S33"><title>Bootstrap tests</title><p id="P74">We calculated 95% confidence intervals for the onsets of the first significant cluster and the peak latency of the observed effects. To achieve this, we created 1,000 bootstrapped samples by sampling the participants with replacement. For each bootstrap sample, we determined the peak latency as well as onsets of the first significant cluster and the offset of the last significant cluster. This resulted in empirical distributions of peak, onset and offset latencies on which we determined 95% confidence intervals.</p><p id="P75">To calculate confidence intervals on mean peak-to-peak latency differences, we created 1,000 bootstrapped samples by sampling the participant-specific latencies with replacement. This yielded an empirical distribution of mean peak-to-peak latencies. If the 95% confidence interval did not include 0, we rejected the null hypothesis of no peak-to-peak latency differences. The threshold <italic>P</italic> &lt; .05 was corrected for multiple comparisons whenever appropriate using FDR correction.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental information</label><media xlink:href="EMS156772-supplement-Supplemental_information.pdf" mimetype="application" mime-subtype="pdf" id="d151aAdCbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S34"><title>Acknowledgments</title><p>R.M.C is supported by Deutsche Forschungsgemeinschaft (DFG) grants (CI241/1-1, CI241/3-1, CI241/7-1) and by a European Research Council Starting Grant (ERC-2018-StG). S.X. is supported by a scholarship by the Chinese Scholarship Council. M.K. and S.H. were supported by the DFG and FWF jointly (grant numbers: KO 6028/1-1; I 4332-B), and S.H. also by the Max Planck Society. Computing resources were provided by the high-performance computing facilities at ZEDAT, Freie Universitaet Berlin.</p></ack><sec id="S35" sec-type="data-availability"><title>Data and code availability</title><p id="P76">Raw and processed data have been deposited at <italic>OSF</italic> and are publicly available as of the date of publication. DOI is listed in the key resources table.</p><p id="P77">All customized codes have been deposited at <italic>GitHub</italic> and are publicly available as of the date of publication. DOI is listed in the key resources table.</p></sec><fn-group><fn id="FN2" fn-type="con"><p id="P78"><bold>Author Contributions</bold></p><p id="P79">Conceptualization: RC, SX, MK, SH, BT. Methodology: SX, RC, MK. Software: SX, MK. Formal analysis: SX, MM. Investigation: EK, CK, Resources: SX, RC, MK. Data curation: SX. Writing - original draft preparation: SX, RC. Writing - review and editing: RC, SX, MK, SH, BT. Visualization: SX. Supervision: RC, MK. Project administrations: RC, SH, MK. Funding acquisition: RC, SH and MK.</p></fn><fn id="FN3" fn-type="conflict"><p id="P80"><bold>Declaration of Interests</bold></p><p id="P81">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potter</surname><given-names>M</given-names></name></person-group><article-title>Meaning in visual search</article-title><source>Science</source><year>1975</year><volume>187</volume><fpage>965</fpage><lpage>966</lpage><pub-id pub-id-type="doi">10.1126/science.1145183</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorpe</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Marlot</surname><given-names>C</given-names></name></person-group><article-title>Speed of processing in the human visual system</article-title><source>Nature</source><year>1996</year><volume>381</volume><fpage>520</fpage><lpage>522</lpage><pub-id pub-id-type="doi">10.1038/381520a0</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><article-title>Brain activity differentiates face and object processing in 6-month-old infants</article-title><source>Developmental Psychology</source><year>1999</year><volume>35</volume><fpage>1113</fpage><lpage>1121</lpage><pub-id pub-id-type="doi">10.1037/0012-1649.35.4.1113</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maurer</surname><given-names>D</given-names></name><name><surname>Lewis</surname><given-names>TL</given-names></name><name><surname>Brent</surname><given-names>HP</given-names></name><name><surname>Levin</surname><given-names>AV</given-names></name></person-group><article-title>Rapid Improvement in the Acuity of Infants After Visual Input</article-title><source>Science</source><year>1999</year><volume>286</volume><fpage>108</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1126/science.286.5437.108</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mareschal</surname><given-names>D</given-names></name><name><surname>Quinn</surname><given-names>PC</given-names></name></person-group><article-title>Categorization in infancy</article-title><source>Trends in Cognitive Sciences</source><year>2001</year><volume>5</volume><fpage>443</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01752-6</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascalis</surname><given-names>O</given-names></name><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><article-title>Is Face Processing Species-Specific During the First Year of Life?</article-title><source>Science</source><volume>296</volume><fpage>1321</fpage><lpage>1323</lpage><pub-id pub-id-type="doi">10.1126/science.1070223</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><article-title>What’s in a look?</article-title><source>Dev Sci</source><year>2007</year><volume>10</volume><fpage>48</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1111/J.1467-7687.2007.00563.X</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aslin</surname><given-names>RN</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><article-title>Methodological challenges for understanding cognitive development in infants</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><fpage>92</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2005.01.003</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Resolving human object recognition in space and time</article-title><source>Nat Neurosci</source><year>2014</year><volume>17</volume><fpage>455</fpage><lpage>462</lpage><pub-id pub-id-type="doi">10.1038/nn.3635</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Furey</surname><given-names>ML</given-names></name><name><surname>Ishai</surname><given-names>A</given-names></name><name><surname>Schouten</surname><given-names>JL</given-names></name><name><surname>Pietrini</surname><given-names>P</given-names></name></person-group><article-title>Distributed and overlapping representations of faces and objects in ventral temporal cortex</article-title><source>Science</source><year>2001</year><volume>293</volume><fpage>2425</fpage><lpage>2430</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><article-title>The Human Visual Cortex</article-title><source>Annual Review of Neuroscience</source><year>2004</year><volume>27</volume><fpage>649</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.27.070203.144220</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Using goal-driven deep learning models to understand sensory cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><fpage>356</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1038/nn.4244</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Synchronization of Cortical Activity and its Putative Role in Information Processing and Learning</article-title><source>Annual Review of Physiology</source><year>1993</year><volume>55</volume><fpage>349</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1146/annurev.ph.55.030193.002025</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Rhythms for Cognition: Communication through Coherence</article-title><source>Neuron</source><year>2015</year><volume>88</volume><fpage>220</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.034</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spriet</surname><given-names>C</given-names></name><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Hochmann</surname><given-names>J-R</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Visual object categorization in infancy</article-title><source>PNAS</source><year>2022</year><volume>119</volume><pub-id pub-id-type="doi">10.1073/pnas.2105866119</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rakison</surname><given-names>DH</given-names></name><name><surname>Poulin-Dubois</surname><given-names>D</given-names></name></person-group><article-title>Developmental origin of the animate–inanimate distinction</article-title><source>Psychological Bulletin</source><year>2001</year><volume>127</volume><fpage>209</fpage><lpage>228</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.127.2.209</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stahl</surname><given-names>AE</given-names></name><name><surname>Feigenson</surname><given-names>L</given-names></name></person-group><article-title>Observing the unexpected enhances infants’ learning and exploration</article-title><source>Science</source><year>2015</year><volume>348</volume><fpage>91</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1126/science.aaa3799</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>A</given-names></name><name><surname>Wiggs</surname><given-names>CL</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>Neural correlates of category-specific knowledge</article-title><source>Nature</source><year>1996</year><volume>379</volume><fpage>649</fpage><lpage>652</lpage></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname><given-names>B</given-names></name><name><surname>Richardson</surname><given-names>H</given-names></name><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Wald</surname><given-names>LL</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Organization of high-level visual cortex in human infants</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><elocation-id>13995</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13995</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosakowski</surname><given-names>HL</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Takahashi</surname><given-names>A</given-names></name><name><surname>Keil</surname><given-names>B</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Selective responses to faces, scenes, and bodies in the ventral visual pathway of infants</article-title><source>Current Biology</source><year>2022</year><volume>32</volume><fpage>265</fpage><lpage>274</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.10.064</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><article-title>A hierarchical, retinotopic proto-organization of the primate visual system at birth</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e26196</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26196</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><article-title>Body map proto-organization in newborn macaques</article-title><source>PNAS</source><year>2019</year><volume>116</volume><fpage>24861</fpage><lpage>24871</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912636116</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Savage</surname><given-names>T</given-names></name></person-group><article-title>Development of the macaque face-patch system</article-title><source>Nature Communications</source><year>2017</year><volume>8</volume><elocation-id>14897</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14897</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Skalaban</surname><given-names>LJ</given-names></name><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Bejjanki</surname><given-names>VR</given-names></name><name><surname>Córdova</surname><given-names>NI</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><article-title>Re-imagining fMRI for awake behaving infants</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><elocation-id>4523</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-18286-y</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yates</surname><given-names>TS</given-names></name><name><surname>Ellis</surname><given-names>CT</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name></person-group><article-title>The promise of awake behaving infant fMRI as a deep measure of cognition</article-title><source>Current Opinion in Behavioral Sciences</source><year>2021</year><volume>40</volume><fpage>5</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.11.007</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoehl</surname><given-names>S</given-names></name></person-group><article-title>The development of category specificity in infancy – What can we learn from electrophysiology?</article-title><source>Neuropsychologia</source><year>2016</year><volume>83</volume><fpage>114</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2015.08.021</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conte</surname><given-names>S</given-names></name><name><surname>Richards</surname><given-names>JE</given-names></name><name><surname>Guy</surname><given-names>MW</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>Roberts</surname><given-names>JE</given-names></name></person-group><article-title>Face-sensitive brain responses in the first year of life</article-title><source>NeuroImage</source><year>2020</year><volume>211</volume><elocation-id>116602</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116602</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational similarity analysis – connecting the branches of systems neuroscience</article-title><source>Front Sys Neurosci</source><year>2008</year><volume>2</volume><fpage>4</fpage><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossmann</surname><given-names>T</given-names></name><name><surname>Gliga</surname><given-names>T</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Mareschal</surname><given-names>D</given-names></name></person-group><article-title>The neural basis of perceptual category learning in human infants</article-title><source>J Cogn Neurosci</source><year>2009</year><volume>21</volume><fpage>2276</fpage><lpage>2286</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21188</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>PC</given-names></name><name><surname>Westerlund</surname><given-names>A</given-names></name><name><surname>Nelson</surname><given-names>CA</given-names></name></person-group><article-title>Neural Markers of Categorization in 6-Month-Old Infants</article-title><source>Psychol Sci</source><year>2006</year><volume>17</volume><fpage>59</fpage><lpage>66</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2005.01665.x</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marinović</surname><given-names>V</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Pauen</surname><given-names>S</given-names></name></person-group><article-title>Neural correlates of human–animal distinction: An ERP-study on early categorical differentiation with 4- and 7-month-old infants and adults</article-title><source>Neuropsychologia</source><year>2014</year><volume>60</volume><fpage>60</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2014.05.013</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Birtles</surname><given-names>D</given-names></name><name><surname>Wattam-Bell</surname><given-names>J</given-names></name><name><surname>Atkinson</surname><given-names>J</given-names></name><name><surname>Braddick</surname><given-names>O</given-names></name></person-group><article-title>Latency Measures of Pattern-Reversal VEP in Adults and Infants: Different Information from Transient P1 Response and Steady-State Phase</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><year>2012</year><volume>53</volume><fpage>1306</fpage><lpage>1314</lpage><pub-id pub-id-type="doi">10.1167/iovs.11-7631</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>DL</given-names></name><name><surname>Skarf</surname><given-names>B</given-names></name></person-group><article-title>Development of the human visual system: monocular and binocular pattern VEP latency</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><year>1991</year><volume>32</volume><fpage>2372</fpage><lpage>2381</lpage></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moskowitz</surname><given-names>A</given-names></name><name><surname>Sokol</surname><given-names>S</given-names></name></person-group><article-title>Developmental changes in the human visual system as reflected by the latency of the pattern reversal VEP</article-title><source>Electroencephalogr Clin Neurophysiol</source><year>1983</year><volume>56</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/0013-4694(83)90002-0</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>A Primer on Pattern-Based Approaches to fMRI: Principles, Pitfalls, and Perspectives</article-title><source>Neuron</source><year>2015</year><volume>87</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.025</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grootswagers</surname><given-names>T</given-names></name><name><surname>Wardle</surname><given-names>SG</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Decoding Dynamic Brain Patterns from Evoked Responses: A Tutorial on Multivariate Pattern Analysis Applied to Time Series Neuroimaging Data</article-title><source>Journal of Cognitive Neuroscience</source><year>2016</year><volume>29</volume><fpage>677</fpage><lpage>697</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01068</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>T</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Representational dynamics of object vision: The first 1000 ms</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1167/13.10.1</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Tovar</surname><given-names>DA</given-names></name><name><surname>Carlson</surname><given-names>TA</given-names></name></person-group><article-title>Emerging Object Representations in the Visual System Predict Reaction Times for Categorization</article-title><source>PLoS Comput Biol</source><year>2015</year><volume>11</volume><elocation-id>e1004316</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004316</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Characterizing the dynamics of mental representations: the temporal generalization method</article-title><source>Trends in Cognitive Sciences</source><year>2014</year><volume>18</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.01.002</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deoni</surname><given-names>SCL</given-names></name><name><surname>Mercure</surname><given-names>E</given-names></name><name><surname>Blasi</surname><given-names>A</given-names></name><name><surname>Gasston</surname><given-names>D</given-names></name><name><surname>Thomson</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>M</given-names></name><name><surname>Williams</surname><given-names>SCR</given-names></name><name><surname>Murphy</surname><given-names>DGM</given-names></name></person-group><article-title>Mapping Infant Brain Myelination with Magnetic Resonance Imaging</article-title><source>J Neurosci</source><year>2011</year><volume>31</volume><fpage>784</fpage><lpage>791</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2106-10.2011</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huttenlocher</surname><given-names>PR</given-names></name><name><surname>Dabholkar</surname><given-names>AS</given-names></name></person-group><article-title>Regional differences in synaptogenesis in human cerebral cortex</article-title><source>Journal of Comparative Neurology</source><year>1997</year><volume>387</volume><fpage>167</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1096-9861(19971020)387:2&lt;167::AID-CNE1&gt;3.0.CO;2-Z</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><article-title>Functional brain development in humans</article-title><source>Nature Reviews Neuroscience</source><year>2001</year><volume>2</volume><fpage>475</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/35081509</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuhl</surname><given-names>PK</given-names></name><name><surname>Williams</surname><given-names>KA</given-names></name><name><surname>Lacerda</surname><given-names>F</given-names></name><name><surname>Stevens</surname><given-names>KN</given-names></name><name><surname>Lindblom</surname><given-names>B</given-names></name></person-group><article-title>Linguistic experience alters phonetic perception in infants by 6 months of age</article-title><source>Science</source><year>1992</year><volume>255</volume><fpage>606</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1126/science.1736364</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spelke</surname><given-names>ES</given-names></name><name><surname>Kinzler</surname><given-names>KD</given-names></name></person-group><article-title>Core knowledge</article-title><source>Developmental Science</source><year>2007</year><volume>10</volume><fpage>89</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2007.00569.x</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>A M/EEG-fMRI Fusion Primer: Resolving Human Brain Responses in Space and Time</article-title><source>Neuron</source><year>2020</year><volume>107</volume><fpage>772</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.001</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence</article-title><source>Scientific Reports</source><year>2016</year><volume>6</volume><elocation-id>27755</elocation-id><pub-id pub-id-type="doi">10.1038/srep27755</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><article-title>An evaluation of the two-dimensional Gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><year>1987</year><volume>58</volume><fpage>1233</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.58.6.1233</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv:1409.1556 [cs]</source><year>2015</year></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>McClure</surname><given-names>P</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep Neural Networks in Computational Neuroscience</article-title><source>Oxford Research Encyclopedia of Neuroscience</source><year>2019</year><pub-id pub-id-type="doi">10.1093/acrefore/9780190264086.013.46</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graumann</surname><given-names>M</given-names></name><name><surname>Ciuffi</surname><given-names>C</given-names></name><name><surname>Dwivedi</surname><given-names>K</given-names></name><name><surname>Roig</surname><given-names>G</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The spatiotemporal neural dynamics of object location representations in the human brain</article-title><source>Nat Hum Behav</source><year>2022</year><volume>6</volume><fpage>796</fpage><lpage>811</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01302-0</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andrews</surname><given-names>TJ</given-names></name><name><surname>Clarke</surname><given-names>A</given-names></name><name><surname>Pell</surname><given-names>P</given-names></name><name><surname>Hartley</surname><given-names>T</given-names></name></person-group><article-title>Selectivity for low-level features of objects in the human ventral stream</article-title><source>NeuroImage</source><year>2010</year><volume>49</volume><fpage>703</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.08.046</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nasr</surname><given-names>S</given-names></name><name><surname>Echavarria</surname><given-names>CE</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name></person-group><article-title>Thinking Outside the Box: Rectilinear Shapes Selectively Activate Scene-Selective Cortex</article-title><source>J Neurosci</source><year>2014</year><volume>34</volume><fpage>6721</fpage><lpage>6735</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4802-13.2014</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><chapter-title>Chapter 2 Building the gist of a scene: the role of global image features in recognition</chapter-title><source>Visual Perception - Fundamentals of Awareness: Multi-Sensory Integration and High-Order Perception</source><publisher-name>Elsevier</publisher-name><year>2006</year><fpage>23</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(06)55002-2</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiorpes</surname><given-names>L</given-names></name></person-group><article-title>The Puzzle of Visual Development: Behavior and Neural Limits</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>11384</fpage><lpage>11393</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2937-16.2016</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterzell</surname><given-names>DH</given-names></name><name><surname>Werner</surname><given-names>JS</given-names></name><name><surname>Kaplan</surname><given-names>PS</given-names></name></person-group><article-title>Individual differences in contrast sensitivity functions: Longitudinal study of 4-, 6- and 8-month-old human infants</article-title><source>Vision Research</source><year>1995</year><volume>35</volume><fpage>961</fpage><lpage>979</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(94)00117-5</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>FW</given-names></name><name><surname>Robson</surname><given-names>JG</given-names></name></person-group><article-title>Application of fourier analysis to the visibility of gratings</article-title><source>J Physiol</source><year>1968</year><volume>197</volume><fpage>551</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1968.sp008574</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Evidence that recurrent circuits are critical to the ventral stream’s execution of core object recognition behavior</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>974</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0392-5</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Fast recurrent processing via ventrolateral prefrontal cortex is needed by the primate ventral stream for robust core visual object recognition</article-title><source>Neuron</source><year>2021</year><volume>109</volume><fpage>164</fpage><lpage>176</lpage></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>H</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Lotter</surname><given-names>W</given-names></name><name><surname>Moerman</surname><given-names>C</given-names></name><name><surname>Paredes</surname><given-names>A</given-names></name><name><surname>Caro</surname><given-names>JO</given-names></name><name><surname>Hardesty</surname><given-names>W</given-names></name><name><surname>Cox</surname><given-names>D</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><article-title>Recurrent computations for visual pattern completion</article-title><source>PNAS</source><year>2018</year><volume>115</volume><fpage>8835</fpage><lpage>8840</lpage><pub-id pub-id-type="doi">10.1073/pnas.1719397115</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupyan</surname><given-names>G</given-names></name><name><surname>Abdel Rahman</surname><given-names>R</given-names></name><name><surname>Boroditsky</surname><given-names>L</given-names></name><name><surname>Clark</surname><given-names>A</given-names></name></person-group><article-title>Effects of Language on Visual Perception</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><fpage>930</fpage><lpage>944</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.08.005</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heering</surname><given-names>A</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Rapid categorization of natural face images in the infant right hemisphere</article-title><source>eLife</source><year>2015</year><volume>4</volume><elocation-id>e06564</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.06564</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><article-title>Neuroscience-Inspired Artificial Intelligence</article-title><source>Neuron</source><year>2017</year><volume>95</volume><fpage>245</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.011</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>H</given-names></name></person-group><article-title>Über das Elektrenkephalogramm des Menschen</article-title><source>Archiv f Psychiatrie</source><year>1929</year><volume>87</volume><fpage>527</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1007/BF01797193</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name><name><surname>Draguhn</surname><given-names>A</given-names></name></person-group><article-title>Neuronal Oscillations in Cortical Networks</article-title><source>Science</source><year>2004</year><volume>304</volume><fpage>1926</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1126/science.1099745</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uhlhaas</surname><given-names>PJ</given-names></name><name><surname>Roux</surname><given-names>F</given-names></name><name><surname>Rodriguez</surname><given-names>E</given-names></name><name><surname>Rotarska-Jagiela</surname><given-names>A</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Neural synchrony and the development of cortical networks</article-title><source>Trends in Cognitive Sciences</source><year>2010</year><volume>14</volume><fpage>72</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.12.002</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshall</surname><given-names>PJ</given-names></name><name><surname>Bar-Haim</surname><given-names>Y</given-names></name><name><surname>Fox</surname><given-names>NA</given-names></name></person-group><article-title>Development of the EEG from 5 months to 4 years of age</article-title><source>Clinical Neurophysiology</source><year>2002</year><volume>113</volume><fpage>1199</fpage><lpage>1208</lpage><pub-id pub-id-type="doi">10.1016/S1388-2457(02)00163-3</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>LM</given-names></name></person-group><article-title>Synchronous neural oscillations and cognitive processes</article-title><source>Trends in Cognitive Sciences</source><year>2003</year><volume>7</volume><fpage>553</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.10.012</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Theta rhythm of navigation: link between path integration and landmark navigation, episodic and semantic memory</article-title><source>Hippocampus</source><year>2005</year><volume>15</volume><fpage>827</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1002/hipo.20113</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Mazaheri</surname><given-names>A</given-names></name></person-group><article-title>Shaping Functional Architecture by Oscillatory Alpha Activity: Gating by Inhibition</article-title><source>Front Hum Neurosci</source><year>2010</year><volume>4</volume><pub-id pub-id-type="doi">10.3389/fnhum.2010.00186</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klimesch</surname><given-names>W</given-names></name></person-group><article-title>Alpha-band oscillations, attention, and controlled access to stored information</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><fpage>606</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.10.007</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donoghue</surname><given-names>T</given-names></name><name><surname>Schaworonkow</surname><given-names>N</given-names></name><name><surname>Voytek</surname><given-names>B</given-names></name></person-group><article-title>Methodological considerations for studying neural oscillations</article-title><source>European Journal of Neuroscience</source><year>2022</year><volume>55</volume><fpage>3502</fpage><lpage>3527</lpage><pub-id pub-id-type="doi">10.1111/ejn.15361</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Dissociable Components of Information Encoding in Human Perception</article-title><source>Cerebral Cortex</source><year>2021</year><volume>31</volume><fpage>5664</fpage><lpage>5675</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhab189</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tallon-Baudry</surname><given-names>C</given-names></name><name><surname>Bertrand</surname><given-names>O</given-names></name></person-group><article-title>Oscillatory gamma activity in humans and its role in object representation</article-title><source>Trends in Cognitive Sciences</source><year>1999</year><volume>3</volume><fpage>151</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01299-1</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sauseng</surname><given-names>P</given-names></name><name><surname>Klimesch</surname><given-names>W</given-names></name><name><surname>Gruber</surname><given-names>WR</given-names></name><name><surname>Hanslmayr</surname><given-names>S</given-names></name><name><surname>Freunberger</surname><given-names>R</given-names></name><name><surname>Doppelmayr</surname><given-names>M</given-names></name></person-group><article-title>Are event-related potential components generated by phase resetting of brain oscillations? A critical discussion</article-title><source>Neuroscience</source><year>2007</year><volume>146</volume><fpage>1435</fpage><lpage>1444</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2007.03.014</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name><name><surname>Halit</surname><given-names>H</given-names></name></person-group><article-title>Development of face-sensitive event-related potentials during infancy: a review</article-title><source>International Journal of Psychophysiology</source><year>2003</year><volume>51</volume><fpage>45</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/S0167-8760(03)00152-1</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halit</surname><given-names>H</given-names></name><name><surname>Csibra</surname><given-names>G</given-names></name><name><surname>Volein</surname><given-names>Á</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><article-title>Face-sensitive cortical processing in early infancy</article-title><source>Journal of Child Psychology and Psychiatry</source><year>2004</year><volume>45</volume><fpage>1228</fpage><lpage>1234</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7610.2004.00321.x</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halit</surname><given-names>H</given-names></name><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Johnson</surname><given-names>MH</given-names></name></person-group><article-title>Cortical specialisation for face processing: face-sensitive event-related potential components in 3- and 12-month-old infants</article-title><source>NeuroImage</source><year>2003</year><volume>19</volume><fpage>1180</fpage><lpage>1193</lpage><pub-id pub-id-type="doi">10.1016/S1053-8119(03)00076-4</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoehl</surname><given-names>S</given-names></name><name><surname>Peykarjou</surname><given-names>S</given-names></name></person-group><article-title>The early development of face processing — What makes faces special?</article-title><source>Neurosci Bull</source><year>2012</year><volume>28</volume><fpage>765</fpage><lpage>788</lpage><pub-id pub-id-type="doi">10.1007/s12264-012-1280-0</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name><name><surname>Shuman</surname><given-names>M</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>A Cortical Area Selective for Visual Processing of the Human Body</article-title><source>Science</source><year>2001</year><volume>293</volume><fpage>2470</fpage><lpage>2473</lpage><pub-id pub-id-type="doi">10.1126/science.1063414</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelen</surname><given-names>MV</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name></person-group><article-title>Selectivity for the Human Body in the Fusiform Gyrus</article-title><source>J Neurophysiol</source><year>2005</year><volume>93</volume><fpage>603</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1152/jn.00513.2004</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peykarjou</surname><given-names>S</given-names></name><name><surname>Pauen</surname><given-names>S</given-names></name><name><surname>Hoehl</surname><given-names>S</given-names></name></person-group><article-title>How do 9-month-old infants categorize human and ape faces? A rapid repetition ERP study</article-title><source>Psychophysiology</source><year>2014</year><volume>51</volume><fpage>866</fpage><lpage>878</lpage><pub-id pub-id-type="doi">10.1111/psyp.12238</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gliga</surname><given-names>T</given-names></name><name><surname>Dehaene-Lambertz</surname><given-names>G</given-names></name></person-group><article-title>Development of a view-invariant representation of the human head</article-title><source>Cognition</source><year>2007</year><volume>102</volume><fpage>261</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2006.01.004</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nat Rev Neurosci</source><year>2014</year><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title><source>Computational Intelligence and Neuroscience</source><year>2010</year><volume>2011</volume><elocation-id>e156869</elocation-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tadel</surname><given-names>F</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name><name><surname>Mosher</surname><given-names>JC</given-names></name><name><surname>Pantazis</surname><given-names>D</given-names></name><name><surname>Leahy</surname><given-names>RM</given-names></name></person-group><article-title>Brainstorm: A User-Friendly Application for MEG/EEG Analysis</article-title><source>Computational Intelligence and Neuroscience</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1155/2011/879716</pub-id></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C</given-names></name><name><surname>Lin</surname><given-names>C</given-names></name></person-group><source>{LIBSVM}: a library for support vector machines</source><year>2001</year></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Information-based functional brain mapping</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2006</year><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The Neural Dynamics of Attentional Selection in Natural Scenes</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Kievit</surname><given-names>RA</given-names></name></person-group><article-title>Representational geometry: integrating cognition, computation, and the brain</article-title><source>Trends in Cognitive Sciences</source><year>2013</year><volume>17</volume><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.06.007</pub-id></element-citation></ref><ref id="R91"><label>91</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggenmos</surname><given-names>M</given-names></name><name><surname>Sterzer</surname><given-names>P</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Multivariate pattern analysis for MEG: A comparison of dissimilarity measures</article-title><source>Neuroimage</source><year>2018</year><volume>173</volume><fpage>434</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.02.044</pub-id></element-citation></ref><ref id="R92"><label>92</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kay</surname><given-names>KN</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Prenger</surname><given-names>RJ</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Identifying natural images from human brain activity</article-title><source>Nature</source><year>2008</year><volume>452</volume><fpage>352</fpage><lpage>355</lpage><pub-id pub-id-type="doi">10.1038/nature06713</pub-id></element-citation></ref><ref id="R93"><label>93</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><article-title>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</article-title><source>arXiv:1312.6034 [cs]</source><year>2013</year></element-citation></ref><ref id="R94"><label>94</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bau</surname><given-names>D</given-names></name><name><surname>Zhou</surname><given-names>B</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name><name><surname>Torralba</surname><given-names>A</given-names></name></person-group><article-title>Network Dissection: Quantifying Interpretability of Deep Visual Representations</article-title><source>arXiv:1704.05796 [cs]</source><year>2017</year></element-citation></ref><ref id="R95"><label>95</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MAJ</given-names></name></person-group><article-title>Deep Neural Networks Reveal a Gradient in the Complexity of Neural Representations across the Ventral Stream</article-title><source>J Neurosci</source><year>2015</year><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R96"><label>96</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title><source>NeuroImage</source><year>2017</year><volume>152</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.10.001</pub-id></element-citation></ref><ref id="R97"><label>97</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep Supervised, but Not Unsupervised, Models May Explain IT Cortical Representation</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></element-citation></ref><ref id="R98"><label>98</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><year>2014</year><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R99"><label>99</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>MJ</given-names></name><name><surname>Ratan Murty</surname><given-names>NA</given-names></name><name><surname>Ajemian</surname><given-names>R</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Integrative Benchmarking to Advance Neurally Mechanistic Models of Human Intelligence</article-title><source>Neuron</source><year>2020</year><volume>108</volume><fpage>413</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.040</pub-id></element-citation></ref><ref id="R100"><label>100</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lenc</surname><given-names>K</given-names></name></person-group><article-title>MatConvNet - Convolutional Neural Networks for MATLAB</article-title><source>arXiv:1412.4564 [cs]</source><year>2016</year></element-citation></ref><ref id="R101"><label>101</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How Does the Brain Solve Visual Object Recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R102"><label>102</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>A Toolbox for Representational Similarity Analysis</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id></element-citation></ref><ref id="R103"><label>103</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2007.03.024</pub-id></element-citation></ref><ref id="R104"><label>104</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name></person-group><article-title>Nonparametric permutation tests for functional neuroimaging: A primer with examples</article-title><source>Hum Brain Mapp</source><year>2002</year><volume>15</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1002/hbm.1058</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental design and results of time-resolved multivariate analysis.</title><p id="P82"><bold>(A)</bold> The stimulus set comprised 32 cut-out images from four categories each: toys, headless bodies, houses, and faces (full set see <bold>OSF Repository</bold>). <bold>(B)</bold> Time-resolved multivariate analysis on EEG data. First, we extracted condition-specific EEG sensor activation values for every time point in the epoch and formed them into response vectors. Then, using a leave-one-out cross-validation scheme, we trained and tested a support vector machine to classify visual object categories from the response vectors. The results (pairwise decoding accuracy, 50% chance level) were aggregated in a decoding accuracy matrix of size 4 × 4, indexed in rows and columns by the conditions classified. The matrix is symmetric along the diagonal, and the diagonal is undefined. Averaging the lower triangular part of the matrix resulted in grand average decoding accuracy as a measure of how well visual representations discriminate categories at a particular time point. <bold>(C,D)</bold> The grand average time course of visual category decoding in infants <bold>(C)</bold> and adults <bold>(D)</bold>. The gray vertical line indicates onset of image presentation. Shaded margins indicate 95% confidence intervals (CIs) of decoding accuracy. Horizontal error bars indicate 95% CIs of peak latency. Rows of asterisks indicate time points with significantly above-chance decoding accuracy (infant <italic>n</italic> = 40 or adult <italic>n</italic> = 20, right-tailed sign permutation tests, cluster-defining threshold <italic>P</italic> &lt; .005, corrected significance level <italic>P</italic> &lt; .05). Detailed statistical information is listed in <xref ref-type="supplementary-material" rid="SD1">Table S1A</xref>. For visualization of single participant infant data see <bold>GitHub Repository: Visualization</bold>. <bold>(E,F)</bold> Results of category classification in EEG channel-space searchlight analysis for infants <bold>(E)</bold> and adults <bold>(F)</bold>. Bold dots indicate the EEG channels with significantly above-chance decoding accuracy (right-tailed sign-permutation tests, <italic>P</italic> &lt; .05, FDR-corrected). <bold>(G,H)</bold> Results of time-generalization analysis for infants <bold>(G)</bold> and adults <bold>(H)</bold>. Detailed statistical information is listed in <xref ref-type="supplementary-material" rid="SD1">Table S1E</xref>. For visualization of single participant infant data see <bold>GitHub Repository: Visualization</bold>. The gray vertical and horizontal lines indicate the onsets of image presentation. Black outlines indicate time-point combinations with significantly above-chance decoding accuracy (right-tailed sign permutation tests, cluster-defining threshold <italic>P</italic> &lt; .005, corrected significance level <italic>P</italic> &lt; .05). See also <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p></caption><graphic xlink:href="EMS156772-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Category representations shared between infants and adults.</title><p id="P83"><bold>(A)</bold> We used RSA to relate category representations in infants and adults. We interpret decoding accuracy as a dissimilarity measure on the assumption that the more dissimilar two representations are, the better the classifier performs. This allowed us to use time-resolved decoding accuracy matrices as representational dissimilarity matrices (RDMs) that summarize representational similarities between category representations. We compared RDMs (Spearman’s <italic>R</italic>) in infants (average across participants) and adults (for each participant separately) for all time point combinations (t<sub>x</sub>, t<sub>y</sub>), assigning the values to a time-generalization matrix indexed in rows and columns by the time in adults (t<sub>x</sub>) and infants (t<sub>y</sub>). <bold>(B)</bold> Average time-generalization matrix relating category representations in infants and adults over time. Detailed statistical information is listed in <xref ref-type="supplementary-material" rid="SD1">Table S2A</xref>. For visualization of single participant data see <bold>GitHub Repository: Visualization</bold>. The gray lines indicate image onset. Black outlines indicate time point combinations with significant correlation (<italic>n</italic> = 20, right-tailed sign permutation tests, cluster-defining threshold <italic>P</italic> &lt; .005, corrected significance level <italic>P</italic> &lt; .05). See also <xref ref-type="supplementary-material" rid="SD1">Figure S2 and Table S2</xref>.</p></caption><graphic xlink:href="EMS156772-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The format of category representations in infants and adults.</title><p id="P84"><bold>(A)</bold> We characterized what type of visual features are encoded in category representations in infants and adults by relating them to computational models using RSA. We ran the stimulus images through a Gabor filter model and the VGG-19 deep neural network trained on object categorization. We constructed RDMs from their unit activation patterns (visualized in <bold>GitHub Repository: Visualization</bold>). We then compared model RDMs to infant and adult neural RDMs (constructed as the average of RDMs over time from 95% CIs around peak latency of time-resolved category classification, see <xref ref-type="fig" rid="F1">Figure 1C,D</xref>). <bold>(B)</bold> Results for infants (left) and adults (right) at the whole-model level. Asterisks indicate significant correlation. <bold>(C)</bold> Results for infants (left) and adults (right) at the DNN layer level. For <bold>(B,C)</bold>, error bars represent standard errors of the mean. Asterisks indicate significant correlation (infant <italic>n</italic> = 40, adult <italic>n</italic> = 20, two-tailed sign-permutation tests, <italic>P</italic> &lt; .05, FDR-corrected). Statistical details (i.e., correlations and <italic>P</italic>-values) are in <xref ref-type="supplementary-material" rid="SD1">Table S3A</xref>. <bold>(D)</bold> Results for infants (left) and adults (right) at the DNN layer level after removing the effect of the other age group respectively by partialling out the average RDM. Error bars represent standard errors of the mean. Asterisks indicate significant correlation. <bold>(E)</bold> Example of Butterworth filtered images in different spatial frequencies. <bold>(F)</bold> We characterized visual features encoded in visual representations in terms of spatial frequency content. We ran the frequency-filtered images through the VGG-19 DNN. We constructed spatial-frequency-specific RDMs from the DNN unit activation patterns. We then compared model RDMs to infant and adult neural RDMs as described in <bold>(A)</bold>. <bold>(G)</bold> Relating frequency-specific image content to neural representations. The results indicate a significant correlation across all frequencies (except one bin at 0.18 cycle per degree (cpd)) in both infants and adults, with higher correlations for adults than infants above 1 cpd. <bold>(H)</bold> Spatial-frequency-specific results for infants (blue curve) and adults (red curve) at the whole-model level. For <bold>(G,H)</bold>, asterisks color-coded as result curves indicate statistical significance (infant <italic>n</italic> = 40, adult <italic>n</italic> = 20, two-tailed sign-permutation tests, <italic>P</italic> &lt; .05, FDR-corrected); black asterisks indicate significant difference between age groups (two-tailed Mann-Whitney U tests, <italic>P</italic> &lt; .05, FDR-corrected). See also <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S3</xref>.</p></caption><graphic xlink:href="EMS156772-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Spectral characterization of infant and adult category representations</title><p id="P85"><bold>(A)</bold> Category classification based on frequency-resolved EEG data. We first decomposed EEG data in time and frequency using <italic>Morlet</italic> wavelets for each trial and each channel, yielding a trial-wise representation of induced oscillatory power. We then conducted the time-resolved multivariate classification of category separately on each frequency bin. This yielded a 4 × 4 matrix of decoding accuracies at each time point and frequency bin, which we either averaged to obtain grand average category classification <bold>(B,C)</bold> or used as an RDM in RSA <bold>(D,E)</bold>. <bold>(B,C)</bold> Results of time- and frequency-resolved MVPA for infants <bold>(B)</bold> and adults <bold>(C)</bold>. Detailed statistical information is listed in <xref ref-type="supplementary-material" rid="SD1">Table S4A</xref>. <bold>(D)</bold> RSA procedure linking oscillation-based visual category representations in infants and adults. We first created a single aggregate infant oscillatory RDM by averaging decoding accuracy matrices based on the extent of the cluster in the infant data. We compared (Spearman’s <italic>R</italic>) this aggregate infant RDM to time- and frequency-resolved RDMs for each participant in the adult sample. This yielded a two-dimensional matrix indicating in which frequency range and when category representations are similar between infants and adults. <bold>(E)</bold> Similarity between infant theta-based category representations and adult category representations resolved in time and frequency. Detailed statistical information is listed in <xref ref-type="supplementary-material" rid="SD1">Table S4B</xref>. For <bold>(B,C,E)</bold>, for the single participant data see <bold>GitHub Repository: Visualization</bold>; the gray vertical lines indicate the onset of image presentation; line profiles of classification accuracy or correlation at the peak latency (gray vertical dashed lines) were shown on the left of the plots, respectively; black outlines indicate time point combinations with significant results (infants <italic>n</italic> = 40, adults <italic>n</italic> = 20, right-tailed permutation test, cluster-defining threshold <italic>P</italic> &lt; .005, corrected significance level <italic>P</italic> &lt; .05). See also <xref ref-type="supplementary-material" rid="SD1">Figure S4</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S4</xref>.</p></caption><graphic xlink:href="EMS156772-f004"/></fig></floats-group></article>