<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS144977</article-id><article-id pub-id-type="doi">10.1101/2021.12.15.472758</article-id><article-id pub-id-type="archive">PPR433615</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Disrupting inferior frontal cortex activity alters affect decoding efficiency from clear but not from ambiguous affective speech</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0638-3981</contrib-id><name><surname>Ceravolo</surname><given-names>Leonardo</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Moisa</surname><given-names>Marius</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Grandjean</surname><given-names>Didier</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ruff</surname><given-names>Christian</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Frühholz</surname><given-names>Sascha</given-names></name><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib></contrib-group><aff id="A1"><label>1</label>Neuroscience of Emotion and Affective Dynamics Lab, University of Geneva, Department of Psychology, Geneva, Switzerland</aff><aff id="A2"><label>2</label>Swiss Center for Affective Sciences, Campus Biotech, Geneva, Switzerland</aff><aff id="A3"><label>3</label>Zurich Center for Neuroeconomics, Department of Neuroeconomics, University of Zurich, Switzerland</aff><aff id="A4"><label>4</label>Neuroscience Center Zurich, University of Zurich and ETH Zurich, Zurich, Switzerland</aff><aff id="A5"><label>5</label>Cognitive and Affective Neuroscience Unit, University of Zurich, Zurich, Switzerland</aff><aff id="A6"><label>6</label>Center for the Interdisciplinary Study of Language Evolution (ISLE), University of Zurich, Zurich, Switzerland</aff><aff id="A7"><label>7</label>Department of Psychology, University of Oslo, Oslo, Norway</aff><author-notes><corresp id="CR1">
<label>*</label><underline>Corresponding author:</underline> Leonardo Ceravolo, PhD, Unimail, office 5133, 40 Boulevard Pont-d'Arve, 1205 Geneva, Switzerland, <email>leonardo.ceravolo@unige.ch</email>,</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>13</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>12</day><month>05</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The evaluation of socio-affective sound information is accomplished by the primate auditory cortex in collaboration with limbic and inferior frontal cortex (IFC)—often observed during affective voice classification. Partly opposing views have been proposed, with IFC either coding cognitive processing challenges in case of sensory ambiguity or representing categorical object and affect information for clear voices. Here, we presented clear and ambiguous affective speech to two groups of human participants during neuroimaging, while in one group we inhibited right IFC activity with transcranial magnetic stimulation. IFC activity inhibition led to faster affective decisions, more accurate choice probabilities, reduced auditory cortical activity and increased fronto-limbic connectivity for clear affective speech. This indicates a more intermediate functional property of the IFC than assumed—namely with normal activity representing a more deliberate form of affective sound processing (i.e., enforcing cognitive analysis) that flags categorical sound decisions with precaution (i.e., representation of categorical uncertainty).</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The neural processing and classification of acoustic information involves a cortical neural network beyond auditory decoding in the auditory cortex (AC; <xref ref-type="bibr" rid="R74">Staib and Frühholz 2020</xref>). The core of this neural network especially for decoding acoustic and social information conveyed by acoustic voice signals involves an integrated functioning of the AC together with various regions in the inferior frontal cortex (IFC; <xref ref-type="bibr" rid="R65">Rauschecker and Scott 2009</xref>, <xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>). This auditory-frontal network is especially relevant for extracting socio-affective information from voice signals (<xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>, <xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R77">Swanborough, Staib et al. 2020</xref>), such as decoding emotional information expressed in affective voices and affective intonations in speech. In the latter case, the auditory-frontal network is often accompanied by neural activity in the limbic system, and this limbic activity is primarily located in the amygdala (<xref ref-type="bibr" rid="R39">Frühholz, Trost et al. 2014</xref>, <xref ref-type="bibr" rid="R40">Frühholz, Trost et al. 2016</xref>).</p><p id="P3">Within this broad auditory-frontal-limbic network for voice signals processing and voice information decoding (<xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R26">Dricu and Frühholz 2020</xref>, <xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>), the involvement of the IFC in decoding emotions from affective speech has been consistently reported (<xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>, <xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>, <xref ref-type="bibr" rid="R77">Swanborough, Staib et al. 2020</xref>), but its functional role in the neural network and its functional contribution to voice signal processing and especially in vocal affect decoding remained debated. Some previous studies indicated that social affective decoding from voice seemed restricted to the AC (<xref ref-type="bibr" rid="R43">Grandjean, Sander et al. 2005</xref>, <xref ref-type="bibr" rid="R30">Ethofer, Van De Ville et al. 2009</xref>) without any functional relevance of the IFC. However, more recent studies highlighted the functional relevance of the IFC in two possible directions. First, some studies seem to suggest that the IFC is especially relevant for elaborate evaluations and classification of socio-affective information in voices (<xref ref-type="bibr" rid="R72">Schirmer, Zysset et al. 2004</xref>, <xref ref-type="bibr" rid="R32">Fecteau, Armony et al. 2005</xref>). These processes are supposed to happen down-stream to the acoustical analysis in the AC (<xref ref-type="bibr" rid="R65">Rauschecker and Scott 2009</xref>, <xref ref-type="bibr" rid="R34">Frühholz, Ceravolo et al. 2012</xref>, <xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>) with different IFC subregions coding for the complexity of the evaluation and classification task (<xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>, <xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R26">Dricu and Frühholz 2020</xref>). According to these studies, the IFC would represent the rather domain-general task, evaluation, and classification difficulty, and would show increased activity for any challenging affective evaluation and classification tasks. This notion would support the related view on the IFC as a region that exerts some top-down control on the AC depending on certain task requirements (<xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>) and explicit sound classifications demands (<xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>). The latter seems especially relevant when classifying ambiguous voices and affect information (<xref ref-type="bibr" rid="R14">Bestelmeyer, Maurage et al. 2014</xref>), which requires that acoustical analyses in AC are supplemented by functional support by neural processing resources of the IFC (<xref ref-type="bibr" rid="R77">Swanborough, Staib et al. 2020</xref>). In fact, ambiguous voices roughly contain the same acoustic information as non-ambiguous voices yet they are perceived differently (<xref ref-type="bibr" rid="R48">Hoekert, Bais et al. 2008</xref>, <xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>).</p><p id="P4">Unlike the aforementioned evidence, some other studies especially from animal research have shown a more concrete psychoacoustic role of the IFC instead of representing abstract decisional task demands, as some IFC subregions seem to be directly involved in the perceptual discrimination of different types of conspecific vocalizations (<xref ref-type="bibr" rid="R7">Averbeck and Romanski 2006</xref>, <xref ref-type="bibr" rid="R18">Cohen, Russ et al. 2009</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>). According to these studies, the IFC would function pre-dominantly as a domain-specific higher-order auditory processing node that represents sound information in relation to categorical certainty or discriminability (<xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>), especially during explicit voice signal classification tasks (<xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>). This notion would suggest some kind of opposite IFC activity patterns as suggested by the first line of studies. Especially, this notion would predict higher IFC activity for clear as opposed to ambiguous affective voices, as clear voices would allow a more direct categorical representation. Given these two different hypothesis about the role of the IFC in processing affective voices, more knowledge is critical to determine the functional role of the IFC for the fine-grained classification of affective voices at different levels of acoustic ambiguity.</p><p id="P5">To mechanistically investigate the functional role of the IFC in the classification of socio-affective voice information, an experimentally induced alteration of IFC activity especially during the processing of both clear and ambiguous affective speech would reveal direct evidence. Unlike clear affective expressions in voices which represent categorical certainty as predicted by the second hypothesis, ambiguous vocal affect might trigger computations in the IFC given their challenging perceptual and cognitive processing according to the first hypothesis. Both cases might additionally require integrated IFC-STC functioning in terms of neural connectivity (<xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>), either for top-down IFC-to-STC facilitations or as co-representations of categorical affective information (<xref ref-type="bibr" rid="R75">Steiner, Bobin et al. 2021</xref>). The use of transcranial magnetic stimulation (TMS) appears to be a reliable and non-invasive option to especially inhibit a proper functioning of computations performed in the IFC in this context. Continuous theta burst stimulation (cTBS), a patterned pulse stimulation, as well as repetitive TMS (rTMS) are known to create reversible neural activity alteration (<xref ref-type="bibr" rid="R63">Oberman, Edwards et al. 2011</xref>). Although the neural effects induced by the cTBS procedure are much faster and potentially longer-lasting than those of rTMS, the cTBS procedure was rather scarcely used for social and especially voice processing studies, even less in combination with functional magnetic resonance imaging (fMRI). Previous studies using cTBS procedure revealed impairment on voice identity but not on affect discrimination tasks when applied over the premotor cortex (<xref ref-type="bibr" rid="R8">Banissy, Sauter et al. 2010</xref>). Similar results were observed in a combined cTBS-fMRI study targeting the premotor cortex in vocal affect processing, showing that cTBS triggered differential brain patterns in fronto-parietal, parahippocampal, and IFC brain regions (<xref ref-type="bibr" rid="R1">Agnew, Banissy et al. 2018</xref>).</p><p id="P6">Compared to cTBS, the use of rTMS revealed more mixed results regarding socio-affective processing from voices. First, the rTMS procedure was unsuccessful on both the superior temporal cortex (STC; <xref ref-type="bibr" rid="R54">Jiahui, Garrido et al. 2017</xref>) and IFC (<xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>) as it did not lead to expected behavioral changes, for example, in terms of a difficulty to classify vocal affect. Second, some reaction time differences were observed for the affect classifications of clear sad and happy voices by the use of rTMS over the right but not the left STC (<xref ref-type="bibr" rid="R2">Alba-Ferrara, Ellison et al. 2012</xref>), showing longer delays to respond in the classification task. Similar results were observed as well when rTMS was applied over the right premotor cortex (<xref ref-type="bibr" rid="R69">Sammler, Grosbras et al. 2015</xref>) and over the right anterior STC and fronto-parietal operculum (<xref ref-type="bibr" rid="R48">Hoekert, Bais et al. 2008</xref>). Third, again no behavioral differences were observed in yet another study when rTMS was applied to either the bilateral STC or IFC (<xref ref-type="bibr" rid="R52">Jacob, Brück et al. 2014</xref>), even though abovementioned results tell another story, and even though the right STC was shown to causally interact with voice detection, independently of emotion (<xref ref-type="bibr" rid="R13">Bestelmeyer, Belin et al. 2011</xref>). Some methodological variations may explain these inconsistencies, most importantly regarding the type of TMS procedure (rTMS or cTBS) and especially large variations in IFC target region(s).</p><p id="P7">Given these limitations and inconsistencies concerning socio-affective classification from affective speech with inhibitory effects especially applied to the IFC, we designed a three-alternative classification task on clear and ambiguous vocal affect while brain activity was influenced and recorded in a combined cTBS-fMRI setup. In order to address more precisely the cognitive and evaluative role of the IFC in such contexts (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>), we created stimuli containing a gradual blending of angry and fearful voices to manipulate affective ambiguity expressed in these voices. Anger and fear are distinctive and well-recognized emotions expressed in vocal affect, and a blending of anger and fear leads to considerable affective ambiguity given their opposite nature for behavioral adaptations in listeners (<xref ref-type="bibr" rid="R83">Whiting, Kotz et al. 2020</xref>). This setup also allowed to determine brain activity patterns as well as functional connectivity changes following the cTBS procedure between two groups of participants in a between-group design. In one group, cTBS was applied to the right IFC (experimental group, 'EG'), while in another group cTBS was applied to the vertex as a control brain site (control group, 'CG'). We hypothesized that: (a) cTBS over the IFC would alter affect classifications and response speed especially for ambiguous affective voices—according to challenging perceptual and cognitive processing of these voices (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>, <xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>, <xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>)—but also potentially for clear affective voices—representing categorical certainty; (b) cTBS over the IFC should also lead to a distinct reorganization of brain activity patterns while classifying the most ambiguous affective voices, especially within the right AC/STC for voice processing (<xref ref-type="bibr" rid="R34">Frühholz, Ceravolo et al. 2012</xref>, <xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>) and in the limbic system (i.e., amygdala) for affect processing (<xref ref-type="bibr" rid="R34">Frühholz, Trost et al. 2014</xref>); and (c) we expected altered functional coupling between IFC and the limbic and auditory cortical system during cTBS to the IFC given the important status of the IFC in the neural network for socio-affective voice processing (<xref ref-type="bibr" rid="R34">Frühholz, Ceravolo et al. 2012</xref>, <xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R42">Grandjean 2020</xref>).</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">The present study used a combined cTBS-fMRI setup to non-invasively and temporarily alter brain functioning in the right IFC while human participants of the EG (cTBS over the right IFC; <xref ref-type="fig" rid="F2">Fig.2</xref>) and the CG (cTBS over the vertex; <xref ref-type="fig" rid="F2">Fig.2</xref>) performed a three-alternative forced choice task on clear and ambiguous affective speech. We note here that all <italic>p</italic>-values reported for the statistical tests on the behavioral choice and reaction time data are corrected for multiple comparisons using the Bonferroni method.</p><p id="P9">For the affective forced choice task, we created blends of vocally expressed anger and fear using a voice morphing procedure (<xref ref-type="bibr" rid="R65">Rauschecker and Scott 2009</xref>, <xref ref-type="bibr" rid="R74">Staib and Frühholz 2020</xref>) on a set of commonly available and validated vocal affective bursts, namely the Montreal affective voice database or 'MAV' (<xref ref-type="bibr" rid="R11">Belin, Fillion-Bilodeau et al. 2008</xref>). From this database we selected five male and five female voices. We created blends of affective voices ranging from 100% of one emotion to 100% of the other emotions in steps of 10% morphing, resulting in 11 different stimulus categories (<xref ref-type="fig" rid="F1">Fig.1C</xref>): 60-100% fear proportions (F<sub>60</sub>, F<sub>70</sub>, F<sub>80</sub>, F<sub>90</sub>, F<sub>100</sub>), 60-100% anger proportions (A<sub>60</sub>, A<sub>70</sub>, A<sub>80</sub>, A<sub>90</sub>, A<sub>100</sub>), and the 50/50 mix of anger and fear in the condition AF<sub>50</sub>.</p><p id="P10">For selecting the appropriate stimuli and morphing rates for the main experiment, we asked an independent sample of participants (N=19) to classify these stimuli in a 2AFC task as portraying either 'anger' or 'fear'. Our aim was to select one emotionally ambiguous morphing level as well as four other gradually less ambiguous emotional voices for a total of five morphed voices. Using generalized linear mixed-effects ('glmer') modelling with the participants' choices as dependent variable, illustrating the probability of an 'anger' response. We observed a significant main effect of morphing on the categorization (χ<sup>2</sup>(10)=1094.3, <italic>p</italic>&lt;.0001); reaction time data were of no major interest here but are reported in <xref ref-type="fig" rid="F1">Fig.1C</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>. We used planned comparisons to explore which morphing levels would be more suitable—i.e., accurately evaluated—and contrasts showed significant differences between any pair of morphing levels (all <italic>p</italic>&lt;.0001, see <xref ref-type="fig" rid="F1">Fig.1</xref>, <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>). Since the AF<sub>50</sub> condition embodied the highest affective ambuguity, we selected it as well as two less ambiguous morphing levels per emotion in equal steps of 20% morphing, that is A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub> and F<sub>70</sub>. This model explained 40.25% of the variance including both fixed and random effects (R2c=0. 4025) and 30.46% of the variance solely for fixed effects (R2m=0.3046).</p><p id="P11">According to this selection criteria, the stimuli for the main experiment consisted of expressions of vocal affect containing 10%, 30%, 50%, 70%, or 90% of fear and therefore 90%, 70%, 50%, 30%, and 10% of anger at the same time, respectively (labels: A<sub>90</sub>, A<sub>70</sub>, AF<sub>50</sub>, F<sub>70</sub>, F<sub>90</sub>; <xref ref-type="fig" rid="F1">Fig.1A</xref>). The 10% conditions referred to rather clearly expressed vocal affect, the 50% condition was of highest affective ambiguity, and the 30% morphing level referred to an intermediate level of ambiguity. In the main fMRI task, participants therefore listened to these five expressions of morphed vocal affect as well as to neutral vocalizations in the main experiment, and they were asked to classify them as 'anger', 'fear', or 'neutral'. The neutral voices were added from each speaker and served as a control condition during affective decisions. Participants performed these decisions in one run before cTBS (cTBS<sub>pre</sub>), and in three runs after cTBS was applied (cTBS<sub>post</sub>) to the vertex (CG) or to the right IFC (EG) (see <xref ref-type="fig" rid="F1">Fig. 1 A</xref>). The inclusion of a cTBS<sub>pre</sub> run was motivated by the need for a control measure of brain activity between groups independently of the cTBS procedure, namely a sanity check for the main task providing an overview of brain activity and behavior depending merely on task conditions.</p><p id="P12">For the main fMRI task and for both behavioral and neuroimaging data analysis, the triple interaction between <italic>group</italic>(CG,EG), <italic>morphing</italic>(A<sub>90</sub>, A<sub>70</sub>, AF<sub>50</sub>, F<sub>70</sub>, F<sub>90</sub>) and <italic>run</italic>(cTBS<sub>pre</sub> run,cTBS<sub>post</sub> run1-3) was computed using two contrasts of interest: [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] and its inverse [CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]. For the <italic>morphing</italic> factor in these contrasts, a specific contrast weighting was applied—namely [2*(A<sub>90</sub>) + 1*(A<sub>70</sub>) -6*(AF<sub>50</sub>) + 1*(F<sub>70</sub>) + 2*(F<sub>90</sub>)]—to better model and characterize the morphing percentages of the affective voices in the contrast vectors. Such weighting was then of course adapted to each comparison between <italic>run</italic> and <italic>group</italic> with necessary sign inversion(s) and multiplication(s).</p><sec id="S3"><title>Right IFC as target region for cTBS procedure</title><p id="P13">As mentioned in the introduction, decisions on vocal affect are typically associated with activity in bilateral IFC. In the current study, we acquired the data of the CG first and we chose the right IFC, namely the right inferior frontal gyrus <italic>pars triangularis</italic> [IFGtri, MNI <italic>xyz</italic> 54,34,10; <xref ref-type="fig" rid="F2">Fig.2AB</xref>], as target region for cTBS. This was based on the observation that the right IFGtri showed higher activity for clear as opposed to ambiguous voice processing in the CG especially when contrasting cTBS<sub>post</sub> compared to cTBS<sub>pre</sub> runs. This contrast matched also the pattern of contrasts we were subsequently interested in for to the data obtained for the EG.</p><p id="P14">Indeed, the right IFC, more specifically the right IFGtri (see Methods for more details), was the global maximum when computing the aforementioned contrast in the CG (<xref ref-type="fig" rid="F2">Fig.2B-D</xref>), and it was therefore used as a target region for the cTBS procedure of the EG instead of using an IFC region reported in the literature. The homologous IFC region in the left hemisphere (<xref ref-type="fig" rid="F2">Fig.2AC</xref>) responded to a lesser extent to our contrast of interest and was therefore not targeted by the cTBS procedure. The cTBS procedure targeted the vertex [MNI <italic>xyz</italic> 0 -20 80] for the CG. Coil positioning is reported in <xref ref-type="fig" rid="F2">Fig.2D</xref> for each group.</p></sec><sec id="S4"><title>Altered behavioral decisions on voice affective ambiguity after cTBS on the right IFC</title><p id="P15">We first assessed the effects of cTBS applied to the right IFC on the decisional patterns during the classification of neutral (no interest condition) as well as clear and ambiguous vocal affect. The behavioral classification data were parametrized along response speeds and decision probabilities in the three-alternative task (neutral, anger, or fear). Response speed and decision probability were analyzed using generalized linear mixed-effects ('glmer') and linear mixed-effects ('lmer') models, respectively. These models allowed us to test our hypothesis according to which participants of the CG would perform better and potentially faster at categorizing blended voices as opposed to those of the EG who received cTBS over the right IFC. As mentioned before, neutral voices were used as baseline control trials and we did not have any hypothesis concerning these, so we modelled these trials in all our analyses but discarded them from behavioral and neuroimaging data results. Their classification accuracy was very high (i.e., they were not misclassified as and mixed-up with the affective trials) and no significant difference between groups across runs was observed (χ<sup>2</sup>(1)=0.08, <italic>p</italic>=1; see <xref ref-type="table" rid="T1">Table 1</xref>). In the first statistical model with the probability of an anger choice on affective trials as the binomial dependent variable (<xref ref-type="fig" rid="F1">Fig.1B</xref>), we observed significant effects along several of our experimental factors. First, we found significantly different choice probabilities between the morphing levels of the voice stimuli (factor <italic>morphing</italic>: χ<sup>2</sup>(5)=1496.55, <italic>p</italic>&lt;.0001), with voices containing a higher proportion of anger being more likely classified as 'anger', while those containing a higher proportion of fear being more likely classified as 'fear' ([A<sub>90</sub> &gt; A<sub>70</sub>]: b=-0.11, χ<sup>2</sup>(1)=83.00, <italic>p</italic>&lt;.0001; [A<sub>90</sub> &gt; AF<sub>50</sub>]: b=-0.25, χ<sup>2</sup>(1)=395.55, <italic>p</italic>&lt;.0001; [A<sub>90</sub> &gt; F<sub>70</sub>]: b=-0.37, χ<sup>2</sup>(1)=870.09, <italic>p</italic>&lt;.0001; [A<sub>90</sub> &gt; F<sub>90</sub>]: b=-0.42, χ<sup>2</sup>(1)=1075.91, <italic>p</italic>&lt;.0001). These expected effects were observed independently of groups and runs. We then observed a significant main effect of the factor <italic>run</italic> (χ<sup>2</sup>(3)=8.56, <italic>p</italic>&lt;.05) explained by the probability of an appropriate choice being higher for the base run as compared to the cTBS<sub>post</sub> runs (b=0.76, χ<sup>2</sup>(1)=7.38, <italic>p</italic>&lt;.001).</p><p id="P16">We furthermore observed a significant <italic>group</italic> x <italic>morphing</italic> interaction (χ<sup>2</sup>(5)=67.05, <italic>p</italic>&lt;.0001) explained by a better fit—namely, response probability matching the morphing percentage in each affective voice—of the choice probability of an anger choice for the EG compared to the CG for clear voices across all runs, namely A<sub>90</sub> and F<sub>90</sub> ([EG &gt; CG] x [A<sub>90</sub>]: b=0.35, χ<sup>2</sup>(1)=3.86, <italic>p</italic>&lt;.05; [EG &gt; CG] x [F<sub>90</sub>]: b=-0.41, χ<sup>2</sup>(1)=5.54, <italic>p</italic>&lt;.05) but not for the most ambiguous voices ([EG &gt; CG] x [AF<sub>50</sub>]: b=0.06, χ<sup>2</sup>(1)=0.14, <italic>p</italic>&gt;.10) or when comparing clear against ambiguous voices ([EG &gt; CG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>]: b=-0.51, χ<sup>2</sup>(1)=1.07, <italic>p</italic>&gt;.10).</p><p id="P17">Although the triple interaction for factors <italic>group</italic> x <italic>morphing</italic> x <italic>run</italic> was not significant (χ<sup>2</sup>(15)=13.60, <italic>p</italic>=.33), it however pertained to our main hypothesis of an impact of cTBS on ambiguous voice categorization. We therefore tested the contrast of a higher probability of an anger choice for the CG when categorizing most ambiguous as compared to least ambiguous voices—the inverse contrast yields to the same test with sign reversal of the difference—as a function of cTBS (contrast: [CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]) and found a significant effect (b=1.94, χ<sup>2</sup>(1)=5.24, <italic>p</italic>&lt;.05). This result shows that there is a greater classification probability difference between clear and ambiguous voices for the EG as compared to the CG for cTBS<sub>post</sub> vs. cTBS<sub>pre</sub> runs.</p><p id="P18">Finally, we tested the effect of right IFC cTBS on choice probabilities specifically on the most ambiguous affective voices between groups, namely those containing fifty percent of fear and anger. This contrast confirmed that cTBS on the right IFC led to a difference between to classify the most ambiguous affective voices—with the CG showing a higher probability of classifying these as 'anger' voices as compared to the EG ([CG &gt; EG] x [AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]; b=0.31, χ<sup>2</sup>(1)=6.05, <italic>p</italic>&lt;.05; see <xref ref-type="fig" rid="F1">Fig.1B</xref>). In other words, this triple interaction contrast illustrates an improvement for the EG compared to the CG in classifying clear as opposed to ambiguous affective speech as a function of cTBS. The last contrast focused specifically on the most ambiguous voices and also shows a fundamental between-group difference triggered by cTBS on the right IFG with the CG's probability of classifying AF<sub>50</sub> voices as 'anger' being higher than that of the EG. Participants of the EG also improved their classification of the most ambiguous voices. This first model on the probability of an anger choice explained 15.85% of the variance including both fixed and random effects (R2c= 0.1585) and 10.23% of the variance solely for fixed effects (R2m= 0.1023).</p><p id="P19">Reaction time data were analyzed in a separate linear mixed-effects model with reaction times used as the dependent variable (<xref ref-type="fig" rid="F3">Fig.3C</xref>), and we observed a main effect of <italic>morphing</italic> (χ<sup>2</sup>(5)=2126.79, <italic>p</italic>&lt;.0001). This was explained by several pairwise differences between morphing levels (see <xref ref-type="supplementary-material" rid="SD1">Table S3</xref>) and generally illustrating a linear decrease of reaction times as a function of the increasing percentage of anger in the voice stimuli. The main effect of <italic>run</italic> was also significant (χ<sup>2</sup>(3)=58.54, <italic>p</italic>&lt;.0001), showing overall slower reaction times for cTBS<sub>pre</sub> than cTBS<sub>post</sub> runs ([base run &gt; cTBS<sub>post</sub> run1]: b=550.07, χ<sup>2</sup>(1)=18.59, <italic>p</italic>&lt;.0001; [base run &gt; cTBS<sub>post</sub> run2]: b=861.96, χ<sup>2</sup>(1)=45.48, <italic>p</italic>&lt;.0001; [base run &gt; cTBS<sub>post</sub> run3]: b=833.80, χ<sup>2</sup>(1)=42.48, <italic>p</italic>&lt;.0001). The <italic>group</italic> x <italic>morphing</italic> and <italic>group</italic> x <italic>run</italic> interactions were also significant (χ<sup>2</sup>(5)=40.67, <italic>p</italic>&lt;.0001 and χ<sup>2</sup>(3)=8.27, <italic>p</italic>&lt;.05, respectively). The former was explained by faster reaction times for the CG compared to the EG to categorize neutral voices (b=-537.23, χ<sup>2</sup>(1)=5.71, <italic>p</italic>&lt;.05; all reaction times for neutral voices in <xref ref-type="table" rid="T1">Table 1</xref>), the latter by participants of the CG being faster than those of the EG to categorize voice stimuli of the cTBS<sub>pre</sub> compared to cTBS<sub>post</sub> run 1 (b=-296.61, χ<sup>2</sup>(1)=5.40, <italic>p</italic>&lt;.05).</p><p id="P20">The triple interaction of interest was not significant (χ<sup>2</sup>(15)=10.98, <italic>p</italic>=.75), but since it was part of our main behavioral hypothesis, we computed the contrasts of interest using planned comparisons testing the effect of group, morphing, and run. Again, we ran the [CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] triple interaction contrast and the [CG &gt; EG] x [AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast but none of them was significant (b=-440.48, χ<sup>2</sup>(1)=0.26, <italic>p</italic>&gt;.10 and b=38.00, χ<sup>2</sup>(1)=0.09, <italic>p</italic>&gt;.10, respectively). This second model explained 27.20% of the variance including both fixed and random effects (R2c=0.2720) and 10.12% of the variance solely for fixed effects (R2m=0.1012).</p></sec><sec id="S5"><title>Voice-sensitive activations in bilateral auditory cortex</title><p id="P21">To determine the regions in bilateral AC that are generally sensitive to voice compared to other types of auditory signals, we analyzed the data of a functional voice localizer scan specific to our sample. This voice localizer scan was specifically performed to spatially localize regions that might show specific increased activity for affective ambiguity expressed in voices in the main fMRI task. During this scan, participants listened to vocal and non-vocal sounds, and we found higher activity in bilateral STC and IFC (<xref ref-type="fig" rid="F3">Fig.3AB</xref>; <xref ref-type="supplementary-material" rid="SD1">Table S4</xref>) when contrasting vocal against non-vocal sounds. This pattern of activations underlying the neural processing of voice sounds is similar to previously reported activation patterns and referred to as the "voice areas" (<xref ref-type="bibr" rid="R12">Belin, Zatorre et al. 2000</xref>) (VA). IFC subregions are also delineated in the maps of <xref ref-type="fig" rid="F3">Fig.3</xref>.</p></sec><sec id="S6"><title>Functional brain activations for morphed voices classifications as a function of cTBS</title><p id="P22">We used the cortical definition of the VA to determine functional activations in the main experiment that were located inside as well as outside this general voice processing area. Imaging data were used to characterize brain mechanisms underlying the processing and classification of affective ambiguity using blended voices following cTBS over a specialized area, the right IFC.</p><p id="P23">Therefore, our contrast of interest was computed to uncover brain activity relating to [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] and its opposite [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>], per group first and then between groups. We found enhanced brain activity in the former but not in the latter contrast. In fact, the [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast yielded enhanced brain activity in the STC (within the temporal voice areas), bilateral IFC especially in the <italic>pars triangularis</italic> for the CG (<xref ref-type="fig" rid="F4">Fig.4BE</xref>) and EG (<xref ref-type="fig" rid="F4">Fig.4CF</xref>), separately.</p><p id="P24">Computing the three-way interaction between the factors <italic>group, morphing,</italic> and <italic>run</italic> revealed activity within the VA (<xref ref-type="fig" rid="F4">Fig.4A</xref>), located in anterior and mid STC as well as middle temporal cortex (MTC), and in superior parietal lobule and superior frontal gyrus only in the right hemisphere ([CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]; <xref ref-type="fig" rid="F4">Fig.4A</xref>). The inverse contrast ([CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]) and the ([CG &gt; EG] x [AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast did not yield any above-threshold brain activity. Peak coordinates and statistical information are reported in <xref ref-type="table" rid="T2">Table 2</xref>.</p></sec><sec id="S7"><title>Functional connectivity data for morphed voice classifications as a function of cTBS</title><p id="P25">Functional connectivity analyses were computed in order to assess the <italic>group</italic> x <italic>morphing</italic> x <italic>run</italic> interaction on the organization of functional networks of ambiguous voice classification. Functional connectivity analyses allow an inference of the association—in the present case, positive or negative linear bivariate correlations with the task-dependent generalized psychophysiological interaction method—between regions observed in the wholebrain results. Seed regions of interest (ROI) overlapping with results from studies targeting the decoding of affective voices in the lateral, superior temporal cortex (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>), medial temporal lobe (<xref ref-type="bibr" rid="R34">Frühholz, Trost et al. 2014</xref>) and more specifically the role of the IFC in vocal affect processing (<xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>) were selected (see Methods). We therefore ended up including eight ROI (bilateral IFC, bilateral anterior, mid and posterior STC, bilateral amygdala) in our seed-to-seed, generalized psychophysiological interaction analysis.</p><p id="P26">These analyses revealed anti-coupling in the right amygdala and left mid STC (<xref ref-type="fig" rid="F5">Fig.5AB</xref>) as well as coupling in the left amygdala and right IFC (<xref ref-type="fig" rid="F5">Fig.5BC</xref>) triggered by cTBS on the right IFC ([EG &gt; CG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast). This result indicated that when the EG as compared to the CG classified clear as opposed to ambiguous voices as a function of the cTBS procedure, linear negative correlation between left mid STC and right amygdala increased. On the other hand, linear association between the left amygdala and right IFC (the cTBS target region for the EG) increased. The cTBS procedure on the right IFC therefore seems to enhance functional connectivity between the bilateral amygdala and subparts of the VA and IFC. Since functional connectivity data were computed using bivariate correlations between neural nodes, inverse contrasts ([EG &gt; CG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] or [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]) yielded to a sign inversion (coupling in the right amygdala and left mid STC; anti-coupling in the left amygdala and right IFC) and were therefore not illustrated.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P27">The present study had the general aim of understanding the causal role of the inferior frontal cortex in classifying and representing speech at different levels of affective clearness and ambiguity. A combined cTBS-fMRI procedure was therefore used to alter activity in the right IFG <italic>pars triangularis</italic> for half of our participants (EG), while the other participants (CG) received cTBS over a control region, namely the vertex, which was supposed to not alter any neural activity relevant for affective sound processing and classification (<xref ref-type="bibr" rid="R55">Jung, Bungert et al. 2016</xref>). The cTBS procedure targeting the IFC led to an altered functioning of this region and affected decisional processes of the participants during the affect classification task. This target region was chosen because it was found active during the processing of affective voices in the CG, and it is a commonly found activation peak across many studies on voice processing, categorization, and classification (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>, <xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>, <xref ref-type="bibr" rid="R76">Suran, Rumiati et al. 2019</xref>, <xref ref-type="bibr" rid="R42">Grandjean 2020</xref>). The cTBS procedure had an impact on both behavioral decisions and cerebral processing. First, we found that cTBS on the right IFC improved voice classification, specifically for ambiguous affective voices. Second, IFC cTBS highlighted a reduced recruitment of STC regions for clear as opposed to ambiguous voices. Third, functional connectivity analyses for clear versus ambiguous voices revealed between-group differences, such that IFC cTBS led to anti-coupling between the mid STC and right amygdala and enhanced coupling between left amygdala and right IFC, the latter region being the target of the cTBS procedure in the EG.</p><p id="P28">The morphing procedure used to create affective ambiguity allowed us to assess if neural dynamics in IFC and connected regions would either represent affective voice classification challenges (as induced by ambiguous affective voices) or represent categorical affective certainty (as induced by clear affective voices). According to the first perspective, we hypothesized that IFC alteration by the cTBS procedure would potentially affect the classification of ambiguous affective voices, namely those with a blend of 50% anger and fear, which would represent the decisional difficulty of this condition. According to the second view, we hypothesized that altering IFC functioning using cTBS would affect the certainty of clear affective voice categorization, namely voices expressing mostly anger (90% anger and 10% fear voices) or mostly fear (90% fear and 10% anger voices). While these perspectives could be in opposition to each other, their mutual existence should also be considered in the IFC. In fact, fine-grained IFC mechanisms underlying the classification of ambiguous as opposed to clear affective voices could encompass both decisional difficulty and categorization certainty, respectively. This integrated view of the complementary roles and functions of the IFC in vocal emotion classification could also help explain research with unexpected categorization results—as well as the absence of expected results, for instance when altering left or right IFC using rTMS (<xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>).</p><p id="P29">Given the abovementioned perspectives of the critical role(s) of the IFC as underlying behavioral decisions on and/or representations of affective voices, we first investigated the behavioral effects of cTBS on the right IFC, specifically the right IFGtri. We found a modulation of reaction times during the vocal affect decision task, but most importantly we found an influence on affect classification probabilities. These results highlight the direct impact of altered IFC functioning following cTBS, a region that seems on the one hand associated with the cognitive evaluation or judgement of affective voices (<xref ref-type="bibr" rid="R35">Frühholz and Grandjean 2012</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>, <xref ref-type="bibr" rid="R77">Swanborough, Staib et al. 2020</xref>) while the IFC is also known to be sensitive to nonverbal vocalizations in both children (<xref ref-type="bibr" rid="R44">Grossmann, Oberecker et al. 2010</xref>) and nonhuman primates (<xref ref-type="bibr" rid="R67">Romanski and Averbeck 2009</xref>), supporting sound classifications (<xref ref-type="bibr" rid="R19">Cohen, Hauser et al. 2006</xref>) and higher-order auditory representations (<xref ref-type="bibr" rid="R18">Cohen, Russ et al. 2009</xref>) on the other hand. Our reaction time data especially revealed that cTBS on the right IFC leads to faster reactions in the experimental run immediately after the application of cTBS for the EG. The neural effects of cTBS are usually largest immediately after the application, and the effects are known to decay over time—they can last up to 60 minutes after this specific type of patterned stimulation (<xref ref-type="bibr" rid="R51">Huang, Edwards et al. 2005</xref>). If the IFC is assumed to be a neural node for lifting affective voice processing on a cognitive-focused level (<xref ref-type="bibr" rid="R27">Ethofer, Anders et al. 2006</xref>, <xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>, <xref ref-type="bibr" rid="R15">Brück, Kreifelts et al. 2011</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>) that demands processing efforts (<xref ref-type="bibr" rid="R81">Verbruggen, Aron et al. 2010</xref>)—as observed in spoken word recognition and phonetic competition (<xref ref-type="bibr" rid="R87">Zhuang, Tyler et al. 2012</xref>, <xref ref-type="bibr" rid="R66">Rogers and Davis 2017</xref>, <xref ref-type="bibr" rid="R85">Xie and Myers 2018</xref>), inhibition of the IFC with cTBS might loosen this cognitive focus to facilitate more intuitive processing (<xref ref-type="bibr" rid="R86">Zander, Horr et al. 2016</xref>) with presumably better processing efficiency (<xref ref-type="bibr" rid="R24">Dippel and Beste 2015</xref>). Our classification choice probability data might also point in a similar direction. We especially observed a better classification probability for the clearest affective voices after cTBS in the EG group compared to the CG. After cTBS, affective decisions of participants much more follow the quantity of sensory acoustic evidence. This might indicate that these decisions are more sensory driven rather than driven by high-level cognitive processing. This also indicates that the IFC normally seems indeed a neural node for high-level cognitive evaluation of sound information and affective values of voice signals but seems to flag the outcome of such evaluation with more categorical uncertainty, especially for more complex affective categorization tasks. These observations together would suggest a mixed functional role of the IFC according to the two primary hypotheses on the IFC: the IFC would both compute cognitive evaluation on socio-affective voice information, but it would also tag such information with flags of affective uncertainty and decisional precaution.</p><p id="P30">The relevance of this IFC subregion as targeted by our cTBS procedure for complex affective evaluation and classification tasks is highlighted by previous studies. The IFC is a rather large region (<xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>) and it includes several subregions or subparts such as its more superior part, <italic>pars opercularis,</italic> the more inferior and anterior part, <italic>pars triangularis</italic> and the most ventral part, <italic>pars orbitalis</italic> that is located next to the orbitofrontal cortex. In our study, the location of the IFC was determined <italic>a priori</italic> to serve as cTBS target region and it was exclusively within the <italic>pars triangularis</italic> in the right hemisphere. In another study, this specific subregion of the right IFC was recruited when more complex affective voice categorizations as opposed to simpler discrimination was performed, also sometimes labelled 'unbiased' versus 'biased' perceptual decision making, respectively (<xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R45">Gruber, Debracque et al. 2020</xref>). Such results are therefore in line with our data, especially when considering the fact that an affect classification task was employed in our procedure. Affective voice discrimination as opposed to categorization would on the other hand depend on a different IFG subregions, namely the bilateral IFG <italic>pars opercularis</italic> (<xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>), to which only residual TMS current could be transmitted due to the cTBS procedure and coil location in our study (see Methods). According to our results and to the literature (<xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>, <xref ref-type="bibr" rid="R25">Dricu, Ceravolo et al. 2017</xref>, <xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>, <xref ref-type="bibr" rid="R45">Gruber, Debracque et al. 2020</xref>), the right and potentially the left IFG <italic>pars triangularis</italic> (<xref ref-type="bibr" rid="R60">Lupyan, Mirman et al. 2012</xref>) could therefore be highly selective to categorization, by flagging the outcome of cognitive-driven affective evaluation processes with some level of uncertainty (<xref ref-type="bibr" rid="R79">Toelch, Bach et al. 2013</xref>, <xref ref-type="bibr" rid="R62">Nastase, Iacovella et al. 2014</xref>).</p><p id="P31">In a more general sense, the IFC was previously also causally linked to social perception and affective processing in social groups. In fact, repetitive TMS over the left IFC revealed perturbed social perception while social cognition was preserved, with slower reaction times observed for emotion recognition (<xref ref-type="bibr" rid="R57">Keuken, Hardie et al. 2011</xref>). A similar TMS procedure targeting the <italic>pars opercularis</italic> of the left IFG led to faster categorization of negative social groups and disrupted semantic priming for negative words, as opposed to TMS over the vertex (<xref ref-type="bibr" rid="R76">Suran, Rumiati et al. 2019</xref>). These two studies emphasize the importance of the target location for a TMS procedure—and add nuance to IFC subparts functioning and specificity—by pointing toward slower or faster responses according precise IFC disruption in the <italic>pars orbitalis</italic> (<xref ref-type="bibr" rid="R57">Keuken, Hardie et al. 2011</xref>) or <italic>opercularis</italic> (<xref ref-type="bibr" rid="R76">Suran, Rumiati et al. 2019</xref>), respectively.</p><p id="P32">Considering our neuroimaging data and when looking at groups separately (CG, EG), bilateral IFG <italic>pars triangularis</italic> activity was observed specifically for clear as compared to ambiguous voices in both groups separately, but not in the inverse contrast. This observation seems contrary to the hypothesis of the IFC being pre-dominantly a brain node coding and regulating decisional challenges especially during sensory ambiguity. The IFC is active when making socio-affective decisions on voice signals, but not simple acoustic decisions (<xref ref-type="bibr" rid="R68">Roswandowitz, Swanborough et al. 2020</xref>), which would again point to the notion of the IFC of representing affective categorical information of voice signals, but maybe flagged with categorical uncertainty and choice precaution (<xref ref-type="bibr" rid="R79">Toelch, Bach et al. 2013</xref>, <xref ref-type="bibr" rid="R62">Nastase, Iacovella et al. 2014</xref>). This might concern especially the right IFC, but some bilateral interaction of homologue areas in left and right IFC could be expected, as we revealed lower bilateral IFC activity in the EG group. The bilateral functional significance of the IFC could be influenced solely by right hemisphere cortical interference through cTBS, as already shown using TMS on the left prefrontal cortex (<xref ref-type="bibr" rid="R61">Nahas, Lomarev et al. 2001</xref>). Such assumptions should however be tested in the future either by including a group of participants undergoing cTBS over the left IFC or by using intermittent TBS, which is known to enhance brain activity and not alter it (<xref ref-type="bibr" rid="R3">Annika, Alia et al. 2010</xref>, <xref ref-type="bibr" rid="R63">Oberman, Edwards et al. 2011</xref>), over the right and/or left IFG <italic>pars triangularis.</italic> Such assumption would however go against inter-hemispheric plasticity, as observed in speech production with right IFC recruitment following left IFC alteration (<xref ref-type="bibr" rid="R47">Hartwigsen, Saur et al. 2013</xref>), but it seems that the impact of stimulation intensity could have a crucial impact on current transmission in the brain tissues (<xref ref-type="bibr" rid="R61">Nahas, Lomarev et al. 2001</xref>).</p><p id="P33">We designed an experimental task that would specifically test the causal role of the IFC in classifying ambiguous and/or clear affective speech and flagging the decision with some weighting of uncertainty, but numerous other functions of this brain region have been proposed and some of them could apply to our data (<xref ref-type="bibr" rid="R59">Liakakis, Nickel et al. 2011</xref>). For instance, behavioral classifications imply not only categorical associations but also action and motor functioning as well as executive functions, most notably inhibition. Inhibition was shown to causally rely on the right IFC, with direct current stimulation over the IFC causing an 'activation of inhibition' (<xref ref-type="bibr" rid="R53">Jacobson, Javitt et al. 2011</xref>). Such topic was also reviewed in detail and while the distinct role of prefrontal cortex subregions was initially questioned (<xref ref-type="bibr" rid="R4">Aron, Robbins et al. 2004</xref>), the specific role of the IFC would be to implement some cautionary mechanisms over immediate response tendencies (<xref ref-type="bibr" rid="R5">Aron, Robbins et al. 2014</xref>). According to this literature and to our data, it is therefore possible that cTBS over the right IFG <italic>pars triangularis</italic> enabled the release of inhibition in the EG. Response inhibition and inhibition of immediate categorical decisions might have appeared in the CG, leading to more conservative decisions and a more cognitive-driven processing especially concerning clear affective voices. In the EG, cTBS might have led to a release of inhibition and maybe to a switch from a cognitive to a more intuition-based processing of affective speech. These two processing modes of a cognitive-driven and intuition-driven processing of sensory information have been documented in the literature (<xref ref-type="bibr" rid="R41">Gilovich, Griffin et al. 2002</xref>, <xref ref-type="bibr" rid="R56">Kahneman 2002</xref>) and are potentially on a temporal continuum (<xref ref-type="bibr" rid="R50">Hogarth 2010</xref>), with intuition-driven processing supposed to be more efficient (<xref ref-type="bibr" rid="R46">Hammond, Hamm et al. 1987</xref>), especially when relying on prior expertise (<xref ref-type="bibr" rid="R22">Dane, Rockmann et al. 2012</xref>). A higher processing efficiency of clear affective speech in our study might be indicated by a significantly lower activity in anterior STC in the EG. The anterior STC is a major brain node in the analysis of affective speech (<xref ref-type="bibr" rid="R28">Ethofer, Bretscher et al. 2012</xref>, <xref ref-type="bibr" rid="R40">Frühholz, Trost et al. 2016</xref>), and shows functional (<xref ref-type="bibr" rid="R28">Ethofer, Bretscher et al. 2012</xref>) and structural (<xref ref-type="bibr" rid="R29">Ethofer, Bretscher et al. 2013</xref>, <xref ref-type="bibr" rid="R37">Frühholz, Gschwind et al. 2015</xref>) connections to the right IFC. Especially, the between-group contrasts highlighted above-threshold voxels in the right mid-to-anterior STC for CG compared to EG, especially when comparing the cTBS-dependent classification of clear as opposed to ambiguous voices—but not for the inverse contrast. The role of the STC in affective speech decoding is well documented (<xref ref-type="bibr" rid="R43">Grandjean, Sander et al. 2005</xref>, <xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>, <xref ref-type="bibr" rid="R28">Ethofer, Bretscher et al. 2012</xref>, <xref ref-type="bibr" rid="R34">Frühholz, Ceravolo et al. 2012</xref>, <xref ref-type="bibr" rid="R84">Witteman, Van Heuven et al. 2012</xref>, <xref ref-type="bibr" rid="R16">Ceravolo, Frühholz et al. 2016</xref>, <xref ref-type="bibr" rid="R38">Frühholz and Schweinberger 2020</xref>, <xref ref-type="bibr" rid="R42">Grandjean 2020</xref>) and among this vocal emotion literature, some studies isolated activity enhancement in the right anterior STC for the categorization of female-ambiguous voices by male participants (<xref ref-type="bibr" rid="R73">Sokhi, Hunter et al. 2005</xref>). Right anterior STC activity was also enhanced as a function of trial-level acoustic distance between voices when categorizing morphed male and female voices to create gender-ambiguous stimuli (<xref ref-type="bibr" rid="R17">Charest, Pernet et al. 2013</xref>). Perceived gender ambiguity did however recruit the bilateral IFG and the posterior and anterior cingulate cortex. These results were interpreted by the authors as a two-stage process for gender voice classification, with auditory feature extraction in the anterior STC and voice categorization in the bilateral IFG, in a similar fashion described by Schirmer and Kotz for processing and making decisions on affective speech (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>). While this interpretation was focused on categorizing gender and gender-ambiguity, it is well suited to our results even though our morphing procedure targets vocal emotion. It also means that mid and anterior STC neurons have the ability to influence classification and decision processes in the IFC and that at some point altered functioning in the IFC—as caused by cTBS in our study for the EG—could be overcome or at least reduced by right anterior STC activity and its feature-selectivity function.</p><p id="P34">Our functional connectivity results also highlight the importance of the limbic system for processing affective speech. The right amygdala showed decreased functional connectivity with the left STC in the EG vs. CG for the classification of clear and ambiguous voices, while the left amygdala was significantly anti-coupled with the right IFC. These results are coherent with existing literature on the brain networks of vocal affect processing (<xref ref-type="bibr" rid="R34">Frühholz, Ceravolo et al. 2012</xref>, <xref ref-type="bibr" rid="R34">Frühholz, Trost et al. 2014</xref>) and argue for a wider role of the limbic system in affective voice classification (<xref ref-type="bibr" rid="R34">Frühholz, Trost et al. 2014</xref>) and feature extraction (<xref ref-type="bibr" rid="R64">Pannese, Grandjean et al. 2016</xref>) depending on sensory ambiguity of the voice signal. In fact, automatic, non-voluntary, and presumably intuitive processing of vocal affect, especially anger (<xref ref-type="bibr" rid="R43">Grandjean, Sander et al. 2005</xref>, <xref ref-type="bibr" rid="R70">Sander, Grandjean et al. 2005</xref>), would take place in the amygdala following which feature extraction would be completed in the STC (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>). According to Schirmer and Kotz (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>), the last of the three-stage process of vocal affect processing would consist of an evaluative process in the IFC, allowing the participants to make a decision. For the categorization of affective speech ambiguity, our results show that such final stage is enhanced by temporary interference in the right IFC with coupling occurring between the left amygdala and right IFC in the EG compared to the CG. This observation suggests a potential enhancement in feedback generation from automatic processing—in the amygdala—and final decision leading to categorization, taking place in the right IFC.</p><p id="P35">In conclusion, our data might provide a more detailed and presumably mechanistic picture about the functional role of the IFC in social sound cognition and especially in voice signal classification along socio-affective dimensions. While previous research has suggested partly opposing roles of the IFC either for coding decisional challenges for difficult sound classifications or for high-level sound and voice categorical representations, the mechanistic functional role of the IFC might fall in-between these two perspectives. When applying cTBS to the right IFC in humans, their decisional responses on clear and ambiguous affective speech become faster immediately after cTBS, they follow more closely sensory evidence especially for clear voices, they show significantly lower activity in the auditory cortex, and they show higher fronto-limbic connectivity. Taken together, these data suggest that inhibition of the IFC leads to a more intuitive—and potentially more efficient—processing mode in the EG compared to a more cognitive- and analysis-driven processing in the CG. In terms of an involvement of the IFC in normal affective speech processing, the IFC might implement a cognitive processing mode that represents affective categories with more caution, and this categorical and representational caution might be shared with the anterior STC and the amygdala as major nodes in the neural affective speech analysis machinery.</p></sec><sec id="S9" sec-type="materials | methods"><title>Material and Methods</title><sec id="S10" sec-type="subjects"><title>Participants</title><p id="P36">Forty healthy volunteers took part in the fMRI study. It was shown that 24 participants for an fMRI study accounted for at least 80% of the variance at the voxel level, as estimated for a conservative [0.01&lt;α&lt;0.05] alpha (<xref ref-type="bibr" rid="R23">Desmond and Glover 2002</xref>). Sample size was therefore calculated using G*Power 3 software (<xref ref-type="bibr" rid="R31">Faul, Erdfelder et al. 2007</xref>) with standard values for the architecture of our study (power of 0.95 and alpha of 0.05) and 80% of explained variance was reached with a sample of 20 participants per group.</p><p id="P37">We hence recruited 40 participants matching our inclusion criteria. Half of them were randomly assigned to the control group (CG) while the remaining participants were assigned to the experimental group (EG). For each group, 10 female and 10 male participants were included. Five participants were excluded from the final sample due to corrupted log files (N=2) and the impossibility to determine a reliable motor threshold (N=3). Therefore, the final groups included 18 participants for the CG (mean age=22.35, SD=1.66, 8 female) and 17 participants for the EG (mean age=25.05, SD=5.18, 7 female) with no significant between-group age difference (F(1,34)=3.66, <italic>p</italic>=.065). Participants were informed of all aspects of the experiment before giving their informed written consent to take part in the study. They were also informed that they could abort the experiment and quit the study at any time without justification. The study was conducted according to the Declaration of Helsinki and approved by the Ethics Committee of the University of Zurich, Switzerland.</p></sec><sec id="S11"><title>Vocal affect categorization task</title><p id="P38">Our stimuli consisted of voice utterances ("Aah") from the Montreal Affective Voices database (<xref ref-type="bibr" rid="R11">Belin, Fillion-Bilodeau et al. 2008</xref>). Voices were pronounced by five female and five male actors. They expressed either a neutral emotional tone or a mix of anger and fear with a varying percentage of each emotion (percentage of anger/fear): 90/10, 70/30, 50/50, 30/70, 10/90 (<xref ref-type="fig" rid="F1">Fig.1</xref>), respectively labelled A<sub>90</sub>, A<sub>70</sub>, AF<sub>50</sub>, F<sub>70</sub>, F<sub>90</sub> in the text. Each voice was morphed using the same actor to avoid creating a strange identity that would add a critical confound to the emotional morphing procedure.</p><p id="P39">For each run of the vocal affect categorization task, we therefore had a total of five conditions of interest involving a morphing of anger and fear emotions (24 trials each) and one control condition involving neutral voices only (10 trials). Each voice had a duration of 500 ms to 1200 ms, the order of which being pseudo-randomized for each run and participant (<xref ref-type="fig" rid="F1">Fig.1A</xref>). Each run included 24 trials of each conditions of interest for a grand total of 72 trials and 30 neutral voice trials across the three experimental runs and the control run.</p><p id="P40">The experiment took place at the University Hospital of Zurich, Switzerland, using the research-dedicated whole-body magnetic resonance imaging (MRI) scanner (Philips Achieva 3T) of the Laboratory for Social and Neural Systems Research (Department of Economics, University of Zürich, Zürich, Switzerland). Participants were first taken to a transcranial magnetic stimulation (TMS) room in order to determine their individual motor threshold through a standard procedure (see TMS procedure below). They were then taken to the MRI room and comfortably installed in the scanner. The MRI session started with one base run, followed by the TMS procedure and the three experimental runs (<xref ref-type="fig" rid="F1">Fig.1A</xref>). At the end of the control run, the participants stayed on the scanner table. The table was taken halfway out and the TMS apparatus was brought to the participant. For the CG, continuous theta burst stimulation (cTBS) was administered for 40 seconds over the vertex (MNI <italic>xyz</italic> [0, -20, 80]) because it was previously successfully used as a good control site for TMS protocols(<xref ref-type="bibr" rid="R55">Jung, Bungert et al. 2016</xref>). For the EG, cTBS was administered to the right inferior frontal cortex (rIFC; MNI <italic>xyz</italic> [54, 34, 10]), for 40 seconds as well (see detailed TMS procedure below). As soon as the TMS procedure ended (for both groups), necessary care was taken to relocate the participant as fast and as smoothly as possible inside the scanner magnet. This procedure was used to minimize the time between stimulation and the start of the first experimental run, in order to have the most reliable TMS effect over the rIFC for the following runs across participants. The average duration between the end of the TMS procedure and the start of the first experimental run was 3.33 minutes (SD=0.47) for the CG and 3.00 minutes (SD=0.52) for the EG. No statistical difference was observed regarding this duration between groups (F(1,34)=0.33, <italic>p</italic>=.57).</p><p id="P41">For all runs, the task of the participants was to explicitly categorize the emotional tone of the voice that was presented in each trial by a key press using three buttons on an MRI-compatible response box (Current Designs Inc., PA, USA). Button mapping was fully randomized between participants. Buttons represented the following options: Anger, Fear, Neutral. Participants were instructed to respond as fast and as accurately as possible during the 2 sec blank screen that appeared after each trial (see <xref ref-type="fig" rid="F1">Fig.1A</xref>).</p></sec><sec id="S12"><title>Behavioral data analysis</title><p id="P42">To take into account within- and between-subject variance for the initial stimulus evaluation and for the main cTBS-fMRI task—for the dependent variable of each model—we used lmerTest (Kuznetsova, Brockhoff et al.) and lme4 packages (<xref ref-type="bibr" rid="R10">Bates, Maechler et al. 2014</xref>) in R Studio (<xref ref-type="bibr" rid="R78">Team 2014</xref>) to perform mixed-effects modelling of the data. For all analyses, models were tested using type II Wald Chi-square tests using the 'Anova' function of the 'car' package (<xref ref-type="bibr" rid="R33">Fox, Friendly et al. 2007</xref>). We report effect sizes using the 'MuMIn' (<xref ref-type="bibr" rid="R9">Barton and Barton 2015</xref>) package based on two indicators, a marginal and a conditional R2 (R2m and R2c, respectively). R2m reflects the variance explained by the fixed factors, R2c the variance explained by the entire model (both fixed and random effects).</p><sec id="S13"><title>Initial stimulus evaluation and selection</title><p id="P43">For the initial evaluation of the stimuli including all possible morphing levels in steps of 10% as evaluated by a sample of nineteen participants, we used a generalized linear mixed-effects model ('glmer') for voice categorization. In this model, the dependent variable was the raw emotion categorization response of our sample of participants, with '1' coding an 'anger' response and '2' a 'fear' response. Fixed effects included the Morphing factor with eleven morphing levels of the voice stimuli (anger/fear percentage: 100/0, 90/10, 80/20, 70/30, 60/40, 50/50, 40/60, 30/70, 20/80, 10/90, 0/100) with N=10 trials per morphing level (N=110 trials per participant). Random effects included in this order: Participant, Age, Sex, Stimulus identity, Stimulus speaker sex. The same model was used to analyze reaction time data—these were of no interest for stimulus selection—with reaction times as dependent variable. All reported <italic>p</italic>-values of these analyses use a Bonferroni correction for multiple comparisons implemented in package 'lmerTest'.</p></sec><sec id="S14"><title>Vocal affect categorization task</title><p id="P44">For the main cTBS-fMRI task, mixed-effects modelling of both the responses (model 1, binomial modelling, generalized linear mixed-effects model 'glmer') and reaction times (model 2, linear modelling, linear mixed-effects model 'lmer') of our participants were computed, for each trial. Data included our conditions of interest, namely the 90/10, 70/30, 50/50, 30/70, 10/90 anger/fear morphed voices—labelled A<sub>90</sub>, A<sub>70</sub>, AF<sub>50</sub>, F<sub>70</sub>, F<sub>90</sub>, respectively, in the manuscript— and the control voices expressing neutral content (no morphing, filler condition of no-interest). For reaction times data and since raw values were not normally distributed, the log of the reaction times was used. For model 1, raw responses were used to determine the accuracy for an anger response for each trial, in a binomial manner. This binomial variable—coded '1' for correct and '0' for incorrect responses—was the dependent variable while the log of the reaction times was the dependent variable for model 2. For both models, fixed effects included, in this order, the interaction between Group, Morphing and Run while Participant, Age and Sex were introduced as random effects. These models allowed us to test our hypothesis according to which participants of the CG would perform significantly better at categorizing morphed voices, taking into account the TMS procedure: [CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] and its inverse regarding morphing [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]. For these two contrasts, the weighting of the vector for the Morphing factor was similar to the one used for fMRI data, namely [6*(AF<sub>50</sub>) &gt; -2*(A<sub>90</sub>), -1*(A<sub>70</sub>), - 2*(F<sub>90</sub>), -1*(F<sub>70</sub>)] and [-6*(AF<sub>50</sub>) &gt; 2*(A<sub>90</sub>), 1*(A<sub>70</sub>), 2*(F<sub>90</sub>), 1*(F<sub>70</sub>)]. A contrast targeting specifically the most ambiguous voices: [CG &gt; EG] x [AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]. All reported <italic>p</italic>-values of these behavioral analyses use a Bonferroni correction for multiple comparisons implemented in package 'lmerTest'.</p></sec></sec><sec id="S15"><title>Voice-sensitive areas localizer task</title><sec id="S16"><title>Stimuli</title><p id="P45">Auditory stimuli consisted of sounds from a variety of sources. Vocal stimuli were obtained from 47 speakers: 7 babies, 12 adults, 23 children and 5 older adults. Stimuli included 20 runs of vocal sounds and 20 runs of non-vocal sounds. Vocal stimuli within a run could be either speech 33%: words, non-words, foreign language or non-speech 67%: laughs, sighs, various onomatopoeia. Non-vocal stimuli consisted of natural sounds 14%: wind, streams, animals 29%: cries, gallops, the human environment 37%: cars, telephones, airplanes or musical instruments 20%: bells, harp, instrumental orchestra. The paradigm, design and stimuli were obtained through the Voice Neurocognition Laboratory website (<ext-link ext-link-type="uri" xlink:href="http://vnl.psy.gla.ac.uk/resources.php">http://vnl.psy.gla.ac.uk/resources.php</ext-link>). Stimuli were presented at an intensity that was kept constant throughout the experiment 70 dB sound-pressure level.</p></sec><sec id="S17"><title>Experimental procedure, paradigm</title><p id="P46">Participants were instructed to actively listen to the sounds, both vocal and non-vocal. The distinction between vocal and non-vocal runs was not revealed to the participants who were therefore naïve to run organization and timing. The silent inter-run interval between each run of either vocal or non-vocal was 8s long. Task duration was about 10 minutes in total. Wholebrain, sample-specific result outline of this task for the vocal &gt; non-vocal contrast of interest are reported in <xref ref-type="fig" rid="F3">Fig.3</xref> and voice areas are outlined in black in <xref ref-type="fig" rid="F4">Fig.4</xref>.</p></sec></sec><sec id="S18"><title>TMS procedure</title><p id="P47">As aforementioned, participants were stimulated either over the rIFC (EG) or over the vertex (CG) by means of standard cTBS (<xref ref-type="bibr" rid="R51">Huang, Edwards et al. 2005</xref>). First, we determined the active motor threshold (aMT) individually for each participant by stimulating the primary motor cortex in the left hemisphere. The aMT was defined as the percent of maximum stimulator output (mean intensity was 49.3%, SD 6.52%) required to elicit a motor-evoked potential larger than 200μV from the contralateral first dorsal interosseous muscle in five out of ten TMS pulses. During the determination of the aMT the participants exerted a constant pressure between the index finger and the thumb of about 20% of the maximum force (<xref ref-type="bibr" rid="R51">Huang, Edwards et al. 2005</xref>). For the cTBS protocol in the MRI room, the stimulation intensity was set to 80% of the aMT (mean intensity was 39.94%, SD 5.21%). Due to unwanted muscle contraction and discomfort at the right IFGtri location, percentage of aMT for the EG was further reduced by 10%, leading to lower amplitude stimulation for the EG compared to the CG in the MRI scanner (CG: mean intensity=42.84, SD=4.78; EG: mean intensity=36.87, SD=3.75; F(1,34)=16.80, <italic>p</italic>&lt;.001).</p><p id="P48">On the bed of the MRI scanner, the participants received cTBS with an MR-compatible coil (MRi-B91 coil, MagVenture A/S, Farum, Denmark). Stimulation site and coil orientation (see TMS site localization; <xref ref-type="fig" rid="F2">Fig.2</xref>) were marked on a fixed cap by means of a TMS Neuronavigation system (BrainSight 2, Rogue Research Inc., Canada). The cTBS stimulation protocol comprised bursts of 3 stimuli at 50Hz that were repeated with a frequency of 5Hz for 40s, resulting in a total of 600 pulses. The implemented cTBS protocol is thought to reduce the excitability of the stimulated brain region for about 60min (<xref ref-type="bibr" rid="R51">Huang, Edwards et al. 2005</xref>).</p></sec><sec id="S19"><title>TMS site localization</title><p id="P49">We determined the stimulation sites using individual T1-weighted structural scans and TMS Neuronavigation system (BrainSight 2, Rogue Research Inc., Canada). We used data from the CG—acquired before those of the EG—to define the coordinates of the rIFC (MNI <italic>xyz</italic> 54 34 10), coordinates that also corresponded to those observed in a previous study on the role of the rIFC for emotional judgments (<xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>) and this location also overlapped with vocal judgments in general (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>) and vocal emotion processing in the inferior frontal gyrus (<xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>). For each participant, we transformed the rIFC peak coordinates into the native space of the individual structural scan using the parameter estimates for spatial normalization of the anatomical scan performed in SPM12. The TMS coil was positioned tangentially to the cortical surface over the rIFG, with the handle perpendicular to the rIFC. As a control site we used the vertex, which was defined as the meeting point of the pre- and post-central sulcus in the interhemispheric fissure. For the control group, the TMS coil was positioned tangentially to the cortical surface over vertex (MNI <italic>xyz</italic> [0, -20, 80]), with the handle pointing in a posterior direction (see <xref ref-type="fig" rid="F2">Fig.2D</xref>).</p></sec><sec id="S20"><title>MRI data acquisition</title><p id="P50">Imaging data acquisition was performed at the Laboratory for Social and Neural Systems research of the University of Zürich, on a Philips Achieva 3T whole-body scanner equipped with an eight channel MR head coil. Four runs of 10 min each were collected for each participant (1 base run, 3 experimental runs). Each run contained 200 volumes (voxel size = 3 x 3 x 3mm3, 0.5mm gap, matrix size = 80 x 80, TR/TE = 2100/30ms, flip angle = 79, parallel imaging factor = 1.5, 35 slices acquired in ascending order for full coverage of the brain). High-resolution T1-weighted 3D turbo field echo structural scans were acquired and used for image registration and normalization (181 sagittal slices, matrix size = 256 x 256, voxel size = 1mm3, TR/TE/TI = 8.3/2.26/181ms).</p></sec><sec id="S21"><title>MRI data analysis</title><p id="P51">Functional images were analyzed with Statistical Parametric Mapping software (SPM12, Wellcome Trust Centre for Neuroimaging, London, UK, <ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). Preprocessing steps included realignment to the first volume of the time series, slice timing, normalization to the Montreal Neurological Institute (MNI; Collins, Neelin et al.) space using the DARTEL toolbox (<xref ref-type="bibr" rid="R6">Ashburner 2007</xref>) and spatial smoothing with an isotropic Gaussian filter of 8mm full width at half maximum. To remove low frequency components, we used a high-pass filter with a cutoff frequency of 128s. Anatomical locations were defined with a standardized MNI coordinate database using xjView toolbox (<ext-link ext-link-type="uri" xlink:href="https://www.nitrc.org/projects/xjview">https://www.nitrc.org/projects/xjview</ext-link>).</p><sec id="S22"><title>Voice-sensitive areas localizer task</title><p id="P52">For the voice-sensitive areas localizer task, a general linear model was used to compute first-level statistics, in which each run was modelled by using a run function and was convolved with the hemodynamic response function, time-locked to the onset of each run. Separate regressors were created for each condition (vocal and non-vocal; Condition factor). Finally, six motion parameters were included as regressors of no interest to account for movement in the data. The Condition regressors were used to compute simple contrasts for each participant, leading to a main effect of vocal and non-vocal material at the first-level of analysis [1 0] for vocal, [0 1] for non-vocal. These simple contrasts were then taken to a flexible factorial second-level analysis in which there were two factors: the Participants factor with independence set to yes, variance set to unequal and the Condition factor with independence set to no, variance set to unequal.</p></sec><sec id="S23"><title>Vocal affect categorization task</title><p id="P53">For the experimental runs and the base run of the main task, we used a first-level general linear model, in which each stimulus display was modelled by using a stick function and was convolved with the hemodynamic response function. Events were time-locked to the onset of the voice stimuli. Separate regressors were created for each condition of interest (five conditions with percentage of anger/percentage of fear: 90/10, 70/30, 50/50, 30/70, 10/90 or respectively A<sub>90</sub>, A<sub>70</sub>, AF<sub>50</sub>, F<sub>70</sub>, F<sub>90</sub>) and for neutral voices (regressor of no-interest). We thus had five regressors of interest including 24 trials each per run (base and experimental runs, respectively cTBS<sub>pre</sub> and cTBS<sub>post</sub> runs) and 72 trials each in total for the 3 experimental runs, in addition to the neutral voice condition as non-interest regressors (10 trials per run, 40 trials in total). Moreover, six motion parameters were included as regressors of no interest to account for movement in the data. Our design matrix was therefore as follows: Anger/Fear 90/10 (A<sub>90</sub>), Anger/Fear 70/30 (A<sub>70</sub>), Anger/Fear 50/50 (AF<sub>50</sub>), Anger/Fear 30/70 (F<sub>70</sub>), Anger/Fear 10/90 (F<sub>90</sub>), Neutral, Movement parameters, Constant term; 11 columns in total per run. We therefore had four sessions per model (base run, experimental run 1, 2, 3), leading to 44 columns in the design matrix. Each regressor of interest (Anger/Fear conditions) was used to compute contrasts for each participant (first-level statistics). First-level contrasts were computed to highlight the difference between difficult (more ambiguous) and easier (less ambiguous) trials for the emotional categorization of voices in all runs, especially the experimental runs. This procedure was decided based on the fact that our TMS inhibitory effect of the rIFC would last approximately 60 min (<xref ref-type="bibr" rid="R51">Huang, Edwards et al. 2005</xref>) and our runs were 10 min each for a total of 30 min, thus clearly within the bounds of the TMS effect. The contrast of interest was therefore the following: [A<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] (contrast vector: -2 -1 6 -1 -2 for each experimental run) and we computed its inverse regarding morphing, [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] (contrast vector: 2 1 -6 1 2 for each experimental run) for each group separately as well as a vector specifically for the most ambiguous voices ([AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>], contrast vector: 0 0 1 0 0 for each experimental run). The weighted contrasts were used to better characterize the level of morphing of the voice stimuli as a function of the BOLD signal.</p><p id="P54">First-level contrast results of each participant were then averaged by group at the second-level using a two-sample t-test analysis. Using this procedure allowed us to test for an effect of cTBS stimulation over the vertex (CG) as opposed to cTBS over the rIFC (EG), our region of interest thought to be responsible for sensitive, accurate vocal emotional judgments. We therefore looked at the interaction between our contrasts of interest between groups: [CG &gt; EG] x [AF<sub>50</sub> &gt; A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] (contrast vector: 1 -1), [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] (contrast vector: -1 1), [CG &gt; EG] x [AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] (contrast vector: 1 -1). Second-level statistical analyses of the main task assumed that Participants (Factor 1) were independent whereas Conditions (Factor 2) were not. Variance estimation was set to unequal for all factors in order to take into account the inhomogeneous variance of the data.</p><p id="P55">For the voice-sensitive areas localizer task, simple contrasts were then taken to a flexible factorial second-level analysis in which there were two factors: the Participant factor with independence set to yes, variance set to unequal and the Voice factor with independence set to no, variance set to unequal.</p><p id="P56">All wholebrain activations are reported at a threshold of <italic>p</italic>&lt;.005 (uncorrected) and a cluster extent threshold of k &gt; 59 voxels, equivalent to a Family-Wise Error correction for multiple comparison of <italic>p</italic>&lt;.05 at the cluster level. This threshold was based on the final FWHM of the data (11.9, 11.9, 11.1 mm), using the '3dClustSim' function in AFNI (<ext-link ext-link-type="uri" xlink:href="http://afni.nimh.nih.gov/afni">http://afni.nimh.nih.gov/afni</ext-link>) software (<xref ref-type="bibr" rid="R21">Cox 1996</xref>), using a non-parametric method with 10'000 iterations to estimate the necessary cluster extent thresholding for side-to-side voxels (NN-2 option). '3dClustSim' reports a cluster extent threshold for each specified statistical <italic>p</italic>-value and follows the assumption that neighboring voxels are part of a similar functional response pattern, rather than a completely different and independent measure as implied by the family-wise error correction at the voxel level. Inferior frontal cortex delineated (<xref ref-type="fig" rid="F2">Fig.2</xref>) using the automated anatomical labelling ('aal') atlas (<xref ref-type="bibr" rid="R80">Tzourio-Mazoyer, Landeau et al. 2002</xref>).</p></sec></sec><sec id="S24"><title>Functional connectivity analysis</title><p id="P57">Seed-to-seed functional analysis was performed for all runs (base and experimental) using the CONN toolbox (<xref ref-type="bibr" rid="R82">Whitfield-Gabrieli and Nieto-Castanon 2012</xref>) version 19.b implemented in Matlab 9.0 (The MathWorks, Inc., Natick, MA, USA). Functional connectivity analyses were computed using as seeds each region of interest (ROI) overlapping with results from studies targeting the decoding of emotional prosody in the lateral, superior temporal cortex (<xref ref-type="bibr" rid="R71">Schirmer and Kotz 2006</xref>), medial temporal lobe (<xref ref-type="bibr" rid="R34">Frühholz, Trost et al. 2014</xref>) and more specifically the role of the IFC in vocal emotion processing (<xref ref-type="bibr" rid="R49">Hoekert, Vingerhoets et al. 2010</xref>, <xref ref-type="bibr" rid="R36">Frühholz and Grandjean 2013</xref>). We therefore ended up including eight ROI (bilateral IFC, bilateral anterior, mid and posterior STC, bilateral amygdala) in our seed-to-seed, generalized psychophysiological interaction analysis. Spurious sources of noise were estimated and removed using the automated toolbox preprocessing algorithm, and the residual BOLD time-series was band-pass filtered using a low frequency window (0.008 &lt; f &lt; 0.09 Hz). Correlation maps were then created for each condition of interest by taking the residual BOLD time-course for each condition from atlas regions of interest and computing bivariate Pearson's correlation coefficients between the time courses of each voxel of each ROI of the atlas, averaged by ROI. We we used generalized psychophysiological interaction (gPPI) measures, representing the level of task-modulated (often labelled 'effective') connectivity between ROI or between ROI and voxels. gPPI is computed using a separate multiple regression model for each target (ROI). Each model includes three predictors: 1) task effects convolved with a canonical hemodynamic response function (psychological factor); 2) each seed ROI BOLD time series (physiological factor) and 3) the interaction term between the psychological and the physiological factors, the output of which is regression coefficients associated with this interaction term. Finally, group-level analyses were performed on these regression coefficients to assess for main effects within-group for contrasts of interest in seed-to-seed and seed-to-voxel analyses. Type I error was controlled by the use of seed-level FDR correction with <italic>p</italic>&lt;.05 two tailed to correct for multiple comparison.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary figures and tables</label><media xlink:href="EMS144977-supplement-Supplementary_figures_and_tables.pdf" mimetype="application" mime-subtype="pdf" id="d16aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S25"><title>Acknowledgements</title><p>The present study was supported by the Swiss National Science Foundation (SNSF 105314_146559/1) and the National Center for Affective Sciences (51NF40-104897). SF receives additional support from the SNSF (PP00P1_157409/1 and PP00P1_183711/1). LC helped program the tasks, collected the behavioral and neuroimaging data, analyzed the data, created and edited the figures and wrote the manuscript. MM collected TMS data and neuroimaging data and helped write the methods of the manuscript. DG helped design the study. CR helped design the study, especially the neuroimaging part to make it compatible with the TMS procedure. SF designed the study, programmed the tasks, collected part of the data and helped design the figures. All authors reviewed and edited the manuscript. The authors declare no competing interests whatsoever. The data and codes can be provided by Leonardo Ceravolo pending scientific review and a completed material transfer agreement. Requests for the data and codes should be submitted to leonardo. <email>ceravolo@uni ge.ch</email>.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agnew</surname><given-names>ZK</given-names></name><name><surname>Banissy</surname><given-names>MJ</given-names></name><name><surname>McGettigan</surname><given-names>C</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Investigating the neural basis of theta burst stimulation to premotor cortex on emotional vocalization perception: A combined TMS-fMRI study</article-title><source>Frontiers in human neuroscience</source><year>2018</year><volume>12</volume><fpage>150</fpage></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alba-Ferrara</surname><given-names>L</given-names></name><name><surname>Ellison</surname><given-names>A</given-names></name><name><surname>Mitchell</surname><given-names>R</given-names></name></person-group><article-title>Decoding emotional prosody: resolving differences in functional neuroanatomy from fMRI and lesion studies using TMS</article-title><source>Brain Stimulation</source><year>2012</year><volume>5</volume><issue>3</issue><fpage>347</fpage><lpage>353</lpage></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Annika</surname><given-names>M</given-names></name><name><surname>Alia</surname><given-names>B</given-names></name><name><surname>E</surname><given-names>UT</given-names></name><name><surname>Klaus</surname><given-names>F</given-names></name></person-group><article-title>Continuous and intermittent transcranial magnetic theta burst stimulation modify tactile learning performance and cortical protein expression in the rat differently</article-title><source>European Journal of Neuroscience</source><year>2010</year><volume>32</volume><issue>9</issue><fpage>1575</fpage><lpage>1586</lpage></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aron</surname><given-names>AR</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><article-title>Inhibition and the right inferior frontal cortex</article-title><source>Trends in cognitive sciences</source><year>2004</year><volume>8</volume><issue>4</issue><fpage>170</fpage><lpage>177</lpage></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aron</surname><given-names>AR</given-names></name><name><surname>Robbins</surname><given-names>TW</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><article-title>Inhibition and the right inferior frontal cortex: one decade on</article-title><source>Trends in cognitive sciences</source><year>2014</year><volume>18</volume><issue>4</issue><fpage>177</fpage><lpage>185</lpage></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashburner</surname><given-names>J</given-names></name></person-group><article-title>A fast diffeomorphic image registration algorithm</article-title><source>Neuroimage</source><year>2007</year><volume>38</volume><issue>1</issue><fpage>95</fpage><lpage>113</lpage></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Romanski</surname><given-names>LM</given-names></name></person-group><article-title>Probabilistic encoding of vocalizations in macaque ventral lateral prefrontal cortex</article-title><source>Journal of Neuroscience</source><year>2006</year><volume>26</volume><issue>43</issue><fpage>11023</fpage><lpage>11033</lpage></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banissy</surname><given-names>MJ</given-names></name><name><surname>Sauter</surname><given-names>DA</given-names></name><name><surname>Ward</surname><given-names>J</given-names></name><name><surname>Warren</surname><given-names>JE</given-names></name><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Suppressing sensorimotor activity modulates the discrimination of auditory emotions but not speaker identity</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><issue>41</issue><fpage>13552</fpage><lpage>13557</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barton</surname><given-names>K</given-names></name><name><surname>Barton</surname><given-names>MK</given-names></name></person-group><source>Package 'mumin' Version 1</source><year>2015</year><fpage>18</fpage></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>Maechler</surname><given-names>M</given-names></name><name><surname>Bolker</surname><given-names>B</given-names></name><name><surname>Walker</surname><given-names>S</given-names></name></person-group><article-title>lme4: Linear mixed-effects models using Eigen and S4</article-title><source>R package version 1</source><year>2014</year><fpage>1</fpage><lpage>7</lpage></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Fillion-Bilodeau</surname><given-names>S</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name></person-group><article-title>The Montreal Affective Voices: A validated set of nonverbal affect bursts for research on auditory affective processing</article-title><year>2008</year></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><year>2000</year><volume>403</volume><issue>6767</issue><fpage>309</fpage><lpage>312</lpage></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bestelmeyer</surname><given-names>PE</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Grosbras</surname><given-names>M-H</given-names></name></person-group><article-title>Right temporal TMS impairs voice detection</article-title><source>Current Biology</source><year>2011</year><volume>21</volume><issue>20</issue><fpage>R838</fpage><lpage>R839</lpage></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bestelmeyer</surname><given-names>PE</given-names></name><name><surname>Maurage</surname><given-names>P</given-names></name><name><surname>Rouger</surname><given-names>J</given-names></name><name><surname>Latinus</surname><given-names>M</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Adaptation to vocal expressions reveals multistep perception of auditory emotion</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>24</issue><fpage>8098</fpage><lpage>8105</lpage></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brück</surname><given-names>C</given-names></name><name><surname>Kreifelts</surname><given-names>B</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><article-title>Emotional voices in context: a neurobiological model of multimodal affective information processing</article-title><source>Physics of Life Reviews</source><year>2011</year><volume>8</volume><issue>4</issue><fpage>383</fpage><lpage>403</lpage></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Proximal vocal threat recruits the right voice-sensitive auditory cortex</article-title><source>Social cognitive and affective neuroscience</source><year>2016</year><volume>11</volume><issue>5</issue><fpage>793</fpage><lpage>802</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Pernet</surname><given-names>C</given-names></name><name><surname>Latinus</surname><given-names>M</given-names></name><name><surname>Crabbe</surname><given-names>F</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Cerebral processing of voice gender studied using a continuous carryover fMRI design</article-title><source>Cerebral Cortex</source><year>2013</year><volume>23</volume><issue>4</issue><fpage>958</fpage><lpage>966</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>Y</given-names></name><name><surname>Russ</surname><given-names>B</given-names></name><name><surname>Davis</surname><given-names>S</given-names></name><name><surname>Baker</surname><given-names>A</given-names></name><name><surname>Ackelson</surname><given-names>A</given-names></name><name><surname>Nitecki</surname><given-names>R</given-names></name></person-group><article-title>A functional role for the ventrolateral prefrontal cortex in non-spatial auditory cognition</article-title><source>Proceedings of the National Academy of Sciences</source><year>2009</year><volume>106</volume><issue>47</issue><fpage>20045</fpage><lpage>20050</lpage></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>YE</given-names></name><name><surname>Hauser</surname><given-names>MD</given-names></name><name><surname>Russ</surname><given-names>BE</given-names></name></person-group><article-title>Spontaneous processing of abstract categorical information in the ventrolateral prefrontal cortex</article-title><source>Biology letters</source><year>2006</year><volume>2</volume><issue>2</issue><fpage>261</fpage><lpage>265</lpage></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>D</given-names></name><name><surname>Neelin</surname><given-names>P</given-names></name><name><surname>Peters</surname><given-names>T</given-names></name><name><surname>Evans</surname><given-names>A</given-names></name></person-group><article-title>Automatic 3D intersubject registration of MR volumetric data in standardized Talairach space</article-title><source>J Comput Assist Tomogr</source><year>1994</year><volume>18</volume><fpage>192</fpage><lpage>205</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Computers and Biomedical research</source><year>1996</year><volume>29</volume><issue>3</issue><fpage>162</fpage><lpage>173</lpage></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dane</surname><given-names>E</given-names></name><name><surname>Rockmann</surname><given-names>KW</given-names></name><name><surname>Pratt</surname><given-names>MG</given-names></name></person-group><article-title>When should I trust my gut? Linking domain expertise to intuitive decision-making effectiveness</article-title><source>Organizational behavior and human decision processes</source><year>2012</year><volume>119</volume><issue>2</issue><fpage>187</fpage><lpage>194</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desmond</surname><given-names>JE</given-names></name><name><surname>Glover</surname><given-names>GH</given-names></name></person-group><article-title>Estimating sample size in functional MRI (fMRI) neuroimaging studies: statistical power analyses</article-title><source>Journal of neuroscience methods</source><year>2002</year><volume>118</volume><issue>2</issue><fpage>115</fpage><lpage>128</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dippel</surname><given-names>G</given-names></name><name><surname>Beste</surname><given-names>C</given-names></name></person-group><article-title>A causal role of the right inferior frontal cortex in implementing strategies for multi-component behaviour</article-title><source>Nature communications</source><year>2015</year><volume>6</volume><elocation-id>6587</elocation-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dricu</surname><given-names>M</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Biased and unbiased perceptual decision-making on vocal emotions</article-title><source>Scientific reports</source><year>2017</year><volume>7</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dricu</surname><given-names>M</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>A neurocognitive model of perceptual decision-making on emotional signals</article-title><source>Human Brain Mapping</source><year>2020</year><volume>41</volume><issue>6</issue><fpage>1532</fpage><lpage>1556</lpage></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Anders</surname><given-names>S</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Herbert</surname><given-names>C</given-names></name><name><surname>Wiethoff</surname><given-names>S</given-names></name><name><surname>Kissler</surname><given-names>J</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><article-title>Cerebral pathways in processing of affective prosody: a dynamic causal modeling study</article-title><source>Neuroimage</source><year>2006</year><volume>30</volume><issue>2</issue><fpage>580</fpage><lpage>587</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Bretscher</surname><given-names>J</given-names></name><name><surname>Gschwind</surname><given-names>M</given-names></name><name><surname>Kreifelts</surname><given-names>B</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>Emotional voice areas: anatomic location, functional properties, and structural connections revealed by combined fMRI/DTI</article-title><source>Cerebral cortex</source><year>2012</year><volume>22</volume><issue>1</issue><fpage>191</fpage><lpage>200</lpage></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Bretscher</surname><given-names>J</given-names></name><name><surname>Wiethoff</surname><given-names>S</given-names></name><name><surname>Bisch</surname><given-names>J</given-names></name><name><surname>Schlipf</surname><given-names>S</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name><name><surname>Kreifelts</surname><given-names>B</given-names></name></person-group><article-title>Functional responses and structural connections of cortical areas for processing faces and voices in the superior temporal sulcus</article-title><source>Neuroimage</source><year>2013</year><volume>76</volume><fpage>45</fpage><lpage>56</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Scherer</surname><given-names>K</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>Decoding of emotional information in voice-sensitive cortices</article-title><source>Current biology</source><year>2009</year><volume>19</volume><issue>12</issue><fpage>1028</fpage><lpage>1033</lpage></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Lang</surname><given-names>A-G</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name></person-group><article-title>G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behavior research methods</source><year>2007</year><volume>39</volume><issue>2</issue><fpage>175</fpage><lpage>191</lpage></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fecteau</surname><given-names>S</given-names></name><name><surname>Armony</surname><given-names>JL</given-names></name><name><surname>Joanette</surname><given-names>Y</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Sensitivity to voice in human prefrontal cortex</article-title><source>Journal of Neurophysiology</source><year>2005</year><volume>94</volume><issue>3</issue><fpage>2251</fpage><lpage>2254</lpage></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>J</given-names></name><name><surname>Friendly</surname><given-names>GG</given-names></name><name><surname>Graves</surname><given-names>S</given-names></name><name><surname>Heiberger</surname><given-names>R</given-names></name><name><surname>Monette</surname><given-names>G</given-names></name><name><surname>Nilsson</surname><given-names>H</given-names></name><name><surname>Ripley</surname><given-names>B</given-names></name><name><surname>Weisberg</surname><given-names>S</given-names></name><name><surname>Fox</surname><given-names>MJ</given-names></name><name><surname>Suggests</surname><given-names>M</given-names></name></person-group><article-title>The car package</article-title><source>R Foundation for Statistical Computing</source><year>2007</year></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Specific brain networks during explicit and implicit decoding of emotional prosody</article-title><source>Cerebral cortex</source><year>2012</year><volume>22</volume><issue>5</issue><fpage>1107</fpage><lpage>1117</lpage></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Towards a fronto-temporal neural network for the decoding of angry vocal expressions</article-title><source>Neuroimage</source><year>2012</year><volume>62</volume><issue>3</issue><fpage>1658</fpage><lpage>1666</lpage></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Processing of emotional vocalizations in bilateral inferior frontal cortex</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2013</year><volume>37</volume><issue>10</issue><fpage>2847</fpage><lpage>2855</lpage></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Gschwind</surname><given-names>M</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Bilateral dorsal and ventral fiber pathways for the processing of affective prosody identified by probabilistic fiber tracking</article-title><source>Neuroimage</source><year>2015</year><volume>109</volume><fpage>27</fpage><lpage>34</lpage></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name></person-group><article-title>Nonverbal Auditory Communication-Evidence for Integrated Neural Systems for Voice Signal Production and Perception</article-title><source>Progress in Neurobiology</source><year>2020</year><elocation-id>101948</elocation-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Trost</surname><given-names>W</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>The role of the medial temporal limbic system in processing emotions in voice and music</article-title><source>Progress in neurobiology</source><year>2014</year><volume>123</volume><fpage>1</fpage><lpage>17</lpage></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Trost</surname><given-names>W</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>The sound of emotions—Towards a unifying neural network perspective of affective sound processing</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2016</year><volume>68</volume><fpage>96</fpage><lpage>110</lpage></element-citation></ref><ref id="R41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gilovich</surname><given-names>T</given-names></name><name><surname>Griffin</surname><given-names>D</given-names></name><name><surname>Kahneman</surname><given-names>D</given-names></name></person-group><source>Heuristics and biases: The psychology of intuitive judgment</source><publisher-name>Cambridge university press</publisher-name><year>2002</year><volume>3</volume><fpage>842</fpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Brain networks of emotional prosody processing</article-title><source>Genes Dev</source><year>2020</year><volume>28</volume><fpage>342</fpage><lpage>56</lpage></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Pourtois</surname><given-names>G</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Seghier</surname><given-names>ML</given-names></name><name><surname>Scherer</surname><given-names>KR</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>The voices of wrath: brain responses to angry prosody in meaningless speech</article-title><source>Nature neuroscience</source><year>2005</year><volume>8</volume><issue>2</issue><fpage>145</fpage><lpage>146</lpage></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossmann</surname><given-names>T</given-names></name><name><surname>Oberecker</surname><given-names>R</given-names></name><name><surname>Schlesinger</surname><given-names>F</given-names></name><name><surname>Koch</surname><given-names>SP</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>The developmental origins of voice processing in the human brain</article-title><source>Neuron</source><year>2010</year><volume>65</volume><issue>6</issue><fpage>852</fpage><lpage>858</lpage></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gruber</surname><given-names>T</given-names></name><name><surname>Debracque</surname><given-names>C</given-names></name><name><surname>Ceravolo</surname><given-names>L</given-names></name><name><surname>Igloi</surname><given-names>K</given-names></name><name><surname>Marin Bosch</surname><given-names>B</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Human discrimination and categorization of emotions in voices: a functional Near-Infrared Spectroscopy (fNIRS) study."</article-title><source>Frontiers in neuroscience</source><year>2020</year><volume>14</volume><fpage>570</fpage></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammond</surname><given-names>KR</given-names></name><name><surname>Hamm</surname><given-names>RM</given-names></name><name><surname>Grassia</surname><given-names>J</given-names></name><name><surname>Pearson</surname><given-names>T</given-names></name></person-group><article-title>Direct comparison of the efficacy of intuitive and analytical cognition in expert judgment</article-title><source>IEEE Transactions on Systems, Man, and Cybernetics</source><year>1987</year><volume>17</volume><issue>5</issue><fpage>753</fpage><lpage>770</lpage></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Saur</surname><given-names>D</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Ulmer</surname><given-names>S</given-names></name><name><surname>Baumgaertner</surname><given-names>A</given-names></name><name><surname>Siebner</surname><given-names>HR</given-names></name></person-group><article-title>Perturbation of the left inferior frontal gyrus triggers adaptive plasticity in the right homologous area during speech production</article-title><source>Proceedings of the National Academy of Sciences</source><year>2013</year><volume>110</volume><issue>41</issue><fpage>16402</fpage><lpage>16407</lpage></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekert</surname><given-names>M</given-names></name><name><surname>Bais</surname><given-names>L</given-names></name><name><surname>Kahn</surname><given-names>RS</given-names></name><name><surname>Aleman</surname><given-names>A</given-names></name></person-group><article-title>Time course of the involvement of the right anterior superior temporal gyrus and the right fronto-parietal operculum in emotional prosody perception</article-title><source>PLoS One</source><year>2008</year><volume>3</volume><issue>5</issue></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekert</surname><given-names>M</given-names></name><name><surname>Vingerhoets</surname><given-names>G</given-names></name><name><surname>Aleman</surname><given-names>A</given-names></name></person-group><article-title>Results of a pilot study on the involvement of bilateral inferior frontal gyri in emotional prosody perception: an rTMS study</article-title><source>BMC neuroscience</source><year>2010</year><volume>11</volume><issue>1</issue><lpage>93</lpage></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hogarth</surname><given-names>RM</given-names></name></person-group><article-title>Intuition: A Challenge for Psychological Research on Decision Making</article-title><source>Psychological Inquiry</source><year>2010</year><volume>21</volume><issue>4</issue><fpage>338</fpage><lpage>353</lpage></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y-Z</given-names></name><name><surname>Edwards</surname><given-names>MJ</given-names></name><name><surname>Rounis</surname><given-names>E</given-names></name><name><surname>Bhatia</surname><given-names>KP</given-names></name><name><surname>Rothwell</surname><given-names>JC</given-names></name></person-group><article-title>Theta burst stimulation of the human motor cortex</article-title><source>Neuron</source><year>2005</year><volume>45</volume><issue>2</issue><fpage>201</fpage><lpage>206</lpage></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>H</given-names></name><name><surname>Brück</surname><given-names>C</given-names></name><name><surname>Plewnia</surname><given-names>C</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><article-title>Cerebral processing of prosodic emotional signals: Evaluation of a network model using rTMS</article-title><source>PloS one</source><year>2014</year><volume>9</volume><issue>8</issue></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobson</surname><given-names>L</given-names></name><name><surname>Javitt</surname><given-names>DC</given-names></name><name><surname>Lavidor</surname><given-names>M</given-names></name></person-group><article-title>Activation of inhibition: diminishing impulsive behavior by direct current stimulation over the inferior frontal gyrus</article-title><source>Journal of cognitive neuroscience</source><year>2011</year><volume>23</volume><issue>11</issue><fpage>3380</fpage><lpage>3387</lpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Garrido</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>RR</given-names></name><name><surname>Susilo</surname><given-names>T</given-names></name><name><surname>Barton</surname><given-names>JJ</given-names></name><name><surname>Duchaine</surname><given-names>B</given-names></name></person-group><article-title>Normal voice processing after posterior superior temporal sulcus lesion</article-title><source>Neuropsycholo gia</source><year>2017</year><volume>105</volume><fpage>215</fpage><lpage>222</lpage></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>J</given-names></name><name><surname>Bungert</surname><given-names>A</given-names></name><name><surname>Bowtell</surname><given-names>R</given-names></name><name><surname>Jackson</surname><given-names>SR</given-names></name></person-group><article-title>Vertex stimulation as a control site for transcranial magnetic stimulation: a concurrent TMS/fMRI study</article-title><source>Brain stimulation</source><year>2016</year><volume>9</volume><issue>1</issue><fpage>58</fpage><lpage>64</lpage></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahneman</surname><given-names>D</given-names></name></person-group><article-title>Maps of bounded rationality: A perspective on intuitive judgment and choice</article-title><source>Nobel prize lecture</source><year>2002</year><volume>8</volume><fpage>351</fpage><lpage>401</lpage></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keuken</surname><given-names>M</given-names></name><name><surname>Hardie</surname><given-names>A</given-names></name><name><surname>Dorn</surname><given-names>B</given-names></name><name><surname>Dev</surname><given-names>S</given-names></name><name><surname>Paulus</surname><given-names>M</given-names></name><name><surname>Jonas</surname><given-names>K</given-names></name><name><surname>Van Den Wildenberg</surname><given-names>W</given-names></name><name><surname>Pineda</surname><given-names>J</given-names></name></person-group><article-title>The role of the left inferior frontal gyrus in social perception: an rTMS study</article-title><source>Brain research</source><year>2011</year><volume>1383</volume><fpage>196</fpage><lpage>205</lpage></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname><given-names>A</given-names></name><name><surname>Brockhoff</surname><given-names>PB</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name></person-group><article-title>lmerTest: Tests in linear mixed effects models R package version 2.0-20</article-title><year>2013</year></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liakakis</surname><given-names>G</given-names></name><name><surname>Nickel</surname><given-names>J</given-names></name><name><surname>Seitz</surname><given-names>R</given-names></name></person-group><article-title>Diversity of the inferior frontal gyrus—a meta-analysis of neuroimaging studies</article-title><source>Behavioural brain research</source><year>2011</year><volume>225</volume><issue>1</issue><fpage>341</fpage><lpage>347</lpage></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lupyan</surname><given-names>G</given-names></name><name><surname>Mirman</surname><given-names>D</given-names></name><name><surname>Hamilton</surname><given-names>R</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><article-title>Categorization is modulated by transcranial direct current stimulation over left prefrontal cortex</article-title><source>Cognition</source><year>2012</year><volume>124</volume><issue>1</issue><fpage>36</fpage><lpage>49</lpage></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nahas</surname><given-names>Z</given-names></name><name><surname>Lomarev</surname><given-names>M</given-names></name><name><surname>Roberts</surname><given-names>DR</given-names></name><name><surname>Shastri</surname><given-names>A</given-names></name><name><surname>Lorberbaum</surname><given-names>JP</given-names></name><name><surname>Teneback</surname><given-names>C</given-names></name><name><surname>McConnell</surname><given-names>K</given-names></name><name><surname>Vincent</surname><given-names>DJ</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>George</surname><given-names>MS</given-names></name></person-group><article-title>Unilateral left prefrontal transcranial magnetic stimulation (TMS) produces intensity-dependent bilateral effects as measured by interleaved BOLD fMRI</article-title><source>Biological psychiatry</source><year>2001</year><volume>50</volume><issue>9</issue><fpage>712</fpage><lpage>720</lpage></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>S</given-names></name><name><surname>Iacovella</surname><given-names>V</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>Uncertainty in visual and auditory series is coded by modality-general and modality-specific neural systems</article-title><source>Human Brain Mapping</source><year>2014</year><volume>35</volume><issue>4</issue><fpage>1111</fpage><lpage>1128</lpage></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oberman</surname><given-names>L</given-names></name><name><surname>Edwards</surname><given-names>D</given-names></name><name><surname>Eldaief</surname><given-names>M</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name></person-group><article-title>Safety of theta burst transcranial magnetic stimulation: a systematic review of the literature</article-title><source>Journal of Clinical Neurophysiology</source><year>2011</year><volume>28</volume><issue>1</issue><fpage>67</fpage></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pannese</surname><given-names>A</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Amygdala and auditory cortex exhibit distinct sensitivity to relevant acoustic features of auditory emotions</article-title><source>Cortex</source><year>2016</year><volume>85</volume><fpage>116</fpage><lpage>125</lpage></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauschecker</surname><given-names>JP</given-names></name><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Maps and streams in the auditory cortex: nonhuman primates illuminate human speech processing</article-title><source>Nature neuroscience</source><year>2009</year><volume>12</volume><issue>6</issue><fpage>718</fpage><lpage>724</lpage></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>JC</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Inferior frontal cortex contributions to the recognition of spoken words and their constituent speech sounds</article-title><source>Journal of cognitive neuroscience</source><year>2017</year><volume>29</volume><issue>5</issue><fpage>919</fpage><lpage>936</lpage></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romanski</surname><given-names>LM</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><article-title>The primate cortical auditory system and neural representation of conspecific vocalizations</article-title><source>Annual review of neuroscience</source><year>2009</year><volume>32</volume><fpage>315</fpage><lpage>346</lpage></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roswandowitz</surname><given-names>C</given-names></name><name><surname>Swanborough</surname><given-names>H</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Categorizing human vocal signals depends on an integrated auditory-frontal cortical network</article-title><source>Human Brain Mapping</source><year>2020</year></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sammler</surname><given-names>D</given-names></name><name><surname>Grosbras</surname><given-names>M-H</given-names></name><name><surname>Anwander</surname><given-names>A</given-names></name><name><surname>Bestelmeyer</surname><given-names>PE</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Dorsal and ventral pathways for prosody</article-title><source>Current Biology</source><year>2015</year><volume>25</volume><issue>23</issue><fpage>3079</fpage><lpage>3085</lpage></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sander</surname><given-names>D</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name><name><surname>Pourtois</surname><given-names>G</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Seghier</surname><given-names>ML</given-names></name><name><surname>Scherer</surname><given-names>KR</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>Emotion and attention interactions in social cognition: brain regions involved in processing anger prosody</article-title><source>Neuroimage</source><year>2005</year><volume>28</volume><issue>4</issue><fpage>848</fpage><lpage>858</lpage></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname><given-names>A</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing</article-title><source>Trends in cognitive sciences</source><year>2006</year><volume>10</volume><issue>1</issue><fpage>24</fpage><lpage>30</lpage></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schirmer</surname><given-names>A</given-names></name><name><surname>Zysset</surname><given-names>S</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>von Cramon</surname><given-names>DY</given-names></name></person-group><article-title>Gender differences in the activation of inferior frontal cortex during emotional speech perception</article-title><source>NeuroImage</source><year>2004</year><volume>21</volume><issue>3</issue><fpage>1114</fpage><lpage>1123</lpage></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokhi</surname><given-names>DS</given-names></name><name><surname>Hunter</surname><given-names>MD</given-names></name><name><surname>Wilkinson</surname><given-names>ID</given-names></name><name><surname>Woodruff</surname><given-names>PW</given-names></name></person-group><article-title>Male and female voices activate distinct regions in the male brain</article-title><source>Neuroimage</source><year>2005</year><volume>27</volume><issue>3</issue><fpage>572</fpage><lpage>578</lpage></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Staib</surname><given-names>M</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Cortical voice processing is grounded in elementary sound analyses for vocalization relevant sound patterns</article-title><source>Progress in Neurobiology</source><year>2020</year><elocation-id>101982</elocation-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steiner</surname><given-names>F</given-names></name><name><surname>Bobin</surname><given-names>M</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Auditory cortical micro-networks show differential connectivity during voice and speech processing in humans</article-title><source>Communications Biology</source><year>2021</year><volume>4</volume><issue>1</issue><fpage>1</fpage><lpage>10</lpage></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suran</surname><given-names>T</given-names></name><name><surname>Rumiati</surname><given-names>RI</given-names></name><name><surname>Piretti</surname><given-names>L</given-names></name></person-group><article-title>The contribution of the left inferior frontal gyrus in affective processing of social groups</article-title><source>Cognitive neuroscience</source><year>2019</year><volume>10</volume><issue>4</issue><fpage>186</fpage><lpage>195</lpage></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swanborough</surname><given-names>H</given-names></name><name><surname>Staib</surname><given-names>M</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name></person-group><article-title>Neurocognitive dynamics of near-threshold voice signal detection and affective voice evaluation</article-title><source>Science advances</source><year>2020</year><volume>6</volume><issue>50</issue><elocation-id>eabb3884</elocation-id></element-citation></ref><ref id="R78"><element-citation publication-type="book"><collab>Team, R. C</collab><source>R: a language and environment for statistical computing [computer program] Version 3.1. 2</source><publisher-name>R Foundation for Statistical Computing</publisher-name><publisher-loc>Vienna, Austria</publisher-loc><year>2014</year></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toelch</surname><given-names>U</given-names></name><name><surname>Bach</surname><given-names>DR</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>The neural underpinnings of an optimal exploitation of social information under uncertainty</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2013</year><volume>9</volume><issue>11</issue><fpage>1746</fpage><lpage>1753</lpage></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name><name><surname>Landeau</surname><given-names>B</given-names></name><name><surname>Papathanassiou</surname><given-names>D</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Etard</surname><given-names>O</given-names></name><name><surname>Delcroix</surname><given-names>N</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name><name><surname>Joliot</surname><given-names>M</given-names></name></person-group><article-title>Automated anatomical labeling of activations in SPM using a macroscopic anatomical parcellation of the MNI MRI single-subject brain</article-title><source>Neuroimage</source><year>2002</year><volume>15</volume><issue>1</issue><fpage>273</fpage><lpage>289</lpage></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verbruggen</surname><given-names>F</given-names></name><name><surname>Aron</surname><given-names>AR</given-names></name><name><surname>Stevens</surname><given-names>MA</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name></person-group><article-title>Theta burst stimulation dissociates attention and action updating in human inferior frontal cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2010</year><volume>107</volume><issue>31</issue><fpage>13966</fpage><lpage>13971</lpage></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitfield-Gabrieli</surname><given-names>S</given-names></name><name><surname>Nieto-Castanon</surname><given-names>A</given-names></name></person-group><article-title>Conn: a functional connectivity toolbox for correlated and anticorrelated brain networks</article-title><source>Brain connectivity</source><year>2012</year><volume>2</volume><issue>3</issue><fpage>125</fpage><lpage>141</lpage></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whiting</surname><given-names>CM</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>The perception of caricatured emotion in voice</article-title><source>Cognition</source><year>2020</year><volume>200</volume><elocation-id>104249</elocation-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witteman</surname><given-names>J</given-names></name><name><surname>Van Heuven</surname><given-names>VJ</given-names></name><name><surname>Schiller</surname><given-names>NO</given-names></name></person-group><article-title>Hearing feelings: a quantitative meta-analysis on the neuroimaging literature of emotional prosody perception</article-title><source>Neuropsychologia</source><year>2012</year><volume>50</volume><issue>12</issue><fpage>2752</fpage><lpage>2763</lpage></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Myers</surname><given-names>E</given-names></name></person-group><article-title>Left inferior frontal gyrus sensitivity to phonetic competition in receptive language processing: A comparison of clear and conversational speech</article-title><source>Journal of cognitive neuroscience</source><year>2018</year><volume>30</volume><issue>3</issue><fpage>267</fpage><lpage>280</lpage></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zander</surname><given-names>T</given-names></name><name><surname>Horr</surname><given-names>NK</given-names></name><name><surname>Bolte</surname><given-names>A</given-names></name><name><surname>Volz</surname><given-names>KG</given-names></name></person-group><article-title>Intuitive decision making as a gradual process: investigating semantic intuition-based and priming-based decisions with fMRI</article-title><source>Brain and Behavior</source><year>2016</year><volume>6</volume><issue>1</issue><elocation-id>e00420</elocation-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><article-title>Optimally Efficient Neural Systems for Processing Spoken Language</article-title><source>Cerebral Cortex</source><year>2012</year><volume>24</volume><issue>4</issue><fpage>908</fpage><lpage>918</lpage></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Teaser</title></caption><p>Inferior frontal cortex enforces cognitive analyses during affect decisions with different levels of sensory ambiguity.</p></boxed-text><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Experimental timeline, example affective voices, and behavioral results for initial stimulus selection and the vocal affect categorization task.</title><p><bold>(A)</bold> Experimental timeline for one participant, showing the different procedures for each group and detailing the three-alternative forced choice task (3-AFC) with examples of stimuli including spectrograms of neutral and morphed voices with percentage of each morphed emotion (anger, fear). <bold>(B)</bold> Probability of an 'anger' response as a function of the morphing procedure for each run, per group for the vocal affect categorization task. <bold>(C)</bold> Following the initial morphing procedure, we had an independent sample of nineteen right-handed participants (10 female, 9 male, mean age 32y, SD 5.2) evaluate the emotionally blended stimuli. Participants were asked to categorize each stimulus (n=110; 10 trials for each of the 11 morphing levels) by a keypress (1: the voice expresses fear; 2: the voice epxresses anger; the keys were counterbalanced across participants). The line plots illustrate on the <italic>y</italic> axis the probability of an 'anger' response (emotion categorization response; black circles) and reaction times (black squares) according to each morphing level (<italic>x</italic> axis) with errorbars representing the standard error of the mean (SEM). <bold>(D)</bold> Averaged reaction times results for the vocal affect categorization task, for each morphing level, run and group. Error bars represent the standard error of the mean (SEM). <italic>aMT</italic> active motor threshold; <italic>cTBS</italic> continuous theta-burst stimulation (transcranial magnetic stimulation procedure); <italic>IFC</italic> inferior frontal cortex target region (MNI <italic>xyz</italic> 54 34 10) based on <italic>a priori</italic> data (see TMS site localization section); <italic>CG</italic> control group; <italic>EG</italic> experimental group; <italic>ITI</italic> inter-trial interval; <italic>cTBS</italic> continuous theta burst stimulation; <italic>Probab</italic> probability; <italic>resp</italic> response. ***<italic>p</italic>&lt;.0001.</p></caption><graphic xlink:href="EMS144977-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Neuroimaging results of the affect classification task for the CG with a focus on the IFC.</title><p><bold>(A)</bold> Whole-brain data showing control group (CG) activations (black circle: left IFC maxima) for the [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast with continuous theta burst stimulation (cTBS) over the vertex, left hemisphere. <bold>(B)</bold> Activations for the same contrast in the right hemisphere, showing global maxima activity in the right inferior frontal gyrus <italic>pars triangularis</italic> (red circle, dashed black outline), used as the target region for the cTBS procedure of the EG. <bold>(C)</bold> Percentage of signal change using a cube of 27 voxels adjacent to the peak voxel—extracted according to activity maxima of the [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast for the CG—for both groups in the left (MNI <italic>xyz</italic> -58, 20, 8; black circle) and right (MNI <italic>xyz</italic> 54,34,10; red circle, dashed black outline) IFGtri for each morphing condition. <bold>(D)</bold> TMS coil positioning for the cTBS procedure in both groups (CG: vertex, red circle with continuous black outline, MNI <italic>xyz</italic> [0, -20, 80]; EG: right IFC within the IFGtri, red circle with dashed black outline, MNI <italic>xyz</italic> [54, 34, 10]), decided after the CG was scanned in whole. Wholebrain activations are reported at <italic>p</italic>&lt;.005 uncorrected with k&gt;59 voxels, equivalent to a FWE cluster correction for multiple comparisons of <italic>p</italic>&lt;.05. The colorbar represents t-statistics. Inferior frontal cortex delineated in white with subregions delineated in black using the automated anatomical labelling ('aal') atlas. CG: control group; EG: experimental group; TMS: transcranial magnetic stimulation; IFC: inferior frontal cortex; IFGop: inferior frontal gyrus <italic>pars opercularis;</italic> IFGtri: inferior frontal gyrus <italic>pars triangularis;</italic> IFGorb: inferior frontal gyrus <italic>pars orbitalis</italic>; cTBS: continuous theta burst stimulation.</p></caption><graphic xlink:href="EMS144977-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Sample-specific (N=35) activations for the VA localizer task.</title><p><bold>(A)</bold> Voice areas and IFC regions (black outline) in the left hemisphere for the [Vocal &gt; Non-vocal] contrast. <bold>(B)</bold> Voice areas and IFC regions (black outline) in the right hemisphere for the [Vocal &gt; Non-vocal] contrast. Wholebrain activations are reported at <italic>p</italic>&lt;.005 uncorrected with k&gt;59 voxels, equivalent to a FWE cluster correction for multiple comparisons of <italic>p</italic>&lt;.05. Colorbars represent t-statistics. <italic>IFGop</italic> inferior frontal gyrus pars opercularis; <italic>IFGtri</italic> inferior frontal gyrus pars triangularis; <italic>IFGorb</italic> inferior frontal gyrus pars orbitalis; <italic>INS</italic> insula; <italic>STC</italic> superior temporal cortex; <italic>STS</italic> superior temporal sulcus; <italic>MTC</italic> middle temporal cortex; <italic>DLPFC</italic> dorsolateral prefrontal cortex; <italic>MFG</italic> middle frontal gyrus.</p></caption><graphic xlink:href="EMS144977-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Neuroimaging results of the vocal affect classification task with outlines of sample-specific voice areas.</title><p><bold>(A)</bold> Whole-brain data showing between-group activations for the [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast with continuous theta burst stimulation (cTBS) over the vertex (red circle, black outline) for the control group (CG) as opposed to over the right inferior frontal gyrus (IFG; red circle, dotted outline) for the experimental group (EG). <bold>(B)</bold> Left hemisphere activations for [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast for the CG. <bold>(C)</bold> Left hemisphere activations for [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub>&gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast for the EG. <bold>(D)</bold> Percentage of signal change (±SEM) extracted in the right mSTG* [MNI <italic>xyz</italic> 58, 0, -12] for the main contrast ([CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]). <bold>(E)</bold> Right hemisphere activations for the [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast for the control group. <bold>(F)</bold> Right hemisphere activations for the [CG &gt; EG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast for the experimental group. Sample-specific voice areas (VA) are outlined in each panel in black. <italic>a/m/p</italic> anterior/mid/posterior; <italic>op/tri</italic> pars opercularis/triangularis; <italic>MFG</italic> middle frontal gyrus; <italic>STC</italic> superior temporal cortex; <italic>MTC</italic> middle temporal cortex; <italic>SPL</italic> superior parietal lobule; <italic>INS</italic> insula; <italic>PTe</italic> planum temporale; <italic>SMG</italic> supramarginal gyrus; <italic>STS</italic> superior temporal sulcus. Wholebrain activations are reported at p&lt;.005 uncorrected with k&gt;59 voxels, equivalent to FWE cluster correction for multiple comparisons of p&lt;.05. Colorbars represent t-statistics.</p></caption><graphic xlink:href="EMS144977-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Seed-to-seed functional connectivity results for the vocal affect categorization task.</title><p><bold>(A)</bold> Anti-coupled functional connectivity was found between the left mid superior temporal cortex (mSTC) and the right amygdala (AMY) for the [EG &gt; CG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast. <bold>(B)</bold> Summary of coupled and anti-coupled functional connectivity data and statistical values. <bold>(C)</bold> Coupled functional connectivity between the left amygdala (AMY) and the right inferior frontal cortex (IFC) for the [EG &gt; CG] x [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] x [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>] contrast. Colorbar shows two-tailed t-statistics. Statistical threshold: <italic>p</italic>&lt;.05 FDR corrected for multiple comparisons at the seed (N=8) level.</p></caption><graphic xlink:href="EMS144977-f005"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Accuracy and reaction times data for neutral trials, for each group and run.</title></caption><table frame="box" rules="cols"><thead><tr style="border-bottom: solid thin"><th align="center" valign="middle"/><th align="center" valign="top">Base run</th><th align="center" valign="top">cTBS<sub>post</sub> Run 1</th><th align="center" valign="top">cTBS<sub>post</sub> Run 2</th><th align="center" valign="top">cTBS<sub>post</sub> Run 3</th></tr></thead><tbody><tr><td align="center" valign="top"><bold>CG acc</bold></td><td align="left" valign="top">87.50% (6.55)</td><td align="left" valign="top">87.78% (4.96)</td><td align="left" valign="top">88.05% (5.55)</td><td align="left" valign="top">88.05% (5.55)</td></tr><tr style="border-bottom: solid thin"><td align="center" valign="top"><bold>CG rt</bold></td><td align="left" valign="top">1032ms (188)</td><td align="left" valign="top">928ms (201)</td><td align="left" valign="top">918ms (213)</td><td align="left" valign="top">944ms (208)</td></tr><tr><td align="center" valign="top"><bold>EG acc</bold></td><td align="left" valign="top">93.23% (6.74)</td><td align="left" valign="top">97.94% (5.10)</td><td align="left" valign="top">97.06% (5.71)</td><td align="left" valign="top">97.94% (5.10)</td></tr><tr><td align="center" valign="top"><bold>EG rt</bold></td><td align="left" valign="top">1112ms (192)</td><td align="left" valign="top">1043ms (204)</td><td align="left" valign="top">1061ms (186)</td><td align="left" valign="top">1137ms (206)</td></tr></tbody></table><table-wrap-foot><fn id="TFN1"><p id="P58">Value: mean (SD). acc: accuracy; rt: reaction times; ms: milliseconds.</p></fn></table-wrap-foot></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Wholebrain activations for contrasts of interest, per group and between-groups</title></caption><table frame="void" rules="none"><tbody><tr><td align="left" valign="middle" colspan="7">[CG &gt; EG] × [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] × [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]</td></tr><tr><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Region label</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Hemisphere</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>X</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Y</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Z</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">T-value</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Cluster size (voxel count)</td></tr><tr><td align="left" valign="middle">SPL</td><td align="left" valign="middle">R</td><td align="left" valign="middle">28</td><td align="left" valign="middle">-42</td><td align="left" valign="middle">48</td><td align="left" valign="middle">4.63</td><td align="left" valign="middle">186</td></tr><tr><td align="left" valign="middle">STG</td><td align="left" valign="middle">R</td><td align="left" valign="middle">58</td><td align="left" valign="middle">0</td><td align="left" valign="middle">-12</td><td align="left" valign="middle">3.74</td><td align="left" valign="middle">84</td></tr><tr><td align="left" valign="middle">MFG</td><td align="left" valign="middle">R</td><td align="left" valign="middle">20</td><td align="left" valign="middle">20</td><td align="left" valign="middle">56</td><td align="left" valign="middle">3.73</td><td align="left" valign="middle">73</td></tr><tr><td align="left" valign="middle">Cuneus</td><td align="left" valign="middle">R</td><td align="left" valign="middle">18</td><td align="left" valign="middle">-80</td><td align="left" valign="middle">32</td><td align="left" valign="middle">3.53</td><td align="left" valign="middle">120</td></tr><tr><td align="left" valign="middle" colspan="7">[CG] × [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] × [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]</td></tr><tr><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Region label</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Hemisphere</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>X</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Y</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Z</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">T-value</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Cluster size (voxel count)</td></tr><tr><td align="left" valign="middle">STG</td><td align="left" valign="middle">R</td><td align="left" valign="middle">58</td><td align="left" valign="middle">-26</td><td align="left" valign="middle">2</td><td align="left" valign="middle">5.70</td><td align="left" valign="middle">549</td></tr><tr><td align="left" valign="middle">Cerebellum</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-20</td><td align="left" valign="middle">-74</td><td align="left" valign="middle">-36</td><td align="left" valign="middle">4.75</td><td align="left" valign="middle">318</td></tr><tr><td align="left" valign="middle">Insula</td><td align="left" valign="middle">R</td><td align="left" valign="middle">32</td><td align="left" valign="middle">20</td><td align="left" valign="middle">-14</td><td align="left" valign="middle">4.69</td><td align="left" valign="middle">218</td></tr><tr><td align="left" valign="middle">STG</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-50</td><td align="left" valign="middle">-16</td><td align="left" valign="middle">-6</td><td align="left" valign="middle">4.49</td><td align="left" valign="middle">686</td></tr><tr><td align="left" valign="middle">STS</td><td align="left" valign="middle">R</td><td align="left" valign="middle">54</td><td align="left" valign="middle">-4</td><td align="left" valign="middle">-14</td><td align="left" valign="middle">4.08</td><td align="left" valign="middle">252</td></tr><tr><td align="left" valign="middle">AMY</td><td align="left" valign="middle">R</td><td align="left" valign="middle">20</td><td align="left" valign="middle">-2</td><td align="left" valign="middle">-12</td><td align="left" valign="middle">3.84</td><td align="left" valign="middle">107</td></tr><tr><td align="left" valign="middle">IFGtri</td><td align="left" valign="middle">R</td><td align="left" valign="middle">50</td><td align="left" valign="middle">46</td><td align="left" valign="middle">-4</td><td align="left" valign="middle">3.78</td><td align="left" valign="middle">105</td></tr><tr><td align="left" valign="middle" colspan="7">[EG] × [A<sub>90</sub>, A<sub>70</sub>, F<sub>90</sub>, F<sub>70</sub> &gt; AF<sub>50</sub>] × [cTBS<sub>post</sub> &gt; cTBS<sub>pre</sub>]</td></tr><tr><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Region label</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Hemisphere</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Y</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Y</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">MNI <italic>Z</italic></td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">T-value</td><td align="left" valign="middle" style="border-bottom: solid thin; border-top: solid thin">Cluster size (voxel count)</td></tr><tr><td align="left" valign="middle">IFGtri</td><td align="left" valign="middle">R</td><td align="left" valign="middle">54</td><td align="left" valign="middle">34</td><td align="left" valign="middle">10</td><td align="left" valign="middle">5.21</td><td align="left" valign="middle">324</td></tr><tr><td align="left" valign="middle">Cerebellum</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-14</td><td align="left" valign="middle">84</td><td align="left" valign="middle">-14</td><td align="left" valign="middle">4.66</td><td align="left" valign="middle">4273</td></tr><tr><td align="left" valign="middle">STS</td><td align="left" valign="middle">R</td><td align="left" valign="middle">58</td><td align="left" valign="middle">-32</td><td align="left" valign="middle">2</td><td align="left" valign="middle">4.60</td><td align="left" valign="middle">822</td></tr><tr><td align="left" valign="middle">STS</td><td align="left" valign="middle">R</td><td align="left" valign="middle">54</td><td align="left" valign="middle">-4</td><td align="left" valign="middle">-12</td><td align="left" valign="middle">4.38</td><td align="left" valign="middle">372</td></tr><tr><td align="left" valign="middle">STG</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-54</td><td align="left" valign="middle">-42</td><td align="left" valign="middle">8</td><td align="left" valign="middle">4.25</td><td align="left" valign="middle">626</td></tr><tr><td align="left" valign="middle">IFGtri</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-58</td><td align="left" valign="middle">20</td><td align="left" valign="middle">8</td><td align="left" valign="middle">4.19</td><td align="left" valign="middle">713</td></tr><tr><td align="left" valign="middle">STG</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-50</td><td align="left" valign="middle">-20</td><td align="left" valign="middle">-6</td><td align="left" valign="middle">3.82</td><td align="left" valign="middle">66</td></tr><tr><td align="left" valign="middle">MFG</td><td align="left" valign="middle">L</td><td align="left" valign="middle">-4</td><td align="left" valign="middle">64</td><td align="left" valign="middle">28</td><td align="left" valign="middle">3.47</td><td align="left" valign="middle">98</td></tr></tbody></table><table-wrap-foot><fn id="TFN2"><p id="P59">Statistical threshold: p&lt;.05 FWE cluster correction (voxelwise <italic>p</italic>&lt;.005 uncorrected, k&gt;59).</p><p id="P60"><italic>SPL</italic> superior parietal lobule; <italic>STG</italic> superior temporal gyrus; <italic>MFG</italic> middle frontal gyrus; <italic>STS</italic> superior temporal sulcus; <italic>AMY</italic> amygdala; <italic>IFGtri</italic> inferior frontal gyrus <italic>pars triangularis;</italic> L: left hemisphere; R: right hemisphere.</p></fn></table-wrap-foot></table-wrap></floats-group></article>