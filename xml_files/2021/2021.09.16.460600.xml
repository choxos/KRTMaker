<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS145917</article-id><article-id pub-id-type="doi">10.1101/2021.09.16.460600</article-id><article-id pub-id-type="archive">PPR396397</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Task-Level Value Affects Trial-Level Reward Processing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hassall</surname><given-names>Cameron D.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Hunt</surname><given-names>Laurence T.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Holroyd</surname><given-names>Clay B.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Psychiatry, University of Oxford, Oxford, UK</aff><aff id="A2"><label>2</label>Department of Experimental Psychology, Ghent University, Ghent, Belgium</aff><aff id="A3"><label>3</label>Department of Psychology, University of Victoria, Victoria, BC</aff><author-notes><corresp id="CR1">Correspondence: <email>clay.holroyd@ugent.be</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="preprint"><day>13</day><month>06</month><year>2022</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Despite disagreement about how anterior cingulate cortex (ACC) supports decision making, a recent hypothesis suggests that activity in this region is best understood in the context of a task or series of tasks. One important task-level variable is <italic>average reward</italic> because it is both a known driver of effortful behaviour and an important determiner of the tasks in which we choose to engage. Here we asked how average task value affects reward-related ACC activity. To answer this question, we measured a reward-related signal said to be generated in ACC called the reward positivity (RewP) while participants gambled in three tasks of differing average value. The RewP was reduced in the high-value task, an effect that was not explainable by either reward magnitude or outcome expectancy. This result suggests that ACC does not evaluate outcomes and cues in isolation, but in the context of the value of the current task.</p></abstract><kwd-group><kwd>electroencephalography</kwd><kwd>reward positivity</kwd><kwd>anterior cingulate cortex</kwd><kwd>average task value</kwd></kwd-group></article-meta></front><body><p id="P2">The role of anterior cingulate cortex (ACC) in decision making has been hotly debated (<xref ref-type="bibr" rid="R18">Holroyd &amp; Verguts, 2021</xref>). According to one view, ACC supports reward-guided behaviours like foraging by encoding the value of switching from one task to another (<xref ref-type="bibr" rid="R21">Kolling et al., 2016</xref>). Others argue that ACC selects from among competing tasks by computing the <italic>expected value of control</italic>, balancing effort and task value (EVC: <xref ref-type="bibr" rid="R42">Shenhav et al., 2013</xref>). Computationally, ACC has been proposed to follow the principles of hierarchical reinforcement learning, exerting control over lower-level systems when ongoing rewards are worse than the average task value (<xref ref-type="bibr" rid="R17">Holroyd &amp; McClure, 2015</xref>).</p><p id="P3">In each of these approaches, ACC activity depends on task value. This claim has empirical support; neurons in monkey ACC, for example, encode both trial-by-trial reward and average task value (<xref ref-type="bibr" rid="R2">Amiez et al., 2006</xref>). But how does <italic>trial</italic>-level reward processing interact with <italic>task</italic>-level value processing in ACC? One possibility is via a reward prediction error (RPE), a concept borrowed from reinforcement learning (RL) that indicates whether ongoing events are better or worse than expected (<xref ref-type="bibr" rid="R43">Sutton &amp; Barto, 2018</xref>). RPEs are proposed to be carried by the midbrain dopamine system to striatal and cortical targets (<xref ref-type="bibr" rid="R40">Schultz et al., 1997</xref>), including ACC (<xref ref-type="bibr" rid="R1">Amiez et al., 2005</xref>; <xref ref-type="bibr" rid="R14">Holroyd &amp; Coles, 2002</xref>). According to the standard “model-free” RL approach, called temporal difference learning, the RPE elicited by reward delivery is computed by subtracting the expected value of an action from the just-received reward. The standard reinforcement learning view is that RPEs are “one-step” computations, comparing the value of the current state to the value of the previous state only. Under this view, dopaminergic RPEs depend only on action/reward values, not the average task value.</p><p id="P4">Alternatively, midbrain dopamine activity may index a more general RPE rather than just an immediate, “one-step” RPE. Indeed, there is evidence that midbrain RPEs are sensitive to task context. For example, monkey dopamine neurons appear to track patterns in reward history beyond what would be predicted by a standard RL algorithm (<xref ref-type="bibr" rid="R31">Nakahara et al., 2004</xref>). Reinforcement learning algorithms have therefore been augmented with internal models of the environment resulting in “model-based” RL algorithms. Both model-free and model-based reinforcement learning computations appear to drive activity of midbrain dopamine neurons (<xref ref-type="bibr" rid="R5">Collins &amp; Cockburn, 2020</xref>; <xref ref-type="bibr" rid="R7">Daw et al., 2011</xref>). There is therefore good reason to believe that midbrain RPE signals may be sensitive to task-level factors such as average task value.</p><p id="P5">In humans, midbrain RPE signals are thought to modulate ACC activity in a way that is measurable at the scalp. It has been proposed that a component of the event-related potential (ERP) called the reward positivity (RewP) varies as a function of RPE magnitude (<xref ref-type="bibr" rid="R14">Holroyd &amp; Coles, 2002</xref>; <xref ref-type="bibr" rid="R39">Sambrook &amp; Goslin, 2015</xref>). In other words, the RewP provides a convenient readout of the degree to which outcomes differ from learned <italic>action</italic> values. Although there is debate about the neural source of the RewP, many studies have localized it to ACC (see review by <xref ref-type="bibr" rid="R45">Walsh &amp; Anderson, 2012</xref>). Our goal here was to investigate whether and how reward-related ACC activity, as indexed by the RewP, varies with <italic>task</italic> value. Importantly, action value and task value are partially dissociable in reinforcement learning frameworks: High-value actions can occur in low-value tasks, and vice-versa.</p><p id="P6">To answer this question, we manipulated task value and action value by varying the proportion of “low-value” and “high-value” actions in three probabilistic learning tasks. EEG was recorded from participants as they attempted to learn correct actions for six predictive cues. Some of the cues were “high-value”, indicating that a correct response would likely yield a reward. Other cues were “low-value”, indicating that reward and non-reward outcomes were equally likely regardless of the response. We did not vary the value of the reward itself, which can affect the amplitude of the RewP (<xref ref-type="bibr" rid="R22">Kreussel et al., 2012</xref>; <xref ref-type="bibr" rid="R39">Sambrook &amp; Goslin, 2015</xref>). Rather, we varied the proportion of high- and low-value cues in each task: either all low-value, all high-value, or an even split. Our goal here was to vary reward at the task level while keeping trial-level rewards constant. Importantly, the same cue type (same reward expectancy and reward magnitude) appeared in multiple tasks, allowing us to isolate the effect of task value on feedback-locked signals.</p><p id="P7">In addition to the RewP, we also examined the cue-locked ERP. There is evidence that reward-predicting cues can elicit a RewP-like signal (<xref ref-type="bibr" rid="R15">Holroyd et al., 2011</xref>; <xref ref-type="bibr" rid="R25">Krigolson et al., 2014</xref>). The computational explanation for this is that RPEs propagate backward in time to the earliest indicator that things are better or worse than expected. In a task with mixed high- and low-value cues, we might therefore expect high-value cues to elicit a positive prediction error (a positive RewP deflection) relative to low-value cues. Conversely, we would not expect a cue-locked prediction error to be elicited in a task with uniform cue values, because the cues all make the same prediction about upcoming rewards within the task. To summarize, trial cues ought to elicit positive/negative prediction errors in the "mixed value" task and no prediction error in the "uniform value" tasks</p><sec id="S1" sec-type="methods"><title>Method</title><sec id="S2" sec-type="subjects"><title>Participants</title><p id="P8">We tested 36 participants with no known neurological impairments and with normal or corrected-to-normal vision (11 male, 5 left-handed, 1 ambidextrous) across two testing sites. At the first testing site we tested 24 University of Victoria (UVic) undergraduate participants. At the second testing site we tested 12 University of Oxford (Oxford) graduate students and community members. Participant ages ranged from 18 to 77, <italic>M</italic> = 27.6, <italic>SD</italic> = 14.5. UVic participants received bonus credit in an undergraduate psychology course. Oxford participants were compensated £20 (£10 per hour). Additionally, each participant received a performance-dependent monetary bonus. The study was approved by the University of Victoria Human Research Ethics Board and the Medical Sciences Interdivisional Research Ethics Committee at the University of Oxford. All participants gave written informed consent.</p><p id="P9">This experiment required that participants learn to make optimal responses when this was possible, i.e., in response to high-value cues. As such, we applied the following a priori criterion: only the data of participants who made a correct response on at least 60% of the learnable trials in both the mid-value task and the high-value task were included in the main EEG analysis. 12 of the 36 participants did not meet this criterion and their data were therefore removed from the main analysis. However, the data of these 12 participants were included in a correlational analysis relating performance to the RewP. Finally, the data of one participant were excluded from all analyses due to excessive EEG artifacts (across all conditions, the average trial rejection rate for this participant was 87%). This left a total of 24 participants for the main analysis and 35 participants for the correlational analysis.</p></sec><sec id="S3"><title>Apparatus and Procedure</title><p id="P10">Participants were seated 60 cm in front of an LCD display (60 Hz, 1024 by 1280 pixels). Visual stimuli were presented using the Psychophysics Toolbox Extension (<xref ref-type="bibr" rid="R4">Brainard, 1997</xref>; <xref ref-type="bibr" rid="R20">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="R34">Pelli, 1997</xref>) for MATLAB (Version 8.2, MathWorks, Natick, USA). Participants were given both verbal and written instructions in which they were asked to minimize head and eye movements.</p><p id="P11">Participants were told that they would be gambling in three different casinos. Each casino contained six different slot machines, and each slot machine was represented by a unique, coloured shape. Prior to "entering" a casino, the message "New Casino, New Coloured Shapes" was displayed. Shapes were reused across casinos, but with different colours (randomly chosen for each participant). The slot machines were described to participants as having two arms – a left arm and a right arm. Participants were instructed to, upon the appearance of a slot machine (a coloured shape), select one of the arms by pressing the corresponding key on a keyboard (the s-key to select the left arm, or the k-key to select the right arm). Participants were also told that their gamble would result in win or a loss – each outcome represented by a randomly-assigned fruit – and that for each slot machine "pulling one arm may be more likely to result in a win compared to pulling the other arm". Wins resulted in a gain of $0.03, while losses resulted in no gain ($0.00). Participants were informed that their goal was to win as much money as possible and that they would be paid their total at the end of the experiment.</p><p id="P12">Trials were grouped by casino, and each casino was entered only once. Casino order was counterbalanced with six possible casino orderings and four participants assigned to each ordering. Within a casino, participants encountered the 6 slot machines 24 times in random order (144 gambles). Unknown to participants, there were two types of slot machine: high-value and low-value. Each high-value slot machine had one arm (randomly chosen) that, when selected, would result in a win with 80% probability (selecting the other arm resulted in a win with a 20% probability). In contrast, low-value slot machines had no correct answer: there was a 50% probability of winning, regardless of which arm was pulled. The casinos contained either only low-value slot machines (the low-value task), high-value slot machines (the high-value task), or an even split between low- and high-value slot machines (the mid-value task). As each slot machine was encountered the same number of times, there was no learning advantage related to the number of exposures. Upon leaving a casino the total amount won within that casino was displayed.</p><p id="P13">Each trial began with the appearance of a white fixation cross presented against a black background (<xref ref-type="fig" rid="F1">Figure 1</xref>). The fixation cross, and all other visual stimuli, subtended 2° of visual angle. After 400–600 ms, the fixation cross was replaced by the coloured shape representing the current trial’s slot machine (the "cue" ). After 1000 ms, a 50 ms 400 Hz sine tone signalled participants to choose an arm (left or right) by pressing the corresponding key on a keyboard. The purpose of the delay between the cue and the tone was to isolate cue-related neural activity from response-related neural activity. The coloured shape/slot machine remained on the display until the participant responded (or until 2000 ms if no valid response was made). Finally, another fixation cross appeared for 400–600 ms, followed by the feedback stimulus for 1000 ms. Two images of fruit were used as feedback stimuli, chosen at random for each participant from six possible images. If the participant responded prior to the onset of the tone, the message "too fast" was displayed instead. If no response was made within the 2000 ms response window, or if a non-response key was pressed, the message "invalid" was displayed. In both cases (fast/invalid responses) the trial was excluded from both the behavioural analysis and the EEG analysis.</p></sec><sec id="S4"><title>Data Collection</title><p id="P14">For each trial, the experimental software recorded reaction time (time since the onset of the tone), response (left, right, invalid, or no response), and casino/slot machine information. We also recorded casino value (low, mid, or high), slot machine value (low or high), trial outcome (win or lose) and – for high-value slot machines – whether or not the correct arm was chosen (that is, the arm more likely to result in a win).</p><p id="P15">EEG was recorded from 35 Ag/AgCl electrode locations using Brain Vision Recorder (Brain Products GmbH, Gilching, Germany). The electrodes were mounted in a fitted cap (EASYCAP GmbH, Wörthsee , Germany) with a standard 10-20 layout and were recorded with respect to an average reference. Electrode impedances were lowered below 20 kΩ prior to recording and the EEG data were sampled at 250 Hz and amplified (UVic: Quick Amp, Oxford: actiCHamp Plus, Brain Products GmbH, Gilching, Germany).</p></sec><sec id="S5"><title>Data Analysis</title><p id="P16">For all three tasks (low-value, mid-value, high-value) we computed the mean proportion of trials resulting in a win. For the mid-value and high-value tasks we computed, for each participant and task, the likelihood of a correct response to the high-value cues. (Recall that low-value cues had no correct response.) As mentioned previously, participants for whom this likelihood fell below 60%, in either the mid-value task or the high-value task, were excluded from the main analyses (seven participants in total). For visualization purposes, we computed a sliding window measure of performance in the mid-value and high-value tasks: the proportion of wins in 30 trials, moving the window by 1 trial each time.</p><p id="P17">The EEG was analyzed in MATLAB 2022a (MathWorks, Natick, USA) using the EEGLAB library (<xref ref-type="bibr" rid="R8">Delorme &amp; Makeig, 2004</xref>). EEG data were first bandpass filtered (0.1– 30 Hz, UVic: 60 Hz notch, Oxford: 50 Hz notch) and re-referenced to the average of the mastoid signals. Ocular artifacts were identified and removed using independent component analysis (ICA). The ICA was trained on three-second epoch starting at the presentation of the cue. Ocular components were identified using the <italic>iclabel</italic> function and removed from the continuous data. We removed components which <italic>iclabel</italic> determined were more likely to be eye-related than brain-related.</p><sec id="S6"><title>ERP Analysis</title><p id="P18">We constructed 800 ms epochs around the appearance of each cue and feedback stimulus (-200 to 600 ms). The data were then checked for remaining artifacts, and any epochs containing a change in voltage of more than 40 μV per sample point or an overall change in voltage of more than 150 μV across the epoch were excluded from the analysis. On average, 1.0% (<italic>SD</italic> = 1.5%) of cue-locked epochs and 0.8% (<italic>SD</italic> = 1.3%) of feedback-locked epochs were excluded. We also excluded the first ten trials of each task, as participants became familiarized with the new cues. We then averaged over the remaining epochs to create mean cue-locked, win-locked, and loss-locked waveforms for each task (low-value, mid-value, high-value), cue (low-value, high-value), and participant. To quantify the RewP, we used the difference wave method by subtracting the mean loss ERP from the corresponding mean win ERP for each task and cue. The difference wave approach is especially useful for isolating the RewP because it minimizes component overlap (<xref ref-type="bibr" rid="R23">Krigolson, 2017</xref>; <xref ref-type="bibr" rid="R26">Luck, 2014</xref>). We then examined the electrode location and time window recommended by <xref ref-type="bibr" rid="R39">Sambrook and Goslin (2015)</xref>: FCz from 240–340 ms post feedback. The RewP was defined as the maximum voltage recorded within this window for each task value, cue value, and participant. For completeness, we also computed the mean voltage from 240–340 ms at electrode FCz for each cue value (low, high), outcome (win, loss), and task value (low, mid, high). We then computed effect sizes (Cohen’s <italic>d</italic>) comparing similar cue values and outcomes (see <xref ref-type="supplementary-material" rid="SD1">Supplemental Table 1</xref>). A similar approach was used for the cue-locked ERPs (see Supplemental Material).</p></sec><sec id="S7"><title>Inferential Statistics</title><p id="P19">To confirm our task value manipulation, we compared the mean proportion of wins in each task using a one-way repeated-measures ANOVA. Partial and generalized eta-squared were computed as:</p><disp-formula id="FD1"><mml:math id="M1"><mml:msup><mml:msub><mml:mi>η</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mo>∨</mml:mo></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mo>∨</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>∨</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:msub><mml:mi>η</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mo>∨</mml:mo></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mo>∨</mml:mo></mml:msub><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>∨</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><p id="P20">where <italic>SS<sub>V</sub></italic> is the sum of squares of the value effect (low, mid, high), <italic>SS<sub>sV</sub></italic> is the error sum of squares of the value effect, and <italic>SS<sub>S</sub></italic> is the sum of squares between subjects. To test whether performance differed between the mid-value and high-value block – that is, whether participants learned about high-value cues differently – we used a repeated-measures <italic>t</italic>-test.</p><p id="P21">To determine the effect of average task value on the RewP, we compared RewP scores using repeated measures <italic>t</italic>-tests. To avoid possible outcome frequency confounds, we only compared RewP scores from conditions with similar outcome frequencies (<xref ref-type="bibr" rid="R23">Krigolson, 2017</xref>). Specifically, we compared the low-task, low-cue RewP (infrequent rewards in an infrequent reward context) to the mid-task, low-cue RewP (infrequent rewards in a mid-frequency reward context). We then compared the mid-task, high-cue RewP (frequent rewards in a mid-frequency reward context) to the high-task, high-cue RewP (frequent rewards in a frequent reward context).</p><p id="P22">For all <italic>t</italic>-tests, we first checked the assumption of normality of each variable using the Shapiro-Wilk test. The assumption was met for all variables except for the RewP score in the high-task, high-cue condition. As the <italic>t</italic>-test is robust to non-normality, no correction was made. For each comparison, we also computed Cohen’s <italic>d</italic> for paired-samples t-tests as:</p><disp-formula id="FD2"><mml:math id="M2"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>'</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula><p id="P23">where <italic>M<sub>diff</sub></italic> is the mean difference between the scores being compared and <italic>s<sub>diff</sub></italic> is the standard deviation of the difference of the scores being compared (<xref ref-type="bibr" rid="R6">Cumming, 2014</xref>).</p></sec><sec id="S8"><title>Modelling</title><p id="P24">We used computational modelling to explore the possibility that participants may have used a different strategy in each task, either due to differences in average task value, or in response to the different outcome probabilities. For example, participants may have adjusted the degree to which they weighed past outcomes when considering their current choice. Participants may have also adjusted their rate of exploration. Each of these behaviours (weighing of past outcomes, exploration) is expressed in a reinforcement learning (RL) model as the learning rate and temperature parameters, respectively. To determine whether these behaviours differed in our tasks, we modelled trial-by-trial choices for each participant, task value, and cue value.</p><p id="P25">For each cue, the model maintained action values <italic>Q</italic> associated with each response (left, right). Following a chosen action <italic>a</italic>, a prediction error <italic>δ</italic> was computed according to:</p><disp-formula id="FD3"><mml:math id="M3"><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula><p id="P26">The action value of the chosen action <italic>a</italic> was then updated according to:</p><disp-formula id="FD4"><mml:math id="M4"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mi>δ</mml:mi></mml:math></disp-formula><p id="P27">where <italic>α</italic> is the learning rate. The likelihood of a particular action <italic>a</italic> was determined by the <italic>Q</italic>-values according to the softmax equation, depending on the temperature parameter <italic>τ</italic>:</p><disp-formula id="FD5"><mml:math id="M5"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:math></disp-formula><p id="P28">Model fit was assessed by combining the chosen action probabilities across all trials <italic>t</italic> using negative log-likelihood:</p><disp-formula id="FD6"><mml:math id="M6"><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mi>∑</mml:mi><mml:mi>t</mml:mi></mml:munder><mml:mrow><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula><p id="P29">A good model fit meant that the model assigned a high likelihood to the chosen action on each trial, resulting in a low negative log-likelihood. Parameters (<italic>α</italic>, <italic>τ</italic>) were individually fit for each participant, task value, and cue value to minimize negative log-likelihood using the MATLAB function <italic>fmincon</italic> (Optimization Toolbox, Version 9.3, MathWorks, Natick, USA). Parameters were restricted to recoverable values: learning rates from 0.1 to 1 and temperatures from 0.01 to 1 (see <xref ref-type="supplementary-material" rid="SD1">Supplemental Material</xref> for parameter and model recovery). For comparison, we also computed the negative log-likelihood of a random model, which used an action likelihood of 0.5 on each trial. Model fits for each cue value were then compared via repeated-measures <italic>t</italic>-tests, as described above. We also tested whether the optimized model parameters (<italic>α</italic>, <italic>τ</italic>) differed by cue value.</p></sec></sec></sec><sec id="S9" sec-type="results"><title>Results</title><sec id="S10"><title>Behavioural Results</title><p id="P30">The proportion of winning trials differed by task (low-value: 49.82 %, 95% CI [48.33, 51.31], mid-value: 59.07 %, 95% CI [57.24, 60.90], high-value: 67.53 %, 95% CI [64.61, 70.46]), <italic>F</italic>(2,46) = 69.38, <italic>p</italic> &lt; .001, η<sub>p</sub><sup>2</sup> = 0.75, η<sub>g</sub><sup>2</sup> = 0.67. Note that since each win was worth the same amount, the total reward also differed between tasks (<xref ref-type="fig" rid="F2">Figure 2a</xref>). There was no evidence of a difference between mean performance in the mid-value block (81.11%, 95% CI [76.26, 85.96]) and mean performance in the high-value block (78.06%, 95% CI [73.64, 82.48]), <italic>t</italic>(23) = 1.29, <italic>p</italic> = .211, Cohen’s <italic>d</italic> = 0.21 (<xref ref-type="fig" rid="F2">Figure 2b–c</xref>). Recall that mean performance for both the mid-value blocks and the high-value blocks was defined as the proportion of high-value cues that were responded to correctly.</p></sec><sec id="S11"><title>Feedback-Locked EEG (RewP)</title><p id="P31">After collapsing across the average feedback-locked waveforms (<xref ref-type="fig" rid="F3">Figure 3a</xref>) we observed a scalp topography difference consistent with a RewP (<xref ref-type="fig" rid="F3">Figure 3b</xref>). After constructing conditional difference waves (<xref ref-type="fig" rid="F3">Figure 3c</xref>), we observed a significant effect of task value on the RewP amplitude elicited by high-value cues, indicating that RewP size was modulated by task value: The RewP following high-value cues was larger in the mid-value task (7.39 μV, 95% CI [5.30, 9.48]), <italic>t</italic>(23) = 2.53, <italic>p</italic> = .019, Cohen’s <italic>d</italic> = 0.52 than in the high-value task (5.29 μV, 95% CI [3.83, 6.75]). The same difference was not observed when the RewP following low-value cues in the low-value task (6.50, 95% CI [4.73, 8.27]) was compared to the RewP following low-value cues in the mid-value task (6.98, 95% CI [4.88, 9.08], <italic>t</italic>(23) = 0.51, <italic>p</italic> = .615, Cohen’s <italic>d</italic> = 0.10 (<xref ref-type="fig" rid="F3">Figure 3d</xref>). The observed RewP increase (mid-value task minus high-value task) depended on the mean performance across all conditions, <italic>r</italic> = 0.45, <italic>p</italic> = .007. In other words, more accurate participants showed a greater RewP increase in the mid-value task (<xref ref-type="fig" rid="F4">Figure 4</xref>).</p></sec><sec id="S12"><title>Cue-Locked EEG (RewP)</title><p id="P32">No cue-locked RewP difference was observed (see <xref ref-type="supplementary-material" rid="SD1">Supplemental Material</xref>).</p></sec><sec id="S13"><title>Modelling Results</title><p id="P33">In the mid-value task, the fit learning rate parameter <italic>α</italic> was greater for low-value cues (0.52, 95% CI [0.39, 0.65]) than high-value cues (0.23, 95% CI [0.14, 0.32]), <italic>t</italic>(23) = 3.72, <italic>p</italic> = .001, Cohen’s <italic>d</italic> = 0.76, suggesting that participants weighed recent outcomes more heavily for low-value cues. The same difference was not observed between low-value cues in the low-value task (0.46, 95% CI [0.32, 0.60]) and high-value cues in the high-value task (0.32, 95% CI [0.20, 0.44]), <italic>t</italic>(23) = 1.64, <italic>p</italic> = .114, Cohen’s <italic>d</italic> = 0.34. See <xref ref-type="fig" rid="F5">Figure 5a</xref>.</p><p id="P34">Furthermore, within the mid-value task, τ was greater for low-value cues (0.46, 95% CI [0.32, 0.59]) compared to high-value cues (0.24, 95% CI [0.13, 0.35]), <italic>t</italic>(23) = 3.13, <italic>p</italic> = .005, Cohen’s <italic>d</italic> = 0.64 (<xref ref-type="fig" rid="F5">Figure 5b</xref>). Likewise, the fit temperature parameter <italic>τ</italic> was greater in the low-value task (0.52, 95% CI [0.37, 0.67]) compared to the high-value task (0.27, 95% CI [0.17, 0.38]), <italic>t</italic>(24) = 3.79, <italic>p</italic> &lt; .001, Cohen’s <italic>d</italic> = 0.77, suggesting that choices were more random and less value-driven when the average task value was low. See <xref ref-type="supplementary-material" rid="SD1">Supplemental Material</xref> for parameter and model recovery results.</p></sec></sec><sec id="S14" sec-type="discussion"><title>Discussion</title><p id="P35">ACC activity is often studied by focussing on relatively short-term, within-trial processing of actions and events, despite empirical evidence and computational considerations suggesting that this region is more concerned about global aspects of task performance (<xref ref-type="bibr" rid="R17">Holroyd &amp; McClure, 2015</xref>; <xref ref-type="bibr" rid="R18">Holroyd &amp; Verguts, 2021</xref>; <xref ref-type="bibr" rid="R19">Holroyd &amp; Yeung, 2012</xref>; <xref ref-type="bibr" rid="R44">Umemoto et al., 2017</xref>). Here we show evidence that trial-to-trial ACC responses, as indexed by the RewP, are influenced by task value. We measured these signals in three tasks of varying average value. Despite matching outcomes for expectancy and trial-level value, the RewP was reduced when average task value was high.</p><p id="P36">This result is in line with a growing body of literature highlighting the importance of task context in understanding ACC activity. By "context" we mean task value (as in the present study) and other relevant task-level variables. For example, RewP amplitude is sensitive to the range of possible outcomes indicated by task instructions, not to the objective values of these outcomes (<xref ref-type="bibr" rid="R16">Holroyd et al., 2004</xref>). Task-level goals also matter: the amplitude of the RewP is greater when reward indicates progress towards a goal (<xref ref-type="bibr" rid="R33">Osinsky et al., 2017</xref>; <xref ref-type="bibr" rid="R41">Shahnazian et al., 2018</xref>). The RewP is also enhanced for self-relevant rewards (<xref ref-type="bibr" rid="R24">Krigolson et al., 2013</xref>; <xref ref-type="bibr" rid="R47">Yu &amp; Zhou, 2006</xref>) and in agentic situations, when perceived control of outcomes is high (<xref ref-type="bibr" rid="R12">Hassall et al., 2019</xref>; <xref ref-type="bibr" rid="R28">Martin &amp; Potts, 2011</xref>; <xref ref-type="bibr" rid="R30">Mühlberger et al., 2017</xref>; <xref ref-type="bibr" rid="R46">Yeung et al., 2005</xref>).</p><p id="P37">Traditional RL models learn action values without representing task context (such as average task value) explicitly. However, here we controlled for trial-level expectancy and yet observed a task-level influence on the RewP. This result – a reduced RewP when task value is high – is inconsistent with a model that learns at the level of actions only, but may be consistent with an average-reward learning model in which prediction errors are defined as the difference between trial rewards and the average task value (e.g., <xref ref-type="bibr" rid="R17">Holroyd &amp; McClure, 2015</xref>). Under this view, the RewP ought to be reduced when trial outcomes are closer in value to the average task value, as we observed for outcomes following high-value cues in the high-value task. Note that we did not observe a similar effect for low-value outcomes (outcomes following low-value cues). This result may be due to a difference in strategy; participants used a larger learning rate when attempting to learn about low-value cues, suggesting they were weighing recent outcomes more heavily compared to high-value cues. Additionally, low-value cues were associated with a larger temperature parameter, indicative of enhanced exploration for the more difficult problems.</p><p id="P38">There is empirical evidence that the RewP is sensitive to average reward. For example, we have previously observed that a simple RL model fails to account for RewP amplitude when participants play virtual casino games in virtual casinos (<xref ref-type="bibr" rid="R44">Umemoto et al., 2017</xref>). There, a RewP was observed to casino game outcomes and to casinos themselves – in other words, ACC is sensitive to both game values and casino values. To explain these neural responses computationally, it was necessary to combine a simple RL model with an average-value learning model. In general, representing contextual information like average task value can be beneficial in a hierarchical framework in which a limited resource such as cognitive control needs to be regulated across multiple levels. These features are implemented in the hierarchical reinforcement learning (HRL) model of ACC (<xref ref-type="bibr" rid="R17">Holroyd &amp; McClure, 2015</xref>). Under this framework, ACC signals a need for control when outcomes are worse than the average task value.</p><p id="P39">Besides the RewP, there is other evidence that the brain tracks average reward over time. In the short-term, unexpected rewards (RPEs) elicit phasic midbrain dopaminergic activity that is linked to trial-to-trial improvements in behaviour (<xref ref-type="bibr" rid="R35">Pessiglione et al., 2006</xref>). Subjectively, RPEs are associated with momentary changes in happiness (<xref ref-type="bibr" rid="R37">Rutledge et al., 2014</xref>, <xref ref-type="bibr" rid="R38">2015</xref>). However, the brain also tracks rewards in the long-term. There is fMRI evidence that BOLD activity in midbrain correlates with average reward, even when controlling for RPEs (<xref ref-type="bibr" rid="R36">Rigoli et al., 2016</xref>). The proposed mechanism behind this effect is tonic midbrain dopamine, which is thought to track average reward rate in order to support motivational vigour (<xref ref-type="bibr" rid="R32">Niv et al., 2007</xref>).</p><p id="P40">Relevant here is the possibility that phasic and tonic dopamine interact. In particular, a high level of tonic dopamine is thought to suppress phasic activity (<xref ref-type="bibr" rid="R3">Bilder et al., 2004</xref>; <xref ref-type="bibr" rid="R10">Grace, 1991</xref>, <xref ref-type="bibr" rid="R11">2000</xref>). One piece of evidence for this comes from EEG; the RewP, which is linked to phasic dopamine, is reduced in individuals with increased tonic dopamine (<xref ref-type="bibr" rid="R27">Marco-Pallarés et al., 2009</xref>; <xref ref-type="bibr" rid="R29">Mueller et al., 2014</xref>; but see <xref ref-type="bibr" rid="R13">Heitland et al., 2012</xref>; <xref ref-type="bibr" rid="R9">Foti &amp; Hajcak, 2012</xref>). This result suggests that the interaction between phasic and tonic dopamine may provide a dopamine-based mechanism for our ERP result: increased tonic activity in the high-value task relative to the mid-value task resulted in reduced phasic activity (and a concomitant decrease in RewP amplitude).</p><p id="P41">The ACC has long been known to respond to trial-level events such as cues and rewards (<xref ref-type="bibr" rid="R14">Holroyd &amp; Coles, 2002</xref>). Another picture of the ACC is emerging – that of a region tasked with supporting extended sequences of actions (<xref ref-type="bibr" rid="R18">Holroyd &amp; Verguts, 2021</xref>; <xref ref-type="bibr" rid="R19">Holroyd &amp; Yeung, 2012</xref>). Here we provide evidence that ACC activity depends not only on trial-level features (is this a good outcome/cue?) but also task-level variables (how good is the task?) We suggest that a global view of ACC function will prove more fruitful than studying its neural responses at a molecular level.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Material</label><media xlink:href="EMS145917-supplement-Supplemental_Material.pdf" mimetype="application" mime-subtype="pdf" id="d300aAdJbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S15"><title>Acknowledgements</title><p>C.D.H. was supported by a Natural Sciences and Engineering Research Council of Canada (NSERC) Postdoctoral Fellowship (PDF-546078-2020). L.T.H. was supported by a Sir Henry Dale Fellowship from the Royal Society and Wellcome (208789/Z/17/Z). C.B.H. was supported by Natural Sciences and Engineering Research Council of Canada (NSERC) Grant # 312409-05 and by funding from the European Research Council (ERC) under the EU’s Horizon 2020 Research and Innovation Programme (grant agreement no. 787307). Data collection at the Oxford testing site was supported by the NIHR Oxford Health Biomedical Research Centre. The Oxford testing site is part of the Wellcome Centre for Integrative Neuroimaging, which was supported by core funding from Wellcome Trust (203139/Z/16/Z).</p></ack><sec id="S16" sec-type="data-availability"><title>Data Availability Statement</title><p id="P42">Data for participants at the Oxford testing site is available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds004147.v1.0.0">https://doi.org/10.18112/openneuro.ds004147.v1.0.0</ext-link>. Participants at the UVic testing site did not consent for their raw or preprocessed EEG data files to be publicly shared. Task and analysis scripts are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/chassall/averagetaskvalue">https://github.com/chassall/averagetaskvalue</ext-link>.</p></sec><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amiez</surname><given-names>C</given-names></name><name><surname>Joseph</surname><given-names>JP</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name></person-group><article-title>Anterior cingulate error-related activity is modulated by predicted reward</article-title><source>European Journal of Neuroscience</source><year>2005</year><volume>21</volume><issue>12</issue><fpage>3447</fpage><lpage>3452</lpage><pub-id pub-id-type="pmcid">PMC1913346</pub-id><pub-id pub-id-type="pmid">16026482</pub-id><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04170.x</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amiez</surname><given-names>C</given-names></name><name><surname>Joseph</surname><given-names>JP</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name></person-group><article-title>Reward Encoding in the Monkey Anterior Cingulate Cortex</article-title><source>Cerebral Cortex</source><year>2006</year><volume>16</volume><issue>7</issue><fpage>1040</fpage><lpage>1055</lpage><pub-id pub-id-type="pmcid">PMC1913662</pub-id><pub-id pub-id-type="pmid">16207931</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhj046</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bilder</surname><given-names>RM</given-names></name><name><surname>Volavka</surname><given-names>J</given-names></name><name><surname>Lachman</surname><given-names>HM</given-names></name><name><surname>Grace</surname><given-names>AA</given-names></name></person-group><article-title>The Catechol-O-Methyltransferase Polymorphism: Relations to the Tonic–Phasic Dopamine Hypothesis and Neuropsychiatric Phenotypes</article-title><source>Neuropsychopharmacology</source><year>2004</year><volume>29</volume><issue>11</issue><fpage>1943</fpage><lpage>1961</lpage><pub-id pub-id-type="pmid">15305167</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AGE</given-names></name><name><surname>Cockburn</surname><given-names>J</given-names></name></person-group><article-title>Beyond dichotomies in reinforcement learning</article-title><source>Nature Reviews Neuroscience</source><year>2020</year><volume>21</volume><issue>10</issue><fpage>576</fpage><lpage>586</lpage><pub-id pub-id-type="pmcid">PMC7800310</pub-id><pub-id pub-id-type="pmid">32873936</pub-id><pub-id pub-id-type="doi">10.1038/s41583-020-0355-6</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cumming</surname><given-names>G</given-names></name></person-group><article-title>The New Statistics: Why and How</article-title><source>Psychological Science</source><year>2014</year><volume>25</volume><issue>1</issue><fpage>7</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">24220629</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Model-Based Influences on Humans’ Choices and Striatal Prediction Errors</article-title><source>Neuron</source><year>2011</year><volume>69</volume><issue>6</issue><fpage>1204</fpage><lpage>1215</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.cell.com/neuron/fulltext/S0896-6273(11)00125-5?">https://doi.org/10/fcnkgn</ext-link></comment><pub-id pub-id-type="pmcid">PMC3077926</pub-id><pub-id pub-id-type="pmid">21435563</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.02.027</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><article-title>EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><year>2004</year><volume>134</volume><issue>1</issue><fpage>9</fpage><lpage>21</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0165027003003479?via%3Dihub">https://doi.org/10/bqr2f2</ext-link></comment><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foti</surname><given-names>D</given-names></name><name><surname>Hajcak</surname><given-names>G</given-names></name></person-group><article-title>Genetic variation in dopamine moderates neural response during reward anticipation and delivery: Evidence from event-related potentials</article-title><source>Psychophysiology</source><year>2012</year><volume>49</volume><issue>5</issue><fpage>617</fpage><lpage>626</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/10.1111/j.1469-8986.2011.01343.x">https://doi.org/10/gfvd4j</ext-link></comment><pub-id pub-id-type="pmid">22335281</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grace</surname><given-names>AA</given-names></name></person-group><article-title>Phasic versus tonic dopamine release and the modulation of dopamine system responsivity: A hypothesis for the etiology of schizophrenia</article-title><source>Neuroscience</source><year>1991</year><volume>41</volume><issue>1</issue><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">1676137</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grace</surname><given-names>AA</given-names></name></person-group><article-title>The tonic/phasic model of dopamine system regulation and its implications for understanding alcohol and psychostimulant craving</article-title><source>Addiction</source><year>2000</year><volume>95</volume><issue>8s2</issue><fpage>119</fpage><lpage>128</lpage><pub-id pub-id-type="pmid">11002907</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassall</surname><given-names>CD</given-names></name><name><surname>Hajcak</surname><given-names>G</given-names></name><name><surname>Krigolson</surname><given-names>OE</given-names></name></person-group><article-title>The importance of agency in human reward processing</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><year>2019</year><comment><ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.3758/s13415-019-00730-2">https://doi.org/10/gf3tgc</ext-link></comment><pub-id pub-id-type="pmid">31187443</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heitland</surname><given-names>I</given-names></name><name><surname>Oosting</surname><given-names>RS</given-names></name><name><surname>Baas</surname><given-names>JMP</given-names></name><name><surname>Massar</surname><given-names>SAA</given-names></name><name><surname>Kenemans</surname><given-names>JL</given-names></name><name><surname>Böcker</surname><given-names>KBE</given-names></name></person-group><article-title>Genetic polymorphisms of the dopamine and serotonin systems modulate the neurophysiological response to feedback and risk taking in healthy humans</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><year>2012</year><volume>12</volume><issue>4</issue><fpage>678</fpage><lpage>691</lpage><pub-id pub-id-type="pmcid">PMC3505534</pub-id><pub-id pub-id-type="pmid">22810728</pub-id><pub-id pub-id-type="doi">10.3758/s13415-012-0108-8</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Coles</surname><given-names>MG</given-names></name></person-group><article-title>The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity</article-title><source>Psychological Review</source><year>2002</year><volume>109</volume><issue>4</issue><fpage>679</fpage><comment><ext-link ext-link-type="uri" xlink:href="https://psycnet.apa.org/doiLanding?doi=10.1037/0033-295X.109.4.679">https://doi.org/10/bcr82b</ext-link></comment><pub-id pub-id-type="pmid">12374324</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Krigolson</surname><given-names>OE</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name></person-group><article-title>Reward positivity elicited by predictive cues</article-title><source>Neuroreport</source><year>2011</year><volume>22</volume><issue>5</issue><fpage>249</fpage><lpage>252</lpage><pub-id pub-id-type="pmid">21386699</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Larsen</surname><given-names>JT</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>Context dependence of the event-related brain potential associated with reward and punishment</article-title><source>Psychophysiology</source><year>2004</year><volume>41</volume><issue>2</issue><fpage>245</fpage><lpage>253</lpage><pub-id pub-id-type="pmid">15032989</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>McClure</surname><given-names>SM</given-names></name></person-group><article-title>Hierarchical control over effortful behavior by rodent medial frontal cortex: A computational model</article-title><source>Psychological Review</source><year>2015</year><volume>122</volume><issue>1</issue><fpage>54</fpage><lpage>83</lpage><pub-id pub-id-type="pmid">25437491</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name></person-group><article-title>The Best Laid Plans: Computational Principles of Anterior Cingulate Cortex</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><volume>25</volume><issue>4</issue><fpage>316</fpage><lpage>329</lpage><pub-id pub-id-type="pmid">33593641</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Yeung</surname><given-names>N</given-names></name></person-group><article-title>Motivation of extended behaviors by anterior cingulate cortex</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><issue>2</issue><fpage>122</fpage><lpage>128</lpage><pub-id pub-id-type="pmid">22226543</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><article-title>What’s new in Psychtoolbox-3</article-title><source>Perception</source><year>2007</year><volume>36</volume><issue>14</issue><fpage>1</fpage></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Wittmann</surname><given-names>MK</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Value, search, persistence and model updating in anterior cingulate cortex</article-title><source>Nature Neuroscience</source><year>2016</year><volume>19</volume><issue>10</issue><fpage>1280</fpage><lpage>1285</lpage><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/nn.4382">https://doi.org/10/f856kc</ext-link><pub-id pub-id-type="pmcid">PMC7116891</pub-id><pub-id pub-id-type="pmid">27669988</pub-id><pub-id pub-id-type="doi">10.1038/nn.4382</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreussel</surname><given-names>L</given-names></name><name><surname>Hewig</surname><given-names>J</given-names></name><name><surname>Kretschmer</surname><given-names>N</given-names></name><name><surname>Hecht</surname><given-names>H</given-names></name><name><surname>Coles</surname><given-names>MGH</given-names></name><name><surname>Miltner</surname><given-names>WHR</given-names></name></person-group><article-title>The influence of the magnitude, probability, and valence of potential wins and losses on the amplitude of the feedback negativity</article-title><source>Psychophysiology</source><year>2012</year><volume>49</volume><issue>2</issue><fpage>207</fpage><lpage>219</lpage><pub-id pub-id-type="pmid">22091824</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krigolson</surname><given-names>OE</given-names></name></person-group><article-title>Event-related brain potentials and the study of reward processing: Methodological considerations</article-title><source>International Journal of Psychophysiology</source><year>2017</year><pub-id pub-id-type="pmid">29154804</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krigolson</surname><given-names>OE</given-names></name><name><surname>Hassall</surname><given-names>CD</given-names></name><name><surname>Balcom</surname><given-names>L</given-names></name><name><surname>Turk</surname><given-names>D</given-names></name></person-group><article-title>Perceived ownership impacts reward evaluation within medial-frontal cortex</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><year>2013</year><volume>13</volume><issue>2</issue><fpage>262</fpage><lpage>269</lpage><pub-id pub-id-type="pmid">23283801</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krigolson</surname><given-names>OE</given-names></name><name><surname>Hassall</surname><given-names>CD</given-names></name><name><surname>Handy</surname><given-names>TC</given-names></name></person-group><article-title>How We Learn to Make Decisions: Rapid Propagation of Reinforcement Learning Prediction Errors in Humans</article-title><source>Journal of Cognitive Neuroscience</source><year>2014</year><volume>26</volume><issue>3</issue><fpage>635</fpage><lpage>644</lpage><ext-link ext-link-type="uri" xlink:href="https://doi.org/10/gd32z4">https://doi.org/10/gd32z4</ext-link><pub-id pub-id-type="pmid">24168216</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><source>An introduction to the event-related potential technique</source><edition>Second edition</edition><publisher-name>The MIT Press</publisher-name><year>2014</year></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marco-Pallarés</surname><given-names>J</given-names></name><name><surname>Cucurell</surname><given-names>D</given-names></name><name><surname>Cunillera</surname><given-names>T</given-names></name><name><surname>Krämer</surname><given-names>UM</given-names></name><name><surname>Càmara</surname><given-names>E</given-names></name><name><surname>Nager</surname><given-names>W</given-names></name><name><surname>Bauer</surname><given-names>P</given-names></name><name><surname>Schüle</surname><given-names>R</given-names></name><name><surname>Schöls</surname><given-names>L</given-names></name><name><surname>Münte</surname><given-names>TF</given-names></name><name><surname>Rodriguez-Fornells</surname><given-names>A</given-names></name></person-group><article-title>Genetic Variability in the Dopamine System (Dopamine Receptor D4, Catechol-O-Methyltransferase) Modulates Neurophysiological Responses to Gains and Losses</article-title><source>Biological Psychiatry</source><year>2009</year><volume>66</volume><issue>2</issue><fpage>154</fpage><lpage>161</lpage><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0006322309000316">https://doi.org/10/cgdhfq</ext-link><pub-id pub-id-type="pmid">19251248</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>LE</given-names></name><name><surname>Potts</surname><given-names>GF</given-names></name></person-group><article-title>Medial Frontal Event Related Potentials and Reward Prediction: Do Responses Matter?</article-title><source>Brain and Cognition</source><year>2011</year><volume>77</volume><issue>1</issue><fpage>128</fpage><lpage>134</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0278262611000686?via%3Dihub">https://doi.org/10/fcgftx</ext-link></comment><pub-id pub-id-type="pmcid">PMC3159831</pub-id><pub-id pub-id-type="pmid">21621891</pub-id><pub-id pub-id-type="doi">10.1016/j.bandc.2011.04.001</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname><given-names>EM</given-names></name><name><surname>Burgdorf</surname><given-names>C</given-names></name><name><surname>Chavanon</surname><given-names>M-L</given-names></name><name><surname>Schweiger</surname><given-names>D</given-names></name><name><surname>Hennig</surname><given-names>J</given-names></name><name><surname>Wacker</surname><given-names>J</given-names></name><name><surname>Stemmler</surname><given-names>G</given-names></name></person-group><article-title>The COMT Val158Met polymorphism regulates the effect of a dopamine antagonist on the feedback-related negativity</article-title><source>Psychophysiology</source><year>2014</year><volume>51</volume><issue>8</issue><fpage>805</fpage><lpage>809</lpage><pub-id pub-id-type="pmid">24773408</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mühlberger</surname><given-names>C</given-names></name><name><surname>Angus</surname><given-names>DJ</given-names></name><name><surname>Jonas</surname><given-names>E</given-names></name><name><surname>Harmon-Jones</surname><given-names>C</given-names></name><name><surname>Harmon-Jones</surname><given-names>E</given-names></name></person-group><article-title>Perceived control increases the reward positivity and stimulus preceding negativity</article-title><source>Psychophysiology</source><year>2017</year><volume>54</volume><issue>2</issue><fpage>310</fpage><lpage>322</lpage><pub-id pub-id-type="pmid">28118688</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakahara</surname><given-names>H</given-names></name><name><surname>Itoh</surname><given-names>H</given-names></name><name><surname>Kawagoe</surname><given-names>R</given-names></name><name><surname>Takikawa</surname><given-names>Y</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><article-title>Dopamine Neurons Can Represent Context-Dependent Prediction Error</article-title><source>Neuron</source><year>2004</year><volume>41</volume><issue>2</issue><fpage>269</fpage><lpage>280</lpage><pub-id pub-id-type="pmid">14741107</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Joel</surname><given-names>D</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Tonic dopamine: Opportunity costs and the control of response vigor</article-title><source>Psychopharmacology</source><year>2007</year><volume>191</volume><issue>3</issue><fpage>507</fpage><lpage>520</lpage><ext-link ext-link-type="uri" xlink:href="https://link.springer.com/article/10.1007/s00213-006-0502-4">https://doi.org/10/b6fhmh</ext-link><pub-id pub-id-type="pmid">17031711</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osinsky</surname><given-names>R</given-names></name><name><surname>Ulrich</surname><given-names>N</given-names></name><name><surname>Mussel</surname><given-names>P</given-names></name><name><surname>Feser</surname><given-names>L</given-names></name><name><surname>Gunawardena</surname><given-names>A</given-names></name><name><surname>Hewig</surname><given-names>J</given-names></name></person-group><article-title>The Feedback-related Negativity Reflects the Combination of Instantaneous and Long-term Values of Decision Outcomes</article-title><source>Journal of Cognitive Neuroscience</source><year>2017</year><volume>29</volume><issue>3</issue><fpage>424</fpage><lpage>434</lpage><ext-link ext-link-type="uri" xlink:href="https://doi.org/10/gdk3pj">https://doi.org/10/gdk3pj</ext-link><pub-id pub-id-type="pmid">28129052</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><issue>4</issue><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pessiglione</surname><given-names>M</given-names></name><name><surname>Seymour</surname><given-names>B</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><article-title>Dopamine-dependent prediction errors underpin reward-seeking behaviour in humans</article-title><source>Nature</source><year>2006</year><volume>442</volume><issue>7106</issue><fpage>1042</fpage><lpage>1045</lpage><pub-id pub-id-type="pmcid">PMC2636869</pub-id><pub-id pub-id-type="pmid">16929307</pub-id><pub-id pub-id-type="doi">10.1038/nature05051</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigoli</surname><given-names>F</given-names></name><name><surname>Chew</surname><given-names>B</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>The Dopaminergic Midbrain Mediates an Effect of Average Reward on Pavlovian Vigor</article-title><source>Journal of Cognitive Neuroscience</source><year>2016</year><volume>28</volume><issue>9</issue><fpage>1303</fpage><lpage>1317</lpage><pub-id pub-id-type="pmid">27082045</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Skandali</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>A computational and neural model of momentary subjective well-being</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>33</issue><fpage>12252</fpage><lpage>12257</lpage><pub-id pub-id-type="pmcid">PMC4143018</pub-id><pub-id pub-id-type="pmid">25092308</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1407535111</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname><given-names>RB</given-names></name><name><surname>Skandali</surname><given-names>N</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name></person-group><article-title>Dopaminergic Modulation of Decision Making and Subjective Well-Being</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><issue>27</issue><fpage>9811</fpage><lpage>9822</lpage><pub-id pub-id-type="pmcid">PMC4495239</pub-id><pub-id pub-id-type="pmid">26156984</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0702-15.2015</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sambrook</surname><given-names>TD</given-names></name><name><surname>Goslin</surname><given-names>J</given-names></name></person-group><article-title>A neural reward prediction error revealed by a meta-analysis of ERPs using great grand averages</article-title><source>Psychological Bulletin</source><year>2015</year><volume>141</volume><issue>1</issue><fpage>213</fpage><lpage>235</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://doi.org/10/f6whtb">https://doi.org/10/f6whtb</ext-link></comment><pub-id pub-id-type="pmid">25495239</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><article-title>A Neural Substrate of Prediction and Reward</article-title><source>Science</source><year>1997</year><volume>275</volume><issue>5306</issue><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahnazian</surname><given-names>D</given-names></name><name><surname>Shulver</surname><given-names>K</given-names></name><name><surname>Holroyd</surname><given-names>CB</given-names></name></person-group><article-title>Electrophysiological responses of medial prefrontal cortex to feedback at different levels of hierarchy</article-title><source>NeuroImage</source><year>2018</year><volume>183</volume><fpage>121</fpage><lpage>131</lpage><pub-id pub-id-type="pmid">30081194</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>The Expected Value of Control: An Integrative Theory of Anterior Cingulate Cortex Function</article-title><source>Neuron</source><year>2013</year><volume>79</volume><issue>2</issue><fpage>217</fpage><lpage>240</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627313006077">https://doi.org/10/f45wsh</ext-link></comment><pub-id pub-id-type="pmcid">PMC3767969</pub-id><pub-id pub-id-type="pmid">23889930</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement learning: An introduction</source><publisher-name>The MIT Press</publisher-name><year>2018</year><edition>Second edition</edition></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umemoto</surname><given-names>A</given-names></name><name><surname>HajiHosseini</surname><given-names>A</given-names></name><name><surname>Yates</surname><given-names>ME</given-names></name><name><surname>Holroyd</surname><given-names>CB</given-names></name></person-group><article-title>Reward-based contextual learning supported by anterior cingulate cortex</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><year>2017</year><volume>17</volume><issue>3</issue><fpage>642</fpage><lpage>651</lpage><pub-id pub-id-type="pmid">28236171</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>MM</given-names></name><name><surname>Anderson</surname><given-names>JR</given-names></name></person-group><article-title>Learning from experience: Event-related potential correlates of reward processing, neural adaptation, and behavioral choice</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2012</year><volume>36</volume><issue>8</issue><fpage>1870</fpage><lpage>1884</lpage><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/abs/pii/S0149763412000875?via%3Dihub">https://doi.org/10/f36rqc</ext-link><pub-id pub-id-type="pmcid">PMC3432149</pub-id><pub-id pub-id-type="pmid">22683741</pub-id><pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.05.008</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>N</given-names></name><name><surname>Holroyd</surname><given-names>CB</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>ERP Correlates of Feedback and Reward Processing in the Presence and Absence of Response Choice</article-title><source>Cerebral Cortex</source><year>2005</year><volume>15</volume><issue>5</issue><fpage>535</fpage><lpage>544</lpage><pub-id pub-id-type="pmid">15319308</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>R</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name></person-group><article-title>Brain responses to outcomes of one’s own and other’s performance in a gambling task</article-title><source>Neuroreport</source><year>2006</year><volume>17</volume><issue>16</issue><fpage>1747</fpage><lpage>1751</lpage><pub-id pub-id-type="pmid">17047466</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Task overview.</title><p>After making a left or right button press, participants were shown fruit indicating the outcome (win or loss).</p></caption><graphic xlink:href="EMS145917-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Behavioural results.</title><p>(a) Mean proportion of wins in each task. (b) Performance curves in the tasks where correct responses were possible (mid-value and high-value tasks), indicating improvement over time. (c) Mean proportion of correct responses within the mid-value and high-value tasks. The shaded regions in each plot indicate 95% confidence intervals. Dots correspond to individual participants.</p></caption><graphic xlink:href="EMS145917-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The reward positivity is reduced when task value is high.</title><p>(a) Win/loss waveforms by task value (low, mid, high) and cue value (low, high). (b) Scalp distribution of the win-loss difference collapsed across conditions. (c) Difference waveforms by task value (low, mid, high) and cue value (low, high). The shaded area shows the region of analysis. (d) RewP scores by task value (low, mid, high) and cue value (low, high). Waveforms are time-locked to feedback onset. Error bars indicate 95% confidence intervals. Dots correspond to individual participants.</p></caption><graphic xlink:href="EMS145917-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>The reward positivity effect depends on performance.</title><p>The difference in RewP between the mid-value and high-value tasks correlated across participants positively with task performance (greater difference associated with more accurate performance).</p></caption><graphic xlink:href="EMS145917-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Computational modelling suggests participants used a different strategy for low-value cues.</title><p>(a) The learning rate parameter, <italic>α</italic>, fit to each participant, was greater for mid-low cues compared to mid-high cues. (b) The temperature parameter, <italic>τ</italic>, fit to each participant, was greater for low-value cues compared to high-value cues. Dots correspond to individual participants.</p></caption><graphic xlink:href="EMS145917-f005"/></fig></floats-group></article>