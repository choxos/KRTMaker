<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" article-type="preprint">
<?all-math-mml yes?>
<?use-mml?>
<?origin ukpmcpa?>
<front>
<journal-meta>
<journal-id journal-id-type="nlm-ta">bioRxiv</journal-id>
<journal-title-group>
<journal-title>bioRxiv : the preprint server for biology</journal-title>
</journal-title-group>
<issn pub-type="ppub"/>
</journal-meta>
<article-meta>
<article-id pub-id-type="manuscript">EMS136061</article-id>
<article-id pub-id-type="doi">10.1101/2021.09.28.461909</article-id>
<article-id pub-id-type="archive">PPR402778</article-id>
<article-version article-version-type="publisher-id">1</article-version>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Article</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Face masks impair reconstruction of acoustic speech features and higher-level segmentational features in the presence of a distractor speaker</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Haider</surname>
<given-names>Chandra Leon</given-names>
</name>
<xref ref-type="corresp" rid="CR1">*</xref>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Suess</surname>
<given-names>Nina</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hauswald</surname>
<given-names>Anne</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Park</surname>
<given-names>Hyojin</given-names>
</name>
<xref ref-type="aff" rid="A2">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Weisz</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="A1">1</xref>
<xref ref-type="aff" rid="A3">3</xref>
</contrib>
</contrib-group>
<aff id="A1">
<label>1</label>Centre for Cognitive Neuroscience and Department of Psychology, University of Salzburg, Austria</aff>
<aff id="A2">
<label>2</label>School of Psychology &amp; Centre for Human Brain Health (CHBH), University of Birmingham, Birmingham, UK</aff>
<aff id="A3">
<label>3</label>Neuroscience Institute, Christian Doppler University Hospital, Paracelsus Medical University, Salzburg, Austria</aff>
<author-notes>
<corresp id="CR1">
<label>*</label>Corresponding author: <email>chandraleon.haider@plus.ac.at</email>
</corresp>
</author-notes>
<pub-date pub-type="nihms-submitted">
<day>06</day>
<month>10</month>
<year>2021</year>
</pub-date>
<pub-date pub-type="preprint">
<day>30</day>
<month>09</month>
<year>2021</year>
</pub-date>
<permissions>
<ali:free_to_read/>
<license>
<ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref>
<license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p>
</license>
</permissions>
<abstract>
<p id="P1">Face masks have become a prevalent measure during the Covid-19 pandemic to counteract the transmission of SARS-CoV 2. An unintended “side-effect“ of face masks is their adverse influence on speech perception especially in challenging listening situations. So far, behavioural studies have not pinpointed exactly which feature(s) of speech processing face masks affect in such listening situations. We conducted an audiovisual (AV) multi-speaker experiment using naturalistic speech (i.e. an audiobook). In half of the trials, the target speaker wore a (surgical) face mask, while we measured the brain activity of normal hearing participants via magnetoencephalography (MEG). A decoding model on the clear AV speech (i.e. no additional speaker and target speaker not wearing a face mask) was trained and used to reconstruct crucial speech features in each condition. We found significant main effects of face masks on the reconstruction of acoustic features, such as the speech envelope and spectral speech features (i.e. pitch and formant frequencies), while reconstruction of higher level features of speech segmentation (phoneme and word onsets) were especially impaired through masks in difficult listening situations, i.e. when a distracting speaker was also presented. Our findings demonstrate the detrimental impact face masks have on listening and speech perception, thus extending previous behavioural results. Supporting the idea of visual facilitation of speech is the fact that we used surgical face masks in our study, which only show mild effects on speech acoustics. This idea is in line with recent research, also by our group, showing that visual cortical regions track spectral modulations. Since hearing impairment usually affects higher frequencies, the detrimental effect of face masks might pose a particular challenge for individuals who likely need the visual information about higher frequencies (e.g. formants) to compensate.</p>
</abstract>
<kwd-group>
<kwd>stimulus reconstruction</kwd>
<kwd>face masks</kwd>
<kwd>audio-visual speech</kwd>
<kwd>formants</kwd>
<kwd>speech envelope</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="S1" sec-type="intro">
<title>Introduction</title>
<p id="P2">In the ongoing Covid-19 pandemic, face masks have become a prevalent and effective measure around the globe to counteract the transmission of SARS-CoV 2 (<xref ref-type="bibr" rid="R45">Rahne et al., 2021</xref>). However, face masks pose issues in everyday listening situations. Face masks have been shown to impair acoustic details of the speech signal in several studies, with N95/FFP2 masks showing quite a strong attenuation in mid to high frequencies, while surgical face masks show only moderate influence on the acoustic speech signal (<xref ref-type="bibr" rid="R11">Caniato et al., 2021</xref>; <xref ref-type="bibr" rid="R14">Corey et al., 2020</xref>). Confirming this impact behaviourally, <xref ref-type="bibr" rid="R45">Rahne et al. (2021)</xref> found impaired speech perception in noise when wearing a surgical face mask compared to no mask, while participants performed even worse when using an N95 mask. As well as reducing acoustic details, information transmitted via visual input is completely missing or severely reduced. It has already been established very early that visual cues originating from the mouth and lips facilitate speech intelligibility, especially in situations with a low speech-to-noise ratio (<xref ref-type="bibr" rid="R52">Sumby &amp; Pollack, 1954</xref>).</p>
<p id="P3">Through analysing continuous speech using decoding models to reconstruct acoustic speech features from the neural signal obtained by electroencephalography (EEG) or magnetoencephalography (MEG), evidence has accumulated that the brain directly tracks crucial speech specific components, like the speech envelope (<xref ref-type="bibr" rid="R9">Brodbeck &amp; Simon, 2020</xref>; <xref ref-type="bibr" rid="R22">Ding &amp; Simon, 2014</xref>) and speech-critical resonant frequencies called formants (<xref ref-type="bibr" rid="R39">Peelle &amp; Sommers, 2015</xref>). Not only does the brain engage in direct tracking of the acoustic signal, but also seeing the talker's face substantially facilitates acoustic signal tracking (<xref ref-type="bibr" rid="R15">Crosse et al., 2015</xref>; <xref ref-type="bibr" rid="R17">Crosse, Di Liberto, &amp; Lalor, 2016</xref>; <xref ref-type="bibr" rid="R26">Golumbic et al., 2013</xref>; <xref ref-type="bibr" rid="R38">Park et al., 2016</xref>).</p>
<p id="P4">On the one hand, this mentioned audiovisual facilitation might be explained by providing simple temporal cues (i.e. opening and closing of mouth) when having to attend to auditory stimuli. On the other hand, visual information might be preselecting certain possible stimuli (e.g. phonemes) and therefore enhancing subsequent auditory processing as a form of crossmodal integration. By using the additive model (i.e. comparing event-related potentials (ERP) to audio stimuli + ERPs visual stimuli (A+V) to ERPs of audiovisual stimuli (AV)), past studies indeed suggested that the brain integrates early (~ 120 ms after stimulus onset) information from the visible lip movements and the auditory input for efficient speech processing (<xref ref-type="bibr" rid="R4">Besle et al., 2004</xref>, <xref ref-type="bibr" rid="R3">2009</xref>).</p>
<p id="P5">In addition to these effects in auditory processing regions, we have provided evidence for a direct visuo-phonological transformation when individuals only process visual information (i.e. silent video recordings of speakers), by showing that the acoustic speech envelope is tracked in visual cortical regions when individuals observe lip movements (<xref ref-type="bibr" rid="R28">Hauswald et al., 2018</xref>; <xref ref-type="bibr" rid="R51">Suess et al., 2021</xref>). Furthermore, the visual cortex also tracks spectral modulations in the range of the pitch, as well as in the second (F2) and third formant (F3) which reflect mainly sounds produced with the visible part of the mouth (<xref ref-type="bibr" rid="R51">Suess et al., 2021</xref>), which aligns well with previous findings by <xref ref-type="bibr" rid="R12">Chandrasekaran et al. (2009)</xref>. Their findings indicate that the area of mouth opening correlates strongest with spectral components of speech in the range of 1kHz - 3kHz, corresponding to the frequency range of F2 and F3. Together, these results reveal that visual lip movements are transformed in order to track acoustic speech features such as the speech envelope and formant frequencies.</p>
<p id="P6">In the context of face masks, a large online study investigated their effects on audiovisual (AV) speech behaviourally (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>). They found no differences in sentence intelligibility between clear AV speech (i.e. no face mask) and face masks of several types (e.g. surgical face mask and N95 mask) in conditions with a quiet background, but differences became apparent in conditions with moderate and high background noise. These drawbacks of face masks might be even more pronounced in individuals with hearing impairments, as they profit more by observing a talking face congruent with the auditory input (<xref ref-type="bibr" rid="R44">Puschmann et al., 2019</xref>). Despite these well-established effects, the behavioural studies have left open which (degraded) speech features are driving these findings. Decoding distinct speech features from the neural signal could be used for addressing this issue. Putting the aforementioned findings together, face masks might adversely impact the processing of diverse speech characteristics at different hierarchical levels, resulting in poor behavioural performance. With face masks still common in everyday life as a measure against Covid-19 and continuing to remain important in medical settings, understanding precisely which features of speech are less well tracked by the brain can help guide decisions on which face mask to use. These considerations are especially important when dealing with hearing-impaired individuals.</p>
<p id="P7">In the current MEG study, we investigated how neural tracking of a variety of speech features (purely acoustic and lexical/phonetic boundaries) in an audio-visual naturalistic speech paradigm is impaired through (surgical) face masks. Special emphasis is placed on an interaction between face masks and difficult listening situations induced via an audio-only distractor speaker, as recent studies emphasised the visual benefit when acoustics are unclear (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>; <xref ref-type="bibr" rid="R35">Mitchel &amp; Weiss, 2014</xref>; <xref ref-type="bibr" rid="R38">Park et al., 2016</xref>). We then trained a backward model on clear speech in order to reconstruct the speech characteristics from the participants' brain data for each condition. Additionally, we measured participants' comprehension performance and subjective difficulty ratings. We hypothesised particularly strong effects on speech features that have been shown to be detectable by visual input (i.e. lip movements), such as the speech envelope, pitch, the averaged F2 and F3 (F2/3) as well as segmentation (i.e. phoneme and word onsets). We found strong adverse general effects of the face mask on acoustic features (i.e. speech envelope and spectral features) reconstruction and difficulty ratings irrespective of a distractor speaker. Importantly, for features of segmentation (word and phoneme onsets) face masks revealed their adverse impact especially in difficult listening situations.</p>
</sec>
<sec id="S2" sec-type="methods">
<title>Methods</title>
<sec sec-type="subjects" id="S3">
<title>Participants</title>
<p id="P8">29 German native speakers (12 female) aged between 22 and 41 years (<italic>M = 26.79, SD = 4.86</italic>) took part in our study. All participants had self-reported normal hearing, verified by a standard clinical audiometry. Further exclusion criteria were non-removable magnetic objects, as well as a history of psychiatric or neurological conditions. Recruitment was done via social media and university lectures. One participant was excluded because signal source separation could not be applied to the MEG dataset. All participants signed an informed consent form and were compensated with €10 per hour or course credit. The experimental protocol was approved by the ethics committee of the University of Salzburg and was carried out in accordance with the Declaration of Helsinki.</p>
</sec>
<sec id="S4">
<title>Stimuli</title>
<p id="P9">We used excerpts of four different stories for our recording read out in German. ‘Die Schokoladenvilla - Zeit des Schicksals. Die Vorgeschichte zu Band 3’ (“The Chocolate Mansion, The Legacy” – prequel of Volume 3”) by Maria Nikolai and ‘Die Federn des Windes’ (“The feathers of the wind”) by Manuel Timm were read out by a female speaker. ‘Das Gestüt am See. Charlottes großer Traum’ (“The stud farm by the lake. Charlotte’s great dream”) by Paula Mattis and ‘Gegen den Willen der Väter’ (“Against the will of their fathers”) by Klaus Tiberius Schmidt were read out by a male speaker.</p>
<p id="P10">Stimuli were recorded using a Sony FS100 camera with a sampling rate of 25 Hz and a Rode NTG 2 microphone with a sampling rate of 48 kHz. We aimed at a duration for each story of approximately ten minutes, which were cut into ten videos of around one minute each (<italic>range: 56 s - 76 s, M = 64 s, SD = 4.8 s)</italic>. All stories were recorded twice, once without the speaker wearing a surgical face mask and once with the speaker wearing a surgical face mask (Type IIR, three-layer single-use medical face mask, see <xref ref-type="fig" rid="F1">Figure 1A</xref>). After cutting the videos, we ended up with 80 videos of approximately one minute each. Forty of those were presented to each participant (20 with a female speaker, 20 with a male speaker) in order to rule out sex-specific effects. The audio track was extracted and stored separately. The audio files were then normalised using the Python function ‘ffmpeg-normalise’ with default options. Pre-recorded audiobooks read out by different speakers (one female, one male) were used for the distractor speaker and normalised using the same method. These audio files contained either a (different) single male or female speaker. The syllable rate was analysed using a Praat script (<xref ref-type="bibr" rid="R5">Boersma &amp; Weenink, 2001</xref>; <xref ref-type="bibr" rid="R20">de Jong &amp; Wempe, 2009</xref>). The target speakers’ syllable rates varied between 3.7 Hz and 4.6 Hz (<italic>M = 4.1 Hz</italic>). Target and distractor stimuli were all played to the participant at the same volume, which was individually set to a comfortable level at the start of the experiment.</p>
</sec>
<sec sec-type="methods" id="S5">
<title>Experimental procedure</title>
<p id="P11">Before the start of the experiment, we performed a standard clinical audiometry using a AS608 Basic (Interacoustics, Middelfart, Denmark) in order to assess participants’ individual hearing ability. Afterwards, participants were prepared for MEG (see <xref ref-type="sec" rid="S6">Data acquisition</xref>).</p>
<p id="P12">We started the MEG measurement with five minutes of resting-state activity (not included in this manuscript). We then assessed the participants' individual hearing threshold in order to adjust our stimulation volume. If the participant stated afterwards that stimulation was not comfortable or not loud enough, we adjusted the volume again manually to the participant’s requirement. Of the four stories, half were randomly chosen with the target speakers wearing face masks in the recording. In the remaining half, speakers did not wear a face mask. Each story presentation functioned as one stimulation block, resulting in four blocks overall. One block consisted of ten ~ 1 minute long trials. In three randomly selected trials per block- (i.e. 30% of trials), a same-sex audio-only distractor speaker was added at equal volume as the target speaker. We only added a distractor speaker in 30% of trials in order to retain enough data to train our backward model on clear speech (see <xref ref-type="sec" rid="S9">stimulus reconstruction</xref> section). Distractor speaker presentation started five seconds after target speaker video and audio onset in order to give the participants time to pay attention to the target speaker. Within the blocks, the story presentation followed a consistent storyline across trials. After each trial, two unstandardised ‘true or false’ statements regarding semantic content were asked to assess comprehension performance and keep participants focused (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Additionally, participants rated subjective difficulty and motivation at four times per block on a five-point likert scale (not depicted in <xref ref-type="fig" rid="F1">Figure 1A</xref>). The participants’ answers were given via button presses. In one half of the four blocks a female target speaker was presented, in the other half a male target speaker. Videos were back-projected on a translucent screen with a screen diagonal of 74 cm via a Propixx DLP projector (Vpixx technologies, Canada) ~ 110 cm in front of the participants. It was projected with a refresh rate of 120 Hz and a resolution of 1920 × 1080 pixels. Including preparation, the experiment took about 2 hours per participant. The experiment was coded and conducted with the Psychtoolbox-3 (<xref ref-type="bibr" rid="R6">Brainard, 1997</xref>; <xref ref-type="bibr" rid="R33">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="R40">Pelli, 1997</xref>) with an additional class-based library (‘Objective Psychophysics Toolbox’, o_ptb) on top of it (<xref ref-type="bibr" rid="R27">Hartmann &amp; Weisz, 2020</xref>).</p>
</sec>
<sec id="S6">
<title>Data acquisition</title>
<p id="P13">We recorded brain data with a sampling rate of 1 kHz at 306-channels (204 first-order planar gradiometers and 102 magnetometers) with a Triux MEG system (MEGIN, Helsinki, Finnland). The acquisition was performed in a magnetically shielded room (AK3B, Vacuumschmelze). Online bandpass filtering was performed from 0.1 Hz to 330 Hz. Prior to the acquisition, cardinal head points (nasion and pre-auricular points) were digitised with a Polhemus Fastrak Digitizer (Polhemus) along with around 300 points on the scalp in order to assess individual head shapes. Using a signal space separation algorithm provided by the MEG manufacturer (Maxfilter, version 2.2.15), we filtered noise resulting from sources outside the head and realigned the data to a standard head position, which was measured at the beginning of each block.</p>
</sec>
<sec id="S7">
<title>Speech feature extraction</title>
<p id="P14">All the speech features investigated are depicted in <xref ref-type="fig" rid="F1">Figure 1B</xref>.</p>
<p id="P15">The speech envelope was extracted using the Chimera toolbox. For this purpose, the speech signal was filtered forward and in reverse with a 4th order Butterworth bandpass filter at nine different frequency bands equidistantly spaced between 100 and 10000 Hz corresponding to the cochlear map (<xref ref-type="bibr" rid="R49">Smith et al., 2002</xref>). Then, a Hilbert transformation was performed to extract the envelopes from the resulting signals. These nine envelopes were then summed up to one general speech envelope and normalised.</p>
<p id="P16">The pitch (fundamental frequency, F0) was extracted using the built-in Matlab Audio Toolbox function <italic>pitch.m</italic> and downsampled to 50 Hz. The speech formants (first, second, third and the averaged second and third formant) were extracted using FormantPro (<xref ref-type="bibr" rid="R57">Xu &amp; Gao, 2018</xref>), a tool for automatic formant detection via Praat (<xref ref-type="bibr" rid="R5">Boersma &amp; Weenink, 2001</xref>) at 50 Hz with an integration window length of 20 ms, and a smoothing window of 10 ms length.</p>
<p id="P17">Phoneme and word onset values were generated using forced alignment with MAUS web services (<xref ref-type="bibr" rid="R32">Kisler et al., 2017</xref>; <xref ref-type="bibr" rid="R48">Schiel, 1999</xref>) in order to obtain a measure for speech segmentation. We generated two time-series with binary values indicating an onset of phoneme or word, respectively. Then, we smoothed the time-series of binary values using a gaussian window with a width of 10 ms. In the end, all features were downsampled to 50 Hz to match the sampling rate of the corresponding brain signal, as most speech relevant signals present themselves below 25 Hz (<xref ref-type="bibr" rid="R18">Crosse et al., 2021</xref>).</p>
</sec>
<sec id="S8">
<title>MEG preprocessing</title>
<p id="P18">The raw data was analysed using Matlab R2020b (The MathWorks, Natick, Massachusetts, USA) and the FieldTrip toolbox (<xref ref-type="bibr" rid="R36">Oostenveld et al., 2011</xref>). First, we computed 50 independent components to remove eye and heart artifacts. We removed on average 2.38 components per participant (<italic>SD = .68</italic>). We further filtered the data using a sixth-order zero-phase Butterworth bandpass filter between 0.1 and 25 Hz. Afterwards, we epoched the data into 2.5 s segments. Finally, we downsampled our data to 50 Hz.</p>
</sec>
<sec id="S9">
<title>Stimulus reconstruction</title>
<p id="P19">To reconstruct the different speech characteristics (speech envelope, pitch, resonant frequencies as well as word and phoneme onsets) from the brain data, we utilised the mTRF Toolbox (<xref ref-type="bibr" rid="R16">Crosse, Di Liberto, Bednar, et al., 2016</xref>). The goal of this approach is to map brain responses (i.e. all 306 MEG channels) back to the stimulus(-feature) (e.g. speech envelope) using linear models in order to obtain a measure of how well a certain characteristic is encoded in the brain. According to our 2x2 experimental design, the stimulus features were reconstructed for each condition. As the distractor speaker starts after five seconds of the trial start, these five seconds were not assigned to the <italic>Distractor</italic> condition, but rather reassigned to their respective condition with only a single speaker.</p>
<p id="P20">The stimulus features and the brain data at all 306 MEG channels were z-scored and the epochs were shuffled. We then used the clear speech condition (with no masks and no distractor speaker presented) to train the backward model with ridge regression. In order to test the model on a clear audio data set as well, we split it into seven parts and trained our model on six parts, while using the remaining part to test it. This results in approximately twelve minutes of data for training the model. We defined our time lags to train our model from -150 ms to 450 ms. Then, we performed seven-fold leave-one-out cross-validation on our training dataset to find the optimal regularisation parameter (<xref ref-type="bibr" rid="R55">Willmore &amp; Smyth, 2003</xref>). We used the same data with the obtained regularisation parameter to train our backward model. For each condition, we used the same backward model trained on clear speech to reconstruct the speech characteristics of interest, namely the speech envelope, pitch, resonant frequencies (F1-3 and F2/3) and segmentational features (phoneme and word onsets). As we used clear audio trials for training the decoding model and added a distractor speaker only in 30 % percent of trials (see <xref ref-type="sec" rid="S5">experimental procedure</xref>, <xref ref-type="fig" rid="F1">Figure 1A</xref>), this resulted in a variable length of test data sets. In the ‘no mask/no distractor’ condition it was ~ 2 minutes, in the ‘mask/no distractor’ condition it was ~ 14 minutes and for ‘no mask/distractor’ as well as ‘mask/distractor’ condition it was ~ 6 minutes each. The process was repeated six times, so that each subset of the clear speech condition had the opportunity to be used as a test set while all other subsets were used for training. For each participant, each speech feature and each of the four conditions we computed the correlation coefficient (Pearson’s <italic>r</italic>) of the reconstructed feature and the original feature as a measure of reconstruction accuracy. This was done by Fisher-Z transformation and averaging all respective correlation coefficients for each test set and each of the seven repetitions obtained through the aforementioned procedure.</p>
</sec>
<sec id="S10">
<title>Statistical analysis</title>
<p id="P21">We performed a repeated measures ANOVA with the within-factors <italic>Mask</italic> (no face mask vs. face mask) and <italic>Distractor</italic> (no distractor speaker vs. distractor speaker) and the obtained Fisher z-transformed correlation coefficients (i.e. reconstruction accuracy) as dependent variables.</p>
<p id="P22">For the behavioural results (comprehension performance and subjective difficulty), we also used a repeated measures ANOVA with the same factors <italic>Mask</italic> and <italic>Distractor</italic>. We used comprehension performance scores (i.e. the percentage of correct answers) and averaged subjective difficulty ratings respectively as dependent variables.</p>
<p id="P23">The statistical analyses for reconstruction accuracies and behavioural data were performed using <italic>pingouin</italic>, a statistics package for Python 3 (<xref ref-type="bibr" rid="R54">Vallat, 2018</xref>). In case of a significant interaction or a trend, a simple effect test was performed via the Matlab's Statistics and Machine Learning Toolbox in order to pinpoint the nature of the interaction. Furthermore, comparisons of spectral fine details between face masks and no masks, were computed in Matlab with the <italic>Measures of Effect Size</italic> toolbox (<xref ref-type="bibr" rid="R29">Hentschke &amp; Stüttgen, 2011</xref>).</p>
</sec>
</sec>
<sec id="S11" sec-type="results">
<title>Results</title>
<sec id="S12">
<title>Behavioural results</title>
<p id="P24">Comprehension performance scores were generated using two ‘true or false’ comprehension questions at the end of each of the 40 trials. We used a two-way repeated measures ANOVA to investigate the influence of the factors <italic>Mask</italic> and <italic>Distractor</italic> on the comprehension performance. Apart from the effect for the distractor speaker (<italic>F(1,28) = 26.15, p &lt;.001, η<sub>p</sub>
<sup>2</sup> = .48)</italic> the results showed no significant influence of face masks (<italic>F(1,28) = 1.03, p = .32, η<sub>p</sub>
<sup>2</sup> = .04</italic>) and no significant interaction (<italic>F(1,28) = .02, p = .88, η<sub>p</sub>
<sup>2</sup> = .001</italic>) between the two factors.</p>
<p id="P25">Furthermore, we analysed the subjectively reported difficulty for each condition. We again used two-way repeated measures ANOVA, which showed a significant effect for the distractor speaker (<italic>F(1,28) = 101.83, p &lt;.001, η<sub>p</sub>
<sup>2</sup> = .78</italic>) as well as the face mask (<italic>F(1,28) = 13.78, p = .001, η<sub>p</sub>
<sup>2</sup> = .33</italic>), while not showing a significant effect for the interaction (<italic>F(1,28) = 1.33, p = .26, η<sub>p</sub>
<sup>2</sup> = .06</italic>). These results suggest that, while face masks do not reduce comprehension performance in our setting, they nonetheless lead to a significant increase in perceived listening difficulty.</p>
</sec>
<sec id="S13">
<title>Analysis of stimulus reconstruction</title>
<p id="P26">Using a stimulus reconstruction approach based on the recorded MEG data, we studied which speech-related features are impaired through face masks, with a special focus on difficult listening situations. We therefore analysed the correlation coefficients (Pearson’s r) obtained using a backward model (<xref ref-type="bibr" rid="R16">Crosse, Di Liberto, Bednar, et al., 2016</xref>). The correlation coefficient represents how well the specific stimulus characteristic was reconstructed from brain data and serves as a proxy for how well these features are represented in the neural signal.</p>
<p id="P27">With this approach, we generated one correlation coefficient for each condition per participant. This process was repeated for each speech feature of interest. To analyse the effect of the face mask and the distractor speaker, we performed a two-way repeated measures ANOVA, with the Fisher z-transformed correlation coefficients as dependent variables. Detailed results and statistical values are found in the supplementary material (see <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>). As expected, results show a strong effect (all <italic>p &lt; .001</italic>) of the distractor speaker on the stimulus reconstruction across all stimulus characteristics of interest. <xref ref-type="fig" rid="F2">Figure 2A</xref> shows example reconstructions for the speech envelope and the averaged second and third formant (Formant 2/3 or F2/3) as well as mean reconstruction accuracies for clear audiovisual speech (i.e. stimulation material with no mask and no distractor) in <xref ref-type="fig" rid="F2">Figure 2B</xref>.</p>
</sec>
<sec id="S14">
<title>Reconstruction of the speech envelope is generally affected by face masks</title>
<p id="P28">We investigated how the stimulus reconstruction of the speech envelope is impaired through face masks, with a particular focus on difficult listening situations induced by a distractor speaker. Apart from the negative impact of the distractor speaker (<italic>F(1,28) = 161.09, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .85</italic>), we observed a strong negative effect of face masks on reconstruction accuracies of the speech envelope (F(1,28) = 24.42, <italic>p</italic> &lt; .001, <italic>η<sub>p</sub>
<sup>2</sup>
</italic> = .47, <xref ref-type="fig" rid="F3">Figure 3A</xref>). We found no significant interaction between the factors <italic>Mask</italic> and <italic>Distractor</italic> (<italic>F(1,28) = .25, p = .619, η<sub>p</sub>
<sup>2</sup> = .01</italic>, <xref ref-type="fig" rid="F3">Figure 3B</xref> and <xref ref-type="fig" rid="F3">Figure 3C</xref>). As the speech envelope conveys crucial information about the syntactic structure of speech (<xref ref-type="bibr" rid="R25">Giraud &amp; Poeppel, 2012</xref>; <xref ref-type="bibr" rid="R43">Poeppel &amp; Assaneo, 2020</xref>), reduced reconstruction accuracy points to difficulties in deriving this information for further higher-level speech processing.</p>
</sec>
<sec id="S15">
<title>Reconstruction of important spectral fine details is generally affected by face masks</title>
<p id="P29">Moreover, we wanted to investigate the influence of face masks on spectral fine details of speech. In this study, we specifically analysed pitch (or fundamental frequency, F0), the first formant (F1), the second formant (F2) and the third formant (F3). When facing concurrent speakers, a listener must segregate the speech signal into different speech streams. It is suggested that pitch serves a fundamental role in this process (<xref ref-type="bibr" rid="R7">Bregman, 1990</xref>). Formants are especially interesting, as they are vital for identifying vowels. Additionally, we investigated the averaged F2 and F3 (F2/3), as these two formants generated in the front cavity converge into ‘focal points’ after specific vowel-consonant combinations (<xref ref-type="bibr" rid="R2">Badin et al., 1990</xref>) and its frequency range has been shown to correlate strongly with lip movements (<xref ref-type="bibr" rid="R12">Chandrasekaran et al., 2009</xref>). Furthermore, this characteristic has been shown to be tracked by visual-only speech and is therefore prone to be affected through face masks (<xref ref-type="bibr" rid="R51">Suess et al., 2021</xref>). Detailed results for F1, F2, F3 are depicted in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref> (see Supplementary Material). Effect sizes of the main effect are presented graphically in <xref ref-type="fig" rid="F3">Figure 3A</xref> and for the interactions in <xref ref-type="fig" rid="F3">Figure 3B</xref>. With a distractor speaker, reconstruction of pitch (<italic>F(1,28) = 89.18, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .76</italic>) and F2/3 (<italic>F(1,28) = 75.81, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .73</italic>) was reduced. The reconstruction of the pitch shows a general impairment through face masks (<italic>F(1,28) = 7.26, p = .018, η<sub>p</sub>
<sup>2</sup> = .21</italic>) with no significant interaction (<italic>F(2,28) = .49, p = .487, η<sub>p</sub>
<sup>2</sup> = .02</italic>). The same is true for F2/3, which showed a significant reduction of reconstruction accuracy through face masks (<italic>F(1,28) = 14.78, p &lt;.001, η<sub>p</sub>
<sup>2</sup> = .35</italic>). While reconstruction was not affected through a face mask when no distractor was present (No Distractor: <italic>MD (SE) = .011 (.006), p = .107</italic>), it was reduced in presence of a distractor (Distractor: <italic>MD (SE) = .025 (.007), p &lt; .001</italic>). This interaction was however not significant (<italic>F(1,28) = 2.76, p = .108, η<sub>p</sub>
<sup>2</sup> = .09</italic>). These results suggest that face masks do impair the tracking of spectral fine details of the speech relevant spectrum generally irrespective of a distractor speaker.</p>
</sec>
<sec id="S16">
<title>Reconstruction for phonetic and lexical boundaries is impaired through face masks specifically in difficult listening situations</title>
<p id="P30">Detecting lexical boundaries is important for chunking the continuous speech stream into meaningful interpretable units. As a last step, we therefore investigated how face masks impair the reconstruction of phoneme and word onsets.</p>
<p id="P31">For phoneme onsets, we found significant main effects of reconstruction accuracies for the factor <italic>Distractor (F(1,28) = 187.81, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .87</italic>) and <italic>Mask (F(1,28) = 16.63, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .37</italic>), as well as a strong significant interaction of <italic>Mask</italic> and <italic>Distractor</italic> (<italic>F(1,28) = 10.75, p = .003, η<sub>p</sub>
<sup>2</sup> = .28</italic>). Similar results can be shown for word onset reconstruction accuracies with significant main effects of the <italic>Distractor</italic> (<italic>F(1,28) = 278.19, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .91</italic>), <italic>Mask</italic> (<italic>F(1,28) = 19.95, p &lt; .001, η<sub>p</sub>
<sup>2</sup> = .42</italic>) and the interaction (<italic>F(1,28) = 11.46, p = .002, η<sub>p</sub>
<sup>2</sup> = .29</italic>). For phoneme onset, post-hoc simple effect tests revealed significant differences for the factor <italic>Mask</italic> when a distractor was present, while only showing a trend when no distractor was presented (No Distractor: <italic>MD (SE) = .008 (.004), p = .058</italic>; Distractor: <italic>MD (SE) = .020 (.004), p &lt; .001</italic>, see <xref ref-type="fig" rid="F3">Figure 3C</xref>). For word onsets, we found significant differences irrespective of a distractor speaker, but a strongly increasing effect when a distractor speaker was presented alongside (No Distractor: <italic>MD (SE) = .004 (.002), p = .017</italic>; Distractor: <italic>MD (SE) = .012 (.003), p &lt; .001</italic>). Following this, face masks seem to decrease the ability to segment the speech stream into meaningful units when listeners are encountering challenging listening situations.</p>
</sec>
<sec id="S17">
<title>Effects of face masks on acoustics of spectral fine details</title>
<p id="P32">In order to rule out that our findings were influenced by major acoustic differences between mask wearing speakers and speakers without masks, we computed paired sample t-tests for pitch and formants. The features (pitch and formants) extracted out of the 40 stimuli with no mask were therefore compared to their counterpart with speakers wearing a mask. Mean frequencies split up by sex of speaker are depicted in <xref ref-type="supplementary-material" rid="SD1">Table S2</xref> (see Supplementary Material). Results showed no significant mean difference for pitch (<italic>t(39) = .79, p = .43, g = .02</italic>) and the first formant (F1) (<italic>t(39) = 1.90, p = .06, g = .14</italic>). We noted strong effects for a reduction of mean frequency for the second formant (F2) (<italic>t(39) = 11. 89, p &lt; .001, g = .26</italic>), the third formant (F3) (<italic>t(39) = 14.01, p &lt; .001, g = .26</italic>) and the averaged second and third formant (Formant 2/3 or F2/3) (<italic>t(39) = 14.98, p &lt; .001, g = .26</italic>) in stimuli with face masks compared to no face masks. However, in absolute terms differences are relatively small (<italic>Hedges g</italic> of around .2).</p>
</sec>
</sec>
<sec id="S18" sec-type="discussion">
<title>Discussion</title>
<p id="P33">The adverse effect of face masks on speech comprehension has been investigated in various studies on a behavioural level (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>; <xref ref-type="bibr" rid="R24">Giovanelli et al., 2021</xref>; <xref ref-type="bibr" rid="R45">Rahne et al., 2021</xref>; <xref ref-type="bibr" rid="R53">Toscano &amp; Toscano, 2021</xref>; <xref ref-type="bibr" rid="R58">Yi et al., 2021</xref>). Despite the overall agreement of the adverse effects of face masks on speech comprehension, it has been unclear which features of speech processing are specifically affected.</p>
<p id="P34">Our results show that tracking of features responsible for successful processing of naturalistic speech are impaired through (surgical) face masks. From general temporal modulations of the speech envelope to modulations of spectral fine details (pitch and formants) and segmentation of speech (phoneme and word onsets), a face mask significantly reduces the decodability of these features from brain data. However, not all these speech features are affected by the face mask the same way. While the brain‘s tracking of low-level acoustic features (i.e. the speech envelope and spectral fine details) are affected generally, the higher-level segmentational features phoneme onset and word onset show particularly strong reduction of reconstruction accuracy through face masks when facing a challenging listening situation (i.e. using a distractor speaker). We used surgical face masks in our study, which have a small influence on the speech acoustics and attenuate only higher frequencies above 3 kHz (<xref ref-type="bibr" rid="R14">Corey et al., 2020</xref>; <xref ref-type="bibr" rid="R53">Toscano &amp; Toscano, 2021</xref>). With this, we attribute these interactions mostly to the missing visual input. Moreover, investigated spectral fine details (namely pitch and formants) present themselves in frequencies below 3 kHz (<xref ref-type="bibr" rid="R41">Peterson &amp; Barney, 1952</xref>). Furthermore, we found only small differences between stimuli with face masks and without a face mask.</p>
<sec id="S19">
<title>Face masks impair tracking of crucial acoustic speech features generally</title>
<p id="P35">The speech envelope, mostly associated with conducting syntactic and phonetic information (<xref ref-type="bibr" rid="R25">Giraud &amp; Poeppel, 2012</xref>; <xref ref-type="bibr" rid="R43">Poeppel &amp; Assaneo, 2020</xref>), has been deemed a core characteristic regarding speech tracking (<xref ref-type="bibr" rid="R9">Brodbeck &amp; Simon, 2020</xref>). In multi-speaker listening situations, attending to the target speaker is related to enhanced tracking of the envelope of the attended speech compared to the unattended speech (<xref ref-type="bibr" rid="R37">O’Sullivan et al., 2015</xref>; <xref ref-type="bibr" rid="R38">Park et al., 2016</xref>). A reduced tracking of this characteristic might represent a difficulty in following and segmenting the target speech stream when confronted with face masks. Nonetheless it is important as the speech envelope does not convey specific information regarding certain phonetic objects, like vowels. Formants on the other side define vowels directly (<xref ref-type="bibr" rid="R41">Peterson &amp; Barney, 1952</xref>). While the first (F1) and (F2) are generally considered core formants in speech (<xref ref-type="bibr" rid="R41">Peterson &amp; Barney, 1952</xref>), using an averaged F2 and F3 (F2/3) instead of F2 has proven to be beneficial as it smooths transitions from one vowel to the other (<xref ref-type="bibr" rid="R50">Stevens, 2000</xref>) and due to their convergence in the front cavity (<xref ref-type="bibr" rid="R2">Badin et al., 1990</xref>). With regards to visual speech tracking, the encompassed frequencies of F2 and F3 correlate strongly with lip movements (<xref ref-type="bibr" rid="R12">Chandrasekaran et al., 2009</xref>) so that these frequencies likely contribute to a visual-phonological transformation (cf. <xref ref-type="bibr" rid="R28">Hauswald et al. (2018)</xref>). While <xref ref-type="bibr" rid="R28">Hauswald et al. (2018)</xref> proposed a role of the visually conveyed envelope information for a visuo-phonological transformation, a study by our group further suggests that also the visually transported formant information contributes to such a transformation (<xref ref-type="bibr" rid="R51">Suess et al., 2021</xref>). Further highlighting their importance, a recent study (<xref ref-type="bibr" rid="R42">Plass et al., 2020</xref>) demonstrated even stronger crossmodal AV enhancement for formant frequencies than for the speech envelope. Finally, the reconstruction of voice pitch or fundamental frequency, used to segregate concurrent speech streams (<xref ref-type="bibr" rid="R7">Bregman, 1990</xref>), is also reduced through face masks, which might lead to difficulties disentangling the target speech stream and the distractor speech stream. Taking the effects face masks have on the envelope, pitch and formants together, face coverings might lead to subsequent difficulties in identifying phonemes and as a consequence also words.</p>
</sec>
<sec id="S20">
<title>Face masks impair tracking of higher-level segmentational features especially in challenging listening situations</title>
<p id="P36">Tracking of phoneme and word onsets is affected such that face masks impair chunking in challenging listening situations especially strong. Studies investigating simple event-related potentials (ERPs) when listening to continuous speech found enhanced responses to word onsets (<xref ref-type="bibr" rid="R47">Sanders et al., 2002</xref>; <xref ref-type="bibr" rid="R46">Sanders &amp; Neville, 2003</xref>), pointing to an internal chunking mechanism of the brain for optimal speech processing. On a lower level, brain responses induced by phoneme onsets are reliably predicted by encoding models (<xref ref-type="bibr" rid="R8">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="R19">Daube et al., 2019</xref>; <xref ref-type="bibr" rid="R21">Di Liberto et al., 2015</xref>), implying chunking already on this level. When deprived of visual cues (through face masks) and in noisy acoustic environments, our findings suggest that individuals face problems with segmenting the continuous speech stream into meaningful units (i.e. words and phonemes). As the speech envelope is associated with conveying syntactic information (<xref ref-type="bibr" rid="R25">Giraud &amp; Poeppel, 2012</xref>), reduced detail of this feature can cause difficulties in identifying semantically meaningful units. Furthermore, formant frequencies might also play an important role in detecting syllables and more importantly phonemes (and their boundaries) (<xref ref-type="bibr" rid="R42">Plass et al., 2020</xref>). For compensating this degradation in challenging listening situations, watching the speaker’s face provides important information (<xref ref-type="bibr" rid="R35">Mitchel &amp; Weiss, 2014</xref>) for word segmentation. Highlighting this even further, visual cues from the mouth area have been found to be enhancing phonetic discrimination, by providing visemic information (<xref ref-type="bibr" rid="R23">Fisher, 1968</xref>). Taken together, depriving listeners of these visual cues through covering the mouth affects an important step of unit identification (words and phonemes), which helps chunking the stream for further processing.</p>
<p id="P37">With this study, we put past findings about AV speech processing and tracking into the context of face masks. Expectations about the influence of face masks on speech characteristics were confirmed in the way that compensating visual information does partly make up for the degraded speech acoustics in difficult listening situations. This effect can be shown in higher-level features of speech segmentation (i.e. phoneme onsets and word onsets) in the form of an interaction between the face mask and the distractor speaker, while reconstruction of acoustic information is generally impaired. One possible mechanism compensating in noisy environments is a visuo-phonological transformation from visual input to a phonetic representation in the range of F2 and F3, which is however not possible when speakers wear a face mask.</p>
</sec>
<sec id="S21">
<title>Face masks increase subjective listening difficulty, while speech comprehension is unaffected</title>
<p id="P38">Regarding our behavioural results, we observed significantly decreased performance through a distractor speaker, but not through the face mask. This is in line with previous findings on audio-only speech (<xref ref-type="bibr" rid="R53">Toscano &amp; Toscano, 2021</xref>) which found no significant effect of surgical face masks on word recognition in easy and challenging listening situations. However, another study with audiovisual speech found significant effects of a surgical face mask in conditions of low (-3 SNR) and high (-9 SNR) background pink noise on sentence intelligibility (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>). As our study used longer duration audiobooks, our behavioural measurements might have not been precise enough (i.e only two binary unstandardised ‘true or false’ statements at the end of each trial regarding semantic comprehension) to detect this influence.</p>
<p id="P39">We also found that subjective ratings of listening difficulty were significantly larger when speakers wore a face mask independent of a distractor speaker. An explanation for this is that removing congruent visual cues leads to an increase of linguistic ambiguity resulting in more effortful mental correction by the listener. This increased effort however might be at the same time negating the negative influence on the aforementioned comprehension performance (<xref ref-type="bibr" rid="R56">Winn &amp; Teece, 2021</xref>). Despite a comparable performance in speech comprehension between with- and without masks, increased listening effort has been found to be associated with social withdrawal in the hearing impaired population (<xref ref-type="bibr" rid="R31">Hughes et al., 2018</xref>) and should not be disregarded. Still, our behavioural results contradict previous findings, which only showed an effect of face masks on listening effort when combined with background noise (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>). Again, differences in study design (one minute audiobooks vs. single sentence) may account for this difference.</p>
<p id="P40">Furthermore, only participants without hearing impairment participated in our study. It has been shown that individuals with hearing loss profit significantly more from visual input (<xref ref-type="bibr" rid="R44">Puschmann et al., 2019</xref>) when it comes to behavioural performance as well as neural tracking of speech features. For future studies, we expect even stronger effects through face masks for individuals with hearing impairments compared to normal hearing persons, already occurring in situations with only a single speaker.</p>
<p id="P41">Following our findings, the use of transparent face masks, especially in critical settings like medical consultations, where poor communication can lead to poorer medical outcomes (<xref ref-type="bibr" rid="R13">Chodosh et al., 2020</xref>), is principally favourable. However, some of the current transparent models come with significantly reduced transmission of acoustic detail (<xref ref-type="bibr" rid="R14">Corey et al., 2020</xref>) resulting in reduced intelligibility and increased difficulty ratings, when presented in noisy environments (<xref ref-type="bibr" rid="R10">Brown et al., 2021</xref>). It is important to consider that this study investigated normal hearing subjects, and results for individuals with hearing loss might be different. In line with this notion, data collected before the Covid-19 pandemic suggests strong benefits of transparent face masks for listeners with hearing loss (<xref ref-type="bibr" rid="R1">Atcherson et al., 2017</xref>). A recent study confirms this by comparing the impact of surgical face masks to (transparent) face shields (<xref ref-type="bibr" rid="R30">Homans &amp; Vroegop, 2021</xref>). Despite the face shield’s larger impact on acoustics compared to the surgical face masks, individuals with hearing loss showed no significant decrease in speech intelligibility when confronted with a face shield compared to no facial covering, while scores were significantly worse when a surgical face mask was worn. Unfortunately, face shields are not a good option in regards to virus transmission as they show basically no blocking of cough aerosol (<xref ref-type="bibr" rid="R34">Lindsley et al., 2021</xref>). Therefore, a transparent face mask which ensures the acoustic details are not lost may be the best solution for reducing mask-related speech processing costs.</p>
</sec>
</sec>
<sec id="S22" sec-type="conclusions">
<title>Conclusion</title>
<p id="P42">With this study, we showed that face masks impair the tracking of several important speech features in normal hearing individuals. Face masks show a more general impairment on tracking of spectral features such as pitch and the F2/3 and speech envelope modulations, while higher-level segmentational features are especially impaired through face masks in difficult listening situations. We attribute these findings to missing visual information, which could facilitate the speech tracking via a visuo-phonological transformation, partly compensating for the noisy acoustic input. Our behavioural results point towards a generally increased listening effort through face masks, which can be leading to social withdrawal particularly for hearing-impaired individuals. Following our findings, the use of acoustics-conserving transparent face masks can severely reduce listening related difficulties and should be considered when dealing with the hearing impaired. However, as we only tested normal hearing individuals, future research should focus on hearing-impaired individuals.</p>
</sec>
<sec sec-type="supplementary-material" id="SM">
<title>Supplementary Material</title>
<supplementary-material content-type="local-data" id="SD1">
<label>Supplementary Material</label>
<media xlink:href="EMS136061-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="N66762" position="anchor"/>
</supplementary-material>
</sec>
</body>
<back>
<ack id="S23">
<title>Acknowledgements</title>
<p>This work is supported by the Austrian Science Fund, P34237 (“Impact of face masks on speech comprehension”). Sound icon made by <italic>Smashicon</italic> from <ext-link ext-link-type="uri" xlink:href="http://www.flaticon.com">www.flaticon.com</ext-link>.</p>
<p>Thanks to the whole research team. Special thanks to Fabian Schmidt for providing support for the early days of PhD life and for graphical design.</p>
</ack>
<fn-group>
<fn id="FN1" fn-type="conflict">
<p id="P43">
<bold>Competing Interest Statement</bold>
</p>
<p id="P44">The authors have declared no competing interest.</p>
</fn>
</fn-group>
<ref-list>
<ref id="R1">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Atcherson</surname>
<given-names>SR</given-names>
</name>
<name>
<surname>Mendel</surname>
<given-names>LL</given-names>
</name>
<name>
<surname>Baltimore</surname>
<given-names>WJ</given-names>
</name>
<name>
<surname>Patro</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Lee</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Pousson</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Spann</surname>
<given-names>MJ</given-names>
</name>
</person-group>
<article-title>The Effect of Conventional and Transparent Surgical Masks on Speech Understanding in Individuals with and without Hearing Loss</article-title>
<source>Journal of the American Academy of Audiology</source>
<year>2017</year>
<volume>28</volume>
<issue>01</issue>
<fpage>058</fpage>
<lpage>067</lpage>
<pub-id pub-id-type="doi">10.3766/jaaa.15151</pub-id>
</element-citation>
</ref>
<ref id="R2">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Badin</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Perrier</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Boë</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Abry</surname>
<given-names>C</given-names>
</name>
</person-group>
<article-title>Vocalic nomograms: Acoustic and articulatory considerations upon formant convergences</article-title>
<source>The Journal of the Acoustical Society of America</source>
<year>1990</year>
<volume>87</volume>
<issue>3</issue>
<fpage>1290</fpage>
<lpage>1300</lpage>
<pub-id pub-id-type="doi">10.1121/1.398804</pub-id>
</element-citation>
</ref>
<ref id="R3">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Besle</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Bertrand</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Giard</surname>
<given-names>M-H</given-names>
</name>
</person-group>
<article-title>Electrophysiological (EEG, sEEG, MEG) evidence for multiple audiovisual interactions in the human auditory cortex</article-title>
<source>Hearing Research</source>
<year>2009</year>
<volume>258</volume>
<issue>1</issue>
<fpage>143</fpage>
<lpage>151</lpage>
<pub-id pub-id-type="doi">10.1016/j.heares.2009.06.016</pub-id>
</element-citation>
</ref>
<ref id="R4">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Besle</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Fort</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Delpuech</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Giard</surname>
<given-names>M-H</given-names>
</name>
</person-group>
<article-title>Bimodal speech: Early suppressive visual effects in human auditory cortex</article-title>
<source>European Journal of Neuroscience</source>
<year>2004</year>
<volume>20</volume>
<issue>8</issue>
<fpage>2225</fpage>
<lpage>2234</lpage>
<pub-id pub-id-type="doi">10.1111/j.1460-9568.2004.03670.x</pub-id>
</element-citation>
</ref>
<ref id="R5">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Boersma</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Weenink</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>PRAAT, a system for doing phonetics by computer</article-title>
<source>Glot International</source>
<year>2001</year>
<volume>5</volume>
<fpage>341</fpage>
<lpage>345</lpage>
</element-citation>
</ref>
<ref id="R6">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brainard</surname>
<given-names>DH</given-names>
</name>
</person-group>
<article-title>The Psychophysics Toolbox</article-title>
<source>Spatial Vision</source>
<year>1997</year>
<volume>10</volume>
<issue>4</issue>
<fpage>433</fpage>
<lpage>436</lpage>
<pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>
</element-citation>
</ref>
<ref id="R7">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bregman</surname>
<given-names>A</given-names>
</name>
</person-group>
<article-title>Auditory Scene Analysis: The Perceptual Organization of Sound</article-title>
<source>Journal of The Acoustical Society of America—J ACOUST SOC AMER</source>
<year>1990</year>
<volume>95</volume>
<pub-id pub-id-type="doi">10.1121/1.408434</pub-id>
</element-citation>
</ref>
<ref id="R8">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brodbeck</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Hong</surname>
<given-names>LE</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>JZ</given-names>
</name>
</person-group>
<article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title>
<source>Current Biology</source>
<year>2018</year>
<volume>28</volume>
<issue>24</issue>
<fpage>3976</fpage>
<lpage>3983</lpage>
<elocation-id>e5</elocation-id>
<pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id>
</element-citation>
</ref>
<ref id="R9">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brodbeck</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>JZ</given-names>
</name>
</person-group>
<article-title>Continuous speech processing</article-title>
<source>Current Opinion in Physiology</source>
<year>2020</year>
<volume>18</volume>
<fpage>25</fpage>
<lpage>31</lpage>
<pub-id pub-id-type="doi">10.1016/j.cophys.2020.07.014</pub-id>
</element-citation>
</ref>
<ref id="R10">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brown</surname>
<given-names>VA</given-names>
</name>
<name>
<surname>Van Engen</surname>
<given-names>KJ</given-names>
</name>
<name>
<surname>Peelle</surname>
<given-names>JE</given-names>
</name>
</person-group>
<article-title>Face mask type affects audiovisual speech intelligibility and subjective listening effort in young and older adults</article-title>
<source>PsyArXiv</source>
<year>2021</year>
<comment>[Preprint]</comment>
<pub-id pub-id-type="doi">10.31234/osf.io/7waj3</pub-id>
</element-citation>
</ref>
<ref id="R11">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Caniato</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Marzi</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Gasparella</surname>
<given-names>A</given-names>
</name>
</person-group>
<article-title>How much COVID-19 face protections influence speech intelligibility in classrooms?</article-title>
<source>Applied Acoustics</source>
<year>2021</year>
<volume>178</volume>
<elocation-id>108051</elocation-id>
<pub-id pub-id-type="doi">10.1016/j.apacoust.2021.108051</pub-id>
</element-citation>
</ref>
<ref id="R12">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chandrasekaran</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Trubanova</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Stillittano</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Caplier</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Ghazanfar</surname>
<given-names>AA</given-names>
</name>
</person-group>
<article-title>The Natural Statistics of Audiovisual Speech</article-title>
<source>PLOS Computational Biology</source>
<year>2009</year>
<volume>5</volume>
<issue>7</issue>
<elocation-id>e1000436</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id>
</element-citation>
</ref>
<ref id="R13">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chodosh</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Weinstein</surname>
<given-names>BE</given-names>
</name>
<name>
<surname>Blustein</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Face masks can be devastating for people with hearing loss</article-title>
<source>BMJ</source>
<year>2020</year>
<volume>370</volume>
<elocation-id>m2683</elocation-id>
<pub-id pub-id-type="doi">10.1136/bmj.m2683</pub-id>
</element-citation>
</ref>
<ref id="R14">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Corey</surname>
<given-names>RM</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>U</given-names>
</name>
<name>
<surname>Singer</surname>
<given-names>AC</given-names>
</name>
</person-group>
<article-title>Acoustic effects of medical, cloth, and transparent face masks on speech signals</article-title>
<source>The Journal of the Acoustical Society of America</source>
<year>2020</year>
<volume>148</volume>
<issue>4</issue>
<fpage>2371</fpage>
<lpage>2375</lpage>
<pub-id pub-id-type="doi">10.1121/10.0002279</pub-id>
</element-citation>
</ref>
<ref id="R15">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crosse</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Butler</surname>
<given-names>JS</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions</article-title>
<source>Journal of Neuroscience</source>
<year>2015</year>
<volume>35</volume>
<issue>42</issue>
<fpage>14195</fpage>
<lpage>14204</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.1829-15.2015</pub-id>
</element-citation>
</ref>
<ref id="R16">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crosse</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Di Liberto</surname>
<given-names>GM</given-names>
</name>
<name>
<surname>Bednar</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title>
<source>Frontiers in Human Neuroscience</source>
<year>2016</year>
<volume>10</volume>
<pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id>
</element-citation>
</ref>
<ref id="R17">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crosse</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Di Liberto</surname>
<given-names>GM</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Eye Can Hear Clearly Now: Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Long-Term Crossmodal Temporal Integration</article-title>
<source>Journal of Neuroscience</source>
<year>2016</year>
<volume>36</volume>
<issue>38</issue>
<fpage>9888</fpage>
<lpage>9895</lpage>
</element-citation>
</ref>
<ref id="R18">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Crosse</surname>
<given-names>MJ</given-names>
</name>
<name>
<surname>Zuk</surname>
<given-names>NJ</given-names>
</name>
<name>
<surname>Liberto</surname>
<given-names>GMD</given-names>
</name>
<name>
<surname>Nidiffer</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Molholm</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Linear Modeling of Neurophysiological Responses to Naturalistic Stimuli: Methodological Considerations for Applied Research</article-title>
<source>PsyArXiv</source>
<year>2021</year>
<pub-id pub-id-type="doi">10.31234/osf.io/jbz2w</pub-id>
</element-citation>
</ref>
<ref id="R19">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Daube</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Ince</surname>
<given-names>RAA</given-names>
</name>
<name>
<surname>Gross</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Simple Acoustic Features Can Explain Phoneme-Based Predictions of Cortical Responses to Speech</article-title>
<source>Current Biology</source>
<year>2019</year>
<volume>29</volume>
<issue>12</issue>
<fpage>1924</fpage>
<lpage>1937</lpage>
<elocation-id>e9</elocation-id>
<pub-id pub-id-type="doi">10.1016/j.cub.2019.04.067</pub-id>
</element-citation>
</ref>
<ref id="R20">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>de Jong</surname>
<given-names>NH</given-names>
</name>
<name>
<surname>Wempe</surname>
<given-names>T</given-names>
</name>
</person-group>
<article-title>Praat script to detect syllable nuclei and measure speech rate automatically</article-title>
<source>Behavior Research Methods</source>
<year>2009</year>
<volume>41</volume>
<issue>2</issue>
<fpage>385</fpage>
<lpage>390</lpage>
<pub-id pub-id-type="doi">10.3758/BRM.41.2.385</pub-id>
</element-citation>
</ref>
<ref id="R21">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Di Liberto</surname>
<given-names>GM</given-names>
</name>
<name>
<surname>O’Sullivan</surname>
<given-names>JA</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title>
<source>Current Biology: CB</source>
<year>2015</year>
<volume>25</volume>
<issue>19</issue>
<fpage>2457</fpage>
<lpage>2465</lpage>
<pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id>
</element-citation>
</ref>
<ref id="R22">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ding</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>JZ</given-names>
</name>
</person-group>
<article-title>Cortical entrainment to continuous speech: Functional roles and interpretations</article-title>
<source>Frontiers in Human Neuroscience</source>
<year>2014</year>
<volume>8</volume>
<fpage>311</fpage>
<pub-id pub-id-type="doi">10.3389/fnhum.2014.00311</pub-id>
</element-citation>
</ref>
<ref id="R23">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fisher</surname>
<given-names>CG</given-names>
</name>
</person-group>
<article-title>Confusions Among Visually Perceived Consonants</article-title>
<source>Journal of Speech and Hearing Research</source>
<year>1968</year>
<volume>11</volume>
<issue>4</issue>
<fpage>796</fpage>
<lpage>804</lpage>
<pub-id pub-id-type="doi">10.1044/jshr.1104.796</pub-id>
</element-citation>
</ref>
<ref id="R24">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Giovanelli</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Valzolgher</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Gessa</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Todeschini</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Pavani</surname>
<given-names>F</given-names>
</name>
</person-group>
<article-title>Unmasking the Difficulty of Listening to Talkers With Masks: Lessons from the COVID-19 pandemic</article-title>
<source>I-Perception</source>
<year>2021</year>
<volume>12</volume>
<issue>2</issue>
<elocation-id>2041669521998393</elocation-id>
<pub-id pub-id-type="doi">10.1177/2041669521998393</pub-id>
</element-citation>
</ref>
<ref id="R25">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Giraud</surname>
<given-names>A-L</given-names>
</name>
<name>
<surname>Poeppel</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>Cortical oscillations and speech processing: Emerging computational principles and operations</article-title>
<source>Nature Neuroscience</source>
<year>2012</year>
<volume>15</volume>
<issue>4</issue>
<fpage>511</fpage>
<lpage>517</lpage>
<pub-id pub-id-type="doi">10.1038/nn.3063</pub-id>
</element-citation>
</ref>
<ref id="R26">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Golumbic</surname>
<given-names>EZ</given-names>
</name>
<name>
<surname>Cogan</surname>
<given-names>GB</given-names>
</name>
<name>
<surname>Schroeder</surname>
<given-names>CE</given-names>
</name>
<name>
<surname>Poeppel</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>Visual Input Enhances Selective Speech Envelope Tracking in Auditory Cortex at a “Cocktail Party”</article-title>
<source>Journal of Neuroscience</source>
<year>2013</year>
<volume>33</volume>
<issue>4</issue>
<fpage>1417</fpage>
<lpage>1426</lpage>
<pub-id pub-id-type="doi">10.1523/JNEUROSCI.3675-12.2013</pub-id>
</element-citation>
</ref>
<ref id="R27">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hartmann</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Weisz</surname>
<given-names>N</given-names>
</name>
</person-group>
<article-title>An Introduction to the Objective Psychophysics Toolbox</article-title>
<source>Frontiers in Psychology</source>
<year>2020</year>
<volume>11</volume>
<pub-id pub-id-type="doi">10.3389/fpsyg.2020.585437</pub-id>
</element-citation>
</ref>
<ref id="R28">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hauswald</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Lithari</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Collignon</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Leonardelli</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Weisz</surname>
<given-names>N</given-names>
</name>
</person-group>
<article-title>A Visual Cortical Network for Deriving Phonological Information from Intelligible Lip Movements</article-title>
<source>Current Biology</source>
<year>2018</year>
<volume>28</volume>
<issue>9</issue>
<fpage>1453</fpage>
<lpage>1459</lpage>
<elocation-id>e3</elocation-id>
<pub-id pub-id-type="doi">10.1016/j.cub.2018.03.044</pub-id>
</element-citation>
</ref>
<ref id="R29">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hentschke</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Stüttgen</surname>
<given-names>MC</given-names>
</name>
</person-group>
<article-title>Computation of measures of effect size for neuroscience data sets</article-title>
<source>European Journal of Neuroscience</source>
<year>2011</year>
<volume>34</volume>
<issue>12</issue>
<fpage>1887</fpage>
<lpage>1894</lpage>
<pub-id pub-id-type="doi">10.1111/j.1460-9568.2011.07902.x</pub-id>
</element-citation>
</ref>
<ref id="R30">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Homans</surname>
<given-names>NC</given-names>
</name>
<name>
<surname>Vroegop</surname>
<given-names>JL</given-names>
</name>
</person-group>
<article-title>The impact of face masks on the communication of adults with hearing loss during COVID-19 in a clinical setting</article-title>
<source>International Journal of Audiology</source>
<year>2021</year>
<volume>0</volume>
<issue>0</issue>
<fpage>1</fpage>
<lpage>6</lpage>
<pub-id pub-id-type="doi">10.1080/14992027.2021.1952490</pub-id>
</element-citation>
</ref>
<ref id="R31">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hughes</surname>
<given-names>SE</given-names>
</name>
<name>
<surname>Hutchings</surname>
<given-names>HA</given-names>
</name>
<name>
<surname>Rapport</surname>
<given-names>FL</given-names>
</name>
<name>
<surname>McMahon</surname>
<given-names>CM</given-names>
</name>
<name>
<surname>Boisvert</surname>
<given-names>I</given-names>
</name>
</person-group>
<article-title>Social Connectedness and Perceived Listening Effort in Adult Cochlear Implant Users: A Grounded Theory to Establish Content Validity for a New Patient-Reported Outcome Measure</article-title>
<source>Ear and Hearing</source>
<year>2018</year>
<volume>39</volume>
<issue>5</issue>
<fpage>922</fpage>
<lpage>934</lpage>
<pub-id pub-id-type="doi">10.1097/AUD.0000000000000553</pub-id>
</element-citation>
</ref>
<ref id="R32">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kisler</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Reichel</surname>
<given-names>U</given-names>
</name>
<name>
<surname>Schiel</surname>
<given-names>F</given-names>
</name>
</person-group>
<article-title>Multilingual processing of speech via web services</article-title>
<source>Computer Speech &amp; Language</source>
<year>2017</year>
<volume>45</volume>
<fpage>326</fpage>
<lpage>347</lpage>
<pub-id pub-id-type="doi">10.1016/j.csl.2017.01.005</pub-id>
</element-citation>
</ref>
<ref id="R33">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kleiner</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Brainard</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Pelli</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Ingling</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Murray</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Broussard</surname>
<given-names>C</given-names>
</name>
</person-group>
<article-title>What’s new in psychtoolbox-3</article-title>
<source>Perception</source>
<year>2007</year>
<volume>36</volume>
<issue>14</issue>
<fpage>1</fpage>
<lpage>16</lpage>
</element-citation>
</ref>
<ref id="R34">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lindsley</surname>
<given-names>WG</given-names>
</name>
<name>
<surname>Blachere</surname>
<given-names>FM</given-names>
</name>
<name>
<surname>Law</surname>
<given-names>BF</given-names>
</name>
<name>
<surname>Beezhold</surname>
<given-names>DH</given-names>
</name>
<name>
<surname>Noti</surname>
<given-names>JD</given-names>
</name>
</person-group>
<article-title>Efficacy of face masks, neck gaiters and face shields for reducing the expulsion of simulated cough-generated aerosols</article-title>
<source>Aerosol Science and Technology</source>
<year>2021</year>
<volume>55</volume>
<issue>4</issue>
<fpage>449</fpage>
<lpage>457</lpage>
<pub-id pub-id-type="doi">10.1080/02786826.2020.1862409</pub-id>
</element-citation>
</ref>
<ref id="R35">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mitchel</surname>
<given-names>AD</given-names>
</name>
<name>
<surname>Weiss</surname>
<given-names>DJ</given-names>
</name>
</person-group>
<article-title>Visual speech segmentation: Using facial cues to locate word boundaries in continuous speech</article-title>
<source>Language and Cognitive Processes</source>
<year>2014</year>
<volume>29</volume>
<issue>7</issue>
<fpage>771</fpage>
<lpage>780</lpage>
<pub-id pub-id-type="doi">10.1080/01690965.2013.791703</pub-id>
</element-citation>
</ref>
<ref id="R36">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oostenveld</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Fries</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Maris</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Schoffelen</surname>
<given-names>J-M</given-names>
</name>
</person-group>
<article-title>FieldTrip: Open Source Software for Advanced Analysis of MEG, EEG, and Invasive Electrophysiological Data</article-title>
<source>Computational Intelligence and Neuroscience</source>
<year>2011</year>
<volume>2011</volume>
<elocation-id>156869</elocation-id>
<pub-id pub-id-type="doi">10.1155/2011/156869</pub-id>
</element-citation>
</ref>
<ref id="R37">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>O’Sullivan</surname>
<given-names>JA</given-names>
</name>
<name>
<surname>Power</surname>
<given-names>AJ</given-names>
</name>
<name>
<surname>Mesgarani</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Rajaram</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Foxe</surname>
<given-names>JJ</given-names>
</name>
<name>
<surname>Shinn-Cunningham</surname>
<given-names>BG</given-names>
</name>
<name>
<surname>Slaney</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Shamma</surname>
<given-names>SA</given-names>
</name>
<name>
<surname>Lalor</surname>
<given-names>EC</given-names>
</name>
</person-group>
<article-title>Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</article-title>
<source>Cerebral Cortex</source>
<year>2015</year>
<volume>25</volume>
<issue>7</issue>
<fpage>1697</fpage>
<lpage>1706</lpage>
<pub-id pub-id-type="doi">10.1093/cercor/bht355</pub-id>
</element-citation>
</ref>
<ref id="R38">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Park</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Kayser</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Thut</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Gross</surname>
<given-names>J</given-names>
</name>
</person-group>
<article-title>Lip movements entrain the observers’ low-frequency brain oscillations to facilitate speech intelligibility</article-title>
<source>ELife</source>
<year>2016</year>
<volume>5</volume>
<elocation-id>e14521</elocation-id>
<pub-id pub-id-type="doi">10.7554/eLife.14521</pub-id>
</element-citation>
</ref>
<ref id="R39">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Peelle</surname>
<given-names>JE</given-names>
</name>
<name>
<surname>Sommers</surname>
<given-names>MS</given-names>
</name>
</person-group>
<article-title>Prediction and constraint in audiovisual speech perception</article-title>
<source>Cortex</source>
<year>2015</year>
<volume>68</volume>
<fpage>169</fpage>
<lpage>181</lpage>
<pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.006</pub-id>
</element-citation>
</ref>
<ref id="R40">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pelli</surname>
<given-names>DG</given-names>
</name>
</person-group>
<article-title>The VideoToolbox software for visual psychophysics: Transforming numbers into movies</article-title>
<source>Spatial Vision</source>
<year>1997</year>
<volume>10</volume>
<issue>4</issue>
<fpage>437</fpage>
<lpage>442</lpage>
</element-citation>
</ref>
<ref id="R41">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Peterson</surname>
<given-names>GE</given-names>
</name>
<name>
<surname>Barney</surname>
<given-names>HL</given-names>
</name>
</person-group>
<article-title>Control Methods Used in a Study of the Vowels</article-title>
<source>The Journal of the Acoustical Society of America</source>
<year>1952</year>
<volume>24</volume>
<issue>2</issue>
<fpage>175</fpage>
<lpage>184</lpage>
<pub-id pub-id-type="doi">10.1121/1.1906875</pub-id>
</element-citation>
</ref>
<ref id="R42">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Plass</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Brang</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Suzuki</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Grabowecky</surname>
<given-names>M</given-names>
</name>
</person-group>
<article-title>Vision perceptually restores auditory spectral dynamics in speech</article-title>
<source>Proceedings of the National Academy of Sciences</source>
<year>2020</year>
<volume>117</volume>
<issue>29</issue>
<fpage>16920</fpage>
<lpage>16927</lpage>
</element-citation>
</ref>
<ref id="R43">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Poeppel</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Assaneo</surname>
<given-names>MF</given-names>
</name>
</person-group>
<article-title>Speech rhythms and their neural foundations</article-title>
<source>Nature Reviews Neuroscience</source>
<year>2020</year>
<volume>21</volume>
<issue>6</issue>
<fpage>322</fpage>
<lpage>334</lpage>
<pub-id pub-id-type="doi">10.1038/s41583-020-0304-4</pub-id>
</element-citation>
</ref>
<ref id="R44">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Puschmann</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Daeglau</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Stropahl</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Mirkovic</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Rosemann</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Thiel</surname>
<given-names>CM</given-names>
</name>
<name>
<surname>Debener</surname>
<given-names>S</given-names>
</name>
</person-group>
<article-title>Hearing-impaired listeners show increased audiovisual benefit when listening to speech in noise</article-title>
<source>NeuroImage</source>
<year>2019</year>
<volume>196</volume>
<fpage>261</fpage>
<lpage>268</lpage>
<pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.017</pub-id>
</element-citation>
</ref>
<ref id="R45">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rahne</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Fröhlich</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Plontke</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Wagner</surname>
<given-names>L</given-names>
</name>
</person-group>
<article-title>Influence of face surgical and N95 face masks on speech perception and listening effort in noise</article-title>
<year>2021</year>
<comment>[Preprint]. In Review</comment>
<pub-id pub-id-type="doi">10.21203/rs.3.rs-343284/v1</pub-id>
</element-citation>
</ref>
<ref id="R46">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sanders</surname>
<given-names>LD</given-names>
</name>
<name>
<surname>Neville</surname>
<given-names>HJ</given-names>
</name>
</person-group>
<article-title>An ERP study of continuous speech processing: I Segmentation, semantics, and syntax in native speakers</article-title>
<source>Cognitive Brain Research</source>
<year>2003</year>
<volume>15</volume>
<issue>3</issue>
<fpage>228</fpage>
<lpage>240</lpage>
<pub-id pub-id-type="doi">10.1016/S0926-6410(02)00195-7</pub-id>
</element-citation>
</ref>
<ref id="R47">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sanders</surname>
<given-names>LD</given-names>
</name>
<name>
<surname>Newport</surname>
<given-names>EL</given-names>
</name>
<name>
<surname>Neville</surname>
<given-names>HJ</given-names>
</name>
</person-group>
<article-title>Segmenting nonsense: An event-related potential index of perceived onsets in continuous speech</article-title>
<source>Nature Neuroscience</source>
<year>2002</year>
<volume>5</volume>
<issue>7</issue>
<fpage>700</fpage>
<lpage>703</lpage>
<pub-id pub-id-type="doi">10.1038/nn873</pub-id>
</element-citation>
</ref>
<ref id="R48">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Schiel</surname>
<given-names>F</given-names>
</name>
</person-group>
<source>Automatic Phonetic Transcription of Non-Prompted Speech</source>
<person-group person-group-type="editor">
<name>
<surname>Ohala</surname>
<given-names>JJ</given-names>
</name>
</person-group>
<year>1999</year>
<fpage>607</fpage>
<lpage>610</lpage>
<pub-id pub-id-type="doi">10.5282/ubm/epub.13682</pub-id>
</element-citation>
</ref>
<ref id="R49">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>ZM</given-names>
</name>
<name>
<surname>Delgutte</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Oxenham</surname>
<given-names>AJ</given-names>
</name>
</person-group>
<article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title>
<source>Nature</source>
<year>2002</year>
<volume>416</volume>
<issue>6876</issue>
<fpage>87</fpage>
<lpage>90</lpage>
<pub-id pub-id-type="doi">10.1038/416087a</pub-id>
</element-citation>
</ref>
<ref id="R50">
<element-citation publication-type="book">
<person-group person-group-type="author">
<name>
<surname>Stevens</surname>
<given-names>KN</given-names>
</name>
</person-group>
<source>Acoustic Phonetics</source>
<publisher-name>MIT Press</publisher-name>
<year>2000</year>
</element-citation>
</ref>
<ref id="R51">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Suess</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Hauswald</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Reisinger</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Roesch</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Keitel</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Weisz</surname>
<given-names>N</given-names>
</name>
</person-group>
<article-title>Age-related decreases of cortical visuo-phonological transformation of unheard spectral fine-details</article-title>
<year>2021</year>
<pub-id pub-id-type="doi">10.1101/2021.04.13.439628</pub-id>
</element-citation>
</ref>
<ref id="R52">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sumby</surname>
<given-names>WH</given-names>
</name>
<name>
<surname>Pollack</surname>
<given-names>I</given-names>
</name>
</person-group>
<article-title>Visual Contribution to Speech Intelligibility in Noise</article-title>
<source>The Journal of the Acoustical Society of America</source>
<year>1954</year>
<volume>26</volume>
<issue>2</issue>
<fpage>212</fpage>
<lpage>215</lpage>
<pub-id pub-id-type="doi">10.1121/1.1907309</pub-id>
</element-citation>
</ref>
<ref id="R53">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Toscano</surname>
<given-names>JC</given-names>
</name>
<name>
<surname>Toscano</surname>
<given-names>CM</given-names>
</name>
</person-group>
<article-title>Effects of face masks on speech recognition in multi-talker babble noise</article-title>
<source>PLOS ONE</source>
<year>2021</year>
<volume>16</volume>
<issue>2</issue>
<elocation-id>e0246842</elocation-id>
<pub-id pub-id-type="doi">10.1371/journal.pone.0246842</pub-id>
</element-citation>
</ref>
<ref id="R54">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vallat</surname>
<given-names>R</given-names>
</name>
</person-group>
<article-title>Pingouin: Statistics in Python</article-title>
<source>Journal of Open Source Software</source>
<year>2018</year>
<volume>3</volume>
<issue>31</issue>
<fpage>1026</fpage>
<pub-id pub-id-type="doi">10.21105/joss.01026</pub-id>
</element-citation>
</ref>
<ref id="R55">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Willmore</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Smyth</surname>
<given-names>D</given-names>
</name>
</person-group>
<article-title>Methods for first-order kernel estimation: Simple-cell receptive fields from responses to natural scenes</article-title>
<source>Network: Computation in Neural Systems</source>
<year>2003</year>
<volume>14</volume>
<issue>3</issue>
<fpage>553</fpage>
<lpage>577</lpage>
<pub-id pub-id-type="doi">10.1088/0954-898X_14_3_309</pub-id>
</element-citation>
</ref>
<ref id="R56">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Winn</surname>
<given-names>MB</given-names>
</name>
<name>
<surname>Teece</surname>
<given-names>KH</given-names>
</name>
</person-group>
<article-title>Listening Effort Is Not the Same as Speech Intelligibility Score</article-title>
<source>Trends in Hearing</source>
<year>2021</year>
<volume>25</volume>
<elocation-id>23312165211027690</elocation-id>
<pub-id pub-id-type="doi">10.1177/23312165211027688</pub-id>
</element-citation>
</ref>
<ref id="R57">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Xu</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Gao</surname>
<given-names>H</given-names>
</name>
</person-group>
<article-title>FormantPro as a Tool for Speech Analysis and Segmentation / FormantPro como uma ferramenta para a análise e segmentação da fala</article-title>
<source>REVISTA DE ESTUDOS DA LINGUAGEM</source>
<year>2018</year>
<volume>26</volume>
<issue>4</issue>
<fpage>1435</fpage>
<lpage>1454</lpage>
<pub-id pub-id-type="doi">10.17851/2237-2083.26.4.1435-1454</pub-id>
</element-citation>
</ref>
<ref id="R58">
<element-citation publication-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yi</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Pingsterhaus</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Song</surname>
<given-names>W</given-names>
</name>
</person-group>
<article-title>The adverse effect of wearing a face mask during the COVID-19 pandemic and benefits of wearing transparent face masks and using clear speech on speech intelligibility</article-title>
<source>PsyArXiv</source>
<year>2021</year>
<pub-id pub-id-type="doi">10.31234/osf.io/z9c4k</pub-id>
</element-citation>
</ref>
</ref-list>
</back>
<floats-group>
<fig id="F1" position="float">
<label>Figure 1</label>
<caption>
<title>Experimental procedure and speech features.</title>
<p>
<bold>Figure A</bold> shows two example blocks with a male target speaker. In the block on the left the speaker did not wear a face mask across the ten trials per block. In 70% of trials the target speaker was presented solitarily, in 30% a same sex audio-only distractor speaker was added at the same volume (denoted by the second sound icon). After each of the ten trials per block, two ‘true or false’ comprehension questions were presented to the participant (italic letters underneath depict English translation). Participants answered via button press (left or right button). On the right, a block is depicted with the male speaker wearing a face mask across the ten trials of the block. Otherwise the procedure is the same as the block without a face mask. Clear speech is defined as the condition without a mask and without a distractor speaker. The two depicted blocks were repeated with a female speaker, resulting in a total of four blocks. <bold>Figure B</bold> shows the speech features investigated. Formants (F1 - F3) are shown in red overlaid on the speech spectrogram. Segmentation in phonemes and words (top row: orthographic word; mid row: phonetic word; bottom row: phoneme) was done using forced alignment. This segmentation can be seen on the bottom of the spectrogram. The speech envelope can be seen on the bottom left of the figure. On the bottom right of the figure, the speaker's pitch or fundamental frequency (F0) is depicted. All depictions are based on the same two-second long speech interval.</p>
<p>*<underline>images have been removed/obscured due to a bioRxiv policy on the inclusion of faces</underline>
</p>
</caption>
<graphic xlink:href="EMS136061-f001"/>
</fig>
<fig id="F2" position="float">
<label>Figure 2</label>
<caption>
<title>Descriptive depiction of stimulus reconstruction accuracies.</title>
<p>
<bold>A</bold> Two example stimulus reconstructions of the speech envelope and the averaged F2 and F3 (Formant 2/3, F2/3) for one participant, stimulated with clear audiovisual speech (i.e. stimuli with no mask and no distractor). <bold>B</bold> Mean stimulus reconstruction accuracy for clear audiovisual speech (i.e. stimuli with no mask and no distractor) across participants. Error Bars denote 95% confidence interval.</p>
</caption>
<graphic xlink:href="EMS136061-f002"/>
</fig>
<fig id="F3" position="float">
<label>Figure 3</label>
<caption>
<title>Depiction of the effects of face masks on several speech characteristics.</title>
<p>
<bold>A</bold> Graphical depiction of the effect size for the main effect of factor Mask. Asterisks denote the significance of the effect of the face mask regarding each characteristic. <bold>B</bold> Graphical depiction of the effect size of the interaction of the factors Mask and Distractor. Asterisks denote the significance of interaction. <bold>C</bold> Depiction of the effects for the speech features speech envelope, averaged F2 and F3 (Formant 2/3, F2/3), and phoneme and word onsets split up for the effects of the face mask and the distractor. Error bars show 95% CI. Asterisks denote the significance of simple effect comparison tests. n.s.: p &gt; .1, ° : p &lt; .1, *: p &lt; .05, **: p &lt; .01, *** : p &lt; .001</p>
</caption>
<graphic xlink:href="EMS136061-f003"/>
</fig>
</floats-group>
</article>
