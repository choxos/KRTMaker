<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190467</article-id><article-id pub-id-type="doi">10.1101/2023.10.30.564638</article-id><article-id pub-id-type="archive">PPR750653</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Neural manifolds carry reactivation of phonetic representations during semantic processing</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Orepic</surname><given-names>Pavo</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Truccolo</surname><given-names>Wilson</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Halgren</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Cash</surname><given-names>Sydney S.</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Giraud</surname><given-names>Anne-Lise</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Proix</surname><given-names>Timotheé</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Basic Neurosciences, Faculty of Medicine, University of Geneva, Geneva, Switzerland</aff><aff id="A2"><label>2</label>Department of Neuroscience, Brown University, Providence, Rhode Island, United States of America</aff><aff id="A3"><label>3</label>Carney Institute for Brain Science, Brown University, Providence, Rhode Island, United States of America</aff><aff id="A4"><label>4</label>Department of Neuroscience &amp; Radiology, University of California San Diego, La Jolla, California, United States of America</aff><aff id="A5"><label>5</label>Department of Neurology, Massachusetts General Hospital, Harvard Medical School, Boston, Massachusetts, United States of America</aff><aff id="A6"><label>6</label>Institut Pasteur, Université Paris Cité, Hearing Institute, Paris, France</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author(s). <email>timothee.proix@unige.ch</email>;</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>31</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Traditional models of speech perception posit that neural activity encodes speech through a hierarchy of cognitive processes, from low-level representations of acoustic and phonetic features to high-level semantic encoding. Yet it remains unknown how neural representations are transformed across levels of the speech hierarchy. Here, we analyzed unique microelectrode array recordings of neuronal spiking activity from the human left anterior superior temporal gyrus, a brain region at the interface between phonetic and semantic speech processing, during a semantic categorization task and natural speech perception. We identified distinct neural manifolds for semantic and phonetic features, with a functional separation of the corresponding low-dimensional trajectories. Moreover, phonetic and semantic representations were encoded concurrently and reflected in power increases in the beta and low-gamma local field potentials, suggesting top-down predictive and bottom-up cumulative processes. Our results are the first to demonstrate mechanisms for hierarchical speech transformations that are specific to neuronal population dynamics.</p></abstract><kwd-group><kwd>speech perception</kwd><kwd>neural manifold</kwd><kwd>neuronal population dynamics</kwd><kwd>phonetic</kwd><kwd>semantic</kwd><kwd>neural oscillations</kwd><kwd>analysis-by-synthesis</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">How does the brain transform sounds into meanings? Most cognitive models of speech perception propose that speech sounds are processed sequentially by distinct neural modules. Semantic and conceptual representations emerge at the end of the ventral stream (i.e. the "what" stream) (<xref ref-type="bibr" rid="R36">Hickok and Poeppel, 2007</xref>), where perceived speech is sequentially transformed from spectro-temporal encoding of sounds in Heschl's gyrus, over phonetic features in the superior temporal gyrus (STG) and sulcus, to lexical and combinatorial semantics in the anterior temporal lobe (ATL) (<xref ref-type="bibr" rid="R55">Mesgarani et al, 2014</xref>; <xref ref-type="bibr" rid="R66">Pylkkänen, 2020</xref>). For instance, the ATL is considered a specialized semantic module, connected to modality-specific sources of information (<xref ref-type="bibr" rid="R61">Patterson et al, 2007</xref>; <xref ref-type="bibr" rid="R69">Ralph et al, 2017</xref>), with causal semantic impairments occurring after bilateral ATL atrophy or damage due to viral infection (<xref ref-type="bibr" rid="R57">Noppeney et al, 2006</xref>; <xref ref-type="bibr" rid="R45">Lambon Ralph et al, 2006</xref>; <xref ref-type="bibr" rid="R72">Schwartz et al, 2009</xref>; <xref ref-type="bibr" rid="R18">Cope et al, 2020</xref>). This modular and sequential perspective implies that a complex neural process transforms speech features from one functional brain region to the next up to lexical and multimodal conceptual representations.</p><p id="P3">Recent neuroimaging findings suggest, however, that these transformations across brain regions might be less modular and sequential than predicted by the classical hierarchical view of language processing (<xref ref-type="bibr" rid="R69">Ralph et al, 2017</xref>; <xref ref-type="bibr" rid="R80">Yi et al, 2019</xref>; <xref ref-type="bibr" rid="R10">Caucheteux et al, 2022</xref>). Fine-grained electrocorticography (ECoG) recordings from the middle and posterior STG reveal that phonetic features are encoded without strict spatial segregation, but rather via mixed interleaved representations (<xref ref-type="bibr" rid="R34">Hamilton et al, 2021</xref>). At the semantic level, the anterior superior temporal gyrus (aSTG), as a part of the ATL, responds more specifically to semantic decisions from heard speech, without a clear anatomo-functional separation from the middle STG (<xref ref-type="bibr" rid="R73">Scott, 2000</xref>; <xref ref-type="bibr" rid="R78">Visser and Lambon Ralph, 2011</xref>; <xref ref-type="bibr" rid="R13">Chang et al, 2015</xref>; <xref ref-type="bibr" rid="R81">Zhang et al, 2021</xref>; <xref ref-type="bibr" rid="R21">Damera et al, 2023</xref>). In functional MRI (fMRI) data, neural activity recorded through incrementally higher cognitive brain regions correlates with increasingly deeper layers in large language models (<xref ref-type="bibr" rid="R10">Caucheteux et al, 2022</xref>). These findings suggest that some brain regions represent multiple speech features, making them suitable candidates for housing transformations across the speech hierarchy.</p><p id="P4">Studies using time-resolved recording techniques such as EEG, MEG, or intracranial EEG, additionally showed simultaneous encoding of features across the speech hierarchy, organized in increasingly larger time scales (<xref ref-type="bibr" rid="R35">Heilbron et al, 2022</xref>; <xref ref-type="bibr" rid="R32">Gwilliams and King, 2020</xref>; <xref ref-type="bibr" rid="R40">Keshishian et al, 2023</xref>). While phonetic features are short-lived and typically processed around 100-200 ms in the STG (<xref ref-type="bibr" rid="R55">Mesgarani et al, 2014</xref>; <xref ref-type="bibr" rid="R34">Hamilton et al, 2021</xref>), semantic processing, e.g. semantic composition and lexical decisions, is associated with a longer-lasting compound event at 400 ms as reflected by the N400 component (<xref ref-type="bibr" rid="R43">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="R25">Dikker et al, 2020</xref>). Yet, syntactic and semantic composition activity can be detected in the ATL as early as 200-250 ms (<xref ref-type="bibr" rid="R66">Pylkkänen, 2020</xref>; <xref ref-type="bibr" rid="R29">Friederici and Kotz, 2003</xref>), suggesting an early use of phonetic features to access meaning, a process that has so far not been described.</p><p id="P5">Such early transformations are envisaged in the analysis-by-synthesis framework, which posits that speech comprehension results from a series of sequential loops (<xref ref-type="bibr" rid="R33">Halle and Stevens, 1959</xref>; <xref ref-type="bibr" rid="R7">Bever and Poeppel, 2010</xref>): incoming speech inputs are first sequentially processed, e.g. at the phonological level, and combined into a first semantic guess based on prior knowledge and context. A word prior elicited by this semantic representation is then generated and directly compared to the actual acoustic input. Depending on the amount of error thus generated, the prior is either accepted or rejected in favor of newly updated hypotheses. These hierarchical interactions are typically reflected in the power of different local field potential (LFP) frequency bands: bottom-up processes have been associated with the low-gamma and theta bands, and top-down ones with the beta band (<xref ref-type="bibr" rid="R2">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="R28">Fontolan et al, 2014</xref>; <xref ref-type="bibr" rid="R76">van Kerkoerle et al, 2014</xref>; <xref ref-type="bibr" rid="R56">Michalareas et al, 2016</xref>; <xref ref-type="bibr" rid="R14">Chao et al, 2018</xref>; <xref ref-type="bibr" rid="R31">Giraud and Arnal, 2018</xref>).</p><p id="P6">To identify the fine-grained mechanisms underlying speech neural transformations, it might be necessary to investigate speech processing at a smaller spatial scale giving access to the neuronal spiking level. Recent findings suggest that complex cognitive processes and behavioral features are encoded in low-dimensional neuronal spaces, also called <italic>neural manifolds</italic> (<xref ref-type="bibr" rid="R64">Pillai and Jirsa, 2017</xref>; <xref ref-type="bibr" rid="R30">Gallego et al, 2017</xref>; <xref ref-type="bibr" rid="R39">Jazayeri and Ostojic, 2021</xref>; <xref ref-type="bibr" rid="R79">Vyas et al, 2020</xref>; <xref ref-type="bibr" rid="R16">Chung and Abbott, 2021</xref>; <xref ref-type="bibr" rid="R75">Truccolo, 2016</xref>; <xref ref-type="bibr" rid="R1">Aghagolzadeh and Truccolo, 2016</xref>). In the primate cortex, the dynamics on the neural manifolds characterize functionally distinct behaviors and conditions, such as sensorimotor computations, decision making, or working memory (<xref ref-type="bibr" rid="R50">Mante et al, 2013</xref>; <xref ref-type="bibr" rid="R70">Remington et al, 2018</xref>; <xref ref-type="bibr" rid="R52">Markowitz et al, 2015</xref>). In humans, this endeavor is largely hindered by the invasiveness of single-neuron recordings, which are rarely performed. Although speech encoding has never been characterized at the level of collective dynamics of action potentials recorded from neuronal ensembles, it seems plausible that different aspects of speech, including transformations across the speech hierarchy, are encoded in such low-dimensional manifold representations. The condition- and history-dependent organization of neuronal trajectories on the manifold could serve the purpose of integrating phonetic features into higher-level representations (<xref ref-type="bibr" rid="R65">Pulvermüller, 2018</xref>; <xref ref-type="bibr" rid="R80">Yi et al, 2019</xref>). The handful of studies that have so far reported single unit activity associated with speech processing have highlighted the tuning of one or a few single units to phonetic features (<xref ref-type="bibr" rid="R12">Chan et al, 2014</xref>, <xref ref-type="bibr" rid="R43">2011</xref>; <xref ref-type="bibr" rid="R60">Ossmy et al, 2015</xref>; <xref ref-type="bibr" rid="R44">Lakretz et al, 2021</xref>). By contrast, we focused here on the collective dynamics of tens of neurons in the light of the neural manifold framework to address the mechanisms underlying phonetic-to-semantic transformations.</p><p id="P7">We used unique recordings coming from a microelectrode array (MEA) implanted in the left anterior STG of a patient with pharmacologically resistant epilepsy, while he performed an auditory semantic categorization task and engaged in a spontaneous natural conversation (<xref ref-type="bibr" rid="R12">Chan et al, 2014</xref>). The MEA was placed at the intersection of areas traditionally associated with the processing of phonetic and semantic information, while an ECoG grid simultaneously recorded LFP activity along the temporal lobe. Despite the absence of detectable power effects on the most proximal ECoG channels, we observed a distributed encoding of phonemes at the local neuronal population scale. The dynamics of individual phoneme trajectories were organized according to their corresponding phonetic features only at specific time periods. Critically, the same phonetic organization generalized to natural speech, with different speakers and a variety of complex linguistic and predictive processes. During natural speech processing, population encoding of phonetic features occurred concurrently with the encoding of semantic features, both culminating at about 400 ms after word onset. This was reflected in the peaks in low-gamma and beta power, in agreement with the analysis-by-synthesis framework (<xref ref-type="bibr" rid="R7">Bever and Poeppel, 2010</xref>). These findings suggest that phonetic features interact with semantic representations by the simultaneous instantiation of predictive bottom-up and top-down mechanisms.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Semantic and phonetic neuronal encodings in the aSTG</title><p id="P8">Microelectrode array and ECoG signals were recorded in a 31-year-old patient with pharmacologically resistant epilepsy who was implanted for clinical purposes (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). The 10x10 MEA was located in the left anterior superior temporal gyrus (aSTG) (square on <xref ref-type="fig" rid="F1">Fig. 1a</xref>). 23 ECoG electrodes of interest covered a large portion of the left temporal cortical surface (circles on <xref ref-type="fig" rid="F1">Fig. 1a</xref>). The participant performed first an auditory semantic categorization task, and then engaged in a spontaneous conversation. In the semantic task, the participant was instructed to indicate, by button press, whether the heard word was smaller or bigger than a foot (<xref ref-type="fig" rid="F1">Fig 1b</xref>). 400 unique nouns were presented, half of which indicated objects (e.g. chair), and the other half animals or body parts (e.g. donkey, eyebrow). In both groups (object, animal), words were equally divided in two categories, either bigger or smaller than a foot, resulting in a balanced 2-by-2 design.</p><p id="P9">We used multivariable temporal response function (mTRF) models to contrast the encoding of different linguistic processes and speech features (see <xref ref-type="sec" rid="S8">Methods</xref>). To obtain an initial list of relevant linguistic processes for the mTRF analysis, we also used a large-scale fMRI database to specify the cognitive processes associated with the exact location in the aSTG where the intracortical MEA was positioned (<xref ref-type="bibr" rid="R26">Dockès et al, 2020</xref>). Nearby brain regions are indicated to process both phonetics (middle STG) (<xref ref-type="fig" rid="F1">Fig. 1c</xref>) and semantic categorization (anterior superior temporal sulcus) (<xref ref-type="fig" rid="F1">Fig. 1d</xref>).</p><p id="P10">Next, for each word, we identified the following speech features (<xref ref-type="fig" rid="F1">Fig 1e</xref>): (i) acoustic, including word onset and acoustic edges (envelope rate peaks) (<xref ref-type="bibr" rid="R59">Oganian and Chang, 2019</xref>); (ii) phonemic, including phoneme onset and each phoneme identity; (iii) phonetic, including features based on vowel first and second formants, and consonant manner and place of articulation; (iv) semantic, including the word's conceptual category (object vs. animal), perceptual category (bigger or smaller than a foot), and semantic decision (participant's response about whether the object was bigger or smaller than a foot) (<xref ref-type="supplementary-material" rid="SD1">Supplementary tables 1, 2, and 3</xref>). The semantic decision feature was regressed separately from the perceptual category feature based on the fact that the participant only responded correctly in 80.25% of the trials. We constructed a baseline model based on acoustic features, and for each feature of interest, we compared the baseline model performance with the performance of a mTRF model containing the feature of interest together with the baseline features. The model performance was computed as Pearson's correlation between the model prediction and the neuronal activity in a 5-fold nested cross-validation procedure (<xref ref-type="sec" rid="S8">Methods</xref>).</p><p id="P11">We fitted the mTRF encoding models to the soft-normalized firing rate of single units recorded with the MEA (<xref ref-type="sec" rid="S8">Methods</xref>). We considered the 23 spike-sorted single units (over a total of 176) with an average firing rate higher than 0.3 spikes/second (mean: 1.6, sd: 1.39, range 0.43 - 5.27 spikes/second, <xref ref-type="sec" rid="S8">Methods</xref>). Only a few units significantly responded to different features. Eight of them correlated significantly to phoneme onsets (p &lt; 0.05 based on the chance level performance of a surrogate distribution, see <xref ref-type="sec" rid="S8">Methods</xref>), one to phonetic features, one to conceptual categories, and two to perceptual categories (<xref ref-type="fig" rid="F1">Fig. 1f</xref>).</p><p id="P12">By contrast, the broadband high-frequency activity (BHA, as a proxy for local firing rate activity (<xref ref-type="bibr" rid="R19">Crone et al, 2001</xref>)) of the ECoG electrode in the immediate vicinity of the MEA did not correlate significantly to any probed features. Six other ECoG electrodes significantly correlated to phoneme onset (<xref ref-type="fig" rid="F1">Fig. 1g</xref>), two for phonetic features (<xref ref-type="fig" rid="F1">Fig. 1i</xref>) in the posterior middle and superior temporal gyrus, and two for semantic decision in the aTL (<xref ref-type="fig" rid="F1">Fig. 1j</xref>). No other models showed significant correlations (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 1</xref>).</p><p id="P13">Our next goal was to further explore the encoding of phonetic features. To this aim, we separated them into four different groups (vowel first formant, vowel second formant, consonant manner of articulation, and consonant place of articulation), and contrasted a model for each phonetic feature group with a base model including only phoneme onsets and acoustic features. Two units showed a significant spiking increase for consonant manner and vowel first formant models, and none for the other two models (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2</xref>). None of the individual phonetic groups were distinguished by the ECoG electrode adjacent to the MEA (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 2</xref>).</p></sec><sec id="S4"><title>Distributed phonetic encoding</title><p id="P14">We showed that only a few single units, when taken independently, encoded phonetic features. However, several other units showed non-significant changes (permutation test, <xref ref-type="fig" rid="F1">Fig. 1f</xref>), which nevertheless might have contributed to the overall encoding through neuronal population dynamics. The time course of the phoneme kernels averaged across all units compared to the distribution of surrogate (chance-level) kernels showed two significant periods, centered around 200 and 400 ms post phoneme onset, suggesting a mean-field population effect (<xref ref-type="fig" rid="F2">Fig. 2a</xref>). We thus hypothesized that a read-out of phonetic encoding emerged at the level of the neuronal population dynamics, and that these dynamics could be captured by a low-dimensional neural manifold.</p><p id="P15">To address this hypothesis, we performed principal component analysis (PCA) on all concatenated phoneme kernels obtained with mTRF (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). The first four principal components (PCs) accounted for about 50% of phoneme feature variance (<xref ref-type="fig" rid="F2">Fig 2c</xref>), and were distributed across different units (<xref ref-type="fig" rid="F2">Fig 2d</xref>). Having half of the variance explained by the low-dimensional and correlated activity of several units indicates that phonemes are dynamically represented within a manifold.</p><p id="P16">We then asked whether the obtained low-dimensional neural manifold carried a functional relevance for phonetic encoding. For this, we analyzed the time course of phoneme kernels projected to the neural manifold (<xref ref-type="fig" rid="F2">Fig 2d</xref>) and investigated whether the phonemes grouped according to phonetic categories. Thus, for each group of phonetic features (vowel first formant, vowel second formant, consonant manner, consonant place), we clustered the corresponding phoneme kernel trajectories at each time point in the low-dimensional space spanned by the first two PCs. Then, we computed a clustering index as the difference of between- and intra-cluster distances (<xref ref-type="fig" rid="F2">Fig 2e</xref>) and compared it against the surrogate distribution (<xref ref-type="sec" rid="S8">Methods</xref>).</p><p id="P17">We observed significant clustering within the vowel first formant group between the two phonetic features that reflect the high and low position of the tongue (<xref ref-type="fig" rid="F2">Fig. 2f</xref>). The clustering specifically occurred at about 200 ms, which was apparent in the position of the vowels in the two-dimensional PC space (<xref ref-type="fig" rid="F2">Fig. 2g</xref>). We replicated these findings using higher dimensional PC spaces (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3</xref>). Contrary to the vowel first formant group, we observed no significant clustering between phonetic features of the vowel second formant group (front and back position of the tongue, <xref ref-type="fig" rid="F2">Fig. 2h</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3 and 4</xref>). These findings indicate selective clustering of phoneme kernels based on vowel first formant at 200 ms.</p><p id="P18">We next investigated the organization of consonant trajectories in the low-dimensional space. The clustering index based on the manner of articulation (plosive, nasal, fricative, approximant, and lateral approximant features) indicated two significant periods centered around 200 ms and 400 ms (<xref ref-type="fig" rid="F2">Fig. 2h</xref>). The same two peaks persisted irrespective of the number of PCs selected (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3</xref>). There was no observable clustering along the consonant place of articulation (bilabial, labiodental, dental, alveolar, velar, uvular, and glottal features) in any low-dimensional space (<xref ref-type="fig" rid="F2">Fig 2l</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 3</xref>).</p><p id="P19">We replicated the observed clustering by vowels first formant and consonant manner of articulation, as well as the lack of clustering by vowel second formant and consonant place of articulation with additional control analyses. Linear discriminant analysis (LDA) classifier revealed separability of these two phonetic categories in the same time windows (i.e. at about 200 ms for vowels first formant and both at about 200 and 400 ms for consonants manner of articulation) (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 5</xref>). Similarly, a rank-regression approach, where the ranks indicate the first or second formant value, showed significant ordering of the vowels along the first formant at 200 ms, and no ordering along the second formant (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 6</xref>). (This analysis cannot be performed for consonants, where no ranking is possible across the different phonetic groups). Finally, we observed similar organizational patterns through k-means clustering, an unsupervised data-driven approach (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 7</xref>).</p><p id="P20">To summarize, we observed a significant clustering of phoneme trajectories on a low-dimensional neural manifold: vowel trajectories clustered along first formants at 200 ms, while consonant trajectories clustered by manner of articulation at 200 and 400 ms.</p><p id="P21">To investigate whether the results generalized to natural speech perception, we analyzed neuronal recordings from the same intracortical MEA during a spontaneous conversation between the participant and another person recorded in a separate experimental session. As for the semantic task, we performed spike sorting and selected the 23 units with the highest spiking rate (mean: 0.58 spikes/second, sd: 0.89, range 0.1 - 3.39) out of 212 clustered single units. We identified 664 words (272 unique) pronounced by the other person in the recording. From those words, we segmented the same 32 phonemes as in the task. We designed two high-level features, namely word class (e.g. noun, adjective, conjunction, etc), and lexical semantics, computed as the Lancaster sensorimotor norms of each word (<xref ref-type="bibr" rid="R49">Lynott et al, 2019</xref>) (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 4 and 5</xref> for an overview). We then fitted the mTRF encoding models as in the semantic task dataset, and likewise observed that only a few units significantly correlated with speech features (<xref ref-type="fig" rid="F3">Fig. 3a</xref>), and that the average phoneme kernel became significant compared to its surrogate distribution at about 200 and 400 ms, suggesting again a population effect (<xref ref-type="fig" rid="F3">Fig. 3b</xref>).</p><p id="P22">We thus proceeded as before by performing PCA on the kernels. Similar to the task, 50% of the variance was explained by a few PCs (<xref ref-type="fig" rid="F3">Fig. 3c</xref>), and the processing of phonemes was distributed across units (<xref ref-type="fig" rid="F3">Fig. 3d</xref>). We further computed the clustering index for phonetic features in the low-dimensional neural manifold spanned by the first two PCs. Remarkably, the clustering index profiles were similar to those in the semantic task. For the vowel first formants, we observed a peak of the clustering index at about 200 ms (<xref ref-type="fig" rid="F3">Fig. 3e</xref>). Although this peak was not significant for the two-dimensional space, it was present for all dimensions, and reached significance for the one-dimensional space (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 8</xref>). This peak was also significant in all control analyses (LDA classifier, rank regression, and k-means, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9-11</xref>). For vowel second formants, a trend consistent across dimensions was observed at 200 ms (<xref ref-type="fig" rid="F3">Fig. 3f</xref>). The second formant phonetic features also clustered before the phoneme onset, specifically at -100 ms, possibly reflecting prediction mechanisms present in natural speech perception but not during the semantic task. For consonant manner of articulation, we observed significant clustering indices at about 400 ms, as well as an additional peak at about -100 ms that was possibly related to prediction mechanisms (<xref ref-type="fig" rid="F3">Fig. 3g</xref>). The peak at 200 ms that was observed in the semantic task did not occur here. Finally, for consonant place of articulation, no significant peak of clustering index was observed (<xref ref-type="fig" rid="F3">Fig. 3h</xref>). We performed the same control analyses as before (higher dimensional PC spaces, LDA classifier, rank regression, and k-means clustering), and replicated all the findings (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 9-11</xref>). The timeline on <xref ref-type="fig" rid="F3">Fig. 3i</xref> summarizes the significant clustering peaks across both datasets.</p><p id="P23">Finally, we compared the similarity of low-dimensional phoneme trajectories obtained in the semantic task versus natural speech. To that aim, we performed canonical correlation analysis (CCA) between the projected phoneme kernels of the two datasets, and compared it against the surrogate distribution of canonical correlations obtained by shuffling the kernels for natural speech. We observed a significant canonical correlation between the individual phoneme kernels for the first nine PC dimensions (<xref ref-type="fig" rid="F3">Fig 3j</xref>). This shows that although the phonemes in the semantic task and natural speech perception are encoded by different units (as both recordings are separated by a few hours), the neuronal population dynamics are encoded similarly, with individual phonemes tracing highly similar trajectories in the low-dimensional neural manifold.</p></sec><sec id="S5"><title>Encoding of semantic features</title><p id="P24">Considering that the MEA is implanted in a cortical area involved in auditory semantic processing (<xref ref-type="bibr" rid="R69">Ralph et al, 2017</xref>), we repeated the same analyses for the semantic kernels, both for the semantic task and natural speech perception. We first considered the time course of the semantic kernels averaged across all units, and compared it to the surrogate distribution (<xref ref-type="fig" rid="F4">Fig 4a</xref>). The perceptual category kernel showed a significant period at about 400 ms after word onset, while other kernels were not significant. We also repeated the same analysis for natural speech perception. The word class kernel (nouns, verbs, etc.) averaged across units showed a significant period at 150 ms (<xref ref-type="fig" rid="F4">Fig 4b</xref>). The lexical semantic kernel averaged across units also showed a significant activation at 400 ms, consistent with our findings on the semantic task.</p><p id="P">We then turned to the neuronal population dynamics. We projected the three semantic kernels of the semantic task to the low-dimensional neural manifold. Because there are only two categories for each of the probed semantic features, we could not perform clustering analysis as for the phonemic features. Instead, at each time point, we computed the Euclidean distance between the projected trajectories and compared the resulting distance against its surrogate distribution (<xref ref-type="fig" rid="F4">Fig 4c</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 12</xref>). We observed a significant separation of perceptual category kernels (bigger vs. smaller) at 400 ms. Interestingly, we also observed a significant separation for the conceptual category kernels (object vs. animal) at about 450 ms, even though the averaged conceptual category kernel was never significant, highlighting that the relevant functional read-out of certain aspects of semantic processing might be emerging only at the level of the neuronal population dynamics. Finally, the Euclidean distance for semantic decision was not significant. For natural speech perception, projecting those kernels to the low-dimensional neural manifold (<xref ref-type="fig" rid="F4">Fig 4d</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 13</xref>), we found a strong separation between word classes at 150-200 ms, possibly reflecting syntactic processing, and a significant separation between lexical semantics kernels at about 400 ms.</p></sec><sec id="S6"><title>Concurrent encoding of bottom-up phonetic and top-down semantic features</title><p id="P25">To further investigate the mechanisms underlying the integration of phonetic and semantic features, we explored the timing of their representations in more detail. Inspired by the analysis-by-synthesis framework, we hypothesized that an early bottom-up phonetic processing enables a later top-down word-level semantic guess, which is compared to actual phonetic features (<xref ref-type="bibr" rid="R33">Halle and Stevens, 1959</xref>; <xref ref-type="bibr" rid="R7">Bever and Poeppel, 2010</xref>). For the late semantic-phonetic comparison to occur, phonetic features should be (re)encoded concurrently with semantic processing at about 400 ms after word onset. To challenge this hypothesis, we created additional mTRF features that regressed phoneme onsets at different positions within each word (i.e., phoneme order). We then aligned these position-based phoneme onset kernels with word onset by shifting each phoneme onset for the corresponding multiple of the average phoneme duration (80 ms). If the hypothesis is true, a significant period of activation should align across the shifted kernels. While no significant peaks for the distinct phoneme onsets were observed in the semantic task (<xref ref-type="supplementary-material" rid="SD1">Supplementary Fig. 14</xref>), we found an alignment of significant peaks for phoneme onsets at positions 1-5 at about 400 ms during natural speech perception, simultaneously to semantic encoding (<xref ref-type="fig" rid="F5">Fig. 5a</xref>). The analysis-by-synthesis framework might thus hold true for natural speech perception, where more semantic top-down processing is expected, and where there are no predictability biases related to the task design, such as fixed stimulus onset timing.</p><p id="P26">To further test the compatibility with the analysis-by-synthesis framework, we asked whether the encoding reflected top-down predictive or bottom-up cumulative processes during natural speech perception. The occurrence of top-down processes is typically accompanied by beta peaks in the LFP power, while bottom-up processes are reflected in LFP low-gamma power peaks (<xref ref-type="bibr" rid="R2">Arnal and Giraud, 2012</xref>; <xref ref-type="bibr" rid="R28">Fontolan et al, 2014</xref>). To identify the timing of top-down and bottom-up processes, we thus extracted and averaged the LFP power across the MEA contacts for both the beta (12-30 Hz) and low-gamma (30-70 Hz) bands. We then fitted an mTRF model to these two variables using word onset features for the beta band (reflecting word-level top-down processing) and position-based phoneme onsets for the low-gamma band (reflecting position-dependent bottom-up processing). For the beta band, we found two significant positive peaks: at word onset and 400 ms after word onset (<xref ref-type="fig" rid="F5">Fig. 5b</xref>, brown trace). In the low-gamma band, we aligned position-based phoneme onset kernels based on word onset as before, and observed negative and positive significant peaks before 400 ms in the low-gamma band that were not accompanied by a beta peak, suggesting early bottom-up processes. We also found an alignment of positive significant peaks at 400 ms after word onset (<xref ref-type="fig" rid="F5">Fig. 5b</xref>, green traces), showing that both top-down semantic and bottom-up phonetic processing occurred at 400 ms. Following the analysis-by-synthesis framework, early low-gamma peaks might reflect early bottom-up-only processes that contribute to information build-up, possibly also observed in the early significant negative peaks of the first and second phoneme at the neuronal level (<xref ref-type="fig" rid="F5">Fig. 5a</xref>). These bottom-up processes then lead to the 400-ms concurrent beta top-down and low-gamma bottom-up peaks, simultaneously to the significant positive neuronal peaks at 400 ms (compare <xref ref-type="fig" rid="F5">Fig. 5a and b</xref>).</p><p id="P27">As an additional confirmation of the 400 ms top-down process, we extracted the broadband high-frequency activity (BHA, 70-150 Hz) across the MEA, as it has recently been shown to originate from both local neuronal firing and feedback predictive information (<xref ref-type="bibr" rid="R47">Leszczyński et al, 2020</xref>). We found a significant peak of the BHA kernel at about 400 ms (<xref ref-type="fig" rid="F5">Fig. 5c</xref>). By contrast, the firing rate kernel aligned to word onset did not show any late significant peaks. This finding further confirms that top-down processes occurred at about 400 ms, as suggested by the corresponding beta peak (<xref ref-type="fig" rid="F5">Fig. 5b</xref>).</p><p id="P28">Finally, we investigated whether the phonetic low-dimensional neuronal dynamics were time causally predictive of the semantic low-dimensional dynamics (reflecting a bottom-up process) or, inversely, whether the semantic trajectories predicted the phonetic ones (for a top-down process). For this, we projected the neuronal firing rate during natural speech to the first three dimensions of the phonetic and semantic PC spaces and investigated Granger causality between the projected time series. We found significant effects in both directions, indicating a bidirectional causal relationship between low-dimensional semantic and phonetic processing (<xref ref-type="fig" rid="F5">Fig. 5d</xref>), which again supports the analysis-by-synthesis framework.</p><p id="P29">Together, these findings suggest that neuronal population dynamics in the aSTG encode phonetic and semantic features through concurrent bottom-up and top-down processes.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P30">Using both an auditory semantic categorization task and natural speech perception, we showed that the low-dimensional neuronal population dynamics recorded in the aSTG during speech processing concurrently encode phonetic and semantic features. We identified a neural manifold for both feature groups and observed a functional separation of their corresponding trajectories across time. Specifically, phoneme trajectories consistently clustered according to phonetic features and were highly correlated across task and natural speech conditions, suggesting invariant representations across different contexts. Similarly, semantic trajectories separated along their perceptual and conceptual features during the semantic task, and along lexical semantic and word class features during natural speech. During natural speech, semantic and phonetic encoding occurred in parallel at 400 ms after word onset with a bidirectional causal relationship between their low-dimensional representations. Phonetic encoding occurred significantly earlier for successive phoneme positions within words, such that processing of all phonemes culminated simultaneously at 400 ms after word onset. In agreement with the analysis-by-synthesis framework, this parallel encoding of phonemes and semantics was mirrored in concurrent bottom-up and top-down processes as measured by peaks in the low-gamma and beta power following early bottom-up-only processes.</p><p id="P100">A large amount of the neuronal variance related to phonetic and semantic encoding was accounted for by low-dimensional neural manifolds. Low-dimensional phoneme trajectories clustered according to phonetic (e.g. the first formant for vowels and the manner of articulation for consonants) and semantic features (e.g. perceptual and lexical semantics), denoting a large representational flexibility. These findings experimentally support the existence of neural manifolds for neural speech processing, extending recent findings in other cognitive domains, such as sensorimotor processing, decision making, or object recognition (<xref ref-type="bibr" rid="R30">Gallego et al, 2017</xref>; <xref ref-type="bibr" rid="R50">Mante et al, 2013</xref>; <xref ref-type="bibr" rid="R24">DiCarlo and Cox, 2007</xref>). We observed that latent dynamics on the neural manifold constitute a robust functional read-out for speech processing. Specifically, the encoding of speech features became more prominent when considering the coordinated dynamics of the PCs, as opposed to the kernel activity simply averaged across units. This might account for the lack of detectable phonetic and semantic encoding at the same aSTG location in the ECoG signals, reflecting the average firing rate over a large population of neurons (<xref ref-type="bibr" rid="R47">Leszczyński et al, 2020</xref>). Further, we found that the low-dimensional phoneme features traced highly correlated trajectories across two conditions (semantic task and natural speech) up to the 9th PC (<xref ref-type="fig" rid="F3">Fig. 3j</xref>). This is remarkable considering that the two conditions were separated by more than two hours and that the spike sorting procedure identified different units on the same array. This shows that despite the difference in the underlying units, common population patterns remain preserved, suggesting that the functional read-out emerges at the level of the neuronal population dynamics (<xref ref-type="bibr" rid="R71">Rutten et al, 2019</xref>; <xref ref-type="bibr" rid="R79">Vyas et al, 2020</xref>; <xref ref-type="bibr" rid="R39">Jazayeri and Ostojic, 2021</xref>; <xref ref-type="bibr" rid="R16">Chung and Abbott, 2021</xref>).</p><p id="P31">Encoding of phonetic and semantic features overlapped both in space and time. Both were encoded in the same focal cortical area of the aSTG, which contrasts with the modular partitioning of phonetic and semantic encoding traditionally observed throughout the temporal lobe (<xref ref-type="bibr" rid="R36">Hickok and Poeppel, 2007</xref>). Temporally, the processing of phonemes aligned to word onset concurred with the semantic processing window at about 400 ms, with a bidirectional causal relationship (<xref ref-type="fig" rid="F5">Fig. 5</xref>). This advocates for the analysis-by-synthesis framework, where the distinct levels are related through dynamic bottom-up and top-down predictive loops (<xref ref-type="bibr" rid="R33">Halle and Stevens, 1959</xref>; <xref ref-type="bibr" rid="R7">Bever and Poeppel, 2010</xref>). Following this approach, we found significant alignments of top-down and bottom-up processes, as indicated by transient increases in beta and low-gamma power at about 400 ms post-word-onset (<xref ref-type="fig" rid="F5">Fig. 5b</xref>). The beta peak might indicate a top-down semantic guess which is compared to a reinstantiation of bottom-up phoneme representations highlighted by concurrent low-gamma peaks. This top-down semantic guess could result from a cumulative build-up of bottom-up phonetic processes reflected by low-gamma peaks occurring before the 400-ms alignment, in agreement with most models of spoken-word recognition (<xref ref-type="bibr" rid="R53">Marslen-Wilson, 1987</xref>; <xref ref-type="bibr" rid="R58">Norris and McQueen, 2008</xref>). The specific mechanisms underlying the emergence of such phonemic-to-semantic interface remain to be uncovered. For example, distinct phonetic features could be represented either sequentially (<xref ref-type="bibr" rid="R36">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="R23">Dehaene et al, 2015</xref>) or persistently (<xref ref-type="bibr" rid="R62">Perdikis et al, 2011</xref>; <xref ref-type="bibr" rid="R80">Yi et al, 2019</xref>; <xref ref-type="bibr" rid="R54">Martin, 2020</xref>) with more resolved temporal mechanisms (<xref ref-type="bibr" rid="R28">Fontolan et al, 2014</xref>; <xref ref-type="bibr" rid="R38">Hovsepyan et al, 2023</xref>) before being pooled together in a semantic representation.</p><p id="P32">The findings obtained in a controlled auditory semantic categorization task generalized to natural conversation. This is particularly remarkable, as the two datasets differ in important ways. First, in the controlled task, the participant heard isolated word recordings, all of which were nouns normalized for duration and sound intensity, while focusing on a simple cognitive task – assessing the size of the heard objects and animals. Natural speech, on the other hand, involves many other cognitive and perceptual mechanisms, including connected speech that amounts to sentences containing all word types. The intrinsic difference between datasets thus allowed us to generalize the semantic findings from a carefully designed semantic categorization task to a larger category of lexical semantics, and to word class encoding hence approaching syntactic processing. Second, even though natural speech was uttered by another speaker, we could retrieve invariant phonetic representations, both in the timing and the shape of the low-dimensional trajectories. Therefore the low-dimensional representations under-pin speaker-normalization of phonetic representations, a notion that was suggested by LFP-level findings (<xref ref-type="bibr" rid="R74">Sjerps et al, 2019</xref>). Third, in the natural speech task, sounds were more variable in duration and intensity than in the controlled task, entailing other processing dimensions, e.g. prosody, accents, intonation, etc. Finally, natural speech involves a whole range of predictive processes spanning the entire speech hierarchy from low-level acoustics to syntax and semantics. Together, these findings suggest that low-dimensional neuronal population encoding is invariant to speech context.</p><p id="P33">Using the natural speech dataset, we contrasted neuronal signatures underlying semantic and syntactic encoding. Syntactic and semantic encoding occurred at about 150 - 200 ms and 400 ms respectively in their corresponding neural manifolds. The early syntactic encoding might reflect a fast combinatorial process, as reported with MEG (<xref ref-type="bibr" rid="R66">Pylkkänen, 2020</xref>) and/or early effects contributing to conceptual and perceptual categorization in the ATL at about 200 ms (<xref ref-type="bibr" rid="R9">Borghesani et al, 2019</xref>; <xref ref-type="bibr" rid="R11">Chan et al, 2011</xref>; <xref ref-type="bibr" rid="R15">Chen et al, 2016</xref>; <xref ref-type="bibr" rid="R22">Dehaene, 1995</xref>; <xref ref-type="bibr" rid="R37">Hinojosa et al, 2001</xref>). The semantic categorization effect about 400 ms is reminiscent of the N400 component frequently reported in the ATL, for both semantic composition and lexical decision (<xref ref-type="bibr" rid="R43">Kutas and Federmeier, 2011</xref>; <xref ref-type="bibr" rid="R25">Dikker et al, 2020</xref>; <xref ref-type="bibr" rid="R48">López Zunini et al, 2020</xref>; <xref ref-type="bibr" rid="R6">Bentin et al, 1985</xref>; <xref ref-type="bibr" rid="R3">Barber et al, 2013</xref>; <xref ref-type="bibr" rid="R77">Vignali et al, 2023</xref>; <xref ref-type="bibr" rid="R68">Rahimi et al, 2022</xref>; <xref ref-type="bibr" rid="R46">Lau et al, 2008</xref>; <xref ref-type="bibr" rid="R42">Kutas and Federmeier, 2000</xref>).</p><p id="P34">These rare human MEA recordings provide a unique opportunity to investigate the neuronal population effects, at a spatial resolution that was never approached before for speech, in particular dynamical effects that were not detectable even at the most adjacent ECoG contact. However, the conclusions must be taken with caution, as should any data coming from a single participant. Further, high-resolution data come with the price of a restricted spatial sampling. Our findings are specific to a small 4-by-4 mm area of aSTG encompassed by the implantation site of the MEA. This might explain why we observed very specific phonetic effects - e.g. vowels clustering according to the first and not the second format. It is thus possible that the aSTG is organized into small functional subdivisions and that we would observe other clustering principles based on other phonetic features if the array was implanted in nearby regions. Finally, the population of recorded neurons is also limited by the design of the MEA. Only a few tens of units over around two hundred identified responded to the probed speech features, suggesting that coordinated neuronal population encoding is relatively sparse.</p><p id="P35">To conclude, our study provides evidence for a parallel, distributed, and low-dimensional encoding of phonetic and semantic features, that is specific to neuronal population firing patterns in a focal region in the aSTG. These local dynamic population effects are part of bottom-up and top-down dynamics involving oscillatory bidirectional activity potentially involving higher- and lower-tier regions of the language hierarchy. Extending the rapidly emerging neural manifold framework to speech processing, these findings shed new light on the brain mechanisms underlying phonetic and semantic integration and pave the way toward the elucidation of the intricacies behind the complex transformations across the speech processing hierarchy.</p></sec><sec id="S8" sec-type="methods"><title>Methods</title><sec id="S9" sec-type="subjects"><title>Participant</title><p id="P36">The study participant, a male in his thirties with pharmacologically resistant epilepsy, underwent intracranial electrode implantation as part of his clinical epilepsy treatment. He was a native English speaker with normal sensory and cognitive functions and demonstrated left-hemisphere language dominance through a WADA test. The patient experienced partial complex seizures originating from mesial temporal region electrode contacts. The surgical intervention involved the removal of the left anterior temporal lobe, along with the microelectrode implantation site, left parahippocampal gyrus, left hippocampus, and left amygdala. The patient achieved seizure-free status one year post-surgery, with no significant changes in language functions observed in formal neuropsychological testing conducted at that time. Informed consent was obtained, and the study was conducted under the oversight of the Massachusetts General Hospital Institutional Review Board (IRB). The study included both the intracortical implantation of a microelectrode array (MEA) and the performance of a semantic task and natural speech. The MEA recordings were used only for scientific research purposes and played no role in the clinical assessments and decisions.</p></sec><sec id="S10"><title>Neural recordings</title><p id="P37">Cortical local field potentials were recorded with an 8-by-8 subdural ECoG array (Adtech Medical), 1-cm electrode distance, implanted above the left lateral cortex, including frontal, temporal, and anterior parietal areas. For the purpose of this article, only the electrodes covering the lateral temporal lobe were included in the analysis. The signal was recorded with a sampling rate of 500 Hz, with a bandpass filter spanning 0.1 to 200 Hz. All electrode positions were accurately localized relative to the participant's reconstructed cortical surface (<xref ref-type="bibr" rid="R27">Dykstra et al, 2012</xref>).</p><p id="P38">Single-unit action potentials were recorded with a 10-by-10, 400 μm electrode distance, microelectrode array (Utah array, Blackrock Neurotech) surgically implanted within the left anterior superior temporal gyrus (aSTG). Electrodes were 1.5 mm long and contained a 20-μm platinum tip. The implantation site was excised and the subsequent histological analysis revealed the spatial orientation of the electrode tips within the depths of cortical layer III, proximal to layer IV, with no notable histological abnormalities in the neighboring cortical environment. Data acquisition was acquired with a Blackrock NeuroPort system, with a sampling frequency of 30 kilosamples per second, and an analog bandpass filter ranging from 0.3 Hz to 7.5 kHz for antialiasing.</p><p id="P101">The location of the recording arrays was based on clinical considerations. In particular, the MEA was placed in the superior temporal gyrus because this was a region within a larger area anticipated to be resected based on prior imaging data.</p></sec><sec id="S11"><title>Auditory stimuli</title><p id="P39">Neural data was recorded during two separate experimental sessions, that took place on the same day, two hours apart (<xref ref-type="bibr" rid="R12">Chan et al, 2014</xref>). In the first session, the participant performed an auditory semantic categorization task. The stimuli were standalone audio files of 400 words pronounced by a male speaker and normalized for intensity and duration (500 ms). The participant was presented with 800 nouns in a randomized order, and with 2.2 s stimulus onset asynchrony. Out of 800 words, 400 were presented only once, while the remaining 400 consisted of 10 words repeated 40 times each. To avoid biasing effects, in our analysis we considered only the 400 words that were repeated only once. Specifically, the inclusion of repeated words leads to the over-representation of a few phonemes compared to other phonemes, biasing the regression analyses. Half of the 400 unique words referred to objects and half to animals. Following a word presentation, the participant was instructed to press a button if the referred item was bigger than a foot in size. Half of the items in each group (animals, objects) were bigger than a foot, resulting in a balanced 2-by-2 design.</p><p id="P102">In the second session, the participant engaged in a conversation with another person present in the room. The natural speech was recorded using a far-field microphone and manually transcribed. The recording was split into 91 segments that contained clear speech recordings of the other person talking (i.e. without overlapping speech or other background sounds). Each segment was cleaned for background noise and amplified to 0 dBFS in Audacity software. We used a total of 664 words (272 unique) across all trials of the natural speech (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 4</xref>).</p></sec><sec id="S12"><title>Signal preprocessing</title><p id="P40">Spike detection and sorting were performed with the semi-automatic wave_clus algorithm (<xref ref-type="bibr" rid="R67">Quiroga et al, 2004</xref>). Across 96 active electrodes, we identified 176 and 212 distinct units for the sessions with the semantic task and the natural speech, respectively. For the semantic task, we considered units with firing rate higher than 0.3 spikes/second, resulting in 23 units (0.43 – 5.27 spikes/s). For a fair comparison across both sessions, we also selected the 23 most spiking units in the natural speech (0.1 – 3.39 spikes/s).</p><p id="P41">The spike train of each unit was smoothed with a 25 ms wide Gaussian kernel to obtain the firing rate time series. Firing rate time series were then soft-normalized by the range of the unit increased with a constant 5, following previous studies (<xref ref-type="bibr" rid="R17">Churchland et al, 2012</xref>), and downsampled at 200 Hz. For the semantic task, firing rate time series were split into 400 trials. Each trial lasted 1.5 seconds and included 0.5-second periods before and after the word presentation. For the natural speech, we selected 91 segments of the firing rate where a person was talking to the participant (see above).</p><p id="P42">The signals from the ECoG grid were first filtered to remove line noise using a notch filter at 60 Hz and harmonics (120, 180, and 240 Hz). We then applied common-average referencing. For each channel, we extracted broadband high-frequency activity (BHA) in the 70-150 Hz range (<xref ref-type="bibr" rid="R19">Crone et al, 2001</xref>). BHA was computed as the average z-scored amplitude of eight band-pass Gaussian filters with center frequencies and bandwidth increasing logarithmically and semi-logarithmically respectively. The resulting BHA was downsampled to 100 Hz.</p></sec><sec id="S13"><title>Phoneme segmentation and categorization</title><p id="P43">Audio files and corresponding transcripts were segmented both into words and phonemes by creating PRAAT TextGrid files (<xref ref-type="bibr" rid="R8">Boersma, 2001</xref>) through WebMAUS software (<xref ref-type="bibr" rid="R41">Kisler et al, 2017</xref>). Phonetic symbols in the resulting files were encoded in X-SAMPA, a phonetic alphabet designed to cover the entire range of characters in the 1993 version of the International Phonetic Alphabet (IPA) in a computer-readable format. All TextGrids were manually inspected and converted into tabular formats using the TEICONVERT tool. Diphtongues and phonemes that occurred less than 5 times throughout the entire session were removed from the analysis.</p><p id="P44">We used 32 segmented phonemes, divided into 11 vowels and 21 consonants, and further labeled according to the standard IPA phonetic categorizations (<xref ref-type="supplementary-material" rid="SD1">Supplementary Tables 1 and 2</xref>): vowels first formant (high, low), vowels second formant (front, back), consonants articulation place (bilabial, labiodental, alveolar, velar, uvular, glottal), consonants articulation manner (plosive, nasal, fricative, approximant, lateral approximant).</p></sec><sec id="S14"><title>mTRF features</title><p id="P45">For both sessions, we extracted the following features: word onset, acoustic edge (envelope rate peaks), phoneme onset, phoneme identity, and phonetic category. For the semantic task, we additionally computed the following semantic features: perceptual category, conceptual category, and semantic decision. For the natural speech, we additionally created word class and lexical semantics features. All features were designed as values located at the onset of the corresponding stimuli, all other values being set to zeros.</p><p id="P46">Word onset was marked by a value of one located at the onset of each word. Acoustic edges were defined as local maxima in the derivative of the speech envelope (<xref ref-type="bibr" rid="R59">Oganian and Chang, 2019</xref>). The speech envelope was computed as the logarithmically scaled root mean square of the audio signal using the MATLAB mTRFenvelope function. Phoneme onset feature indicates onsets of all phonemes in a word. Phoneme identity feature was multivariable with 32 regressors, each indicating onsets of a different phoneme, as defined by the IPA table. Phonetic category feature was multivariable, and included four phonetic groups (vowel first and second formant, consonant manner, and place of articulations). All other features were multivariables with a value located at the corresponding word onset. Perceptual category, conceptual category, and semantic decision had two regressors, defined respectively as bigger and smaller, animal and object, and decision on whether the object/animal was bigger or smaller than a foot. Word class had 13 regressors, indicating different word classes (noun, verb, adjective, adverb, article, auxiliary, demonstrative, quantifier, preposition, pronoun, conjunction, interjection, number). Finally, the lexical semantics feature was multivariable, designed by regressing each of the 11 sensorimotor norms at the corresponding word onset (<xref ref-type="bibr" rid="R49">Lynott et al, 2019</xref>). All stimuli were smoothed with a 25-ms-wide Gaussian kernel and downsampled to either 200 Hz (to match single unit firing rates) or 100 Hz (to match BHA from ECoG channels) before fitting the mTRF models.</p></sec><sec id="S15"><title>mTRF estimation</title><p id="P47">mTRFs were estimated using the mTRF MATLAB toolbox (<xref ref-type="bibr" rid="R20">Crosse et al, 2016</xref>). All mTRFs were always of encoding type, relating the stimulus features to neural data, with resulting kernels in the time range between -200 and 600 ms. The first and last 50 ms were not considered in the analysis, to avoid possible edge effects. Both for semantic task and natural speech, the baseline model contained the word onset and the acoustic onset edge features. All other models included the baseline features and one of the additional target features defined above. Estimation was performed by a ridge regression, using a nested cross-validation procedure (see below). The goodness of fit was defined as Pearson's correlation between model prediction and neural data.</p></sec><sec id="S16"><title>Cross-validation</title><p id="P48">For the semantic task, we performed a nested cross-validation. In the outer cross-validation loop, we split the 400 words randomly into 5 sets of 80 (20% of the words each). Thus, in each fold, 80 trials belonged to a hold-out test set, while the remaining 320 words belonged to the train set. The five folds were identical across all models. For a given outer fold, among the 320 train-set trials, we performed another 8-fold inner cross-validation loop for the ridge regression hyperparameter tuning, with λ ranging from 10<sup>−6</sup> to 10<sup>6</sup>. The optimal lambda was then used to retrain the model on the 320 words of the training set of the outer cross-validation loop fold. The model predictions and Pearson's correlation with the neural data were then computed for the 80 words of the test set. Across the 5 folds, we thus obtained five correlation values, of which we report the average.</p><p id="P49">For natural speech, we also used nested cross-validation. The 5-fold outer cross-validation loop is performed by splitting the 91 segments into five folds of approximately similar duration (mean: 393.28 s; sd: 6.73 s), chosen through random shuffling across the five folds until the standard deviation of the duration distribution across folds was smaller than 10 seconds. We applied a similar procedure for the inner cross-validation loop in each of the five folds.</p></sec><sec id="S17"><title>Surrogate distributions and statistical significance</title><p id="P50">For each model of the semantic task, we created a distribution of 1000 surrogate models by shuffling the target feature across words, and keeping the baseline features constant. For instance, in the model that contained the phoneme onset feature together with the baseline features (word onset and acoustic edges), the surrogate model was created by randomly shuffling the 400 phoneme onset features across 400 words independently, while keeping the order of the baseline features constant. In this way, the baseline features were properly regressed to the neural data, while the target feature (e.g. phoneme onset) was randomly assigned to neural data.</p><p id="P103">For the natural speech condition, it was not possible to shuffle trials in the same way, as each auditory segment was of a different duration. This posed a problem because the mTRF features have to be of the same length as the neural data, which was not the case for natural speech, contrary to the semantic task where each word had the same duration. Instead, we used the following method: for multivariable features, the surrogates were computed by randomly assigning each non-zero value to a particular regressor (e.g. for the phoneme identity feature, the first phoneme is randomly assigned to any of the 32 regressors, the second to any of the remaining 31, etc). For features with a single variable, surrogates were computed by performing a circular shift with a random onset. For instance, for the phoneme onset feature, which had only one regressor, we randomly split the trace into 2 parts and switched the order of the parts.</p><p id="P104">A model was considered statistically significant if the original model performed better than the 95th percentile of the surrogate distribution (one-tailed test).</p></sec><sec id="S18"><title>Averaged kernels</title><p id="P51">To obtain averaged kernels, we took the mean of kernels across all units (and phonemes for the phoneme kernels). An averaged kernel was considered statistically significant if higher or lower than the 97.5th or 2.5th percentile respectively of the surrogate distribution (two-tailed test).</p></sec><sec id="S19"><title>Correcting for multiple comparisons</title><p id="P52">To control for multiple comparisons, we used a nonparametric approach based on the cluster mass test (<xref ref-type="bibr" rid="R51">Maris and Oostenveld, 2007</xref>). From each permutation, we subtracted the mean value of the surrogate distribution and clustered consecutive time points that were outside the 95% confidence region of the surrogate distribution. Cluster-level statistics was defined as the sum of the mean-centered values within a cluster. For negative clusters, we considered the absolute value of the sum. Clusters of the observed data that were higher than the 95th percentile of the distribution of maximum cluster surrogates were considered as passing the multiple comparison correction.</p></sec><sec id="S20"><title>Clustering of phoneme kernels in the PC space</title><p id="P53">Clustering was performed by assigning each phoneme to a particular phonetic class and computing the clustering index, defined as the difference between between-cluster and intra-cluster distances. Specifically, for each cluster, we first found the location of the centroid by averaging the coordinates of all cluster elements. Between-cluster distance was defined as the average Euclidean distance between all pairs of cluster centroids. Intra-cluster distance was defined as the Euclidean distance of each cluster element to the corresponding centroid. By subtracting intra- from between-cluster distance, our index rewarded cluster separability (between-cluster distance) and penalized spatial dispersion (intra-cluster distance).</p><p id="P54">We performed clustering in the two-dimensional PC space (<xref ref-type="sec" rid="S2">Results</xref>), but systematically confirmed our results in the PC spaces using up to six dimensions (<xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref>). Clustering was considered statistically significant if the original model had a higher clustering index than the 95th percentile of the surrogate distribution (one-tailed test).</p><p id="P55">To confirm our clustering results, we performed several control analyses, described here shortly and in detail in <xref ref-type="supplementary-material" rid="SD1">Supplementary Material</xref>: <list list-type="order" id="L1"><list-item><p id="P56">linear discriminant analysis (LDA) classifier: at each time point and for each phonetic feature group, we first ran an LDA classifier to compute the means of the multivariate normal distributions of phonemes sharing the same phonetic feature. Then, we computed the average Euclidean distance between all phonetic feature means and compared this distance against the distribution of 1000 surrogates.</p></list-item><list-item><p id="P57">rank regression for vowels: we additionally explored whether the actual first and second formant frequency values were encoded in the low-dimensional space. To that aim, we assigned a rank value (1-7) to each vowel, based on the formant values indicated in the standard IPA table. At each time point, the ranked order of vowels was correlated with their coordinates on the first three PCs, and compared against a distribution of 1000 surrogates.</p></list-item><list-item><p id="P58">correlation with K-means connectivity matrices: to investigate whether the same results would emerge in a data-driven fashion, for each time point, we ran K-means clustering 1000 times (clustering results slightly differed depending on the algorithm's random initialization). Then, we computed an average N-by-N connectivity matrix that indicated how often each of the N phonemes was clustered together. Finally, we correlated the resulting connectivity matrix with the connectivity matrix of the actual, phonetic-based clusters (vowel first formant, vowel second formant, consonants manner, consonants place), and compared the correlation value against the distribution of 1000 surrogates.</p></list-item></list>
</p></sec><sec id="S21"><title>Separability of semantic feature kernels in the PC space</title><p id="P59">Each semantic feature of the semantic task only had two regressors (e.g. animal vs object, bigger vs smaller), hence not allowing any clustering. To identify the periods during which the two kernels of each semantic feature were significantly separated in the PC space, we computed the average Euclidean distance between the two. The same process was repeated for each of the 1000 surrogate models, and the distance was considered significant when higher than the 95th percentile of the surrogate distribution (one-tailed test).</p><p id="P60">Semantic features of the natural speech were computed similarly. For the Word class feature, we averaged the Euclidean distances between all pairs of regressors (noun, verb, adjective, etc). For the Lexical semantic feature, we computed the distance between the visual Lancaster sensorimotor norm and the average of the 10 other norms (<xref ref-type="bibr" rid="R49">Lynott et al, 2019</xref>), as visual norm was the most present in our dataset (visual norm strength: 2.73; other norms, mean: 1.22, standard deviation: 0.6).</p></sec><sec id="S22"><title>Comparison of trajectories between the semantic task and natural speech</title><p id="P61">To investigate whether the trajectories of the phoneme kernels projected to the low-dimensional space were similar between the semantic task and the natural speech, we computed the canonical correlation between each kernel pair (e.g. kernel of phoneme /k/ extracted during the semantic task and the /k/ kernel from the natural speech). The canonical correlation was compared against the distribution of surrogate model canonical correlations for all 23 dimensions. Particularly, the surrogate distribution was computed by shuffling the kernels in the natural speech, which constitutes a strong surrogate test. The correlation was considered significant if it was higher than the value of the 95th percentile of the surrogate correlation distribution (one-tailed test).</p></sec><sec id="S23"><title>Position-based phoneme onset kernels</title><p id="P62">To probe for interactions between the encoding of phonetic and semantic features, we first created mTRF kernels for phoneme onsets at each phoneme position within the word and then aligned the resulting kernels with the corresponding semantics kernel (lexical semantics for natural speech and perceptual semantics for the task). Position-based phoneme onset was thus a multivariable feature with five regressors, each indicating the onset of the corresponding phoneme position across all words. For instance, phoneme onset at position 2 indicates the onset of second phonemes in all words, regardless of the phoneme type. The resulting kernels were then shifted by the multiples of 80 ms, which is a rounded value of average phoneme duration (mean: 82.6 ms, sd: 3.1 ms). Thus, the kernel for position two was shifted by 80 ms, the kernel for position three by 160 ms, etc. Surrogate kernel distributions were computed as described above. Phoneme onset kernels are considered significant if they are higher or lower than the value of the 97.5th or 2.5th percentile respectively of their surrogate kernel distributions (two-tailed test). This allowed us to observe when significant peaks for each phoneme occurred in reference to the same time point: word onset.</p></sec><sec id="S24"><title>LFP beta, low-gamma, and BHA power analysis</title><p id="P63">We further investigated the nature of these kernels within the analysis-by-synthesis framework, by fitting mTRF encoding-type models with either word onset or position-based phoneme onset as stimulus, and either beta (12 - 30 Hz), low-gamma (30 - 70 Hz), or BHA (70 - 150 Hz) LFP power as response. Word onset and phoneme onset stimuli were the same as the ones used before. For each microelectrode channel, LFP power bands were computed by first applying a 9-order bandpass Butterworth filter with zero-phase forward and reverse digital filtering, then subtracting the mean from the resulting trace, and finally computing the absolute value of its Hilbert transform. The resulting LFP powers were then averaged across all channels and entered into the mTRF models. Finally, we compared the significant periods of the word-onset beta-power kernel and position-based phoneme-onset low-gamma kernels with the same aligning procedure as above.</p></sec><sec id="S25"><title>Granger causality</title><p id="P64">We adopted a Granger causality measure (<xref ref-type="bibr" rid="R63">Pesaran et al, 2018</xref>) to explore the temporal causality between the low-dimensional phonetic and semantic representations. We used the multivariate Granger causality toolbox (MVGC v1.3, 2022), which is based on a state-space formulation of Granger causal analysis (<xref ref-type="bibr" rid="R4">Barnett and Seth, 2014</xref>, <xref ref-type="bibr" rid="R13">2015</xref>). We first applied a half-Gaussian filter 25 ms wide to the spiking traces of individual units to obtain a causal firing rate estimate. The resulting firing rates were then projected into the three-dimensional phonetic and semantic PC spaces, constructed by performing PCA on the corresponding phonetic and semantic mTRF kernels as described above. The Granger models were created by separately predicting each of the three PCs of one feature (e.g. phonetic) from all three dimensions of another feature (e.g. semantic). We then combined the resulting Granger coefficients into two 3-by-3 matrices, one for phonetic-to-semantic and another one for semantic-to-phonetic predictions. Autoregression model parameters were estimated from data via the Levinson-Wiggins-Robinson algorithm. The order of AR models was selected via the Akaike Information Criterion (AIC). Statistical significance of estimated Granger causality measures was assessed via the corresponding F-test as implemented by the toolbox.</p></sec><sec id="S26"><title>fMRI databases</title><p id="P65">Functional classification of the cortical surface surrounding the MEA with respect to different linguistic processes was performed using NeuroQuery database (<xref ref-type="bibr" rid="R26">Dockès et al, 2020</xref>). It uses text mining and meta-analysis techniques to automatically produce large-scale mappings between fMRI brain activity and a cognitive process of interest. We were primarily interested in observing the proximity of phonetic and semantic processing close to the MEA implantation site. As keywords, we used "phonetics" and "semantic categorization".</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>supplementary material</label><media xlink:href="EMS190467-supplement-supplementary_material.pdf" mimetype="application" mime-subtype="pdf" id="d24aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S27"><title>Acknowledgments</title><p>This work was funded by Swiss National Science Foundation career grant 193542 (T.P.), EU FET-BrainCom project (A.G.), NCCR Evolving Language, Swiss National Science Foundation Agreement #51NF40_180888 (A.G.), Swiss National Science Foundation project grant 163040 (A.G.), National Institutes of Health (NIH), National Institute of Neurological Disorders and Stroke (NINDS), grants R01NS079533 (WT) and R01NS062092 (SSC), and the Pablo J. Salame Goldman Sachs endowed Associate Professorship of Computational Neuroscience at Brown University (WT).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aghagolzadeh</surname><given-names>M</given-names></name><name><surname>Truccolo</surname><given-names>W</given-names></name></person-group><article-title>Inference and Decoding of Motor Cortex Low-Dimensional Dynamics via Latent State-Space Models</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><year>2016</year><volume>24</volume><issue>2</issue><fpage>272</fpage><lpage>282</lpage><pub-id pub-id-type="pmcid">PMC4910090</pub-id><pub-id pub-id-type="pmid">26336135</pub-id><pub-id pub-id-type="doi">10.1109/TNSRE.2015.2470527</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><article-title>Cortical oscillations and sensory predictions</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><issue>7</issue><fpage>390</fpage><lpage>398</lpage><pub-id pub-id-type="pmid">22682813</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barber</surname><given-names>HA</given-names></name><name><surname>Otten</surname><given-names>LJ</given-names></name><name><surname>Kousta</surname><given-names>ST</given-names></name><etal/></person-group><article-title>Concreteness in word processing: ERP and behavioral effects in a lexical decision task</article-title><source>Brain and Language</source><year>2013</year><volume>125</volume><issue>1</issue><fpage>47</fpage><lpage>53</lpage><pub-id pub-id-type="pmid">23454073</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>The MVGC multivariate Granger causality toolbox: A new approach to Granger-causal inference</article-title><source>Journal of Neuroscience Methods</source><year>2014</year><volume>223</volume><fpage>50</fpage><lpage>68</lpage><pub-id pub-id-type="pmid">24200508</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnett</surname><given-names>L</given-names></name><name><surname>Seth</surname><given-names>AK</given-names></name></person-group><article-title>Granger causality for state-space models</article-title><source>Physical Review E</source><year>2015</year><volume>91</volume><issue>4</issue><elocation-id>040101</elocation-id><pub-id pub-id-type="pmid">25974424</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bentin</surname><given-names>S</given-names></name><name><surname>Gregory</surname><given-names>McCarthy</given-names></name><name><surname>Wood</surname><given-names>CC</given-names></name></person-group><article-title>Event-related potentials, lexical decision and semantic priming</article-title><source>Electroencephalography and clinical Neurophysiology</source><year>1985</year><volume>60</volume><fpage>343</fpage><lpage>355</lpage><pub-id pub-id-type="pmid">2579801</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bever</surname><given-names>TG</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Analysis by Synthesis: A (Re-)Emerging Program of Research for Language and Vision</article-title><source>Biolinguistics</source><year>2010</year><volume>4</volume><issue>2-3</issue><fpage>174</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.5964/bioling.8783</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boersma</surname><given-names>P</given-names></name></person-group><article-title>Praat, a system for doing phonetics by computer</article-title><source>Glot Int</source><year>2001</year><volume>5</volume><issue>9</issue><fpage>341</fpage><lpage>345</lpage></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borghesani</surname><given-names>V</given-names></name><name><surname>Buiatti</surname><given-names>M</given-names></name><name><surname>Eger</surname><given-names>E</given-names></name><etal/></person-group><article-title>Conceptual and Perceptual Dimensions of Word Meaning Are Recovered Rapidly and in Parallel during Reading</article-title><source>Journal of Cognitive Neuroscience</source><year>2019</year><volume>31</volume><issue>1</issue><fpage>95</fpage><lpage>108</lpage><pub-id pub-id-type="pmid">30156506</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Deep language algorithms predict semantic comprehension from brain activity</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><elocation-id>16327</elocation-id><pub-id pub-id-type="pmcid">PMC9522791</pub-id><pub-id pub-id-type="pmid">36175483</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-20460-9</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>AM</given-names></name><name><surname>Baker</surname><given-names>JM</given-names></name><name><surname>Eskandar</surname><given-names>E</given-names></name><etal/></person-group><article-title>First-Pass Selectivity for Semantic Categories in Human Anteroventral Temporal Lobe</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>49</issue><fpage>18119</fpage><lpage>18129</lpage><pub-id pub-id-type="pmcid">PMC3286838</pub-id><pub-id pub-id-type="pmid">22159123</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3122-11.2011</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>AM</given-names></name><name><surname>Dykstra</surname><given-names>AR</given-names></name><name><surname>Jayaram</surname><given-names>V</given-names></name><etal/></person-group><article-title>Speech-Specific Tuning of Neurons in Human Superior Temporal Gyrus</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><issue>10</issue><fpage>2679</fpage><lpage>2693</lpage><pub-id pub-id-type="pmcid">PMC4162511</pub-id><pub-id pub-id-type="pmid">23680841</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bht127</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>EF</given-names></name><name><surname>Raygor</surname><given-names>KP</given-names></name><name><surname>Berger</surname><given-names>MS</given-names></name></person-group><article-title>Contemporary model of language organization: An overview for neurosurgeons</article-title><source>Journal of Neurosurgery</source><year>2015</year><volume>122</volume><issue>2</issue><fpage>250</fpage><lpage>261</lpage><pub-id pub-id-type="pmid">25423277</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>ZC</given-names></name><name><surname>Takaura</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><etal/></person-group><article-title>Large-Scale Cortical Networks for Hierarchical Prediction and Prediction Error in the Primate Brain</article-title><source>Neuron</source><year>2018</year><volume>100</volume><issue>5</issue><fpage>1252</fpage><lpage>1266</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmid">30482692</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Shimotake</surname><given-names>A</given-names></name><name><surname>Matsumoto</surname><given-names>R</given-names></name><etal/></person-group><article-title>The ‘when’ and ‘where’ of semantic coding in the anterior temporal lobe: Temporal representational similarity analysis of electrocorticogram data</article-title><source>Cortex</source><year>2016</year><volume>79</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC4884671</pub-id><pub-id pub-id-type="pmid">27085891</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2016.02.015</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name></person-group><article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><year>2021</year><volume>70</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="pmcid">PMC10695674</pub-id><pub-id pub-id-type="pmid">34801787</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2021.10.010</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><etal/></person-group><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><year>2012</year><volume>487</volume><issue>7405</issue><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="pmcid">PMC3393826</pub-id><pub-id pub-id-type="pmid">22722855</pub-id><pub-id pub-id-type="doi">10.1038/nature11129</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cope</surname><given-names>TE</given-names></name><name><surname>Shtyrov</surname><given-names>Y</given-names></name><name><surname>MacGregor</surname><given-names>LJ</given-names></name><etal/></person-group><article-title>Anterior temporal lobe is necessary for efficient lateralised processing of spoken word identity</article-title><source>Cortex</source><year>2020</year><volume>126</volume><fpage>107</fpage><lpage>118</lpage><pub-id pub-id-type="pmcid">PMC7253293</pub-id><pub-id pub-id-type="pmid">32065956</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2019.12.025</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crone</surname><given-names>NE</given-names></name><name><surname>Boatman</surname><given-names>D</given-names></name><name><surname>Gordon</surname><given-names>B</given-names></name><etal/></person-group><article-title>Induced electrocorticographic gamma activity during auditory perception</article-title><source>Clinical Neurophysiology</source><year>2001</year><volume>112</volume><issue>4</issue><fpage>565</fpage><lpage>582</lpage><pub-id pub-id-type="pmid">11275528</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><etal/></person-group><article-title>The multivariate temporal response function (mTRF) toolbox: A MATLAB toolbox for relating neural signals to continuous stimuli</article-title><source>Frontiers in human neuroscience</source><year>2016</year><volume>10</volume><fpage>604</fpage><pub-id pub-id-type="pmcid">PMC5127806</pub-id><pub-id pub-id-type="pmid">27965557</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damera</surname><given-names>SR</given-names></name><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Nikolov</surname><given-names>PP</given-names></name><etal/></person-group><article-title>Evidence for a Spoken Word Lexicon in the Auditory Ventral Stream</article-title><source>Neurobiology of Language</source><year>2023</year><volume>4</volume><issue>3</issue><fpage>420</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC10426387</pub-id><pub-id pub-id-type="pmid">37588129</pub-id><pub-id pub-id-type="doi">10.1162/nol_a_00108</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name></person-group><article-title>Electrophysiological evidence for category-specific word processing in the normal human brain</article-title><source>NeuroReport</source><year>1995</year><volume>6</volume><issue>16</issue><fpage>2153</fpage><pub-id pub-id-type="pmid">8595192</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name><name><surname>Wacongne</surname><given-names>C</given-names></name><etal/></person-group><article-title>The Neural Representation of Sequences: From Transition Probabilities to Algebraic Patterns and Linguistic Trees</article-title><source>Neuron</source><year>2015</year><volume>88</volume><issue>1</issue><fpage>2</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">26447569</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><year>2007</year><volume>11</volume><issue>8</issue><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dikker</surname><given-names>S</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Gwilliams</surname><given-names>L</given-names></name><etal/></person-group><article-title>Magnetoencephalography and Language</article-title><source>Neuroimaging Clinics of North America</source><year>2020</year><volume>30</volume><issue>2</issue><fpage>229</fpage><lpage>238</lpage><pub-id pub-id-type="pmid">32336409</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dockès</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Primet</surname><given-names>R</given-names></name><etal/></person-group><article-title>NeuroQuery, comprehensive meta-analysis of human brain mapping</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e53385</elocation-id><pub-id pub-id-type="pmcid">PMC7164961</pub-id><pub-id pub-id-type="pmid">32129761</pub-id><pub-id pub-id-type="doi">10.7554/eLife.53385</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dykstra</surname><given-names>AR</given-names></name><name><surname>Chan</surname><given-names>AM</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><etal/></person-group><article-title>Individualized localization and cortical surface-based registration of intracranial electrodes</article-title><source>NeuroImage</source><year>2012</year><volume>59</volume><issue>4</issue><fpage>3563</fpage><lpage>3570</lpage><pub-id pub-id-type="pmcid">PMC3288767</pub-id><pub-id pub-id-type="pmid">22155045</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.11.046</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Liegeois-Chauvel</surname><given-names>C</given-names></name><etal/></person-group><article-title>The contribution of frequency-specific activity to hierarchical information processing in the human auditory cortex</article-title><source>Nature Communications</source><year>2014</year><volume>5</volume><elocation-id>4694</elocation-id><pub-id pub-id-type="pmcid">PMC4164774</pub-id><pub-id pub-id-type="pmid">25178489</pub-id><pub-id pub-id-type="doi">10.1038/ncomms5694</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>The brain basis of syntactic processes: Functional imaging and lesion studies</article-title><source>NeuroImage</source><year>2003</year><volume>20</volume><fpage>S8</fpage><lpage>S17</lpage><pub-id pub-id-type="pmid">14597292</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><etal/></person-group><article-title>Neural Manifolds for the Control of Movement</article-title><source>Neuron</source><year>2017</year><volume>94</volume><issue>5</issue><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="pmcid">PMC6122849</pub-id><pub-id pub-id-type="pmid">28595054</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name></person-group><article-title>Hierarchical Predictive Information Is Channeled by Asymmetric Oscillatory Activity</article-title><source>Neuron</source><year>2018</year><volume>100</volume><issue>5</issue><fpage>1022</fpage><lpage>1024</lpage><pub-id pub-id-type="pmid">30521776</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>JR</given-names></name></person-group><article-title>Recurrent processes support a cascade of hierarchical decisions</article-title><source>eLife</source><year>2020</year><volume>9</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC7513462</pub-id><pub-id pub-id-type="pmid">32869746</pub-id><pub-id pub-id-type="doi">10.7554/eLife.56603</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Halle</surname><given-names>M</given-names></name><name><surname>Stevens</surname><given-names>K</given-names></name></person-group><source>Analysis by synthesis</source><person-group person-group-type="editor"><name><surname>Wathen-Dunn</surname><given-names>W</given-names></name><name><surname>Woods</surname><given-names>LE</given-names></name></person-group><conf-name>Proceeding of the Seminar on Speech Compression and Processing, Paper D7</conf-name><year>1959</year><volume>II</volume></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Oganian</surname><given-names>Y</given-names></name><name><surname>Hall</surname><given-names>J</given-names></name><etal/></person-group><article-title>Parallel and distributed encoding of speech across human auditory cortex</article-title><source>Cell</source><year>2021</year><volume>184</volume><issue>18</issue><fpage>4626</fpage><lpage>4639</lpage><elocation-id>e13</elocation-id><pub-id pub-id-type="pmcid">PMC8456481</pub-id><pub-id pub-id-type="pmid">34411517</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.07.019</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Armeni</surname><given-names>K</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name><etal/></person-group><article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>32</issue><elocation-id>e2201968119</elocation-id><pub-id pub-id-type="pmcid">PMC9371745</pub-id><pub-id pub-id-type="pmid">35921434</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews Neuroscience</source><year>2007</year><volume>8</volume><issue>5</issue><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinojosa</surname><given-names>JA</given-names></name><name><surname>Martín-Loeches</surname><given-names>M</given-names></name><name><surname>Munoz</surname><given-names>F</given-names></name><etal/></person-group><article-title>Electrophysiological evidence of a semantic system commonly accessed by animals and tools categories</article-title><source>Cognitive Brain Research</source><year>2001</year><volume>12</volume><issue>2</issue><fpage>321</fpage><lpage>328</lpage><pub-id pub-id-type="pmid">11587901</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hovsepyan</surname><given-names>S</given-names></name><name><surname>Olasagasti</surname><given-names>I</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><article-title>Rhythmic modulation of prediction errors: A top-down gating role for the beta-range in speech processing</article-title><source>PLOS Computational Biology</source><year>2023</year><volume>19</volume><issue>11</issue><elocation-id>e1011595</elocation-id><pub-id pub-id-type="pmcid">PMC10655987</pub-id><pub-id pub-id-type="pmid">37934766</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1011595</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><article-title>Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity</article-title><source>Current Opinion in Neurobiology</source><year>2021</year><volume>70</volume><fpage>113</fpage><lpage>120</lpage><pub-id pub-id-type="pmcid">PMC8688220</pub-id><pub-id pub-id-type="pmid">34537579</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2021.08.002</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshishian</surname><given-names>M</given-names></name><name><surname>Akkol</surname><given-names>S</given-names></name><name><surname>Herrero</surname><given-names>J</given-names></name><etal/></person-group><article-title>Joint, distributed and hierarchically organized encoding of linguistic features in the human auditory cortex</article-title><source>Nature Human Behaviour</source><year>2023</year><volume>7</volume><issue>5</issue><fpage>740</fpage><lpage>753</lpage><pub-id pub-id-type="pmcid">PMC10417567</pub-id><pub-id pub-id-type="pmid">36864134</pub-id><pub-id pub-id-type="doi">10.1038/s41562-023-01520-0</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kisler</surname><given-names>T</given-names></name><name><surname>Reichel</surname><given-names>U</given-names></name><name><surname>Schiel</surname><given-names>F</given-names></name></person-group><article-title>Multilingual processing of speech via web services</article-title><source>Computer Speech &amp; Language</source><year>2017</year><volume>45</volume><fpage>326</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.csl.2017.01.005</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Electrophysiology reveals semantic memory use in language comprehension</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>12</issue><fpage>463</fpage><lpage>470</lpage><pub-id pub-id-type="pmid">11115760</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutas</surname><given-names>M</given-names></name><name><surname>Federmeier</surname><given-names>KD</given-names></name></person-group><article-title>Thirty Years and Counting: Finding Meaning in the N400 Component of the Event-Related Brain Potential (ERP)</article-title><source>Annual Review of Psychology</source><year>2011</year><volume>62</volume><issue>1</issue><fpage>621</fpage><lpage>647</lpage><pub-id pub-id-type="pmcid">PMC4052444</pub-id><pub-id pub-id-type="pmid">20809790</pub-id><pub-id pub-id-type="doi">10.1146/annurev.psych.093008.131123</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Ossmy</surname><given-names>O</given-names></name><name><surname>Friedmann</surname><given-names>N</given-names></name><etal/></person-group><article-title>Single-cell activity in human STG during perception of phonemes is organized according to manner of articulation</article-title><source>NeuroImage</source><year>2021</year><volume>226</volume><elocation-id>117499</elocation-id><pub-id pub-id-type="pmid">33186717</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name><name><surname>Lowe</surname><given-names>C</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Neural basis of category-specific semantic deficits for living things: Evidence from semantic dementia, HSVE and a neural network model</article-title><source>Brain</source><year>2006</year><volume>130</volume><issue>4</issue><fpage>1127</fpage><lpage>1137</lpage><pub-id pub-id-type="pmid">17438021</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>EF</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>A cortical network for semantics: (de)constructing the N400</article-title><source>Nature Reviews Neuroscience</source><year>2008</year><volume>9</volume><issue>12</issue><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="pmid">19020511</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leszczyński</surname><given-names>M</given-names></name><name><surname>Barczak</surname><given-names>A</given-names></name><name><surname>Kajikawa</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Dissociation of broadband high-frequency activity and neuronal firing in the neocortex</article-title><source>Science Advances</source><year>2020</year><volume>6</volume><issue>33</issue><elocation-id>eabb0977</elocation-id><pub-id pub-id-type="pmcid">PMC7423365</pub-id><pub-id pub-id-type="pmid">32851172</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abb0977</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>López Zunini</surname><given-names>RA</given-names></name><name><surname>Baart</surname><given-names>M</given-names></name><name><surname>Samuel</surname><given-names>AG</given-names></name><etal/></person-group><article-title>Lexical access versus lexical decision processes for auditory, visual, and audiovisual items: Insights from behavioral and neural measures</article-title><source>Neuropsychologia</source><year>2020</year><volume>137</volume><elocation-id>107305</elocation-id><pub-id pub-id-type="pmid">31838100</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynott</surname><given-names>D</given-names></name><name><surname>Connell</surname><given-names>L</given-names></name><name><surname>Brysbaert</surname><given-names>M</given-names></name><etal/></person-group><article-title>The Lancaster Sensorimotor Norms: Multidimensional measures of Perceptual and Action Strength for 40,000 English words</article-title><year>2019</year><pub-id pub-id-type="pmcid">PMC7280349</pub-id><pub-id pub-id-type="pmid">31832879</pub-id><pub-id pub-id-type="doi">10.3758/s13428-019-01316-z</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><etal/></person-group><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>503</volume><issue>7474</issue><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC4121670</pub-id><pub-id pub-id-type="pmid">24201281</pub-id><pub-id pub-id-type="doi">10.1038/nature12742</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><issue>1</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markowitz</surname><given-names>DA</given-names></name><name><surname>Curtis</surname><given-names>CE</given-names></name><name><surname>Pesaran</surname><given-names>B</given-names></name></person-group><article-title>Multiple component networks support working memory in prefrontal cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2015</year><volume>112</volume><issue>35</issue><fpage>11084</fpage><lpage>11089</lpage><pub-id pub-id-type="pmcid">PMC4568266</pub-id><pub-id pub-id-type="pmid">26283366</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1504172112</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><article-title>Functional parallelism in spoken word-recognition</article-title><source>Cognition</source><year>1987</year><volume>25</volume><issue>1-2</issue><fpage>71</fpage><lpage>102</lpage><pub-id pub-id-type="pmid">3581730</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><article-title>A Compositional Neural Architecture for Language</article-title><source>Journal of Cognitive Neuroscience</source><year>2020</year><volume>32</volume><issue>8</issue><fpage>1407</fpage><lpage>1427</lpage><pub-id pub-id-type="pmid">32108553</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><etal/></person-group><article-title>Phonetic Feature Encoding in Human Superior Temporal Gyrus</article-title><source>Science</source><year>2014</year><volume>343</volume><issue>6174</issue><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="pmcid">PMC4350233</pub-id><pub-id pub-id-type="pmid">24482117</pub-id><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>van Pelt</surname><given-names>S</given-names></name><etal/></person-group><article-title>Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward Influences among Human Visual Cortical Areas</article-title><source>Neuron</source><year>2016</year><volume>89</volume><issue>2</issue><fpage>384</fpage><lpage>397</lpage><pub-id pub-id-type="pmcid">PMC4871751</pub-id><pub-id pub-id-type="pmid">26777277</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.018</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noppeney</surname><given-names>U</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><etal/></person-group><article-title>Temporal lobe lesions and semantic impairment: A comparison of herpes simplex virus encephalitis and semantic dementia</article-title><source>Brain</source><year>2006</year><volume>130</volume><issue>4</issue><fpage>1138</fpage><lpage>1147</lpage><pub-id pub-id-type="pmid">17251241</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name></person-group><article-title>Shortlist B: A Bayesian model of continuous speech recognition</article-title><source>Psychological Review</source><year>2008</year><volume>115</volume><issue>2</issue><fpage>357</fpage><lpage>395</lpage><pub-id pub-id-type="pmid">18426294</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oganian</surname><given-names>Y</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>A speech envelope landmark for syllable encoding in human superior temporal gyrus</article-title><source>Science advances</source><year>2019</year><volume>5</volume><issue>11</issue><elocation-id>eaay6279</elocation-id><pub-id pub-id-type="pmcid">PMC6957234</pub-id><pub-id pub-id-type="pmid">31976369</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aay6279</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ossmy</surname><given-names>O</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Mukamel</surname><given-names>R</given-names></name></person-group><article-title>Decoding speech perception from single cell activity in humans</article-title><source>NeuroImage</source><year>2015</year><volume>117</volume><fpage>151</fpage><lpage>159</lpage><pub-id pub-id-type="pmid">25976925</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Nestor</surname><given-names>PJ</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>Where do you know what you know? The representation of semantic knowledge in the human brain</article-title><source>Nature Reviews Neuroscience</source><year>2007</year><volume>8</volume><issue>12</issue><fpage>976</fpage><lpage>987</lpage><pub-id pub-id-type="pmid">18026167</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdikis</surname><given-names>D</given-names></name><name><surname>Huys</surname><given-names>R</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>Time Scale Hierarchies in the Functional Organization of Complex Behaviors</article-title><source>PLoS Computational Biology</source><year>2011</year><volume>7</volume><issue>9</issue><elocation-id>e1002198</elocation-id><pub-id pub-id-type="pmcid">PMC3182871</pub-id><pub-id pub-id-type="pmid">21980278</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002198</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesaran</surname><given-names>B</given-names></name><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Einevoll</surname><given-names>GT</given-names></name><etal/></person-group><article-title>Investigating large-scale brain dynamics using field potential recordings: Analysis and interpretation</article-title><source>Nature Neuroscience</source><year>2018</year><pub-id pub-id-type="pmcid">PMC7386068</pub-id><pub-id pub-id-type="pmid">29942039</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0171-8</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillai</surname><given-names>AS</given-names></name><name><surname>Jirsa</surname><given-names>VK</given-names></name></person-group><article-title>Symmetry Breaking in Space-Time Hierarchies Shapes Brain Dynamics and Behavior</article-title><source>Neuron</source><year>2017</year><volume>94</volume><issue>5</issue><fpage>1010</fpage><lpage>1026</lpage><pub-id pub-id-type="pmid">28595045</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><article-title>Neural reuse of action perception circuits for language, concepts and communication</article-title><source>Progress in Neurobiology</source><year>2018</year><volume>160</volume><fpage>1</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">28734837</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylkkänen</surname><given-names>L</given-names></name></person-group><article-title>Neural basis of basic composition: What we have learned from the red–boat studies and their extensions</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2020</year><volume>375</volume><issue>1791</issue><elocation-id>20190299</elocation-id><pub-id pub-id-type="pmcid">PMC6939357</pub-id><pub-id pub-id-type="pmid">31840587</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0299</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quiroga</surname><given-names>RQ</given-names></name><name><surname>Nadasdy</surname><given-names>Z</given-names></name><name><surname>Ben-Shaul</surname><given-names>Y</given-names></name></person-group><article-title>Unsupervised spike detection and sorting with wavelets and superparamagnetic clustering</article-title><source>Neural computation</source><year>2004</year><volume>16</volume><issue>8</issue><fpage>1661</fpage><lpage>1687</lpage><pub-id pub-id-type="pmid">15228749</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahimi</surname><given-names>S</given-names></name><name><surname>Farahibozorg</surname><given-names>SR</given-names></name><name><surname>Jackson</surname><given-names>R</given-names></name><etal/></person-group><article-title>Task modulation of spatiotemporal dynamics in semantic brain networks: An EEG/MEG study</article-title><source>NeuroImage</source><year>2022</year><volume>246</volume><elocation-id>118768</elocation-id><pub-id pub-id-type="pmcid">PMC8784826</pub-id><pub-id pub-id-type="pmid">34856376</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.118768</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><etal/></person-group><article-title>The neural and computational bases of semantic cognition</article-title><source>Nature Reviews Neuroscience</source><year>2017</year><volume>18</volume><issue>1</issue><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remington</surname><given-names>ED</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><etal/></person-group><article-title>Flexible Sensorimotor Computations through Rapid Reconfiguration of Cortical Dynamics</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>5</issue><fpage>1005</fpage><lpage>1019</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC6009852</pub-id><pub-id pub-id-type="pmid">29879384</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.020</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutten</surname><given-names>S</given-names></name><name><surname>Santoro</surname><given-names>R</given-names></name><name><surname>Hervais-Adelman</surname><given-names>A</given-names></name><etal/></person-group><article-title>Cortical encoding of speech enhances task-relevant acoustic information</article-title><source>Nature Human Behaviour</source><year>2019</year><volume>3</volume><issue>9</issue><fpage>974</fpage><lpage>987</lpage><pub-id pub-id-type="pmid">31285622</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>MF</given-names></name><name><surname>Kimberg</surname><given-names>DY</given-names></name><name><surname>Walker</surname><given-names>GM</given-names></name><etal/></person-group><article-title>Anterior temporal involvement in semantic word retrieval: Voxel-based lesion-symptom mapping evidence from aphasia</article-title><source>Brain</source><year>2009</year><volume>132</volume><issue>12</issue><fpage>3411</fpage><lpage>3427</lpage><pub-id pub-id-type="pmcid">PMC2792374</pub-id><pub-id pub-id-type="pmid">19942676</pub-id><pub-id pub-id-type="doi">10.1093/brain/awp284</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SK</given-names></name></person-group><article-title>Identification of a pathway for intelligible speech in the left temporal lobe</article-title><source>Brain</source><year>2000</year><volume>123</volume><issue>12</issue><fpage>2400</fpage><lpage>2406</lpage><pub-id pub-id-type="pmcid">PMC5630088</pub-id><pub-id pub-id-type="pmid">11099443</pub-id><pub-id pub-id-type="doi">10.1093/brain/123.12.2400</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjerps</surname><given-names>MJ</given-names></name><name><surname>Fox</surname><given-names>NP</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><etal/></person-group><article-title>Speaker-normalized sound representations in the human auditory cortex</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC6549175</pub-id><pub-id pub-id-type="pmid">31165733</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-10365-z</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name></person-group><article-title>From point process observations to collective neural dynamics: Nonlinear Hawkes process GLMs, low-dimensional dynamics and coarse graining</article-title><source>Journal of Physiology-Paris</source><year>2016</year><volume>110</volume><issue>4</issue><fpage>336</fpage><lpage>347</lpage><pub-id pub-id-type="pmcid">PMC5610574</pub-id><pub-id pub-id-type="pmid">28336305</pub-id><pub-id pub-id-type="doi">10.1016/j.jphysparis.2017.02.004</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Dagnino</surname><given-names>B</given-names></name><etal/></person-group><article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><issue>40</issue><fpage>14332</fpage><lpage>14341</lpage><pub-id pub-id-type="pmcid">PMC4210002</pub-id><pub-id pub-id-type="pmid">25205811</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1402773111</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vignali</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Turini</surname><given-names>J</given-names></name><etal/></person-group><article-title>Spatiotemporal dynamics of abstract and concrete semantic representations</article-title><source>Brain and Language</source><year>2023</year><volume>243</volume><elocation-id>105298</elocation-id><pub-id pub-id-type="pmid">37399687</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visser</surname><given-names>M</given-names></name><name><surname>Lambon Ralph</surname><given-names>MA</given-names></name></person-group><article-title>Differential Contributions of Bilateral Ventral Anterior Temporal Lobe and Left Anterior Superior Temporal Gyrus to Semantic Processes</article-title><source>Journal of Cognitive Neuroscience</source><year>2011</year><volume>23</volume><issue>10</issue><fpage>3121</fpage><lpage>3131</lpage><pub-id pub-id-type="pmid">21391767</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vyas</surname><given-names>S</given-names></name><name><surname>Golub</surname><given-names>MD</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><etal/></person-group><article-title>Computation Through Neural Population Dynamics</article-title><source>Annual Review of Neuroscience</source><year>2020</year><volume>43</volume><issue>1</issue><fpage>249</fpage><lpage>275</lpage><pub-id pub-id-type="pmcid">PMC7402639</pub-id><pub-id pub-id-type="pmid">32640928</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>HG</given-names></name><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>The Encoding of Speech Sounds in the Superior Temporal Gyrus</article-title><source>Neuron</source><year>2019</year><volume>102</volume><issue>6</issue><fpage>1096</fpage><lpage>1110</lpage><pub-id pub-id-type="pmcid">PMC6602075</pub-id><pub-id pub-id-type="pmid">31220442</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.04.023</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Ding</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Hierarchical cortical networks of “voice patches” for processing voices in human brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>52</issue><elocation-id>e2113887118</elocation-id><pub-id pub-id-type="pmcid">PMC8719861</pub-id><pub-id pub-id-type="pmid">34930846</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2113887118</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Semantic and phonetic encoding at the single-unit level in the aSTG.</title><p><bold>a</bold>. Locations of the microelectrode array (square) and the ECoG array (circles) on the cortical surface. <bold>b</bold>. Experimental design of the auditory semantic categorization task. The participant heard 400 nouns that denoted either animals or objects and was instructed to press a button if the item was bigger than a foot in size. <bold>c</bold>. fMRI correlates of phonetic processing obtained from fMRI large-scale database (<xref ref-type="bibr" rid="R26">Dockès et al, 2020</xref>). <bold>d</bold>. fMRI correlates of semantic categorization. <bold>e</bold>. mTRF features for the example English word "llama". The time series at the top indicates the speech envelope. The features used in the baseline model are indicated by a dotted square. <bold>f</bold>. Pearson correlation coefficient (r values) for each model fitted to each of the 23 single units with the highest firing rates in the ensemble. The lines for different units are color-coded based on firing rate, from lower (blue) to higher (yellow). Red stars indicate units for which r values of the fitted models are significantly higher (p &lt; 0.05) than the chance level r values of a surrogate distribution. <bold>g-j</bold> Encoding of phoneme onsets, phonemes, phonetic categories, and semantic decision across ECoG channels. Colors indicate differences in r values compared to the baseline model. Red stars indicate significance as above.</p></caption><graphic xlink:href="EMS190467-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Distributed encoding and clustering of phonemes along phonetic categories.</title><p><bold>a</bold>. Average across mTRF phoneme kernels. The 95% confidence region of the surrogate (chance-level) distribution is shown with brighter shading for increasingly peripheral percentiles. Red segments indicate significant periods after multiple comparison correction (cluster-based test). The averaged phoneme kernel was significant at two time periods, centered around 200 and 400 ms, with the second period surviving multiple comparisons. <bold>b</bold>. mTRF kernels identified on each single unit for one example phoneme /k/. The units are sorted increasingly by their mean firing rate. Brighter colors indicate higher values of the kernel. These kernels are projected into a two-dimensional space using principal component analysis (PCA), resulting in a single phoneme trajectory. Numbers along the trajectory indicate the time points at which the trajectory reached the corresponding location. <bold>c</bold>. PCA variance explained. Four PCs explained 50% of variance (dotted line) and 15 explained 90% (dashed line). <bold>d</bold>. Principal component (PC) coefficients of isolated single units. Several units are represented in the first PCs, indicating distributed encoding of phonemes. <bold>e</bold>. Schematics showing how the clustering index is computed. Intra-cluster distance (full lines) was subtracted from between-cluster distances (bold lines). <bold>f</bold>. Clustering index for vowels grouped by the first formant (high and low tongue position). Shading and red segments are as in a. This clustering index was significant during a time interval centered around 200 ms. <bold>g</bold>. Distribution of vowels in the two-dimensional PC space at 0 and 200 ms. Vowels are color-coded based on their first formant. Mirroring the increase in the clustering index, the separation between those two phonetic features in the PC space was absent at 0 ms, and became evident at 200 ms. <bold>h</bold>. Clustering index for vowels grouped by the second formant did not reveal any significant periods. <bold>i</bold>. Clustering index for consonants grouped by the manner of articulation revealed two significant periods, centered around 200 and 400 ms. <bold>j</bold>. Clustering index for consonants grouped by the place of articulation did not reveal any significant periods.</p></caption><graphic xlink:href="EMS190467-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Generalization to natural speech perception</title><p><bold>a</bold>. r values for each model and the 23 most-spiking units. The units are color-coded based on their firing rate, from blue (lower) to yellow (higher). Red stars indicate model-unit pairs significantly higher than the baseline in a permutation test. <bold>b</bold>. Average across mTRF phoneme kernels. The 95% confidence region of the surrogate (chance-level) distribution is shown with brighter shading for increasingly peripheral percentiles. The averaged phoneme kernel was significant at two time periods, centered around 200 and 400 ms. <bold>c</bold>. PCA variance explained. Five PCs explained 50% of variance (dotted line) and 15 explained 90% (dashed line). <bold>d</bold>. Principal component (PC) coefficients of identified units. Several units are represented in the first PCs, indicating distributed encoding of phonemes also during natural speech perception. <bold>e-h</bold>. Clustering index for the four groups of phonetic features. Shading as in b. Red segments indicate significant periods after multiple comparison correction (cluster-based test). <bold>i</bold>. Overview of the significant clustering peaks observed both during the semantic task and natural speech perception. <bold>j</bold>. Canonical correlation analysis (CCA) between the PC projections of phoneme kernels obtained during the semantic task and natural speech perception. Shading and red segments as in <bold>b</bold>. CCA revealed significant correlations for the first nine PCs.</p></caption><graphic xlink:href="EMS190467-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Encoding of semantic features.</title><p><bold>a</bold>. Semantic feature kernels of the semantic task. The 95% confidence region of the surrogate (chance-level) distribution is shown with brighter shading for increasingly peripheral percentiles. Red segments indicate significant periods after multiple comparison correction (cluster-based test). The perceptual category feature was significantly encoded during a time window centered at about 400 ms. <bold>b</bold>. Semantic kernels of natural speech perception. Shading and red segments as in a. Word class was processed at about 150 ms, and lexical semantics was processed in a broad window at about 400 ms. <bold>c</bold>. Euclidean distances in the PC space between the semantic task feature kernels. The perceptual semantic feature kernels were separated in the PC space slightly before the categorical semantic feature kernels (at about 400 and 450 ms, respectively). <bold>d</bold>. Euclidean distances in the PC space between the semantic features during natural speech perception were maximal at about 150 ms and 400 ms respectively. Shading and red segments as in <bold>a</bold>.</p></caption><graphic xlink:href="EMS190467-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Concurrent encoding of bottom-up phonetic and top-down semantic features</title><p><bold>a</bold>. Kernels for position-based phoneme onsets (green) within a word shifted by average phoneme duration (80 ms) and aligned with the lexical semantic kernel at word onset (brown). The 95% confidence region of the surrogate (chance-level) distribution is shown with brighter shading for increasingly peripheral percentiles. Red segments indicate significant periods after multiple comparison correction (cluster- based test). Aligned phoneme onset kernels peaked simultaneously with the lexical semantic kernel peak (dashed rectangle). <bold>b</bold>. Beta band kernel for word onset is indicated in brown, and low-gamma kernels for position-based phoneme onsets are indicated in green and shifted by average phoneme duration. Shading and red segments as in a. The late peak in the beta word-onset kernel at about 400 ms occurred simultaneously with the aligned low-gamma phoneme onset peaks (dashed rectangle). <bold>c</bold>. Word onset kernels for firing rate (left) and broadband high-frequency activity (right). Despite the absence of the late effects on the firing rate kernel, significant broadband high-frequency activity was observed after 400 ms. <bold>d</bold>. Granger causality between phonetic and semantic low-dimensional representations. The left plot shows a 1.5-second snippet of the spiking data recorded during natural speech perception. These are projected onto three PCs to obtain time-varying phonetic (P) and semantic (S) features, which are then used in the Granger causality analysis. Matrices indicate p-values of Granger's F-test for a causal relationship from the dimensions indicated on the x-axis to the dimensions indicated on the y-axis. The stars indicate significant relationships at the 0.05 threshold. There was a significant causal relationship in both directions (i.e. phonetic to semantic and vice versa).</p></caption><graphic xlink:href="EMS190467-f005"/></fig></floats-group></article>