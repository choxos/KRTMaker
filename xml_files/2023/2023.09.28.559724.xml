<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS188921</article-id><article-id pub-id-type="doi">10.1101/2023.09.28.559724</article-id><article-id pub-id-type="archive">PPR734381</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Learning patterns of HIV-1 co-resistance to broadly neutralizing antibodies with reduced subtype bias using multi-task learning</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Igiraneza</surname><given-names>Aime Bienfait</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Zacharopoulou</surname><given-names>Panagiota</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Hinch</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wymant</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Abeler-Dörner</surname><given-names>Lucie</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Frater</surname><given-names>John</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Fraser</surname><given-names>Christophe</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Pandemic Sciences Institute, Nuffield Department of Medicine, University of Oxford, Oxford, UK</aff><aff id="A2"><label>2</label>Big Data Institute, Li Ka Shing Centre for Health Information and Discovery, Nuffield Department of Medicine, University of Oxford, Oxford, UK</aff><aff id="A3"><label>3</label>Peter Medawar Building for Pathogen Research, Nuffield Department of Medicine, University of Oxford</aff><aff id="A4"><label>4</label>NIHR Oxford Biomedical Research Centre, Oxford University Hospitals NHS Foundation Trust, John Radcliffe Hospital, Oxford, UK</aff><author-notes><corresp id="CR1"><label>*</label>Correspondence to: <email>aime.igiraneza@gtc.ox.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>30</day><month>09</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The ability to predict HIV-1 resistance to broadly neutralizing antibodies (bnAbs) will increase bnAb therapeutic benefits. Machine learning is a powerful approach for such prediction. One challenge is that some HIV-1 subtypes in currently available training datasets are underrepresented, which likely affects models’ generalizability across subtypes. A second challenge is that combinations of bnAbs are required to avoid the inevitable resistance to a single bnAb, and computationally determining optimal combinations of bnAbs is an unsolved problem. Recently, machine learning models trained using resistance outcomes for multiple antibodies at once, a strategy called multi-task learning (MTL), have been shown to achieve better performance in several cases than previous approaches. We develop a new model and show that, beyond the boost in performance, MTL also helps address the previous two challenges. Specifically, we demonstrate empirically that MTL can mitigate bias from underrepresented subtypes, and that MTL allows the model to learn patterns of co-resistance between antibodies, thus providing tools to predict antibodies’ epitopes and to potentially select optimal bnAb combinations. Our analyses, publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/iaime/LBUM">https://github.com/iaime/LBUM</ext-link>, can be adapted to other infectious diseases that are treated with antibody therapy.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Broadly neutralizing antibodies (bnAbs) exhibiting exceptional breadth and potency have revived the hope for immunotherapy against HIV-1 (<xref ref-type="bibr" rid="R1">1</xref>). To neutralize most viruses and to prevent viral escape, bnAbs will likely be given in combinations. For example, one cocktail, made of 3BNC117 and 10-1074, has achieved viral suppression for roughly 20 weeks without antiretroviral therapy in 9 out of 11 individuals and 13 out 17 individuals participating in two phase 1b clinical trials, respectively (<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref>). Nonetheless, the general question of which bnAbs to administer together to achieve maximum efficacy is still outstanding.</p><p id="P3">Given that bnAbs target HIV’s envelope glycoprotein (Env), neutralization assays are traditionally used to determine the breadth and potency of different bnAbs against panels of Env-pseudotyped viruses (<xref ref-type="bibr" rid="R4">4</xref>). For each pseudovirus, these experiments determine the bnAb concentration needed to reduce infectivity by 50% or 80% (i.e., IC<sub>50</sub> or IC<sub>80</sub>, respectively). These assays are expensive and slow. In particular, when the goal is to identify bnAbs that are likely to neutralize most viruses in a given population, there is the need for scalable computational methods to predict Env sequences’ sensitivity to bnAbs.</p><p id="P4">Several machine learning (ML) models (<xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R12">12</xref>) to map Env sequences to bnAb susceptibility have been developed using neutralization data compiled in the web server CATNAP (<xref ref-type="bibr" rid="R13">13</xref>). The generalizability of these methods beyond the training data is unclear, as the training datasets have HIV-1 subtype compositions that are unrepresentative of large epidemics in sub-Saharan Africa (<xref ref-type="supplementary-material" rid="SD1">Fig S1</xref>) (<xref ref-type="bibr" rid="R14">14</xref>), where the two thirds of people living with HIV-1 worldwide reside (<xref ref-type="bibr" rid="R15">15</xref>). This is particularly worrying since susceptibility to bnAbs can be subtype-dependent (<xref ref-type="bibr" rid="R16">16</xref>).</p><p id="P5">Some of the most recent ML models in predicting HIV-1 resistance to many bnAbs use multi-task learning (MTL) (<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R12">12</xref>). The premise of MTL is that information from different but related tasks is beneficial to specific tasks of interest (<xref ref-type="bibr" rid="R17">17</xref>). In this context one model is trained using neutralization outcomes for multiple antibodies at once, as opposed to only considering one antibody per model. Here we show that, in addition to a boost in performance in some cases, MTL provides solutions, at least partially, to the challenges related to data imbalances and to the selection of optimal bnAb combinations.</p><p id="P6">Specifically, we empirically show that: a) MTL can mitigate bias against underrepresented HIV-1 subtypes; b) MTL allows learning patterns of co-resistance between antibodies, thus providing tools to predict antibodies’ epitopes and to potentially select optimal bnAb combinations.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Model rationale</title><p id="P7">A common modeling choice is to align Env sequences and treat a site in the alignment as a categorical variable (5–8,10,11). However, Env is highly variable, thus making multiple sequence alignment very challenging. Natural language processing (NLP) techniques offer alignment-free methods, which leverage the distributional hypothesis originating from linguistics (<xref ref-type="bibr" rid="R18">18</xref>). The hypothesis stipulates that similar words tend to occur in similar contexts. This allows language models trained on large corpora to learn semantically meaningful vector representations of words, called word embeddings. In the case of modeling protein sequences, each amino acid can be treated as a word whose embedding is learned based on its co-occurrences with other amino acids in many sequences. Importantly, the embeddings need not be fixed for each amino acid, but can rather vary depending on the rest of the sequence, resulting in contextualized embeddings.</p><p id="P8">Using architectural details from (<xref ref-type="bibr" rid="R19">19</xref>), we trained a base Env language model to learn contextualized embeddings. This task consisted in predicting each amino acid in the sequence given the rest of the sequence. The average of these embeddings across all amino acids in a sequence can be understood as the overall vector representing the sequence. Such vectors can be used to explain variations between sequences (<xref ref-type="bibr" rid="R19">19</xref>). This phase of training the base model is what we call “pretraining,” which only requires Env sequences, without any neutralization data attached to them. In this work, we pretrained using 71390 Env sequences from the Los Alamos National Laboratory HIV Sequence Database (<ext-link ext-link-type="uri" xlink:href="https://www.hiv.lanl.gov/">https://www.hiv.lanl.gov/</ext-link>). As many sequences without neutralization data are available, we hypothesized that pretraining would potentially improve the model’s generalizability, in addition to making the model learn alignment-free sequence encodings.</p><p id="P9">The second component of an input to a MTL model is the antibody of interest. Inspired by works in NLP (<xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref>), we represent each antibody by a unique vector. We call this vector an antibody context. Based on the distributional hypothesis, we reasoned that differences between learned antibody contexts would encode correlations between antibodies’ resistance profiles, thus offering insights into potential optimal bnAb combinations. For simplicity, we did not consider antibody sequences themselves, unlike in (<xref ref-type="bibr" rid="R12">12</xref>). Instead, antibody contexts were randomly initialized and tuned using neutralization data linking antibodies to Env sequences in the training data. The resulting MTL model is what we refer to as a language-based universal model (LBUM) (<xref ref-type="fig" rid="F1">Fig 1</xref>). Further details of the model are given in <italic>Methods</italic>.</p></sec><sec id="S4"><title>No single model dominates across all bnAbs</title><p id="P10">We considered 33 bnAbs targeting five different epitopes: the membrane-proximal external region (MPER), the gp120-gp41 interface, the CD4 binding sites (CD4bs), the third constant region and the third variable loop (C3/V3), and the first and second variable loops (V1/V2) (<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R23">23</xref>). The first two columns of <xref ref-type="table" rid="T1">Table 1</xref> show the epitope that each of the 33 bnAbs targets.</p><p id="P11">Our aim was not to simply develop models that predict HIV-1 resistance to the 33 bnAbs; however, we still compared the LBUM to models developed with classical machine learning algorithms, namely random forests (RF) and gradient boosting machines (GBM). We caution against comparisons to previously published performances since CATNAP data has changed over time, and preprocessing and model-selection techniques vary across publications (<xref ref-type="bibr" rid="R24">24</xref>).</p><p id="P12">We assessed models using three metrics: the area under the receiver operating characteristic curve (AUC), interpreted as the probability that a model ranks resistant sequences above sensitive ones; the area under the precision-recall curve (PR AUC), which measures how the model trades off precision for sensitivity, an important metric especially when resistance sequences are rare; and the binary cross-entropy (Log Loss), which measures the difference between predicted resistance probabilities and the ground truth.</p><p id="P13">The LBUM achieved higher AUC and higher PR AUC than both RF and GBM models did on the same 12 bnAbs out of the 33 bnAbs (<xref ref-type="table" rid="T1">Table 1</xref>, <xref ref-type="supplementary-material" rid="SD1">S1 Table</xref>). Notably, 6 of those 12 bnAbs target the CD4bs. In terms of Log Loss, the LBUM scored better than both RF and GBM models did on 10 bnAbs, 4 of which target the CD4bs (<xref ref-type="supplementary-material" rid="SD1">S1 Table</xref>). Interestingly, the LBUM’s AUC and PR AUC on two of the 10 bnAbs (i.e., PGT135 and PGT128) was slightly lower than RF and GBM models’. Otherwise, for the remaining 8 bnAbs, the lower LBUM’s Log Loss meant higher AUC and higher PR AUC than RF and GBM models’.</p><p id="P14">Overall, there was no single model that consistently outperformed all other models across all bnAbs. Nevertheless, averaging predicted resistance probabilities from the three models—defining the ensemble model, ENS—mitigated underperformances from individual models. The ensemble model achieved the highest AUC on 24 bnAbs (<xref ref-type="table" rid="T1">Table 1</xref>), the highest PR AUC on 21 bnAbs (<xref ref-type="supplementary-material" rid="SD1">S1 Table</xref>), and the lowest Log Loss on 22 bnAbs (<xref ref-type="supplementary-material" rid="SD1">S1 Table</xref>).</p></sec><sec id="S5"><title>Multi-task learning can mitigate HIV-1 subtype bias</title><p id="P15">Publicly available training datasets are very imbalanced in terms of HIV-1 subtypes (<xref ref-type="supplementary-material" rid="SD1">Fig S1</xref>), which can compromise models’ generalizability to underrepresented subtypes, a problem we call “subtype bias” hereafter. The LBUM uses, in addition to the usual bnAb data, large numbers of Env sequences with no neutralization data, and also data from antibodies not deemed as bnAbs. We hypothesized that both of these data sources help to mitigate subtype bias, because they have more balanced availability across subtypes than bnAb data. To test the two aspects separately, one would ideally vary the composition of subtypes at different training stages. However, only subtype B and subtype C had sufficient data to run meaningful tests (<xref ref-type="supplementary-material" rid="SD1">Fig S1</xref>). In addition, data on bnAbs NIH45-46, VRC13, VRC07, and VRC26.08 was excluded from this analysis because it lacked enough subtype and phenotype diversity.</p><p id="P16">To quantify the level of subtype bias, we trained two models, one with only subtype B data and one with only subtype C data. We then evaluated the models on the subtype B and subtype C bnAb testing sets separately. With the exception of gp120-gp41 bnAbs (n=4) and MPER bnAbs (n=2), the AUC was greater by roughly 0.3 on the matched subtype than on the unmatched (<xref ref-type="fig" rid="F2">Fig 2A</xref>). The model trained on both subtypes did equally well at classifying both subtypes (<xref ref-type="fig" rid="F2">Fig 2A</xref>). While experimental data from antibody neutralization assays was not available for all subtypes, there was sequence data for all subtypes. To test whether using the additional sequence data improved the generalizability of the models, we re-trained the subtype-specific model but included both subtypes in the initial pretraining step. Unfortunately, using the additional sequences improved the generalizability only minimally, if at all (<xref ref-type="fig" rid="F2">Fig 2B</xref>).</p><p id="P17">While neutralization data for bnAbs is limited, there is often data for other antibodies, which we label as “non-bnAbs”. We tested whether including this non-bnAb data in the training of the subtype-specific models improved their generalizability. Except on gp120-gp41 bnAbs, these models showed much greater generalizability, with the difference in AUC between the two subtypes dropping to roughly 0.1 or less in many cases (<xref ref-type="fig" rid="F2">Fig 2C</xref>).</p><p id="P18">PR AUC and Log Loss generally showed similar patterns of subtype bias to those seen for AUC (<xref ref-type="supplementary-material" rid="SD1">Fig S2 and Fig S3</xref>). That is, subtype representativeness in non-bnAb data improved PR AUC on the unmatched subtype (<xref ref-type="supplementary-material" rid="SD1">Fig S2C &amp; Fig S3C</xref>), while pretraining with the subtype of interest had very minimal effects on subtype bias (<xref ref-type="supplementary-material" rid="SD1">Fig S2B &amp; Fig S3B</xref>). The exceptions for gp120-gp41 and MPER bnAbs remained. We also note that Log Loss discrepancy did not change as much on C3/V3 bnAbs for some models, despite subtype representativeness in non-bnAb data (<xref ref-type="supplementary-material" rid="SD1">Fig S3C</xref>).</p></sec><sec id="S6"><title>Do the learned antibody contexts encode co-resistance patterns?</title><p id="P19">If learned antibody contexts encode co-resistance patterns, we would expect many bnAbs targeting similar epitopes to have similar contexts, given that bnAbs targeting similar epitopes tend to have similar resistance patterns (<xref ref-type="bibr" rid="R16">16</xref>). Clustering by epitope could be observed after projecting the dimensionality of the antibody contexts to a two-dimensional space (<xref ref-type="fig" rid="F3">Fig 3A-E</xref>). Without any further training we could predict bnAb epitopes solely based on the epitope targeted by the closest bnAb in that context space with at least 72% accuracy (<xref ref-type="table" rid="T2">Table 2</xref>). We defined closeness between bnAbs in terms of cosine similarity, L1 distance and L2 distance between their context vectors. In at least 85% of cases, at least one of the 5 closest bnAbs targeted the same epitope as the bnAb in question (<xref ref-type="table" rid="T2">Table 2</xref>), further suggesting that antibody contexts captured epitope-specific resistance patterns.</p><p id="P20">Although bnAbs targeting similar epitopes generally tend to have similar resistance profiles, that is not always the case. Indeed, we observed outliers in epitope clusters (<xref ref-type="fig" rid="F3">Fig 3</xref>). We therefore investigated whether such within-epitope dissimilarities imply different resistance patterns among bnAbs targeting similar epitopes. A known example of dissimilar patterns within the V1/V2 epitope was captured by learned bnAb contexts: VRC26.08 and VRC26.25 clustered away from the rest of V1/V2 bnAbs (<xref ref-type="fig" rid="F3">Fig 3A,B,C,E</xref>). Contrary to the rest of V1/V2 bnAbs, the potency of CAP256-VRC26 bnAbs, which include the two bnAbs, is known to be inversely dependent on the presence of a glycan at the N160 position in Env (<xref ref-type="bibr" rid="R25">25</xref>). VRC38.01 was also another outlier in the V1/V2 cluster (<xref ref-type="fig" rid="F3">Fig 3B,C&amp;E</xref>). Unlike the other V1/V2 bnAbs, VRC38.01 has a unique binding mode that allows it to have non-protruding heavy-chain complementarity-determining region 3 (HCDR3) (<xref ref-type="bibr" rid="R26">26</xref>). Furthermore, b12 and HJ16 clustered away from the rest of CD4bs bnAbs (<xref ref-type="fig" rid="F3">Fig 3A-D</xref>). Unlike VRC01-like bnAbs, b12 does not mimic CD4 binding and binds Env in its relaxed conformation (<xref ref-type="bibr" rid="R27">27</xref>). On the other hand, HJ16 interferes with CD4 by binding to a glycan at site N270 in Env. The mutation N270D, which removes the glycan, makes viruses resistant to HJ16, while making them more sensitive to other CD4bs bnAbs such as VRC01 and VRC03 (<xref ref-type="bibr" rid="R28">28</xref>). Another interesting dissimilarity was between 2F5 and 4E10, which target different MPER regions (<xref ref-type="bibr" rid="R29">29</xref>). Mutations with opposing effects on resistances to 2F5 versus 4E10 have been reported (<xref ref-type="bibr" rid="R16">16</xref>).</p><p id="P21">Finally, we note that some bnAbs, such as 2F5 and b12 (<xref ref-type="fig" rid="F3">Fig 3B&amp;C</xref>), appeared to have similar bnAb contexts despite targeting different epitopes. Whether such cases imply cross-epitope resistance correlation is an interesting question, which we leave for future work.</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P22">In summary, we developed a model to predict the neutralization of different HIV-1 Env sequences by different broadly neutralizing antibodies (bnAbs). Our model, which we named a language-based universal model (LBUM), is a type of multi-task learning (MTL) model. The LBUM was pretrained using Env sequences with no associated neutralization data and fine-tuned with Env sequences with both non-bnAb and bnAb outcome data. We first showed that the LBUM’s performance is comparable to that of Gradient Boosting Machine (GBM) models and Random Forest (RF) models, with some improvements over both methods (<xref ref-type="table" rid="T1">Table 1</xref>, <xref ref-type="supplementary-material" rid="SD1">S1 Table</xref>). Unlike the other two methods, the LBUM does not require aligning input Env sequences, which is an advantage given the incredible variability of Env, including structural variability that makes alignment challenging. As for previous methods, all models in this work were trained to predict <italic>in vitro</italic> bnAb resistance: we did not validate them with clinical outcomes, and we relied on data showing correlations between <italic>in vitro</italic> susceptibility to bnAbs and <italic>in vivo</italic> outcomes (<xref ref-type="bibr" rid="R30">30</xref>).</p><p id="P23">A thorough systematic comparison between published methods requires testing different combinations of preprocessing techniques, feature selection methods and learning algorithms. In this work, we only compared learning algorithms applied on full Env sequences with neutralization data preprocessed similarly. We compared the LBUM to both RF and GBM models because both boosting trees and RF underlie recently published methods that do not use neural networks (<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R7">7</xref>).</p><p id="P24">The most common subtypes in sub-Saharan Africa are A, C, D and several circulating recombinant forms (CRFs) (<xref ref-type="bibr" rid="R14">14</xref>). CATNAP, from which most of the training datasets come, has mostly subtype B and subtype C sequences (<xref ref-type="supplementary-material" rid="SD1">Fig S1</xref>). This subtype mismatch is problematic because, as we have shown, models do not necessarily generalize across subtypes (<xref ref-type="fig" rid="F2">Fig 2A</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig S2A, Fig S3A</xref>). MTL and pretraining give access to more data with potentially more subtype representativeness. Although no solution trumps having all subtypes represented in bnAb data, our results suggest that MTL can alleviate subtype bias if neutralization data with all subtypes is available for antibodies not considered bnAbs (<xref ref-type="fig" rid="F2">Fig 2B</xref>, <xref ref-type="supplementary-material" rid="SD1">Fig S2B, Fig S3B</xref>).</p><p id="P25">We introduced the concept of antibody contexts, which we defined as vector representations unique to each antibody and updated during the fine-tuning process. We showed that bnAbs targeting similar epitopes tended to have similar contexts, to such an extent that we could use closeness between antibody contexts to predict antibody epitopes (<xref ref-type="table" rid="T2">Table 2</xref>). In this regard, our methods can be used to generate hypotheses about epitopes targeted by new antibodies, as long as relevant neutralization data is part of the LBUM’s training data. Nonetheless, some bnAbs had distant contexts despite targeting similar epitopes (<xref ref-type="fig" rid="F3">Fig 3</xref>), and we highlighted known mechanistic reasons that support our hypothesis that differences in antibody contexts capture differences in resistance profiles. A possible limitation is that negatively correlated resistance profiles can possibly lead to similar bnAb contexts, the same way antonyms can have similar word embeddings in the English language (<xref ref-type="bibr" rid="R31">31</xref>). Nonetheless, we assumed that such cases were rare if present at all, given that most bnAb contexts tended to cluster per epitope. Analyses of structural data on antibodies and their respective targets on Env will help further show the extent to which antibody contexts capture co-resistance patterns.</p><p id="P26">An interesting extension of our methods could be to pretrain using generic protein language models, such as those in the BERT and ESM families (<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref>). We expect MTL models’ performance to increase as their size increases along with the increase in the quantity and diversity of their training data. We chose small architectures because of computational requirements imposed by deep neural network models and because of the availability of only small amounts of data on which to fine-tune.</p><p id="P27">The potential of MTL revealed in our study addresses key challenges in HIV-1 vaccine research. All models developed in this work, along with the used code, can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/iaime/LBUM">https://github.com/iaime/LBUM</ext-link>. The framework presented here is a starting point towards designing effective immunotherapies. We hope that our analyses can be relevant to other infectious diseases for which monoclonal antibodies are being explored as therapeutic solutions.</p></sec><sec id="S8" sec-type="materials | methods"><title>Materials and methods</title><sec id="S9"><title>Data preprocessing</title><p id="P28">We binarized the neutralization outcome—resistant or sensitive—i.e., we aimed to predict whether positive neutralization is observed within a certain range of antibody concentrations. This is because the main use-case envisioned for our models is the identification of bnAbs that are likely to neutralize most viruses in given populations. Once bnAbs with largest coverage are identified, other methods will need to be used to determine the bnAbs’ exact potencies.</p><p id="P29">We determined the phenotype based on IC<sub>50</sub> because this was more commonly available than IC<sub>80</sub> for Env sequences in CATNAP. We transformed left-censored IC<sub>50</sub> values to the detection threshold. That is, &lt;x values became x. Since CATNAP Env sequences could have multiple IC<sub>50</sub> values from different studies, we calculated the geometric mean whenever more than one value was available, as long as none of the values was right-censored. If any reported IC<sub>50</sub> value for a sequence-antibody pair was right-censored, the sequence was deemed resistant to the antibody, unless the detection threshold was lower than 10 μg/mL, in which case the sequence-antibody entry was discarded. If no right-censored IC<sub>50</sub> values were recorded for the sequence-antibody entry and if the geometric mean IC<sub>50</sub> was greater than 50 μg/mL, the sequence was also labeled as resistant to the antibody. Otherwise, the sequence was labeled as sensitive to that antibody (i.e., sensitive sequences had no right-censored IC<sub>50</sub> values and the geometric mean IC<sub>50</sub> was less than 50 μg/mL).</p><p id="P30">In all our experiments, we only considered sequences that are 800 to 900 amino acid long (ignoring non-amino-acid characters) to match the expected length of a full Env sequence. Part of our analysis compared our model to random forests (RF) and gradient boosting machines (GBM) models. Since RF and GBM models required aligned sequences, we used the alignment provided in CATNAP to one-hot encode sequences. That is, each amino acid was represented as a vector of all zeros except a 1 at the index of that amino acid. For non-amino acid characters, the vector was all zeros. The LBUM did not require aligning sequences, and all non-amino acid characters were removed from their input sequences.</p></sec><sec id="S10"><title>Gradient Boosting Machines and Random Forests</title><p id="P31">Both GBM and RF build ensemble models based on decision trees. For complete mathematical descriptions of GBM and RF, we refer to (<xref ref-type="bibr" rid="R34">34</xref>) and (<xref ref-type="bibr" rid="R35">35</xref>), respectively. 5-fold nested cross-validation was used to select and evaluate both types of models. Log Loss was used to select the best classifiers. <xref ref-type="table" rid="T3">Table 3</xref> provides all hyperparameters considered. Both GBM and RF models were implemented using scikit-learn (v1.1.1) (<xref ref-type="bibr" rid="R36">36</xref>).</p></sec><sec id="S11"><title>Language-based universal model (LBUM)</title><p id="P32">The overall architecture of the proposed LBUM is illustrated in <xref ref-type="fig" rid="F1">Fig 1</xref> and rationalized in the <italic>Results</italic> section. There were two main steps in the development of the LBUM, namely pretraining and fine-tuning. First, we pretrained the model using the same architectural and training details as Hie <italic>et al</italic>’s method (<xref ref-type="bibr" rid="R19">19</xref>). Essentially, the method was a two-layer bidirectional Long Short-Term Memory (LSTM) model (<xref ref-type="bibr" rid="R37">37</xref>). Next, we fine-tuned the LBUM using data on 378 antibodies in addition to the 33 bnAbs of interest. During fine-tuning, we froze all pretrained layers except the last right and left LSTM layers, and we applied early stopping with a 10-epoch patience.</p><p id="P33">As a regularization technique, we added a secondary output layer in the LBUM that directly predicts log<sub>10</sub>(IC<sub>50</sub>). However, sequence-antibody pairs with IC<sub>50</sub> beyond the detection threshold (i.e., right-censored IC<sub>50</sub>) did not contribute towards the training of the regression branch. A question not addressed here is how to incorporate censored data into the training data of models that predict IC<sub>50</sub>. For now, we recommend against making predictions with the regression branch of the trained model, as it cannot be relied on given its biased training data. The LBUM’s overall loss function was simply the weighted average of binary cross-entropy and mean squared error, with weights of 0.6 and 0.4, respectively.</p><p id="P34">The dimension of antibody context vectors was set to 128. These vectors were fine-tuned through an attention mechanism that was a combination of at least three methods (<xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref>). To visualize the antibody contexts in <xref ref-type="fig" rid="F3">Fig 3</xref>, we used the Uniform Manifold Approximation and Projection algorithm (UMAP) (<xref ref-type="bibr" rid="R38">38</xref>). Below we detail the attention layer.</p><p id="P35">Let <italic>C</italic><sub><italic>t</italic></sub> be the context of an antibody t. Let <italic>E</italic><sub><italic>j</italic></sub> be the embedding of a token <italic>j</italic> in a sequence x of length <italic>n</italic>. The attention weight <italic>a</italic><sub><italic>j</italic></sub> to the token <underline>j</underline> given the antibody <italic>t</italic> context was calculated as follows: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>W</italic> and <italic>b</italic> are weight matrix and bias vector, respectively, and <italic>tann</italic> is the hyperbolic tangent used as an activation function. We note that <inline-formula><mml:math id="M4"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> for each sequence. The weighted average embedding <inline-formula><mml:math id="M5"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> was then input to a dense output layer. We added two dropout layers at a 10% rate, one before the attention layer and another before the final dense layer. We also used temperature scaling as the calibration method (<xref ref-type="bibr" rid="R39">39</xref>). Each antibody had its own temperature value initialized to 1.5 and optimized during the fine-tuning process. At inference time, we averaged predictions from running 10 forward passes with dropout turned on.</p><p id="P36">The LBUM hyperparameters were tuned on non-bnAb data to avoid data leakage. Using Bayesian optimization implemented in KeraTuner (<xref ref-type="bibr" rid="R40">40</xref>), we tuned the learning rate for the fine-tuning phase, the dimension of antibody context vectors, the dropout rate, the number of pretrained layers to unfreeze during fine-tuning, and the weights for the classification and regression output branches of the LBUM. Considered values for these hyperparameters are shown in <xref ref-type="table" rid="T3">Table 3</xref>. Values that achieved the lowest binary cross-entropy within 10 trials were chosen for the final model. All the other hyperparameters of the LBUM were set to default values in Tensorflow Keras (v2.12.0).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supporting information</label><media xlink:href="EMS188921-supplement-Supporting_information.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d91aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S12"><title>Acknowledgments</title><p>We thank all members of the Pathogen Dynamics Group in the Big Data institute at the University of Oxford for their helpful feedback during the course of this project. This work was supported by a Li Ka Shing Foundation Grant awarded to C.F. The computational aspects of this research were funded from the NIHR Oxford BRC with additional support from the Wellcome Trust Core Award Grant Number 203141/Z/16/Z. The views expressed are those of the author(s) and not necessarily those of the NHS, the NIHR or the Department of Health.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klein</surname><given-names>F</given-names></name><name><surname>Mouquet</surname><given-names>H</given-names></name><name><surname>Dosenovic</surname><given-names>P</given-names></name><name><surname>Scheid</surname><given-names>JF</given-names></name><name><surname>Scharf</surname><given-names>L</given-names></name><name><surname>Nussenzweig</surname><given-names>MC</given-names></name></person-group><article-title>Antibodies in HIV-1 Vaccine Development and Therapy</article-title><source>Science</source><year>2013</year><month>Sep</month><day>13</day><volume>341</volume><issue>6151</issue><fpage>1199</fpage><lpage>204</lpage><pub-id pub-id-type="pmcid">PMC3970325</pub-id><pub-id pub-id-type="pmid">24031012</pub-id><pub-id pub-id-type="doi">10.1126/science.1241144</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mendoza</surname><given-names>P</given-names></name><name><surname>Gruell</surname><given-names>H</given-names></name><name><surname>Nogueira</surname><given-names>L</given-names></name><name><surname>Pai</surname><given-names>JA</given-names></name><name><surname>Butler</surname><given-names>AL</given-names></name><name><surname>Millard</surname><given-names>K</given-names></name><etal/></person-group><article-title>Combination therapy with anti-HIV-1 antibodies maintains viral suppression</article-title><source>Nature</source><year>2018</year><month>Sep</month><volume>561</volume><issue>7724</issue><fpage>479</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC6166473</pub-id><pub-id pub-id-type="pmid">30258136</pub-id><pub-id pub-id-type="doi">10.1038/s41586-018-0531-2</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaebler</surname><given-names>C</given-names></name><name><surname>Nogueira</surname><given-names>L</given-names></name><name><surname>Stoffel</surname><given-names>E</given-names></name><name><surname>Oliveira</surname><given-names>TY</given-names></name><name><surname>Breton</surname><given-names>G</given-names></name><name><surname>Millard</surname><given-names>KG</given-names></name><etal/></person-group><article-title>Prolonged viral suppression with anti-HIV-1 antibody therapy</article-title><source>Nature</source><year>2022</year><month>Jun</month><volume>606</volume><issue>7913</issue><fpage>368</fpage><lpage>74</lpage><pub-id pub-id-type="pmcid">PMC9177424</pub-id><pub-id pub-id-type="pmid">35418681</pub-id><pub-id pub-id-type="doi">10.1038/s41586-022-04597-1</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montefiori</surname><given-names>DC</given-names></name></person-group><article-title>Measuring HIV neutralization in a luciferase reporter gene assay</article-title><source>Methods Mol Biol Clifton NJ</source><year>2009</year><volume>485</volume><fpage>395</fpage><lpage>405</lpage><pub-id pub-id-type="pmid">19020839</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rawi</surname><given-names>R</given-names></name><name><surname>Mall</surname><given-names>R</given-names></name><name><surname>Shen</surname><given-names>CH</given-names></name><name><surname>Farney</surname><given-names>SK</given-names></name><name><surname>Shiakolas</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><etal/></person-group><article-title>Accurate Prediction for Antibody Resistance of Clinical HIV-1 Isolates</article-title><source>Sci Rep</source><year>2019</year><month>Oct</month><day>11</day><volume>9</volume><elocation-id>14696</elocation-id><pub-id pub-id-type="pmcid">PMC6789020</pub-id><pub-id pub-id-type="pmid">31604961</pub-id><pub-id pub-id-type="doi">10.1038/s41598-019-50635-w</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hake</surname><given-names>A</given-names></name><name><surname>Pfeifer</surname><given-names>N</given-names></name></person-group><article-title>Prediction of HIV-1 sensitivity to broadly neutralizing antibodies shows a trend towards resistance over time</article-title><source>PLoS Comput Biol</source><year>2017</year><month>Oct</month><volume>13</volume><issue>10</issue><elocation-id>e1005789</elocation-id><pub-id pub-id-type="pmcid">PMC5669501</pub-id><pub-id pub-id-type="pmid">29065122</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005789</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williamson</surname><given-names>BD</given-names></name><name><surname>Magaret</surname><given-names>CA</given-names></name><name><surname>Gilbert</surname><given-names>PB</given-names></name><name><surname>Nizam</surname><given-names>S</given-names></name><name><surname>Simmons</surname><given-names>C</given-names></name><name><surname>Benkeser</surname><given-names>D</given-names></name></person-group><article-title>Super LeArner Prediction of NAb Panels (SLAPNAP): a containerized tool for predicting combination monoclonal broadly neutralizing antibody sensitivity</article-title><source>Bioinformatics</source><year>2021</year><month>Nov</month><day>15</day><volume>37</volume><issue>22</issue><fpage>4187</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC9502160</pub-id><pub-id pub-id-type="pmid">34021743</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btab398</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magaret</surname><given-names>CA</given-names></name><name><surname>Benkeser</surname><given-names>DC</given-names></name><name><surname>Williamson</surname><given-names>BD</given-names></name><name><surname>Borate</surname><given-names>BR</given-names></name><name><surname>Carpp</surname><given-names>LN</given-names></name><name><surname>Georgiev</surname><given-names>IS</given-names></name><etal/></person-group><article-title>Prediction of VRC01 neutralization sensitivity by HIV-1 gp160 sequence features</article-title><source>PLoS Comput Biol</source><year>2019</year><month>Apr</month><volume>15</volume><issue>4</issue><elocation-id>e1006952</elocation-id><pub-id pub-id-type="pmcid">PMC6459550</pub-id><pub-id pub-id-type="pmid">30933973</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006952</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conti</surname><given-names>S</given-names></name><name><surname>Karplus</surname><given-names>M</given-names></name></person-group><article-title>Estimation of the breadth of CD4bs targeting HIV antibodies by molecular modeling and machine learning</article-title><source>PLOS Comput Biol</source><year>2019</year><month>Apr</month><day>10</day><volume>15</volume><issue>4</issue><elocation-id>e1006954</elocation-id><pub-id pub-id-type="pmcid">PMC6457539</pub-id><pub-id pub-id-type="pmid">30970017</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006954</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buiu</surname><given-names>C</given-names></name><name><surname>Putz</surname><given-names>MV</given-names></name><name><surname>Avram</surname><given-names>S</given-names></name></person-group><article-title>Learning the Relationship between the Primary Structure of HIV Envelope Glycoproteins and Neutralization Activity of Particular Antibodies by Using Artificial Neural Networks</article-title><source>Int J Mol Sci</source><year>2016</year><month>Oct</month><day>11</day><volume>17</volume><issue>10</issue><elocation-id>1710</elocation-id><pub-id pub-id-type="pmcid">PMC5085742</pub-id><pub-id pub-id-type="pmid">27727189</pub-id><pub-id pub-id-type="doi">10.3390/ijms17101710</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hepler</surname><given-names>NL</given-names></name><name><surname>Scheffler</surname><given-names>K</given-names></name><name><surname>Weaver</surname><given-names>S</given-names></name><name><surname>Murrell</surname><given-names>B</given-names></name><name><surname>Richman</surname><given-names>DD</given-names></name><name><surname>Burton</surname><given-names>DR</given-names></name><etal/></person-group><article-title>IDEPI: Rapid Prediction of HIV-1 Antibody Epitopes and Other Phenotypic Features from Sequence Data Using a Flexible Machine Learning Platform</article-title><source>PLOS Comput Biol</source><year>2014</year><month>Sep</month><day>25</day><volume>10</volume><issue>9</issue><elocation-id>e1003842</elocation-id><pub-id pub-id-type="pmcid">PMC4177671</pub-id><pub-id pub-id-type="pmid">25254639</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003842</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dănăilă</surname><given-names>VR</given-names></name><name><surname>Buiu</surname><given-names>C</given-names></name></person-group><article-title>Prediction of HIV sensitivity to monoclonal antibodies using aminoacid sequences and deep learning</article-title><source>Bioinformatics</source><year>2022</year><month>Sep</month><day>15</day><volume>38</volume><issue>18</issue><fpage>4278</fpage><lpage>85</lpage><pub-id pub-id-type="pmcid">PMC9477525</pub-id><pub-id pub-id-type="pmid">35876860</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btac530</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoon</surname><given-names>H</given-names></name><name><surname>Macke</surname><given-names>J</given-names></name><name><surname>West</surname><given-names>AP</given-names><suffix>Jr</suffix></name><name><surname>Foley</surname><given-names>B</given-names></name><name><surname>Bjorkman</surname><given-names>PJ</given-names></name><name><surname>Korber</surname><given-names>B</given-names></name><etal/></person-group><article-title>CATNAP: a tool to compile, analyze and tally neutralizing antibody panels</article-title><source>Nucleic Acids Res</source><year>2015</year><month>Jul</month><day>1</day><volume>43</volume><issue>W1</issue><fpage>W213</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC4489231</pub-id><pub-id pub-id-type="pmid">26044712</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkv404</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bbosa</surname><given-names>N</given-names></name><name><surname>Kaleebu</surname><given-names>P</given-names></name><name><surname>Ssemwanga</surname><given-names>D</given-names></name></person-group><article-title>HIV subtype diversity worldwide</article-title><source>Curr Opin HIV AIDS</source><year>2019</year><month>May</month><volume>14</volume><issue>3</issue><fpage>153</fpage><pub-id pub-id-type="pmid">30882484</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="web"><source>HIV and AIDS</source><date-in-citation>cited 2023 May 24</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.who.int/news-room/fact-sheets/detail/hiv-aids">https://www.who.int/news-room/fact-sheets/detail/hiv-aids</ext-link></comment></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bricault</surname><given-names>CA</given-names></name><name><surname>Yusim</surname><given-names>K</given-names></name><name><surname>Seaman</surname><given-names>MS</given-names></name><name><surname>Yoon</surname><given-names>H</given-names></name><name><surname>Theiler</surname><given-names>J</given-names></name><name><surname>Giorgi</surname><given-names>EE</given-names></name><etal/></person-group><article-title>HIV-1 Neutralizing Antibody Signatures and Application to Epitope-Targeted Vaccine Design</article-title><source>Cell Host Microbe</source><year>2019</year><month>Jan</month><day>9</day><volume>25</volume><issue>1</issue><fpage>59</fpage><lpage>72</lpage><elocation-id>e8</elocation-id><pub-id pub-id-type="pmcid">PMC6331341</pub-id><pub-id pub-id-type="pmid">30629920</pub-id><pub-id pub-id-type="doi">10.1016/j.chom.2018.12.001</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>R</given-names></name></person-group><article-title>Multitask Learning</article-title><source>Mach Learn</source><year>1997</year><month>Jul</month><day>1</day><volume>28</volume><issue>1</issue><fpage>41</fpage><lpage>75</lpage></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>ZS</given-names></name></person-group><chapter-title>Distributional Structure</chapter-title><person-group person-group-type="editor"><name><surname>Harris</surname><given-names>ZS</given-names></name><name><surname>Hiż</surname><given-names>H</given-names></name></person-group><source>Papers on Syntax</source><publisher-loc>Dordrecht</publisher-loc><publisher-name>Springer Netherlands</publisher-name><year>1981</year><fpage>3</fpage><lpage>22</lpage><date-in-citation>cited 2023 Sep 7</date-in-citation><comment>[Internet] (Synthese Language Library). Available from:</comment><pub-id pub-id-type="doi">10.1007/978-94-009-8467-7_1</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hie</surname><given-names>B</given-names></name><name><surname>Zhong</surname><given-names>ED</given-names></name><name><surname>Berger</surname><given-names>B</given-names></name><name><surname>Bryson</surname><given-names>B</given-names></name></person-group><article-title>Learning the language of viral evolution and escape</article-title><source>Science</source><year>2021</year><month>Jan</month><day>15</day><date-in-citation>cited 2022 Jan 18</date-in-citation><comment>[Internet]</comment><pub-id pub-id-type="pmid">33446556</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>D</given-names></name><name><surname>Dyer</surname><given-names>C</given-names></name><name><surname>He</surname><given-names>X</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name><name><surname>Hovy</surname><given-names>E</given-names></name></person-group><source>Hierarchical Attention Networks for Document Classification</source><conf-name>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</conf-name><conf-loc>San Diego, California</conf-loc><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2016</year><fpage>1480</fpage><lpage>9</lpage><date-in-citation>cited 2022 Aug 4</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://aclanthology.org/N16-1174">https://aclanthology.org/N16-1174</ext-link></comment></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title>Neural Machine Translation by Jointly Learning to Align and Translate [Internet]</article-title><source>arXiv</source><year>2016</year><date-in-citation>cited 2022 Nov 2</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</ext-link></comment></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Ellis</surname><given-names>DPW</given-names></name></person-group><article-title>Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems [Internet]</article-title><source>arXiv</source><year>2016</year><date-in-citation>cited 2022 Nov 2</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1512.08756">http://arxiv.org/abs/1512.08756</ext-link></comment></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burton</surname><given-names>DR</given-names></name><name><surname>Hangartner</surname><given-names>L</given-names></name></person-group><article-title>Broadly Neutralizing Antibodies to HIV and Their Role in Vaccine Design</article-title><source>Annu Rev Immunol</source><year>2016</year><month>May</month><day>20</day><volume>34</volume><fpage>635</fpage><lpage>59</lpage><pub-id pub-id-type="pmcid">PMC6034635</pub-id><pub-id pub-id-type="pmid">27168247</pub-id><pub-id pub-id-type="doi">10.1146/annurev-immunol-041015-055515</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dănăilă</surname><given-names>VR</given-names></name><name><surname>Avram</surname><given-names>S</given-names></name><name><surname>Buiu</surname><given-names>C</given-names></name></person-group><article-title>The applications of machine learning in HIV neutralizing antibodies research—A systematic review</article-title><source>Artif Intell Med</source><year>2022</year><month>Dec</month><day>1</day><volume>134</volume><elocation-id>102429</elocation-id><pub-id pub-id-type="pmid">36462896</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doria-Rose</surname><given-names>NA</given-names></name><name><surname>Bhiman</surname><given-names>JN</given-names></name><name><surname>Roark</surname><given-names>RS</given-names></name><name><surname>Schramm</surname><given-names>CA</given-names></name><name><surname>Gorman</surname><given-names>J</given-names></name><name><surname>Chuang</surname><given-names>GY</given-names></name><etal/></person-group><article-title>New Member of the V1V2-Directed CAP256-VRC26 Lineage That Shows Increased Breadth and Exceptional Potency</article-title><source>J Virol</source><year>2015</year><month>Dec</month><day>17</day><volume>90</volume><issue>1</issue><fpage>76</fpage><lpage>91</lpage><pub-id pub-id-type="pmcid">PMC4702551</pub-id><pub-id pub-id-type="pmid">26468542</pub-id><pub-id pub-id-type="doi">10.1128/JVI.01791-15</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cale</surname><given-names>EM</given-names></name><name><surname>Gorman</surname><given-names>J</given-names></name><name><surname>Radakovich</surname><given-names>NA</given-names></name><name><surname>Crooks</surname><given-names>ET</given-names></name><name><surname>Osawa</surname><given-names>K</given-names></name><name><surname>Tong</surname><given-names>T</given-names></name><etal/></person-group><article-title>Virus-like Particles Identify an HIV V1V2 Apex-Binding Neutralizing Antibody that Lacks a Protruding Loop</article-title><source>Immunity</source><year>2017</year><month>May</month><day>16</day><volume>46</volume><issue>5</issue><fpage>777</fpage><lpage>791</lpage><elocation-id>e10</elocation-id><pub-id pub-id-type="pmcid">PMC5512451</pub-id><pub-id pub-id-type="pmid">28514685</pub-id><pub-id pub-id-type="doi">10.1016/j.immuni.2017.04.011</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guttman</surname><given-names>M</given-names></name><name><surname>Cupo</surname><given-names>A</given-names></name><name><surname>Julien</surname><given-names>JP</given-names></name><name><surname>Sanders</surname><given-names>RW</given-names></name><name><surname>Wilson</surname><given-names>IA</given-names></name><name><surname>Moore</surname><given-names>JP</given-names></name><etal/></person-group><article-title>Antibody potency relates to the ability to recognize the closed, pre-fusion form of HIV Env</article-title><source>Nat Commun</source><year>2015</year><month>Feb</month><day>5</day><volume>6</volume><issue>1</issue><elocation-id>6144</elocation-id><pub-id pub-id-type="pmcid">PMC4338595</pub-id><pub-id pub-id-type="pmid">25652336</pub-id><pub-id pub-id-type="doi">10.1038/ncomms7144</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balla-Jhagjhoorsingh</surname><given-names>SS</given-names></name><name><surname>Corti</surname><given-names>D</given-names></name><name><surname>Heyndrickx</surname><given-names>L</given-names></name><name><surname>Willems</surname><given-names>E</given-names></name><name><surname>Vereecken</surname><given-names>K</given-names></name><name><surname>Davis</surname><given-names>D</given-names></name><etal/></person-group><article-title>The N276 Glycosylation Site Is Required for HIV-1 Neutralization by the CD4 Binding Site Specific HJ16 Monoclonal Antibody</article-title><source>PLOS ONE</source><year>2013</year><month>Jul</month><day>17</day><volume>8</volume><issue>7</issue><elocation-id>e68863</elocation-id><pub-id pub-id-type="pmcid">PMC3714269</pub-id><pub-id pub-id-type="pmid">23874792</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0068863</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwick</surname><given-names>MB</given-names></name><name><surname>Labrijn</surname><given-names>AF</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Spenlehauer</surname><given-names>C</given-names></name><name><surname>Saphire</surname><given-names>EO</given-names></name><name><surname>Binley</surname><given-names>JM</given-names></name><etal/></person-group><article-title>Broadly Neutralizing Antibodies Targeted to the Membrane-Proximal External Region of Human Immunodeficiency Virus Type 1 Glycoprotein gp41</article-title><source>J Virol</source><year>2001</year><month>Nov</month><day>15</day><volume>75</volume><issue>22</issue><fpage>10892</fpage><lpage>905</lpage><pub-id pub-id-type="pmcid">PMC114669</pub-id><pub-id pub-id-type="pmid">11602729</pub-id><pub-id pub-id-type="doi">10.1128/JVI.75.22.10892-10905.2001</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corey</surname><given-names>L</given-names></name><name><surname>Gilbert</surname><given-names>PB</given-names></name><name><surname>Juraska</surname><given-names>M</given-names></name><name><surname>Montefiori</surname><given-names>DC</given-names></name><name><surname>Morris</surname><given-names>L</given-names></name><name><surname>Karuna</surname><given-names>ST</given-names></name><etal/></person-group><article-title>Two Randomized Trials of Neutralizing Antibodies to Prevent HIV-1 Acquisition</article-title><source>N Engl J Med</source><year>2021</year><month>Mar</month><day>18</day><volume>384</volume><issue>11</issue><fpage>1003</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC8189692</pub-id><pub-id pub-id-type="pmid">33730454</pub-id><pub-id pub-id-type="doi">10.1056/NEJMoa2031738</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ono</surname><given-names>M</given-names></name><name><surname>Miwa</surname><given-names>M</given-names></name><name><surname>Sasaki</surname><given-names>Y</given-names></name></person-group><source>Word Embedding-based Antonym Detection using Thesauri and Distributional Information</source><conf-name>Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies [Internet]</conf-name><conf-loc>Denver, Colorado</conf-loc><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2015</year><fpage>984</fpage><lpage>9</lpage><date-in-citation>cited 2023 Aug 29</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://aclanthology.org/N15-1100">https://aclanthology.org/N15-1100</ext-link></comment></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>R</given-names></name><name><surname>Bouatta</surname><given-names>N</given-names></name><name><surname>Biswas</surname><given-names>S</given-names></name><name><surname>Floristean</surname><given-names>C</given-names></name><name><surname>Kharkar</surname><given-names>A</given-names></name><name><surname>Roy</surname><given-names>K</given-names></name><etal/></person-group><article-title>Single-sequence protein structure prediction using a language model and deep learning</article-title><source>Nat Biotechnol</source><year>2022</year><month>Nov</month><volume>40</volume><issue>11</issue><fpage>1617</fpage><lpage>23</lpage><pub-id pub-id-type="pmcid">PMC10440047</pub-id><pub-id pub-id-type="pmid">36192636</pub-id><pub-id pub-id-type="doi">10.1038/s41587-022-01432-w</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>J</given-names></name><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Verkuil</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Sercu</surname><given-names>T</given-names></name><name><surname>Rives</surname><given-names>A</given-names></name></person-group><chapter-title>Language models enable zero-shot prediction of the effects of mutations on protein function</chapter-title><source>Advances in Neural Information Processing Systems [Internet]</source><publisher-name>Curran Associates, Inc</publisher-name><year>2021</year><fpage>29287</fpage><lpage>303</lpage><date-in-citation>cited 2023 May 23</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/f51338d736f95dd42427296047067694-Abstract.html</ext-link></comment></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>JH</given-names></name></person-group><article-title>Greedy function approximation: A gradient boosting machine</article-title><source>Ann Stat</source><year>2001</year><month>Oct</month><volume>29</volume><issue>5</issue><fpage>1189</fpage><lpage>232</lpage></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Random Forests</article-title><source>Mach Learn</source><year>2001</year><month>Oct</month><day>1</day><volume>45</volume><issue>1</issue><fpage>5</fpage><lpage>32</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine Learning in Python</article-title><source>J Mach Learn Res</source><year>2011</year><volume>12</volume><issue>85</issue><fpage>2825</fpage><lpage>30</lpage></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title>Long Short-Term Memory</article-title><source>Neural Comput</source><year>1997</year><month>Nov</month><day>1</day><volume>9</volume><issue>8</issue><fpage>1735</fpage><lpage>80</lpage><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction [Internet]</article-title><source>arXiv</source><year>2020</year><date-in-citation>cited 2022 Nov 2</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1802.03426">http://arxiv.org/abs/1802.03426</ext-link></comment></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>C</given-names></name><name><surname>Pleiss</surname><given-names>G</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name><name><surname>Weinberger</surname><given-names>KQ</given-names></name></person-group><article-title>On Calibration of Modern Neural Networks [Internet]</article-title><source>arXiv</source><year>2017</year><date-in-citation>cited 2022 Oct 19</date-in-citation><comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1706.04599">http://arxiv.org/abs/1706.04599</ext-link></comment></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>O’Malley</surname><given-names>T</given-names></name><name><surname>Bursztein</surname><given-names>E</given-names></name><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Chollet</surname><given-names>F</given-names></name><name><surname>Jin</surname><given-names>H</given-names></name><name><surname>Invernizzi</surname><given-names>L</given-names></name><etal/></person-group><source>KerasTuner</source><year>2019</year><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://github.com/keras-team/keras-tuner">https://github.com/keras-team/keras-tuner</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig 1</label><caption><title>The architecture of a language-based universal model (LBUM).</title></caption><graphic xlink:href="EMS188921-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><title>Effect of subtype representativeness on AUC.</title><p>We named models according to the subtype combinations contained in the pretraining data (shown as “Pretraining”), in data on non-bnAbs (shown as “non-bnAbs”), and in bnAb data (shown as “bnAbs”). AUC discrepancy means AUC on subtype B minus AUC on subtype C. (A) shows the bias introduced by only training on one subtype, and how that bias is eliminated by more subtype diversity. (B) shows that subtype representativeness in the pretraining data reduces subtype bias only to a small extent, if at all. (C) shows how subtype representativeness in non-bnAb data reduces subtype bias. Error bars represent the 95% confidence intervals computed using 1000 bootstrap samples.</p></caption><graphic xlink:href="EMS188921-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><title>Learned antibody contexts.</title><p>Antibody contexts (i.e., vector representations of antibodies) learned as part of the attention mechanism. As a result of performing 5-fold cross-validation, 5 LBUMs were available, and antibody contexts from each of these are shown in one of the 5 subfigures. BnAbs are color-coded according to their epitopes. Arrows point to some bnAbs of interest; dashed-line circles show where one arrow points to two very close bnAbs.</p></caption><graphic xlink:href="EMS188921-f003"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Models’ AUC.</title><p>Area under the receiver operating characteristic curve (AUC). GBM is Gradient Boosting Machines; RF is Random Forests; LBUM is language-based universal model; ENS is the ensemble model that averages predictions from GBM, RF and LBUM. The red shade means that LBUM had a better score than both RF and GBM models did. The blue shade means the ensemble model scored better than all three individual models did. Numbers between parentheses are standard deviations.</p></caption><table frame="box" rules="cols"><thead><tr><th align="center" valign="middle">Epitope</th><th align="center" valign="middle">BnAb</th><th align="center" valign="middle">GBM</th><th align="center" valign="middle">RF</th><th align="center" valign="middle">LBUM</th><th align="center" valign="middle">ENS</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="center" valign="middle"/><td align="center" valign="middle">VRC07</td><td align="center" valign="middle">0.78 (0.08)</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.93 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.94 (0.05)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC01</td><td align="center" valign="middle">0.86 (0.03)</td><td align="center" valign="middle">0.89 (0.02)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.91 (0.01)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.93 (0.03)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">NIH45-46</td><td align="center" valign="middle">0.84 (0.04)</td><td align="center" valign="middle">0.84 (0.05)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.91 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.93 (0.03)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC-CH31</td><td align="center" valign="middle">0.77 (0.06)</td><td align="center" valign="middle">0.78 (0.07)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.82 (0.10)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.86 (0.06)</td></tr><tr><td align="center" valign="middle">CD4bs</td><td align="center" valign="middle">VRC-PG04</td><td align="center" valign="middle">0.80 (0.10)</td><td align="center" valign="middle">0.82 (0.07)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.85 (0.07)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.89 (0.08)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">HJ16</td><td align="center" valign="middle">0.53 (0.07)</td><td align="center" valign="middle">0.53 (0.07)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.63 (0.04)</td><td align="center" valign="middle">0.62 (0.07)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">3BNC117</td><td align="center" valign="middle">0.89 (0.04)</td><td align="center" valign="middle">0.92 (0.04)</td><td align="center" valign="middle">0.91 (0.02)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.93 (0.03)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC03</td><td align="center" valign="middle">0.83 (0.03)</td><td align="center" valign="middle">0.83 (0.01)</td><td align="center" valign="middle">0.81 (0.05)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.86 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC13</td><td align="center" valign="middle">0.83 (0.01)</td><td align="center" valign="middle">0.86 (0.03)</td><td align="center" valign="middle">0.75 (0.07)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.87 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">b12</td><td align="center" valign="middle">0.80 (0.04)</td><td align="center" valign="middle">0.81 (0.03)</td><td align="center" valign="middle">0.73 (0.05)</td><td align="center" valign="middle">0.81 (0.03)</td></tr><tr style="border-top: solid thin"><td align="center" valign="middle"/><td align="center" valign="middle">DH270.1</td><td align="center" valign="middle">0.89 (0.03)</td><td align="center" valign="middle">0.91 (0.05)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.94 (0.02)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.95 (0.01)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC29.03</td><td align="center" valign="middle">0.78 (0.10)</td><td align="center" valign="middle">0.80 (0.07)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.86 (0.06)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.87 (0.07)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">DH270.5</td><td align="center" valign="middle">0.92 (0.02)</td><td align="center" valign="middle">0.93 (0.02)</td><td align="center" valign="middle">0.93 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.96 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">DH270.6</td><td align="center" valign="middle">0.93 (0.05)</td><td align="center" valign="middle">0.95 (0.03)</td><td align="center" valign="middle">0.93 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.97 (0.02)</td></tr><tr><td align="center" valign="middle">C3/V3</td><td align="center" valign="middle">PGT135</td><td align="center" valign="middle">0.77 (0.05)</td><td align="center" valign="middle">0.77 (0.06)</td><td align="center" valign="middle">0.76 (0.04)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.79 (0.04)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PGT128</td><td align="center" valign="middle">0.87 (0.04)</td><td align="center" valign="middle">0.86 (0.02)</td><td align="center" valign="middle">0.84 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.89 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">2G12</td><td align="center" valign="middle">0.91 (0.02)</td><td align="center" valign="middle">0.90 (0.03)</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle">0.90 (0.03)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PGT121</td><td align="center" valign="middle">0.91 (0.02)</td><td align="center" valign="middle">0.91 (0.02)</td><td align="center" valign="middle">0.84 (0.02)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.93 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">10-1074</td><td align="center" valign="middle">0.97 (0.02)</td><td align="center" valign="middle">0.97 (0.01)</td><td align="center" valign="middle">0.88 (0.04)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.98 (0.01)</td></tr><tr style="border-top: solid thin"><td align="center" valign="middle">MPER</td><td align="center" valign="middle">4E10</td><td align="center" valign="middle">0.60 (0.12)</td><td align="center" valign="middle">0.63 (0.15)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.73 (0.06)</td><td align="center" valign="middle">0.72 (0.10)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">2F5</td><td align="center" valign="middle">0.96 (0.03)</td><td align="center" valign="middle">0.95 (0.03)</td><td align="center" valign="middle">0.90 (0.03)</td><td align="center" valign="middle">0.95 (0.03)</td></tr><tr style="border-top: solid thin"><td align="center" valign="middle"/><td align="center" valign="middle">35O22</td><td align="center" valign="middle">0.56 (0.13)</td><td align="center" valign="middle">0.62 (0.08)</td><td align="center" valign="middle">0.59 (0.06)</td><td align="center" valign="middle">0.61 (0.10)</td></tr><tr><td align="center" valign="middle">gp120-gp41</td><td align="center" valign="middle">VRC34.01</td><td align="center" valign="middle">0.82 (0.03)</td><td align="center" valign="middle">0.80 (0.07)</td><td align="center" valign="middle">0.74 (0.06)</td><td align="center" valign="middle">0.81 (0.04)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PGT151</td><td align="center" valign="middle">0.78 (0.06)</td><td align="center" valign="middle">0.79 (0.04)</td><td align="center" valign="middle">0.65 (0.07)</td><td align="center" valign="middle">0.78 (0.05)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">8ANC195</td><td align="center" valign="middle">0.88 (0.05)</td><td align="center" valign="middle">0.86 (0.05)</td><td align="center" valign="middle">0.63 (0.04)</td><td align="center" valign="middle">0.88 (0.05)</td></tr><tr style="border-top: solid thin"><td align="center" valign="middle"/><td align="center" valign="middle">VRC26.08</td><td align="center" valign="middle">0.88 (0.02)</td><td align="center" valign="middle">0.87 (0.02)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.93 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.94 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC26.25</td><td align="center" valign="middle">0.86 (0.03)</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.87 (0.06)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.91 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PG16</td><td align="center" valign="middle">0.82 (0.04)</td><td align="center" valign="middle">0.83 (0.03)</td><td align="center" valign="middle" style="background-color:#FFCCCB">0.84 (0.02)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.85 (0.03)</td></tr><tr><td align="center" valign="middle">V1/V2</td><td align="center" valign="middle">PG9</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle">0.84 (0.02)</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.89 (0.03)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">CH01</td><td align="center" valign="middle">0.79 (0.03)</td><td align="center" valign="middle">0.79 (0.03)</td><td align="center" valign="middle">0.76 (0.05)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.82 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PGDM1400</td><td align="center" valign="middle">0.90 (0.02)</td><td align="center" valign="middle">0.89 (0.02)</td><td align="center" valign="middle">0.90 (0.02)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.94 (0.02)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">VRC38.01</td><td align="center" valign="middle">0.69 (0.10)</td><td align="center" valign="middle">0.66 (0.03)</td><td align="center" valign="middle">0.66 (0.09)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.71 (0.09)</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle">PGT145</td><td align="center" valign="middle">0.85 (0.03)</td><td align="center" valign="middle">0.85 (0.05)</td><td align="center" valign="middle">0.79 (0.07)</td><td align="center" valign="middle" style="background-color:#ADD8E6">0.87 (0.03)</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Proportion of bnAbs whose epitope was targeted by at least one of the closest bnAbs.</title><p>The numbers between parentheses are standard deviations, since there are 5 LBUMs that resulted from performing 5-fold cross-validation.</p></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="middle" style="background-color:#8EB3E2">Number of closest bnAbs considered</th><th align="center" valign="middle" style="background-color:#8EB3E2">Cosine similarity</th><th align="center" valign="middle" style="background-color:#8EB3E2">L1 distance</th><th align="center" valign="middle" style="background-color:#8EB3E2">L2 distance</th></tr></thead><tbody><tr><td align="center" valign="middle">1</td><td align="center" valign="middle">0.74 (0.06)</td><td align="center" valign="middle">0.72 (0.04)</td><td align="center" valign="middle">0.75 (0.05)</td></tr><tr><td align="center" valign="middle">2</td><td align="center" valign="middle">0.82 (0.03)</td><td align="center" valign="middle">0.79 (0.06)</td><td align="center" valign="middle">0.79 (0.06)</td></tr><tr><td align="center" valign="middle">3</td><td align="center" valign="middle">0.89 (0.04)</td><td align="center" valign="middle">0.83 (0.06)</td><td align="center" valign="middle">0.84 (0.03)</td></tr><tr><td align="center" valign="middle">4</td><td align="center" valign="middle">0.94 (0.02)</td><td align="center" valign="middle">0.84 (0.06)</td><td align="center" valign="middle">0.84 (0.04)</td></tr><tr><td align="center" valign="middle">5</td><td align="center" valign="middle">0.95 (0.02)</td><td align="center" valign="middle">0.85 (0.05)</td><td align="center" valign="middle">0.87 (0.05)</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><title>Hyperparameters considered in the development of different models</title></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="middle" style="background-color:#C9DAF8">Model type</th><th align="left" valign="middle" style="background-color:#C9DAF8">Hyperparameters</th></tr></thead><tbody><tr><td align="left" valign="top">Random Forests</td><td align="left" valign="middle">max depth: 1, 2, 3, 4, 5<break/>max features: 0.03, 0.1, 0.2, 0.3, 0.5<break/>number of trees: 10, 50, 100, 500, 1000<break/>alibration method: “sigmoid”, “isotonic”</td></tr><tr><td align="left" valign="top">Gradient Boosting Machines</td><td align="left" valign="middle">learning rate: 0.001, 0.01, 0.05, 0.1, 0.2<break/>max features: 0.03, 0.1, 0.2, 0.3, 0.5<break/>max depth: 1, 2, 3, 4, 5<break/>number of trees: 10, 50, 100, 500, 1000<break/>calibration method: “sigmoid”, “isotonic”</td></tr><tr><td align="left" valign="top">Language-based universal model</td><td align="left" valign="middle">learning rate: 0.0001, 0.0003, 0.001, 0.003<break/>antibody context dimension: 32, 64, 128, 256<break/>dropout rate: 0.1,0.2, 0.3, 0.4, 0.5<break/>number of pretrained layers to unfreeze: 0, 2, 4, all<break/>classification loss weight: 0.5, 0.6, 0.7, 0.8, 0.9, 1</td></tr></tbody></table></table-wrap></floats-group></article>