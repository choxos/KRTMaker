<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS191780</article-id><article-id pub-id-type="doi">10.1101/2023.11.24.568598</article-id><article-id pub-id-type="archive">PPR764589</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Exploring the impact of variability in cell segmentation and tracking approaches</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Wiggins</surname><given-names>Laura</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>O’Toole</surname><given-names>Peter J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Brackenbury</surname><given-names>William J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wilson</surname><given-names>Julie</given-names></name><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>York Biomedical Research Institute, University of York, York, UK</aff><aff id="A2"><label>2</label>Department of Biology, University of York, York, UK</aff><aff id="A3"><label>4</label>Department of Mathematics, University of York, York, UK</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: <email>julie.wilson@york.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>26</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>25</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Segmentation and tracking are essential preliminary steps in the analysis of almost all live cell imaging applications. Although the number of open-source software systems that facilitate automated segmentation and tracking continue to evolve, many researchers continue to opt for manual alternatives for samples that are not easily auto-segmented, tracing cell boundaries by hand and re-identifying cells on consecutive frames by eye. Such methods are subject to inter-user variability, introducing idiosyncrasies into the results of downstream analysis that are a result of subjectivity and individual expertise. Such methods are also susceptible to intra-user variability, meaning findings are challenging to reproduce. Here we demonstrate and quantify the degree of intra- and inter-user variability in manual cell segmentation and tracking by comparing the phenotypic metrics extracted from cells segmented and tracked by different members of our research team. Furthermore, we compare the segmentation results for a ptychographic cell image obtained using different automated software and demonstrate the high dependence of performance on their imaging modality optimisation. Our results show that choice of segmentation and tracking methods should be considered carefully in order to enhance the quality and reproducibility of results.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">With microscopic imaging becoming an essential step in almost all areas of biological research, there is an increased demand for image analysis tools that provide reliable phenotype quantification.<sup><xref ref-type="bibr" rid="R1">1</xref></sup> A particular challenge in the face of image analysis is <italic>segmentation</italic>, a means of distinguishing objects from background. Objects of interest vary depending on application, with examples including single-cells within 2D microscopy images,<sup><xref ref-type="bibr" rid="R2">2</xref></sup> tumours within magnetic resonance images<sup><xref ref-type="bibr" rid="R3">3</xref></sup> or tissue from histopathological whole-slide images.<sup><xref ref-type="bibr" rid="R4">4</xref></sup> At present, many studies make use of manual segmentation to identify regions of interest (ROIs), though recent research has highlighted the issue of both inter- and intra-operator variability in manual segmentation and the impact this has upon reproducibility and biological conclusions.<sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref></sup> The subjectivity of manual segmentation and the time required to identify ROIs by hand has motivated a shift to automated approaches that are standardised, delineated and high-throughput.<sup><xref ref-type="bibr" rid="R9">9</xref></sup> This has had a particular impact in clinical settings where automated approaches are now used to make diagnoses through computer-aided diagnosis (CAD) systems.<sup><xref ref-type="bibr" rid="R10">10</xref></sup></p><p id="P3">Although providing benefits in terms of reproducibility, automated segmentation approaches still involve some degree of user interactivity, with manual segmentation often considered the gold standard against which to benchmark the results of automated segmentation.<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup> Furthermore, model-based segmentation approaches, such as U-Nets, make use of manually segmented ROIs for ground truth training sets.<sup><xref ref-type="bibr" rid="R13">13</xref></sup> Automated approaches are also often specific to certain imaging modalities, with models learning characteristics of such images and using this knowledge to accurately identify ROIs whilst excluding background and image artefacts. For all of these reasons, choice of segmentation approach prior to quantification from microscopy images is a hugely important step that should not be overlooked and should instead be carefully considered.</p><p id="P4">There exist several established, open-source software packages for the task of segmenting cells from 2D microscopy images, each with their own strengths and limitations.<sup><xref ref-type="bibr" rid="R14">14</xref></sup> Studies have been carried out to benchmark such software against one another based on accuracy of segmentation and ease of usability, though notably fluorescence images that have been used to trial the majority of these software. With the emergence of label-free imaging, efforts are now being made to form ground truth data sets of label-free cells to assist in the training of segmentation models that are specific to label-free imaging techniques such as phase-contrast, bright-field and quantitative phase imaging (QPI).<sup><xref ref-type="bibr" rid="R15">15</xref></sup> More recent cell segmentation software such as CellPose<sup><xref ref-type="bibr" rid="R16">16</xref></sup> emphasise their focus on developing models that are adaptable to new imaging techniques.</p><p id="P5">Here we demonstrate the direct impact that manual segmentation can have on downstream analysis by quantifying the inter- and intra-user variability of phenotypic metrics extracted from manually obtained ROIs. Moreover, we show that manual cell tracking also results in large variability in the extraction of motility metrics due to subjectivity in the choice of cell centroid position as well as re-identification of cells on consecutive frames. By comparing the results of automated and manual cell segmentation and tracking, we demonstrate greater reproducibility for a panel of automated packages, but large inter-software variability arising from differences in imaging modality optimisation.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3"><title>Cell lines</title><p id="P6">The MDA-MB-231 breast cancer cell line used within this study was a gift from M. Djamgoz, Imperial College, London. The molecular identity of this cell line was verified by short tandem repeat analysis.<sup><xref ref-type="bibr" rid="R17">17</xref></sup> Thawed cells were sub-cultured 1-2 times prior to discarding and thawing a new stock to ensure that the molecular identity of cells was retained throughout. Cells were cultured in Dulbecco’s modified eagle medium supplemented with 5% fetal bovine serum (FBS) and 4mM L-glutamine.<sup><xref ref-type="bibr" rid="R18">18</xref></sup> FBS was filtered using a 0.22<italic>μ</italic>m syringe filter prior to use to reduce artefacts when imaging. Cells were incubated at 37°C in plastic filter-cap T-25 flasks and were split at a 1:6 ratio when passaged. No antibiotics were added to cell culture medium.</p></sec><sec id="S4"><title>Image acquisition</title><p id="P7">Cells were placed onto the Phasefocus Livecyte 2 (Phasefocus Limited, Sheffield, UK) to incubate for 30 minutes prior to image acquisition to allow for temperature equilibration. One 500<italic>μ</italic>m x 500<italic>μ</italic>m field of view per well was imaged to capture as many cells, and therefore data observations, as possible.</p></sec><sec id="S5"><title>Manual segmentation and tracking</title><p id="P8">In order to compare inter-user variability, members of our research group with different research expertise performed the same segmentation and tracking tasks. Manual segmentation was performed using ImageJ’s freehand area selection tool to trace around individual cells and selections were added to ImageJ’s ROI manager using Edit &gt; Selection &gt; “Add to Manager”. Morphological features were then extracted from each ROI using ImageJ’s Measure function, where all features were selected from the available list provided in Analyze &gt; Set Measurements. A full list of extracted features together with definitions is provided within the ImageJ documentation.</p><p id="P9">ImageJ’s MTrackJ plugin was used for manual tracking of cells. Each cell was tracked for 50 frames by clicking the cells’ position on consecutive frames, with X and Y coordinates from all final trajectories exported for feature extraction. R’s <bold>trajr</bold><sup><xref ref-type="bibr" rid="R19">19</xref></sup> package was used for the extraction of several features to characterise cell trajectories. The list of extracted metrics comprises: mean and standard deviation of cell speed, sinuosity, trajectory straightness index, mean and standard deviation of track length, trajectory distance and mean trajectory turning angle.</p><p id="P10">Throughout the paper, we refer to the results obtained by different research team members by a number so that the effect of differences in research experience can be considered.</p></sec><sec id="S6"><title>Automated segmentation and tracking</title><sec id="S7"><title>FIJI segmentation</title><p id="P11">Fiji<sup><xref ref-type="bibr" rid="R20">20</xref></sup> is an open-source software for processing of biological microscopy images. It offers accessible implementation of a range of traditional image processing algorithms, with the user setting subjective thresholds throughout the course of a segmentation workflow. Initially a rolling ball algorithm with radius size 30px was applied to the image for the removal of background noise and imaging artefacts. The image was then smoothed using Gaussian blurring with a standard deviation of 3 to further denoise the image. The image was then converted to a binary mask where each pixel was classified as either background (black within the binary mask) or cell (white within the binary mask) and the watershed method used to separate connected cells ahead of final segmentation by using the “create selection” feature in Fiji for cell boundary detection. A number of alternative workflows were assessed in Fiji such as the use of “find edges” and thresholding prior to binary mask conversion but these methods proved unsuccessful, the details included here describe the workflow identified as being most successful for the segmentation of our images.</p></sec><sec id="S8"><title>Livecyte segmentation</title><p id="P12">Phase Focus’ CATbox software facilitates automated cell segmentation by allowing the user to create a unique image processing recipe for each experiment. This recipe is produced through user-imposed thresholds on a range of image processing techniques allowing for live inspection of segmentation results. A rolling ball algorithm was applied to the image with radius 29.24 <italic>μ</italic>m and a height of 2, to remove background noise and imaging artefacts. The image was then smoothed with a parameter of 19.11<italic>μ</italic>m ahead of seedpoint detection. Local pixel intensity maxima identified from the smoothed image were used as seedpoints with an intensity threshold of 0.8. Nearby seedpoints were then consolidated by combining two seedpoints if there was a path between them that did not change in phase by more than a threshold of 1.09. Thresholds were then applied to distinguish between the pixel intensities of cell and background. A background threshold of 0.05 was used, any pixel with intensity below this threshold was classified as background. A feature threshold (for detection of cells) of 0.6 was used, any pixel above this threshold was classified as belonging to a cell. Pixels that did not meet these requirements were then scaled to a range of 0-1 and formed a “grey area” where pixel classification was uncertain. A biased fuzzy distance transform<sup><xref ref-type="bibr" rid="R21">21</xref></sup> was then used to classify “grey area” pixels with a fuzzy distance threshold of 0.2 <italic>μ</italic>m and a phase weighting of 6. Final segmentation results were then exported as.roi files. All thresholds were selected subjectively by assessing live segmentation and revising thresholds to yield optimum results.</p></sec><sec id="S9"><title>Icy segmentation</title><p id="P13">Icy<sup><xref ref-type="bibr" rid="R22">22</xref></sup> is a free, open-source image processing software. A key component of Icy is its public repository for the sharing of analysis plug-ins and workflows. The “HK-means” plug-in was utilised for initial segmentation of the cells. This plug-in performs <italic>K</italic>-means clustering of the image pixel intensity histogram to distinguish between background and cells, and then uses size filtering to split undersegmented cells. A Gaussian pre-filter was applied with standard deviation set equal to 5 for image smoothing prior to cell edge detection. <italic>K</italic> was set to 2 to establish two classes of pixel intensity: background and cell. Area size filters were then imposed with a minimum threshold of 100px and a maximum of 5000px, detected objects with areas outside of this range were discarded. The “active contours” plug-in was then used with default parameter settings for closer sculpting of ROIs to cell edges. Finally the “split ROI” plug-in was used to separate cells that were originally undersegmented. This plug-in is semi-automatic and relies on the user to enter how many cells are incorrectly clustered within a group before selecting the group that needs separating. This method was applied to all instances of undersegmentation but some cells were unable to be corrected and were therefore left as is in the final segmentation.</p></sec><sec id="S10"><title>CellProfiler segmentation</title><p id="P14">CellProfiler<sup><xref ref-type="bibr" rid="R23">23</xref></sup> is an open-source software for the analysis and measurement of cell images in which pipelines can be constructed by the user for tailored cell segmentation and tracking. Cells were detected using the “identify primary objects” module with a minimum expected object diameter of 10px and a maximum of 50px. Objects detected outside of this range and objects that were touching the borders were discarded. The default image processing settings associated with this module were used, hence the image was globally thresholded prior to segmentation. A minimum cross entropy thresholding method<sup><xref ref-type="bibr" rid="R24">24</xref></sup> was used to minimise the average error in pixel classification and Gaussian smoothing with a standard deviation of 1 was performed prior to final segmentation. CellProfiler allows the user to select which method should be used to distinguish and separate clumped objects, in this case intensity was selected as opposed to the use of shape. After running the completed pipeline, segmentation masks exported as.tiff files. To convert image masks into ImageJ ROIs, the masks were read into ImageJ and an ROI of the full mask created using Edit &gt; Selection &gt; Create Selection. This selection was then added to the ROI manager where the “Split” function was used to create individual ROIs from the full mask selection.</p></sec><sec id="S11"><title>CellPose segmentation</title><p id="P15">CellPose<sup><xref ref-type="bibr" rid="R25">25</xref></sup> is an open-source, deep learning-based segmentation tool together with pre-trained models for the segmentation of cells, nuclei and tissue sections without the need for parameter fine-tuning. The underlying algorithm involves the transformation of ground-truth cell segmentation into horizontal and vertical flow representations that can be predicted by a neural network to restore accurate segmentation of even unusual cell shapes. CellPose was utilised for cell segmentation using FIJI’s CellPose-TrackMate plugin. The pre-trained cytoplasm model was used for detection of cell boundaries with expected cell diameter of 10 microns.</p></sec><sec id="S12"><title>Statistical analyses</title><p id="P16">All tests of statistical significance within this study were performed using Graphpad Prism 9.1.0 (GraphPad Software, San Diego, CA). Data were tested for normality using the D’Agostino &amp; Pearson test. Parametric tests (t-tests and F-tests) were used where suitable with non-parametric Mann-Whitney U-tests in place of t-tests where data did not follow a normal distribution. For comparison of three or more populations, ANOVA was used when data followed a normal distribution or Kruskal-Wallis tests if not. Tukey’s post-hoc test and Dunn’s multiple comparisons were performed following ANOVA and Kruskal-Wallis respectively. Results were considered significant if <italic>p</italic> &lt; 0.05. Levels of significance used: * &lt; 0.05, ** &lt; 0.01, *** &lt; 0.001, **** &lt; 0.0001. Full details of statistical tests used for each analysis are provided in the figure legend for the corresponding figure. Sørensen–Dice coefficients to quantify the similarity of segmented regions were computed using MATLAB’s dice() function.</p></sec></sec></sec><sec id="S13" sec-type="results"><title>Results</title><sec id="S14"><title>Inter-user variability in manual cell segmentation</title><p id="P17">Within the current study, researchers 1, 2 and 5 are researchers who have received in-depth microscopy training, including the acquisition and interpretation of microscopy images. In comparison, researchers 3 and 4 are not experienced microscopists but have extensive experience working with the imaged cell line, MDAMB-231. All 5 researchers were set the task of manually segmenting three specified cells from a single microscopy image of MDA-MB-231 cells, the three cells are highlighted in <xref rid="F1" ref-type="fig">Figure 1(a)</xref>. As the appearance of the image could differ with the researchers’ computer monitor and display preferences, we provided a contrast enhanced cell image in which cell protrusions not clearly visible within the original image could be observed (<xref rid="F1" ref-type="fig">Figure 1(a)</xref>). The extent of protrusions that could be observed by researchers would likely be impacted by their monitor and display preferences.</p><p id="P18">Inter-user variability in manual segmentation was observed from each researcher’s identified ROIs (<xref rid="F1" ref-type="fig">Figure 1(b)</xref>). Researchers 1, 2 and 5 tended to closely sculpt the brightest intensity pixels, treating the cell boundary as the region around the cell in which bright intensity pixels are neighboured with dark intensity background pixels. Researchers 3 and 4, on the other hand, tended to include a large number of dark intensity pixels within the areas they determined to be the cell interior, despite the resulting ROIs differing in morphology to that expected of an MDA-MB-231 cell. It is possible that researchers 3 and 5 were considering the brightest intensity pixels as cell nuclei, and darker intensity pixels surrounding these areas as cytoplasm. This could be due to their experience with nucleic stains such as DAPI rather than label-free imaging in which whole cells can be visualised.</p><p id="P19">Sørensen–Dice coefficients confirmed the presence of inter-user variability in segmentation across all three cells, with higher coefficients indicative of significant overlap in segmentation results and lower coefficients representing manual segmentation that differs (<xref rid="F1" ref-type="fig">Figure 1(c)</xref>). The Dice coefficients show the impact of research expertise on manual cell segmentation results, with researchers with similar expertise (researchers 1, 2 and 5 or researchers 3 and 4) obtaining high pair-wise coefficients in comparison to the coefficients obtained for researchers with differing expertise.</p><p id="P20">For each cell, bar charts showing the distribution of Dice coefficients for each researcher’s segmentation data are plotted in <xref rid="F1" ref-type="fig">Figure 1(d)</xref> to aid visualisation. Two-way ANOVA revealed a statistically significant relationship between Dice coefficients and cell number, indicating greater inter-user variability in manual segmentation for certain cells. In comparison, no relationship was found between Dice coefficients and researcher number, suggesting that inter-user variability affected all researchers as opposed to one particular researcher. No statistical significance was found between Dice coefficients and the interaction between cell number and researcher, suggesting that inter-user variability is not related to particular cells.</p></sec><sec id="S15"><title>Intra-user variability in cell segmentation</title><p id="P21">The segmentation of the three cells was repeated 5 times and the intra-user variability of each researcher’s 5 attempts is shown in <xref rid="F2" ref-type="fig">Figure 2(a)</xref>. Visual inspection of each researchers’ segmentation attempts showed that some were more consistent in their segmentation approach, repeatedly outlining similar ROIs. This was the case for researchers 1, 2 and 5 who all aimed to closely sculpt the bright intensity pixels that neighbour dark intensity background pixels. In comparison, researchers 3 and 4 tended to show greater variance in their segmentation attempts where ROI selection, and therefore expected cell shape, was not guided as much by the bright intensity pixels.</p><p id="P22">Intra-user Dice coefficients were calculated and the results for each researcher and each cell are displayed in <xref rid="F2" ref-type="fig">Figure 2(b)</xref>. Two-way ANOVA showed a highly significant relationship between intra-user Dice coefficients and researcher number, confirming greater consistency in manual segmentation for some researchers. Differences between researchers accounted for 25% of the total variance present within the data set. A statistically significant relationship was identified between intra-user Dice coefficients and cell number, indicating that intra-user variability was more prevalent for certain cells in comparison to others, though cell number only accounted for 3% of the total variance within the data set. High statistical significance was also found between intra-user Dice coefficients and the interaction between cell number and researchers, showing that maintaining consistency in segmentation proved more challenging with some cells than others for certain researchers. The interaction between cell number and researcher accounted for 28% of the total variance present within the data set.</p><p id="P23">To assess whether automated approaches to cell segmentation would provide less variable, and therefore more reproducible, results, the same three cells were segmented using the Phasefocus CATbox system. To replicate the potential variance introduced by researchers, the cell image was independently processed five times by an expert with extensive experience in working with ptychographic images and the MDA-MB-231 cell line, and ROIs of the three cells were obtained from each of the five processed images. Visual inspection of these ROIs showed greater consistency in comparison to the intra-user results of the researchers, across all three cells (<xref rid="F2" ref-type="fig">Figure 2(c)i</xref>), with differing image processing pipelines resulting in subtle differences in segmentation. Dice coefficients were calculated to assess the similarity of ROIs obtained from the five processed images, the results are displayed in <xref rid="F2" ref-type="fig">Figure 2(c)ii</xref>. One-way ANOVA revealed no statistically significant difference in Dice coefficients as a result of cell number (<italic>p</italic> value = 0.2407), indicating greater consistency in segmentation results irrespective of cell number for Phasefocus segmentation.</p></sec><sec id="S16"><title>The impact of cell segmentation on extracted metrics</title><p id="P24">As the morphological metrics that can be extracted from cell images rely on initial segmentation of individual cells, accuracy and consistency in identification of ROIs is vital for meaningful downstream analyses. We hypothesised that the inter- and intra-user variability present in manual segmentation would also be prevalent in the morphological metrics extracted from the ROIs. We therefore extracted several morphological metrics describing cell shape, size and texture from the ROIs in <xref rid="F2" ref-type="fig">Figure 2(a)</xref>, as well as the Phasefocus CATbox segmentation results (described from now on as Livecyte) in <xref rid="F2" ref-type="fig">Figure 2(c)</xref>, and assessed their variability, both inter- and intra-user. Cell area, circularity and mean gray value were chosen as representative descriptors of cell size, shape and texture respectively and their varying values can be seen in <xref rid="F3" ref-type="fig">Figure 3(a)</xref>. ANOVA of all three metrics identified statistically significant differences in size, shape and texture of cells as a result of inter-user manual segmentation with <italic>p</italic> values &lt; 0.0001 for all three metrics.</p><p id="P25">Post-hoc analysis was performed using Tukey’s multiple comparisons test and the full results for cell area, circularity and mean gray value are provided in <bold>Supplementary Table 1, 2</bold> and <bold>3</bold> respectively. Notably, researchers 1, 2 and 5 frequently displayed no significant differences in extracted metrics when compared with one another (with the exception of researchers 2 and 5 scoring a <italic>p</italic> value of 0.0356 for circularity), but did consistently differ significantly from researchers 3 and 4 across all metrics. Furthermore, researchers 3 and 4 frequently displayed no significant differences between one another. Differences in research expertise between researchers 1, 2 and 5, and researchers 3 and 4 highlights the influence of research background on the results of downstream analyses. Researchers 1, 2 and 5 showed no statistical significance from Livecyte segmented cells in terms of cell area, but were statistically significant for both circularity and mean gray value. In contrast, researchers 3 and 4 showed no statistical significance from Livecyte segmented cells in terms of circularity, but were statistically significant for both area and mean gray value.</p><p id="P26">The impact of inter- and intra-user variability in extracted metrics was further investigated using PCA. Scores plots for cells 1, 2 and 3 are provided in <xref rid="F3" ref-type="fig">Figure 3(b) i, ii and iii</xref> respectively. Researcher-specific clusters can be identified within the scores plots demonstrating idiosyncrasies in cell segmentation and their impact on the resulting extracted metrics. Clusters from researchers 1, 2 and 5 tended to overlap with one another as did the clusters from researchers 3 and 4, again suggesting a relationship between research experience and the results obtained. Intra-user variability is visible within all three scores plots, with larger ellipses representative of greater variability. Tighter clusters were frequently obtained with Livecyte, even in cases where all researchers displayed extensive variability. This is the case for cell 3, where greater consistency in results is obtained with Livecyte across all three cells.</p><p id="P27">Additionally, intra-user variability in extracted metrics was quantified through the calculation of Euclidean distances to measure the pair-wise similarity of data vectors obtained by each researcher, the results are shown in <xref rid="F3" ref-type="fig">Figure 3(c)</xref>. ANOVA identified statistical significant differences as a result of cell number (<italic>p</italic> value = 0.0273), indicating that certain cells invoked greater variability in extracted metrics in comparison to others. A statistically significant difference was also identified between researchers, demonstrating that some were more consistent in their segmentation approach and this was reflected in the consistency of the extracted metrics. The interaction between cell number and researcher was also highly statistically significant, indicating that consistency posed a greater challenge for some cells for certain researchers, resulting in greater variability in extracted metrics for some cells. Notably the distances calculated for Livecyte segmentation remained consistent across all three cells, with the distances themselves minimised in comparison to those obtained by other researchers indicating greater consistency in extracted metrics as a result of Livecyte segmentation.</p></sec><sec id="S17"><title>Inter-user variability in cell tracking</title><p id="P28">As well as segmenting each of the three cells 5 times, the researchers tracked cell 1, 2 and 3 for a total of 50 frames three times each to assess inter-user variability in cell tracking. Note that the researcher previously referred to as researcher 3 did not take part in this cell tracking study and was replaced with a different researcher referred to as researcher 6, with similar expertise to researcher 3. Researchers selected the cell’s position on consecutive frames to form three, 50-frame long trajectories. The inter-user variability in cell trajectories can be observed in <xref rid="F4" ref-type="fig">Figure 4(a)</xref>, where, for each cell, the researchers’ first tracking attempt is shown. By eye, the general shape of trajectories obtained for cells 1 and 2 appear fairly consistent, with subtle differences along the trajectories induced by variability in click position of each researcher. The trajectory shape for cell 3, shows a noticeable difference for researchers 4 and 6 in comparison to researchers 1, 2 and 5 although the trajectories produced by researchers 4 and 6 are similar.</p><p id="P29">Further investigation showed that the difference is due to disagreement in cell identification from one frame to the next, as visualised in <xref rid="F4" ref-type="fig">Figure 4(b)</xref>. All researchers correctly identify cell 3 in frame 11, as demonstrated by all 5 points within the cell interior. The cell then undergoes a notable change in morphology on frame 12, making it correct identification of cell 3 challenging on this frame. Cell 3 is correctly identified by researchers 4 and 6 (green and purple points) in frame 12 whereas researchers 2 and 5 (red and yellow points) misidentify a different cell as cell 3. Researcher 1 (blue point) selects a position that is actually background as opposed to a cell interior on frame 12. This could be due to an unexpected change in morphology, and therefore position, of cell 3 between frames 11 and 12. Researcher 1 does select a cell interior on frame 13 but again this is a misidentification rather than cell 3. All researchers then continue to track their selected cells for the remainder of the time-lapse, resulting in different trajectories with only researchers 4 and 6 correctly tracking the original cell 3.</p></sec><sec id="S18"><title>Intra-user variability in cell tracking</title><p id="P30">As the main source of variation in cell tracking arises from inconsistencies in researcher click positions, we sought to explore the intra-user variability present across three independent cell tracking attempts for each cell. We quantified each researcher’s click spread for each of the three cells and scatter plots displaying these values are provided in <xref rid="F5" ref-type="fig">Figure 5(a)</xref>. Here the origins are representative of the average click position from three tracking attempts, with the click spread of each point then calculated as its deviation from the average. Researchers who remain consistent in their click positions will have all points closely clustered and centred around the origin. Researchers with inconsistencies in click position will have deviated scatter plots with points positioned at varying distances from the origin. All researchers experienced intra-user variability in click position across all cells, with researcher 6 providing the largest inconsistencies for cells 1 and 2 as evidenced by greater deviation in personal click spread for these cells.</p><p id="P31">Click spread for cell 3 was considered separately to cells 1 and 2 due to the impact of frequent misidentification of cell 3. Researchers 4 and 6, the two researchers who successfully identified cell 3 in <xref rid="F4" ref-type="fig">Figure 4(b)</xref> successfully identified cell 3 in all three of their tracking attempts, resulting lower click spread for cell 3 in comparison to all other researchers. All other researchers identified different cells as cell 3 within their three tracking attempts, resulting in large click spreads for researchers 1, 2 and 5 for cell 3. Interestingly these results suggest a relationship between cell tracking results and research expertise, with cell biologists 4 and 6 displaying greater accuracy in cell tracking in comparison to microscopists 1, 2 and 5.</p><p id="P32">In the cases of cell 1 and 2, the click spread deviation for each researcher is likely a result of differences in determination of the cell centroid. Researchers with low click spread potentially aim for the same sub-cellular region on each attempt, whereas high click spread indicates inconsistency in selection of sub-cellular region. The click spreads for cell 3 are dominated by the challenge of identifying cell 3 when it experiences a change in morphology on frame 12 with the additional deviation as a result of variability in selection of a sub-cellular region.</p><p id="P33">We next sought to compare manual tracking results with those obtained from automated tracking produced by the Phasefocus CATbox. Replicates were obtained by tracking the cells after three independent image processing pipelines were performed. Different processing methods were found to affect the determination of cell centroids. The obtained cell trajectories, together with centroid deviation, for each cell are provided in <xref rid="F5" ref-type="fig">Figure 5(b)</xref>. The plots show that despite different image processing pipelines used, the tracking results are consistent for cell 1 with negligible variation in the cell trajectories and very little deviation in centroid position with points in the scatter plot forming a tight cluster centered at the origin. The same can be seen for cell 3 although each track ends at frame 11, just before the cell changes morphology on frame 12 where each tracking repeat generated a new identifier for the cell that was used for the remainder of the time-lapse. There was notable misidentification of cell 2 in one tracking repeat as evidenced by a change in the cell trajectory and centroid deviation, shown for a subset of points within the centroid scatter plot.</p><p id="P34">Standard distance deviation (SDD) was used to quantify the overall deviation of points for each cell and each researcher (<xref rid="F5" ref-type="fig">Figure 5(c)</xref>). This confirmed observations from the click spread plots with researcher 6 displaying greater inconsistencies in click positions for cell 1 and 2 in comparison to all other researchers. Variability in SDD was greatest for cell 3 due to repeated misidentification of cell 3 by multiple researchers. SDD was minimal for Livecyte tracking of cells 1 and 3, but higher than all other researchers for cell 2. These results highlight greater consistency and reproducibility of automated tracking in comparison to manual tracking but also highlight problems in automated cell tracking.</p></sec><sec id="S19"><title>The impact of cell tracking on extracted metrics</title><p id="P35">As inter- and intra-user variability in manual segmentation was found to have a major impact on morphological metrics, we hypothesised that the same would be the case for extracted characteristic metrics of cell trajectories. Variability in cell trajectories, both between researchers and within each researcher’s own repeats, occurs due to variability in click positions on consecutive frames. Certain dynamical features, such as speed of cells, are extremely sensitive to such deviations resulting in large variability in the time series obtained for these features (<xref rid="F6" ref-type="fig">Figure 6(a)i, ii</xref>). The variability across the whole cell trajectory has a cumulative effect on summary statistics that are calculated to characterise cell behaviour throughout the whole time-lapse (such as mean speed), with each researcher obtaining different values dependent on their own tracking approach. The mean speeds calculated from each researchers’ cell 1 trajectories are shown in <xref rid="F6" ref-type="fig">Figure 6(a)iii</xref>. ANOVA revealed statistically significant differences in mean speed between researchers performing the tracking (<italic>p</italic> value = 0.0017), highlighting the impact of subjectivity on quantification. Notably, the minimal standard error of the mean was achieved by automated Livecyte tracking (s.e = 0.034) in comparison to manual tracking (s.e = 0.057, 0.054, 0.088, 0.12 for researchers 1, 2, 4 and 6 respectively). PCA was used to assess the cumulative effect of such variability on a combination of motility metrics. As for PCA of morphological metrics shown in <xref rid="F3" ref-type="fig">Figure 3(b)</xref>, researcher-specific clusters can be identified within the scores plots demonstrating idiosyncrasies in cell tracking and the impact these have on the resulting extracted metrics. Intra-user variability is visible within all three scores plots, with larger ellipses representative of greater variability. Largest ellipses within scores plots for cell 1 and 2 are observed for researcher 6, found to have the largest click spread deviation for these cells. Automated Livecyte tracking resulted in the smallest ellipse, and therefore the least variability in motility metrics, for cell 1 but the impact of misidentification of cell 2 can be observed in the cell 2 scores plot where a much larger Livecyte ellipse and more dispersed points can be observed. Note that Livecyte points are not included in the scores plot for cell 3 due to the premature ending of all 3 Livecyte trajectories for this cell meaning metrics such as track length could not be compared with full length manual trajectories.</p></sec><sec id="S20"><title>Variability in extracted metrics from segmentation software</title><p id="P36">Cell segmentation software were benchmarked on their ability to obtain accurate cell ROIs that reliably capture the true morphology of the cells within the images. For this analysis, a cell image was manually segmented by an expert with extensive experience working with ptychographic images as well as the MDA-MB-231 cells within the image (<xref rid="F7" ref-type="fig">Figure 7a</xref>). Manually segmented cells were used as a gold standard, with the accuracy of automated software quantified by how closely the outputs matched those from manual segmentation. From the automated segmentation results presented within <bold>Supplementary Figure 1</bold> it can be observed that morphology of ROIs from FIJI and Icy, in particular, differ from those of the other 3 software. Notably, FIJI and Icy tend to segment the cell body, excluding the lower intensity cell protrusions that are identified by Livecyte, CellProfiler, CellPose and manual segmentation. PCA of all morphological metrics extracted from each software’s obtained ROIs confirmed this to be true, with FIJI and Icy ellipses providing smallest overlap with the manual segmentation ellipse and greatest distance between group means in the PCA scores plot (<xref rid="F7" ref-type="fig">Figure 7b</xref>).</p><p id="P37">Further investigation into the morphological metrics impacted by this difference in segmentation showed that FIJI and Icy segmentation result in cells that are significantly smaller in size, have increased circularity and have higher mean gray value compared to manually segmented cells (<xref rid="F7" ref-type="fig">Figure 7c</xref>). In comparison, Livecyte, Cell-Profiler and CellPose segmentation resulted in the extraction of morphological metrics that were not significantly different to those obtained by manual segmentation, the exception being circularity of CellPose ROIs (<italic>p</italic> value = 0.046).</p></sec></sec><sec id="S21" sec-type="discussion"><title>Discussion</title><p id="P38">The results presented here demonstrate the importance of choosing a segmentation and tracking approach that is optimised for the application in question. Most segmentation and tracking software rely on the contrast in intensity between cells and background to accurately detect cell boundaries. These software are often optimised for fluorescence images where fluorescent labels enhance the intensity of pixels within cell interiors. With pytchography being a relatively new imaging modality, pre-existing algorithms do not perform as well on these images due to differences in the distribution of cell pixel intensities within label-free images. The newer segmentation and tracking software, Livecyte and CellPose-TrackMate, were found to be optimal for our images in comparison to the other, well-established software tested: FIJI, Icy and CellProfiler. Livecyte algorithms are optimised for ptychographic imaging and this shows in its superiority for both segmentation and tracking. CellPose is the most recent software within the field and already offers several pre-built models that are optimised for imaging modalities, namely phase-contrast and brightfield, that often pose the greatest challenge for segmentation.<sup><xref ref-type="bibr" rid="R16">16</xref></sup> As label-free imaging becomes more established, it is likely that pre-existing segmentation and tracking software will further develop their current algorithms to handle such images in order to compete with newer approaches.</p><p id="P39">Our findings suggest that the results of manual segmentation and tracking should be used as a gold standard with caution. Deep learning algorithms frequently make use of manual results for training and testing performance, yet manual results vary significantly with experience and subjectivity. It is important that manual segmentation and tracking for such ground truth data sets is performed by an expert in the particular field in question, and that their own results provide minimal intrauser variability. The subjectivity of manual segmentation is of particular concern for medical applications in which segmentation results are used for diagnosis of diseases, such as the detection of breast tumours in mammograms. Furthermore, the morphological metrics extracted from segmented regions are used to assess response to treatment and disease progression (such as size of tumour following chemotherapy). This motivates the need for standardised, automated approaches to segmentation that eradicate the potential for bias within results.</p><p id="P40">The variability in segmentation and tracking results, both inter- and intra-user as well as inter-software support the ongoing replicability and reproducibility crisis within scientific studies. The results of this study showed that idiosyncrasies present within initial segmentation and tracking are further exacerbated by the extraction of metrics to quantify cell morphology and motility. This led to results from identical cell populations and even same cells being found statistically significantly different based on their initial segmentation and tracking. Automated software takes on an algorithmic approach to cell segmentation and tracking, following the same formulaic pipeline both within and between images. On the other hand, researchers performing manual segmentation and tracking often do not have a set of rules to follow in order to detect cell boundaries or centroids, as evidenced by intrauser variability in identified ROIs and researcher click spread, therefore adding cumulative inaccuracies to their results. Intra-software variability was minimal which motivates the use of automated approaches to improve the repeatability of results. However, though results may be more reproducible this does not ensure that they are accurate. It is therefore vital that background research is done on the choice of automated segmentation and tracking software prior to downstream analysis.</p><p id="P41">The similarity of results from manual segmentation and tracking tend to be associated with the similarity of research background. Notably, the trained microscopists (researchers 1, 2 and 5) produced manual segmentation results with greater similarity to each other than to cell biologists (researchers 3 and 4). This was supported by the calculation of Dice coefficients for quantification of segmentation overlap. This suggests that research background is a latent variable in terms of quantification from cell segmentation and tracking, though this impact is often not assessed or accounted for in scientific research.</p><p id="P42">The challenge caused by cell 3, even for trained microscopists and automated software, emphasises the importance of minimising interval times between frames during image acquisition. Cells that undergo a drastic morphology change from one frame to another are difficult to identify in consecutive frames, and misidentification results in erroneous changes to trajectories and consequently the time series of extracted metrics. These results demonstrate that both misidentification and erroneous cell segmentation are visible within extracted morphology and motility metrics and suggest the possibility of automating detection of such instances to clean up data sets prior to downstream analyses. Shorter cell tracks would not cause a problem in terms of biological conclusions, as the remaining cell track is still represented within the data set but linked to a different cell identifier. To handle the longer, inaccurate cell tracks, machine learning models could be used to identify interruptions in cell time series induced by misidentification and exclude these cells from further analysis, a step that is included in the CellPhe toolkit for automated cell phenotyping.<sup><xref ref-type="bibr" rid="R26">26</xref></sup></p></sec></body><back><ack id="S22"><title>Acknowledgements</title><p>This work was supported by the BBSRC (grant number B/S507416/1). The authors thank the following members of the research team for assisting with data collection: Dr. Serife Yerlikaya, Nattanan Sajjaboontawee, Jodie Malcolm, and Blythe Wright from the University of York, and Kundi Umar and Abdurrahman Alkham from Yobe State University.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Dong</surname><given-names>X</given-names></name><etal/></person-group><article-title>A review of biological image analysis</article-title><source>Current Bioinformatics</source><volume>12</volume><year>2017</year></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al-Kofahi</surname><given-names>Y</given-names></name><name><surname>Zaltsman</surname><given-names>A</given-names></name><name><surname>Graves</surname><given-names>R</given-names></name><etal/></person-group><article-title>A deep learning-based algorithm for 2-d cell segmentation in microscopy images</article-title><source>BMC bioinformatics</source><volume>19</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><year>2018</year><pub-id pub-id-type="pmcid">PMC6171227</pub-id><pub-id pub-id-type="pmid">30285608</pub-id><pub-id pub-id-type="doi">10.1186/s12859-018-2375-z</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zhong</surname><given-names>P</given-names></name><name><surname>Jie</surname><given-names>D</given-names></name><etal/></person-group><article-title>Brain tumor segmentation from multi-modal mr images via ensembling unets</article-title><source>Frontiers in Radiology</source><fpage>11</fpage><year>2021</year><pub-id pub-id-type="pmcid">PMC10365098</pub-id><pub-id pub-id-type="pmid">37492172</pub-id><pub-id pub-id-type="doi">10.3389/fradi.2021.704888</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bándi</surname><given-names>P</given-names></name><name><surname>Van de Loo</surname><given-names>R</given-names></name><name><surname>Intezar</surname><given-names>M</given-names></name><etal/></person-group><source>Comparison of different methods for tissue segmentation in histopathological whole-slide images</source><conf-name>2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017)</conf-name><fpage>591</fpage><lpage>595</lpage><conf-sponsor>IEEE</conf-sponsor><year>2017</year></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dionisio</surname><given-names>F</given-names></name><name><surname>Oliveira</surname><given-names>LS</given-names></name><name><surname>Hernandes</surname><given-names>M</given-names></name><etal/></person-group><article-title>Manual versus semiautomatic segmentation of soft-tissue sarcomas on magnetic resonance imaging: evaluation of similarity and comparison of segmentation times</article-title><source>Radiologia Brasileira</source><volume>54</volume><fpage>155</fpage><lpage>164</lpage><year>2021</year><pub-id pub-id-type="pmcid">PMC8177681</pub-id><pub-id pub-id-type="pmid">34108762</pub-id><pub-id pub-id-type="doi">10.1590/0100-3984.2020.0028</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Covert</surname><given-names>E</given-names></name><name><surname>Fitzpatrick</surname><given-names>K</given-names></name><name><surname>Mikell</surname><given-names>J</given-names></name><etal/></person-group><article-title>Intra-and inter-operator variability in mribased manual segmentation of hcc lesions and its impact on dosimetry</article-title><source>EJNMMI physics</source><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage><year>2022</year><pub-id pub-id-type="pmcid">PMC9772368</pub-id><pub-id pub-id-type="pmid">36542239</pub-id><pub-id pub-id-type="doi">10.1186/s40658-022-00515-6</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joskowicz</surname><given-names>L</given-names></name><name><surname>Cohen</surname><given-names>D</given-names></name><name><surname>Caplan</surname><given-names>N</given-names></name><etal/></person-group><article-title>Inter-observer variability of manual contour delineation of structures in ct</article-title><source>European radiology</source><volume>29</volume><fpage>1391</fpage><lpage>1399</lpage><year>2019</year><pub-id pub-id-type="pmid">30194472</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bø</surname><given-names>HK</given-names></name><name><surname>Solheim</surname><given-names>O</given-names></name><name><surname>Jakola</surname><given-names>AS</given-names></name><name><surname>Kvistad</surname><given-names>K</given-names></name><etal/></person-group><article-title>Intra-rater variability in lowgrade glioma segmentation</article-title><source>Journal of Neuro-oncology</source><volume>131</volume><fpage>393</fpage><lpage>402</lpage><year>2017</year><pub-id pub-id-type="pmid">27837437</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>N</given-names></name><name><surname>Aggarwal</surname><given-names>L</given-names></name><etal/></person-group><article-title>Automated medical image segmentation techniques</article-title><source>Journal of medical physics</source><volume>35</volume><issue>1</issue><fpage>3</fpage><year>2010</year><pub-id pub-id-type="pmcid">PMC2825001</pub-id><pub-id pub-id-type="pmid">20177565</pub-id><pub-id pub-id-type="doi">10.4103/0971-6203.58777</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>H</given-names></name><name><surname>Hadjiiski</surname><given-names>LM</given-names></name><name><surname>Samala</surname><given-names>RK</given-names></name></person-group><article-title>Computer-aided diagnosis in the era of deep learning</article-title><source>Medical physics</source><volume>47</volume><issue>5</issue><fpage>e218</fpage><lpage>e227</lpage><year>2020</year><pub-id pub-id-type="pmcid">PMC7293164</pub-id><pub-id pub-id-type="pmid">32418340</pub-id><pub-id pub-id-type="doi">10.1002/mp.13764</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morey</surname><given-names>RA</given-names></name><name><surname>Petty</surname><given-names>CM</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group><article-title>A comparison of automated segmentation and manual tracing for quantifying hippocampal and amygdala volumes</article-title><source>Neuroimage</source><volume>45</volume><issue>3</issue><fpage>855</fpage><lpage>866</lpage><year>2009</year><pub-id pub-id-type="pmcid">PMC2714773</pub-id><pub-id pub-id-type="pmid">19162198</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.12.033</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verma</surname><given-names>K</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Paydarfar</surname><given-names>D</given-names></name></person-group><article-title>Automatic segmentation and quantitative assessment of stroke lesions on mr images</article-title><source>Diagnostics</source><volume>12</volume><issue>9</issue><fpage>2055</fpage><year>2022</year><pub-id pub-id-type="pmcid">PMC9497525</pub-id><pub-id pub-id-type="pmid">36140457</pub-id><pub-id pub-id-type="doi">10.3390/diagnostics12092055</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>J</given-names></name><name><surname>Othmani</surname><given-names>A</given-names></name><name><surname>Jain</surname><given-names>M</given-names></name><etal/></person-group><article-title>Using u-net network for efficient brain tumor segmentation in mri images</article-title><source>Healthcare Analytics</source><volume>2</volume><elocation-id>100098</elocation-id><year>2022</year></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesmann</surname><given-names>V</given-names></name><name><surname>Franz</surname><given-names>D</given-names></name><name><surname>Held</surname><given-names>C</given-names></name><etal/></person-group><article-title>Review of free software tools for image analysis of fluorescence cell micrographs</article-title><source>Journal of Microscopy</source><volume>251</volume><fpage>39</fpage><lpage>53</lpage><year>2015</year><pub-id pub-id-type="pmid">25359577</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edlund</surname><given-names>C</given-names></name><name><surname>Jackson</surname><given-names>TR</given-names></name><name><surname>Khalid</surname><given-names>N</given-names></name><etal/></person-group><article-title>Livecell—a large-scale dataset for label-free live cell segmentation</article-title><source>Nature methods</source><volume>18</volume><issue>9</issue><fpage>1038</fpage><lpage>1045</lpage><year>2021</year><pub-id pub-id-type="pmcid">PMC8440198</pub-id><pub-id pub-id-type="pmid">34462594</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01249-6</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name></person-group><article-title>Cellpose 2.0: how to train your own model</article-title><source>BioRxiv</source><fpage>2022</fpage><lpage>04</lpage><year>2022</year><pub-id pub-id-type="pmcid">PMC9718665</pub-id><pub-id pub-id-type="pmid">36344832</pub-id><pub-id pub-id-type="doi">10.1038/s41592-022-01663-4</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masters</surname><given-names>JR</given-names></name><etal/></person-group><article-title>Short tandem repeat profiling provides an international reference standard for human cell lines</article-title><source>Proceedings of the National Academy of Sciences</source><volume>98</volume><issue>14</issue><fpage>8012</fpage><lpage>8017</lpage><year>2001</year><pub-id pub-id-type="pmcid">PMC35459</pub-id><pub-id pub-id-type="pmid">11416159</pub-id><pub-id pub-id-type="doi">10.1073/pnas.121616198</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>M</given-names></name><name><surname>Kozminski</surname><given-names>DJ</given-names></name><name><surname>Wold</surname><given-names>LA</given-names></name><etal/></person-group><article-title>Therapeutic potential for phenytoin: targeting nav1.5 sodium channels to reduce migration and invasion in matastatic breast cancer</article-title><source>Breast Cancer Research and Treatment</source><volume>134</volume><issue>2</issue><fpage>603</fpage><lpage>615</lpage><year>2012</year><pub-id pub-id-type="pmcid">PMC3401508</pub-id><pub-id pub-id-type="pmid">22678159</pub-id><pub-id pub-id-type="doi">10.1007/s10549-012-2102-9</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McLean</surname><given-names>DJ</given-names></name><name><surname>Skowron</surname><given-names>V</given-names></name><name><surname>Marta</surname><given-names>A</given-names></name></person-group><article-title>trajr: An r package for characterisation of animal trajectories</article-title><source>Ethology</source><volume>124</volume><issue>6</issue><fpage>440</fpage><lpage>448</lpage><year>2018</year></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schindelin</surname><given-names>J</given-names></name><name><surname>Arganda-Carreras</surname><given-names>I</given-names></name><name><surname>Frise</surname><given-names>E</given-names></name><etal/></person-group><article-title>Fiji: an open-source platform for biological-image analysis</article-title><source>Nature Methods</source><volume>9</volume><fpage>676</fpage><lpage>682</lpage><year>2012</year><pub-id pub-id-type="pmcid">PMC3855844</pub-id><pub-id pub-id-type="pmid">22743772</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.2019</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saha</surname><given-names>Punam K</given-names></name><name><surname>Wehrli</surname><given-names>Felix W</given-names></name><name><surname>Gomberg</surname><given-names>Bryon R</given-names></name></person-group><article-title>Fuzzy distance transform: Theory, algorithms, and applications</article-title><source>Computer Vision and Image Understanding</source><volume>86</volume><fpage>171</fpage><lpage>190</lpage><year>2002</year></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Chaumont</surname><given-names>F</given-names></name><name><surname>Dallongeville</surname><given-names>S</given-names></name><name><surname>Chenouard</surname><given-names>N</given-names></name><etal/></person-group><article-title>Icy: an open bioimage informatics platform for extended reproducible research</article-title><source>Nature methods</source><volume>9</volume><fpage>690</fpage><lpage>696</lpage><year>2012</year><pub-id pub-id-type="pmid">22743774</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McQuin</surname><given-names>C</given-names></name><name><surname>Goodman</surname><given-names>A</given-names></name><name><surname>Chernyshev</surname><given-names>V</given-names></name><etal/></person-group><article-title>Cellprofiler 3.0: Next-generation image processing for biology</article-title><volume>16</volume><year>2018</year><pub-id pub-id-type="pmcid">PMC6029841</pub-id><pub-id pub-id-type="pmid">29969450</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005970</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>CH</given-names></name><name><surname>Lee</surname><given-names>CK</given-names></name></person-group><article-title>Minimum cross entropy thresholding</article-title><source>Pattern Recognition</source><volume>26</volume><fpage>617</fpage><lpage>625</lpage><year>1993</year></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><etal/></person-group><article-title>Cellpose: a generalist algorithm for cellular segmentation</article-title><source>Nature methods</source><volume>18</volume><issue>1</issue><fpage>100</fpage><lpage>106</lpage><year>2021</year><pub-id pub-id-type="pmid">33318659</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiggins</surname><given-names>L</given-names></name><name><surname>Lord</surname><given-names>A</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name><etal/></person-group><article-title>The cellphe toolkit for cell phenotyping using time-lapse imaging and pattern recognition</article-title><source>Nature Communications</source><volume>14</volume><issue>1</issue><year>1854</year><elocation-id>2023</elocation-id><pub-id pub-id-type="pmcid">PMC10070448</pub-id><pub-id pub-id-type="pmid">37012230</pub-id><pub-id pub-id-type="doi">10.1038/s41467-023-37447-3</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Inter-user variability in manual cell segmentation</title><p><bold>(a)</bold> i. Phase image of MDA-MB-231 cells with the three cells to be manually segmented highlighted and labelled. ii. The cell image with contrast enhanced to display cell protrusions that are not clearly visible within the original cell image. The three cells to segment are highlighted and labelled. <bold>(b)</bold> Manual segmentation of each cell performed by each researcher. Inter-user variability can be observed from differences in outlined ROIs for each cell. <bold>(c)</bold> Sørensen–Dice coefficients to quantify the degree of similarity of pair-wise manual segmentation. Images show segmentation maps where black represents background, white represents segmentation overlap and green and pink represent differences in pair-wise segmentation. Areas in green appear in the segmentation map from the researcher listed in the row position, but not the researcher listed in the column position and viceversa for areas in pink. <bold>(d)</bold> Sørensen–Dice coefficients coefficients, grouped by researcher and coloured by cell number. Bars are representative of group means and error bars are representative of standard deviations to display the spread of coefficients. A two-way ANOVA was performed, obtaining <italic>p</italic> values of 0.0498, 0.3558 and 0.9944 for cell number, researcher, and their interaction, respectively.</p></caption><graphic xlink:href="EMS191780-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Intra-user variability in manual cell segmentation</title><p><bold>(a)</bold> Manual segmentation of each cell performed by each researcher, each colour represents one of the five segmentation attempts performed. Intra-user variability can be observed from differences in outlined ROIs. <bold>(b)</bold> Sørensen–Dice coefficients, grouped by researcher and coloured by cell number. Bars show group means with error bars representing standard deviations. Two-way ANOVA showed that cell number, researcher and their interaction all have a statistically significant relationship with intra-user Dice coefficients, contributing to 3%, 25% and 28% of the variance prevalent within the data set, respectively. <bold>(c)</bold> i. Automated segmentation of each cell performed using Phasefocus CATbox. Each colour represents one of the five segmentation results obtained from the same cell image processed independently. ii. Sørensen–Dice coefficients, coloured by cell number. ANOVA showed no statistically significant relationship between Dice coefficients and cell number, indicating consistency in segmentation results obtained from Phasefocus CATbox irrespective of cell number.</p></caption><graphic xlink:href="EMS191780-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The impact of cell segmentation on extracted metrics</title><p><bold>(a)</bold> Distributions for cell area (top), circularity (middle) and mean gray value (bottom) extracted from different manual segmentation results and Livecyte automated segmentation. Plots are grouped by researcher and coloured by cell number. Bars show group means with error bars representing SEM. ANOVA results show high statistical significance across all three metrics, demonstrating the impact of researcher-specific segmentation on extracted morphological metrics. <bold>(b)</bold> PCA scores plots for i. cell 1, ii. cell 2 and iii. cell 3 obtained from morphological metrics extracted from each manual segmentation and Livecyte automated segmentation. Points are coloured according to researcher and 95% concentration ellipses are displayed. Small and large ellipses are representative of low and high intra-user variability in extracted metrics respectively. <bold>(c)</bold> Pair-wise Euclidean distances between extracted data vectors, grouped by researcher and coloured by cell number. ANOVA revealed statistical significance differences in Euclidean distances as a result of cell number, researcher and the interaction between the two. Bars show group means and error bars represent standard deviations to display the spread of coefficients.</p></caption><graphic xlink:href="EMS191780-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Inter-user variability in cell tracking</title><p><bold>(a)</bold> Trajectories from each researcher’s first cell tracking of cells 1, 2 and 3. For cells 1 and 2, subtle differences between trajectories can be observed, induced by variability in click position between researchers. More notable inter-user variability can be observed for cell 3, where differences in cell identification between researchers resulted in the tracking of different cells that were misidentified as cell 3. <bold>(b)</bold> Visualisation of the misidentification of cell 3 throughout the time-lapse. All researchers correctly identify cell 3 in frame 11, where points represent click position coloured by researcher. Cell 3 undergoes a change in morphology on frame 12 which causes disagreement on which cell is actually cell 3. Researchers 4 and 6 correctly identify cell 3 on frame 12, researchers 2 and 5 misidentify a different cell as cell 3 and researcher 1 identifies background as cell 3. Each researcher then continues to track their chosen cells for the remainder of the time-lapse, with researcher 1 misidentifying a cell as cell 3 in frame 13.</p></caption><graphic xlink:href="EMS191780-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Intra-user variability in cell tracking</title><p><bold>(a)</bold> Click spread plots for each researcher, coloured by cell number. Origins represent the average click position across three tracking attempts, with the click spread of each point calculated as deviation from this average. Click spread plots for cell 3 show greater deviance due to repeated misidentification of cell 3 throughout the time-lapse resulting in variable cell trajectories. <bold>(b)</bold> i. Cell trajectories for cell 1, 2 and 3 obtained from automated tracking using Phasefocus CATbox. Each colour represents one of three trajectories obtained from the same cell time-lapse independently processed to replicate inter-user variability. Misidentification of cell 2 in one time-lapse results in an inaccurate trajectory (shown in blue). Trajectories for cell 3 are shorter than those obtained by manual tracking due to the premature ending of tracking on frame 11 when cell 3 undergoes a morphology change, after which the true cell 3 is given a new cell identifier for the remainder of the time-lapse. ii. Centroid spread plots for each researcher, coloured by cell number. Origins are represent the average centroid position across each of three tracking repeats, with the spread of each centroid calculated as deviation from this average. Deviation is minimal for cell 1 and cell 3, but the misidentification of cell 2 in one trajectory causes deviation in centroids. <bold>(c)</bold> Standard distance deviation (SDD) values to quantify the manual click spread from <bold>(a)</bold> and automated centroid spread from <bold>(b)</bold>.</p></caption><graphic xlink:href="EMS191780-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>The impact of cell tracking on extracted metrics</title><p><bold>(a)</bold> i. Time series for mean speed obtained from three tracking attempts for cell 1, coloured by researcher. ii. The same plot as in i but with the region between SEM error bars shaded to display intra-user inconsistencies. iii. Separate scatter plots showing mean speeds calculated for each researcher and each tracking repeat of cell 1. ANOVA revealed a statistically significant relationship between mean speed and the researcher performing tracking (<italic>p</italic> value = 0.0017). <bold>(b)</bold> The resulting PCA scores plots for i. cell 1, ii. cell 2 and iii. cell 3, performed with motility metrics extracted from the trajectories. Points are coloured according to researcher and 95% confidence ellipses are displayed. Small and large ellipses are representative of low and high intra-user variability in extracted metrics respectively. Note that Livecyte points are not included in the scores plot for cell 3 due to the premature ending of all 3 Livecyte trajectories.</p></caption><graphic xlink:href="EMS191780-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Variability in extracted metrics from segmentation software</title><p><bold>(a)</bold> Results of manual segmentation on the 0h cell image, performed by an expert with extensive experience working with ptychographic images and the MDA-MB-231 cell line used in this study. <bold>(b)</bold> PCA of morphological metrics extracted from cell ROIs, coloured by segmentation approach. FIJI and Icy display greatest variability to manual segmentation with their ellipses sharing the minimum overlap with the manual segmentation ellipse. Larger points are representative of group means. <bold>(c)</bold> Distributions for cell area, circularity and mean gray value extracted by manual segmentation and five different automated software packages. Bars show group means and error bars represent SEM. Results of Dunn’s multiple comparisons test are displayed on plots, with greatest significance across all metrics obtained by FIJI and Icy segmented cells when compared with manual segmentation.</p></caption><graphic xlink:href="EMS191780-f007"/></fig></floats-group></article>