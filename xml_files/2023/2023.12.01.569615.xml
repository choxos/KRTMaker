<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192511</article-id><article-id pub-id-type="doi">10.1101/2023.12.01.569615</article-id><article-id pub-id-type="archive">PPR767950</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Adaptive stretching of representations across brain regions and deep learning model layers</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Zhang</surname><given-names>Xin-Ya</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Bobadilla-Suarez</surname><given-names>Sebastian</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Luo</surname><given-names>Xiaoliang</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Lemonari</surname><given-names>Marilena</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Brincat</surname><given-names>Scott L.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Siegel</surname><given-names>Markus</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Miller</surname><given-names>Earl K.</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Love</surname><given-names>Bradley C.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>School of Physical Science and Engineering, Tongji University, Shanghai, P. R. China</aff><aff id="A2"><label>2</label>Department of Experimental Psychology, University College London, United Kingdom</aff><aff id="A3"><label>3</label>Computer Science, University of Cyprus, Nicosia, Cyprus</aff><aff id="A4"><label>4</label>The Picower Institute for Learning and Memory, Massachusetts Institute of Technology, Cambridge, MA, USA</aff><aff id="A5"><label>5</label>Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, Cambridge, MA, USA</aff><aff id="A6"><label>6</label>Department of Neural Dynamics and Magnetoencephalography, Hertie Institute for Clinical Brain Research, University of Tübingen, Tübingen, Germany</aff><aff id="A7"><label>7</label>Centre for Integrative Neuroscience, University of Tübingen, Tübingen, Germany</aff><aff id="A8"><label>8</label>The Alan Turing Institute, London, United Kingdom</aff><author-notes><corresp id="CR1"><label>*</label>To whom correspondence should be addressed; <email>b.love@ucl.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Prefrontal cortex (PFC) is known to modulate the visual system to favor goal-relevant information by accentuating task-relevant stimulus dimensions. Does the brain broadly re-configures itself to optimize performance by stretching visual representations along task-relevant dimensions? We considered a task that required monkeys to selectively attend on a trial-by-trial basis to one of two dimensions (color or motion direction) to make a decision. Except for V4 (color bound) and MT (motion bound), the brain radically reconfigured itself to stretch representations along task-relevant dimensions in lateral PFC, frontal eye fields (FEF), lateral intraparietal cortex (LIP), and inferotemporal cortex (IT). Spike timing was crucial to this code. A deep learning model was trained on the same visual input and rewards as the monkeys. Despite lacking an explicit selective attention or other control mechanism, the model displayed task-relevant stretching as a consequence of error minimization, indicating that stretching is an adaptive strategy.</p></abstract></article-meta></front><body><p id="P2">Adaptive agents, whether biological or artificial, configure themselves for the learning and decision task at hand. For example, when searching for one’s car keys, one might focus on features consistent with the shape and metallic sheen of a key. Prefrontal cortex (PFC) modulates the visual system to favour goal-relevant information [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>]. Goal-directed attention can reconfigure the visual system to highlight task-relevant features and suppress irrelevant features [<xref ref-type="bibr" rid="R3">3</xref>]. Over longer time horizons, learning processes build internal representations that reflect these task pressures [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>] with vmPFC critical for determining which aspects of the current context are relevant [<xref ref-type="bibr" rid="R6">6</xref>]. Attention can be viewed as <italic>stretching</italic> representations along relevant dimensions [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>], which is reflected both in behavior and brain response [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. Stretching accentuates differences along goal-relevant dimensions while minimizing differences along goal-irrelevant dimensions. For example, when searching for one’s car keys, objects varying in metallic sheen should become more dissimilar.</p><p id="P3">One possibility is that the brain radically re-configures itself across regions to optimize for the current task. Consistent with this possibility, effects of endogenous attention have been observed across the visual cortical hierarchy [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>], including as early as V1 [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>]. Alternatively, some areas devoted to modality specific processing, such as the middle temporal cortex (MT) for movement direction and visual area V4 for object properties like color, may be invariant across task contexts.</p><p id="P4">The ideal study to evaluate whether radical reconfiguration occurs would record from multiple brain sites while cueing the relevant dimension for a categorization decision on a trial-by-trial basis. We analyzed spiking data from a study[<xref ref-type="bibr" rid="R17">17</xref>] that met these criteria. On each trial, the rhesus monkeys viewed a cue that indicated whether color or direction of movement was the relevant dimension for the decision (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Once cued, colored dots moved with 100% coherency in a given direction. The monkeys responded by moving their eyes left or right, depending on the value of relevant (either color or motion) stimulus dimension (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). Recording sites included lateral prefrontal cortex (PFC), frontal eye fields (FEF), lateral intraparietal cortex (LIP), inferotemporal cortex (IT), V4, and MT. Unlike previous fMRI studies [<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R11">11</xref>] that considered stretching along relevant dimensions during categorization decisions, monkey multi-unit spiking data affords the possibility of evaluating whether spike timing, over and above spiking rate, is a critical component of the neural code.</p><p id="P5">Considering goal-relevant stretching along a relevant dimension in non-human animals offers opportunities to examine how control systems themselves can be learned and configured absent language instruction. Non-human animals, like artificial neural networks (ANNs), are naive to these laboratory tasks and, unlike verbally instructed humans, have to learn how to allocate their attention through trial-and-error. Nevertheless, non-human animals, including rats, can learn to selectively attend to relevant dimensions in related paradigms [<xref ref-type="bibr" rid="R18">18</xref>].</p><p id="P6">While models of goal-directed attention contain dedicated control systems that selectively weight relevant dimensions [<xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>], one possibility is that these control systems themselves can be learned by non-human animals and ANNs, enabling them to reconfigure themselves in response to task cues. According to this hypothesis, brains and ANNs are powerful statistical learning machines that build control structures that adaptively stretch learnt representations along relevant dimensions to facilitate task performance. In effect, these systems may build the cognitive machinery that is presupposed in previous work.</p><p id="P7">To evaluate this possibility, we constructed a ANN consisting of a deep convolutional neural network (CNN) and a stacked long short-term memory (LSTM). The CNN was pretrained to perform object recognition on naturalistic images [<xref ref-type="bibr" rid="R21">21</xref>]. The CNN part of the ANN, akin to the monkeys’ visual system [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>], was assumed to be developed and stable prior to the study. Thus, its weights were fixed. The sequences of images forming a trial (e.g., the task cue and the moving dots; <xref ref-type="fig" rid="F1">Fig. 1a</xref>) were fed into the CNN, whose outputs served as inputs to the LSTM [<xref ref-type="bibr" rid="R27">27</xref>]. The LSTM is a recurrent network that can process time series to make the left/right decision the monkeys did (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref> for details). Like the monkeys, the LSTM part of ANN learned through trial-and-error from sequences of images, absent verbal instruction.</p><p id="P8">Of course, the brain embodies constraints that ANNs do not. Brain regions may be limited to processing certain stimulus dimensions and may lack the flexibility to reconfigure. For example, in the present investigation, V4 and MT may remain bounded to processing color and motion, respectively. However, to the extent the brain can be viewed as an overparameterized statistical learning engine [<xref ref-type="bibr" rid="R28">28</xref>], the ANN and brain should converge. We predict this convergence will be realized by representations stretching along the goal-relevant stimulus dimension. Observing stretching in the ANN would provide a formal account of how control and top-down attentional mechanisms can arise from simply maximizing task performance, as opposed to relying on preordained mechanisms.</p><p id="P9">To foreshadow the results, we observed stretching across all layers of the ANN’s LSTM and in the brain, except for areas V4 and MT, which were modality bound. We also used a cognitive model with a dedicated attention mechanism to assess stretching in the brain data, which corroborated these conclusions. Finally, we analyzed the similarity and fidelity of neural representations [<xref ref-type="bibr" rid="R29">29</xref>] and discovered that spike timing was critical to how the brain coded representations of the stimuli and task.</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>Spike timing and neural representation</title><p id="P10">One initial question is which measure of neural similarity is most aligned with the neural recordings [<xref ref-type="bibr" rid="R29">29</xref>]. To answer this question, we assessed various measures of neural similarity to determine which one maximized representational similarity [<xref ref-type="bibr" rid="R30">30</xref>] with the experimenter-defined stimulus coordinates of color and motion. The high temporal resolution of the spike trains allowed us to consider whether dissimilarity measures that take into account spike timing, such as ISI [<xref ref-type="bibr" rid="R31">31</xref>] and SPIKE [<xref ref-type="bibr" rid="R32">32</xref>], offer advantages over rate-based measures, including Euclidean, cosine, and Pearson measures. ISI and SPIKE did surpass the non-timing measures (<xref ref-type="fig" rid="F2">Fig. 2</xref>). To confirm the significance of the type of measure, we conducted a one-way ANOVA analysis encompassing all measures, <italic>F</italic> (4, 105) = 143.06, <italic>p</italic> &lt; .0001. In particular, ISI performed best and will be used throughout the remaining analyses in this contribution (see Bonferroni corrected <italic>t</italic>-tests in <xref ref-type="table" rid="T1">Extended data Tables 1</xref>, <xref ref-type="table" rid="T2">2</xref>). ISI emphasizes the relative intervals between spikes, whereas SPIKE also incorporates the absolute timing of spikes which can be useful for evaluating synchrony between spike trains (see Methods and <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>). The advantage of spike timing measures over rate-based coding held across all recording sites (<xref ref-type="fig" rid="F8">Extended Data Figs. 3</xref> and <xref ref-type="fig" rid="F9">4</xref>).</p></sec><sec id="S3"><title>Dimensional stretching found in both brain and model activity</title><p id="P11">One key question is whether the brain radically re-configures itself across regions to optimize for the current task, which in the present study would manifest as stretching along the relevant dimension (color or motion) on each trial. Like-wise, we consider whether the LSTM, simply by maximizing performance absent an explicit control or attentional mechanism, will adaptively stretch its representations.</p><p id="P12">Stretching was assessed by considering item pairs that mismatched on one dimension and matched on the other. For example, we predict items mismatching on color and matching on motion should be more dissimilar when color is relevant than when motion is relevant. The dissimilarity of the 24 (<italic>i, j</italic>) item pairs that mismatch on color and match on motion is denoted <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> when color is relevant and <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> when motion is relevant. The 24 item pairs that mismatch on motion and match on color is denoted <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> and <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> for when color and motion are relevant, respectively. Stretching should be greatest when items mismatch on the relevant dimension.</p><p id="P13">This prediction was confirmed both in the brain and LSTM activity (<xref ref-type="fig" rid="F3">Fig. 3</xref>). Mismatches along a stimulus dimension were more consequential when that dimension was task relevant. In the monkey data, <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> − <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> and <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> − <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> were significantly different than 0, <italic>M</italic> = 0.0007, <italic>t</italic>(23) = −4.274, <italic>p</italic> &lt; .0001 and <italic>M</italic> = 0.0007, <italic>t</italic>(23) = −4.132, <italic>p</italic> = 1.4<italic>e</italic> − 4, respectively. In the LSTM simulations, <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> − <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> and <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> − <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> were significant different than 0, <italic>M</italic> = 4.738, <italic>t</italic>(23) = −7.259, <italic>p</italic> &lt; .0001 and <italic>M</italic> = 7.0694, <italic>t</italic>(23) = −9.887, <italic>p</italic> &lt; .0001), respectively.</p></sec><sec id="S4"><title>Stretching across model layers and brain regions</title><p id="P14">To assess how the six brain regions and LSTM model layers are modulated by task, we applied a cognitive model adapted from the generalized context model [<xref ref-type="bibr" rid="R8">8</xref>]. The model fit assessed how much attention, <italic>w<sub>r</sub></italic> (between 0 and 1), is dedicated to the task-relevant dimension with the irrelevant dimension weighted by 1−<italic>w<sub>r</sub></italic> (see Methods). This cognitive model assumes the two-dimensional psychological space shown in <xref ref-type="fig" rid="F1">Figure 1c</xref> and is useful for estimating the best fitting attention parameter <italic>w<sub>r</sub></italic>, which characterizes representational stretching.</p><p id="P15"><xref ref-type="fig" rid="F4">Figure 4a</xref> shows how brain regions representation spaces stretch along the relevant dimension, except for MT and V4. To assess these effects, we conducted permutation tests in which dimension considered relevant was shuffled (1000 times to construct the null distribution). For the PFC, FEF, LIP and IT, <italic>w<sub>r</sub></italic> was significantly larger than expected in both tasks contexts, all p-values less than .01. However, MT is modality bounded and focuses on motion dimension in both task contexts, whereas V4 focuses solely on the color dimension. Accordingly, these two modality bound regions had higher <italic>w<sub>r</sub></italic> values for their preferred dimension and lower <italic>w<sub>r</sub></italic> for their non-preferred dimension as compared to the null distribution, all p-values less than .01. For the LSTM, apart from earliest layer (<italic>p</italic> = .33), all layers had <italic>w<sub>r</sub></italic> values greater than expected by chance, all p-values less than .05 (see <xref ref-type="table" rid="T3">Extended Data Tables 3</xref> and <xref ref-type="table" rid="T4">4</xref>).</p></sec><sec id="S5"><title>LSTM re-configures itself based on the task goal unlike MT and V4</title><p id="P16">There is no straightforward layer-to-brain correspondence between LSTM layers and brain regions (<xref ref-type="fig" rid="F5">Fig. 5</xref>). Whereas MT and V4 appear dedicated to processing motion and color, respectively, LSTM layers freely reconfigure as a function of the task context. For example, V4 shows a decent correspondence with LSTM layers when color is relevant, whereas MT does when motion is relevant. Unlike some brain regions, the LSTM is more flexible and can fully reconfigure itself depending on the task.</p></sec></sec><sec id="S6" sec-type="discussion"><title>Discussion</title><p id="P17">We considered whether the brain re-configures itself to optimize for the current task. For example, do neural representations of stimuli stretch along dimensions relevant to the task? We analyzed data from a task that, depending on a visual cue at the beginning of each trial, required monkeys to selectively attend to one of two stimulus dimensions to make a decision. We found support for the hypothesis that the brain radically re-configures itself to optimize performance by accentuating differences along task-relevant dimensions. An ANN that was trained on the same visual information as the monkeys also displayed task-relevant stretching as a consequence of trying to reduce error, indicating that stretching is an adaptive strategy that can arise in systems that aim to optimize performance.</p><p id="P18">Stretching was established in the monkey data by analyses that compared stimulus pairs matching or mismatching on a relevant dimension. Pairs that mismatched on a relevant dimension were more dissimilar than those that mismatched on an irrelevant dimension (<xref ref-type="fig" rid="F3">Fig. 3</xref>). We also evaluated stretching by using a model-based approach in which a cognitive model with a selective attention parameter was fit to the neural data (<xref ref-type="fig" rid="F4">Fig. 4</xref>). Stretching was found at all recording sites, except for V4 and MT, which were bound to processing color and direction information, respectively.</p><p id="P19">One way ANN activity diverged from the brain data is that the ANN’s LSTM layers freely reconfigured themselves as a function of the task context. In other words, unlike the brain, the ANN was not modality bound. Accordingly, there was no straightforward relationship between brain regions and LSTM layers. For example, when color was relevant, V4 and network activity correlated, whereas when motion was relevant, MT and network activity correlated (see <xref ref-type="fig" rid="F5">Fig. 5</xref>).</p><p id="P20">Finally, although not a primary hypothesis, the monkey spiking data afforded the possibility of evaluating whether spike timing or rate-based measures better characterize the neural code. Spike timing did appear relevant to how the brain represented stimulus information (see <xref ref-type="fig" rid="F2">Fig. 2</xref>). These findings suggest a roll for spike timing that should be further explored, including in computational models. While our ANN was a recurrent network that processed the sequences of images forming a trial through time, the activity of each of its units was a scalar akin to rate-based coding. New models will need to developed to assess hypothesis concerning the functional role of spike-timing information in decision making.</p><p id="P21">One strength of the modeling approach is that the ANN was applied to the same image sequences forming a trial as those the monkeys experienced. Rather than attempt to incorporate biological constraints, we treated the model as a general-purpose statistical machine and observed how it tailored itself to the task. Under this approach, convergences between the ANN and the monkeys may reflect general information processing principles, whereas divergences may reflect unique properties of biological systems.</p><p id="P22">We found that the ANN performed as if it had a cognitive control mechanism that reconfigured network layers based on the task cue, stretching representations along task-relevant dimensions. In contrast, cognitive models, like the model we used in our analyses (<xref ref-type="fig" rid="F4">Fig. 4</xref>), have dedicated attentional mechanisms. The ANN results suggest that general-purpose learning systems can learn to control themselves absent purpose-built control circuits. In terms of divergences, we found that, unlike V4 and MT, the ANN was not modality bound by layer. This result suggests that there is a benefit or constraint in biological systems for localized processing of perceptual streams or dimensions.</p><p id="P23">Although this is just one study and model, we believe this modeling approach could complement other approaches in the literature, such as approaches that aim to test a priori hypotheses regarding correspondences between brain regions and model layers as is commonly done in the object recognition literature [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. Our approach may prove useful in other domains, including the meta-learning challenge of “learning to learn”. In particular, one interesting question is whether the implicit control structures the ANN developed for the present task would provide a useful starting point for mastering new tasks that involve selective attention. There might be a great deal to learn about the brain by considering the computational challenges it faces.</p></sec><sec id="S7" sec-type="methods"><title>Methods</title><sec id="S8"><title>Experiment and simulation description</title><p id="P24">In this work, we aim to model monkey brain learning on a flexible visuomotor decision-making task with our CNN-LSTM architecture. In the original experiment [<xref ref-type="bibr" rid="R17">17</xref>], two rhesus (one male and one female) monkeys were shown a series of stimuli videos and learned to categorize the color or the orientation of stimuli dots based on cues they saw. Recordings were performed using three stereotactically positioned recording chambers, which covered the frontal cortex (FEF and PFC), parietal cortex (LIP), and occipitotemporal cortex (IT, MT and V4). There were 21 color-motion stimuli (5 stimuli on or near the category boundary and 16 stimuli evenly spread across the four quadrants) from 7 possible colors and 7 possible motion directions (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). On each trial, monkeys saw one stimulus (dot pattern), that is, all dots with the same color moved 100% coherency in the same direction. Depending on the task cued at the beginning of each trial, the monkeys categorized either the color (red vs. green) or motion direction (up vs. down) of the stimulus and reported their percept with a left or right saccade. We did our best to replicate the training procedure as faithfully as possible by tasking our model with the same categorization problems, using identical fixation, cue and stimulus images (dot pattern) and training our model in a trial-by-trial fashion. We detail how experiments were performed while highlighting and justifying the necessary adjustments we have made in <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>.</p><p id="P25">To model the learning process and relate the neural data of trained monkeys, we employ training trials and test trials into our model and predict responses. We have a total of 6720 trials covering 4 cues, 2 speeds, 21 color-motion stimuli with 40 random stimulus dots’ initial positions (4 × 2 × 21 × 40 = 6720) and the training/test trial is 3 : 1 (i.e., 5040 training trials and 1680 test trials involved 30 and 10 random initial positions, respectively). Notice that although these trials use the same color-motion stimulus (dot pattern), the dots’ initial position in each trial is random, exactly as the original experiment. Matched with the original experiment, we include ambiguous trials of five boundary stimuli in our training and test but excluding them from all data analyses, that is we only consider 16 color-motion stimuli in our analyses. We use L (left) to represent the categorization of stimuli with upward motion in the motion task or stimuli made of greenish dots in the color task as the same class. Likewise, we use R (right) to represent the categorization of stimuli with downward motion in the motion task or stimuli made of reddish dots in the color task as the other class. We use N (no response) to represent ambiguous trials in training ANN like monkeys did.</p></sec><sec id="S9"><title>CNN-LSTM architecture</title><p id="P26">We tailor a CNN-LSTM framework containing a deep convolutional neural network (CNN) and a stacked long short-term memory neural network (multilayer LSTM). The overall learning task of our model is to categorize the same set of stimuli (a left or right saccade based on either motion or color and no response if ambiguous trials), which is identical to the training procedure as monkeys did. The CNN front-end is used to simulate the monkey visual system and the stacked LSTM is to simulate multiple brain regions such as MT, V4 which might be involved in the learning task.</p><p id="P27">Formally, we denote the CNN front-end as <bold>f</bold> which in our case is a pre-trained VGG-16 up to the pre-softmax layer. Given a trial of <italic>n</italic> stimuli, <italic>T<sub>i</sub></italic>, <bold>f</bold> transforms raw images into representations <italic>Q</italic> ∈ ℝ<sup><italic>n</italic>×<italic>d</italic></sup>, where <italic>d</italic> is the size of the pre-softmax layer, <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mstyle><mml:mtext>f</mml:mtext></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P28">We then denote the stacked LSTM as <bold>g</bold> which transforms the visual representations <italic>Q</italic> into <italic>H</italic> ∈ ℝ<sup><italic>d</italic>×<italic>h</italic></sup> following <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mtext>g</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>H</mml:mi></mml:mrow></mml:math></disp-formula> where <italic>h</italic> represents the number of LSTM cells in each layer. Finally, to transform LSTM representations into the decision response, we apply two linear transformations <italic>W</italic><sub>1</sub> ∈ ℝ<sup><italic>h</italic>×<italic>ht</italic></sup> and <italic>W</italic><sub>2</sub> ∈ ℝ<sup><italic>ht</italic>×<italic>m</italic></sup>, mapping <italic>H</italic> to <italic>P</italic> ∈ ℝ<sup><italic>n</italic>×<italic>m</italic></sup> where each row of <italic>P</italic> being a probability distribution over the <italic>m</italic> classes that the original experiment was trained to classify,<disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P29">In our simulations, we employ 270 images in each trial (<italic>n</italic> = 270), extract 1000 high-level features from VGG-16 to represent each stimulus image (<italic>d</italic> = 1000), feed these features into 6-layer stacked LSTM with 1000 cells (<italic>h</italic> = 1000) and adopt linear transforms of 256 units (<italic>ht</italic> = 256) and 3 units (L, R and N, <italic>m</italic> = 3). The number of layers stacked in LSTM is a hyperparameter. We adopt 6-layer stacked for the reason that we expect the number of 6 could give us a good chance to match brain regions and better examine whether each LSTM layer can learn representations at different levels of abstraction.</p></sec><sec id="S10"><title>Training and model evaluation</title><p id="P30">In training our deep neural network, a common cross-entropy loss function is used for optimization, estimated using the maximum likelihood method: <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mstyle><mml:mrow><mml:mi>∑</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where label <italic>y</italic><sub>0</sub> = 1 if monkey had a leftward saccade and <italic>y</italic><sub>1</sub> = 1 if monkey had a rightward saccade else <italic>y</italic><sub>2</sub> = 1. The five stimuli on the boundary corresponded to <italic>y</italic><sub>2</sub> = 1 regardless of the cue shape. The Adam optimizer is used with the default learning rate of 1<italic>e</italic> − 5. We use a batch size of 1 (i.e., trial-by-trial learning) to match the training procedure of the monkeys. All the simulations are implemented using PyTorch [<xref ref-type="bibr" rid="R34">34</xref>].</p><p id="P31">To evaluate the categorization accuracy of our model, we use micro-averaged F1 score metric. The micro-averaged F1 score pools per-sample classifications across classes to compute the micro-averaged precision (P) and recall (R) by counting the total true positives, false negatives, and false positives, and then a harmonic class of P and R, as follows <disp-formula id="FD5"><label>(3)</label><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>×</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>+</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> ranges between 0 and 1. The closer it is to 1, the better the model. Our model achieves the F1 score of 98% at least in multiple sampling frames (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. S2</xref>).</p></sec><sec id="S11"><title>Representational similarity analysis</title><p id="P32">In order to relate our artificial deep neural network to the brain, we utilize representational similarity analysis (RSA), an experimental and analytical framework, relating representations in the brain and our model by computing and comparing representational dissimilarity matrices (RDMs) that characterize the information encoded in a given brain or model. Each 16 × 16 RDM in this study contains distances/dissimilarities for pairwise stimuli. The distance indicates how dissimilar those two stimuli are in the monkey brain or how dissimilar those two model representations are in the model.</p></sec><sec id="S12"><title>Candidate neural dissimilarity measures</title><p id="P33">There are many possible measures to calculate the distance/dissimilarity between two or more task-relevant and stimuli-relevant spike trains in neural data. In this study, we consider candidate measures, such as rate coding, inter-spike interval (ISI) distance and SPIKE distance. Rate coding considers spike count within a time bin and represents each stimulus as a neural firing rate vector which consists of spike count across neurons whereas ISI and SPIKE distance take spike timing into account. ISI emphasizes the relative intervals between spikes while SPIKE also incorporates the absolute timing of spikes which can be useful for evaluating synchrony between spike trains. Pairwise-stimuli ISI/SPIKE distance is calculated by temporal averaging over a period of trial time. Specifically, rate coding explicitly constructs representation vector for individual stimulus before constructing the RDM using Euclidean distance, cosine distance or Pearson correlation, and the other two measures construct pairwise statistics towards RDM directly. More detailed information of candidate neural dissimilarity measures, please see <xref ref-type="supplementary-material" rid="SD1">Supplementary Information</xref>.</p><p id="P34">When processing neural data, we extract the spike trains of isolated neurons detected under electrodes. Regardless of which candidate measure we use, each element in task-relevant RDMs is the averaged distance (dissimilarity) from spike trains (in the time bins) across task-relevant trials and monkeys using a python toolbox PySpike [<xref ref-type="bibr" rid="R35">35</xref>] implemented ISI-distance and SPIKE-distance for the numerical analysis of spike train dissimilarity.</p></sec><sec id="S13"><title>Measure evaluation</title><p id="P35">To identify the best neural dissimilarity measure to analyze neural data, we opt for a RSA-based approach where we correlate RDMs derived from candidate neural dissimilarity measures to a static reference matrix of stimuli. This reference matrix reflects experimenter’s intended relationship between stimuli and we assume that the best measure should be the one that yields the highest Spearman’s rank correlation to the reference matrix. To construct such a reference matrix <italic>E</italic>, we represent each stimulus using experimenter-indented coordinates and compute pairwise dissimilarities. 16 color-motion stimuli are distributed in four quadrants of a coordinate system where each stimulus has a unique x-y coordinate. For example, bottom-left item in the coordinate system is denoted as (1, 1) and the top-right item is denoted as (4, 4) in <xref ref-type="fig" rid="F1">Fig. 1c</xref>. For any two stimuli <italic>a</italic> and <italic>b</italic> with coordinates (<italic>a<sub>x</sub>, a<sub>y</sub></italic>) and (<italic>b<sub>x</sub>, b<sub>y</sub></italic>), we compute their distance/dissimilarity <italic>E</italic>(<italic>a, b</italic>) as <disp-formula id="FD6"><label>(4)</label><mml:math id="M6"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P36">We derive such an experimenters-intended-based RDM, <italic>E</italic>, by calculating the exponential distance of the coordinates of pairwise color-motion stimuli. Note that being subtracted by 1 and the exponential function have no effect on the calculation of Spearman rank correlation. Although experimenter-intended-based RDM provides a fixed reference in both motion and color contexts, it inherently incorporated task-relevant distinctions in how it organized information based on stimuli coordinates.</p></sec><sec id="S14"><title>RDM based on model representations</title><p id="P37">To derive task-relevant RDMs from model representations in the deep neural network, we fill the entries by taking the averaged distance (dissimilarity) between hidden states in the LSTM across task-relevant trials. Although we employ Euclidean distance to calculate dissimilarities, the results were not affected by using Cosine distance (<xref ref-type="fig" rid="F10">Extended Data Fig. 5</xref>).</p></sec><sec id="S15"><title>LSTM time estimated</title><p id="P38">Given that the stacked LSTM processes stimulus images in a sequential manner, our model contains hidden states that evolve dynamically over trial time (i.e., stimulus images). By capturing the hidden states from processing the first image (fixation) to the last image (moving colored stimulus), we can derive <italic>n</italic> task-relevant RDMs from trials, where <italic>n</italic> is the number of stimulus images. To relate with sliding windows, we use LSTM time estimated by the frames of images. By selecting 60 frames per second, each image is displayed for 1/60 of a second, allowing the input of each image to be considered as a time stamp (<xref ref-type="fig" rid="F6">Extended Data Fig. 1</xref>).</p></sec><sec id="S16"><title>Dimensional stretching</title><p id="P39">In two categorization tasks, dimensional stretching refers to the expansion of the perceived psychological difference beyond the physical difference between stimuli. In our study, attentional effects refer to how the monkeys allocate attention to relevant dimension in response to their saccade response in task-relevant context. We evaluate dimensional stretching through model-free and model-fit approaches to gain insight into how the monkey brain reconfigured along motion-relevant and color-relevant dimensions in task-relevant context.</p></sec><sec id="S17"><title>Dimensional stretching: a model-free approach</title><p id="P40">The model-free approach evaluates dimensional stretching without assuming an underlying psychological space. Simply, we compare the dissimilarity between item pairs mismatch on one dimension and match on the other under two task contexts.</p><p id="P41">We denote the dissimilarity between item pairs (<italic>i, j</italic>) as <italic>D<sub>m</sub></italic>(<italic>i, j</italic>) and <italic>D<sub>c</sub></italic>(<italic>i, j</italic>) that mismatch on motion and mismatch on color, respectively, for instance, in <xref ref-type="fig" rid="F1">Fig. 1c</xref>, stimuli in the bottom-left corner (pink and upward) and stimuli in the top-left corner (green and upward) mismatch on color and match on motion. For <italic>N</italic>(<italic>i, j</italic>) = 24 pairs under motion-relevant context (i.e., (<italic>i, j</italic>)<sup><italic>m</italic></sup>) and color-relevant context (i.e., (<italic>i, j</italic>)<sup><italic>c</italic></sup>), we perform two-tailed paired <italic>t</italic>-test to show the difference of pairs that in different tasks, e.g., <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> − <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> and <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> − <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup>.</p></sec><sec id="S18"><title>Dimensional stretching: a cognitive model</title><p id="P42">The model-fit approach aims to explicitly model the psychological space itself, which assumes attentional effects exist and finds the best-fit attention for task-relevant context. We use a cognitive model that hypothesizes attention weights towards task-relevant and task-irrelevant dimension, then derive an attentional RDM, <italic>AE</italic>. Specifically, we assign the stimulus coordinates and for pairwise stimuli item <italic>a</italic> and <italic>b</italic>, the distance/dissimilarity in the task content, for example, the horizontal dimension is the task-relevant dimension, is calculated as <disp-formula id="FD7"><label>(5)</label><mml:math id="M7"><mml:mrow><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo>∗</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:msup><mml:mo>|</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>c</italic> is constant, <italic>w<sub>r</sub></italic> represents the attention weight in the dimension relevant to the task and <italic>w<sub>r</sub></italic> &gt;= 0. We find best-fit parameters <italic>w<sub>r</sub></italic> to the task-relevant dimension by maximizing the Spearman rank correlation between attentional matrix <italic>AE</italic> and representation-based RDM with the grid search, <disp-formula id="FD8"><label>(6)</label><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>max</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>Spearman</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>R</mml:mi><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where all stimuli items are fit at once to provide one estimate of stretching along the dimension of color and motion. Based on the cognitive model, we can explore how attention weights change depending on the task cued.</p></sec><sec id="S19"><title>Quantification and statistical analysis</title><p id="P43">In general, we assume that data were normally distributed but this is not formally tested. We conduct a one-way ANOVA analysis to validate the significance of the type of measure, and employ a two-tailed paired <italic>t</italic>-test with Bonferroni correction for pairwise comparisons to demonstrate that ISI outperforms other measures. In order to evaluate adaptive stretching, we utilize a two-tailed paired <italic>t</italic>-test and calculate the p-value to establish statistical significance. We perform 1000 permutation tests by shuffling task labels to assess how extreme the attention weight is in that distribution is compared to the null distribution. All statistical tests are described in the main text. Levels of statistical significance are indicated as follows: *<italic>p</italic> &lt; .05, **<italic>p</italic> &lt; .01, ***<italic>p</italic> &lt; .001, ****<italic>p</italic> &lt; .0001.</p></sec></sec><sec id="S20" sec-type="extended-data"><title>Extended Data</title><fig id="F6" position="anchor"><label>Extended Data Fig. 1</label><caption><title>CNN-LSTM model</title><p id="P44">In our artificial neural network (ANN), a pre-trained VGG-16 that maps each two-dimensional image (input) to a pre-softmax tensor of size 1000 (output). The output of the CNN, concatenated with pre-softmax logits, serves as an input to a six-layer stacked LSTM. To transform LSTM representations into the decision response, we apply two fully connected (FC) linear transformations, one with 256 units and the other with 3 units (L, R and no response). Each stimulus image is a time stamp, denoted as LSTM time.</p></caption><graphic xlink:href="EMS192511-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Fig. 2</label><caption><title>Spike timing measures best capture the experimenter intended coordinates.</title><p id="P45">We compared the Spearman rank correlation between dissimilarity matrices (i.e., RDMs) constructed from the monkey data for the 16 unambiguous items with an RDM derived from the experimenter-intended stimulus coordinates. A 20 ms sliding window is used and moved in 10 ms steps. A one-way ANOVA analysis is conducted to show the significance of different types of measures, <italic>F</italic>(4, 105) = 124.67, <italic>p</italic> &lt; .0001. ISI measure surpasses other measures and best captures the experimenter-intended coordinates. A 50 ms time window is shown in the main text, indicating the length of time window chosen does not affect the advantage of timing measures in assessing spike trains in such categorization tasks. Time on the horizontal axis is measured from stimulus onset.</p></caption><graphic xlink:href="EMS192511-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Fig. 3</label><caption><title>Spike timing matters in each individual brain region.</title><p id="P46">We compared the Spearman rank correlation between dissimilarity matrices (i.e., RDMs) constructed from the monkey data in each individual brain region for the 16 unambiguous items with an RDM derived from the experimenter-intended stimulus coordinates. A 50 ms sliding window is used here. Spike timing measures best capture the experimenter intended coordinates in six brain regions.</p></caption><graphic xlink:href="EMS192511-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Fig. 4</label><caption><title>Spike timing matters in each individual lobe.</title><p id="P47">We compared the Spearman rank correlation between dissimilarity matrices (i.e., RDMs) constructed from the monkey data in each individual lobe for the 16 unambiguous items with an RDM derived from the experimenter-intended stimulus coordinates. 50 ms sliding window is chosen here. Spike timing measures best capture the experimenter intended coordinates in three lobes (frontal, parietal and temporal).</p></caption><graphic xlink:href="EMS192511-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Fig. 5</label><caption><title>Dimensional stretching occurs in model representations with cosine distance.</title><p id="P48"><bold>a</bold>, Dissimilarity between item pairs mismatching on motion or color is increased when that dimension is task relevant. The density distributions of these dissimilarities also indicate this task-modulation. <bold>b</bold>, Changes in dissimilarity over LSTM time with 60 frames per second in model representations. Stimuli pairs that mismatch on task-relevant dimension become more different in task-relevant context than task-irrelevant context, and dimensional stretching was significantly different than 0 (<italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup> − <italic>D<sub>c</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup>: <italic>M</italic> = 0.4909, <italic>t</italic>(23) = −8.541, <italic>p</italic> &lt; .0001; <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>m</italic></sup> − <italic>D<sub>m</sub></italic>(<italic>i, j</italic>)<sup><italic>c</italic></sup>: <italic>M</italic> = 0.4481, <italic>t</italic>(23) = −9.178, <italic>p</italic> &lt; .0001).</p></caption><graphic xlink:href="EMS192511-f010"/></fig><table-wrap id="T1" orientation="portrait" position="anchor"><label>Extended Data Table 1</label><caption><title>Two-tailed <italic>t</italic>-test with Bonferroni correction <italic>α</italic> = 0.05/<italic>n</italic> = 0.0125 (<italic>n</italic> = 4) was conducted to show that the ISI measure performed best (50 ms) in pairwise comparisons.</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top"><italic>t</italic>-test</th><th align="center" valign="top">ISI</th></tr></thead><tbody><tr><td align="center" valign="top">SPIKE</td><td align="center" valign="top">t(21) = 4.43; p&lt;.0001</td></tr><tr><td align="center" valign="top">Euclidean</td><td align="center" valign="top">t(21) = 14.69; p&lt;.0001</td></tr><tr><td align="center" valign="top">Cosine</td><td align="center" valign="top">t(21) = 15.03; p&lt;.0001</td></tr><tr><td align="center" valign="top">Pearson</td><td align="center" valign="top">t(21) = 15.13; p&lt;.0001</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="anchor"><label>Extended Data Table 2</label><caption><title>Two-tailed <italic>t</italic>-test with Bonferroni correction <italic>α</italic> = 0.05/<italic>n</italic> = 0.0125 (<italic>n</italic> = 4) was conducted to show that the ISI measure performed best (20 ms) in pairwise comparisons.</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top"><italic>t</italic>-test</th><th align="center" valign="top">ISI</th></tr></thead><tbody><tr><td align="center" valign="top">SPIKE</td><td align="center" valign="top">t(21) = 5.80; p&lt;.0001</td></tr><tr><td align="center" valign="top">Euclidean</td><td align="center" valign="top">t(21) = 13.00; p&lt;.0001</td></tr><tr><td align="center" valign="top">Cosine</td><td align="center" valign="top">t(21) = 14.72; p&lt;.0001</td></tr><tr><td align="center" valign="top">Pearson</td><td align="center" valign="top">t(21) = 14.16; p&lt;.0001</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="anchor"><label>Extended Data Table 3</label><caption><title>Permutation test statistics on attentional effects in six brain regions. We performed 1000 times to assess how extreme it is according to the null distributions built up by shuffling task labels.</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top">Region</th><th align="center" valign="top">Permutation test (motion task)</th><th align="center" valign="top">Permutation test (color task)</th></tr></thead><tbody><tr><td align="center" valign="top">PFC</td><td align="center" valign="top">rank=1 , p =.0020</td><td align="center" valign="top">rank=1 , p=.0020</td></tr><tr><td align="center" valign="top">FEF</td><td align="center" valign="top">rank=2 , p=.0040</td><td align="center" valign="top">rank=1 , p=.0020</td></tr><tr><td align="center" valign="top">LIP</td><td align="center" valign="top">rank=2, p=.0040</td><td align="center" valign="top">rank=1, p=.0020</td></tr><tr><td align="center" valign="top">IT</td><td align="center" valign="top">rank=1, p=.0020</td><td align="center" valign="top">rank=2, p=.0040</td></tr><tr><td align="center" valign="top">MT</td><td align="center" valign="top">rank=1, p=.0020</td><td align="center" valign="top">rank=1001, p=.0020</td></tr><tr><td align="center" valign="top">V4</td><td align="center" valign="top">rank=1000, p=.0040</td><td align="center" valign="top">rank=2, p= .0040</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="anchor"><label>Extended Data Table 4</label><caption><title>Permutation test on attentional effects in LSTM multilayers. We performed 1000 times to assess how extreme it is according to the null distributions built up by permuting task labels.</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top">Layer</th><th align="center" valign="top">Permutation test</th></tr></thead><tbody><tr><td align="center" valign="top">1</td><td align="center" valign="top">rank=164, p=.3277</td></tr><tr><td align="center" valign="top">2</td><td align="center" valign="top">rank=8, p=.0160</td></tr><tr><td align="center" valign="top">3</td><td align="center" valign="top">rank=5, p=.0100</td></tr><tr><td align="center" valign="top">4</td><td align="center" valign="top">rank=3, p=.0060</td></tr><tr><td align="center" valign="top">5</td><td align="center" valign="top">rank=1, p=.0020</td></tr><tr><td align="center" valign="top">6</td><td align="center" valign="top">rank=1, p=.0020</td></tr></tbody></table></table-wrap></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS192511-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d210aAdMbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S21"><title>Acknowledgements</title><p>This work was supported by ESRC (ES/W007347/1), Wellcome Trust Investigator Award WT106931MA, and Royal Society Wolfson Fellowship 183029 to B.C.L, JPB Foundation, Picower Institute for Learning and Memory, and Office of Naval Research N00014-23-1-2768 to EKM, and China Scholarship Council (Grant No. 202206260103) to X.Y.Z.</p></ack><sec id="S22" sec-type="data-availability"><title>Data and Code availability</title><p id="P49">The codes used in this work are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/xinyacheung/neural_similarity">https://github.com/xinyacheung/neural_similarity</ext-link>. All behavioral and electrophysiological data are archived at the Centre for Integrative Neuroscience, University of Tübingen, Germany.</p></sec><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual review of neuroscience</source><year>2001</year><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><article-title>Selection, integration, and conflict monitoring: assessing the nature and generality of prefrontal cognitive control mechanisms</article-title><source>Neuron</source><year>2004</year><volume>41</volume><fpage>473</fpage><lpage>487</lpage><pub-id pub-id-type="pmid">14766185</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Roads</surname><given-names>BD</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>The costs and benefits of goal-directed attention in deep convolutional neural networks</article-title><source>Computational Brain &amp; Behavior</source><year>2021</year><volume>4</volume><fpage>213</fpage><lpage>230</lpage><pub-id pub-id-type="pmcid">PMC8550459</pub-id><pub-id pub-id-type="pmid">34723095</pub-id><pub-id pub-id-type="doi">10.1007/s42113-021-00098-y</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braunlich</surname><given-names>K</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>Bidirectional influences of information sampling and concept learning</article-title><source>Psychological review</source><year>2022</year><volume>129</volume><fpage>213</fpage><pub-id pub-id-type="pmcid">PMC8766620</pub-id><pub-id pub-id-type="pmid">34279981</pub-id><pub-id pub-id-type="doi">10.1037/rev0000287</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name></person-group><article-title>Dynamic updating of hippocampal object representations reflects new conceptual knowledge</article-title><source>Proceedings of the National Academy of Sciences</source><year>2016</year><volume>113</volume><fpage>13203</fpage><lpage>13208</lpage><pub-id pub-id-type="pmcid">PMC5135299</pub-id><pub-id pub-id-type="pmid">27803320</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1614048113</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Preston</surname><given-names>AR</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>Ventromedial prefrontal cortex compression during concept learning</article-title><source>Nature communications</source><year>2020</year><volume>11</volume><fpage>46</fpage><pub-id pub-id-type="pmcid">PMC6946809</pub-id><pub-id pub-id-type="pmid">31911628</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13930-8</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruschke</surname><given-names>J</given-names></name></person-group><article-title>Alcove: A connectionist model of human category learning</article-title><source>Advances in Neural Information Processing Systems</source><year>1990</year><volume>3</volume></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosofsky</surname><given-names>RM</given-names></name></person-group><article-title>Attention, similarity, and the identification–categorization relationship</article-title><source>Journal of experimental psychology: General</source><year>1986</year><volume>115</volume><fpage>39</fpage><pub-id pub-id-type="pmid">2937873</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Medin</surname><given-names>DL</given-names></name><name><surname>Gureckis</surname><given-names>TM</given-names></name></person-group><article-title>Sustain: a network model of category learning</article-title><source>Psychological review</source><year>2004</year><volume>111</volume><fpage>309</fpage><pub-id pub-id-type="pmid">15065912</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braunlich</surname><given-names>K</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>Occipitotemporal representations reflect individual differences in conceptual knowledge</article-title><source>Journal of Experimental Psychology: General</source><year>2019</year><volume>148</volume><fpage>1192</fpage><pub-id pub-id-type="pmcid">PMC6586152</pub-id><pub-id pub-id-type="pmid">30382719</pub-id><pub-id pub-id-type="doi">10.1037/xge0000501</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Folstein</surname><given-names>JR</given-names></name><name><surname>Palmeri</surname><given-names>TJ</given-names></name><name><surname>Gauthier</surname><given-names>I</given-names></name></person-group><article-title>Category learning increases discriminability of relevant object dimensions in visual cortex</article-title><source>Cerebral Cortex</source><year>2013</year><volume>23</volume><fpage>814</fpage><lpage>823</lpage><pub-id pub-id-type="pmcid">PMC3593573</pub-id><pub-id pub-id-type="pmid">22490547</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs067</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buffalo</surname><given-names>EA</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Landman</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><article-title>A backward progression of attentional effects in the ventral stream</article-title><source>Proceedings of the National Academy of Sciences</source><year>2010</year><volume>107</volume><fpage>361</fpage><lpage>365</lpage><pub-id pub-id-type="pmcid">PMC2806732</pub-id><pub-id pub-id-type="pmid">20007766</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0907658106</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><article-title>Neural mechanisms of spatial selective attention in areas v1, v2, and v4 of macaque visual cortex</article-title><source>Journal of neurophysiology</source><year>1997</year><volume>77</volume><fpage>24</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">9120566</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname><given-names>BC</given-names></name></person-group><article-title>Focal attention produces spatially selective processing in visual cortical areas v1, v2, and v4 in the presence of competing stimuli</article-title><source>Journal of neurophysiology</source><year>1993</year><volume>70</volume><fpage>909</fpage><lpage>919</lpage><pub-id pub-id-type="pmid">8229178</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jehee</surname><given-names>JF</given-names></name><name><surname>Brady</surname><given-names>DK</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Attention improves encoding of task-relevant features in the human visual cortex</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><fpage>8210</fpage><lpage>8219</lpage><pub-id pub-id-type="pmcid">PMC3134176</pub-id><pub-id pub-id-type="pmid">21632942</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6153-09.2011</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamitani</surname><given-names>Y</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Decoding the visual and subjective contents of the human brain</article-title><source>Nature neuroscience</source><year>2005</year><volume>8</volume><fpage>679</fpage><lpage>685</lpage><pub-id pub-id-type="pmcid">PMC1808230</pub-id><pub-id pub-id-type="pmid">15852014</pub-id><pub-id pub-id-type="doi">10.1038/nn1444</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>M</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>Cortical information flow during flexible sensorimotor decisions</article-title><source>Science</source><year>2015</year><volume>348</volume><fpage>1352</fpage><lpage>1355</lpage><pub-id pub-id-type="pmcid">PMC4721574</pub-id><pub-id pub-id-type="pmid">26089513</pub-id><pub-id pub-id-type="doi">10.1126/science.aab0551</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broschard</surname><given-names>MB</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Wasserman</surname><given-names>EA</given-names></name><name><surname>Freeman</surname><given-names>JH</given-names></name></person-group><article-title>Selective attention in rat visual category learning</article-title><source>Learning &amp; Memory</source><year>2019</year><volume>26</volume><fpage>84</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC6380202</pub-id><pub-id pub-id-type="pmid">30770465</pub-id><pub-id pub-id-type="doi">10.1101/lm.048942.118</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>X</given-names></name><name><surname>Mok</surname><given-names>RM</given-names></name><name><surname>Roads</surname><given-names>BD</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>A controller-peripheral architecture and costly energy principle for learning</article-title><source>bioRxiv</source><year>2023</year><fpage>2023</fpage><lpage>01</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>How biological attention mechanisms improve task performance in a large-scale visual system model</article-title><source>eLife</source><year>2018</year><volume>7</volume><elocation-id>e38105</elocation-id><pub-id pub-id-type="pmcid">PMC6207429</pub-id><pub-id pub-id-type="pmid">30272560</pub-id><pub-id pub-id-type="doi">10.7554/eLife.38105</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><source>Very deep convolutional networks for large-scale image recognition</source><conf-name>The 3rd International Conference on Learning Representations (ICLR)</conf-name><year>2015</year></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Deep supervised, but not unsupervised, models may explain it cortical representation</article-title><source>PLoS computational biology</source><year>2014</year><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="pmcid">PMC4222664</pub-id><pub-id pub-id-type="pmid">25375136</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DL</given-names></name><etal/></person-group><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2014</year><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="pmcid">PMC4060707</pub-id><pub-id pub-id-type="pmid">24812127</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Güçlü</surname><given-names>U</given-names></name><name><surname>van Gerven</surname><given-names>MA</given-names></name></person-group><article-title>Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream</article-title><source>Journal of Neuroscience</source><year>2015</year><volume>35</volume><fpage>10005</fpage><lpage>10014</lpage><pub-id pub-id-type="pmcid">PMC6605414</pub-id><pub-id pub-id-type="pmid">26157000</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5023-14.2015</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><article-title>Seeing it all: Convolutional network layers map the function of the human visual system</article-title><source>NeuroImage</source><year>2017</year><volume>152</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="pmid">27777172</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeman</surname><given-names>AA</given-names></name><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><article-title>Orthogonal representations of object shape and category in deep convolutional neural networks and human visual cortex</article-title><source>Scientific reports</source><year>2020</year><volume>10</volume><elocation-id>2453</elocation-id><pub-id pub-id-type="pmcid">PMC7016009</pub-id><pub-id pub-id-type="pmid">32051467</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-59175-0</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title>Long short-term memory</article-title><source>Neural computation</source><year>1997</year><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name></person-group><article-title>Direct fit to nature: an evolutionary perspective on biological and artificial neural networks</article-title><source>Neuron</source><year>2020</year><volume>105</volume><fpage>416</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC7096172</pub-id><pub-id pub-id-type="pmid">32027833</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.12.002</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Ahlheim</surname><given-names>C</given-names></name><name><surname>Mehrotra</surname><given-names>A</given-names></name><name><surname>Panos</surname><given-names>A</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>Measures of neural similarity</article-title><source>Computational brain &amp; behavior</source><year>2020</year><volume>3</volume><fpage>369</fpage><lpage>383</lpage><pub-id pub-id-type="pmcid">PMC7671987</pub-id><pub-id pub-id-type="pmid">33225218</pub-id><pub-id pub-id-type="doi">10.1007/s42113-019-00068-5</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Representational similarity analysis-connecting the branches of systems neuroscience</article-title><source>Frontiers in systems neuroscience</source><year>2008</year><volume>4</volume><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreuz</surname><given-names>T</given-names></name><name><surname>Haas</surname><given-names>JS</given-names></name><name><surname>Morelli</surname><given-names>A</given-names></name><name><surname>Abarbanel</surname><given-names>HD</given-names></name><name><surname>Politi</surname><given-names>A</given-names></name></person-group><article-title>Measuring spike train synchrony</article-title><source>Journal of neuroscience methods</source><year>2007</year><volume>165</volume><fpage>151</fpage><lpage>161</lpage><pub-id pub-id-type="pmid">17628690</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreuz</surname><given-names>T</given-names></name><name><surname>Chicharro</surname><given-names>D</given-names></name><name><surname>Houghton</surname><given-names>C</given-names></name><name><surname>Andrzejak</surname><given-names>RG</given-names></name><name><surname>Mormann</surname><given-names>F</given-names></name></person-group><article-title>Monitoring spike train synchrony</article-title><source>Journal of neurophysiology</source><year>2013</year><volume>109</volume><fpage>1457</fpage><lpage>1472</lpage><pub-id pub-id-type="pmid">23221419</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sexton</surname><given-names>NJ</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><article-title>Reassessing hierarchical correspondences between brain and deep networks through direct interface</article-title><source>Science Advances</source><year>2022</year><volume>8</volume><elocation-id>eabm2219</elocation-id><pub-id pub-id-type="pmcid">PMC9278854</pub-id><pub-id pub-id-type="pmid">35857493</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abm2219</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><etal/></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>Advances in neural information processing systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulansky</surname><given-names>M</given-names></name><name><surname>Kreuz</surname><given-names>T</given-names></name></person-group><article-title>Pyspike—a python library for analyzing spike train synchrony</article-title><source>SoftwareX</source><year>2016</year><volume>5</volume><fpage>183</fpage><lpage>189</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Overview of the behavioral task and CNN-LSTM modeling.</title><p><bold>a</bold>, Monkeys were shown images depicting the fixation point (0.5 s), cue (1 s), and stimulus (up to 3 s). During each trial, monkeys held central fixation until they responded by making either a left or right saccade. They were not to respond on ambiguous trials. These same images were shown to the ANN model consisting of a pretrained CNN for visual processing and a stacked (i.e., multilayer) LSTM that learned how to appropriately respond given the task cue and stimulus. Like the monkeys, the CNN-LSTM learned through trial-and-error to respond left (L), right (R), or to withhold (N) a response on ambiguous trials. <bold>b</bold>, Spiking data were analyzed over a 250 ms period (shown in purple) that began at stimulus onset. Analyses either considered this time period as a whole or in 50 ms overlapping segments using a sliding window (shown in green) that was 50 ms wide and moved in 10 ms steps. <bold>c</bold> There were 21 color-motion stimuli constructed from the color and motion space. Five stimuli were ambiguous at the origin of the color-motion space. The remaining 16 stimuli were evenly spread across the four quadrants. A cross or quatrefoil cue indicated that motion was relevant to the decision. A circle or triangle cue indicated the color was relevant. In both task contexts, the monkeys responded with a leftward or rightward saccade. <bold>d</bold>, From the 16 non-ambiguous trials, we constructed 16x16 dissimilarity matrices (i.e., RDMs) for data analyses. To determine the value of an entry in the RDM, the spiking measures for two items would be compared using a particular distance function (e.g., ISI, Euclidean distance, etc.) and the value was stored in the row and column corresponding to the two items. Likewise, RDMs were made for the ANN using the activity patterns from LSTM’s hidden unit activations.</p></caption><graphic xlink:href="EMS192511-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Spike timing measures best capture the experimenter intended coordinates.</title><p>We compared the Spearman rank correlation between dissimilarity matrices (i.e., RDMs) constructed from the monkey data for the 16 unambiguous items (see <xref ref-type="fig" rid="F1">Fig 1d</xref>) with an RDM derived from the experimenter-intended stimulus coordinates (see <xref ref-type="fig" rid="F1">Fig 1c</xref>). Higher correlation indicates better agreement. <bold>a</bold>, When motion was relevant, the timing measures ISI and SPIKE surpass rate coding measures and best capture the experimenter-intended coordinates. <bold>b</bold>, The same pattern was found when color was relevant to the monkey’s decision with ISI once again proving best. The band around each dissimilarity measure depicts the 95% confidence intervals. Time on the horizontal axis is measured from stimulus onset (see <xref ref-type="fig" rid="F1">Fig 1b</xref>). <bold>c, d</bold>, The same pattern of results holds when the entire 250 ms time period is analyzed as a whole.</p></caption><graphic xlink:href="EMS192511-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Dimensional stretching occurs in both neural data and model representations.</title><p><bold>a</bold>, Dissimilarity between item pairs mismatching on motion or color is increased when that dimension is task relevant. The density distributions of these dissimilarities also indicate this task-modulation. <bold>b</bold>, Changes in dissimilarity over trial time (time 0 s is stimulus onset). For stimulus pair (<italic>i, j</italic>), the underscore index indicates the dimension they mismatch, e.g., <italic>D<sub>c</sub></italic>(<italic>i, j</italic>) indicates items <italic>i</italic> and <italic>j</italic> mismatch on color and match on motion. Item pairs that mismatch on motion are more different in the motion-relevant context than in the color-relevant context. Mirroring, item pairs that mismatch on color are more different in the color-relevant context than in the motion-relevant context. <bold>c</bold>, <bold>d</bold> The CNN-LSTM model shows the same qualitative pattern of performance – stimulus pairs that mismatch on the task relevant dimension were most dissimilar.</p></caption><graphic xlink:href="EMS192511-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Task-relevant attention allocation in brain regions and LSTMs layers.</title><p><bold>a</bold>, The allocation of attention (estimated by fitting a cognitive model) across the six brain regions is shown. MT prioritizes motion and V4 color regardless of the task context. <bold>b</bold>, Fitting the cognitive model to LSTM representations revealed that more advanced layers devoted more attention to the relevant stimulus dimension.</p></caption><graphic xlink:href="EMS192511-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Alignment between brain region and LSTM layers.</title><p><bold>a</bold>, One hypothesis, supported by our previous analyses, is that recording sites, with the exception of MT (motion fixated) and V4 (color fixated), should align equally well with LSTM activity in the motion and color tasks. <bold>b</bold>, Alignment, as measured by the Spearman correlation of the RDMs (<xref ref-type="fig" rid="F1">Fig. 1d</xref>) for brain and model activity, is shown. Most brain regions generally aligned across model layers and tasks. IT was generally misaligned. In contrast, MT better aligned in the motion task, whereas V4 better aligned in the color task. Consistent with previous analyses, MT and V4 do not seem to reconfigure as a function of task.</p></caption><graphic xlink:href="EMS192511-f005"/></fig></floats-group></article>