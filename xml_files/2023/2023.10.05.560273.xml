<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189386</article-id><article-id pub-id-type="doi">10.1101/2023.10.05.560273</article-id><article-id pub-id-type="archive">PPR738274</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Beyond human-likeness: Socialness is more influential when attributing mental states to robots</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jastrzab</surname><given-names>Laura E.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Chaudhury</surname><given-names>Bishakha</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ashley</surname><given-names>Sarah A.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Koldewyn</surname><given-names>Kami</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Cross</surname><given-names>Emily S.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref></contrib><aff id="A1"><label>1</label>Institute for Cognitive Neuroscience, School of Human and Behavioural Science, Bangor University, Wales, UK</aff><aff id="A2"><label>2</label>Institute for Neuroscience and Psychology, School of Psychology, University of Glasgow, UK</aff><aff id="A3"><label>3</label>Division of Psychiatry, Institute of Mental Health, University College London, UK</aff><aff id="A4"><label>4</label>Chair for Social Brain Sciences, Department of Humanities, Social and Political Sciences, ETHZ, Zürich, Switzerland</aff></contrib-group><author-notes><corresp id="CR1"><label>*</label><italic>Correspondence</italic>: Emily S. Cross: <email>ecross@ethz.ch</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>06</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P1">We sought to replicate and expand previous work showing that the more human-like a robot appears, the more willing people are to attribute mind-like capabilities and socially engage with it. Forty-two participants played games against a human, a humanoid robot, a mechanoid robot, and a computer algorithm while undergoing functional neuroimaging. Replicating previous studies, we confirmed that the more human-like the agent, the more participants attributed a mind to them. However, exploratory analyses revealed that beyond humanness, the perceived <italic>socialness</italic> of an agent appeared to be as important, if not more so, for mind attribution. Our findings suggest that top-down knowledge cues are just as important, if not more so, than bottom-up stimulus cues when exploring mind attribution in non-human agents. While further work is now required to test this hypothesis directly, these preliminary findings hold important implications for robotic design and to understand and test the flexibility of human social cognition when people engage with artificial agents.</p></abstract><kwd-group><kwd>Social robotics</kwd><kwd>Second-person neuroscience</kwd><kwd>Social Cognition</kwd><kwd>Mentalizing</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Robots have sparked curiosity and been romanticised in popular culture since von Kempelen’s “Chess Turk” was introduced in 1769. In the mid-20<sup>th</sup> century, Alan Turing formalised the philosophical debate as to whether “machines think”,<sup><xref ref-type="bibr" rid="R1">1</xref></sup> a question that continues to captivate many philosophical and science fiction writers. With the present study, however, we ask what might be thought of as the <italic>opposite</italic> question: namely, regardless of whether robots think, do <italic>we humans</italic> perceive robots as having minds of their own? If so, do we do so primarily based on how human-like the robot looks, or does its perceived socialness also matter?</p><p id="P3">Robots are already commonplace in assembly lines, factories, and dangerous jobs such as pipeline and fuel tank inspections, as well as underwater and space exploration.<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref></sup> As the deployment of robots in these contexts grows, so does their introduction to social and leisure domains, aiding people with, for example, surgeries in healthcare, serving customers in restaurants, learning in schools, and supporting adults who need help with daily living skills (for example, <sup><xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R8">8</xref></sup>). Robots’ roles in our day-to-day lives so far, however, are typically “single-use” (e.g., robot vacuum cleaners or a robot check-in assistant at a hotel), and the ability of even the most sophisticated social robots to engage us socially is still far removed from depictions in science fiction novels and films.<sup><xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup> Rapid advances in hardware and artificial intelligence are expected over the coming decades, making this a crucial time to examine human engagement with robots. This is particularly true in the social domain if we are to develop machines that can indeed engage and collaborate with humans in complex social contexts.</p><p id="P4">As adults, humans typically and intuitively think of other humans as having a mind, thoughts, and intentions that are different from their own, a skill known as mentalizing.<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup> Mentalizing is important for social interactions, allowing us to read and react to others’ unspoken mental and emotional states, and their intended actions.<sup><xref ref-type="bibr" rid="R11">11</xref></sup> Neuroimaging studies have used implicit (e.g., economic games) and explicit (e.g., mind-in-the-eyes) tasks to probe human brain activity associated with mentalizing (for a review, see <sup><xref ref-type="bibr" rid="R13">13</xref></sup>). This work has identified the so-called mentalizing network, a group of brain regions thought to support thinking about others’ minds. The core regions reliably included as part of the mentalizing network include bilateral temporal-parietal junction (TPJ), medial prefrontal cortex (mPFC), and Precuneus (PreC) but engagement of additional brain regions, including posterior superior temporal sulcus (pSTS), temporal poles, and posterior cingulate cortex (PCC), have also been implicated.<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R18">18</xref></sup>The mentalizing network is readily engaged during interactions with other humans, especially when trying to predict their future actions. Very few neuroimaging studies, however, have directly addressed the extent to which mentalizing brain regions, which have ostensibly evolved to interpret other people’s actions and intentions, also process non-human social partners such as robots. Understanding whether humans mentalize about robots is important for at least two reasons. First, the more we attribute a mind to robots, the more likely we are to interact with and engage with them socially.<sup><xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup> Second, examining mentalizing in response to robot social partners tests the flexibility of our social cognitive system by assessing the extent to which a system that evolved to support interactions with fellow humans can be engaged during interactions with non-human agents (in this case, robots).<sup><xref ref-type="bibr" rid="R22">22</xref></sup> Prior neuroimaging studies studying the extent to which humans mentalize about robots have used empathy tasks,<sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup> spatial cueing tasks,<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R25">25</xref></sup>and economic games.<sup><xref ref-type="bibr" rid="R26">26</xref>–<xref ref-type="bibr" rid="R28">28</xref></sup>Several of these studies demonstrate that human—robot interactions (HRI) activate the mentalizing network, but to a lesser degree than human—human interactions <sub>(HHI).</sub>26,27,29</p><p id="P5">One influential theory that might help to explain the pattern of activity reported so far is the ‘like-me’ hypothesis,<sup><xref ref-type="bibr" rid="R30">30</xref></sup> which posits that the more human-like a non-human agent appears, the more readily social brain networks are engaged. Indeed, behavioural data generally support this idea. For example, the more human-like a robot appears, the more a human user will expect that robot to behave like a human.<sup><xref ref-type="bibr" rid="R31">31</xref></sup> Furthermore, a robot’s appearance influences our assumptions about its behavioural capabilities<sup><xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R34">34</xref></sup> and the extent to which we attribute intentionality or a mind to them.<sup><xref ref-type="bibr" rid="R19">19</xref>–<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R35">35</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup> Likewise, the degree to which we anthropomorphize robots (or attribute human-like qualities to them) has also been found to depend upon a robot’s human-like appearance and behaviour.<sup><xref ref-type="bibr" rid="R37">37</xref>–<xref ref-type="bibr" rid="R40">40</xref></sup> Given the behavioural evidence, it is perhaps not surprising that similar results are found when examining socio-cognitive brain systems. For example, Krach and colleagues<sup><xref ref-type="bibr" rid="R27">27</xref></sup> reported that the increasing human-likeness of game partners’ physical features was associated with increasing engagement of mentalizing network regions during an implicit mentalizing task (in this case, an iterative prisoner’s dilemma game). Together, behavioural and brain imaging findings support the idea that the human-likeness of an interactive partner’s appearance plays a key role in engaging socio-cognitive processes like mentalizing. However, emerging evidence raises the possibility that human-likeness alone may not fully explain which robots are seen as more desirable social partners and, thus, which robot features might be most effective at eliciting the strongest human-like social-cognition processes.<sup><xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup> The influence of a robot’s <italic>social</italic> features, per se, on human perception and engagement is an emerging area of research that will benefit from expertise from the Human Robot Interaction (HRI), social robotics, and cognitive neuroscience communities.</p><p id="P6">In the current study, we sought to replicate prior findings that the mentalizing network increases in responsiveness as the appearance of robots increases in human-likeness. In additional exploratory analyses, we sought to explore the extent to which a partner’s perceived <italic>socialness</italic> (independent from human-like physical features) might also contribute to this process. To do so, we used an established implicit mentalizing task where participants play rock-paper-scissors (RPS)<sup><xref ref-type="bibr" rid="R43">43</xref></sup> against a human and several artificial agents. We followed an experimental design like that reported by Chaminade and colleagues.<sup><xref ref-type="bibr" rid="R26">26</xref></sup> The RPS game itself is familiar across cultures and age groups, and if it is unfamiliar, it is easy to learn. Also like Chaminade and colleagues,<sup><xref ref-type="bibr" rid="R26">26</xref></sup> we used videos of game partners to increase the sense of live interactions during game play. We controlled wins and losses across all game partners, and explicitly told participants that the robot competitors had been endowed with artificial intelligence and would play strategically. Similar to Krach and colleagues,<sup><xref ref-type="bibr" rid="R27">27</xref></sup> we included two robotic partners that differed in their human-like appearance. One robot appeared humanoid, with clear human-like features including a body, torso, arms, hands, fingers, head and eyes. The other was a mechanoid robot, which had expressive eyes but no other human-like physical features (see Fig 5). Importantly, both the humanoid and mechanoid robots in our study are designed to engage people with socially interactive behaviours.</p><p id="P7">From prior data, we expected that both robots would engage the mentalizing network, though to a lesser extent than the human game-partner. Indeed, we preregistered a prediction that the magnitude of response of core brain regions within the mentalizing network (specifically TPJ, mPFC, and Precuneus) would linearly increase as game partners increased in human-like appearance. We further explored the extent to which participants found each robotic game partner fun, sympathetic, competitive, successful, strategic, intelligent, and competitive. Here we hypothesized, again based on previous findings<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> that these factors would increase with increasing human-likeness. Finally, in an exploratory analysis, to address questions related to participants’ perceptions of the socialness of the different game partners, we reversed the order of the robots in our linear contrast models, allowing us to test the extent to which this “perceived socialness” might explain differences in the engagement of the mentalizing network across game partners <italic>better</italic> than simply the agents’ physical appearance.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Neuroimaging Results</title><sec id="S4"><title>Socialness and human-likeness influence mentalizing but socialness is more robust</title><sec id="S5"><title>Pre-registered</title><p id="P8">Repeated-measures ANOVAs with game partner as a within-subjects factor was significant in several key mentalizing ROIs during game play (bilateral TPJ and left middle frontal gyrus (lmFG)), as well as, bilateral pSTS. Follow-up paired sample t-tests in bilateral pSTS and lTPJ revealed that this was largely driven by higher activity in response to the human compared to all other conditions, suggesting that these regions are more reliably engaged by human than artificial stimuli (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table 5</xref>). Right TPJ was an exception, in that, while the human significantly differed from both robots, no significant difference between the human and computer was found. No other significant comparisons during gameplay and within these ROIs remained after correcting for multiple comparisons.</p><p id="P9">Results from the pSTS revealed significant differences between game players while playing the game (rpSTS: F(3, 123) = 12.39, p &lt; 0.001, np = 0.23; lpSTS: F(3, 123) = 6.96, p &lt; 0.001, np = 0.15), which was unexpected as there were no visual differences during game play across the 4 conditions.</p><p id="P10">Contrary to our expectations, mentalizing regions were not activated above baseline during the RPS games. Average activity across the group was close to zero or, indeed, slightly negative across nearly all conditions (see <xref ref-type="fig" rid="F1">Figs 1</xref>, <xref ref-type="supplementary-material" rid="SD1">S2 &amp; Table S5</xref>).</p></sec><sec id="S6"><title>Exploratory</title><p id="P11">Additionally, the pSTS revealed strong significant differences across game partners while participants watched the introductory video (video 1) of each game partner before playing commencing each game series (rpSTS: F(3, 123) = 29.40, p &lt; 0.001, np = 0.42; lpSTS: F(3, 123) = 13.26, p &lt; 0.001, np = 0.24). While none of the other ROIs revealed significant pairwise differences between either robot and the computer, there was a significant difference between MR and CP in rpSTS (and approached significance in lpSTS) during the video preceding gameplay (rpSTS: p &lt; 0.001, d = -0.73; lpSTS: p = 0.056, d = -0.32; See <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S5</xref>).</p></sec></sec><sec id="S7"><title>Linear effect of human-likeness in mentalizing ROIs during gameplay</title><sec id="S8"><title>Pre-Registered</title><p id="P12">All mentalizing ROIs which revealed a significant within subject effect of partner (Bilateral TPJ, lmFG, and bilateral pSTS) also revealed a significant linear within-subjects contrast effect of human-likeness (HP &gt; HR &gt; MR &gt; CP), as predicted (see <xref ref-type="supplementary-material" rid="SD1">Table S5</xref>).</p></sec><sec id="S9"><title>Exploratory</title><p id="P13">We explored whether changing the order of the robots in the within-subject contrasts according to socialness ratings further bolstered the linear effect (HP &gt; MR &gt; HR &gt; CP, see <xref ref-type="supplementary-material" rid="SD1">Table S5</xref>). Results from behavioural ratings suggested that socialness (as assessed by perceived fun, competitiveness, and sympathy, see below) models were improved by reversing the order of the robots. Indeed, across ROIs, the mechanoid robot evoked numerically higher, though often not significantly so, responses than the humanoid robot. Despite the lack of statistically significant differences between the robots in pairwise comparisons, the linear effect of ‘socialness’ resulted in a larger effect size than the ‘humanness’ model, suggesting socialness may be even more important than humanness in mind attribution toward robots, as measured by engagement of brain regions associated with mentalizing.</p></sec></sec><sec id="S10"><title>The mechanoid is more similar to the human than the humanoid or computer</title><sec id="S11"><title>Pre-Registered</title><p id="P14">No FWE (p &lt; .05) or uncorrected (p &lt; .001) clusters survived simple whole brain contrasts between the humanoid or mechanoid and the computer (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S4</xref>). There were no significant clusters during the [Humanoid (HR) &gt; Mechanoid (MR)] but the inverse contrast revealed a significant cluster (k = 313) in nucleus accumbens (MNI: -4 10 -10). The [Human Partner (HP) &gt; Computer Partner (CP)] contrast resulted in significant mentalizing clusters in bilateral TPJ, mFG, mPFC, precuneus, rpSTS, IFG, nucleus accumbens, and cerebellum.</p><p id="P15">To assess whether regions outside our pre-selected ROIs might be sensitive to Human-likeness, we tested whether any brain regions showed a pattern of activity such that Human Partner (HP) &gt; Humanoid Robot (HR) &gt; Mechanoid Robot (MR) &gt; Computer Partner (CP). This analysis revealed that rTPJ, precuneus, mPFC, bilateral mFG, and nucleus accumbens all survived the FWE-corrected peak-level threshold.</p></sec><sec id="S12"><title>Exploratory</title><p id="P16">When the human was compared to the humanoid and mechanoid robots, several regions associated with mentalizing were significant at the cluster level after FWE correction (see <xref ref-type="fig" rid="F2">Fig. 2</xref>). The [HP &gt; HR] contrast resulted in significant clusters in bilateral TPJ, precuneus, rmFG, rIFG, rpSTS after FWE corrections. The [HP &gt; MR] contrast yielded significant engagement of rTPJ, precuneus, rpSTS, and cerebellum after FWE corrections.</p><p id="P17">In line with our socialness questions, we also tested whether any brain regions showed a pattern of activity if we reversed the order of the robots in our parametric analysis; i.e., so that the order was now: Human Partner (HP) &gt; Mechanoid Robot (MR) &gt; Humanoid Robot (HR) &gt; Computer Partner (CP). Results revealed a similar pattern to both HP&gt;CP and the HP&gt;HR&gt;MR&gt;CP model above but now also included significant clusters in: bilateral pSTS, supplementary motor area, rIFG,&amp; lTPJ. Please see <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref> and <xref ref-type="supplementary-material" rid="SD1">Table S4</xref>.</p></sec></sec></sec><sec id="S13"><title>Behavioural Results</title><sec id="S14"><title>Manipulation Check</title><p id="P18">During verbal debriefing with participants, six out of 42 neuroimaging participants questioned whether the videos were live during our verbal debriefing. Given this, we re-ran all behavioural and neuroimaging analyses with only the “true believers” (see OSF project page for details). Doing so did not change the findings in either degree or direction of significance. Therefore, the analyses are reported with the full sample, including the non-believers.</p></sec><sec id="S15"><title>Debrief Questions: Mechanoid perceived as more social, but not intelligent, than the humanoid</title><sec id="S16"><title>Pre-Registered</title><p id="P19">All pairwise comparisons in this section were corrected for multiple comparisons (Bonferroni). Greenhouse-Geisser corrections were made if any rmANOVA was found to violate Mauchley’s tests of sphericity (see <xref ref-type="fig" rid="F3">Figure 3</xref>, S3, &amp; <xref ref-type="supplementary-material" rid="SD1">Table S6</xref> for details from this section).</p><p id="P20">We found no effect of perceived <italic>success</italic> in winning (F(3, 123) = 0.50, p = 0.685, ηp<sup>2</sup>= .012) or <italic>strategy</italic> employed (F(3, 123) = 0.32, p = 0.811, ηp<sup>2</sup> = .008) against each game partner, despite stressing to participants that the computer was using a random algorithm, while the other partners were all trying to win. <italic>Fun</italic> (F(3, 123) = 33.90, p &lt; 0.001, ηp<sup>2</sup> = .453), <italic>Competitiveness</italic> (F(3, 123) = 17.24, p &lt; 0.001, ηp<sup>2</sup> = .296), <italic>Sympathy</italic> (F(2.50, 102.58) = 58.59, p &lt; 0.001, ηp<sup>2</sup> = .588; Greenhouse-Geisser corrected) and <italic>Intelligence</italic> (F(2.51, 102.91) = 12.16, p &lt; 0.001, ηp<sup>2</sup> = .229; Greenhouse-Geisser correction) were all significantly different amongst the four conditions and followed a significant linear pattern based on human-likeness.</p></sec><sec id="S17"><title>Exploratory</title><p id="P21">However, <italic>Fun, Competitiveness</italic>, and <italic>Sympathy</italic>, revealed a stronger linear pattern based on socialness, wherein the two robots were reversed in the order of the within subject contrasts. However, only post-hoc tests on ratings of <italic>Fun</italic> and <italic>Competitiveness</italic> showed differences between robots, where mean ratings for the mechanoid robot were higher than for the humanoid robot (p=0.006 &amp; p=0.049, respectively).</p></sec></sec><sec id="S18"><title>Inclusion of Others and Self (IOS): No difference in perception of closeness between the robots or a human stranger</title><p id="P22">IoS scores varied significantly between the 6 agents (F(3.70, 148.02) = 122.40, p &lt; 0.001, ηp<sup>2</sup> = 0.754). Pairwise comparisons of the computer, human game partner, and close friend significantly differed from all other agents and each other on the IoS, even after correcting for multiple comparisons (Bonferroni). Pairwise comparisons of the mechanoid robot, humanoid robot, and human stranger did not significantly differ from each other (see <xref ref-type="fig" rid="F4">Fig. 4</xref> &amp; <xref ref-type="supplementary-material" rid="SD1">Table S7</xref>).</p></sec></sec></sec><sec id="S19" sec-type="discussion"><title>Discussion</title><p id="P23">With the present study, we have replicated and extended previous findings, demonstrating that both human likeness and perceived ‘socialness’ shape the extent to which participants engage mentalizing regions while playing games against robotic partners. We found that although human-likeness models showed increased theory-of-mind network engagement (as predicted and pre-registered), the socialness model was even more robust. While this analysis was exploratory and will require replication via hypothesis-confirming follow-up work, it is important for two reasons. First, it suggests that mentalizing processes during interactive exchanges (in this case, a game) are better predicted by how <italic>social</italic> we find our interaction partner, rather than being solely based on how human-like they look. This finding has the potential to update our models of how mentalizing systems can be engaged, particularly by non-human interactants. Secondly, the extent to which humans will ascribe mental states to robots is likely to become increasingly relevant as roboticists develop increasingly sophisticated embodied artificial agents designed to engage human users on a social level. Successful social interactions with such social robots will require people to think about how the robot “thinks”. A better understanding of the factors that influence mentalizing towards and about robots should lead to higher quality and more sustained long-term interactions with robots in social domains (e.g.,<sup><xref ref-type="bibr" rid="R42">42</xref></sup>).</p><p id="P24">As with the two previous neuroimaging studies on which we based our current study, we found increasing activation in mentalizing regions with increasing human-likeness.<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> We also found similar behavioural ratings, showing that while participants did not perceive strategy and success differently across game partners (suggesting participants did not feel that they won or lost more against any one game partners), participants did perceive the game partners differently based on social factors like perceived intelligence, fun, competitiveness, and sympathy. However, unlike previous studies, we explored how these social factors might contribute to mind attribution and found that reversing the order of robots in our linear contrast models to reflect participants’ evaluations of socialness resulted in numerically stronger models than those based on the human-likeness of physical features alone.</p><sec id="S20"><title>Quantifying and exploring human-likeness vs. socialness</title><p id="P25">While the human and social models were both significant and strong, one possibility for the numerically stronger social model is that the mechanoid robot was perceived as more social because it exhibited higher levels of hedonic factors (as rated by fun, competitiveness, and sympathy) than did the humanoid robot. This finding is consistent with participant qualitative perceptions and behavioural ratings of this same robot in recently published work.<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup> For example, in one scenario from our study, when the mechanoid robot lost the RPS series, it pouted and slammed its forklift on the table while moving around in circles in protest. Whereas, when the humanoid robot lost, it responded similarly to the human in a more measured manner, by lowering its arms and shaking its head and/or looking down in defeat. While these differences in personality and behaviour were not objectively measured in our study, others report that manipulating social features of robots such as personality,<sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup> emotional arousal,<sup><xref ref-type="bibr" rid="R48">48</xref></sup> and other hedonic features such as enjoyment and sociability<sup><xref ref-type="bibr" rid="R49">49</xref></sup> can increase user engagement, acceptance, and/or satisfaction.</p><p id="P26">The neuroimaging evidence from this study supports both human likeness and socialness models when attributing mental states. Bilateral TPJ, bilateral pSTS, and lmFG showed significant increases with human-likeness and a numerically stronger linear increase with socialness. While we expected the whole mentalizing network and pSTS to show a similar response pattern, the exceptions were in mPFC, precuneus, and rmFG.</p><p id="P27">We were unable to clearly assess the role played by our mPFC, Precuneus, and rmFG ROIs in this study, as we found no significant differences to emerge between the agents during game play. However, a wealth of research has proposed that these regions are central to mentalising and animacy (e.g. <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>). As our localisers did not reliably elicit mPFC or rmFG response in this participant cohort, we created ROI from coordinates in the original localiser paper.<sup><xref ref-type="bibr" rid="R50">50</xref></sup> It is possible that our “generic” ROIs failed to capture individuals’ peak mentalizing voxels across these regions. However, mPFC and rmFG activation clearly emerges in many of our group whole brain contrasts. Precuneus clusters in our localizer and whole brain contrasts were large and the peak cluster from the localiser was more inferior and lateral than the peak clusters in the whole brain contrasts. Last, it is also possible that our localisers produced coordinates for offline social cognition or mentalizing and not for online social cognition.<sup><xref ref-type="bibr" rid="R51">51</xref></sup>Thus, it seems likely that mPFC, rmFG, and Precuneus do likely play a role in mentalizing in our study, but were not well captured by our chosen ROI coordinates.</p><p id="P28">We also explored the response profile of a region in the pSTS that is sensitive to interactive information in observed dyadic social interactions.<sup><xref ref-type="bibr" rid="R52">52</xref></sup> This region is nearby, but distinct from the TPJ, and might plausibly discriminate between game partners. Response in the pSTS discriminated between game partners both during game play and during the video preceding each game series. This was somewhat surprising as the pSTS is largely responsive to the perceptual features of interactions, particularly biological motion.<sup><xref ref-type="bibr" rid="R53">53</xref>,<xref ref-type="bibr" rid="R54">54</xref></sup> In our design, there were no social perceptual features to process during game play as players observed the same visual stimuli during game play across all four conditions. This suggests that perhaps top-down knowledge cues may be more influential in this region than previously thought. We further explored this data by testing our linear human-likeness and socialness models on the pSTS data from video 1 and gameplay. Both models were significant, but in this case the social model was numerically stronger during both gameplay (rpSTS only) and video 1 (bilateral pSTS). The pSTS has been implicated as a part of the social cognition and mentalising networks and has previously been shown to integrate both perceptual and social features.<sup><xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R55">55</xref>–<xref ref-type="bibr" rid="R57">57</xref></sup>The pSTS also responds strongly to social interactions between non-human agents such as moving shapes and dots of light that mimic social scenarios (e.g. <sup><xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R57">57</xref>,<xref ref-type="bibr" rid="R58">58</xref></sup>), and do so even more strongly when participants are led to believe an object is animate versus inanimate.<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R59">59</xref></sup> One possibility is that because participants were engaging in a real-time interaction in our study, the pSTS was more strongly driven by the social features of game partners rather than their visual features. When motion and visual cues to humanness conflict or are not reliably aligned with more top-down attributions of socialness, the more superior regions in the pSTS may prioritise top-down knowledge cues to humanness in social interactions.</p><p id="P29">While our neuroimaging and behavioural results indicate a linear effect of human likeness and socialness across conditions, pairwise comparisons from our ROIs also show that the human partner is perceived significantly differently from all others game partners. While this result is perhaps unsurprising, it suggests that a uniquely human factor still differentiates people from animate non-human entities, even when they are quite ‘human like’ in appearance or behaviour. This result has been reported previously,<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup> and is consistent with the idea that the mentalizing system may be best tuned to human actors and human social cues. It is possible with advancing technology and design that the line between robots and humans may blur, and mentalising regions will become increasingly recruited.</p><p id="P30">One surprise in our results is that game play did not drive responses in mentalizing regions above baseline. Our expectation, based on prior,<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup> was that this task would indeed drive engagement of the mentalizing network, at least for the human partner, above baseline. Previous studies<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> found negative activation to the computer condition and to the non-android robots in mentalizing ROIs, but above-baseline response to the human partner. One possibility here is that our task was particularly demanding, requiring not only mentalizing but also analysing and remembering strategies for each opponent. Thus, we think it is likely that the negative responses seen in our results are a result of most mentalizing regions being part of, or close to, the default mode network, which tends to deactivate during difficult or demanding tasks.<sup><xref ref-type="bibr" rid="R60">60</xref></sup> If true, an active baseline<sup><xref ref-type="bibr" rid="R61">61</xref></sup> such as the computer condition, might be a better baseline for computing response differences across the experimental conditions. Despite the negative response in mentalizing regions, the between-condition differences, evident in mentalizing regions across both ROI and whole-brain analyses, are still interpretable.</p><p id="P31">As with previous studies, and unbeknownst to the participants, we controlled wins and losses amongst game partners so our findings could not be explained by winning or losing more to any one partner. Participants’ ratings of success and strategy against each of the 4 game partners did not significantly differ, suggesting that they accurately perceived their own performance, including that their strategy did not work any more efficiently for one partner than another, similar to previous findings.<sup><xref ref-type="bibr" rid="R26">26</xref></sup> Therefore, it is unlikely that our findings are due to perceived differences in difficulty in playing each partner. Employing a strategic approach to the game likely relates to thinking about the mind of the other player, and thus to activity in the mentalizing network. As a result, participants in this study may have reduced their mentalizing about game partners as they found that their strategies were not working. Future studies might look at manipulating wins and losses or alter initial briefing instructions to create different impressions of each game partner’s fun and competitiveness in order to explore the extent to which socialness can be manipulated to influence mind attribution toward robots.</p><sec id="S21"><title>Theoretical implications</title><p id="P32">Our results support growing evidence emerging from the intersection of social robotics and social neuroscience that multiple routes exist to non-human agents being perceived as “like-me”,<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R42">42</xref></sup> including not only a human-like appearance or motion profile, but also being perceived as ‘social’ based on behaviours or background knowledge about a robot. Significant R&amp;D investment continues to fuel the development of socially interactive robots with whom human users can intuitively and effectively collaborate, which often attempt to capture as much human-likeness as possible while also avoiding the uncanny valley.<sup><xref ref-type="bibr" rid="R62">62</xref>–<xref ref-type="bibr" rid="R67">67</xref></sup> However, the extent to which an agent is perceived as “like-me” extends beyond physical form, capabilities, and movement, and growing evidence supports that prior knowledge about and the perceived socialness of a robot may more strongly influence their reception (and people’s ability to collaborate or cooperate with them in an intuitive manner) in social settings<sup><xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R68">68</xref>–<xref ref-type="bibr" rid="R73">73</xref></sup></p><p id="P33">A few neuroimaging studies have investigated how these top-down knowledge cues and bottom-up stimulus cues influence perceptions of animacy and the flexibility of our social cognitive system. One study found that stimulus cues overrode knowledge cues to animacy <sup><xref ref-type="bibr" rid="R74">74</xref></sup>; whereas, others found the inverse, knowledge, not stimulus, cues more strongly influenced animacy perception.<sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R75">75</xref></sup> Yet, a key mentalizing region (rTPJ) was most sensitive when <italic>both</italic> stimulus and knowledge cues to animacy were presented compared to when only one (or none) of those cues were present.<sup><xref ref-type="bibr" rid="R36">36</xref></sup> These various findings are likely influenced by the type of task and cues used, and our study adds to the narrative that top-down knowledge-based cues of socialness can be just as, if not more, powerful for driving mind attribution during social interactions with artificial agents than bottom-up visual cues to human-likeness alone.</p><p id="P34">Therefore, physical features denoting human-likeness may not be the most important consideration for those designing socially engaging robots, and instead a reorientation toward an emphasis on socialness may be more fruitful for fostering social behaviours and attitudes toward robots. Ultimately, our findings set the stage for future work to disentangle not only which physical and social features play the most important roles in mind attribution to artificial agents, but also how ongoing experience with such agents changes and develops such perceptions.</p></sec><sec id="S22"><title>Concluding thoughts</title><p id="P35">Our primary findings confirm previous research that human-likeness plays an important role in the attribution of mind to robots. However, our exploratory analyses suggest that the perceived socialness of a robot also plays an equally, if not more important role than physical features denoting human-likeness in mind attribution. Incorporating knowledge- or experience-based social cues and features into robots who are designed to engage human users on a social level has the potential to increase user engagement and interest for more lasting and higher quality relationships with our robotic partners.</p></sec></sec></sec><sec id="S23"><title>Star Methods</title><table-wrap id="T1" position="anchor"><table frame="box" rules="all"><thead><tr style="border-top:solid 1px #f5f4f4;border-right:solid 1px #f5f4f4;border-left:solid 1px #f5f4f4;border-bottom:solid 1px #80c6ee"><th valign="top" align="left">REAGENT or RESOURCE</th><th valign="top" align="left">SOURCE</th><th valign="top" align="left">IDENTIFIER</th></tr></thead><tbody><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" colspan="3"><bold>Deposited Data</bold></td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Data supporting the main findings<sup><xref ref-type="fn" rid="FN1">1</xref></sup></td><td valign="top" align="left" style="border-right:solid 2px #000000">OpenNeuro</td><td valign="top" align="left"/></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Group level whole brain results</td><td valign="top" align="left" style="border-right:solid 2px #000000">Neurovault</td><td valign="top" align="left"><ext-link ext-link-type="uri" xlink:href="https://neurovault.org/collections/EERVYAPX/">https://neurovault.org/collections/EERVYAPX/</ext-link></td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Behavioural data &amp; stimuli examples</td><td valign="top" align="left" style="border-right:solid 2px #000000">OSF</td><td valign="top" align="left"/></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Preregistration</td><td valign="top" align="left" style="border-right:solid 2px #000000">AsPredicted.org</td><td valign="top" align="left"><ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/CBG_ZPG">https://aspredicted.org/CBG_ZPG</ext-link></td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" colspan="3"><bold>Software &amp; Algorithms</bold></td></tr><tr style="border-right:solid 1px #000000;border-left:solid 1px #000000;border-bottom:solid 1px #000000"><td valign="top" align="left">fMRI analyses</td><td valign="top" align="left">Matlab 2018a, MathWorks Inc <ext-link ext-link-type="uri" xlink:href="https://it.mathworks.com/products/matlab.html">https://it.mathworks.com/pr oducts/matlab.html</ext-link></td><td valign="top" align="left">RRID:SCR_001622</td></tr><tr style="border-right:solid 1px #000000;border-left:solid 1px #000000;border-bottom:solid 1px #000000"><td valign="top" align="left">fMRI analyses</td><td valign="top" align="left">Statistical Parametric Mapping 12 (SPM12) <ext-link ext-link-type="uri" xlink:href="https://www.fil.ion.ucl.ac.uk/spm/">https://www.fil.ion.ucl.ac.uk/spm/</ext-link></td><td valign="top" align="left">RRID:SCR_007037</td></tr><tr style="border-right:solid 1px #000000;border-left:solid 1px #000000;border-bottom:solid 1px #000000"><td valign="top" align="left">Visual Presentation of Main Task and to create robot actions</td><td valign="top" align="left">Python software foundation <ext-link ext-link-type="uri" xlink:href="http://www.python.org/">http://www.python.org/</ext-link></td><td valign="top" align="left">RRID:SCR_008394</td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Visual Presentation of localiser (TOM)</td><td valign="top" align="left" style="border-right:solid 2px #000000">Psychopy <ext-link ext-link-type="uri" xlink:href="https://www.psychopy.org/">https://www.psychopy.org/</ext-link></td><td valign="top" align="left" style="border-right:solid 2px #000000">RRID:SCR_006571</td></tr><tr style="border-right:solid 1px #000000;border-left:solid 1px #000000;border-bottom:solid 1px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000;border-left:solid 2px #000000">Visual presentation of localiser (INT)</td><td valign="top" align="left" style="border-right:solid 2px #000000">PsychToolBox 3 <ext-link ext-link-type="uri" xlink:href="http://psychtoolbox.org/">http://psychtoolbox.org/</ext-link></td><td valign="top" align="left" style="border-right:solid 2px #000000">RRID:SCR_002881</td></tr><tr style="border-right:solid 1px #000000;border-left:solid 1px #000000;border-bottom:solid 1px #000000"><td valign="top" align="left">R Project for Statistical Computing (for figs)</td><td valign="top" align="left">The R Foundation <ext-link ext-link-type="uri" xlink:href="http://www.r-project.org/">http://www.r-project.org/</ext-link></td><td valign="top" align="left">RRID:SCR_001905</td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Code for robot introduction &amp; main experiment</td><td valign="top" align="left" style="border-right:solid 2px #000000">GitHub</td><td valign="top" align="left"><ext-link ext-link-type="uri" xlink:href="https://github.com/chaudhuryB">https://github.com/chaudhuryB</ext-link></td></tr><tr style="border-right:solid 2px #000000;border-left:solid 2px #000000;border-bottom:solid 2px #000000"><td valign="top" align="left" style="border-right:solid 2px #000000">Code for analyses</td><td valign="top" align="left" style="border-right:solid 2px #000000">upon request</td><td valign="top" align="left"/></tr></tbody></table></table-wrap><sec id="S24"><title>Human subjects</title><p id="P36">Due to the availability of scanning resources, participants were recruited from 2 sites: (i) the greater Glasgow area (Scotland, UK); and (ii) the greater Bangor area (Wales, UK). Glasgow participants completed the study at the Centre for Cognitive Neuroimaging (CCNi) at the University of Glasgow, while Bangor participants completed the study at the Bangor Imaging Unit (BIU) at Bangor University.</p><p id="P37">Twenty right-handed males (mean age = 20.95 years; SD = 1.82; range = 19-26) participated from the greater Bangor area and 24 right-handed males (mean age = 22.45 years; SD = 3.63; range = 18-32) participated from Glasgow. Only males were recruited, consistent with previous studies in this area,<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> in order to control for any potential effects of gender on mentalizing.<sup><xref ref-type="bibr" rid="R78">78</xref></sup> Two participants withdrew from the study due to claustrophobia (1 subject from each site). The final fMRI participant sample included a total of 42 participants (mean age = 21.74 years; SD = 3.03; range = 18-32).</p><p id="P38">All participants reported normal or corrected-to-normal vision, no history of neurological or psychiatric disorders, and were right-handed as confirmed on the Edinburgh Handedness Questionnaire<sup><xref ref-type="bibr" rid="R79">79</xref></sup>; mean = 1.48, sd = .34).</p><p id="P39">All participants reported low familiarity with robots. Median engagement with robots in daily life (measured from 1 (never) to 7 (daily)) was 2 (IQR 1). Median number of robot-themed movies or TV shows seen was 4 (IQR 1) out of the 14 listed (Riek et al, 2011).<sup><xref ref-type="bibr" rid="R80">80</xref></sup></p><p id="P40">All participants provided written informed consent prior to their involvement and received monetary compensation for study participation (£12/hour). All study procedures were approved by the respective university ethics boards: (i) Bangor University (Approval no. 2019-16639) and (ii) Glasgow University (Approval no. 300180110).</p><p id="P41">Study site was a significantly different between subjects factor in rmANOVA for rTPJ, rmFG &amp; Precuneus but when we ran site separately for each of those ROIs, the results did not differ from the combined group or change the outcome, therefore, both sites were kept together in the main tables above. Please see our OSF for details on the results from the separate groups. Further, we ran site as a covariate of no-interest in our model estimation and did not find differences in our whole brain data; therefore, the sites were subsequently analysed and reported together (please see our OSF for more details).</p></sec><sec id="S25"><title>Experimental Design</title><p id="P42">We designed a Rock-Paper-Scissors (RPS) task similar to a previous study,<sup><xref ref-type="bibr" rid="R26">26</xref></sup> and followed a similar briefing procedure.<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup> RPS was chosen for its familiarity across ages and cultures, and ease of rule learning. Previous studies have shown this game to engage mentalizing regions when played against human and non-human partners.<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup></p><p id="P43">Participants saw videos of their respective game partners before and after each 3-game series (see <xref ref-type="fig" rid="F2">Fig 2</xref>). Each video was unique and all participants saw the same set of videos. The button press for rock, paper, and scissors were assigned randomly across participants.</p><p id="P44">In-line with previous designs,<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup> participants were told that they were playing a live game and viewing their game-partners through a live video feed, but in reality, neither the remote practice nor the in-scanner games (described below) were live. All videos were pre-recorded and designed to give the impression of a live game. Wins and losses were controlled across the four conditions so that each participant won 10 rounds and lost 10 rounds against each partner. The order in which participants played partners was pseudo-randomized across four 8-minute functional runs.</p><p id="P45">To give the impression of a live game, participants met all game partners in person in the “game room” and played one truly live, in-person, round of rock-paper-scissors with each partner. They went to the imaging suite to play a “live” practice round of RPS with their partners via the “video feed”. This practice round served to familiarise participants with the game and practice pushing the buttons to register their answer with the correct timing. Participants played each partner twice in each practice round and could complete up to 3 practice rounds (24 total games) to ensure they understood the game before entering the scanner. All participants demonstrated understanding of the game and button presses by the 3<sup>rd</sup> practice round.</p><p id="P46">Participants then completed the fMRI task, playing the same RPS game. Each fMRI run contained 5 rounds with each of the 4 partners (20 rounds per partner across all 4 runs), pseudorandomized across participants. In total, each participant completed four, 8-minute RPS runs. After the scan, participants completed several questionnaires (listed below) on a laptop and were then debriefed. The debriefing unveiled the study deception (that the various game partners were pre-recorded, not live, and that all partners used the same random algorithm and were not independently controlled). Both the practice round and game in the scanner were programmed in Python 3.7 and run from the command line (see STAR Methods Table).</p></sec></sec><sec id="S26" sec-type="materials"><title>Materials</title><sec id="S27"><title>MRI Parameters, pre-processing, &amp; GLM estimation fMRI data acquisition &amp; Pre-processing</title><p id="P47">At both data collection sites (CCNI and BIU), stimuli were projected onto a mirror from a projector located behind the scanner. Responses were recorded with an MRI-compatible keypad.</p><p id="P48">A dual-echo EPI sequence was used to improve signal-to-noise ratio (SNR) in frontal and temporal regions.<sup><xref ref-type="bibr" rid="R81">81</xref></sup> All structural and functional sequence parameters are detailed in <xref ref-type="supplementary-material" rid="SD1">Supplemental Tables 1</xref> &amp; 2.</p><p id="P49">Data pre-processing was carried out in SPM12 (Wellcome Trust Centre for Neuroimaging, London) implemented in Matlab 2018a (Mathworks, Natick, MA, USA). Pre-processing consisted of standard SPM12 defaults for slice time correction, realignment and re-slicing, co-registration, unified segmentation &amp; normalisation, and smoothing; except for a 6mm FWHM Gaussian smoothing kernel. All analyses were performed in normalized MNI space. Block durations and onsets for each of the 4 experimental conditions during Video 1, the RPS game, and Video 2 were modelled by convolving the hemodynamic response function and with a high pass filter of 128s. Head motion parameters were modelled as nuisance regressors. Functional scans provided whole brain coverage.</p></sec><sec id="S28"><title>ROI Creation &amp; Analyses</title><p id="P50">Our choice of ROIs was informed by previous studies<sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref></sup>; however, ROI placement was based on peak activation from the independent localizers (see <xref ref-type="supplementary-material" rid="SD1">Fig S2 &amp; Table S3</xref>). Participants undertook two passive-viewing tasks to help identify brain regions of interest after playing the RPS game. Localizer 1 was a short-animated film (‘Partly Cloudy’; Pixar Animation Studios, 2009) used to localize bilateral TPJ, bilateral mFG, and Precuneus with the mentalizing &gt; pain contrast. Neither Medial Prefrontal Cortex (mPFC) nor rmFG activation appeared as expected in Localiser 1, therefore, we used mPFC &amp; rmFG coordinates from the original localiser paper<sup><xref ref-type="bibr" rid="R50">50</xref></sup> and created 6mm spheres around those coordinates. Localizer 2 was a point-light figure social interaction task<sup><xref ref-type="bibr" rid="R52">52</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup> to localize bilateral pSTS with the interaction &gt; scrambled contrast (i.e., two human point light figures interacting vs. scrambled dot motion).</p><p id="P51">We used a control ROI (V1/BA17) from the WFU PickAtlas<sup><xref ref-type="bibr" rid="R82">82</xref></sup> as a form of verification that activity differences seen between conditions during game play was not attributable to non-specific whole brain activation differences. In other words, we would not expect differences between conditions in V1 activity during game play, as participants saw the same set-up across all conditions, and this control ROI allowed us to evaluate this possibility.</p><p id="P52">Briefly, group-constrained, subject specific ROIs were created like the methods described elsewhere<sup><xref ref-type="bibr" rid="R52">52</xref></sup> using an uncorrected height threshold of p &lt; .0001. This protocol creates subject-specific ROIs using a leave-one-subject-out iterative process so that ROIs for individuals are based on independent data. ROIs were created based on group activation from the localizer tasks defined by intersecting a 6mm sphere with the cluster peak (i.e. highest voxel t-value) in each of the regions named above. That subject specific search space was then applied to the participant’s individual data to create the final ROIs. Percent signal change was extracted from ROIs using in-house scripts in Matlab 2018a and the MarsBar toolbox.</p><p id="P53">None of the ROIs overlapped. Both right and left TPJ were slightly shifted so the entire sphere was within the boundaries of the brain; all other ROIs created from the localisers remained true to the peak activation. Please refer to Supplementary Figures for all ROI coordinates.</p><sec id="S29"><title>Pre-Registered</title><p id="P54">Repeated measures ANOVAs were run for each ROI to assess the effect of game-partner and pairwise comparisons were run only if a main effect of game-partner was found. We assessed the linear effect of human-likeness using a linear repeated contrast in a within-subject ANOVA, which compares means across the different levels of the independent variable according to the following order: computer &lt; mechanoid &lt; humanoid &lt; human.</p></sec><sec id="S30"><title>Exploratory</title><p id="P55">Ratings results from the <italic>Fun, Competitiveness</italic>, and <italic>Sympathy</italic> questions in the Debrief, suggested swapping the robot orders in the linear model (see below). As an exploratory analysis, we ran a linear repeated contrast in a within-subject ANOVA to compare means across different levels of the independent variable according to the following order based on socialness ratings: computer &lt; humanoid &lt; mechanoid &lt; human.</p><p id="P56">Additionally, we assessed whether the pSTS would show a linear pattern based on human-likeness or socialness during game play and whilst watching the video introduction which preceded each round.</p><sec id="S31"><title>Whole brain analyses</title></sec></sec><sec id="S32"><title>Pre-Registered</title><p id="P57">A GLM comprising the four conditions (CP = Computer Partner, MR = Mechanoid Robot, HR= Humanoid Robot, HP = Human Partner) was specified for each participant. Simple contrasts were compared against: (1) HP &gt; CP, (2) HR &gt; CP, (3) MR &gt; CP, (4) HR &gt; MR, (5) HP &gt; HR. Based on previous findings (Krach et al, 2010) and our hypothesis, we expected to see a linear increase in neural activity based on human-likeness of agent. To evaluate this, we calculated a parametric modulation of gameplay partner (actual model weights used: CP = -3, MR = -1, HR = 1, HP = 3). For the second level group analyses, we used a FWE-corrected threshold (p<sub>uncorr</sub> &lt; 0.001) and a minimum cluster size (k = 100).</p></sec><sec id="S33"><title>Exploratory</title><p id="P58">While not pre-registered, we also included the following simple contrasts: (6) HP &gt; MR, (7) MR &gt; HR. We also calculated the parametric modulation of gameplay partners based on socialness (actual model weights used: CP = -3, HR = -1, MR = 1, HP = 3).</p></sec></sec><sec id="S34"><title>Behavioural Measures</title><sec id="S35"><title>Debrief Questions</title><sec id="S36"><title>Pre-Registered</title><p id="P59">After scanning, participants answered questions about their experience of the study using FormR.<sup><xref ref-type="bibr" rid="R83">83</xref></sup> Participants rated responses to the following questions on a scale from 0-10: (i) how well they were able to adopt an efficient <italic>strategy</italic> against each partner, (ii) how <italic>successful</italic> they were against each partner, (iii) how much <italic>fun</italic> it was to play each partner, (iv) how much <italic>sympathy</italic> they had for each partner when they lost, and then each partner’s (v) <italic>competitiveness</italic>, and (v) <italic>intelligence</italic>. As pre-registered, rmANOVAs were run on each question to assess the effect of agent. Pairwise comparisons between agents were run only if an agent effect was identified. We assessed the linear effect of human-likeness using a linear repeated contrast in a within-subject ANOVA, which compares means across different levels of the independent variable.</p></sec><sec id="S37"><title>Exploratory</title><p id="P60">Furthermore, based on participant-reported perceptions of socialness of the individual agents, we ran an exploratory (not pre-registered) linear repeated contrast in a within-subjects ANOVA that reversed the order of the robots in the linear model.</p></sec></sec><sec id="S38"><title>Inclusion of Others and Self (IOS)</title><p id="P61">The Inclusion of Others and Self (IOS) is a measure of closeness and interconnectedness between two individuals.<sup><xref ref-type="bibr" rid="R84">84</xref></sup> A series of 7 increasingly overlapping circles are presented to the participant on paper. Each pair of circles contains the word “self” in one circle and “other” in the other circle. Participants are then asked to choose which circle represents their relationship to the agent in question. We asked participants to show which set of overlapping circles best describes the following agents: (1) computer, (2) mechanoid robot, (3) humanoid robot, (4) a human stranger, (5) the human from the experiment (LEJ), and (6) a close friend. Non-robot items were included for comparison to determine where the robot stood relative to other people in the participant’s lives. The IOS provides another way to address the participant’s view of their relationship to various humans and robots. Responses from the paper and pencil format of the IOS were recorded onto a 7-point scale from 1 (no overlap) to 7 (nearly complete overlap). As pre-registered, rmANOVA was run to assess the effect of agent and pairwise comparisons were run only if an effect of agent was found.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Information</label><media xlink:href="EMS189386-supplement-Supplemental_Information.pdf" mimetype="application" mime-subtype="pdf" id="d49aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S39"><title>Acknowledgements</title><p>This work has received funding from the European Research Council under the European Union’s Horizon 2020 research and innovation programme (Grant Agreement numbers: 677270 (Social Robots) &amp; 716974: Becoming Social)).</p><p>The authors thank Nikolas Vitsakis, Kiara Jackson, and Jacynth Grundy for assistance with data collection and Julia Landsiedel for programming advice.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P62">Bangor neuroimaging data are available for analysis through request due to BIDS conversion difficulties with Phillips multi-echo data</p></fn><fn fn-type="conflict" id="FN2"><p id="P63"><bold>Declaration of Interests</bold></p><p id="P64">The authors declare no competing interests.</p></fn><fn fn-type="con" id="FN3"><p id="P65"><bold>Author Contributions</bold></p><p id="P66">Conceptualisation: LEJ, ESC, KK; Methodology: LEJ, ESC, KK, BC; Formal Analysis: LEJ; Investigation: LEJ, BC, SAA; Writing - Original Draft : LEJ, ESC, KK; Writing - Review &amp; Editing: LEJ, ESC, KK, BC, SAA; Supervision: ESC &amp; KK; Funding Acquisition: ESC &amp; KK.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turing</surname><given-names>A</given-names></name></person-group><article-title>Computing Machinery and Intelligence</article-title><source>Mind</source><year>1950</year><volume>236</volume><fpage>433</fpage><lpage>460</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shukla</surname><given-names>A</given-names></name><name><surname>Karki</surname><given-names>H</given-names></name></person-group><article-title>Application of robotics in onshore oil and gas industry—A review Part I</article-title><source>Robot Auton Syst</source><year>2016</year><volume>75</volume><fpage>490</fpage><lpage>507</lpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kas</surname><given-names>KA</given-names></name><name><surname>Johnson</surname><given-names>GK</given-names></name></person-group><article-title>Using unmanned aerial vehicles and robotics in hazardous locations safely</article-title><source>Process Saf Prog</source><year>2020</year><volume>39</volume></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Cifuentes</surname><given-names>CA</given-names></name><name><surname>Pinto</surname><given-names>MJ</given-names></name><name><surname>Céspedes</surname><given-names>N</given-names></name><name><surname>Múnera</surname><given-names>M</given-names></name></person-group><article-title>Social Robots in Therapy and Care</article-title><source>Curr Robotics Reports</source><year>2020</year><volume>1</volume><fpage>59</fpage><lpage>74</lpage></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belpaeme</surname><given-names>T</given-names></name><name><surname>Kennedy</surname><given-names>J</given-names></name><name><surname>Ramachandran</surname><given-names>A</given-names></name><name><surname>Scassellati</surname><given-names>B</given-names></name><name><surname>Tanaka</surname><given-names>F</given-names></name></person-group><article-title>Social robots for education: A review</article-title><source>Sci Robotics</source><year>2018</year><volume>3</volume><elocation-id>eaat5954</elocation-id><pub-id pub-id-type="pmid">33141719</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawe</surname><given-names>J</given-names></name><name><surname>Sutherland</surname><given-names>C</given-names></name><name><surname>Barco</surname><given-names>A</given-names></name><name><surname>Broadbent</surname><given-names>E</given-names></name></person-group><article-title>Can social robots help children in healthcare contexts? A scoping review</article-title><source>Bmj Paediatr Open</source><year>2019</year><volume>3</volume><elocation-id>e000371</elocation-id><pub-id pub-id-type="pmcid">PMC6361370</pub-id><pub-id pub-id-type="pmid">30815587</pub-id><pub-id pub-id-type="doi">10.1136/bmjpo-2018-000371</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drexler</surname><given-names>N</given-names></name><name><surname>Lapré</surname><given-names>VB</given-names></name></person-group><article-title>For better or for worse: Shaping the hospitality industry through robotics and artificial intelligence</article-title><source>Res Hosp Management</source><year>2019</year><volume>9</volume><fpage>117</fpage><lpage>120</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>JA</given-names></name><name><surname>MacDonald</surname><given-names>BA</given-names></name><name><surname>Kuo</surname><given-names>I-H</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Broadbent</surname><given-names>E</given-names></name></person-group><article-title>People respond better to robots than computer tablets delivering healthcare instructions</article-title><source>Comput Hum Behav</source><year>2015</year><volume>43</volume><fpage>112</fpage><lpage>117</lpage></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>ES</given-names></name><name><surname>Ramsey</surname><given-names>R</given-names></name></person-group><article-title>Mind Meets Machine: Towards a Cognitive Science of Human– Machine Interactions</article-title><source>Trends Cogn Sci</source><year>2020</year><volume>25</volume><fpage>200</fpage><lpage>212</lpage><pub-id pub-id-type="pmid">33384213</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruana</surname><given-names>N</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>Autonomous social robots are real in the mind’s eye of many</article-title><source>Behav Brain Sci</source><year>2023</year><volume>46</volume><elocation-id>e26</elocation-id><pub-id pub-id-type="pmid">37017050</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><article-title>Interacting Minds--A Biological Basis</article-title><source>Science</source><year>1999</year><volume>286</volume><fpage>1692</fpage><lpage>1695</lpage><pub-id pub-id-type="pmid">10576727</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>R</given-names></name><name><surname>Carey</surname><given-names>S</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Understanding Other Minds: Linking Developmental Psychology and Functional Neuroimaging</article-title><source>Psychology</source><year>2004</year><volume>55</volume><fpage>87</fpage><lpage>124</lpage><pub-id pub-id-type="pmid">14744211</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schurz</surname><given-names>M</given-names></name><name><surname>Radua</surname><given-names>J</given-names></name><name><surname>Aichhorn</surname><given-names>M</given-names></name><name><surname>Richlan</surname><given-names>F</given-names></name><name><surname>Perner</surname><given-names>J</given-names></name></person-group><article-title>Fractionating theory of mind: A meta-analysis of functional brain imaging studies</article-title><source>Neurosci Biobehav Rev</source><year>2014</year><volume>42</volume><fpage>9</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">24486722</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><source>The Neural Basis of Mentalizing</source><year>2021</year><fpage>17</fpage><lpage>45</lpage><pub-id pub-id-type="pmid">16701204</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><article-title>Development and neurophysiology of mentalizing</article-title><source>Philos Trans R Soc Lond Ser B: Biol Sci</source><year>2003</year><volume>358</volume><fpage>459</fpage><lpage>473</lpage><pub-id pub-id-type="pmcid">PMC1693139</pub-id><pub-id pub-id-type="pmid">12689373</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2002.1218</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overwalle</surname><given-names>FV</given-names></name><name><surname>Baetens</surname><given-names>K</given-names></name></person-group><article-title>Understanding others’ actions and goals by mirror and mentalizing systems: A meta-analysis</article-title><source>Neuroimage</source><year>2009</year><volume>48</volume><fpage>564</fpage><lpage>584</lpage><pub-id pub-id-type="pmid">19524046</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overwalle</surname><given-names>FV</given-names></name></person-group><article-title>Social cognition and the brain: a meta-analysis</article-title><source>Hum Brain Mapp</source><year>2009</year><volume>30</volume><fpage>829</fpage><lpage>58</lpage><pub-id pub-id-type="pmcid">PMC6870808</pub-id><pub-id pub-id-type="pmid">18381770</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20547</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molenberghs</surname><given-names>P</given-names></name><name><surname>Johnson</surname><given-names>H</given-names></name><name><surname>Henry</surname><given-names>JD</given-names></name><name><surname>Mattingley</surname><given-names>JB</given-names></name></person-group><article-title>Understanding the minds of others: A neuroimaging meta-analysis</article-title><source>Neurosci Biobehav Rev</source><year>2016</year><volume>65</volume><fpage>276</fpage><lpage>291</lpage><pub-id pub-id-type="pmid">27073047</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiese</surname><given-names>E</given-names></name><name><surname>Wykowska</surname><given-names>A</given-names></name><name><surname>Zwickel</surname><given-names>J</given-names></name><name><surname>Müller</surname><given-names>HJ</given-names></name></person-group><article-title>I See What You Mean: How Attentional Selection Is Shaped by Ascribing Intentions to Others</article-title><source>Plos One</source><year>2012</year><volume>7</volume><elocation-id>e45391</elocation-id><pub-id pub-id-type="pmcid">PMC3458834</pub-id><pub-id pub-id-type="pmid">23049794</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0045391</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wykowska</surname><given-names>A</given-names></name><name><surname>Wiese</surname><given-names>E</given-names></name><name><surname>Prosser</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>HJ</given-names></name></person-group><article-title>Beliefs about the Minds of Others Influence How We Process Sensory Information</article-title><source>Plos One</source><year>2014</year><volume>9</volume><elocation-id>e94339</elocation-id><pub-id pub-id-type="pmcid">PMC3979768</pub-id><pub-id pub-id-type="pmid">24714419</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0094339</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Özdem</surname><given-names>C</given-names></name><etal/></person-group><article-title>Believing androids – fMRI activation in the right temporo-parietal junction is modulated by ascribing intentions to non-human agents</article-title><source>Soc Neurosci</source><year>2016</year><volume>12</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">27391213</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henschel</surname><given-names>A</given-names></name><name><surname>Hortensius</surname><given-names>R</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>Social Cognition in the Age of Human–Robot Interaction</article-title><source>Trends Neurosci</source><year>2020</year><volume>43</volume><fpage>373</fpage><lpage>384</lpage><pub-id pub-id-type="pmid">32362399</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>ES</given-names></name><etal/></person-group><article-title>A neurocognitive investigation of the impact of socializing with a robot on empathy for pain</article-title><source>Philosophical Transactions Royal Soc B</source><year>2019</year><volume>374</volume><elocation-id>20180034</elocation-id><pub-id pub-id-type="pmcid">PMC6452253</pub-id><pub-id pub-id-type="pmid">30852995</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2018.0034</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pütten</surname><given-names>AMRder</given-names></name><etal/></person-group><article-title>Investigations on empathy towards humans and robots using fMRI</article-title><source>Comput Hum Behav</source><year>2014</year><volume>33</volume><fpage>201</fpage><lpage>212</lpage></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiese</surname><given-names>E</given-names></name><name><surname>Buzzell</surname><given-names>GA</given-names></name><name><surname>Abubshait</surname><given-names>A</given-names></name><name><surname>Beatty</surname><given-names>PJ</given-names></name></person-group><article-title>Seeing minds in others: Mind perception modulates low-level social-cognitive performance and relates to ventromedial prefrontal structures</article-title><source>Cognitive Affect Behav Neurosci</source><year>2018</year><volume>18</volume><fpage>837</fpage><lpage>856</lpage><pub-id pub-id-type="pmid">29992485</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaminade</surname><given-names>T</given-names></name><etal/></person-group><article-title>How do we think machines think? An fMRI study of alleged competition with an artificial intelligence</article-title><source>Front Hum Neurosci</source><year>2012</year><volume>6</volume><fpage>103</fpage><pub-id pub-id-type="pmcid">PMC3347624</pub-id><pub-id pub-id-type="pmid">22586381</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2012.00103</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krach</surname><given-names>S</given-names></name><etal/></person-group><article-title>Can Machines Think? Interaction and Perspective Taking with Robots Investigated via fMRI</article-title><source>Plos One</source><year>2008</year><volume>3</volume><elocation-id>e2597</elocation-id><pub-id pub-id-type="pmcid">PMC2440351</pub-id><pub-id pub-id-type="pmid">18612463</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0002597</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>H</given-names></name><etal/></person-group><article-title>Different impressions of other agents obtained through social interaction uniquely modulate dorsal and ventral pathway activities in the social human brain</article-title><source>Cortex J Devoted Study Nerv Syst Behav</source><year>2014</year><volume>58</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="pmid">24880954</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaminade</surname><given-names>T</given-names></name><name><surname>Fonseca</surname><given-names>DD</given-names></name><name><surname>Rosset</surname><given-names>D</given-names></name><name><surname>Cheng</surname><given-names>G</given-names></name><name><surname>Deruelle</surname><given-names>C</given-names></name></person-group><article-title>Atypical modulation of hypothalamic activity by social context in ASD</article-title><source>Res Autism Spect Dis</source><year>2015</year><volume>10</volume><fpage>41</fpage><lpage>50</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meltzoff</surname><given-names>AN</given-names></name></person-group><article-title>The ‘like me’ framework for recognizing and becoming an intentional agent</article-title><source>Acta Psychol</source><year>2007</year><volume>124</volume><fpage>26</fpage><lpage>43</lpage><pub-id pub-id-type="pmcid">PMC1852490</pub-id><pub-id pub-id-type="pmid">17081488</pub-id><pub-id pub-id-type="doi">10.1016/j.actpsy.2006.09.005</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joue</surname><given-names>BRDGI</given-names></name></person-group><source>Robot Being in</source><year>2004</year></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goetz</surname><given-names>J</given-names></name><name><surname>Kiesler</surname><given-names>S</given-names></name><name><surname>Powers</surname><given-names>A</given-names></name></person-group><article-title>Matching Robot Appearance and Behavior to Tasks to Improve Human-Robot Cooperation</article-title><source>12th IEEE Int Work Robot Hum Interact Commun, 2003 Proc Rom 2003</source><year>2003</year><fpage>55</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1109/roman.2003.1251796</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abubshait</surname><given-names>A</given-names></name><name><surname>Wiese</surname><given-names>E</given-names></name></person-group><article-title>You Look Human, But Act Like a Machine: Agent Appearance and Behavior Modulate Different Aspects of Human-Robot Interaction</article-title><source>Front Psychol</source><year>2017</year><volume>8</volume><fpage>1393</fpage><pub-id pub-id-type="pmcid">PMC5572356</pub-id><pub-id pub-id-type="pmid">28878703</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2017.01393</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>ES</given-names></name><etal/></person-group><article-title>Robotic movement preferentially engages the action observation network</article-title><source>Hum Brain Mapp</source><year>2012</year><volume>33</volume><fpage>2238</fpage><lpage>2254</lpage><pub-id pub-id-type="pmcid">PMC6870135</pub-id><pub-id pub-id-type="pmid">21898675</pub-id><pub-id pub-id-type="doi">10.1002/hbm.21361</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teufel</surname><given-names>C</given-names></name><name><surname>Fletcher</surname><given-names>PC</given-names></name><name><surname>Davis</surname><given-names>G</given-names></name></person-group><article-title>Seeing other minds: attributed mental states influence perception</article-title><source>Trends Cogn Sci</source><year>2010</year><volume>14</volume><fpage>376</fpage><lpage>382</lpage><pub-id pub-id-type="pmid">20576464</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klapper</surname><given-names>A</given-names></name><name><surname>Ramsey</surname><given-names>R</given-names></name><name><surname>Wigboldus</surname><given-names>D</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>The Control of Automatic Imitation Based on Bottom–Up and Top–Down Cues to Animacy: Insights from Brain and Behavior</article-title><source>J Cognitive Neurosci</source><year>2014</year><volume>26</volume><fpage>2503</fpage><lpage>2513</lpage><pub-id pub-id-type="pmid">24742157</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epley</surname><given-names>N</given-names></name><name><surname>Waytz</surname><given-names>A</given-names></name><name><surname>Cacioppo</surname><given-names>JT</given-names></name></person-group><article-title>On Seeing Human: A Three-Factor Theory of Anthropomorphism</article-title><source>Psychol Rev</source><year>2007</year><volume>114</volume><fpage>864</fpage><lpage>886</lpage><pub-id pub-id-type="pmid">17907867</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiesler</surname><given-names>S</given-names></name><name><surname>Powers</surname><given-names>A</given-names></name><name><surname>Fussell</surname><given-names>SR</given-names></name><name><surname>Torrey</surname><given-names>C</given-names></name></person-group><article-title>Anthropomorphic Interactions with a Robot and Robot–like Agent</article-title><source>Soc Cogn</source><year>2008</year><volume>26</volume><fpage>169</fpage><lpage>181</lpage></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tung</surname><given-names>F-W</given-names></name></person-group><article-title>Human-Computer Interaction. Users and Applications, 14th International Conference, HCI International 2011, Orlando, FL, USA, July 9-14, 2011, Proceedings, Part IV</article-title><source>Lect Notes Comput Sci</source><year>2011</year><fpage>637</fpage><lpage>646</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-21619-0_76</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiSalvo</surname><given-names>CF</given-names></name><name><surname>Gemperle</surname><given-names>F</given-names></name><name><surname>Forlizzi</surname><given-names>J</given-names></name><name><surname>Kiesler</surname><given-names>S</given-names></name></person-group><article-title>All robots are not created equal: the design and perception of humanoid robot heads</article-title><source>Proc 4th Conf Des Interact Syst: Process, Pr, methods, Tech</source><year>2002</year><fpage>321</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1145/778712.778756</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Henschel</surname><given-names>A</given-names></name><name><surname>Laban</surname><given-names>G</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>What Makes a Robot Social? A Review of Social Robots from Science Fiction to a Home or Hospital Near You</article-title><source>Curr Robotics Reports</source><year>2021</year><volume>2</volume><fpage>9</fpage><lpage>19</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cross</surname><given-names>ES</given-names></name><name><surname>Ramsey</surname><given-names>R</given-names></name><name><surname>Liepelt</surname><given-names>R</given-names></name><name><surname>Prinz</surname><given-names>W</given-names></name><name><surname>Hamilton</surname><given-names>AFdeC</given-names></name></person-group><article-title>The shaping of social perception by stimulus and knowledge cues to human animacy</article-title><source>Philosophical Transactions Royal Soc Lond Ser B Biological Sci</source><year>2016</year><volume>371</volume><elocation-id>20150075</elocation-id><pub-id pub-id-type="pmcid">PMC4685521</pub-id><pub-id pub-id-type="pmid">26644594</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0075</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallagher</surname><given-names>HL</given-names></name><name><surname>Jack</surname><given-names>AI</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><article-title>Imaging the Intentional Stance in a Competitive Game</article-title><source>Neuroimage</source><year>2002</year><volume>16</volume><fpage>814</fpage><lpage>821</lpage><pub-id pub-id-type="pmid">12169265</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname><given-names>T-Y</given-names></name><name><surname>Chaudhury</surname><given-names>B</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>Human–Robot Cooperation in Economic Games: People Show Strong Reciprocity but Conditional Prosociality Toward Robots</article-title><source>Int J Soc Robot</source><year>2023</year><volume>15</volume><fpage>791</fpage><lpage>805</lpage></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsieh</surname><given-names>T-Y</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>People’s dispositional cooperative tendencies towards robots are unaffected by robots’ negative emotional displays in prisoner’s dilemma games</article-title><source>Cogn Emot</source><year>2022</year><volume>36</volume><fpage>995</fpage><lpage>1019</lpage><pub-id pub-id-type="pmid">35389323</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andriella</surname><given-names>A</given-names></name><etal/></person-group><article-title>Do I Have a Personality? Endowing Care Robots with Context- Dependent Personality Traits</article-title><source>Int J Soc Robot</source><year>2021</year><volume>13</volume><fpage>2081</fpage><lpage>2102</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittaker</surname><given-names>S</given-names></name><name><surname>Rogers</surname><given-names>Y</given-names></name><name><surname>Petrovskaya</surname><given-names>E</given-names></name><name><surname>Zhuang</surname><given-names>H</given-names></name></person-group><article-title>Designing Personas for Expressive Robots</article-title><source>Acm Transactions Human-robot Interact Thri</source><year>2021</year><volume>10</volume><fpage>1</fpage><lpage>25</lpage></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirby</surname><given-names>R</given-names></name><name><surname>Forlizzi</surname><given-names>J</given-names></name><name><surname>Simmons</surname><given-names>R</given-names></name></person-group><article-title>Affective social robots</article-title><source>Robot Auton Syst</source><year>2010</year><volume>58</volume><fpage>322</fpage><lpage>332</lpage></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graaf</surname><given-names>MMAde</given-names></name><name><surname>Allouch</surname><given-names>SB</given-names></name></person-group><article-title>Exploring influencing variables for the acceptance of social robots</article-title><source>Robot Auton Syst</source><year>2013</year><volume>61</volume><fpage>1476</fpage><lpage>1486</lpage></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacoby</surname><given-names>N</given-names></name><name><surname>Bruneau</surname><given-names>E</given-names></name><name><surname>Koster-Hale</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Localizing Pain Matrix and Theory of Mind networks with both verbal and non-verbal stimuli</article-title><source>Neuroimage</source><year>2016</year><volume>126</volume><fpage>39</fpage><lpage>48</lpage><pub-id pub-id-type="pmcid">PMC4733571</pub-id><pub-id pub-id-type="pmid">26589334</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.025</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schilbach</surname><given-names>L</given-names></name></person-group><article-title>On the relationship of online and offline social cognition</article-title><source>Front Hum Neurosci</source><year>2014</year><volume>8</volume><fpage>278</fpage><pub-id pub-id-type="pmcid">PMC4018539</pub-id><pub-id pub-id-type="pmid">24834045</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2014.00278</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walbrin</surname><given-names>J</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name></person-group><article-title>Dyadic interaction processing in the posterior temporal cortex</article-title><source>Neuroimage</source><year>2019</year><volume>198</volume><fpage>296</fpage><lpage>302</lpage><pub-id pub-id-type="pmcid">PMC6610332</pub-id><pub-id pub-id-type="pmid">31100434</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.05.027</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deen</surname><given-names>B</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Saxe</surname><given-names>R</given-names></name></person-group><article-title>Functional Organization of Social Perception and Cognition in the Superior Temporal Sulcus</article-title><source>Cereb Cortex</source><year>2015</year><volume>25</volume><fpage>4596</fpage><lpage>4609</lpage><pub-id pub-id-type="pmcid">PMC4816802</pub-id><pub-id pub-id-type="pmid">26048954</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhv111</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Landsiedel</surname><given-names>J</given-names></name><name><surname>Daughters</surname><given-names>K</given-names></name><name><surname>Downing</surname><given-names>PE</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name></person-group><article-title>The role of motion in the neural representation of social interactions in the posterior temporal cortex</article-title><source>NeuroImage</source><year>2022</year><volume>262</volume><elocation-id>119533</elocation-id><pub-id pub-id-type="pmcid">PMC9485464</pub-id><pub-id pub-id-type="pmid">35931309</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119533</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname><given-names>F</given-names></name><name><surname>Happé</surname><given-names>F</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name></person-group><article-title>Movement and Mind: A Functional Imaging Study of Perception and Interpretation of Complex Intentional Movement Patterns</article-title><source>Neuroimage</source><year>2000</year><volume>12</volume><fpage>314</fpage><lpage>325</lpage><pub-id pub-id-type="pmid">10944414</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name></person-group><article-title>The social brain: allowing humans to boldly go where no other species has been</article-title><source>Philosophical Transactions Royal Soc B Biological Sci</source><year>2010</year><volume>365</volume><fpage>165</fpage><lpage>176</lpage><pub-id pub-id-type="pmcid">PMC2842701</pub-id><pub-id pub-id-type="pmid">20008394</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2009.0160</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name><name><surname>Beeler</surname><given-names>D</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>Perceiving social interactions in the posterior superior temporal sulcus</article-title><source>Proc National Acad Sci</source><year>2017</year><volume>114</volume><fpage>E9145</fpage><lpage>E9152</lpage><pub-id pub-id-type="pmcid">PMC5664556</pub-id><pub-id pub-id-type="pmid">29073111</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1714471114</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walbrin</surname><given-names>J</given-names></name><name><surname>Downing</surname><given-names>P</given-names></name><name><surname>Koldewyn</surname><given-names>K</given-names></name></person-group><article-title>Neural responses to visually observed social interactions</article-title><source>Neuropsychologia</source><year>2018</year><volume>112</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="pmcid">PMC5899757</pub-id><pub-id pub-id-type="pmid">29476765</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.02.023</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatley</surname><given-names>T</given-names></name><name><surname>Milleville</surname><given-names>SC</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>Understanding Animate Agents</article-title><source>Psychol Sci</source><year>2006</year><volume>18</volume><fpage>469</fpage><lpage>474</lpage><pub-id pub-id-type="pmid">17576256</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKiernan</surname><given-names>KA</given-names></name><name><surname>Kaufman</surname><given-names>JN</given-names></name><name><surname>Kucera-Thompson</surname><given-names>J</given-names></name><name><surname>Binder</surname><given-names>JR</given-names></name></person-group><article-title>A Parametric Manipulation of Factors Affecting Task-induced Deactivation in Functional Neuroimaging</article-title><source>J Cogn Neurosci</source><year>2003</year><volume>15</volume><fpage>394</fpage><lpage>408</lpage><pub-id pub-id-type="pmid">12729491</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shine</surname><given-names>JM</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name></person-group><article-title>Understanding the Brain, By Default</article-title><source>Trends Neurosci</source><year>2018</year><volume>41</volume><fpage>244</fpage><lpage>247</lpage><pub-id pub-id-type="pmid">29703375</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>T</given-names><suffix>(Christina)</suffix></name></person-group><article-title>Leveraging “human-likeness” of robotic service at restaurants</article-title><source>Int J Hosp Manag</source><year>2021</year><volume>94</volume><elocation-id>102823</elocation-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Fink</surname><given-names>J</given-names></name></person-group><source>Lecture Notes in Computer Science</source><year>2012</year><fpage>199</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-34103-8_20</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothstein</surname><given-names>N</given-names></name><name><surname>Kounios</surname><given-names>J</given-names></name><name><surname>Ayaz</surname><given-names>H</given-names></name><name><surname>de Visser</surname><given-names>EJ</given-names></name></person-group><article-title>Advances in Neuroergonomics and Cognitive Engineering, Proceedings of the AHFE 2020 Virtual Conferences on Neuroergonomics and Cognitive Engineering, and Industrial Cognitive Ergonomics and Engineering Psychology, July 16-20, 2020, USA</article-title><source>Adv Intell Syst</source><year>2020</year><fpage>190</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-51041-1_26</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roselli</surname><given-names>C</given-names></name><name><surname>Ciardo</surname><given-names>F</given-names></name><name><surname>Tommaso</surname><given-names>DD</given-names></name><name><surname>Wykowska</surname><given-names>A</given-names></name></person-group><article-title>Human-likeness and attribution of intentionality predict vicarious sense of agency over humanoid robot actions</article-title><source>Sci Rep-uk</source><year>2022</year><volume>12</volume><elocation-id>13845</elocation-id><pub-id pub-id-type="pmcid">PMC9381554</pub-id><pub-id pub-id-type="pmid">35974080</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-18151-6</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mejia</surname><given-names>C</given-names></name><name><surname>Kajikawa</surname><given-names>Y</given-names></name></person-group><article-title>Assessing the Sentiment of Social Expectations of Robotic Technologies</article-title><source>2017 Portland Int Conf Manag Eng Technol (PICMET)</source><year>2017</year><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.23919/picmet.2017.8125441</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ishiguro</surname><given-names>H</given-names></name><name><surname>Nishio</surname><given-names>S</given-names></name></person-group><article-title>Building artificial humans to understand humans</article-title><source>J Artif Organs</source><year>2007</year><volume>10</volume><fpage>133</fpage><lpage>142</lpage><pub-id pub-id-type="pmid">17846711</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reeves</surname><given-names>B</given-names></name><name><surname>Hancock</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><article-title>Social robots are like real people: First impressions, attributes, and stereotyping of social robots</article-title><source>Technology Mind Behav</source><year>2020</year><volume>1</volume></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coradeschi</surname><given-names>S</given-names></name><etal/></person-group><article-title>Human-Inspired Robots</article-title><source>Ieee Intell Syst</source><year>2006</year><volume>21</volume><fpage>74</fpage><lpage>85</lpage></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polakow</surname><given-names>T</given-names></name><name><surname>Laban</surname><given-names>G</given-names></name><name><surname>Teodorescu</surname><given-names>A</given-names></name><name><surname>Busemeyer</surname><given-names>JR</given-names></name><name><surname>Gordon</surname><given-names>G</given-names></name></person-group><article-title>Social robot advisors: effects of robot judgmental fallacies and context</article-title><source>Intel Serv Robot</source><year>2022</year><volume>15</volume><fpage>593</fpage><lpage>609</lpage></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breazeal</surname><given-names>C</given-names></name></person-group><article-title>Emotion and sociable humanoid robots</article-title><source>Int J Hum-comput St</source><year>2003</year><volume>59</volume><fpage>119</fpage><lpage>155</lpage></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hortensius</surname><given-names>R</given-names></name><name><surname>Cross</surname><given-names>ES</given-names></name></person-group><article-title>From automata to animate beings: the scope and limits of attributing socialness to artificial agents</article-title><source>Ann Ny Acad Sci</source><year>2018</year><volume>1426</volume><fpage>93</fpage><lpage>110</lpage><pub-id pub-id-type="pmid">29749634</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="confpro"><person-group person-group-type="author"><name><surname>Wykowska</surname><given-names>A</given-names></name><name><surname>Chaminade</surname><given-names>T</given-names></name><name><surname>Cheng</surname><given-names>G</given-names></name></person-group><article-title>Embodied artificial agents for understanding human social cognition</article-title><source>Philosophical Transactions Royal Soc B Biological Sci</source><year>2016</year><volume>371</volume><elocation-id>20150375</elocation-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Press</surname><given-names>C</given-names></name><name><surname>Gillmeister</surname><given-names>H</given-names></name><name><surname>Heyes</surname><given-names>C</given-names></name></person-group><article-title>Bottom-up, not top-down, modulation of imitation by human and robotic models</article-title><source>Eur J Neurosci</source><year>2006</year><volume>24</volume><fpage>2415</fpage><lpage>2419</lpage><pub-id pub-id-type="pmid">17042792</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanley</surname><given-names>J</given-names></name><name><surname>Gowen</surname><given-names>E</given-names></name><name><surname>Miall</surname><given-names>RC</given-names></name></person-group><article-title>How instructions modify perception: An fMRI study investigating brain areas involved in attributing human agency</article-title><source>NeuroImage</source><year>2010</year><volume>52</volume><fpage>389</fpage><lpage>400</lpage><pub-id pub-id-type="pmcid">PMC2887490</pub-id><pub-id pub-id-type="pmid">20398769</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.04.025</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galak</surname><given-names>J</given-names></name><name><surname>LeBoeuf</surname><given-names>RA</given-names></name><name><surname>Nelson</surname><given-names>LD</given-names></name><name><surname>Simmons</surname><given-names>JP</given-names></name></person-group><article-title>Correcting the Past: Failures to Replicate Psi</article-title><source>J Pers Soc Psychol</source><year>2012</year><volume>103</volume><fpage>933</fpage><lpage>948</lpage><pub-id pub-id-type="pmid">22924750</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munafò</surname><given-names>MR</given-names></name><etal/></person-group><article-title>A manifesto for reproducible science</article-title><source>Nat Hum Behav</source><year>2017</year><volume>1</volume><fpage>0021</fpage><pub-id pub-id-type="pmcid">PMC7610724</pub-id><pub-id pub-id-type="pmid">33954258</pub-id><pub-id pub-id-type="doi">10.1038/s41562-016-0021</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krach</surname><given-names>S</given-names></name><etal/></person-group><article-title>Are women better mindreaders? Sex differences in neural correlates of mentalizing detected with functional MRI</article-title><source>Bmc Neurosci</source><year>2009</year><volume>10</volume><fpage>9</fpage><pub-id pub-id-type="pmcid">PMC2667181</pub-id><pub-id pub-id-type="pmid">19193204</pub-id><pub-id pub-id-type="doi">10.1186/1471-2202-10-9</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><article-title>The assessment and analysis of handedness: The Edinburgh inventory</article-title><source>Neuropsychologia</source><year>1971</year><volume>9</volume><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riek</surname><given-names>LD</given-names></name><name><surname>Rabinowitch</surname><given-names>T</given-names></name><name><surname>Chakrabarti</surname><given-names>B</given-names></name><name><surname>Robinson</surname><given-names>P</given-names></name></person-group><article-title>How anthropomorphism affects empathy toward robots</article-title><source>2009 4th Acm Ieee Int Conf Human-robot Interact Hri</source><year>2009</year><fpage>245</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1145/1514095.1514158</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halai</surname><given-names>AD</given-names></name><name><surname>Welbourne</surname><given-names>SR</given-names></name><name><surname>Embleton</surname><given-names>K</given-names></name><name><surname>Parkes</surname><given-names>LM</given-names></name></person-group><article-title>A comparison of dual gradient-echo and spin-echo fMRI of the inferior temporal lobe</article-title><source>Hum Brain Mapp</source><year>2014</year><volume>35</volume><fpage>4118</fpage><lpage>4128</lpage><pub-id pub-id-type="pmcid">PMC6869502</pub-id><pub-id pub-id-type="pmid">24677506</pub-id><pub-id pub-id-type="doi">10.1002/hbm.22463</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maldjian</surname><given-names>JA</given-names></name><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Kraft</surname><given-names>RA</given-names></name><name><surname>Burdette</surname><given-names>JH</given-names></name></person-group><article-title>An automated method for neuroanatomic and cytoarchitectonic atlas-based interrogation of fMRI data sets</article-title><source>NeuroImage</source><year>2003</year><volume>19</volume><fpage>1233</fpage><lpage>1239</lpage><pub-id pub-id-type="pmid">12880848</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arslan</surname><given-names>RC</given-names></name><name><surname>Walther</surname><given-names>MP</given-names></name><name><surname>Tata</surname><given-names>CS</given-names></name></person-group><article-title>formr: A study framework allowing for automated feedback generation and complex longitudinal experience-sampling studies using R</article-title><source>Behav Res Methods</source><year>2020</year><volume>52</volume><fpage>376</fpage><lpage>387</lpage><pub-id pub-id-type="pmcid">PMC7005096</pub-id><pub-id pub-id-type="pmid">30937847</pub-id><pub-id pub-id-type="doi">10.3758/s13428-019-01236-y</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aron</surname><given-names>A</given-names></name><name><surname>Aron</surname><given-names>EN</given-names></name><name><surname>Smollan</surname><given-names>D</given-names></name></person-group><article-title>Inclusion of Other in the Self Scale and the Structure of Interpersonal Closeness</article-title><source>J Pers Soc Psychol</source><year>1992</year><volume>63</volume><fpage>596</fpage><lpage>612</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig 1</label><caption><p>Average percent signal change (PSC) during gameplay in mentalizing ROIs and pSTS with significant within subject rmANOVA (Error bars are SEM).</p></caption><graphic xlink:href="EMS189386-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Whole brain T-map overlap analysis (Human &gt; Computer (Red); Human &gt; Humanoid (Blue); Human &gt; Mechanoid (Green)). There were no significant whole brain differences between either robot and the computer condition nor between the robots themselves; the robots and the computer conditions may be perceived similarly. The Human-Computer contrast (red) revealed significant clusters in mentalizing regions (ie Precuneus, bilateral TPJ, and mPFC), similar with previous findings. There was less overlap between the Human-Mechanoid contrast (green) with the Human-Computer contrast suggesting that there may be differences in how participants attribute a mind to the mechanoid compared to the humanoid and computer.</p></caption><graphic xlink:href="EMS189386-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><title>Average Likert (0-10) scale ratings of Debrief questions (Error bars are SEM).</title></caption><graphic xlink:href="EMS189386-f003"/></fig><fig id="F4" position="float"><label>Fig 4</label><caption><p>Experimental paradigm. Each game lasts 4 seconds and includes a countdown from 2 to 0; participants respond on ‘0’ and then the results of the game are shown to the participant before the next game in the series commences. Participants play 3 games against one opponent per trial. Each trial is preceded by a 2.5 second video of their opponent (video framing) and concluded by a 2.75 second (video feedback) of their opponent; each series of 3 games lasts 12 seconds without video framing and feedback (average of 19.25 second in total).</p></caption><graphic xlink:href="EMS189386-f004"/></fig></floats-group></article>