<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189718</article-id><article-id pub-id-type="doi">10.1101/2023.10.11.561860</article-id><article-id pub-id-type="archive">PPR742499</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Rationalised experiment design for parameter estimation with sensitivity clustering</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9252-1855</contrib-id><name><surname>Chhajer</surname><given-names>Harsh</given-names></name><xref ref-type="aff" rid="A1">a</xref></contrib><contrib contrib-type="author"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3329-8803</contrib-id><name><surname>Roy</surname><given-names>Rahul</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>a</label>Department of Bioengineering, Indian Institute of Science, Bangalore 560012, India</aff><aff id="A2"><label>b</label>Department of Chemical Engineering, Indian Institute of Science, Bangalore 560012, India</aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label>Corresponding author: Rahul Roy. Department of Chemical Engineering, Indian Institute of Science, Bangalore, Karnataka, India-560012. Phone: 91-80-2293-3115 <email>rahulroy@iisc.ac.in</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>15</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Quantitative experiments are essential for investigating, uncovering and confirming our understanding of complex systems, necessitating the use of effective and robust experimental designs. Despite generally outperforming other approaches, the broader adoption of model-based design of experiments (MBDoE) has been hindered by oversimplified assumptions and computational overhead. To address this, we present PARameter SEnsitivity Clustering (PARSEC), an MBDoE framework that identifies informative measurable combinations through parameter sensitivity (PS) clustering. We combined PARSEC with a new variant of Approximate Bayesian Computation for rapid, automated assessment and ranking of designs. By inherent design, PARSEC can take into account experimental restrictions and parameter variability. We show that PARSEC improves parameter estimation for two different types of biological models. Importantly, PARSEC can determine the optimal sample size for information gain, which we show correlates well with the optimal number of PS clusters. This supports our rationale for PARSEC and demonstrates the potential to harness both model structure and system behaviour to efficiently navigate the experiment design space.</p></abstract><kwd-group><kwd>Approximate Bayesian computation</kwd><kwd>Model fitting</kwd><kwd>Informative experiment design</kwd><kwd>Parameter sensitivity</kwd><kwd>Clustering-based experiment design</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The scientific approach relies on careful experiments to understand complex systems. Experiments must not only provide quantitative information such as model parameter estimates but also be economically and practically viable. Accurate parameter estimation is essential for refining model representations and enhancing model predictions. Compared to traditional model-free techniques, Model-Based Design of Experiments (MBDoE) utilises system-specific models to optimise experiment design for improved parameter estimation [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>]. MBDoE allows for a more effective evaluation of the experimental design space and offers a more comprehensive understanding of the variables and the interactions between the components. It can facilitate resource optimisation, decrease the number of trials, and improve the accuracy of the estimation of the model parameters [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>] and model selection [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>]. Over the years, MBDoE has become widely applied in a variety of fields, including engineering and biomedicine, due to the increasing use of quantitative models to understand complex systems. Despite the advantages of MBDoE, its application in parameter estimation is hindered by the increasing intricacies of modern experiments and their inherent constraints.</p><p id="P3">Fisher’s Information Matrix (FIM) is a popular framework for designing experiments aimed at estimating model parameter values [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>]. It is based on the concept of the expected information gain from an experiment, which is calculated by taking the expected value of the second derivative of the log-likelihood function with respect to the model parameters [<xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. If changes in the values of the parameters have a major effect on the output of the model, it implies that the output is useful in providing information about the parameters. Nevertheless, FIM-MBDoEs usually assume a linear statistical model that links measurements to parameter values, requiring the linearisation of nonlinear models at expected ground-truth parameter values. Therefore, the efficiency of designs is markedly diminished when the guesses are not precise or accurate [<xref ref-type="bibr" rid="R9">9</xref>]. This limits the ability of FIM methods to incorporate parameter uncertainty in the experimental design. The complexity of matrix inversion and the need for a large experimental sample size to achieve the desired accuracy and a Gaussian noise distribution present additional difficulties that restrict the use of FIM-MBDoE [<xref ref-type="bibr" rid="R11">11</xref>]. Recently, new approaches have been proposed to address some of these issues [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R11">11</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R9">9</xref>]. For example, design space discretization can be employed to handle non-linear models, and the average information content and risk can be calculated for a large uncertainty to identify robust and reliable designs [<xref ref-type="bibr" rid="R14">14</xref>]. Additionally, the pseudo-inverse [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>] can be used to tackle ill-conditioned matrices [<xref ref-type="bibr" rid="R8">8</xref>].</p><p id="P4">Alternately, Bayesian approaches to MBDoE operate independently of assumptions related to linearity or direct likelihood estimations [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R23">23</xref>, <xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R25">25</xref>]. These techniques can take into account any prior information or convictions about the parameters and offer more flexibility that successfully sidesteps the issues with a single-point estimation. However, they suffer from high computational demands, particularly when dealing with complex systems or large design spaces, and often produce biased and potentially suboptimal designs when tailored to prespecified sample sizes [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R20">20</xref>]. To address these limitations, certain algorithms employ a sequential greedy approach, which build on existing designs to manage complexity and improve design efficiency [<xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R19">19</xref>]. However, such design search approaches are influenced by the initial design choice and may be susceptible to local optima artefacts. Here, we introduce the PARSEC DoE algorithm, which uses the model architecture of the system through parameter sensitivity analysis to direct the search for informative experiment designs. PARSEC recognises combinations of measurables with a low overlap in the vectors that represent their parameter sensitivity indices (PSI). These PSI indices indicate how the value of the measurable changes when the value of the parameter changes. We demonstrate that the precision of parameter estimates increases when the overlap in the PSI vectors is reduced, thus enabling PARSEC to generate an ‘optimal’ DoE effectively. We show that fuzzy c-means clustering of PSI can reduce the number of evaluations required to identify ‘optimal’ designs substantially. PARSEC combines the PSI evaluated at different parameter values to identify generalist and robust designs informed by the parameter sensitivities while accommodating the uncertainty in prior knowledge. PARSEC combines concepts from FIM-based and Bayesian MBDoEs to determine the best experimental designs. We find that the optimal number of PSI clusters is in agreement with designs that yield the highest information gain, thus introducing a new way to calculate the sampling frequency.</p><p id="P5">In order to implement PARSEC, a reliable high-throughput parameter estimation framework is needed to assess the predicted designs. Existing methods for parameter estimation are restricted by their inherent assumptions or are not suitable for high-throughput analysis. For instance, likelihood-based techniques are limited to systems where the likelihood function can be specified and a Gaussian distribution for parameter distribution and measurement noise can be assumed.[<xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>, <xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R30">30</xref>]). Rather than using error thresholds or correlated sampling, Approximate Bayesian Computation (ABC) based methods [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>]) typically rely on data-dependent stipulations. This can make the estimation vulnerable to local minima and initial biases. To alleviate this, here we develop the Approximate Bayesian Computation - Fixed Acceptance Rate (ABC-FAR) method [<xref ref-type="bibr" rid="R34">34</xref>] for parameter estimation for PARSEC designs. Our ABC-based algorithm uses a global and relative parameter sampling rejection criterion (FAR). Thus, ABC-FAR avoids data-dependent stipulations, making it suitable for automated analysis and fair comparison of estimation derived from different data sets. We show that parameter estimation via ABC-FAR is accurate, free of computational artefacts, and less susceptible to noise in the data and initial guess bias.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Sensitivity driven design of experiments</title><p id="P6">The PARSEC (PARameter SEnsitivity driven Clustering based DoE) framework represents a MBDoE rooted in four key ideas (<xref ref-type="fig" rid="F1">Figure 1</xref>). Firstly, it employs the parameter sensitivity of a variable as an indicator of its informativeness towards the estimation of a parameter value [<xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R10">10</xref>]. The information content profile of a measurement is, thus, approximated using a vector of its parameter sensitivity indices (PSI) (Step 1, <xref ref-type="fig" rid="F1">Figure 1</xref>). Secondly, PARSEC computes the PSI vectors at various parameter values that sample the distribution linked to parameter uncertainty. Concatenating the PSI vectors for a measurement candidate yields the composite PARSEC-PSI vector. This approach accounts for the uncertainty and helps inform robust DoE. The intricacies of creating the PARSEC-PSI vector are detailed in the <xref ref-type="supplementary-material" rid="SD1">Supplementary text (SI S2</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref>). The third idea revolves around the selection of measurements in a manner that minimizes the overlap amongst the corresponding PARSEC-PSI vectors. This maximizes the overall information yield from their combination. The process involves clustering measurements based on their PARSEC-PSI vectors and subsequently selecting a representative measurement from each cluster (Step 2, <xref ref-type="fig" rid="F1">Figure 1</xref>). Finally, we expect the performance of a PSI-clustering-based design to depend on the quality of clustering, which in turn hinges on the number of clusters. As the number of clusters dictates the size of the experimental sample, we posit that optimal clustering can provide a well-informed guess for the sample size, which can be tuned to identify efficient designs that are both economical and informative (Step 3, <xref ref-type="fig" rid="F1">Figure 1</xref>). The choice of clustering depends on the design specifications. For predetermined sample sizes, k-means and c-means clustering algorithms are suitable, especially for PARSEC-PSI vectors which would typically be high-dimensional, continuous data. In this study, we focus on two alternate implementations of PARSEC, namely PARSEC(k) and PARSEC(c), that uses the k-means and c-means clustering algorithms respectively (see Methods).</p><p id="P7">Optimizing the design parameters requires evaluation of the associated designs predicted. Moreover, given the stochastic nature of PSI-clustering and subsequent measurement selection, PARSEC predicts multiple designs. Thus the process of design comparison is vital for selecting the most favorable design from the array of designs predicted by the PARSEC framework (Step 4, <xref ref-type="fig" rid="F1">Figure 1</xref>). In this context, we introduce the Approximate Bayesian Computation - Fixed Acceptance Rate (ABC-FAR) technique, designed to automate parameter estimation and facilitate impartial comparison of the generated experimental designs.</p><p id="P8">For a likelihood-free approach and wider applicability, we propose an ABC-based algorithm for parameter estimation. Like the other ABC methods, ABC-FAR iteratively refines the parameter value distribution using χ<sup>2</sup> statistics. In each iteration (<xref ref-type="fig" rid="F2">Figure 2a</xref>), it samples the current distribution estimate (step 2, <xref ref-type="fig" rid="F2">Figure 2a</xref>), and among them chooses some based on the corresponding χ<sup>2</sup> values to update the distribution estimate (steps 3 &amp; 4, <xref ref-type="fig" rid="F2">Figure 2a</xref>). However, ABC-FAR differs from other ABC methods in that it selects a fixed fraction (FAR) of parameter values with the lowest χ<sup>2</sup> values to update the marginals. The rejection criterion employs relative comparison and eliminates the need for specifying absolute χ<sup>2</sup> thresholds, thereby avoiding data-specific customization. Unlike other relative rejection criterion-based approaches that involve local comparisons with neighboring parameter combinations, ABC-FAR performs a global comparison, considering all combinations sampled from the marginal together. Latin Hypercube Sampling (LHS, [<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>]) is employed to sample the marginal, eliminating the need for transition kernels. Additionally, a small noise is introduced in the sampling distribution, resembling Simulated Annealing [<xref ref-type="bibr" rid="R38">38</xref>], to enhance robustness against local minima and initial guess biases (step 1, <xref ref-type="fig" rid="F2">Figure 2a</xref>).</p></sec><sec id="S4"><title>Accurate and robust parameter estimation using ABC-FAR</title><p id="P9">We demonstrate the performance of ABC-FAR using the popular Lotka-Volterra model. We simulate the model for a known combination of parameter values (ground truth) and initial conditions to generate synthetic data (<xref ref-type="fig" rid="F2">Figure 2b</xref>). Assuming the same initial conditions, we use ABC-FAR to estimate parameter values using this data. Estimation is done in the logarithmic scale (base 10) of parameter values to explore a large dynamical range. We use a uniform initial guess for all parameters to indicate a lack of prior knowledge about the values. We see that the deviation between model predictions and data (χ<sup>2</sup> statistics) decreases with every iteration of ABC-FAR, and the model predictions recapitulate the data well (<xref ref-type="fig" rid="F2">Figure 2b, c</xref>). The algorithm iteratively transforms the prior guess for the model parameters into sharp distribution around the corresponding ground truth value used (<xref ref-type="fig" rid="F2">Figure 2c</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S2</xref>). However, it doesn’t alter the marginal of a dummy parameter (<xref ref-type="fig" rid="F3">Figure 3c</xref>), a parameter that doesn’t affect the model predictions (and hence χ<sup>2</sup>) but undergoes the same treatment as the model parameters. This suggests that the algorithm doesn’t introduce unwanted computation artifacts.</p><p id="P10">Additionally, we identify higher-order relations among model parameter values, conditioned on data. For example, the selected values of the ‘birth rate of prey’ (<italic>a</italic>) and ‘death rate of prey due to predation’ (<italic>b</italic>) show a strong positive correlation (PnI = .74, <xref ref-type="fig" rid="F3">Figure 3b</xref>), indicative of significant practical non-identifiability (PnI). The slope of linear regression (=0.93) suggests that changes in χ<sup>2</sup> due to a 1% increase in log<sub>10</sub>(a) value can be compensated by .93% increase in the value log<sub>10</sub>(b). Although local, the findings are statistically significant as realized by comparing them to similar analysis on dummy parameter (<xref ref-type="fig" rid="F3">Figure 3d</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figures S3</xref> and <xref ref-type="supplementary-material" rid="SD1">S4</xref>). We further verify that the estimation is robust to measurement noise (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S6</xref>) and see that ABC-FAR can easily be adapted to exploit the model structure and system-specific knowledge to speed up and improve convergence (<xref ref-type="supplementary-material" rid="SD1">Supplementary text SI S7</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S7</xref>).</p><p id="P11">The efficiency of ABC-based methods typically depends on the implementation of their rejection criterion. In the previous analysis, we used a FAR value of 0.25. Here we vary it to see the effect on the algorithm’s efficiency. For this, we monitor two performance statistics at every iteration for the different implementations - the accuracy of estimation (inversely related to χ<sup>2</sup> statistics) and cumulative computational cost (∝ EAR<sup><italic>−</italic>1</sup>, described in methods, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figures S8</xref>, <xref ref-type="supplementary-material" rid="SD1">S9</xref> and <xref ref-type="supplementary-material" rid="SD1">S10</xref>). The χ<sup>2</sup> statistics monotonically decrease with iteration irrespective of the FAR value used; this is expected as we use the ‘history-dependent update strategy’ (strategy described in Methods). However, the estimation converges at higher accuracy (lower χ<sup>2</sup> statistics) as the FAR value decreases. Among those reaching a particular level of accuracy, the schemes with higher FAR values (weaker rejection criteria) seem to be more efficient. The efficiency of our schemes is comparable to that of the popular ABC-SMC algorithm [<xref ref-type="bibr" rid="R32">32</xref>] (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S10</xref>, comparison detailed in Methods).</p></sec><sec id="S5"><title>Identifying informative designs using PARSEC with ABC-FAR</title><p id="P12">We use PARSEC(k) (PARSEC employing k-means clustering, see Methods) and ABC-FAR to design experiments characterizing the parameters of a three-gene repressilator model (Elowitz and Leibler in 2000, details in Methods and SI). First, we establish the methodology with predetermined initial conditions and sample size, assuming an accurate prior knowledge of parameters. We account for practical considerations, such as (a) limiting the experiment duration, (b) preferring simultaneous measurements when multiple variables are observed, and (c) constraining the measurables based on feasibility (see Methods).</p><p id="P13">To validate PARSEC(k)’s performance, we compare PARSEC(k) designs (PD<sub><italic>BC</italic></sub>) against random designs (RD<sub><italic>BC</italic></sub>), for parameter estimation accuracy. The suffix indicates the variables being measured. We also evaluate the importance of clustering, using anti-PARSEC(k) designs (WD<sub><italic>BC</italic></sub>) where all measurements are selected from the fewest of clusters (the ones with the most candidates). The parameter estimation error for the random designs (RD<sub><italic>BC</italic></sub> and RD<sub><italic>AB</italic></sub>) are approximately uniformly distributed (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Interestingly, the spread of errors becomes strongly skewed when we consider designs based on PSI vector clustering (PD<sub><italic>BC</italic></sub> and WD<sub><italic>BC</italic></sub>, <xref ref-type="fig" rid="F4">Figure 4a</xref>). The error for designs where all the measurements belong to a single cluster (WD<sub><italic>BC</italic></sub>) tends to be higher; whereas designs (PD<sub><italic>BC</italic></sub>) generated using measurements from different clusters are highly informative with lower error (barring few outliers). The top ten percentile of designs monitoring the levels of proteins B and C (RD<sub><italic>BC</italic></sub>, WD<sub><italic>BC</italic></sub> and PD<sub><italic>BC</italic></sub>) are predominantly PARSEC-predicted ones (PD<sub><italic>BC</italic></sub>). Despite no significant bias in the selection of measurements in PD<sub><italic>BC</italic></sub> compared to that of RD<sub><italic>BC</italic></sub> (<xref ref-type="fig" rid="F4">Figure 4c</xref>), the average estimation error for the PD<sub><italic>BC</italic></sub> is about three times lower than that for the RD<sub><italic>BC</italic></sub> (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Furthermore, the clustering-based algorithm samples from a small and informative sub-space of the experiment design space, indicated by the low variance in estimation error (<xref ref-type="fig" rid="F4">Figure 4a</xref>) and performance of sub-samplings of PD<sub><italic>BC</italic></sub> compared to that of RD<sub><italic>BC</italic></sub> (<xref ref-type="fig" rid="F4">Figure 4b</xref>).</p><p id="P14">We also observe that the performance of designs generated by PARSEC(k) depends on how closely certain design specifications are implemented. For instance, PD<sub><italic>BC</italic></sub> optimized the measurements of proteins B and C based on their PSI. But if we quantify proteins A and B using the measurement time points optimized for the quantification of proteins B and C (PD<sub><italic>AB</italic></sub>), we observe a decrease in estimation accuracy compared to PD<sub><italic>BC</italic></sub>. However, PD<sub><italic>AB</italic></sub> still outperforms RD<sub><italic>AB</italic></sub> on average, with a 1.5 times lower error, which can be attributed to two factors: (a) PD<sub><italic>BC</italic></sub> accounts for the sensitivity profiles of protein B, which is one of the variables measured in PD<sub><italic>AB</italic></sub>, and (b) a potential correlation between the sensitivity profiles of proteins A and C due to the coupled dynamics.</p></sec><sec id="S6"><title>PARSEC(k) tolerates parameter uncertainty</title><p id="P15">In the previous analysis, we optimized PD<sub><italic>BC</italic></sub> using PSI evaluated at a guess of parameter values (GT<sub><italic>G</italic></sub>). But the guess might differ from the actual ground truth value (GT<sub><italic>T</italic></sub>). PARSEC(k) tolerates a significant disparity between the guess and actual value of ground truth but suffers when the guess is very bad as indicated by the accuracy ratio (<xref ref-type="fig" rid="F5">Figure 5a</xref>), defined as the ratio of mean estimation error for random design to that for PARSEC(k) designs.</p><p id="P16">Rather than informing designs using a single-point guess, PARSEC can do so for multiple guesses to accommodate uncertainty associated with parameter knowledge (<xref ref-type="fig" rid="F5">Figure 5b</xref>). To do so, PARSEC samples the associated distribution for representative guesses called training samples (Θ<sup><italic>k</italic></sup>). PSI evaluated at the training samples are concatenated together to form the PARSEC-PSI vectors, used for clustering and subsequent design selection. Estimation errors for the PARSEC(k) and random designs are evaluated using data generated at the training samples (Θ<sup><italic>k</italic></sup>) and validated at another set of parameter samples (test samples, denoted as Ω<sup><italic>k</italic></sup>).</p><p id="P17">PARSEC(k) designs are on average twice as informative as random designs given the parameter uncertainty (<xref ref-type="fig" rid="F5">Figure 5c</xref>). Interestingly, the top 5% of the PARSEC(k) designs in the training analysis, are also the best performers in the test analysis. Such designs robust to parameter uncertainty, can be good candidates for generalist designs. Moreover, comparable accuracy ratios for training and test samples (2 ± .85 and 2.05 ± .53, respectively) further emphasize the robustness of PARSEC(k) designs.</p></sec><sec id="S7"><title>Optimizing the sample size</title><p id="P18">In the previous analyses, we constrained the design search to those with pre-defined experiment sample size; here we optimize it. Random and PARSEC(k) designs lead to more accurate parameter estimation when their sample size increases (<xref ref-type="fig" rid="F6">Figure 6a</xref>). However, the marginal gain in estimation accuracy decreases steadily (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S12</xref>). Notably, PARSEC(k) designs of sample sizes five and higher exhibit similar performance on average (<xref ref-type="fig" rid="F6">Figure 6a</xref>), suggesting that a five-measurement design would be both informative and economical.</p><p id="P19">As the number of clusters in PARSEC defines the experiment sample size, we posit that the clustering efficiency affects the information gain. PARSEC allows us to evaluate the relationship between the accuracy ratio and the clustering efficiency for a given sample size. In our example of designing experiments for the repressilator network, we find that the intra-cluster distance for the PARSEC-PSI vectors reaches an elbow point between five to six clusters, indicating optimal partitioning (<xref ref-type="fig" rid="F6">Figure 6b</xref>). Interestingly, five clusters also correspond to repressilator PARSEC(k) designs with the highest accuracy ratio.</p><p id="P20">This suggests that clustering efficiency can serve as a valuable indicator for determining sample size for PARSEC, leading to high accuracy ratios. This arises due to two reasons. On one hand, using insufficient clusters limits the representation of PARSEC-PSI vectors in the PARSEC designs. Therefore, increasing the number of clusters significantly enhances the representation and information content of the designs, thereby improving the accuracy ratio. On the other hand, excessive clustering results in an over-representation of similar PARSEC-PSI vectors, resulting in only a modest increase in the information contents compared to that in random designs. Hence the advantage of using a PARSEC approach is likely to decrease with over-clustering. The observed correlation between clustering efficiency and accuracy ratio further validates the effectiveness of the PSI-clustering-based approach in experiment design.</p><p id="P21">While our implementation of PARSEC(k) provides a good estimate for optimal sample size, it still demands a substantial computational cost due to the evaluation of numerous designs. This cost escalates when considering parameter uncertainty across multiple parameter values. The multitude of predictions in PARSEC(k) arises from the discrete partitioning of continuous PARSEC-PSI vectors using the k-means clustering algorithm and subsequent candidate selection. An alternative approach is to employ the fuzzy c-means clustering (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S13</xref>), as in PARSEC(c). This method results in fewer unique optima, drastically reducing the computational burden (<xref ref-type="fig" rid="F6">Figures 6b</xref>). PARSEC(c) exhibits similar clustering convergence (<xref ref-type="fig" rid="F6">Figure 6a</xref>: inset) and comparable, if not superior, estimation error and accuracy ratio profiles (<xref ref-type="fig" rid="F6">Figures 6a</xref> and <xref ref-type="fig" rid="F6">b</xref>) when varying sample sizes, all while requiring nearly twenty times fewer computations (<xref ref-type="fig" rid="F6">Figure 6b</xref>). This enhanced efficiency renders sample size optimization for generalist design search manageable. We identified informative, generalist designs for various sample size experiments requiring a total of nine design evaluations (<xref ref-type="fig" rid="F6">Figure 6c</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S14</xref>), compared to the 100 designs evaluated to identify a generalist design of sample size six via PARSEC(k) (in <xref ref-type="fig" rid="F5">Figure 5c</xref>).</p></sec></sec><sec id="S8" sec-type="methods"><title>Methods</title><sec id="S9"><title>Implementation of PARSEC</title><p id="P22">First, we identify the feasible measurement candidates according to design specifications and sample the parameter uncertainty selecting the ‘training samples’. PARSEC then evaluates the parameter sensitivity indices (PSI) for each variable within a measurement candidate across these training sample (step 1, <xref ref-type="fig" rid="F1">Figure 1</xref>). These PSIs are concatenated into PARSEC-PSI vector for each candidate (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref>). The candidates are grouped by similarity in their PARSEC-PSI vectors (step 2, <xref ref-type="fig" rid="F1">Figure 1</xref>), guiding PARSEC’s final selection of measurement candidates (step 3, <xref ref-type="fig" rid="F1">Figure 1</xref>). The candidates are selected such that each cluster is well-represented in the design.</p><p id="P23">We use Latin Hypercube Sampling (LHS [<xref ref-type="bibr" rid="R36">36</xref>]), a Monte-Carlo algorithm to efficiently sample multi-dimensional parameter spaces [<xref ref-type="bibr" rid="R37">37</xref>]. To generate N samples, LHS divides the distribution for each parameter into N equal probability intervals. Then it randomly picks an interval (without repetition) for each parameter and based on the distribution, chooses a random value from it. For sensitivity analysis, we use extended Fourier Amplitude Sensitivity Test (eFAST, [<xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R40">40</xref>]). It is a variance-based method that gauges how parameter fluctuations contribute to uncertainty in the variable of interest. eFAST provides a global measure of sensitivity and captures non-linear relationships among parameters and outputs.</p><p id="P24">In our implementation, we apply either k-means or fuzzy c-means clustering algorithm. Suitable for partitioning continuous data into predetermined number of clusters, these unsupervised learning methods aim to minimize the total intra-cluster distance. k-means algorithm designates each data point to a single cluster. Whereas the c-means clustering algorithm assigns a probability of the data point belonging to each of the clusters. Upon c-means clustering, we guide the selection of measurement candidates using this probability measure of cluster membership. We iteratively select the measurement candidate corresponding to the highest degree of membership to any of the clusters (randomly breaking ties). Once selected, the measurement candidate and its corresponding cluster are ignored from the subsequent selection process. We denote to this flavor of PARSEC as PARSEC(c). Alternatively in PARSEC(k), we use k-means clustering algorithm and construct the design by randomly selecting a measurement candidate from each cluster. Properties such as silhouette scores can be used to bias selection of candidates from a cluster, alternate to the random analyzed in this study.</p><p id="P25">It’s important to note that the clustering and subsequent measurement selection steps in PARSEC can be stochastic. Thus we execute PARSEC to predict multiple designs, eventually selecting the most informative one. Designs are evaluated at one or more parameter values (step 4, <xref ref-type="fig" rid="F1">Figure 1</xref>). At each parameter combination, we simulate the model and generate the data set according to the design being evaluated. We employ ABC-FAR to estimate the parameter values using the data set and calculate the estimation error.</p></sec><sec id="S10"><title>Implementation of ABC-FAR</title><p id="P26">ABC-FAR iteratively samples parameter combinations and rejects/accepts them based on associated χ<sup>2</sup> values, which quantifies the deviation between data (R<sub><italic>D</italic></sub>) and corresponding model prediction (R<sub><italic>P</italic></sub>). <inline-formula><mml:math id="M1"><mml:mrow><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mstyle mathsize="140" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>D</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>
, where the summation is over all data points and <italic>σ<sub>D</sub></italic> denotes the standard deviation in data <italic>R<sub>D</sub></italic>.</p><p id="P27">ABC-FAR samples a noisy version of the current estimate of the distribution. However, the amplitude of noise added decreases with iteration (mimicking simulated annealing [<xref ref-type="bibr" rid="R38">38</xref>]). This improves exploration in early iterations without drastically slowing convergence in the later iterations. For sampling, we again rely on Latin Hypercube Sampling (LHS [<xref ref-type="bibr" rid="R36">36</xref>]). Typically each parameter is sampled independently, although correlations can be imposed using modified LHS algorithms [<xref ref-type="bibr" rid="R41">41</xref>, <xref ref-type="bibr" rid="R42">42</xref>]. Independent sampling ignores the joint distribution of model parameters which can affect convergence. However, in an iteration, the algorithm allows us to include the combinations selected in the previous iteration during the parameter selection step (History-dependent update strategy, HDUS). This maintains the joint distribution to a certain extent, particularly when the current sampling of parameters is not as good as the previous selection. It also ensures that good combinations once sampled are not lost, guaranteeing a monotonic decrease in χ<sup>2</sup> statistics with iteration. We can choose to not recall the previously selected parameter combinations (History independent update strategy, HIUS). HIUS saves on memory but does not guarantee convergence when weak rejection conditions (high FAR values) are used (<xref ref-type="supplementary-material" rid="SD1">Figures S6</xref> and <xref ref-type="supplementary-material" rid="SD1">S7</xref>).</p><p id="P28">We also look at relationships among the components of the selected parameter combinations, like correlation and linear regression. This helps infer data-dependent properties like practical non-identifiability and associated degree of compensation between the parameters, respectively. Such measures capture linear and local relationships conditioned on values of other model parameters. Since parameter estimation and higher-order relationships are derived from a sampling-based approach, we infer their significance using a corresponding analysis of the dummy parameter. Although the dummy parameter doesn’t affect model prediction and hence χ<sup>2</sup>, we let ABC-FAR modify its distribution. Ideally, the distribution of the selected dummy parameter values should ideally mimic that of the sampled values. A deviation can help us identify computational artifacts and sampling biases. Higher-order relationships between a model parameter and the dummy parameter can help infer the statistical significance of relationships between model parameters.</p></sec><sec id="S11"><title>DoE for a three-gene repressilator system using PARSEC</title><p id="P29">A three-gene repressilator system (modeled by Elowitz and Leibler in 2000 [<xref ref-type="bibr" rid="R43">43</xref>]) involves the cyclic repression of gene expression, where one gene represses the expression of another. We use a simple ODE model that monitors the levels of the proteins expressed by these genes, with parameters representing the repression kinetic rates and thresholds. The protein levels show oscillatory dynamics.</p><p id="P30">In the analysis corresponding to <xref ref-type="fig" rid="F4">Figure 4</xref>, we aim to identify a set of six synchronized measurements of protein B and C levels within 72 units of time (e.g. hours), for accurate parameter estimation. For this PARSEC calculates the parameter sensitivity indices (PSI) at a specific time point for levels of both proteins B and C. We concatenate the indices for proteins B and C to create a PARSEC-PSI vector for the measurement candidate at that time point. Based on the similarity of the PARSEC-PSI vectors, the measurement candidates are clustered via k-means clustering. We create six clusters, adhering to the constraint of having six measurements. From each cluster, we randomly select one representative candidate, which together forms the final design (PD<sub><italic>BC</italic></sub>). We predict 100 designs and compare them using ABC-FAR to identify the most informative one. Design comparison is based on the error in parameter estimation derived from the data emulating the design. To test the performance of PARSEC, we generate random designs (RD<sub><italic>BC</italic></sub> and RD<sub><italic>AB</italic></sub>), non-specific designs (PD<sub><italic>AB</italic></sub>), and anti-PARSEC designs (WD<sub><italic>BC</italic></sub>). Each design constitutes six simultaneous measurements of the two variables (the protein levels). We generate a total of 100 designs for each mechanism, except for WD<sub><italic>BC</italic></sub> where we generate only 40 designs.</p><p id="P31">Next, we study the robustness of PARSEC designs to uncertainty, characterized by a uniform distribution in log scale, over a four-fold range of values of two of the six model parameters (<xref ref-type="fig" rid="F5">Figures 5a</xref> and <xref ref-type="fig" rid="F1">b</xref>). PARSEC-PSI vector is constructed using PSI evaluated at five training values (Θ<sup><italic>k</italic></sup>) sampling the uncertainty via Latin Hypercube Sampling. Design evaluations are done at these five training samples and at another set of four test values (sampled via LHS). Since we consider a large dynamical range of uncertainty, we allow for a longer time frame of 120 units. Finally, while tuning the sample size (<xref ref-type="fig" rid="F5">Figures 5c</xref> and <xref ref-type="fig" rid="F5">d</xref>), we ignore uncertainty considerations evaluating PARSEC-PSI vectors at a single parameter guess and limit the duration of experiments to 72 units of time. Since the sample size is altered, we accordingly change the number of clusters which is used to partition the PARSEC-PSI vectors.</p></sec><sec id="S12"><title>Computational evaluation of a design</title><p id="P32">We evaluate design evaluation at specific values of model parameters. At the parameter combination, we simulate the model behavior based on the design specifications and sample this behavior according to the designs of interest to generate the associated data set. ABC-FAR is used to recover parameter values from the data. ABC-FAR selected parameter values are compared to the parameter combination used to generate the data, to calculate the estimation error associated with the design.</p><p id="P33">
<disp-formula id="FD1"><mml:math id="M2"><mml:mrow><mml:mspace width="0.2em"/><mml:mtext>Estimation error</mml:mtext><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>r</mml:mi></mml:munderover><mml:msqrt><mml:mrow><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>j</mml:mi><mml:mi>D</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p id="P34">Here <inline-formula><mml:math id="M3"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>j</mml:mi><mml:mi>D</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> denotes the value of the j<sup><italic>th</italic></sup> parameter of the combination used to generate data, <inline-formula><mml:math id="M4"><mml:mrow><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>j</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the corresponding value of the m<sup><italic>th</italic></sup> parameter combination selected by ABC-FAR. r and M indicate the number of free parameters to be estimated and the number of parameter combinations selected by ABC-FAR. In our implementation, ABC-FAR refines the posterior five times starting from a prior uniform distribution in log scale, spanning across two orders of magnitude. In each iteration, we sample 60,000 parameter combinations using a noisy version of the current estimate of marginal. We employ the History-dependent update strategy and a FAR value of 0.01. Furthermore, we add a uniform sampling noise <inline-formula><mml:math id="M5"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>}</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> according to the following expression,</p><p id="P35">
<disp-formula id="FD2"><mml:math id="M6"><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mi>k</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>0.25</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p id="P36">where <inline-formula><mml:math id="M7"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is the current estimate of marginal used to generate the sampling distribution <inline-formula><mml:math id="M8"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>m</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mi>k</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for the j<sup><italic>th</italic></sup> free parameter (θ<sub><italic>j</italic></sub>) in the k<sup><italic>th</italic></sup> iteration. Note <inline-formula><mml:math id="M9"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> would be the prior or initial guess.</p></sec></sec><sec id="S13" sec-type="discussion"><title>Discussion</title><p id="P37">PARameter SEnsitivity Clustering-based algorithm (PARSEC) exploits the model structure and its parameter sensitivity to optimise the experimental design.First, even though the distribution of measurement candidates selected in PARSEC(k) is similar to that in the random designs (<xref ref-type="fig" rid="F4">Figure 4c</xref>), the distribution of PARSEC-derived estimation errors is comparatively narrower and low (<xref ref-type="fig" rid="F4">Figure 4a</xref>). This suggests that the degree of overlap between PARSEC-PSI vectors is more important to DoE than the individual characteristics of the measurements. Although we pick candidates randomly from each cluster, a systematic selection can further improve the efficiency of design search. Second, the parameter estimation error derived from designs decreases with a reduction in the overlap of the parameter sensitivity (PSI) vectors of measurements (comparison of PD<sub><italic>BC</italic></sub> and WD<sub><italic>BC</italic></sub>, <xref ref-type="fig" rid="F4">Figure 4a</xref>). Third, the efficiency of PSI clustering correlates with the accuracy ratio of the designs, as we vary the sample size (<xref ref-type="fig" rid="F5">Figure 5d</xref>). This highlights the advantage of clustering the parameter sensitivity profiles in design generation and proposes a good guess for the optimal sample size for the experiment.</p><p id="P38">The critical ingredients of the algorithm are the PARSEC-PSI vectors. Our approach of vectorially conjoining the PSIs to create the PARSEC-PSI vectors allows us to identify generalist designs to mitigate parameter uncertainty. Here we concatenate the PARSEC-PSI evaluated at various parameter combinations (sampling the uncertainty), to construct the high-dimensional PARSEC-PSI vectors used for subsequent clustering and design selection. The robustness of designs to parameter uncertainty is a desirable property distinguishing PARSEC from locally optimal MBDoE approaches like Fisher’s information Matrix-based methods. Depending on the extent of uncertainty, PARSEC can either plan a ‘robust generalist’ design or a ‘locally optimized’ design. Nevertheless, robustness and accuracy are negatively affected as uncertainty increases (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S11</xref>).</p><p id="P39">The vector-conjunction approach also allows us to easily implement various design constraints. For example, in our current analysis, we enforced simultaneous measurements of the different variables. Alternately, suppose that measurements of different variables have to be acquired at a fixed offset; then the PSI of the variables would be concatenated together in an appropriately staggered manner, to construct the PARSEC-PSI vectors. In case such restrictions are not there, one can treat each of the individual variables as different measurement candidates, during clustering and design selection (see <xref ref-type="supplementary-material" rid="SD1">Supplementary text SI S2</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S1</xref>). Furthermore the type of measurement also influences design specifications: certain variables may be more convenient/cheaper to measure than others. PARSEC can be implemented to identify variable combinations for economical and informative designs. Other specifications, like the choice of initial conditions, can also be tuned to improve designs for parameter estimation.</p><p id="P40">Optimizing the design parameters and design selection requires design evaluation. PARSEC employs Approximate Bayesian Computation-Fixed Acceptance Rate (ABC-FAR) algorithm for automated and fair comparison of designs. This is possible as ABC-FAR considers a global and relative sampling-rejection criterion (FAR), bypassing data-dependent stipulations. Additionally, a global parameter sampling method incorporating simulated annealing is used to improve the robustness of estimation and avoid local minima and biases. We have validated the algorithm for accuracy, reliability, efficiency, and robustness to initial bias and measurement noise. The efficiency can be further improved by adapting the FAR value for each iteration based on the improvement of convergence seen in the previous step. It has wide applicability and is amenable to the incorporation of available system-specific knowledge to improve convergence accuracy and speed (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S10</xref>). Furthermore, we derive data-specific correlations and dependencies among the estimated parameter values, which can be used to infer properties, like practical identifiability. Like some of the other ABC methods, ABC-FAR can also be adapted for model selection by treating the identity of the model as a free parameter.</p><p id="P41">Parameter estimation-based design comparison, the final step in PARSEC is computationally intensive, much like other Bayesian Model-Based Design of Experiments (MBDoEs). However, the tight distribution of low estimation errors in PARSEC(k) indicates that its predicted designs are generally informative. This suggests that a limited number of design space samplings through PARSEC(k) should suffice, thereby reducing computational costs.</p><p id="P42">n alternative approach to mitigate computational burdens involves the use of fuzzy clustering. When considering the two variations of PARSEC presented here, it’s worth noting that PARSEC(c) designs (fuzzy clustering) represent a subset of PARSEC(k) designs. PARSEC(c) is highly efficient at identifying good designs but may overlook the most informative ones. In contrast, PARSEC(k) conducts a more exhaustive search, aiming to identify the most informative designs.</p><p id="P43">In summary, PARSEC leverages mathematical models and simulations to guide experimental design, maximizing the information obtained about model parameters within constraints. It can be applied to characterize complex systems or investigate the effects of perturbations. For instance, PARSEC can identify standardized, informative, and cost-effective drug screening experiments, facilitating the quantification and comparison of the impacts of different drugs on model parameters.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS189718-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d82aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S14"><title>Acknowledgement</title><p>We thank Dr. Mohit Kumar Jolly for sharing computational resources for carrying out the various analyses. We also appreciate the valuable feedback provided by Thiruvickraman Jothiprakasam, Ramya Boddepalli, Paras Jain, Sumanta Mukerjee and Suraj Jagtap on the manuscript.</p><sec id="S15"><title>Funding</title><p>This work was supported by the Indian Institute of Science Bangalore (RR), Wellcome Trust—DBT India Alliance intermediate fellowship (RR) and the Prime Minister Research Fellowship (HC).</p></sec></ack><fn-group><fn fn-type="con" id="FN1"><p id="P44"><bold>Author Contributions</bold></p><p id="P45">Conceptualization, result interpretation and original draft preparation: HC and RR. Software and formal analysis: HC.</p></fn><fn fn-type="conflict" id="FN2"><p id="P46"><bold>Competing interests</bold></p><p id="P47">The authors declare that no competing interests exist.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St John</surname><given-names>RC</given-names></name><name><surname>Draper</surname><given-names>Norman R</given-names></name></person-group><article-title>D-optimality for regression designs: a review</article-title><source>Technometrics</source><year>1975</year><volume>17</volume><issue>1</issue><fpage>15</fpage><lpage>23</lpage></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Welch</surname><given-names>William J</given-names></name></person-group><article-title>Computer-aided design of experiments for response estimation</article-title><source>Technometrics</source><year>1984</year><volume>26</volume><issue>3</issue><fpage>217</fpage><lpage>224</lpage></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franceschini</surname><given-names>Gaia</given-names></name><name><surname>Macchietto</surname><given-names>Sandro</given-names></name></person-group><article-title>Model-based design of experiments for parameter precision: State of the art</article-title><source>Chemical Engineering Science</source><year>2008</year><volume>63</volume><issue>19</issue><fpage>4846</fpage><lpage>4872</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walter</surname><given-names>Éric</given-names></name><name><surname>Pronzato</surname><given-names>Luc</given-names></name></person-group><article-title>Qualitative and quantitative experiment design for phenomenological models—a survey</article-title><source>Automatica</source><year>1990</year><volume>26</volume><issue>2</issue><fpage>195</fpage><lpage>213</lpage></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shirt</surname><given-names>Roger W</given-names></name><name><surname>Harris</surname><given-names>Thomas J</given-names></name><name><surname>Bacon</surname><given-names>David W</given-names></name></person-group><article-title>Experimental design considerations for dynamic systems</article-title><source>Industrial &amp; engineering chemistry research</source><year>1994</year><volume>33</volume><issue>11</issue><fpage>2656</fpage><lpage>2667</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkinson</surname><given-names>Anthony C</given-names></name><collab>VV370955 Fedorov</collab></person-group><article-title>The design of experiments for discriminating between two rival models</article-title><source>Biometrika</source><year>1975</year><volume>62</volume><issue>1</issue><fpage>57</fpage><lpage>70</lpage></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atkinson</surname><given-names>Anthony C</given-names></name><name><surname>Fedorov</surname><given-names>Valerii Vadimovich</given-names></name></person-group><article-title>Optimal design: Experiments for discriminating between several models</article-title><source>Biometrika</source><year>1975</year><volume>62</volume><issue>2</issue><fpage>289</fpage><lpage>303</lpage></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahmohammadi</surname><given-names>Ali</given-names></name><name><surname>McAuley</surname><given-names>Kimberley B</given-names></name></person-group><article-title>Sequential model-based a-optimal design of experiments when the fisher information matrix is noninvertible</article-title><source>Industrial &amp; Engineering Chemistry Research</source><year>2018</year><volume>58</volume><issue>3</issue><fpage>1244</fpage><lpage>1261</lpage></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanlier</surname><given-names>Joep</given-names></name><name><surname>Tiemann</surname><given-names>Christian A</given-names></name><name><surname>Hilbers</surname><given-names>Peter AJ</given-names></name><name><surname>van Riel</surname><given-names>Natal AW</given-names></name></person-group><article-title>Optimal experiment design for model selection in biochemical networks</article-title><source>BMC systems biology</source><year>2014</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmcid">PMC3946009</pub-id><pub-id pub-id-type="pmid">24555498</pub-id><pub-id pub-id-type="doi">10.1186/1752-0509-8-20</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Jialu</given-names></name><name><surname>Dowling</surname><given-names>Alexander W</given-names></name></person-group><article-title>Pyomo. doe: An open-source package for model-based design of experiments in python</article-title><source>AIChE Journal</source><year>2022</year><volume>68</volume><issue>12</issue><elocation-id>e17813</elocation-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoica</surname><given-names>Petre</given-names></name><name><surname>Marzetta</surname><given-names>Thomas L</given-names></name></person-group><article-title>Parameter estimation problems with singular information matrices</article-title><source>IEEE Transactions on Signal Processing</source><year>2001</year><volume>49</volume><issue>1</issue><fpage>87</fpage><lpage>90</lpage></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Yen-Huan</given-names></name><name><surname>Yeh</surname><given-names>Ping-Cheng</given-names></name></person-group><article-title>An interpretation of the moore-penrose generalized inverse of a singular fisher information matrix</article-title><source>IEEE Transactions on Signal Processing</source><year>2012</year><volume>60</volume><issue>10</issue><fpage>5532</fpage><lpage>5536</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giessmann</surname><given-names>Robert T</given-names></name><name><surname>Krausch</surname><given-names>Niels</given-names></name><name><surname>Kaspar</surname><given-names>Felix</given-names></name><name><surname>Bournazou</surname><given-names>Mariano Nicolas Cruz</given-names></name><name><surname>Wagner</surname><given-names>Anke</given-names></name><name><surname>Neubauer</surname><given-names>Peter</given-names></name><name><surname>Gimpel</surname><given-names>Matthias</given-names></name></person-group><article-title>Dynamic modelling of phosphorolytic cleavage catalyzed by pyrimidine-nucleoside phosphorylase</article-title><source>Processes</source><year>2019</year><volume>7</volume><issue>6</issue><fpage>380</fpage></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusumo</surname><given-names>Kennedy Putra</given-names></name><name><surname>Kuriyan</surname><given-names>Kamal</given-names></name><name><surname>Vaidyaraman</surname><given-names>Shankarraman</given-names></name><name><surname>García-Muñoz</surname><given-names>Salvador</given-names></name><name><surname>Shah</surname><given-names>Nilay</given-names></name><name><surname>Chachuat</surname><given-names>Benoît</given-names></name></person-group><article-title>Risk mitigation in model-based experiment design: a continuous-effort approach to optimal campaigns</article-title><source>Computers &amp; Chemical Engineering</source><year>2022</year><volume>159</volume><elocation-id>107680</elocation-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><collab>TNE Greville</collab><article-title>The pseudoinverse of a rectangular or singular matrix and its application to the solution of systems of linear equations</article-title><source>SIAM review</source><year>1959</year><volume>1</volume><issue>1</issue><fpage>38</fpage><lpage>43</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hero</surname><given-names>Alfred O</given-names></name><name><surname>Fessler</surname><given-names>Jeffrey A</given-names></name><name><surname>Usman</surname><given-names>Mohammad</given-names></name></person-group><article-title>Exploring estimator bias-variance trade-offs using the uniform cr bound</article-title><source>IEEE Transactions on Signal Processing</source><year>1996</year><volume>44</volume><issue>8</issue><fpage>2026</fpage><lpage>2041</lpage></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sen</surname><given-names>A</given-names></name><name><surname>STNHA</surname><given-names>NK</given-names></name></person-group><article-title>A generalized pseudoinverse algorithm for unbiased parameter estimation</article-title><source>International Journal of Systems Science</source><year>1975</year><volume>6</volume><issue>12</issue><fpage>1103</fpage><lpage>1109</lpage></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaloner</surname><given-names>Kathryn</given-names></name><name><surname>Verdinelli</surname><given-names>Isabella</given-names></name></person-group><article-title>Bayesian experimental design: A review</article-title><source>Statistical science</source><year>1995</year><fpage>273</fpage><lpage>304</lpage><comment>pages</comment></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Imani</surname><given-names>Mahdi</given-names></name><name><surname>Ghoreishi</surname><given-names>Seyede Fatemeh</given-names></name></person-group><source>Bayesian optimization objective-based experimental design</source><conf-name>2020 American control conference (ACC)</conf-name><year>2020</year><fpage>3405</fpage><lpage>3411</lpage><publisher-name>IEEE</publisher-name></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainforth</surname><given-names>Tom</given-names></name><name><surname>Foster</surname><given-names>Adam</given-names></name><name><surname>Ivanova</surname><given-names>Desi R</given-names></name><name><surname>Smith</surname><given-names>Freddie Bickford</given-names></name></person-group><article-title>Modern bayesian experimental design</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2302 14545</elocation-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Den Berg</surname><given-names>Jojanneke</given-names></name><name><surname>Curtis</surname><given-names>Andrew</given-names></name><name><surname>Trampert</surname><given-names>Jeannot</given-names></name></person-group><article-title>Optimal nonlinear bayesian experimental design: an application to amplitude versus offset experiments</article-title><source>Geophysical Journal International</source><year>2003</year><volume>155</volume><issue>2</issue><fpage>411</fpage><lpage>421</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Cong</given-names></name><name><surname>Chaloner</surname><given-names>Kathryn</given-names></name></person-group><article-title>Bayesian experimental design for nonlinear mixed-effects models with application to hiv dynamics</article-title><source>Biometrics</source><year>2004</year><volume>60</volume><issue>1</issue><fpage>25</fpage><lpage>33</lpage><pub-id pub-id-type="pmid">15032770</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durán</surname><given-names>Marco A</given-names></name><name><surname>White</surname><given-names>Benjamin S</given-names></name></person-group><article-title>Bayesian estimation applied to effective heat transfer coefficients in a packed bed</article-title><source>Chemical engineering science</source><year>1995</year><volume>50</volume><issue>3</issue><fpage>495</fpage><lpage>510</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruggoo</surname><given-names>Arvind</given-names></name><name><surname>Vandebroek</surname><given-names>Martina</given-names></name></person-group><article-title>Bayesian sequential dd optimal model-robust designs</article-title><source>Computational statistics &amp; data analysis</source><year>2004</year><volume>47</volume><issue>4</issue><fpage>655</fpage><lpage>673</lpage></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahmohammadi</surname><given-names>Ali</given-names></name><name><surname>McAuley</surname><given-names>Kimberley B</given-names></name></person-group><article-title>Using prior parameter knowledge in model-based design of experiments for pharmaceutical production</article-title><source>AIChE Journal</source><year>2020</year><volume>66</volume><issue>11</issue><elocation-id>e17021</elocation-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Letham</surname><given-names>Benjamin</given-names></name><name><surname>Letham</surname><given-names>Portia A</given-names></name><name><surname>Rudin</surname><given-names>Cynthia</given-names></name><name><surname>Browne</surname><given-names>Edward P</given-names></name></person-group><article-title>Prediction uncertainty and optimal experimental design for learning dynamical systems</article-title><source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source><year>2016</year><volume>26</volume><issue>6</issue><elocation-id>063110</elocation-id><pub-id pub-id-type="pmid">27368775</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>Donald B</given-names></name></person-group><article-title>Bayesianly justifiable and relevant frequency calculations for the applied statistician</article-title><source>The Annals of Statistics</source><year>1984</year><fpage>1151</fpage><lpage>1172</lpage><comment>pages</comment></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavaré</surname><given-names>Simon</given-names></name><name><surname>Balding</surname><given-names>David J</given-names></name><name><surname>Griffiths</surname><given-names>Robert C</given-names></name><name><surname>Donnelly</surname><given-names>Peter</given-names></name></person-group><article-title>Inferring coalescence times from dna sequence data</article-title><source>Genetics</source><year>1997</year><volume>145</volume><issue>2</issue><fpage>505</fpage><lpage>518</lpage><pub-id pub-id-type="pmcid">PMC1207814</pub-id><pub-id pub-id-type="pmid">9071603</pub-id><pub-id pub-id-type="doi">10.1093/genetics/145.2.505</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pritchard</surname><given-names>Jonathan K</given-names></name><name><surname>Seielstad</surname><given-names>Mark T</given-names></name><name><surname>Perez-Lezaun</surname><given-names>Anna</given-names></name><name><surname>Feldman</surname><given-names>Marcus W</given-names></name></person-group><article-title>Population growth of human y chromosomes: a study of y chromosome microsatellites</article-title><source>Molecular biology and evolution</source><year>1999</year><volume>16</volume><issue>12</issue><fpage>1791</fpage><lpage>1798</lpage><pub-id pub-id-type="pmid">10605120</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marjoram</surname><given-names>Paul</given-names></name><name><surname>Molitor</surname><given-names>John</given-names></name><name><surname>Plagnol</surname><given-names>Vincent</given-names></name><name><surname>Tavaré</surname><given-names>Simon</given-names></name></person-group><article-title>Markov chain monte carlo without likelihoods</article-title><source>Proceedings of the National Academy of Sciences</source><year>2003</year><volume>100</volume><issue>26</issue><fpage>15324</fpage><lpage>15328</lpage><pub-id pub-id-type="pmcid">PMC307566</pub-id><pub-id pub-id-type="pmid">14663152</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0306899100</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Moral</surname><given-names>Pierre</given-names></name><name><surname>Doucet</surname><given-names>Arnaud</given-names></name><name><surname>Jasra</surname><given-names>Ajay</given-names></name></person-group><article-title>Sequential monte carlo samplers</article-title><source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source><year>2006</year><volume>68</volume><issue>3</issue><fpage>411</fpage><lpage>436</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toni</surname><given-names>Tina</given-names></name><name><surname>Welch</surname><given-names>David</given-names></name><name><surname>Strelkowa</surname><given-names>Natalja</given-names></name><name><surname>Ipsen</surname><given-names>Andreas</given-names></name><name><surname>Stumpf</surname><given-names>Michael PH</given-names></name></person-group><article-title>Approximate bayesian computation scheme for parameter inference and model selection in dynamical systems</article-title><source>Journal of the Royal Society Interface</source><year>2009</year><volume>6</volume><issue>31</issue><fpage>187</fpage><lpage>202</lpage><pub-id pub-id-type="pmcid">PMC2658655</pub-id><pub-id pub-id-type="pmid">19205079</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2008.0172</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marin</surname><given-names>Jean-Michel</given-names></name><name><surname>Pudlo</surname><given-names>Pierre</given-names></name><name><surname>Robert</surname><given-names>Christian P</given-names></name><name><surname>Ryder</surname><given-names>Robin J</given-names></name></person-group><article-title>Approximate bayesian computational methods</article-title><source>Statistics and Computing</source><year>2012</year><volume>22</volume><issue>6</issue><fpage>1167</fpage><lpage>1180</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chhajer</surname><given-names>Harsh</given-names></name><name><surname>Rizvi</surname><given-names>Vaseef A</given-names></name><name><surname>Roy</surname><given-names>Rahul</given-names></name></person-group><article-title>Life cycle process dependencies of positive-sense rna viruses suggest strategies for inhibiting productive cellular infection</article-title><source>J R Soc Interface</source><year>2021</year><pub-id pub-id-type="pmcid">PMC8580453</pub-id><pub-id pub-id-type="pmid">34753308</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2021.0401</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raue</surname><given-names>Andreas</given-names></name><name><surname>Kreutz</surname><given-names>Clemens</given-names></name><name><surname>Maiwald</surname><given-names>Thomas</given-names></name><name><surname>Bachmann</surname><given-names>Julie</given-names></name><name><surname>Schilling</surname><given-names>Marcel</given-names></name><name><surname>Klingmüller</surname><given-names>Ursula</given-names></name><name><surname>Timmer</surname><given-names>Jens</given-names></name></person-group><article-title>Structural and practical identifiability analysis of partially observed dynamical models by exploiting the profile likelihood</article-title><source>Bioinformatics</source><year>2009</year><volume>25</volume><issue>15</issue><fpage>1923</fpage><lpage>1929</lpage><pub-id pub-id-type="pmid">19505944</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKay</surname><given-names>Michael D</given-names></name><name><surname>Beckman</surname><given-names>Richard J</given-names></name><name><surname>Conover</surname><given-names>William J</given-names></name></person-group><article-title>A comparison of three methods for selecting values of input variables in the analysis of output from a computer code</article-title><source>Technometrics</source><year>2000</year><volume>42</volume><issue>1</issue><fpage>55</fpage><lpage>61</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kucherenko</surname><given-names>Sergei</given-names></name><name><surname>Albrecht</surname><given-names>Daniel</given-names></name><name><surname>Saltelli</surname><given-names>Andrea</given-names></name></person-group><article-title>Exploring multi-dimensional spaces: A comparison of latin hypercube and quasi monte carlo sampling techniques</article-title><source>arXiv preprint</source><year>2015</year><elocation-id>arXiv:1505 02350</elocation-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>Scott</given-names></name><name><surname>Gelatt</surname><given-names>Daniel</given-names></name><name><surname>Vecchi</surname><given-names>Mario P</given-names></name></person-group><article-title>Optimization by simulated annealing</article-title><source>science</source><year>1983</year><volume>220</volume><issue>4598</issue><fpage>671</fpage><lpage>680</lpage><pub-id pub-id-type="pmid">17813860</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marino</surname><given-names>S</given-names></name><name><surname>Hogue</surname><given-names>IB</given-names></name><name><surname>Ray</surname><given-names>CJ</given-names></name><name><surname>Kirschner</surname><given-names>DE</given-names></name></person-group><article-title>A methodology for performing global uncertainty and sensitivity analysis in systems biology</article-title><source>J Theor Biol</source><year>2008</year><month>Sep</month><volume>254</volume><issue>1</issue><fpage>178</fpage><lpage>196</lpage><pub-id pub-id-type="pmcid">PMC2570191</pub-id><pub-id pub-id-type="pmid">18572196</pub-id><pub-id pub-id-type="doi">10.1016/j.jtbi.2008.04.011</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saltelli</surname><given-names>Andrea</given-names></name><name><surname>Bolado</surname><given-names>Ricardo</given-names></name></person-group><article-title>An alternative way to compute Fourier amplitude sensitivity test (FAST</article-title><source>Computational Statistics &amp; Data Analysis</source><year>1998</year><volume>26</volume><issue>4</issue><fpage>445</fpage><lpage>460</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iman</surname><given-names>Ronald L</given-names></name><name><surname>Conover</surname><given-names>William-Jay</given-names></name></person-group><article-title>A distribution-free approach to inducing rank correlation among input variables</article-title><source>Communications in Statistics-Simulation and Computation</source><year>1982</year><volume>11</volume><issue>3</issue><fpage>311</fpage><lpage>334</lpage></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iman</surname><given-names>Ronald L</given-names></name><name><surname>Davenport</surname><given-names>James M</given-names></name></person-group><article-title>Rank correlation plots for use with correlated input variables</article-title><source>Communications in Statistics-Simulation and Computation</source><year>1982</year><volume>11</volume><issue>3</issue><fpage>335</fpage><lpage>360</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elowitz</surname><given-names>Michael B</given-names></name><name><surname>Leibler</surname><given-names>Stanislas</given-names></name></person-group><article-title>A synthetic oscillatory network of transcriptional regulators</article-title><source>Nature</source><year>2000</year><volume>403</volume><issue>6767</issue><fpage>335</fpage><lpage>338</lpage><pub-id pub-id-type="pmid">10659856</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Schematic for PARSEC: PARameter-SEnsitivity driven Clustering based experiment design algorithm.</title><p>Given the parameter uncertainty, PARSEC samples some representative values (Θ<sup><italic>k</italic></sup>) and evaluates parameter sensitivity indices (PSI) for each of the feasible measurement candidates there (Step 1). Here we assume that simultaneous measurement of species 1 and 2 constitute a single measurement candidate. The PSI vectors for the same measurement, but evaluated at different parameter values, are vectorially conjoined to create the PARSEC-PSI vector (enclosed in the black rectangle). Based on the similarity among the PARSEC-PSI vectors, the measurement candidates are clustered (step 2). One candidate from each cluster is randomly selected to constitute the predicted design (step 3). Thus the number of clusters used in partitioning translates to the experiment sample size, which can be optimized within the practical constraints of the experiment. Since clustering and subsequent candidate selection are stochastic, PARSEC delivers multiple realizations of design. The predicted designs are computationally compared to identify the optimal one (step 4). It should be noted that PSI vectors can come from continuous range and hence an appropriate clustering algorithm should be used. Furthermore, the set of candidates before clustering can be filtered optionally to enrich for high overall PSI values.</p></caption><graphic xlink:href="EMS189718-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Schematic and performance of ABC-FAR.</title><p>(a) Schematic: ABC-FAR uses an iterative, ABC approach to estimate the likely distribution of model parameter values that minimizes the χ<sup>2</sup> function (see Methods). In an iteration, ABC-FAR (1) augments the current guess of marginal with a sampling noise, (2) uses them to sample N parameter combinations (via Latin Hypercube Sampling [<xref ref-type="bibr" rid="R36">36</xref>]), (3) at which the χ<sup>2</sup> function is evaluated, (4) to select M combinations (=FAR × N) with the lowest χ<sup>2</sup> values to update the marginal, and the iteration restarts with the updated estimate of the marginal. (b) Model fitting using ABC-FAR: We use ABC-FAR to fit the Lotka Volterra model to a computationally generated data set. The figure shows the model prediction due to each of the 250 parameter combinations (thin solid lines) selected after eight iterations, along with the averaged dynamics (thick solid lines) and the data set used for fitting (open circles). Here we used a FAR value of 0.25 and the history-dependent update strategy (see Methods). (c) ABC-FAR reduces χ<sup>2</sup> iteratively: The distribution of χ<sup>2</sup> values corresponding to the parameter combinations selected in each of the eight iterations are shown.</p></caption><graphic xlink:href="EMS189718-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Accuracy and efficiency of the parameter estimation algorithm.</title><p>(a) We plot the value of one of the model parameters (<italic>δ</italic>, death rate of predators) along with the χ<sup>2</sup> value corresponding to the parameter combinations selected in every iteration associated with <xref ref-type="fig" rid="F2">figure 2b</xref>. The inset show the initial guess and the estimated posterior for <italic>δ</italic>. The distribution converges to sharp distribution around the ground truth value. (b) The value of two model parameters (<italic>a</italic>: birth rate of prey, and <italic>b</italic>: prey death rate due to predation) corresponding to the selected combinations are shown. The correlation is used to approximate the practical non-identifiability, and the regression (inset) is used to measure the degree of compensation for the two model parameters. (c) We plot the value of a dummy parameters along with the χ<sup>2</sup> value corresponding to the parameter combinations selected in every iteration associated with <xref ref-type="fig" rid="F2">figure 2b</xref>. The absence of any structure in the selected dummy parameter values suggests an absence of computational artifacts due to the algorithm. (d) The value of <italic>a</italic> and the dummy parameter corresponding to the selected combinations are shown. The correlation and regression (inset) measures associated with dummy parameter, can be used to identify thresholds for statistical significance.</p></caption><graphic xlink:href="EMS189718-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Performance of PARSEC-predicted designs.</title><p>(a) To validate the performance of PARSEC(k), we compare designs generated by multiple modalities (illustrated in the top panel). RD<sub><italic>BC</italic></sub> and RD<sub><italic>AB</italic></sub> denote random designs where the measurement time points were chosen in an unbiased, non-repetitive manner. PD<sub><italic>BC</italic></sub>, WD<sub><italic>BC</italic></sub>, and PD<sub><italic>AB</italic></sub> were constructed based on the clustering of sensitivity profiles of levels of proteins B and C. For PD<sub><italic>BC</italic></sub> and PD<sub><italic>AB</italic></sub>, we pick one and only one measurement candidate from each cluster; whereas for WD<sub><italic>BC</italic></sub>, all measurement candidates were chosen from the same cluster. PD<sub><italic>BC</italic></sub>, WD<sub><italic>BC</italic></sub> and RD<sub><italic>BC</italic></sub> denote designs where levels of proteins B and C are measured simultaneously, whereas PD<sub><italic>AB</italic></sub> and RD<sub><italic>AB</italic></sub> involve simultaneous measurement of levels of proteins A and B. The bottom panel displays the parameter estimation error corresponding to different realizations and its approximate distribution, for the different modalities. We consider 40 realizations of WD<sub><italic>BC</italic></sub> and 100 realizations of each of the other modalities. The height of the bars depicts their mean. (b) The average estimation error of different sub-samplings of the 100 realizations of PD<sub><italic>BC</italic></sub> and RD<sub><italic>BC</italic></sub> is shown. (c) The distribution of different measurements represented in the 100 realizations of PD<sub><italic>BC</italic></sub> and RD<sub><italic>BC</italic></sub> is shown. The distributions, discretized in 12 bins, look similar have comparable entropy (PD<sub><italic>BC</italic></sub>: 3.55 bits; RD<sub><italic>BC</italic></sub>: 3.58 bits), and closely resemble a uniform distribution (3.59 bits).</p></caption><graphic xlink:href="EMS189718-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Robustness of PARSEC(k) designs.</title><p>(a) Accuracy ratio evaluates how informative PARSEC(k) designs are compared to random designs. We see that when the ground truth guess (<inline-formula><mml:math id="M10"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>, used to implement PARSEC) is close to the actual ground truth (<italic>k</italic><sub>1</sub>), PARSEC(k) designs lead to about 3.2 fold lower estimation error compared to random designs. This ratio (denoted as accuracy ratio) falls as <italic>k</italic><sub>1</sub> deviates from <inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mn>0</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> (when the ground truth guess is bad). (b) However, PARSEC(k) can accommodate parameter uncertainty to identify generalist designs. It samples the range (Θ<sup><italic>k</italic></sup>) and clubs together the parameter sensitivity indices evaluated at these samples to create the PSI vector. The designs identified are based on the clustering of these PSI Vectors. The performance of PARSEC(k) is evaluated at Θ<sup><italic>k</italic></sup>’s (train analysis) and another set of parameter samples (Ω<sup><italic>k</italic></sup>, test analysis). (c) We use PARSEC(k) to accommodate a four-fold uncertainty in values of two of the six parameters characterizing the repressilator behavior and predict generalist or robust designs (schematically shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure S11</xref>). Using LHS, we identify five training samples (representing the uniform distribution in the log scales) and inform the PARSEC-PSI vectors; design evaluations are done at these training samples and four test samples. Train and test errors are calculated as the average estimation error across the training samples and test samples, respectively. The averaged estimation errors of 100 PARSEC(k) (black scatter) and random (red scatter) designs in train and test analyses are shown. The corresponding marginal is also shown. The plot highlights that PARSEC(k) designs are more informative. Interestingly, the PARSEC(k) designs that perform best in the training analysis also do so in the test analysis, thus identifying generalist designs.</p></caption><graphic xlink:href="EMS189718-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Optimizing experimental sample size.</title><p>(a)We vary the sample size of the experiment and monitor the performance of PARSEC(k), PARSEC(c) and random designs, generalizing the design problem analyzed in <xref ref-type="fig" rid="F4">Figure 4</xref>. The inset shows how the intra-cluster distance varies with number of clusters, due to k-means (open circle) and c-means (open square) clustering employed by PARSEC(k) and PARSEC(c) respectively. (b) We plot how the accuracy advantage of PARSEC(k) (open circle) and PARSEC(c) (open square), defined as the ratio of the mean estimation error of random designs and that of the corresponding PARSEC designs, vary with the number of clusters, which corresponds to the number of measurements. Here we construct 100 designs of each modality: random, PARSEC(k) and PARSEC(c). However, not all the 100 PARSEC(c) designs were unique resulting in a reduced downstream computation burden. Hence we also plot the ratio of PARSEC(c) designs and PARSEC(k) designs evaluated, as the fuzzy burden ratio.</p></caption><graphic xlink:href="EMS189718-f006"/></fig></floats-group></article>