<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189014</article-id><article-id pub-id-type="doi">10.1101/2023.10.02.560418</article-id><article-id pub-id-type="archive">PPR734838</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cochlear tuning characteristics arise from temporal prediction of natural sounds</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Trinh</surname><given-names>Freddy</given-names></name></contrib><contrib contrib-type="author"><name><surname>King</surname><given-names>Andrew J</given-names></name></contrib><contrib contrib-type="author"><name><surname>Willmore</surname><given-names>Ben D B</given-names></name></contrib><contrib contrib-type="author"><name><surname>Harper</surname><given-names>Nicol</given-names></name></contrib><aff id="A1">Department of Physiology, Anatomy and Genetics, University of Oxford, Sherrington Building, Parks Road, Oxford, OX1 3PT, United Kingdom</aff></contrib-group><pub-date pub-type="nihms-submitted"><day>04</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>02</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The cochlea decomposes incoming sound waveforms into different frequency components along the length of its basilar membrane. The receptor hair cells at the apical end of this resonant membrane are tuned to the lowest sound frequencies, with the preferred sound frequency of hair cell tuning increasing near-exponentially along the length of the membrane towards its basal end. This frequency composition of the sound is then transmitted to the brain by the auditory nerve fibers that innervate the inner hair cells. Hair cells respond to a sound impulse with a temporally asymmetric envelope and the sharpness of their tuning changes as the frequency to which they are most sensitive varies with their position along the basilar membrane. We ask if there is a normative explanation for why the cochlea decomposes sounds in this manner. Inspired by findings in the retina, we propose that cochlear tuning properties may be optimized for temporal prediction. This principle states that the sensory features represented by neurons are optimized to predict immediate future input from recent past input. We show that an artificial neural network optimized for temporal prediction of the immediate future of raw waveforms of natural sounds from their recent past produces tuning properties that resemble those observed in the auditory nerve. Specifically, the model captures the temporally asymmetric impulse responses, the tonotopic distribution and variation in tuning sharpness along the cochlea, and the frequency glide polarity of the impulse responses. These characteristics are not captured by a similar model optimized for compression of the sound waveform, rather than prediction. Given its success in accounting for the tuning properties at various processing levels in the auditory and visual systems, this finding for the cochlea provides further evidence that temporal prediction may be a general principle of sensory processing.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Sensory neurons are tuned to diverse but specific features of the environment. While mechanistic models may account for the physical processes behind the tuning properties of sensory neurons, normative models aim to explain what role that tuning plays for the animal and consequently why those properties are seen as opposed to others. Various normative principles have been proposed to explain why particular receptive field properties may evolve; for example, the efficient (<xref ref-type="bibr" rid="R2">Barlow, 1959</xref>) or sparse (<xref ref-type="bibr" rid="R45">Olshausen and Field, 1996</xref>, <xref ref-type="bibr" rid="R46">1997</xref>; Hateren and Ruderman, 1998; <xref ref-type="bibr" rid="R7">Carlson et al., 2012</xref>) use of coding resources for information transmission, or the finding of slowly-varying features (<xref ref-type="bibr" rid="R29">Hyvärinen et al., 2004</xref>; <xref ref-type="bibr" rid="R6">Carlin and Elhilali, 2013</xref>), given the statistical regularities of natural sensory stimuli. Here, we propose that sensory systems, including the peripheral auditory system, may be governed by a different normative principle, namely temporal prediction (<xref ref-type="bibr" rid="R3">Bialek et al., 2001</xref>; <xref ref-type="bibr" rid="R49">Palmer et al., 2015</xref>); that is, they are optimized to represent features that can efficiently predict the immediate future of natural sensory input given the recent past input. Optimization for efficient temporal prediction may result in representations that better reflect underlying causes, eliminate irrelevant information, and guide future actions in response to the sensory input (<xref ref-type="bibr" rid="R3">Bialek et al., 2001</xref>).</p><p id="P3">There is evidence for temporal prediction in other sensory systems and at different processing levels. Notably, the temporal prediction principle has been examined in the larval salamander retina (<xref ref-type="bibr" rid="R49">Palmer et al., 2015</xref>; <xref ref-type="bibr" rid="R53">Salisbury and Palmer, 2016</xref>), where small groups of cells encode near-maximal predictive information. Theoretical work has also found that artificial neural networks optimized for temporal prediction on movies of natural scenes produce model units with receptive fields resembling those of neurons in primary visual cortex (<xref ref-type="bibr" rid="R55">Singer et al., 2018</xref>). Furthermore, a hierarchical temporal prediction model, where each subsequent layer is optimized to predict the activity of its input layer, can explain aspects of sensory tuning along the visual pathway, from the retina to the cortex (<xref ref-type="bibr" rid="R56">Singer et al., 2019</xref>). Finally, when simple networks are optimized to predict the future of natural sound spectrograms from their past for a dataset of natural sounds, the resulting receptive fields of the hidden units were found to resemble those of neurons recorded in primary auditory cortex (<xref ref-type="bibr" rid="R55">Singer et al., 2018</xref>). In this modelling of auditory cortex, the properties of the cochlea were fixed and assumed to be approximated by a spectrogram-like process.</p><p id="P4">Here, we investigate whether the temporal prediction principle applied to raw natural sound waveforms, rather than spectrograms, can also account for the well-known tuning properties of the cochlea, the site of auditory transduction. The cochlea acts as a frequency analyser and consists of a coiled tube divided lengthwise into three fluid-filled chambers (reviewed in <xref ref-type="bibr" rid="R15">Fettiplace, 2017</xref>). The central chamber is bounded on one side by the basilar membrane, which supports the Organ of Corti, where the receptor hair cells are located. The mechanical properties (notably stiffness) of the basilar membrane vary smoothly along the length of the cochlea, resulting in a gradient of frequency tuning. The sound frequency to which each region of the basilar membrane is tuned increases approximately exponentially from apex to base (<xref ref-type="bibr" rid="R36">Liberman, 1982</xref>), so that relatively more length is dedicated to low frequencies than to high frequencies. Vibration at a point on the basilar membrane is detected by the hair cells in the corresponding part of the Organ of Corti, and then transmitted to the brainstem by the auditory nerve fibers.</p><p id="P5">The impulse response of a point on the basilar membrane is largely inherited by the corresponding inner hair cell and auditory nerve fibers, such that auditory nerve fiber activity can be used to assess cochlear tuning (<xref ref-type="bibr" rid="R43">Narayan et al., 1998</xref>). In this paper, we focus on the impulse response function inherited from the cochlea, as measured from the firing of auditory nerve fibers. We will refer to these inherited impulse responses as ‘cochlear filters’. This tuning can be approximated by a gammatone filter (de Boer, 1975; Carney, 1993), which (in the time domain) is a sinusoidal function characterized by a temporally asymmetric envelope with a fast onset and a more gradual offset (<xref ref-type="bibr" rid="R18">Goblick and Pfeiffer, 1969</xref>). The absolute filter bandwidth increases with the characteristic frequency of the auditory nerve fibers, i.e. the sound frequency where the minimum response threshold is found (Glasberg and Moore, 1990; <xref ref-type="bibr" rid="R59">Walker et al., 2019</xref>). However, the relative bandwidth decreases with characteristic frequency, that is sharpness of tuning expressed as the quality factor (Q = characteristic frequency/bandwidth) increases (Evans, 1972; <xref ref-type="bibr" rid="R47">Oxenham and Shera, 2003</xref>; <xref ref-type="bibr" rid="R30">Joris et al., 2011</xref>; <xref ref-type="bibr" rid="R58">Sumner and Palmer, 2012</xref>).</p><p id="P6">The representation of sound in the cochlea has previously been investigated by models optimized for efficient coding of sound (<xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="R57">Smith and Lewicki, 2006</xref>). These models produced units with impulse responses that were similar to cochlear filters in many respects, showing sinusoidal ringing within a temporally limited envelope, and a range of frequency tuning across units. However, the envelopes of the impulse responses in <xref ref-type="bibr" rid="R35">Lewicki (2002)</xref> were temporally symmetrical in form, in contrast to the asymmetric temporal envelopes of cochlear filters. The model of <xref ref-type="bibr" rid="R57">Smith and Lewicki (2006)</xref> produced temporally asymmetric gammatone-like impulse responses, but the inference process used by the model did not respect causality, as it employed an iterative process that required all time points comprising a long waveform to be available simultaneously.</p><p id="P7">We asked whether the temporal prediction principle could capture features of cochlear tuning. Our initial approach made relatively few assumptions about the components of the cochlea. We modelled the cochlea as a bank of filters that were optimized to represent the features that best predicted the immediate future of natural sound waveforms from their recent past. We found that this simple model could reproduce many of the tuning properties of cochlear filters.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>The cochlear temporal prediction model</title><p id="P8">To examine whether optimization for temporal prediction could explain cochlear tuning properties, we trained a simple neural network model to predict the immediate future of sound waveforms from their recent past. The model was a feedforward neural network with one layer of hidden units (<xref ref-type="fig" rid="F1">Fig. 1</xref>) (see <xref ref-type="sec" rid="S12">Methods</xref> for more details). The input layer consisted of 128 units fully connected to 64 hidden units, and the hidden units were fully connected to 16 output units. The input was the immediate past waveform, and the activity of each hidden unit was a weighted sum of the input followed by a nonlinear transform using the scaled hyperbolic tangent function. The output weights of the network produce the output unit activity, which is a linear estimate of the upcoming future from the activity of the hidden units. We assumed that the code used by the cochlea can be approximately linearly decoded to provide a prediction of the future input. This is consistent with the observation that decoding stimuli from neural responses is often performed surprisingly well by a linear transformation (<xref ref-type="bibr" rid="R13">Eliasmith and Anderson, 2002</xref>).</p><p id="P9">To train the model to predict future input, we used a set of sound recordings representative of the natural auditory environment. This dataset included birdsong, animal vocalizations, environmental sounds, human speech by children and adults, and baby vocalizations. The stimuli were linearly filtered to mimic the transformation performed by the middle ear (see <xref ref-type="sec" rid="S12">Methods</xref>). We then added Gaussian noise to the input (9 dB signal-to-noise ratio, SNR), which has been found to aid in the capturing of sensory tuning properties by temporal prediction models (<xref ref-type="bibr" rid="R55">Singer et al., 2018</xref>). The model parameters were trained to predict the waveform over a brief timespan into the future (16 samples = 0.73 ms) from the present, given the recent past (128 samples = 5.8 ms) of the waveform immediately before the present. The model was trained using backpropagation to minimize the mean squared error (MSE) between the future waveform and the prediction of the waveform by the model. Once the model was trained, we interpreted the pattern of input weights to each hidden unit as being analogous to the impulse response of a specific site along the basilar membrane.</p></sec><sec id="S4"><title>Properties of the model when trained on natural sounds</title><p id="P10">The trained temporal prediction model produced substantial impulse responses for most of the hidden units (51 out of 64) (<xref ref-type="fig" rid="F2">Fig. 2</xref>). Hidden units with impulse response power below 1% of the average impulse response power were deemed not substantial and removed from further analysis. The impulse responses showed the characteristic sinusoidal ringing elicited by the basilar membrane with a rapid onset followed by a more prolonged offset. The envelopes were temporally asymmetric, with most energy at recent time lags and the energy decaying into the past. Hence, the model impulse responses replicate characteristics of cochlear filters. However, the onset of the sinusoidal ringing was generally faster for the model impulse responses than in real cochlear filters. Unlike biological impulse responses, some model impulse responses also showed a strong preference for the most recent stimulus sample, as seen by a click-like rise. <xref ref-type="fig" rid="F3">Figure 3</xref> illustrates the similarity between representative impulse responses of the trained model (<xref ref-type="fig" rid="F3">Fig. 3A</xref>) and impulse responses recorded from auditory nerve fibers of anesthetized cats (dataset from <xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>) (<xref ref-type="fig" rid="F3">Fig. 3B</xref>).</p><p id="P11">We determined the center frequency of each hidden unit as the peak of its impulse response in frequency space. As our model units only had a simple compressive nonlinearity, their center frequency will be the same as their best frequency at each sound intensity and their characteristic frequency. Each hidden unit displayed sharp frequency tuning, and when the units were ranked by their center frequency, they smoothly spanned a range from ~175 Hz up to ~7000 Hz (<xref ref-type="fig" rid="F4">Fig. 4A</xref>). We used an exponential equation that describes the tonotopic cochlear map in mammals (<xref ref-type="bibr" rid="R21">Greenwood, 1961</xref>, <xref ref-type="bibr" rid="R22">1990</xref>, <xref ref-type="bibr" rid="R23">1996</xref>; <xref ref-type="bibr" rid="R52">Robles and Ruggero, 2001</xref>) (<xref ref-type="fig" rid="F4">Fig. 4B</xref>) to fit the center frequency (<italic>f</italic>) as a function of the units’ rank order (<xref ref-type="fig" rid="F4">Fig. 4B</xref>). This is given by <italic>f</italic> = <italic>A</italic>(<italic>10</italic><sup><italic>Cz</italic></sup> <italic>+ B</italic>), where <italic>z</italic> is the distance from the cochlear apex scaled from 0 to 1, and <italic>A, B</italic> and <italic>C</italic> are the fitted parameters. Our model does not provide an absolute measure of distance from the cochlear apex, but we took each unit, ordered by center frequency, to represent an equal-sized adjacent space along the basilar membrane. Thus, the units represent a contiguous section somewhere along the basilar membrane. We took z = 0 at the position of the lowest-frequency model unit (not the apex), and z = 1 to be the position of the highest-frequency model unit (not the base), with each unit in between assigned a value of z proportional to its rank order r between these limits, that is <italic>z</italic> = (<italic>r</italic>-1)/(<italic>r</italic><sub>max</sub>-1), where <italic>r</italic><sub>max</sub> = 51 is the number of units with substantial impulse responses. The exponential function provided a better fit than a linear fit (F test, <italic>F</italic>(49,48) = 320.05, <italic>p</italic> = 1.11×10<sup>-16</sup>). Hence, the range of impulse responses in the model approximately captured the smooth, near-exponential arrangement of tuning frequency exhibited by the cochlea (<xref ref-type="bibr" rid="R21">Greenwood, 1961</xref>, <xref ref-type="bibr" rid="R22">1990</xref>, <xref ref-type="bibr" rid="R23">1996</xref>; <xref ref-type="bibr" rid="R36">Liberman, 1982</xref>; <xref ref-type="bibr" rid="R52">Robles and Ruggero, 2001</xref>).</p><p id="P12">The bandwidth of the impulse responses increased as the center frequency increased (Pearson correlation coefficient = 0.816, <italic>p</italic> = 7.01x10<sup>-12</sup>), as seen for cochlear filters (<xref ref-type="fig" rid="F4">Fig. 4C</xref>) (<xref ref-type="bibr" rid="R40">Moore, 1986</xref>; Glasberg and Moore, 1990). The relative sharpness of each impulse response was estimated by dividing the center frequency of the impulse response by its bandwidth at 10 dB below the spectral peak of the impulse response, thereby generating Q<sub>10</sub> values that could be compared to physiological data. In the auditory nerve, the Q<sub>10</sub> increases with characteristic frequency (Pearson correlation coefficient = 0.816, <italic>p</italic> = 2.43x10<sup>-24</sup> for the cat data in <xref ref-type="bibr" rid="R14">Evans, 1975</xref> (<xref ref-type="fig" rid="F4">Fig. 4D</xref>). The filter sharpness of the hidden units of our model similarly increased with centre frequency (Pearson correlation coefficient = 0.44, <italic>p</italic> = 0.00248), which matched the experimental findings fairly well over the frequency range of the hidden units (<xref ref-type="fig" rid="F4">Fig. 4D</xref>). The joint distribution of the model hidden units’ center frequencies and sharpness measures was not significantly different from that of the physiological data (Wald-Wolfowitz test, W = 0.393, <italic>p</italic> = 0.6527) (<xref ref-type="bibr" rid="R17">Friedman and Rafsky, 1979</xref>; <xref ref-type="bibr" rid="R39">Monaco, 2014</xref>). The temporal envelope of an impulse response is the temporal span that captures 95% of the impulse response’s power (<xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>). We found that this decreased for the model hidden units with increasing center frequency (Pearson correlation coefficient = -0.86, <italic>p</italic> = 7.592x10<sup>-14</sup>) (<xref ref-type="fig" rid="F4">Fig. 4E</xref>).</p><p id="P13">Finally, we investigated the instantaneous tuning properties of the model impulse responses. Detailed investigation of the impulse responses of auditory nerve fibers of the cat suggests that the frequency tuning of the impulse response changes slightly over its duration; rather than being exactly a gammatone, the impulse response instead often exhibits a glide either up or down in frequency (<xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>). This time-dependent frequency tuning (‘instantaneous frequency’ as a function of time) can be measured by looking at the timing of the zero-crossings of the impulse responses over time (See <xref ref-type="sec" rid="S12">Methods</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 1</xref>). <xref ref-type="bibr" rid="R8">Carney et al. (1999)</xref> found that auditory nerve fibers tuned to low frequencies (&lt;1500 Hz) tend to decrease or remain steady in instantaneous frequency over time (downward frequency glides), whereas those tuned to higher frequencies (&gt;1500 Hz) tend to increase in instantaneous frequency over time (upwards frequency glides). Thus, the slope of instantaneous frequency over time for impulse responses goes from negative to positive as the preferred sound frequency of auditory nerve fibers increases. We plotted these slopes as a function of center frequency for the impulse responses of the temporal prediction model (<xref ref-type="fig" rid="F4">Fig. 4F</xref>). This was done over the same range of auditory nerve fiber best frequencies measured by <xref ref-type="bibr" rid="R8">Carney et al. (1999)</xref> to facilitate comparison with the experimental data and because estimates of instantaneous frequency by the zero-crossing method are inaccurate above half the Nyquist frequency (<xref ref-type="bibr" rid="R27">Hoeks et al., 1984</xref>). Consistent with the cat auditory nerve recordings findings (<xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>), we found that the slopes of the model units increased with their center frequency (Pearson correlation coefficient 0.44, <italic>p</italic> = 0.0086). Thus, although the magnitude of the slopes was typically several times smaller than that seen in the experimental data, the same general trend of slope polarity going from negative to positive with increasing center frequency was seen in the temporal prediction model.</p></sec><sec id="S5"><title>The role of noise</title><p id="P14">We found that the addition of weak Gaussian noise to the input was necessary for the model to learn cochlea-like impulse responses (<xref ref-type="fig" rid="F3">Fig. 3</xref>). An absence of noise in the training set produced a representation that differed from that found in the cochlea and auditory nerve fibres (<xref ref-type="fig" rid="F5">Fig. 5A,B</xref>). The impulse responses did not reproduce gammatone impulse responses, and did not exhibit a gradual offset. Instead, the impulse responses displayed high-frequency oscillations with sequences of staggered onsets and offsets, and some contained an additional underlying low frequency oscillation (<xref ref-type="fig" rid="F5">Fig. 5A</xref>). Most impulse responses were not sharply tuned in frequency space and the center frequencies of the impulse responses were separated onto opposite ends of the frequency space with virtually no tuning to the midrange frequencies (<xref ref-type="fig" rid="F5">Fig. 5B</xref>).</p><p id="P15">To determine the effect of input noise on the prediction performance of our network, we explored the effect of applying noise to the input at a range of different signal-to-noise ratios (SNRs) (<xref ref-type="fig" rid="F5">Fig. 5C-F</xref>). Across a range from 3 to 15 dB SNR, hidden units exhibited cochlealike tuning (<xref ref-type="fig" rid="F5">Fig. 5D, E</xref>). The similarity between the hidden units and biological tuning deteriorated for noise levels outside this SNR range (<xref ref-type="fig" rid="F5">Fig. 5C, F</xref>). We speculated that the noise might act as a regularizer, in which case we might expect that the added noise should enable the temporal prediction model to better predict future input when evaluated on a held-out noiseless test set. In fact, we found that the no-noise case provided the best predictions on the noiseless test set, suggesting that the input noise does not regularize the model (<xref ref-type="fig" rid="F5">Fig. 5G</xref>). However, when we added noise to the test set inputs, we found that models trained in the range 3-15 dB SNR were particularly good at predicting the future across a range of test SNR levels (3-72 dB and no noise) (<xref ref-type="fig" rid="F5">Fig. 5H</xref>). This suggests that the cochlea may use a representation that is robust to the presence of varying levels of internal and external noise.</p><p id="P16">We also examined whether where in the model the noise was added was important for getting gammatone-like tuning curves. When Gaussian noise was added to the future targets rather than the past input, there was a deterioration of the biological similarity of the model (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 2A-B</xref>), similar to what was seen in the noiseless case. However, adding noise on both input and target resulted in a similar representation to the model trained on noisy input alone (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 2C-D</xref>), though there were fewer active units (34/64 hidden units were active) and a subset of these lacked sharp frequency tuning. We conclude that noise added to the input was the crucial factor in reproducing cochlear tuning properties.</p><p id="P17">To investigate the possibility that the noise enabled avoidance of local minima and saddle points during gradient descent, we took the network trained with 9 dB SNR and then trained it further with the noiseless stimuli. However, the resulting network showed the same results as the network that was simply trained with the noiseless stimuli (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 2E-F</xref>). Finally, we asked whether the reason why noise was necessary to obtain cochlea-like model features was due to it acting as a form of data augmentation. We trained a network on the same dataset without any added noise, but with approximately 10 times more sound snippets. We did this by choosing 6.5 ms snippets from the sound clips with onsets that were spaced by 0.65 ms, rather than 6.5 ms, providing many more snippets from the same sound clips, which now overlapped. This model still produced impulse responses that were not gammatone-like and without the characteristic cochlear tonotopic mapping (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig 2G-H</xref>). This suggests that the addition of noise did not function as data augmentation in order to settle on cochlea-like properties.</p></sec><sec id="S6"><title>The cochlear autoencoder model</title><p id="P18">We next also asked whether temporal prediction itself was crucial for our model to learn cochlea-like tuning. It is possible that a network that merely compresses auditory stimuli is sufficient to produce cochlea-like tuning, and that temporal prediction <italic>per se</italic> is not required. Two previous models based on the principles of efficient coding have had some success in reproducing aspects of cochlear filters (<xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="R57">Smith and Lewicki, 2006</xref>), though they have some limitations as outlined in the Introduction. Efficient coding seeks to use coding resources efficiently to represent all the incoming sensory input, and can be viewed as a form of compression of the input (<xref ref-type="bibr" rid="R9">Chalk et al., 2018</xref>). This is in contrast to temporal prediction, where incoming sensory input is only represented to the degree that it is predictive of future input. To investigate the extent to which shared characteristics between the temporal prediction model and cochlear features can be explained by compression rather than prediction, we trained an autoencoder neural network (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 3</xref>). The autoencoder model is identical to the temporal prediction model (<xref ref-type="fig" rid="F1">Figs. 1-4</xref>), apart from the output layer, which recreates the input rather than predicting the immediate future.</p><p id="P19">Unlike the temporal prediction model, the autoencoder was unable to capture the same range in frequency tuning and did not display sharp frequency tuning apart from a small subset of hidden units (<xref ref-type="fig" rid="F6">Fig. 6A</xref>). The span of the center frequencies of the hidden units did not capture the exponential relationship between center frequency and basilar membrane position seen in the cochlea, and was best fitted linearly (F test, <italic>F</italic>(62,61) = 0.34, <italic>p</italic> &gt; 0.99) (<xref ref-type="fig" rid="F6">Fig. 6B</xref>). A few somewhat gammatone-like impulse responses were produced, but without the temporal asymmetry exhibited by biological cochlear filters or the temporal prediction model (<xref ref-type="fig" rid="F6">Fig. 6C</xref>). Furthermore, the bandwidth of the impulse responses did not uniformly increase as a function of center frequency (ρ = 0.19, <italic>p</italic> = 0.13), (<xref ref-type="fig" rid="F6">Fig. 6D</xref>). The filter sharpness increased as a function of center frequency (ρ = 0.6, <italic>p</italic> = 1.5×10<sup>-7</sup>), but overshot the biological data across the frequency range (Wald-Wolfowitz test, W = -1.34, <italic>p</italic> = 0.0901) (<xref ref-type="fig" rid="F6">Fig. 6E</xref>). Finally, in contrast to cochlear filters, the temporal envelope of the autoencoder impulse responses was not correlated with the center frequency (Pearson correlation coefficient = -0.044, <italic>p</italic> = 0.731) (<xref ref-type="fig" rid="F6">Fig. 6F</xref>).</p><p id="P20">The autoencoder was trained on the 9 dB SNR sounds. For completeness we also trained the autoencoder on the noiseless sounds. The results were similar to the autoencoder trained on the noisy sounds, but the impulse responses were even less tightly frequency tuned (<xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 4</xref>).</p></sec></sec><sec id="S7" sec-type="discussion"><title>Discussion</title><p id="P21">We asked whether temporal prediction applied to natural sounds might be a computational principle underlying stimulus representation in the cochlea. To investigate this, we optimized a feedforward network with one hidden layer to predict the immediate future of sound waveforms given their recent past. The model was able to capture several features of cochlear tuning. The hidden units showed a wide range of frequency tuning, with more units tuned to lower frequencies than higher frequencies, consistent with the near-exponential dependence of characteristic frequency on basilar membrane position (<xref ref-type="bibr" rid="R21">Greenwood, 1961</xref>, <xref ref-type="bibr" rid="R22">1990</xref>, <xref ref-type="bibr" rid="R23">1996</xref>; <xref ref-type="bibr" rid="R36">Liberman, 1982</xref>). The impulse responses of the hidden units had gammatone-like temporal profiles and resembled those recorded from cat auditory nerve fibers (<xref ref-type="fig" rid="F2">Fig. 2</xref> and <xref ref-type="fig" rid="F3">3</xref>). More specifically, the impulse responses were temporally asymmetric with a rapid onset followed by a slower offset. Consistent with the biology, the absolute bandwidth and filter sharpness (expressed as Q<sub>10</sub>) of the model impulse responses increased as a function of centre frequency, while the temporal envelope decreased (<xref ref-type="fig" rid="F4">Fig 4</xref>) (<xref ref-type="bibr" rid="R14">Evans, 1975</xref>; <xref ref-type="bibr" rid="R40">Moore, 1986</xref>; <xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>). Additionally, the model exhibited a dependence of the polarity of the frequency glides of the impulse responses on centre frequency, as seen in the real auditory nerve (<xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>). In contrast to the temporal prediction model, we found that a simple autoencoder failed to reproduce many of these cochlear features. Together, these findings suggest that optimization for temporal prediction on natural sound waveforms can account for many aspects of cochlear sound processing.</p><sec id="S8"><title>Factors important for the model</title><p id="P22">The addition of Gaussian noise to the input at ~3-15 dB SNR proved to be needed for the model to replicate cochlear features (<xref ref-type="fig" rid="F5">Fig 5</xref> and <xref ref-type="fig" rid="F6">6</xref>). We investigated three technical reasons that might explain why the noise was needed: regularization, data augmentation, and avoiding local minima or saddle points. The addition of noise to network input can act as regularization that reduces overfitting and helps with generalization of a model (<xref ref-type="bibr" rid="R54">Sietsma and Dow, 1991</xref>). It has been shown that, under certain assumptions, adding noise is equivalent to Tikhonov regularization (a generalization of L2 regularization) when the noise amplitude is small (<xref ref-type="bibr" rid="R5">Bishop, 1995</xref>, <xref ref-type="bibr" rid="R4">2006</xref>). If this is the reason why noise was beneficial in our model, we would expect noise to improve prediction performance on a held-out noiseless dataset. This was not the case (<xref ref-type="fig" rid="F5">Fig. 5G</xref>), however, suggesting that noise is not acting as a regularizer.</p><p id="P23">Another possibility is that the addition of noise approximates having a larger training set; addition of noise to a training set is often used as a type of data augmentation in machine learning (<xref ref-type="bibr" rid="R19">Goodfellow et al., 2016</xref>). If this were the case in our model, we might expect that a larger, noiseless data set would produce similar results to those found for the dataset with noise added. This was not the case (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2E-F</xref>). Finally, noise can help during gradient descent to avoid local minima and saddle points. To test whether this was the case in our model, we took the model trained with noise and then trained it further on the same dataset but without noise. If the noiseless model was finding a poor local minimum or saddle point, then pre-training with noisy data should avoid this. However, even with pre-training (which produced gammatone-like impulse responses), subsequent training on noise-free data resulted in a model with non-gammatone impulse responses. Thus, while our exploration of this question is not exhaustive, these results suggest that the noise does not seem to be simply enabling better training of the network.</p><p id="P24">This leaves open the possibility that the noise we added mimics some form of noise that is unavoidably present in the middle or inner ear or in the environment. Thermal noise may induce stochasticity at the middle ear (<xref ref-type="bibr" rid="R24">Harrison, 2009</xref>), basilar membrane (<xref ref-type="bibr" rid="R44">Nuttall et al., 1997</xref>) and hair cell stereocilia (<xref ref-type="bibr" rid="R32">Kozlov et al., 2012</xref>). Furthermore, noise from cardiovascular and respiratory muscle activity (<xref ref-type="bibr" rid="R51">Ren et al., 1995</xref>; <xref ref-type="bibr" rid="R44">Nuttall et al., 1997</xref>) and other muscle activity, such as chewing (<xref ref-type="bibr" rid="R1">Bárány, 1938</xref>; <xref ref-type="bibr" rid="R28">Huxley, 1990</xref>), may also have an impact at low sound frequencies (<xref ref-type="bibr" rid="R44">Nuttall et al., 1997</xref>; Kirk and Smith, 2003). There is also substantial stochasticity in the vesicle release process of the inner hair cell ribbon synapse (<xref ref-type="bibr" rid="R26">Heil and Peterson, 2017</xref>). Given the relative quietness of many natural environments (Kirk and Smith, 2003), this internally-generated noise may be of relevance. However, the presence of externallygenerated nuisance noise, including wind (<xref ref-type="bibr" rid="R11">Chung, 2012</xref>), reverberation (Traer et al. 2016) and other background natural sound textures, such a rain or flowing water (McDermott et al. 2011; Mishra et al. 2021), may also have influenced the evolution of the auditory system. Hence, it may be that the cochlea is optimized for temporal prediction of natural sounds in moderately noisy backgrounds with noisy biological mechanisms. This is supported by the fact that training in the 9-15d B SNR range appears to allow reasonably good prediction of the test set over a wide range of SNRs (from no noise to 3 dB SNR) (<xref ref-type="fig" rid="F5">Fig. 5H</xref>).</p><p id="P25">Our results also suggest that compression of the sound waveform is not sufficient to account for cochlear tuning. An autoencoding, compressive network trained to recreate its natural sound waveform input did not emulate cochlear features well, as the autoencoder did not produce tightly-tuned, appropriately asymmetric, and exponentially-distributed impulse responses. This suggests that the cochlear features produced by temporal prediction are not solely due to compression and are a consequence of encoding the predictive features of the natural soundscape.</p></sec><sec id="S9"><title>Comparison to other models</title><p id="P26">Models of sensory coding have previously been based on several other normative principles, one being efficient coding (<xref ref-type="bibr" rid="R2">Barlow, 1959</xref>). Efficient coding postulates that sensory systems are optimized to represent natural stimuli by minimizing redundant activity between transmitting neurons (<xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>). Cochlear models optimized to efficiently represent natural sound waveforms have been shown to produce filters that resemble the impulse response of auditory nerve fibres (<xref ref-type="bibr" rid="R35">Lewicki, 2002</xref>; <xref ref-type="bibr" rid="R57">Smith and Lewicki, 2006</xref>). However, the efficient coding approach used by <xref ref-type="bibr" rid="R35">Lewicki (2002)</xref> did not capture the asymmetry of biological impulse responses. The encoding algorithm of <xref ref-type="bibr" rid="R57">Smith and Lewicki (2006)</xref> did produce asymmetric impulse responses, but is also limited in its biological plausibility by producing temporally-sparse spikes, each with a scalar value, rather than binary spikes (as in the auditory nerve) or a continuous real number (as with the inner hair cell membrane potential). Also, while this encoding algorithm produces spikes sequentially over time, it treated time as a dimension viewed all at once, with the timing of past spikes being able to depend on those of future spikes, something inconsistent with biology. Our model has addressed some of these shortcomings by using temporal prediction as a governing principle. The impulse responses from the temporal prediction model capture the temporal asymmetry of cochlear filters, and its encoding method operates sequentially over time, without the need for an acausal iterative settling process to produce the model output.</p></sec><sec id="S10"><title>Arguments for temporal prediction in the cochlea</title><p id="P27">Evolution by natural selection maximizes reproductive success (<xref ref-type="bibr" rid="R20">Grafen, 2014</xref>; <xref ref-type="bibr" rid="R34">Levin and Grafen, 2019</xref>). However, individual physiological systems of an organism can be seen as being approximately optimized for some more specialized objective or set of objectives (subject, of course, to certain constraints) that serves this ultimate objective. There are several arguments as to why the functional properties of sensory structures, such as the cochlea, may be governed by temporal prediction. First, efficient temporal prediction may induce encoding of ‘underlying’ features behind the stimuli (<xref ref-type="bibr" rid="R3">Bialek et al., 2001</xref>) – the requirement to accurately predict future input should help to establish an accurate neural model of the external world. Second, a sensory representation optimized for temporal prediction will disregard information in the input stimulus that is not predictive of the future, and therefore arguably superfluous. This can provide an initial filter to restrict the amount of information the brain has to deal with, since gathering and transmission of unnecessary information is energetically costly (<xref ref-type="bibr" rid="R38">Marzen and DeDeo, 2017</xref>). Third, sensory processing guides action, rendering the future sensory environment highly relevant; optimization for temporal prediction should enable more accurate and faster future actions, which can be critically important in a survival context. Finally, an empirical argument for temporal prediction comes from its capacity to account for the temporal and spectral characteristics of neuronal receptive fields found in diverse sensory regions and modalities, including the retina and the visual and auditory cortex (<xref ref-type="bibr" rid="R49">Palmer et al., 2015</xref>; <xref ref-type="bibr" rid="R55">Singer et al., 2018</xref>, <xref ref-type="bibr" rid="R56">2019</xref>), in addition to the cochlea.</p></sec><sec id="S11"><title>Limitations and future development of the model</title><p id="P28">The temporal prediction model captures some important physiological features of the cochlea, but has certain limitations. The rise time of the model impulse responses is somewhat faster than that measured for the auditory nerve, and some of those responses show an initial click that is not seen in the experimental data. Also, the cochlea exhibits nonlinear properties that our model, by its design, could not capture. One example is two-tone suppression, where the simultaneous presentation of two tones with nearby frequencies nonlinearly attenuates basilar membrane responses (<xref ref-type="bibr" rid="R52">Robles and Ruggero, 2001</xref>). Potential developments of the model that could help capture these properties include increasing its complexity and nonlinearity to more fully enable reflection of the biophysical properties of the cochlea. In particular, adopting a recurrent network model, rather than the purely feedforward version used here, should better reflect the mechanical interactions that take place along the basilar membrane and with the outer hair cells and surrounding cochlear fluids. Furthermore, incorporating an additional layer to potentially capture the integration and adaptation at inner hair cells, auditory nerve fibers and the intervening ribbon synapses (Eatock 2000; Zilany et al., 2009; <xref ref-type="bibr" rid="R42">Moser and Beutner, 2000</xref>; Raman et al., 1994; Wen et al., 2012) would represent another important extension, as would introducing longer input windows and higher sampling rates in order to extend the frequency range that can be explored.</p></sec></sec><sec id="S12" sec-type="methods"><title>Methods</title><sec id="S13"><title>Auditory dataset and preprocessing</title><p id="P29">We used a dataset composed of natural sounds: birdsong and other animal calls, environmental sounds, and human speech by children and adults as well as baby vocalizations. Sound recordings of birdsong and mammal and insect calls were drawn from the Macauley Library at the Cornell Lab of Ornithology<sup><xref ref-type="fn" rid="FN1">1</xref></sup> (<ext-link ext-link-type="uri" xlink:href="https://www.macaulaylibrary.org/">https://www.macaulaylibrary.org/</ext-link>), the British Library Sounds Environment &amp; Nature Collection (<ext-link ext-link-type="uri" xlink:href="https://sounds.bl.uk/Environment">https://sounds.bl.uk/Environment</ext-link>) and <ext-link ext-link-type="uri" xlink:href="https://animal-sounds.org/farm-animal-sounds.html">https://animal-sounds.org/farm-animal-sounds.html</ext-link>. We also used a corpus of ferret vocalizations recorded in our laboratory (by Kerry Walker, Oxford Auditory Neuroscience Group) and an additional recording from <ext-link ext-link-type="uri" xlink:href="https://freesound.org/people/J.Zazvurek/sounds/155115/">https://freesound.org/people/J.Zazvurek/sounds/155115/</ext-link>. Recordings of adult human speech were taken from <ext-link ext-link-type="uri" xlink:href="http://databases.forensic-voice-comparison.net/">http://databases.forensic-voice-comparison.net/</ext-link> (<xref ref-type="bibr" rid="R41">Morrison et al., 2012</xref>). We also used some human speech and environmental sounds, such as snapping twigs which were recorded in our laboratory in an anechoic chamber. Recordings of children speaking were from the CHILDES database (<xref ref-type="bibr" rid="R37">MacWhinney, 2000</xref>; <xref ref-type="bibr" rid="R12">Edwards and Beckman, 2008</xref>) from TalkBank (<ext-link ext-link-type="uri" xlink:href="https://talkbank.org">talkbank.org</ext-link>). Baby vocalizations were from the OxVoc database (<xref ref-type="bibr" rid="R50">Parsons et al., 2014</xref>).</p><p id="P30">Each recording was resampled to 22,050 Hz and convolved with a filter that mimics the transformation of raw sound by the middle ear. This filter was provided by the Python package “Brian Hears” (<xref ref-type="bibr" rid="R16">Fontaine et al., 2011</xref>). To lessen edge effects, we also applied high- and low-pass filters (5<sup>th</sup>-order Butterworth) with cut-off frequencies of 500 Hz and 5,512.5 Hz, respectively. The high-pass filter was applied because the time window of the network input limited the capacity to capture frequencies with a period greater than this time window. The high-pass filter provided a smooth fall-off in power near this cut-off to avoid potential edge effects. The low-pass filter was applied to provide a smooth fall-off near the Nyquist frequency to avoid potential edge effects and because auditory nerve fibers do not phase lock at high sound frequencies.</p><p id="P31">We then divided each of the sound clips into consecutive snippets of 6.53ms (144 samples) duration. We excluded sound snippets with a root mean square value below a threshold (0.105) to avoid silent sound clips as training input. Each individual sound snippet was then normalized have a standard deviation of one. Finally, we added Gaussian noise to the input of the sound snippets at an SNR of 9 dB typically, but different values were also explored. For training and testing the temporal prediction and autoencoder models, the selected snippets were divided into a training set of <italic>N</italic> = 2,038,187 snippets, and a test set of 164,193 snippets.</p><p id="P32">For <xref ref-type="supplementary-material" rid="SD1">Supplemental Fig. 2G-H</xref>, we used a larger dataset where we selected snippets from the same clips as before, but the snippets overlapped, with the starting points of the 6.53 ms long snippets spaced by 0.65 ms. Otherwise pre-processing of the larger dataset was exactly the same as for the standard dataset.</p></sec><sec id="S14"><title>The cochlear temporal prediction model</title><p id="P33">The cochlear temporal prediction model was trained to predict the future of sound snippets from their past. The model consisted of a feedforward artificial neural network with one hidden layer with a nonlinear activation function. It had <italic>i</italic> = 1 to <italic>I</italic> = 128 input units, one for each time step of the input, <italic>j</italic> = 1 to <italic>J</italic> = 64 hidden units, and <italic>k</italic> = 1 to <italic>K</italic> = 16 output units, one for each time step of the target.</p><p id="P34">The <italic>N</italic> = 2,038,187 sound snippets of the training set were used to train the model. Each snippet, <italic>n</italic>, was <italic>I</italic>+<italic>K</italic> = 144 time steps (samples) long. The first <italic>I</italic> = 128 time steps (5.80 ms) of a snippet were taken as the past and provided the input unit activity <italic>u</italic><sub><italic>in</italic></sub> to the model, indexed from <italic>i</italic> = 1 to <italic>I</italic>. The last <italic>K</italic> = 16 time steps (0.73 ms) of a snippet were taken as the future and provided the target values <italic>y</italic><sub><italic>kn</italic></sub> that the model aimed to predict, indexed from <italic>k</italic> = 1 to <italic>K</italic>.</p><p id="P35">The activation <italic>x</italic><sub><italic>jn</italic></sub> of each hidden unit of the model was a linear mapping from the input unit activity, and was given by, <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>b</italic><sub><italic>j</italic></sub> is the bias value for hidden unit <italic>j, w</italic><sub><italic>ji</italic></sub> is the weight from input unit <italic>i</italic> to hidden unit <italic>j</italic>, and <italic>u</italic><sub><italic>in</italic></sub> is the activity of input unit <italic>i</italic> for snippet <italic>n</italic>.</p><p id="P36">The activity <italic>a</italic><sub><italic>jn</italic></sub> of hidden unit <italic>j</italic> for training example <italic>n</italic> was then given by applying an activation function to the hidden unit’s activation, <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>g</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula> where the scaled hyperbolic tangent g(<italic>x</italic>) = tanh (<italic>αx</italic>)<italic>β</italic> was used as the activation function of the hidden units. Scaling factors <italic>α</italic> and <italic>β</italic> were set to the standard values of 2/3 and 1.7159, respectively (<xref ref-type="bibr" rid="R33">LeCun et al., 1998</xref>).</p><p id="P37">The model’s prediction, <italic>v</italic><sub><italic>kn</italic></sub>, of the target, <italic>y</italic><sub><italic>kn</italic></sub> (the future), was provided by a linear mapping from the hidden unit activity, and was given by <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>w</italic><sub><italic>kj</italic></sub> is the weight from hidden unit <italic>j</italic> to output unit <italic>k</italic>, and <italic>b</italic><sub><italic>k</italic></sub> is the bias for output unit <italic>k</italic>.</p><p id="P38">The trainable parameters, <italic>w</italic><sub><italic>ji</italic></sub>, <italic>b</italic><sub><italic>j</italic></sub>, <italic>w</italic><sub><italic>kj</italic></sub> and <italic>b</italic><sub><italic>k</italic></sub>, were optimized to perform temporal prediction by minimizing the objective function <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>N</mml:mi><mml:mi>K</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>K</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>N</italic> is the total amount of training snippets, and <italic>y<sub>kn</sub></italic> is the target value for output unit <italic>k</italic> for training example <italic>n</italic>. The optimization was performed using stochastic gradient descent on mini-batches of 100 snippets and the update function Adaptive Moment Estimation (<xref ref-type="bibr" rid="R31">Kingma and Ba, 2015</xref>).</p></sec><sec id="S15"><title>The cochlear autoencoder model</title><p id="P39">The cochlear autoencoder model was exactly the same as the cochlear temporal prediction model, except that it has <italic>K</italic> = <italic>I</italic> = 128 output units and that it aimed to estimate its own input. Instead of the target being y<sub><italic>kn</italic></sub>, the future of a snippet, it was <italic>u</italic><sub><italic>kn</italic></sub>, the past of a snippet, which is the same as <italic>u</italic><sub><italic>in</italic></sub>, the input to the model.</p></sec><sec id="S16"><title>Impulse response analysis</title><p id="P40">Hidden unit impulse responses with a power value of less than 1% of the mean impulse response power for all hidden units in the models were removed from analysis. The impulse response characteristics were found by determining the spectrum using the Fast Fourier Transform. The center frequency was defined as the frequency bin containing the spectral peak for each impulse response. The bandwidth was defined as the frequency span within a 10 dB drop on either side of the center frequency of the impulse response. Only units with a drop on both sides of the spectral peak were included in the impulse response bandwidth analysis. The temporal envelope duration of each impulse response was interpreted as being the shortest temporal span that covers 95% of the total impulse response power.</p><p id="P41">The frequency glide slopes were measured by the zero-crossing method of <xref ref-type="bibr" rid="R8">Carney et al. (1999)</xref>, using methods very similar to theirs (see <xref ref-type="supplementary-material" rid="SD1">Supplemental Figure 1</xref>). First, the impulse response was smoothed. To do this, the first sample of the impulse response was excluded (due to its click characteristics) and the impulse response was padded with 5.8 ms of zeros on either side. This was then filtered forwards and backward in time with a 4<sup>th</sup> order Butterworth bandpass filter, and the section corresponding to the impulse response extracted. The net result of this process produced the impulse response filtered by an 8<sup>th</sup> order bandpass filter with zero phase, and hence with no introduced time delay. The bandwidth of the 4<sup>th</sup> order Butterworth filter was one octave, centered at the unit’s center frequency. The envelope of the filtered impulse response was measured by taking its Hilbert transform and a 3-point average, and the time range for which the amplitude of the envelope was within 12 dB of the peak was found. Next, the mean (DC) value was calculated for a span that was the time range rounded down to an integer number of cycles of the centre frequency, centered at the midpoint of the range. This DC value was subtracted from the impulse response. The impulse response was then linearly interpolated by a factor of 1000, any zero crossings within the time range were found, and the time difference between consecutive zero crossings determined. The reciprocal of this time difference is the instantaneous frequency at the midpoint of the two timepoints. The instantaneous frequency was plotted as a function of these midpoints, a straight line was fitted to this plot by least squares, and its slope was measured. This was done for each impulse response that included at least two periods within the 5.8 ms span of the impulse response and which had a center frequency of less than 4.6 kHz, since this was the highest characteristic frequency of the auditory nerve fibers recorded by <xref ref-type="bibr" rid="R8">Carney et al. (1999)</xref>. An additional reason for limiting this frequency range is that the zero-crossing method of measuring instantaneous frequency is inaccurate above half the Nyquist frequency, 5.5 kHz (<xref ref-type="bibr" rid="R27">Hoeks et al., 1984</xref>).</p></sec><sec id="S17"><title>Computational implementation</title><p id="P42">Custom code was written in Python for data handling, modelling and data analysis, and run on NVIDIA GeForce GTX-1080 GPUs.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Figures</label><media xlink:href="EMS189014-supplement-Supplemental_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d71aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S18"><title>Acknowledgements</title><p>We thank Laurel Carney for the data in <xref ref-type="fig" rid="F3">Figure 3B</xref>. We are grateful for the funding provided by a Clarendon Fund Graduate Scholarship to FT and a Wellcome Principal Fellowship to AJK (WT108369/Z/2015/Z).</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P43">Specifically, we use the following recordings from the Macauley Library: ML3429, ML3891, ML53181, ML55348, ML55350, ML59291, ML61494, ML62970, ML67895, ML70539, ML72995, ML80731, ML100723, ML100797, ML100857, ML111595, ML116302, ML116303, ML126289, ML129248, ML130909, ML132511, ML132526, ML136504, Ml164696, ML171372, ML191165, ML191178, ML191290, ML197064, ML199078, ML201215, ML205481, ML205774, ML207517, ML206448, ML207181, ML210645, ML213055, ML213356, ML217313, ML217490, ML220049, ML233387, ML516692, ML527184 and ML527292.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bárány</surname><given-names>E</given-names></name></person-group><article-title>A Contribution to the Physiology of Bone Conduction</article-title><source>Acta Oto-laryngolog</source><year>1938</year><volume>16</volume><issue>Suppl 26</issue><fpage>1</fpage><lpage>223</lpage></element-citation></ref><ref id="R2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><chapter-title>The coding of sensory messages</chapter-title><person-group person-group-type="editor"><name><surname>Thorpe</surname><given-names>WH</given-names></name><name><surname>Zangwill</surname><given-names>OL</given-names></name></person-group><source>Current Problems in Animal Behaviour</source><publisher-name>Cambridge University Press</publisher-name><publisher-loc>Cambridge</publisher-loc><year>1959</year><volume>1961</volume><fpage>331</fpage><lpage>360</lpage></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>Nemenman</surname><given-names>I</given-names></name><name><surname>Tishby</surname><given-names>N</given-names></name></person-group><article-title>Predictability, complexity, and learning</article-title><source>Neural Comput</source><year>2001</year><volume>13</volume><fpage>2409</fpage><lpage>2463</lpage><pub-id pub-id-type="pmid">11674845</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>C</given-names></name></person-group><source>Pattern Recognition and Machine Learning</source><publisher-name>Springer-Verlag</publisher-name><publisher-loc>New York</publisher-loc><year>2006</year><edition>1st ed</edition></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name></person-group><article-title>Training with noise is equivalent to tikhonov regularization</article-title><source>Neural Comput</source><year>1995</year><volume>7</volume><fpage>108</fpage><lpage>116</lpage></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlin</surname><given-names>MA</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name></person-group><article-title>Sustained Firing of Model Central Auditory Neurons Yields a Discriminative Spectro-temporal Representation for Natural Sounds</article-title><source>PLoS Comput Biol</source><year>2013</year><volume>9</volume><elocation-id>e1002982</elocation-id><pub-id pub-id-type="pmcid">PMC3610626</pub-id><pub-id pub-id-type="pmid">23555217</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002982</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>NL</given-names></name><name><surname>Ming</surname><given-names>VL</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name></person-group><article-title>Sparse codes for speech predict spectrotemporal receptive fields in the inferior colliculus</article-title><source>PLoS Comput Biol</source><year>2012</year><volume>8</volume><elocation-id>e1002594</elocation-id><pub-id pub-id-type="pmcid">PMC3395612</pub-id><pub-id pub-id-type="pmid">22807665</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002594</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carney</surname><given-names>LH</given-names></name><name><surname>McDuffy</surname><given-names>MJ</given-names></name><name><surname>Shekhter</surname><given-names>I</given-names></name></person-group><article-title>Frequency glides in the impulse responses of auditory-nerve fibers</article-title><source>J Acoust Soc Am</source><year>1999</year><volume>105</volume><fpage>2384</fpage><lpage>2391</lpage><pub-id pub-id-type="pmid">10212419</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chalk</surname><given-names>M</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><article-title>Toward a unified theory of efficient, predictive, and sparse coding</article-title><source>Proc Natl Acad Sci U S A</source><year>2018</year><volume>115</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="pmcid">PMC5776796</pub-id><pub-id pub-id-type="pmid">29259111</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1711114115</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Christopher Kirk</surname><given-names>E</given-names></name><name><surname>Smith</surname><given-names>DW</given-names></name></person-group><article-title>Protection from acoustic trauma is not a primary function of the medial olivocochlear efferent system</article-title><source>J Assoc Res Otolaryngol</source><year>2003</year><volume>4</volume><fpage>445</fpage><lpage>465</lpage><pub-id pub-id-type="pmcid">PMC3202749</pub-id><pub-id pub-id-type="pmid">12784134</pub-id><pub-id pub-id-type="doi">10.1007/s10162-002-3013-y</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>K</given-names></name></person-group><article-title>Comparisons of spectral characteristics of wind noise between omnidirectional and directional microphones</article-title><source>J Acoust Soc Am</source><year>2012</year><volume>131</volume><fpage>4508</fpage><lpage>4517</lpage><pub-id pub-id-type="pmid">22712924</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edwards</surname><given-names>J</given-names></name><name><surname>Beckman</surname><given-names>ME</given-names></name></person-group><article-title>Methodological questions in studying consonant acquisition</article-title><source>Clin Linguist Phonetics</source><year>2008</year><volume>22</volume><fpage>937</fpage><lpage>956</lpage><pub-id pub-id-type="pmcid">PMC2728799</pub-id><pub-id pub-id-type="pmid">19031192</pub-id><pub-id pub-id-type="doi">10.1080/02699200802330223</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eliasmith</surname><given-names>C</given-names></name><name><surname>Anderson</surname><given-names>CH</given-names></name></person-group><source>Neural Engineering: Computation, Representation, and Dynamics in Neurobiological Systems</source><publisher-name>The MIT Press</publisher-name><year>2002</year></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>EF</given-names></name></person-group><article-title>The sharpening of cochlear frequency selectivity in the normal and abnormal cochlea</article-title><source>Int J Audiol</source><year>1975</year><volume>14</volume><fpage>419</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">1156249</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fettiplace</surname><given-names>R</given-names></name></person-group><article-title>Hair cell transduction, tuning, and synaptic transmission in the mammalian cochlea</article-title><source>Compr Physiol</source><year>2017</year><volume>7</volume><fpage>1197</fpage><lpage>1227</lpage><pub-id pub-id-type="pmcid">PMC5658794</pub-id><pub-id pub-id-type="pmid">28915323</pub-id><pub-id pub-id-type="doi">10.1002/cphy.c160049</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fontaine</surname><given-names>B</given-names></name><name><surname>Goodman</surname><given-names>DFM</given-names></name><name><surname>Benichoux</surname><given-names>V</given-names></name><name><surname>Brette</surname><given-names>R</given-names></name></person-group><article-title>Brian hears: online auditory processing using vectorization over channels</article-title><source>Front Neuroinform</source><year>2011</year><volume>5</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC3143729</pub-id><pub-id pub-id-type="pmid">21811453</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00009</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedman</surname><given-names>JH</given-names></name><name><surname>Rafsky</surname><given-names>LC</given-names></name></person-group><article-title>Multivariate generalizations of the Wald-Wolfowitz and Smirnov two-sample tests</article-title><source>Ann Stat</source><year>1979</year><volume>7</volume><fpage>697</fpage><lpage>717</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goblick</surname><given-names>TJ</given-names></name><name><surname>Pfeiffer</surname><given-names>RR</given-names></name></person-group><article-title>Time-domain measurements of cochlear nonlinearities using combination click stimuli</article-title><source>J Acoust Soc Am</source><year>1969</year><volume>46</volume><fpage>924</fpage><lpage>938</lpage><pub-id pub-id-type="pmid">4309951</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><source>Deep Learning</source><publisher-name>The MIT Press</publisher-name><year>2016</year></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grafen</surname><given-names>A</given-names></name></person-group><article-title>The formal darwinism project in outline</article-title><source>Biol Philos</source><year>2014</year><volume>29</volume><fpage>155</fpage><lpage>174</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenwood</surname><given-names>DD</given-names></name></person-group><article-title>Critical bandwidth and the frequency coordinates of the basilar membrane</article-title><source>J Acoust Soc Am</source><year>1961</year><volume>33</volume><fpage>1344</fpage><lpage>1356</lpage></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenwood</surname><given-names>DD</given-names></name></person-group><article-title>A cochlear frequency-position function for several species—29 years later</article-title><source>J Acoust Soc Am</source><year>1990</year><volume>87</volume><fpage>2592</fpage><lpage>2605</lpage><pub-id pub-id-type="pmid">2373794</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greenwood</surname><given-names>DD</given-names></name></person-group><article-title>Comparing octaves, frequency ranges, and cochlear-map curvature across species</article-title><source>Hear Res</source><year>1996</year><volume>94</volume><fpage>157</fpage><lpage>162</lpage><pub-id pub-id-type="pmid">8789821</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>MJ</given-names></name></person-group><source>Sivian and White revisited: the role of resonant thermal noise pressure on the eardrum in auditory thresholds</source><year>2009</year><comment>Available at :<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/0910.3170">https://arxiv.org/abs/0910.3170</ext-link></comment></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>J</given-names></name><name><surname>Ruderman</surname><given-names>D</given-names></name></person-group><article-title>Independent component analysis of natural image sequences yields spatio-temporal filters similar to simple cells in primary visual cortex</article-title><source>Proc R Soc Lond B</source><year>1998</year><elocation-id>2652315–2320</elocation-id><pub-id pub-id-type="pmcid">PMC1689525</pub-id><pub-id pub-id-type="pmid">9881476</pub-id><pub-id pub-id-type="doi">10.1098/rspb.1998.0577</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heil</surname><given-names>P</given-names></name><name><surname>Peterson</surname><given-names>AJ</given-names></name></person-group><article-title>Spike timing in auditory-nerve fibers during spontaneous activity and phase locking</article-title><source>Synapse</source><year>2017</year><volume>71</volume><fpage>5</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">27466786</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoeks</surname><given-names>APG</given-names></name><name><surname>Peeters</surname><given-names>HHPM</given-names></name><name><surname>Ruissen</surname><given-names>CJ</given-names></name><name><surname>Reneman</surname><given-names>RS</given-names></name></person-group><article-title>A Novel Frequency Estimator for Sampled Doppler Signals</article-title><source>IEEE Trans Biomed Eng</source><year>1984</year><fpage>212</fpage><lpage>220</lpage><comment>BME-31</comment><pub-id pub-id-type="pmid">6706350</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huxley</surname><given-names>A</given-names></name></person-group><article-title>Bone-conducted sound</article-title><source>Nature</source><year>1990</year><volume>343</volume><fpage>28</fpage><pub-id pub-id-type="pmid">2296287</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvärinen</surname><given-names>A</given-names></name><name><surname>Hurri</surname><given-names>J</given-names></name><name><surname>Väyrynen</surname><given-names>J</given-names></name></person-group><article-title>A unifying framework for natural image statistics: Spatiotemporal activity bubbles</article-title><source>Neurocomputing</source><year>2004</year><volume>58–60</volume><fpage>801</fpage><lpage>806</lpage></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname><given-names>PX</given-names></name><name><surname>Bergevin</surname><given-names>C</given-names></name><name><surname>Kalluri</surname><given-names>R</given-names></name><name><surname>Laughlin</surname><given-names>Mc</given-names></name><name><surname>Michelet</surname><given-names>P</given-names></name><name><surname>van der Heijden</surname><given-names>M</given-names></name><name><surname>Shera</surname><given-names>CA</given-names></name></person-group><article-title>Frequency selectivity in Old-World monkeys corroborates sharp cochlear tuning in humans</article-title><source>Proc Natl Acad Sci U S A</source><year>2011</year><volume>108</volume><fpage>17516</fpage><lpage>17520</lpage><pub-id pub-id-type="pmcid">PMC3198376</pub-id><pub-id pub-id-type="pmid">21987783</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1105867108</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><source>Adam: A method for stochastic optimization</source><conf-name>Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015)</conf-name><year>2015</year><fpage>1</fpage><lpage>15</lpage></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozlov</surname><given-names>AS</given-names></name><name><surname>Risler</surname><given-names>T</given-names></name><name><surname>Hinterwirth</surname><given-names>AJ</given-names></name><name><surname>Hudspeth</surname><given-names>AJ</given-names></name></person-group><article-title>Relative stereociliary motion in a hair bundle opposes amplification at distortion frequencies</article-title><source>J Physiol</source><year>2012</year><volume>590</volume><fpage>301</fpage><lpage>308</lpage><pub-id pub-id-type="pmcid">PMC3285066</pub-id><pub-id pub-id-type="pmid">22124150</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.2011.218362</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Orr</surname><given-names>GB</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name></person-group><chapter-title>Efficient BackProp</chapter-title><source>Neural Networks: Tricks of the Trade</source><publisher-name>Springer</publisher-name><publisher-loc>Berlin, Heidelberg</publisher-loc><year>1998</year><fpage>9</fpage><lpage>50</lpage></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levin</surname><given-names>SR</given-names></name><name><surname>Grafen</surname><given-names>A</given-names></name></person-group><article-title>Inclusive fitness is an indispensable approximation for understanding organismal design</article-title><source>Evolution (N Y)</source><year>2019</year><volume>73</volume><fpage>1066</fpage><lpage>1076</lpage><pub-id pub-id-type="pmcid">PMC6593845</pub-id><pub-id pub-id-type="pmid">30993671</pub-id><pub-id pub-id-type="doi">10.1111/evo.13739</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><article-title>Efficient coding of natural sounds</article-title><source>Nat Neurosci</source><year>2002</year><volume>5</volume><fpage>356</fpage><lpage>363</lpage><pub-id pub-id-type="pmid">11896400</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liberman</surname><given-names>MC</given-names></name></person-group><article-title>The cochlear frequency map for the cat: Labeling auditory-nerve fibers of known characteristic frequency</article-title><source>J Acoust Soc Am</source><year>1982</year><volume>72</volume><fpage>1441</fpage><lpage>1449</lpage><pub-id pub-id-type="pmid">7175031</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacWhinney</surname><given-names>B</given-names></name></person-group><source>The CHILDES Project: Tools for analyzing talk</source><publisher-name>NJ: Lawrence Erlbaum Associates</publisher-name><publisher-loc>Mahwah</publisher-loc><year>2000</year><edition>3rd ed</edition></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marzen</surname><given-names>SE</given-names></name><name><surname>DeDeo</surname><given-names>S</given-names></name></person-group><article-title>The evolution of lossy compression</article-title><source>J R Soc Interface</source><year>2017</year><volume>14</volume><elocation-id>20170166</elocation-id><pub-id pub-id-type="pmcid">PMC5454305</pub-id><pub-id pub-id-type="pmid">28490604</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2017.0166</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monaco</surname><given-names>JV</given-names></name></person-group><article-title>Classification and authentication of one-dimensional behavioral biometrics</article-title><source>IJCB 2014 - 2014 IEEE/IAPR Int Jt Conf Biometrics</source><year>2014</year></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>BCJ</given-names></name></person-group><article-title>Parallels betwen frequency selectivity measured psychophysically and in cochlear mechanics</article-title><source>Scand Audiol</source><year>1986</year><volume>15</volume><fpage>139</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">3472318</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname><given-names>GS</given-names></name><name><surname>Rose</surname><given-names>P</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>Protocol for the collection of databases of recordings for forensic-voice-comparison research and practice</article-title><source>Aust J Forensic Sci</source><year>2012</year><volume>44</volume><fpage>155</fpage><lpage>167</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>T</given-names></name><name><surname>Beutner</surname><given-names>D</given-names></name></person-group><article-title>Kinetics of exocytosis and endocytosis at the cochlear inner hair cell afferent synapse of the mouse</article-title><source>Proc Natl Acad Sci U S A</source><year>2000</year><volume>97</volume><fpage>883</fpage><lpage>888</lpage><pub-id pub-id-type="pmcid">PMC15425</pub-id><pub-id pub-id-type="pmid">10639174</pub-id><pub-id pub-id-type="doi">10.1073/pnas.97.2.883</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narayan</surname><given-names>SS</given-names></name><name><surname>Temchin</surname><given-names>AN</given-names></name><name><surname>Recio</surname><given-names>A</given-names></name><name><surname>Ruggero</surname><given-names>MA</given-names></name></person-group><article-title>Frequency tuning of basilar membrane and auditory nerve fibers in the same cochleae</article-title><source>Science</source><year>1998</year><volume>282</volume><fpage>1882</fpage><lpage>1884</lpage><pub-id pub-id-type="pmcid">PMC3578392</pub-id><pub-id pub-id-type="pmid">9836636</pub-id><pub-id pub-id-type="doi">10.1126/science.282.5395.1882</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nuttall</surname><given-names>AL</given-names></name><name><surname>Menhe</surname><given-names>G</given-names></name><name><surname>Tianying</surname><given-names>R</given-names></name><name><surname>Dolan</surname><given-names>DF</given-names></name></person-group><article-title>Basilar membrane velocity noise</article-title><source>Hear Res</source><year>1997</year><volume>114</volume><fpage>35</fpage><lpage>42</lpage><pub-id pub-id-type="pmid">9447916</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><year>1996</year><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="pmid">8637596</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><article-title>Sparse coding with an overcomplete basis set: A strategy employed by V1?</article-title><source>Vision Res</source><year>1997</year><volume>37</volume><fpage>3311</fpage><lpage>3325</lpage><pub-id pub-id-type="pmid">9425546</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oxenham</surname><given-names>AJ</given-names></name><name><surname>Shera</surname><given-names>CA</given-names></name></person-group><article-title>Estimates of human cochlear tuning at low levels using forward and simultaneous masking</article-title><source>J Assoc Res Otolaryngol</source><year>2003</year><volume>4</volume><fpage>541</fpage><lpage>554</lpage><pub-id pub-id-type="pmcid">PMC3202745</pub-id><pub-id pub-id-type="pmid">14716510</pub-id><pub-id pub-id-type="doi">10.1007/s10162-002-3058-y</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>AR</given-names></name><name><surname>Russell</surname><given-names>IJ</given-names></name></person-group><article-title>Phase-locking in the cochlear nerve of the guinea-pig and its relation to the receptor potential of inner hair-cells</article-title><source>Hear Res</source><year>1986</year><volume>24</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmid">3759671</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>SE</given-names></name><name><surname>Marre</surname><given-names>O</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><article-title>Predictive information in a sensory population</article-title><source>Proc Natl Acad Sci U S A</source><year>2015</year><volume>112</volume><fpage>6908</fpage><lpage>6913</lpage><pub-id pub-id-type="pmcid">PMC4460449</pub-id><pub-id pub-id-type="pmid">26038544</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1506855112</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parsons</surname><given-names>CE</given-names></name><name><surname>Young</surname><given-names>KS</given-names></name><name><surname>Craske</surname><given-names>MG</given-names></name><name><surname>Stein</surname><given-names>AL</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name></person-group><article-title>Introducing the Oxford Vocal (OxVoc) Sounds database: A validated set of non-acted affective sounds from human infants, adults, and domestic animals</article-title><source>Front Psychol</source><year>2014</year><volume>5</volume><fpage>562</fpage><pub-id pub-id-type="pmcid">PMC4068198</pub-id><pub-id pub-id-type="pmid">25009511</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00562</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Nuttall</surname><given-names>AL</given-names></name><name><surname>Miller</surname><given-names>JM</given-names></name></person-group><article-title>Heart beat modulation of spontaneous otoacoustic emissions in guinea pig</article-title><source>Acta Otolaryngol</source><year>1995</year><volume>115</volume><fpage>725</fpage><lpage>731</lpage><pub-id pub-id-type="pmid">8749191</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robles</surname><given-names>L</given-names></name><name><surname>Ruggero</surname><given-names>MA</given-names></name></person-group><article-title>Mechanics of the mammalian cochlea</article-title><source>Physiol Rev</source><year>2001</year><volume>81</volume><fpage>1305</fpage><lpage>1352</lpage><pub-id pub-id-type="pmcid">PMC3590856</pub-id><pub-id pub-id-type="pmid">11427697</pub-id><pub-id pub-id-type="doi">10.1152/physrev.2001.81.3.1305</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salisbury</surname><given-names>JM</given-names></name><name><surname>Palmer</surname><given-names>SE</given-names></name></person-group><article-title>Optimal prediction in the retina and natural motion statistics</article-title><source>J Stat Phys</source><year>2016</year><volume>162</volume><fpage>1309</fpage><lpage>1323</lpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sietsma</surname><given-names>J</given-names></name><name><surname>Dow</surname><given-names>RJF</given-names></name></person-group><article-title>Creating artificial neural networks that generalize</article-title><source>Neural Networks</source><year>1991</year><volume>4</volume><fpage>67</fpage><lpage>79</lpage></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Teramoto</surname><given-names>Y</given-names></name><name><surname>Willmore</surname><given-names>BD</given-names></name><name><surname>Schnupp</surname><given-names>JW</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><article-title>Sensory cortex is optimized for prediction of future input</article-title><source>Elife</source><year>2018</year><volume>7</volume><elocation-id>e31557</elocation-id><pub-id pub-id-type="pmcid">PMC6108826</pub-id><pub-id pub-id-type="pmid">29911971</pub-id><pub-id pub-id-type="doi">10.7554/eLife.31557</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>Y</given-names></name><name><surname>Willmore</surname><given-names>BDB</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name></person-group><article-title>Hierarchical temporal prediction captures motion processing from retina to higher visual cortex</article-title><source>bioRxiv</source><year>2019</year><elocation-id>575464</elocation-id><pub-id pub-id-type="pmid">37844199</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>EC</given-names></name><name><surname>Lewicki</surname><given-names>MS</given-names></name></person-group><article-title>Efficient auditory coding</article-title><source>Nature</source><year>2006</year><volume>439</volume><fpage>978</fpage><lpage>982</lpage><pub-id pub-id-type="pmid">16495999</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sumner</surname><given-names>CJ</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name></person-group><article-title>Auditory nerve fibre responses in the ferret</article-title><source>Eur J Neurosci</source><year>2012</year><volume>36</volume><fpage>2428</fpage><lpage>39</lpage><pub-id pub-id-type="pmcid">PMC6485459</pub-id><pub-id pub-id-type="pmid">22694786</pub-id><pub-id pub-id-type="doi">10.1111/j.1460-9568.2012.08151.x</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>KM</given-names></name><name><surname>Gonzalez</surname><given-names>R</given-names></name><name><surname>Kang</surname><given-names>JZ</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><article-title>Across-species differences in pitch perception are consistent with differences in cochlear filtering</article-title><source>eLife</source><year>2019</year><volume>8</volume><elocation-id>e41626</elocation-id><pub-id pub-id-type="pmcid">PMC6435318</pub-id><pub-id pub-id-type="pmid">30874501</pub-id><pub-id pub-id-type="doi">10.7554/eLife.41626</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Schematic of the temporal prediction model of the cochlea.</title><p>A large dataset of natural sound waveforms (sampled at 22,050 Hz) was divided into snippets of 144 samples (6.5 ms) and presented to the network in random order. The first 128 samples (5.8 ms) of each snippet were provided as inputs to a feedforward neural network, and the network was optimized to perform temporal prediction by predicting the 16 future samples (0.73 ms) following the input samples. Each input unit, <italic>u</italic><sub><italic>i</italic></sub>, represents the waveform displacement at a particular input sample with index <italic>i</italic>, while each output unit, <italic>v</italic><sub><italic>k</italic></sub>, predicts the displacement at a future sample, with index <italic>k</italic>. The nonlinear hidden units are indexed by <italic>j</italic> and have activity <italic>a</italic><sub><italic>j</italic></sub>. The linear weights, <italic>w</italic><sub><italic>ji</italic></sub> and <italic>w</italic><sub><italic>kj</italic></sub>, and the biases, <italic>b</italic><sub><italic>j</italic></sub>, and <italic>b</italic><sub><italic>k</italic></sub>, were optimized to produce a prediction, <italic>v</italic><sub><italic>k</italic></sub>, of the future, <italic>y</italic><sub><italic>k</italic></sub>, given the past, <italic>u</italic><sub><italic>i</italic></sub>, over the dataset of natural sounds. The upper inset figure shows an example sound snippet representing the past, <italic>u</italic><sub><italic>i</italic></sub>, and the future, <italic>y</italic><sub><italic>k</italic></sub>, which served as input and target, respectively, for the temporal prediction objective. An example of the tuning properties learned by the model is shown in the lower inset figure. The incoming weights, <italic>w</italic><sub><italic>ji</italic></sub>, to hidden unit <italic>j</italic> can be interpreted as the impulse response of this hidden unit. Note for clarity we have dropped the snippet index <italic>n</italic> that we use in the Methods from <italic>u</italic><sub><italic>i</italic></sub>, <italic>a</italic><sub><italic>j</italic></sub>, <italic>v</italic><sub><italic>k</italic></sub> and <italic>y</italic><sub><italic>k</italic></sub>.</p></caption><graphic xlink:href="EMS189014-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>All impulse responses for the temporal prediction model.</title><p>The impulse responses of all 64 hidden units of the model sorted by center frequency, which was determined by the position of its peak value in frequency space. The impulse response for hidden unit <italic>j</italic> is the weight, <italic>w</italic><sub><italic>ji</italic></sub>, from all input units, <italic>i</italic>, with each input unit providing the input from a particular time point in the past. If the power of an impulse response failed to exceed a threshold of 1% of the average model impulse response power, it was removed from further analysis (last 13 hidden units shown).</p></caption><graphic xlink:href="EMS189014-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Impulse responses of representative hidden units of the temporal prediction model and auditory nerve fibers.</title><p>Twelve examples are shown in each case, illustrating the similarity between them. (A) Impulse responses of the network model. The impulse response for each hidden unit was normalized to its respective maximum absolute value. (B) Impulse responses of auditory nerve fibers recorded in anesthetized cats and obtained using reverse correlation (<xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>).</p></caption><graphic xlink:href="EMS189014-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Impulse response characteristics of the temporal prediction model.</title><p>(A) The normalized power spectrum of the impulse responses of each hidden unit of the feedforward model, with the units ranked by center frequency. The color-bar represents the normalized power. (B) The center frequency of each model unit as a function of hidden unit rank. The hidden units are interpreted to span a contiguous section of the basilar membrane in tonotopic order, with each unit evenly spaced along this section. The model centre frequencies are fitted with an exponential equation that has been used to describe the relationship between distance along the basilar membrane and characteristic frequency (<xref ref-type="bibr" rid="R21">Greenwood, 1961</xref>, <xref ref-type="bibr" rid="R22">1990</xref>, <xref ref-type="bibr" rid="R23">1996</xref>; <xref ref-type="bibr" rid="R52">Robles and Ruggero, 2001</xref>). The best linear fit of the model center frequencies is also shown. (C) The impulse response bandwidth of each hidden unit plotted against center frequency. The impulse response bandwidth was estimated at 10 dB below the spectral peak of the impulse response. (D) The filter sharpness (Q<sub>10</sub>) of each hidden unit plotted against center frequency. The filter sharpness was determined by dividing the center frequency by the impulse response bandwidth. Blue dots indicate data points taken from the temporal prediction model. Orange dots indicate data points recorded from cat auditory nerve fibers; the data were extracted from <xref ref-type="fig" rid="F2">Figure 2</xref> of <xref ref-type="bibr" rid="R14">Evans (1975)</xref>. The best linear fit of both the model and experimental data are shown as dashed lines in their respective color. (E) The temporal envelope of each hidden unit plotted against center frequency. The temporal envelope of each impulse response was determined by the shortest time window possible that captured 95% of the power of the impulse response. (F) For each impulse response, the instantaneous frequency was measured at different points in time using the zero-crossing method, and then fitted with a straight line to obtain the slope of its frequency glide (<xref ref-type="bibr" rid="R8">Carney et al., 1999</xref>). See <xref ref-type="sec" rid="S12">Methods</xref> for details. Each point is the slope of the frequency glide of an impulse response, plotted against the center frequency of that impulse response, for the range of neuronal best frequencies measured in <xref ref-type="bibr" rid="R8">Carney et al. (1999)</xref>. Negative slope values indicate a decrease in frequency as the impulse response progresses (a downwards glide); positive slope values indicate an increase (an upwards glide). The dashed line is the best linear fit.</p></caption><graphic xlink:href="EMS189014-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Impulse responses of temporal prediction models trained with various levels of noise added to the input.</title><p><bold>(A,B)</bold> Impulse response characteristics of the temporal prediction model trained on noiseless sound. <bold>(A)</bold> The normalized power spectrum of the impulse response of the hidden units of the feedforward model. The color-bar represents the normalized power. <bold>(B)</bold> Example impulse responses of the network model trained on noiseless sound. The impulse response for each hidden unit was normalized to its respective maximum absolute value. <bold>(C-F)</bold> Impulse responses of models trained on a signal-to-noise ratio (SNR) of 21, 15, 3 and 0 dB, respectively. The impulse response for each hidden unit was normalized to its respective maximum absolute value. <bold>(G)</bold> Prediction performance (MSE, mean squared error) of temporal prediction models trained with various levels of noise added to the input (but not the target). In each case, the models were tested on a held-out test set consisting of natural sounds without added noise. <bold>(H)</bold> As G, but models were tested using inputs consisting of natural sounds with various levels of noise added. The bottom row (noiseless test data) is the same as G.</p></caption><graphic xlink:href="EMS189014-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Impulse response characteristics of the autoencoder model.</title><p><bold>(A)</bold> The normalized power spectrum of the impulse response of the hidden units of the feedforward model. The color bar represents the normalized power. <bold>(B)</bold> The center frequency of each model unit as a function of hidden unit rank. The hidden units are interpreted to span a contiguous section of the basilar membrane in tonotopic order, with each unit evenly spaced along this section. The model center frequencies are fitted with a logarithmic equation that has been used to describe the relationship between characteristic frequency and distance along the basilar membrane (<xref ref-type="bibr" rid="R21">Greenwood, 1961</xref>, <xref ref-type="bibr" rid="R22">1990</xref>, <xref ref-type="bibr" rid="R23">1996</xref>; <xref ref-type="bibr" rid="R52">Robles and Ruggero, 2001</xref>). The best linear fit of the model center frequencies is also shown. <bold>(C)</bold> Impulse responses of the network model. The impulse response for each hidden unit was normalized to its respective maximum absolute value. <bold>(D)</bold> The impulse response bandwidth of each hidden unit plotted against center frequency. The impulse response bandwidth was estimated as a 10 dB decrease on either side of the spectral peak of the impulse response. <bold>(E)</bold> The filter sharpness of each hidden unit plotted against center frequency. The filter sharpness was determined by dividing center frequency by impulse response bandwidth at 10 dB below the peak response (Q<sub>10</sub>). Blue dots indicate data points taken from the temporal prediction model. Orange dots indicate data points taken from cat auditory nerve fibers (<xref ref-type="bibr" rid="R14">Evans, 1975</xref>). <bold>(F)</bold> The temporal envelope of each hidden unit plotted against center frequency. The temporal envelope of each impulse response was determined by the shortest time window possible that captured 95% of the power of the impulse response.</p></caption><graphic xlink:href="EMS189014-f006"/></fig></floats-group></article>