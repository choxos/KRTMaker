<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS191465</article-id><article-id pub-id-type="doi">10.1101/2023.11.19.567606</article-id><article-id pub-id-type="archive">PPR761813</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>DL4MicEverywhere: Deep Learning for Microscopy Made Flexible, Shareable, and Reproducible</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hidalgo-Cenalmor</surname><given-names>Iván</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Pylvänäinen</surname><given-names>Joanna W</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Ferreira</surname><given-names>Mariana G</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Russell</surname><given-names>Craig T</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Arganda-Carreras</surname><given-names>Ignacio</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><collab>AI4Life Consortium <contrib-group><contrib contrib-type="author"><name><surname>Muñoz-Barrutia</surname><given-names>Arrate</given-names></name><aff id="A12">Bioengineering Department, Universidad Carlos III de Madrid, Leganes, Spain; Instituto de Investigación Sanitaria Gregorio Marañón, Madrid, Spain</aff></contrib><contrib contrib-type="author"><name><surname>Serrano-Solano</surname><given-names>Beatriz</given-names></name><aff id="A13">Euro-BioImaging ERIC Bio-Hub, European Molecular Biology Laboratory (EMBL) Heidelberg, Heidelberg, Germany</aff></contrib><contrib contrib-type="author"><name><surname>Barcelo</surname><given-names>Caterina Fuster</given-names></name><aff id="A14">Bioengineering Department, Universidad Carlos III de Madrid, Leganes, Spain; Instituto de Investigación Sanitaria Gregorio Marañón, Madrid, Spain</aff></contrib><contrib contrib-type="author"><name><surname>Pape</surname><given-names>Constantin</given-names></name><aff id="A15">Georg-August-University Göttingen, Institute of Computer Science, Göttingen, Germany</aff></contrib><contrib contrib-type="author"><name><surname>Lundberg</surname><given-names>Emma</given-names></name><aff id="A16">Department of Pathology, Stanford University School of Medicine, Stanford, CA, USA; Science for Life Laboratory, School of Engineering Sciences in Chemistry, Biotechnology and Health, KTH Royal Institute of Technology, Stockholm, Sweden; Department of Bioengineering, Stanford University, Stanford, CA, USA</aff></contrib></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Jug</surname><given-names>Florian</given-names></name></contrib><contrib contrib-type="author"><name><surname>Deschamps</surname><given-names>Joran</given-names></name></contrib><aff id="A17">Fondazione Human Technopole, Milan, Italy</aff></contrib-group><contrib-group><contrib contrib-type="author"><name><surname>Hartley</surname><given-names>Matthew</given-names></name><aff id="A18">European Molecular Biology Laboratory, European Bioinformatics Institute, EMBL-EBI, Wellcome Genome Campus, Cambridge, UK</aff></contrib><contrib contrib-type="author"><name><surname>Seifi</surname><given-names>Mehdi</given-names></name><aff id="A19">Fondazione Human Technopole, Milan, Italy</aff></contrib><contrib contrib-type="author"><name><surname>Zulueta-Coarasa</surname><given-names>Teresa</given-names></name><aff id="A20">European Molecular Biology Laboratory, European Bioinformatics Institute, EMBL-EBI, Wellcome Genome Campus, Cambridge, UK</aff></contrib><contrib contrib-type="author"><name><surname>Galinova</surname><given-names>Vera</given-names></name><aff id="A21">Fondazione Human Technopole, Milan, Italy</aff></contrib><contrib contrib-type="author"><name><surname>Ouyang</surname><given-names>Wei</given-names></name><aff id="A22">Department of Applied Physics, Science for Life Laboratory, KTH Royal Institute of Technology, Stockholm, Sweden</aff></contrib></contrib-group><xref ref-type="fn" rid="FN1">*</xref></collab></contrib><contrib contrib-type="author"><name><surname>Jacquemet</surname><given-names>Guillaume</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="aff" rid="A9">9</xref><xref ref-type="aff" rid="A10">10</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Henriques</surname><given-names>Ricardo</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A11">11</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Gómez-de-Mariscal</surname><given-names>Estibaliz</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib></contrib-group><aff id="A1"><label>1</label>Optical cell biology group, Instituto Gulbenkian de Ciência, Oeiras, Portugal</aff><aff id="A2"><label>2</label>Faculty of Science and Engineering, Cell Biology, Åbo Akademi University, Turku, Finland</aff><aff id="A3"><label>3</label>European Molecular Biology Laboratory, European Bioinformatics Institute, Wellcome Genome Campus, United Kingdom</aff><aff id="A4"><label>4</label>Dept. Computer Science and Artificial Intelligence, University of the Basque Country (UPV/EHU), Spain</aff><aff id="A5"><label>5</label>IKERBASQUE, Basque Foundation for Science, Spain</aff><aff id="A6"><label>6</label>Donostia International Physics Center (DIPC), Spain</aff><aff id="A7"><label>7</label>Biofisika Institute, Spain</aff><aff id="A8"><label>8</label>Turku Bioscience Centre, University of Turku and Åbo Akademi University, Turku, Finland</aff><aff id="A9"><label>9</label>Turku Bioimaging, University of Turku and Åbo Akademi University, Turku, Finland</aff><aff id="A10"><label>10</label>InFLAMES Research Flagship Center, Åbo Akademi University</aff><aff id="A11"><label>11</label>MRC Laboratory for Molecular Cell Biology, University College London, London, United Kingdom</aff><author-notes><corresp id="CR1"><bold>Correspondence</bold>: <italic>(G. Jacquemet)</italic> <email>guillaume.jacquemet@abo.fi</email>, <italic>(R. Henriques)</italic> <email>rjhenriques@igc.gulbenkian.pt</email> <italic>(E. Gómez-de-Mariscal)</italic> <email>egomez@igc.gulbenkian.pt</email></corresp><fn id="FN1"><label>*</label><p id="P1">Authors of the AI4Life Consortium listed at the end</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>22</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Deep learning has revolutionised the analysis of extensive microscopy datasets, yet challenges persist in the widespread adoption of these techniques. Many lack access to training data, computing resources, and expertise to develop complex models. We introduce DL4MicEverywhere, advancing our previous ZeroCostDL4Mic platform, to make deep learning more accessible. DL4MicEverywhere uniquely allows flexible training and deployment across diverse computational environments by encapsulating methods in interactive Jupyter notebooks within Docker containers –a standalone virtualisation of required packages and code to reproduce a computational environment–. This enhances reproducibility and convenience. The platform includes twice as many techniques as originally provided by ZeroCostDL4Mic and enables community contributions via automated build pipelines. DL4MicEverywhere empowers participatory innovation and aims to democratise deep learning for bioimage analysis.</p></abstract><kwd-group><kwd>phototoxicity</kwd><kwd>live microscopy</kwd><kwd>machine learning</kwd><kwd>cell division</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Deep learning enables the transformative analysis of large multidimensional microscopy datasets, but barriers remain in implementing these advanced techniques (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>). Many researchers lack access to sufficient annotated data, high-performance computing resources, and expertise to develop, train, and deploy complex deep-learning models. In recent years, several approaches have been developed to democratise the usage of deep learning for microscopy (<xref ref-type="bibr" rid="R4">4</xref>). Multiple tools, such as BioImage.io, facilitate sharing and reusing broadly useful, previously trained deep learning models, distributing them as one-click image analysis solutions (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R5">5</xref>). Yet often, deep learning models need to be trained or finetuned on the end user dataset to perform well (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>). We previously released ZeroCostDL4Mic (<xref ref-type="bibr" rid="R2">2</xref>), an online platform relying on Google Colab that helped democratise deep learning by providing a zero-code interface to train and evaluate models capable of performing various bioimage analysis tasks, such as segmentation, object detection, denoising, super-resolution microscopy, and image-to-image translation. Here, we introduce DL4MicEverywhere, a major advancement of the ZeroCostDL4Mic (<xref ref-type="bibr" rid="R2">2</xref>) framework (<xref ref-type="fig" rid="F1">Fig.1</xref>). DL4MicEverywhere allows users the flexibility to train and deploy their models across various computational environments, including Google Colab, their own computational resources (<italic>e.g.</italic>, desktop or laptop), or high-performance computing systems. This flexibility is made possible by enclosing each deep learning technique in an interactive Jupyter notebook, which is then contained in a Docker (<xref ref-type="bibr" rid="R7">7</xref>)-based environment. This enables users to install and interact with deep learning techniques easily. Incorporating crossplatform containerisation technology boosters the long-term platform’s stability and reproducibility and enhances user convenience (<xref ref-type="bibr" rid="R8">8</xref>).</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P4">DL4MicEverywhere introduces a novel and user-friendly graphical interface that enables users to easily access and launch a comprehensive collection of interactive Jupyter notebooks. Each notebook comes packaged into a Docker container with all necessary software dependencies, as illustrated in <xref ref-type="fig" rid="F2">Fig. 2a-c</xref>.</p><p id="P5">DL4MicEverywhere has gone beyond simply containerising notebooks, providing a zero-code interface that handles all behind-the-scenes complexities. Users are not required to deal with the intricacies of Docker or configuring deep learning frameworks. The intuitive interface abstracts away these technical details, while the Docker encapsulation provides a standardised and rich environment for executing advanced techniques reliably (<xref ref-type="fig" rid="F2">Figure 2b</xref>). Researchers can select a notebook, choose computing resources, and run the corresponding deep learning-powered analysis with just a few clicks. The platform handles deploying the encapsulated coding environment seamlessly in the background. This allows users to train and apply models on various computing resources they control, eliminating reliance on third-party platforms. Furthermore, researchers can launch a notebook on local or remote systems with GPU acceleration on clusters whenever available, without worrying about complex software dependencies, docker container management or losing access to deep-learning frameworks (<xref ref-type="fig" rid="F2">Fig. 2d-f</xref>). DL4MicEverywhere offers twice the number of deep learning approaches than what was initially available in ZeroCostDL4Mic. The platform is designed to encourage sharing and reuse of models via the BioImage Model Zoo. DL4MicEverywhere’s infrastructure is strengthened by automated build pipelines (<xref ref-type="bibr" rid="R9">9</xref>), which allows for the seamless integration of new trainable models contributed by the community (<xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R13">13</xref>) (as shown in <xref ref-type="fig" rid="F1">Fig. 1b</xref>). These contributions are further facilitated through user-friendly templates, allowing new notebooks to be added independently of the original ZeroCostDL4Mic framework. By empowering participatory innovation in an open and flexible platform, DL4MicEverywhere aims to make deep learning more accessible for bioimage analysis. Developers can share a notebook based on our template and metadata for their method, and DL4MicEverywhere handles the testing and building of fully documented and open-source containerisation. Note that notebook containerisation allows others to reliably replicate analyses and build on the latest methods. The highly flexible nature of Docker containers encapsulating notebooks enhances long-term reproducibility across operating systems and computing environments. Researchers can easily share not just code, but the full software environment required to run it reliably. This reusable encapsulation empowers others to replicate analysis, evaluate methods, and build on research.</p></sec><sec id="S3" sec-type="discussion"><title>Discussion</title><p id="P6">Deep learning is revolutionising microscopy through data-driven analysis and discovery (<xref ref-type="bibr" rid="R14">14</xref>). However, significant barriers persist in accessing these advanced techniques, including a lack of training data, computing resources, and expertise (<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R14">14</xref>). Proprietary platforms create technological and cultural obstacles, while complex workflows impede adoption by non-experts. DL4MicEverywhere is an initiative that aims to make deep learning accessible to everyone by providing a flexible and community-driven platform. Encapsulating software in Docker containers makes it possible to integrate new methods and enrich the microscopy community through participatory innovation. Intuitive graphical user interfaces also lower the barriers to entry, making it easier for non-experts to use the platform. Users can rely on shared techniques while customising models across diverse hardware, retaining control over data and analysis. The platform will particularly be useful with the increasing development and use of cutting-edge foundation models (<xref ref-type="bibr" rid="R15">15</xref>). By bundling these sophisticated models into shareable containers, researchers can customise and exploit them in their microscopy applications. DL4MicEverywhere also simplifies complex deep learning workflows for nonexperts through automated pipelines, and is optimised for use with local computational resources, high-performance computing, and cloud-based solutions. This flexibility is precious for 1) sensitive biomedical data, where privacy risks may limit reliance on public cloud platforms, and 2) continuously scaling data such as time-lapse volumetric images or high-throughput high-content imaging data, where storage, dissemination and access rely on institutional infrastructures with specific data sharing protocols. DL4MicEverywhere also adheres to FAIR principles, enhancing discoverability and interoperability. We expect DL4MicEverywhere to represent an important step towards reliable, transparent, and participatory artificial intelligence in microscopy.</p></sec><sec id="S4" sec-type="methods" specific-use="web-only"><title>Methods</title><sec id="S5"><title>DL4MicEverywhere Platform Implementation</title><p id="P7">The core DL4MicEverywhere platform was implemented in Bash packaging and managing Python notebook workflows through Docker containers. An overview of the key technical components is provided below.</p></sec><sec id="S6"><title>Docker Containerization</title><p id="P8">Each notebook is encapsulated into a Docker container, including all dependencies required for smooth runtime (Docker v24.0.5, Docker Inc.). These containers are functional instances of Docker images –software units that contain the virtualisation of a specific computational environment, with all the specified dependencies and packages included. Images were built from Ubuntu (v20.04/22.04) base images, with optional Nvidia CUDA support for GPU acceleration. Python (v3.7/3.8/3.10), deep learning packages (TensorFlow, Keras or Pytorch), and notebook packages were installed according to the requirements into the containers. Unique containers were constructed for each notebook using a parameterised Docker file build process, taking metadata like notebook URL and software versions as input. These images are uploaded to Docker hub so they can be distributed as free and open source (FOSS) and belong to the Open Container Initiative (OCI) (<ext-link ext-link-type="uri" xlink:href="https://opencontainers.org/">https://opencontainers.org/</ext-link>).</p></sec><sec id="S7"><title>Launch Script and GUI</title><p id="P9">A Bash shell script launch.sh was implemented to manage the building, running, and monitoring of the notebook containers based on user input. Key functions included argument parsing, installation checking, Docker image building, and Jupyter Lab invocation within the container. A graphical user interface was additionally created using Wish (a Tcl/Tk application) to enable intuitive notebook and parameter selection through a desktop window. This is invoked by the launch script and passed user selections.</p></sec><sec id="S8"><title>Configuration Metadata</title><p id="P10">Inspired by the BioImage Model Zoo (<xref ref-type="bibr" rid="R1">1</xref>) specifications, notebook container construction was driven by human-readable YAML configuration files specifying necessary build metadata for each notebook, including the URL of the notebook itself, Python requirements, and Docker parameters. These configurations were loaded by the launch script when initialising a container. This format establishes a basis for a seamless connection with the BioEngine of the Zoo.</p></sec><sec id="S9"><title>Testing and Deployment Pipelines</title><p id="P11">GitHub Actions workflows were implemented to automatically build and publish container images for each new notebook, handling testing across platforms like AMD64 and ARM64. Images were versioned based on notebook metadata and published to DockerHub for distribution. Strict conventions enforced by templates facilitated notebook contributions from the community. These contributions are further checked via GitHub Actions to assert that they follow the specified format with valid URLs and that it is possible to build a Docker image.</p></sec><sec id="S10"><title>Jupyter Notebooks and Widgets</title><p id="P12">Notebooks were adapted from the ZeroCostDL4Mic Colab format to interactive Jupyter notebooks leveraging <italic>ipywidgets</italic> for a simplified user interface requiring no coding. Parameters could be configured via graphical elements rather than edits to code.</p></sec></sec></body><back><ack id="S11"><title>Acknowledgements</title><p>I.H.C., M.G.F., C.T.R., R.H., and E.G.M. received funding from the European Commission through the Horizon Europe program (AI4LIFE project with grant agreement 101057970-AI4LIFE, and RT-SuperES project with grant agreement 101099654-RT-SuperES to R.H.). I.H.C., M.G.F., E.G.M. and R.H. also acknowledge the support of the Gulbenkian Foundation (Fundação Calouste Gulbenkian) and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101001332 to R.H.). Funded by the European Union. Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. This work was also supported by the European Molecular Biology Organization (EMBO) Installation Grant (EMBO-2020-IG-4734 to R.H.), the EMBO Postdoctoral Fellowship (EMBO ALTF 174-2022 to E.G.M.), the Chan Zuckerberg Initiative Visual Proteomics Grant (vpi-0000000044 with DOI:10.37921/743590vtudfp to R.H.) and the Chan Zuckerberg Initiative DAF, an advised fund of Silicon Valley Community Foundations (Chan Zuckerberg Initiative Napari Plugin Foundations Grant Cycle 2, NP2-0000000085 granted to R.H.). R.H. also acknowledges the support of LS4FUTURE Associated Laboratory (LA/P/0087/2020). This work is partially supported by grant GIU19/027 (to I.A.C.) funded by the University of the Basque Country (UPV/EHU), grant PID2021-126701OB-I00 (to I.A.C.) funded by the Ministerio de Ciencia, Inno-vación y Universidades, AEI, MCIN/AEI/10.13039/501100011033, and by "ERDF A way of making Europe” (to I.A.C.). This study was also supported by the Academy of Finland (338537 to G.J.), the Sigrid Juselius Foundation (to G.J.), the Cancer Society of Finland (Syöpäjärjestöt; to G.J.), and the Solutions for Health strategic funding to Åbo Akademi University (to G.J.). This research was supported by InFLAMES Flagship Programme of the Academy of Finland (decision number: 337531). We would like to thank Amin Rezaei, Ainhoa Serrano, Pablo Alonso, Urtzi Beorlegui, Andoni Rodriguez, Erlantz Calvo, Soham Mandal, and Virginie Uhlmann for their contributions to the ZeroCostDL4Mic notebook collection.</p></ack><sec id="S12" sec-type="data-availability"><title>Code availability</title><p id="P13">The source code, documentation, and tutorials for DL4MicEverywhere can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/HenriquesLab/DL4MicEverywhere">https://github.com/HenriquesLab/DL4MicEverywhere</ext-link>. DL4MicEverywhere is made available under the Creative Commons CC-BY-4.0 license.</p></sec><ref-list><title>Bibliography</title><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>Wei</given-names></name><name><surname>Beuttenmueller</surname><given-names>Fynn</given-names></name><name><surname>Gómez-De-Mariscal</surname><given-names>Estibaliz</given-names></name><name><surname>Pape</surname><given-names>Constantin</given-names></name><name><surname>Burke</surname><given-names>Tom</given-names></name><name><surname>Garcia-López-De-Haro</surname><given-names>Carlos</given-names></name><name><surname>Russell</surname><given-names>Craig</given-names></name><name><surname>Moya-Sans</surname><given-names>Lucía</given-names></name><name><surname>De-La-Torre-Gutiérrez</surname><given-names>Cristina</given-names></name><name><surname>Schmidt</surname><given-names>Deborah</given-names></name><name><surname>Kutra</surname><given-names>Dominik</given-names></name><etal/></person-group><article-title>BioImage Model Zoo: A community-driven resource for accessible deep learning in bioimage analysis</article-title><source>bioRxiv</source><publisher-name>Cold Spring Harbor Laboratory</publisher-name><year>2022</year><month>June</month><elocation-id>2022.06.07.495102</elocation-id><pub-id pub-id-type="doi">10.1101/2022.06.07.495102</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>von Chamier</surname><given-names>Lucas</given-names></name><name><surname>Laine</surname><given-names>Romain F</given-names></name><name><surname>Jukkala</surname><given-names>Johanna</given-names></name><name><surname>Spahn</surname><given-names>Christoph</given-names></name><name><surname>Krentzel</surname><given-names>Daniel</given-names></name><name><surname>Nehme</surname><given-names>Elias</given-names></name><name><surname>Lerche</surname><given-names>Martina</given-names></name><name><surname>Hernández-Pérez</surname><given-names>Sara</given-names></name><name><surname>Mattila</surname><given-names>Pieta K</given-names></name><name><surname>Karinou</surname><given-names>Eleni</given-names></name><name><surname>Holden</surname><given-names>Séa-mus</given-names></name><etal/></person-group><article-title>Democratising deep learning for microscopy with ZeroCostDL4Mic</article-title><source>Nature Communications</source><year>2021</year><month>December</month><volume>12</volume><issue>1</issue><elocation-id>2276</elocation-id><comment>ISSN 2041-1723</comment><pub-id pub-id-type="pmcid">PMC8050272</pub-id><pub-id pub-id-type="pmid">33859193</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22518-0</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moen</surname><given-names>Erick</given-names></name><name><surname>Bannon</surname><given-names>Dylan</given-names></name><name><surname>Kudo</surname><given-names>Takamasa</given-names></name><name><surname>Graf</surname><given-names>William</given-names></name><name><surname>Covert</surname><given-names>Markus</given-names></name><name><surname>Van Valen</surname><given-names>David</given-names></name></person-group><article-title>Deep learning for cellular image analysis</article-title><source>Nature Methods</source><year>2019</year><month>December</month><volume>16</volume><issue>12</issue><fpage>1233</fpage><lpage>1246</lpage><comment>ISSN 1548-7091</comment><pub-id pub-id-type="pmcid">PMC8759575</pub-id><pub-id pub-id-type="pmid">31133758</pub-id><pub-id pub-id-type="doi">10.1038/s41592-019-0403-1</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pylvänäinen</surname><given-names>Joanna W</given-names></name><name><surname>Gómez-de Mariscal</surname><given-names>Estibaliz</given-names></name><name><surname>Henriques</surname><given-names>Ricardo</given-names></name><name><surname>Jacquemet</surname><given-names>Guillaume</given-names></name></person-group><article-title>Live-cell imaging in the deep learning era</article-title><source>Current Opinion in Cell Biology</source><year>2023</year><month>December</month><volume>85</volume><elocation-id>102271</elocation-id><comment>ISSN 0955-0674</comment><pub-id pub-id-type="pmid">37897927</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gómez-de Mariscal</surname><given-names>Estibaliz</given-names></name><name><surname>Haro</surname><given-names>Carlos García-López-de</given-names></name><name><surname>Ouyang</surname><given-names>Wei</given-names></name><name><surname>Donati</surname><given-names>Laurène</given-names></name><name><surname>Lundberg</surname><given-names>Emma</given-names></name><name><surname>Unser</surname><given-names>Michael</given-names></name><name><surname>Muñoz-Barrutia</surname><given-names>Arrate</given-names></name><name><surname>Sage</surname><given-names>Daniel</given-names></name></person-group><article-title>DeepImageJ: A user-friendly environment to run deep learning models in ImageJ</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2021</year><month>October</month><volume>18</volume><issue>10</issue><fpage>1192</fpage><lpage>1195</lpage><comment>ISSN 1548-7105 Number: 10</comment><pub-id pub-id-type="pmid">34594030</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Laine</surname><given-names>Romain F</given-names></name><name><surname>Arganda-Carreras</surname><given-names>Ignacio</given-names></name><name><surname>Henriques</surname><given-names>Ricardo</given-names></name><name><surname>Jacquemet</surname><given-names>Guillaume</given-names></name></person-group><article-title>Avoiding a replication crisis in deep-learning-based bioimage analysis</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2021</year><month>October</month><volume>18</volume><issue>10</issue><fpage>1136</fpage><lpage>1144</lpage><comment>2021 18:10. ISSN 1548-7105</comment><pub-id pub-id-type="pmcid">PMC7611896</pub-id><pub-id pub-id-type="pmid">34608322</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01284-3</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Merkel</surname><given-names>Dirk</given-names></name></person-group><article-title>Docker: lightweight linux containers for consistent development and deployment</article-title><source>Linux j</source><year>2014</year><volume>239</volume><issue>2</issue><fpage>2</fpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Moreau</surname><given-names>David</given-names></name><name><surname>Wiebels</surname><given-names>Kristina</given-names></name><name><surname>Boettiger</surname><given-names>Carl</given-names></name></person-group><article-title>Containers for computational reproducibility</article-title><source>Nature Reviews Methods Primers</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>July</month><volume>3</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage><comment>ISSN 2662-8449 Number: 1</comment><pub-id pub-id-type="doi">10.1038/s43586-023-00236-9</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beaulieu-Jones</surname><given-names>Brett K</given-names></name><name><surname>Greene</surname><given-names>Casey S</given-names></name></person-group><article-title>Reproducibility of computational workflows is automated using continuous analysis</article-title><source>Nature Biotechnology</source><publisher-name>Nature Publishing Group</publisher-name><year>2017</year><month>April</month><volume>35</volume><issue>4</issue><fpage>342</fpage><lpage>346</lpage><comment>ISSN 1546-1696 Number: 4</comment><pub-id pub-id-type="pmcid">PMC6103790</pub-id><pub-id pub-id-type="pmid">28288103</pub-id><pub-id pub-id-type="doi">10.1038/nbt.3780</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spahn</surname><given-names>Christoph</given-names></name><name><surname>Gómez-de Mariscal</surname><given-names>Estibaliz</given-names></name><name><surname>Laine</surname><given-names>Romain F</given-names></name><name><surname>Pereira</surname><given-names>Pedro M</given-names></name><name><surname>von Chamier</surname><given-names>Lucas</given-names></name><name><surname>Conduit</surname><given-names>Mia</given-names></name><name><surname>Pinho</surname><given-names>Mariana G</given-names></name><name><surname>Jacquemet</surname><given-names>Guillaume</given-names></name><name><surname>Holden</surname><given-names>Séamus</given-names></name><name><surname>Heilemann</surname><given-names>Mike</given-names></name><name><surname>Henriques</surname><given-names>Ricardo</given-names></name></person-group><article-title>DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches</article-title><source>Communications Biology</source><year>2022</year><month>July</month><volume>5</volume><issue>1</issue><elocation-id>688</elocation-id><comment>ISSN 2399-3642</comment><pub-id pub-id-type="pmcid">PMC9271087</pub-id><pub-id pub-id-type="pmid">35810255</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-03634-z</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Speiser</surname><given-names>Artur</given-names></name><name><surname>Müller</surname><given-names>Lucas-Raphael</given-names></name><name><surname>Hoess</surname><given-names>Philipp</given-names></name><name><surname>Matti</surname><given-names>Ulf</given-names></name><name><surname>Obara</surname><given-names>Christopher J</given-names></name><name><surname>Legant</surname><given-names>Wesley R</given-names></name><name><surname>Kreshuk</surname><given-names>Anna</given-names></name><name><surname>Macke</surname><given-names>Jakob H</given-names></name><name><surname>Ries</surname><given-names>Jonas</given-names></name><name><surname>Turaga</surname><given-names>Srinivas C</given-names></name></person-group><article-title>Deep learning enables fast and dense single-molecule localization with high accuracy</article-title><source>Nature Methods</source><publisher-name>Nature Publishing Group</publisher-name><year>2021</year><month>September</month><volume>18</volume><issue>9</issue><fpage>1082</fpage><lpage>1090</lpage><comment>ISSN 1548-7105 Number: 9</comment><pub-id pub-id-type="pmcid">PMC7611669</pub-id><pub-id pub-id-type="pmid">34480155</pub-id><pub-id pub-id-type="doi">10.1038/s41592-021-01236-x</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khadangi</surname><given-names>Afshin</given-names></name><name><surname>Boudier</surname><given-names>Thomas</given-names></name><name><surname>Rajagopal</surname><given-names>Vijay</given-names></name></person-group><article-title>EM-stellar: benchmarking deep learning for electron microscopy image segmentation</article-title><source>Bioinformatics</source><year>2021</year><month>April</month><volume>37</volume><issue>1</issue><fpage>97</fpage><lpage>106</lpage><comment>ISSN 1367-4803</comment><pub-id pub-id-type="pmcid">PMC8034537</pub-id><pub-id pub-id-type="pmid">33416852</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btaa1094</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priessner</surname><given-names>Martin</given-names></name><name><surname>Gaboriau</surname><given-names>David CA</given-names></name><name><surname>Sheridan</surname><given-names>Arlo</given-names></name><name><surname>Lenn</surname><given-names>Tchern</given-names></name><name><surname>Chubb</surname><given-names>Jonathan R</given-names></name><name><surname>Manor</surname><given-names>Uri</given-names></name><name><surname>Vilar</surname><given-names>Ramon</given-names></name><name><surname>Laine</surname><given-names>Romain F</given-names></name></person-group><article-title>Content-aware frame interpolation (CAFI): Deep Learning-based temporal super-resolution for fast bioimaging</article-title><source>bioRxiv</source><year>2021</year><comment>2021</comment><pub-id pub-id-type="pmid">38238557</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Hanchen</given-names></name><name><surname>Fu</surname><given-names>Tianfan</given-names></name><name><surname>Du</surname><given-names>Yuanqi</given-names></name><name><surname>Gao</surname><given-names>Wenhao</given-names></name><name><surname>Huang</surname><given-names>Kexin</given-names></name><name><surname>Liu</surname><given-names>Ziming</given-names></name><name><surname>Chandak</surname><given-names>Payal</given-names></name><name><surname>Liu</surname><given-names>Shengchao</given-names></name><name><surname>Van Katwyk</surname><given-names>Peter</given-names></name><name><surname>Deac</surname><given-names>Andreea</given-names></name><name><surname>Anandkumar</surname><given-names>Anima</given-names></name><etal/></person-group><article-title>Scientific discovery in the age of artificial intelligence</article-title><source>Nature</source><publisher-name>Nature Publishing Group</publisher-name><year>2023</year><month>August</month><volume>620</volume><issue>7972</issue><fpage>47</fpage><lpage>60</lpage><comment>ISSN 1476-4687 Number: 7972</comment><pub-id pub-id-type="pmid">37532811</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bommasani</surname><given-names>Rishi</given-names></name><name><surname>Hudson</surname><given-names>Drew A</given-names></name><name><surname>Adeli</surname><given-names>Ehsan</given-names></name><name><surname>Altman</surname><given-names>Russ</given-names></name><name><surname>Arora</surname><given-names>Simran</given-names></name><name><surname>von Arx</surname><given-names>Sydney</given-names></name><name><surname>Bernstein</surname><given-names>Michael S</given-names></name><name><surname>Bohg</surname><given-names>Jeannette</given-names></name><name><surname>Bosselut</surname><given-names>Antoine</given-names></name><name><surname>Brunskill</surname><given-names>Emma</given-names></name><name><surname>Brynjolfsson</surname><given-names>Erik</given-names></name><etal/></person-group><article-title>On the opportunities and risks of foundation models</article-title><source>arXiv:2108.07258 [cs]</source><year>2022</year><month>July</month></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>DL4MicEverywhere platform.</title><p>a) DL4MicEverywhere eases deep learning workflow sharing, deployment, and showcasing by providing a user-friendly interactive environment to train and use models. Enabling cross-platform compatibility ensures deep-learning model training reproducibility. DL4MicEverywhere contributes to deep learning standardisation in bioimage analysis by promoting transferable, FAIR, and transparent pipelines. The platform exports models compatible with the BioImage Model Zoo(<xref ref-type="bibr" rid="R1">1</xref>) and populates the Docker hub with free and open source (FOSS) container images that developers can reuse, incrementing the list of available workflows. b) DL4MicEverywhere accepts three types of notebook contributions: ZeroCostDL4Mic(<xref ref-type="bibr" rid="R2">2</xref>) notebooks, bespoke notebooks inspired by ZeroCostDL4Mic(<xref ref-type="bibr" rid="R2">2</xref>), and notebooks hosted in external repositories that are compliant with our format. These contributions are automatically tested to ensure the correct requirements and format.</p></caption><graphic xlink:href="EMS191465-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>a) When running DL4MicEverywhere, the user interacts with an interface to choose a notebook, image and output folder, and choose a GPU running model if possible. b) DL4MicEverywhere will automatically identify the system architecture and requirements to build a Docker container image. If the image is not available in the Docker hub, it is built in the user’s machine. This image is used to create a Docker container: a functional instance of the image that gathers the code environment to use the chosen notebook. c) A Jupyter lab session is launched inside the Docker container to train, evaluate or use the chosen deep learning model. DL4MicEverywhere notebooks are also interactive and equivalent to ZeroCostDL4Mic (<xref ref-type="bibr" rid="R2">2</xref>) notebooks. d-f) DL4MicEverywhere enables the use of the same notebooks in different local or remote infrastructures such as workstations, the cloud or high-performance computing clusters. This is, researchers could run exactly the same d) super-resolution, e) artificial-labelling or g) segmentation pipelines, among many others, in different systems.</p></caption><graphic xlink:href="EMS191465-f002"/></fig></floats-group></article>