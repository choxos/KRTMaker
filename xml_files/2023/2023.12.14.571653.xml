<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192844</article-id><article-id pub-id-type="doi">10.1101/2023.12.14.571653</article-id><article-id pub-id-type="archive">PPR774312</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Striatal dopamine reflects individual long-term learning trajectories</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Garcia</surname><given-names>Samuel Liebana</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Laffere</surname><given-names>Aeron</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Toschi</surname><given-names>Chiara</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Schilling</surname><given-names>Louisa</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Podlaski</surname><given-names>Jacek</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Fritsche</surname><given-names>Matthias</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Zatka-Haas</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Yulong</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bogacz</surname><given-names>Rafal</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Saxe</surname><given-names>Andrew</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Lak</surname><given-names>Armin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>1</label>Department of Physiology, Anatomy &amp; Genetics, University of Oxford, Oxford, United Kingdom</aff><aff id="A2"><label>2</label>School of Life Sciences, Peking University, 100871 Beijing, China</aff><aff id="A3"><label>3</label>MRC Brain Network Dynamics Unit, University of Oxford, Oxford, United Kingdom</aff><aff id="A4"><label>4</label>Gatsby Computational Neuroscience Unit &amp; Sainsbury Wellcome Centre, University College London, London, United Kingdom</aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label>Correspondence: <email>samuel.liebanagarcia@dpag.ox.ac.uk</email> and <email>armin.lak@dpag.ox.ac.uk</email>
</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>16</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>14</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Learning from naïve to expert occurs over long periods of time, accompanied by changes in the brain’s neuronal signals. The principles governing behavioural and neuronal dynamics during long-term learning remain unknown. We developed a psychophysical visual decision task for mice that allowed for studying learning trajectories from naïve to expert. Mice adopted sequences of strategies that became more stimulus-dependent over time, showing substantial diversity in the strategies they transitioned through and settled on. Remarkably, these transitions were systematic; the initial strategy of naïve mice predicted their strategy several weeks later. Longitudinal imaging of dopamine release in dorsal striatum demonstrated that dopamine signals evolved over learning, reflecting stimulus-choice associations linked to each individual’s strategy. A deep neural network model trained on the task with reinforcement learning captured behavioural and dopamine trajectories. The model’s learning dynamics accounted for the mice’s diverse and systematic learning trajectories through a hierarchy of saddle points. The model used prediction errors mirroring recorded dopamine signals to update its parameters, offering a concrete account of striatal dopamine’s role in long-term learning. Our results demonstrate that long-term learning is governed by diverse yet systematic transitions through behavioural strategies, and that dopamine signals exhibit key characteristics to support this learning.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Main</title><p id="P2">Over long periods of time, individuals learn to make increasingly accurate choices in response to stimuli they perceive. For instance, after extended training, a naïve individual can learn to play tennis expertly. Past studies have argued that humans and animals often pass through several stages of behaviour as they learn (<xref ref-type="bibr" rid="R1">1</xref>). This long-term learning rarely occurs in lockstep. Rather, individuals often take various paths through the space of behaviour, resulting in diverse learning trajectories. Neuroscientific studies of learning have typically ignored the dynamics of these diverse learning trajectories. They have either investigated neuronal signals in expert animals as they fine-tune an already learned task, or contrasted neuronal signals between naïve and expert animals without consideration of their learning trajectories (<xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R13">13</xref>). As such, the behavioural, neural, and computational underpinnings of long-term learning trajectories remain unknown.</p><p id="P3">Long-term learning often occurs by trial and error, a process well-captured by reinforcement learning (RL) models (<xref ref-type="bibr" rid="R14">14</xref>). Dopaminergic neurons of the midbrain play an essential role in reinforcement learning, signalling reward prediction error (RPE), the difference between predicted and acquired reward (<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref>). This RPE can be used to improve reward predictions, and therefore, future choices. However, little is known about the relationship between dopamine (DA) signals and the dynamics of long-term learning. Studies relating DA signals and learning have either measured these signals during Pavlovian conditioning, or during trial-by-trial decision making after task acquisition (<xref ref-type="bibr" rid="R17">17</xref>–<xref ref-type="bibr" rid="R30">30</xref>). It therefore remains unclear whether dopamine-driven learning can account for individual long-term learning trajectories.</p><p id="P4">Here we characterise individual behaviour as mice learned a perceptual decision making task over several weeks, while longitudinally measuring DA release in dorsolateral striatum (DLS), a region of the basal ganglia implicated in reinforcing reward-associated choices (<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R32">32</xref>). Mice improved their performance by adopting sequences of strategies that became more stimulus-dependent over time. This sequence varied widely across individuals. However, the strategy transitions were remarkably systematic; each animal’s future behaviour could be predicted days in advance based on its current behaviour. DLS DA signals reflected each individual’s evolving behavioural strategies, showing strikingly similar patterns of diverse yet systematic transitions. DA responses to stimuli developed reflecting stimulus-choice associations. Importantly, this encoding was dissociated from choice accuracy; DA responses were absent for stimuli that predicted reward but did not inform choice. DA outcome signals mirrored the stimulus responses, decreasing in magnitude as DA responses to stimuli grew. A simple computational model based on reinforcement learning in a deep neural network effectively reproduced the observed distribution of behavioural and DA trajectories throughout learning. This model had two important features: depth, i.e. three layers (necessary for explaining learning trajectories), and teaching signals defined based on subsets of inputs (necessary to account for DA signals). An analytical description of the model’s learning dynamics revealed a hierarchy of saddle points that accounted for the systematic transitions of behaviour and DA signals from naïve to expert. These results suggest a model in which long-term learning traverses a continuous space of behaviours by acquiring stimulus-choice associations while greedily improving reward prediction, driven by dopamine-based learning signals.</p><sec id="S2"><title>Long-term learning trajectories from naïve to expert</title><p id="P5">To study long-term learning of perceptual decision making, we further developed an established visual decision task in head-fixed mice (<xref ref-type="bibr" rid="R33">33</xref>). In each trial, we presented a visual stimulus (a grating) on the left or right side of a screen. The mouse, head-fixed in front of the screen, reported the stimulus position (left or right) by steering a wheel with its forepaws to bring the stimulus to the centre of the screen (<xref ref-type="fig" rid="F1">Fig.1a</xref>, <xref ref-type="fig" rid="F4">Extended Data Fig.1a</xref>, Methods). The mouse received a drop of water reward for each correct choice, or a white noise sound and brief timeout for incorrect choices (<xref ref-type="fig" rid="F1">Fig.1a</xref>). The contrast of the grating stimulus changed across trials, making the trials easier or harder. Some trials did not have a stimulus (i.e. zero-contrast trials), for which animals were rewarded randomly regardless of the wheel movement direction (<xref ref-type="fig" rid="F1">Fig.1a</xref>). We trained the mice over multiple days with a single session (approximately 250 trials) each day. Importantly, we kept the task unchanged throughout the entire experiment, presenting the full set of stimuli, task contingencies and trial timing from day one until expert performance (<xref ref-type="fig" rid="F1">Fig.1a</xref>, <xref ref-type="fig" rid="F4">Extended Data Fig.1a</xref>, Methods). This ensured that any changes in behaviour are a consequence of animals’ internal learning mechanisms, rather than a consequence of experimentally imposed changes to the task. It also enabled us to directly compare behavioural and neural data across days. Thus, the training differed from conventional shaping curricula in which the task becomes incrementally harder to guide learning. Mice learned the task reaching accuracies of at least 70% over many days of training (<xref ref-type="fig" rid="F1">Fig.1b</xref>, <xref ref-type="fig" rid="F4">Extended Data Fig.1b,c</xref>, median days = 19, median trial number = 3430), showing progressively steeper psychometric curves (<xref ref-type="fig" rid="F1">Fig.1c</xref>), and faster choice response times (<xref ref-type="fig" rid="F4">Extended Data Fig.1d</xref>). The decrease in response times (RT) occurred before the increase in the choice accuracy (<xref ref-type="fig" rid="F4">Extended Data Fig.1e,f</xref>), leading to an early increase in reward rate (i.e., reward per unit time) while the accuracy was still at chance level (<xref ref-type="fig" rid="F4">Extended Data Fig.1g</xref>).</p><p id="P6">In early days of learning, mice favoured left or right choices to varying degrees, exhibiting flat, but often biased, psychometric curves (<xref ref-type="fig" rid="F1">Fig.1d</xref>, first column, <xref ref-type="fig" rid="F1">Fig.1f,g</xref>). The biases often increased in these early days (<xref ref-type="fig" rid="F1">Fig.1f</xref>), while RTs decreased (<xref ref-type="fig" rid="F4">Extended Data Fig.1d,e</xref>). During this initial period, psychometric slopes were often close to zero, i.e. flat psychometric curves (<xref ref-type="fig" rid="F1">Fig.1c,d</xref>). Thus, initial days were marked by strategies where the mice ignored the position of the visual stimulus for making choices, showing increasing biases with different directions across animals.</p><p id="P7">During later days of learning, mice’s choices began to depend on the location of visual stimuli, resulting in psychometric curves with increasing slopes. Importantly, slopes often developed differently for left and right stimuli, with a vast diversity across mice (<xref ref-type="fig" rid="F1">Fig.1h-l</xref>). To visualise this diversity, we coloured each mouse’s learning trajectory based on the asymmetry of its psychometric slopes (<xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>, Methods). While the diversity of slopes formed a continuum across mice (<xref ref-type="fig" rid="F1">Fig.1j</xref>), to better visualise the main trends we clustered slope trajectories over learning (Methods). In some mice the slopes increased similarly on both sides, resulting in more balanced psychometric curves (<xref ref-type="fig" rid="F1">Fig.1j</xref>). This indicates that their strategy involved associating both left and right stimuli with their corresponding choice directions (<xref ref-type="fig" rid="F1">Fig.1d</xref> middle row). However, in several other mice, the slope primarily increased on one side while the other side remained flat, forming a one-sided psychometric curve (<xref ref-type="fig" rid="F1">Fig.1j</xref>). These mice therefore solved the task by associating stimuli on one side of the screen with their corresponding choice direction, and making the alternative choice in trials in which the associated stimulus was absent (<xref ref-type="fig" rid="F1">Fig.1d</xref> top and bottom rows). Consistent with this strategy, choices in zero-contrast trials and trials with non-associated stimuli were indistinguishable, i.e., the psychometric curve was flat on one side (<xref ref-type="fig" rid="F1">Fig.1d</xref>, right column, <xref ref-type="fig" rid="F5">Extended Data Fig.2d</xref>). These slope asymmetries often persisted as learning progressed (<xref ref-type="fig" rid="F1">Fig.1l</xref>, <xref ref-type="fig" rid="F5">Extended Data Fig.2f,g</xref>), and their corresponding signatures were observed in RTs as well as eye movements (<xref ref-type="fig" rid="F5">Extended Data Fig.2b,e,h</xref>). Importantly, despite strong asymmetries in slopes, one-sided left- and right-associating animals reached similar levels of choice accuracy compared to balanced animals (<xref ref-type="fig" rid="F1">Fig.1l</xref>). This is because both the presence and absence of stimuli can inform correct choices in the task, as there is only one stimulus per trial. Overall, we observed that in later days mice transitioned to more stimulus-dependent strategies, while exhibiting diversity in their use of left and right stimuli to make choices.</p><p id="P8">Behavioural transitions throughout learning were systematic. The bias in initial days strongly predicted the biases and psychometric curves in later stages of learning (<xref ref-type="fig" rid="F1">Fig.1g,i</xref>). Mice with initial left bias developed a larger slope on the left side of their psychometric curve, whereas mice with initial right bias developed a larger slope on the right side (<xref ref-type="fig" rid="F5">Extended Data Fig.2g</xref>). To achieve high accuracy despite asymmetric slopes, mice with more one-sided strategies reversed their initial bias during learning (<xref ref-type="fig" rid="F1">Fig.1g,k,m</xref>). Thus, naïve mice started with varying levels of bias, and this initial bias determined which stimuli were associated with choices in later stages of learning.</p><p id="P9">Taken together, the results show that in learning to make visual decisions from naïve to expert, mice exhibited diverse learning trajectories involving systematic transitions through behavioural strategies.</p></sec><sec id="S3"><title>Dorsal striatal dopamine signals from naïve to expert</title><p id="P10">We recorded DA release from day one until expert behaviour in the dorsolateral striatum (DLS). We injected GRAB-DA (<xref ref-type="bibr" rid="R34">34</xref>) in the DLS of wild-type mice, implanted one or two optic fibres dorsal to the injection site (one for each hemisphere), and imaged DLS DA release every day during learning (<xref ref-type="fig" rid="F2">Fig.2a,b</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig.3a</xref>; see <xref ref-type="sec" rid="S6">Methods</xref>).</p><p id="P11">DA release occurred in response to specific events within a trial, and changed throughout learning. A kernel regression revealed that the onset of visual stimulus, the arrival of the stimulus to the centre of the screen in correct trials (i.e., ‘stim. centre’), and water reward (or its absence in error trials) were the main events modulating DA release (<xref ref-type="fig" rid="F6">Extended Data Fig.3b,c</xref>). For our analyses, we defined a DA ‘stimulus’ and ‘outcome’ response to examine changes over learning. The ‘stimulus’ response quantified DA release levels in a short time window after stimulus onset (Methods). The ‘outcome’ response was defined as the sum of DA responses to completion of choice (marked by the stimulus arriving to the centre of the screen in the case of correct choices, or by the stimulus leaving the screen in the case of incorrect choices) and water reward delivery (or its absence). This is because the final stimulus position determines whether water reward will be subsequently delivered. In initial days, DA release mostly occurred in response to rewarded outcomes but not visual stimuli. As learning progressed, DA responses to visual stimuli grew and DA responses to water rewards diminished (<xref ref-type="fig" rid="F2">Fig.2b-e</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig.3b,c</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4a-c</xref>). In error trials, when reward was not delivered, DA signals transiently decreased (<xref ref-type="fig" rid="F6">Extended Data Fig.3b</xref>).</p><p id="P12">The development of DA responses during learning mirrored the diverse learning trajectories observed in behaviour. DA responses in individual mice showed strong correspondence to the development of their psychometric curves (<xref ref-type="fig" rid="F2">Fig.2c</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4a</xref>). In mice developing more one-sided psychometric curves, DA responses to stimuli emerged most strongly for stimuli presented on the associated side (top and bottom rows in <xref ref-type="fig" rid="F2">Fig.2c,d</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4a,b</xref>). However, in mice developing more balanced psychometric curves, DA responses to stimuli presented on left and right sides were similar (middle rows in <xref ref-type="fig" rid="F2">Fig.2c,d</xref> and <xref ref-type="fig" rid="F7">Extended Data Fig.4a,b</xref>). The diverse behavioural trajectories were also reflected in the rewarded outcome DA response; these DA responses were small after associated stimuli and large after non-associated stimuli (<xref ref-type="fig" rid="F2">Fig.2c,d</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4a,b</xref>).</p><p id="P13">In early days, when psychometric curves were still flat, DA responses to stimuli were already appearing, developing differently for stimuli on the left and right side depending on the animal’s bias (<xref ref-type="fig" rid="F2">Fig.2e</xref> Left). These DA responses were strongest for stimuli appearing on the same side as the choice bias (<xref ref-type="fig" rid="F2">Fig.2e</xref> Left, <xref ref-type="fig" rid="F7">Extended Data Fig.4c</xref>). We asked whether these signals reflected associations that the animals were forming between stimuli and choices, or whether they simply reflected each animal’s choice side preference. To address this, we inspected trials without visual stimuli (zero-contrast trials), and did not observe a difference in DA responses for the two choices (<xref ref-type="fig" rid="F7">Extended Data Fig.4d</xref>). This indicated that these DA signals are a signature of the association animals were forming between stimuli and choices in early days, which also manifested in a decreased RT for choices after associated stimuli (<xref ref-type="fig" rid="F7">Extended Data Fig.4e</xref>). Moreover, DA responses to stimuli were not evident on the first day of the experiment (<xref ref-type="fig" rid="F7">Extended Data Fig.4f</xref>), indicating that they emerge over learning, rather than reflecting stimulus novelty (<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R35">35</xref>). The early DA stimulus responses predicted both future DA responses to stimuli (<xref ref-type="fig" rid="F2">Fig.2f</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4g,h</xref>) as well as the shape of psychometric curves in later days (<xref ref-type="fig" rid="F2">Fig.2g</xref>). DA signatures of associations being formed between stimuli and choices were also evident in DA responses at trial outcome (<xref ref-type="fig" rid="F2">Fig.2d</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4b,c,h,i</xref>). Thus, during initial days, DA responses emerged, reflecting the first signature of learning to associate stimuli and choices, and these DA signals predicted behaviour and DA signals many days later.</p><p id="P14">In later days, as the psychometric curves developed, DA responses strongly reflected the growing psychometric slopes observed across animals (<xref ref-type="fig" rid="F2">Fig.2c-f</xref>). In right-associating mice (i.e. those with a steep psychometric curve on the right and a flat curve on the left), DA responses to stimuli were evident in response to right but not left stimuli (<xref ref-type="fig" rid="F2">Fig.2e</xref>, top row). This pattern was opposite in left-associating mice (<xref ref-type="fig" rid="F2">Fig.2e</xref>, bottom row). In balanced mice (i.e., those associating both stimuli with their corresponding choices), both left and right stimuli elicited strong DA responses (<xref ref-type="fig" rid="F2">Fig.2e</xref>, middle row). These observations held independent of stimulus laterality with respect to the recorded brain hemisphere (<xref ref-type="fig" rid="F8">Extended Data Fig.5a,b</xref>). The relationship between psychometric slopes and DA responses to stimuli was maintained even after controlling for any differences in the accuracy of left and right choices, i.e., selecting days where the accuracies were matched but the slopes differed (<xref ref-type="fig" rid="F2">Fig.2l,m</xref>, <xref ref-type="fig" rid="F8">Extended Data Fig.5c-e</xref>). Thus, in left- and right-associating mice, DA responses to associated stimuli were significantly larger than to non-associated stimuli, despite equally high choice accuracy for both stimuli (<xref ref-type="fig" rid="F2">Fig.2m</xref>). This indicates that the DA responses correlate with the psychometric slope, i.e. the formed association between the stimulus and choice, rather than reflecting the choice accuracy. Moreover, the encoding of psychometric slope was largely invariant to RTs (<xref ref-type="fig" rid="F8">Extended Data Fig.5f</xref>) and the accuracy of pending choice (i.e., correct/error, <xref ref-type="fig" rid="F8">Extended Data Fig.5g</xref>). DA outcome responses reflected the difference between outcome value and the learned stimulus-choice association, i.e., the reward that animals learned to predict using each stimulus. Hence, these DA responses decreased as stimulus-choice associations further increased (<xref ref-type="fig" rid="F2">Fig.2d</xref>, <xref ref-type="fig" rid="F8">Extended Data Fig.5g</xref>, middle row), instead of reflecting choice accuracy (<xref ref-type="fig" rid="F2">Fig.2m</xref>).</p><p id="P15">The dynamics of DA signals throughout the entirety of learning showed striking similarities to behavioural trajectories (<xref ref-type="fig" rid="F2">Fig.2h-k</xref>, neural trajectories plotted with colours and clusters obtained from behavioural trajectories). DA responses to stimuli developed across days reflecting the dynamics of slope and bias that each animal exhibited during learning (<xref ref-type="fig" rid="F2">Fig.2h-j</xref>). Similarly, DA responses to rewards developed over learning mirroring the evolving stimulus responses (<xref ref-type="fig" rid="F2">Fig.2k</xref>, <xref ref-type="fig" rid="F9">Extended Data Fig.6a,b</xref>). Thus, from DA signals at any point during learning, it was possible to infer the animals’ past and future DA signals and behavioural strategies (<xref ref-type="fig" rid="F2">Fig.2f-k</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4g-i</xref>, and <xref ref-type="fig" rid="F9">Extended Data Fig.6a,b</xref>).</p><p id="P16">Taken together, these results show that during long-term learning to make visual decisions, DLS DA responses to stimuli emerge as strategies become more stimulus-dependent, encoding stimulus-choice associations and predicting individuals’ learning trajectories. DLS DA outcome responses encode the difference between the value of trial outcome and reward predictions based on the visual stimulus, thus reflecting individuals’ learning trajectories.</p></sec><sec id="S4"><title>A deep linear neural network model of long-term learning</title><p id="P17">To understand the principles underlying the learning trajectories and their dopaminergic correlates, we designed a deep linear neural network model. In essence, this model contains multiple layers of neurons that learn to predict the reward associated with each stimulus and choice. The model has a simple architecture: an input layer of three neurons, a hidden layer of three neurons, and an output layer of two neurons (<xref ref-type="fig" rid="F3">Fig.3a</xref> Left, Methods). A matrix of nonnegative weights denoted by <italic>W</italic><sup><italic>1</italic></sup> connects the input layer to the hidden layer, and similarly nonnegative weights in <italic>W</italic><sup><italic>2</italic></sup> connect the hidden layer to the output layer. The weights in <italic>W</italic><sup><italic>1</italic></sup> form independent channels from the input layer neurons to the ‘cortical’ neurons in the hidden layer, resembling the anatomical segregation of visual inputs in each brain hemisphere. The weights in <italic>W</italic><sup><italic>2</italic></sup> represent the brain’s ‘cortico-striatal’ synapses and determine the contribution of neurons in the hidden layer to the output. The outputs of the network are two action values, <italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>, reflecting the learned value (i.e., reward prediction) of taking each choice as a function of the inputs.</p><p id="P18">On each trial, the model receives inputs, makes a choice, and learns from the resulting outcome. The sensory inputs are binary values (represented as 0/1). Two inputs ‘<italic>Vis. stim. R</italic>’ and ‘<italic>Vis. stim. L</italic>’ indicate which of the stimuli is presented on a particular trial. The third input ‘<italic>constant</italic>’ is always set to 1 to reflect stimulus-independent input, capturing environmental features that do not change trial-by-trial, e.g. the auditory go cue. The model makes choices by comparing <italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub> using a softmax choice rule. The model then compares the outcome of the choice with its corresponding reward prediction to calculate reward prediction errors (RPEs) used to update the weights <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup>. To have teaching signals proportional to our DLS DA signals, the model uses three different RPEs to update its parameters. For <italic>W</italic><sup><italic>1</italic></sup>, the RPE is calculated using a ‘total’ reward prediction using all the inputs to the network (<italic>Q</italic><sub><italic>ch</italic></sub>). For <italic>W</italic><sup><italic>2</italic></sup>, the updates are separated into two pathways: the ‘stimulus’ and ‘constant’ pathway (<xref ref-type="fig" rid="F3">Fig.3a</xref> Left, pink and aqua arrows respectively). The RPE in the ‘stimulus’ pathway is calculated using a ‘partial’ reward prediction <inline-formula><mml:math id="M1"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> based on the stimulus inputs (<italic>Vis stim. R</italic> and <italic>L</italic>), whereas the RPE in the ‘constant’ pathway is calculated using a prediction based only on the constant input <inline-formula><mml:math id="M2"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The stimulus-based RPE arising from this segregation is our account of DLS DA signals. This is motivated by the lack of DLS DA stimulus responses in trials with non-associated stimuli, despite high choice accuracy (<xref ref-type="fig" rid="F2">Fig.2l,m</xref>). This learning rule yields updates that minimise three different losses through gradient descent: the ‘cortical’ loss equal to the total RPE<sup>2</sup>, and the two ‘cortical-striatal’ losses equal to the pathway-specific RPE<sup>2</sup>’s (see <xref ref-type="sec" rid="S6">Methods</xref>). We term this model the ‘tutor-executor’ network because the ‘cortical’ learning (<italic>W</italic><sup><italic>1</italic></sup>) <italic>tutors</italic> downstream ‘cortico-striatal’ learning (<italic>W</italic><sup><italic>2</italic></sup>) by determining the relative salience of the inputs and balancing updates in the <italic>executor</italic> pathways to minimise its ‘total’ loss.</p><p id="P19">The model captured individual learning trajectories. Similar to mice, the model initiated learning by developing varying degrees of left/right bias (<xref ref-type="fig" rid="F3">Fig.3b,c</xref> cf. <xref ref-type="fig" rid="F1">Fig.1f,g</xref>). Subsequently, the model’s biases reversed as its psychometric slopes grew, exhibiting a similar diversity of left/right slope differences across trajectories to that seen in the mice (<xref ref-type="fig" rid="F3">Fig.3d,e</xref> cf. <xref ref-type="fig" rid="F1">Fig.1h,i</xref>). The model’s bias early in learning predicted bias and psychometric slopes later in learning, again showing striking similarity to mice data (<xref ref-type="fig" rid="F3">Fig.3c,e</xref> cf. <xref ref-type="fig" rid="F1">Fig.1g,i</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7a</xref> cf. <xref ref-type="fig" rid="F5">Extended Data Fig.2g</xref>). As such, the model’s entire learning trajectories resembled those of mice, capturing their diversity and systematicity, and exhibiting similar sigmoidal accuracy curves over comparable time scales (<xref ref-type="fig" rid="F3">Fig.3f-i</xref> cf. <xref ref-type="fig" rid="F1">Fig.1j-m</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7c-e</xref> cf. <xref ref-type="fig" rid="F1">Fig.1b-d</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7b</xref> cf. <xref ref-type="fig" rid="F4">Extended Data Fig.1c</xref>). The simplicity of the model allowed us to derive expressions for its average learning dynamics which showed close proximity to our behavioural data and model simulations (<xref ref-type="fig" rid="F3">Fig.3b,d,f-i</xref>, thick dashed lines). Therefore, a deep ‘tutor-executor’ RL network accounts for behavioural signatures of long-term learning within and across individuals.</p><p id="P20">DLS DA responses over learning were well captured by the model. We derived expressions for the trial-by-trial DA responses to stimuli and outcome using reward predictions from the ‘stimulus’ pathway (Methods). Similar to the empirical data, model-derived DA responses to stimuli grew over learning, reflecting stimulus-choice association (<xref ref-type="fig" rid="F3">Fig.3j</xref> cf. <xref ref-type="fig" rid="F2">Fig.2f</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7f</xref> cf. <xref ref-type="fig" rid="F2">Fig.2d</xref>). The model-derived DA signals early in training predicted both the model’s slope difference (<xref ref-type="fig" rid="F3">Fig.3k</xref>) and DA signals (<xref ref-type="fig" rid="F10">Extended Data Fig.7g,h</xref>) late in training, similar to DLS DA data (<xref ref-type="fig" rid="F2">Fig.2g</xref>, <xref ref-type="fig" rid="F7">Extended Data Fig.4g,h</xref>). Thus, model-derived DA responses to stimuli across learning exhibited the diverse yet systematic progression of the empirical DA signals (<xref ref-type="fig" rid="F3">Fig.3l-n</xref> cf. <xref ref-type="fig" rid="F2">Fig.2h-j</xref>). Finally, model-derived DA responses to outcome also showed strong similarity to our data, encoding the difference between the value of each trial’s outcome and the reward predicted by the stimulus, i.e. RPEs in the ‘stimulus’ pathway (<xref ref-type="fig" rid="F3">Fig.3o</xref> cf. <xref ref-type="fig" rid="F2">Fig.2k</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7i-k</xref> cf. <xref ref-type="fig" rid="F7">Extended Data Fig.4i</xref>, <xref ref-type="fig" rid="F9">6a,b</xref>). Taken together, the stimulus-based prediction errors of the deep ‘tutor-executor’ RL network mirror DLS DA signals throughout learning.</p><p id="P21">Analysis of the model average dynamics revealed a hierarchy of saddle points that explained the behavioural and neuronal trajectories. To find the average dynamics, we derived the average update to <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> across inputs and choices (Methods). Analysing the stationary points of the average dynamics (i.e., weight configurations where the average update goes to zero), revealed a series of saddle points. Saddle points have both stable and unstable manifolds, and in their vicinity learning momentarily slows down (<xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref>). These points span the entire learning process, starting from a ‘naïve’ weight configuration (<italic>0</italic>) and converging on a final expert global minimum (<italic>4</italic>) (<xref ref-type="fig" rid="F3">Fig.3a</xref> Right). The saddle points establish a systematic flow through the parameter space, whereby the saddle points approached early in learning influence those approached later in learning.</p><p id="P22">Each stationary point in the model has a characteristic behavioural and dopaminergic signature, similar to mice behavioural strategies and DLS DA responses. Simulations start close to the first saddle point (<italic>0</italic> in <xref ref-type="fig" rid="F3">Fig.3a</xref> Right) corresponding to a network configuration with all weights set to zero. Next, simulations that, for example, develop an initial right side bias learn in the direction of the <italic>1R</italic> saddle point, developing a strong association between the ‘constant’ input and <italic>Q</italic><sub><italic>R</italic></sub>, evident in its corresponding weight configuration diagram (<italic>1R</italic> in <xref ref-type="fig" rid="F3">Fig.3a</xref> Right). These simulations then move preferentially towards the next saddle point, <italic>2R</italic>, developing an association between the right stimulus and <italic>Q</italic><sub><italic>R</italic></sub> while maintaining a strong right bias. This happens as the simulations are still making mostly right choices (weight between ‘constant’ and <italic>Q</italic><sub><italic>R</italic></sub> is larger than between ‘constant’ and <italic>Q</italic><sub><italic>L</italic></sub>) but start to learn the correlation between right stimuli and reward after right choices. Simulations approaching <italic>2R</italic> then move towards <italic>3R</italic>, where psychometric slopes emerge, and the bias starts to reverse. Here, the simulations maintain their association between the right stimulus and <italic>Q</italic><sub><italic>R</italic></sub> while developing a strong weight between ‘constant’ and <italic>Q</italic><sub><italic>L</italic></sub>, all without using the left stimulus. Thus, in the vicinity of this saddle point the simulation infers correct left choices from the <italic>absence</italic> of the right stimulus, showing psychometric curves and DA signals similar to expert ‘right-associating’ mice. A mirror image of this trajectory is observed in simulations that initially develop a left bias (follow <italic>1L, 2L</italic> and <italic>3L</italic>), whereas more balanced simulations (those with a negligible initial bias) move from <italic>1B</italic> towards <italic>4</italic>, i.e., the global minimum. As such, the saddle points of the deep RL ‘tutor-executor’ network govern the learning trajectories (see saddle points visualised in <xref ref-type="fig" rid="F3">Fig.3f-i and l-o</xref>), explaining their diverse yet systematic transitions between strategies and corresponding DA signals.</p><p id="P23">Certain features of our model are critical for capturing our data. The tutor-executor learning rule is not strictly necessary to capture our behavioural data; a deep RL model that minimises a single ‘total’ RPE through gradient descent achieves qualitatively similar results (<xref ref-type="fig" rid="F11">Extended Data Fig.8</xref> Left). However, such a single loss model uses teaching signals different from our recorded DLS DA signals (<xref ref-type="fig" rid="F11">Extended Data Fig.8</xref> Right). The depth of the network instead <italic>is</italic> critical in defining the learning trajectories; a shallow model does not reproduce the one-sided trajectories we observed empirically. In fact, depth is required to obtain a saddle point structure yielding the systematic transitions through strategies we observed in the mice (<xref ref-type="fig" rid="F12">Extended Data Fig.9</xref>).</p></sec></sec><sec id="S5" sec-type="discussion"><title>Discussion</title><p id="P24">Our results show that in learning to make perceptual decisions from naïve to expert, mice display substantial individual variability while exhibiting systematic transitions through behavioural strategies. They initially increased their rate of reward harvest by making fast, non-accurate (biased) choices. This initial strategy determined their later strategy, i.e., which visual stimuli they use to solve the task. DA signals in the dorsolateral striatum (DLS) developed reflecting the stimulus-choice associations determined by each individual’s strategy. A deep reinforcement learning model trained using prediction errors analogous to empirical dopamine signals yielded diverse but systematic learning trajectories similar to those of mice. The learning trajectories were qualitatively governed by saddle points and their connecting manifolds, providing a formal account for how a biological learning mechanism can steer long-term learning.</p><p id="P25">Unlike conventional shaping methods for training animals that gradually change the task from easy to difficult, we chose to introduce mice to the full (i.e., relatively difficult) task from the onset of the experiment, and kept the task unchanged throughout learning. This training procedure allowed mice to freely explore and self-define their trajectories through the space of behaviour. In fact, the substantial early side biases, and later one-sided strategies we observed could be due to the higher starting difficulty of our task. The effect of task difficulty in developing biased strategies has been observed with other sensory modalities such as in whisker-based tasks (<xref ref-type="bibr" rid="R38">38</xref>). Future studies can examine the effects of various training curricula on learning trajectories. Our modelling framework allows for such investigation <italic>in silico</italic>, and for informing the design of curricula that accelerate learning.</p><p id="P26">Our work provides insights into the principles underlying the individual variability and systematicity of the mice’s learning trajectories. The saddle point structure emerging from the model’s architecture and learning rule reveals three main paths through the space of behaviour, giving a normative explanation for the diversity observed in mice. Within each path, the behaviour progressed systematically, such that the behaviour late in learning could be predicted by the behaviour in earlier stages of learning. The temporal resolution at which we observed systematic trajectories was of the order of tens of trials. However, there could also be behavioural strategy switches happening on faster time scales (<xref ref-type="bibr" rid="R39">39</xref>), which may be consistent with our observations. Further, in our model, the early variability in side bias emerges from imbalances in rewards after left and right choices. However, the early side bias in mice may also be influenced by other factors such as their position and handedness. As such, understanding the source of individual variability in these early days of learning requires further experiments.</p><p id="P27">DLS DA signals developed to encode stimulus-choice associations, such that stimuli whose presence – but not absence – informed choice and predicted reward, caused DLS DA release. Our model captures these signals using the stimulus-based prediction errors of its ‘stimulus pathway’. The model suggests that such local prediction errors could reflect the topography of connections between cortex and striatum, leading to different DA signals across striatal regions, as suggested by previous studies (<xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R41">41</xref>–<xref ref-type="bibr" rid="R43">43</xref>). For instance, our DLS DA signals were bilateral, in contrast to DA signals in more medial regions of dorsal striatum measured in a similar task (<xref ref-type="bibr" rid="R44">44</xref>). This difference could be due to DLS receiving more bilateral inputs from frontal and association cortical areas compared to dorsomedial striatum (<xref ref-type="bibr" rid="R45">45</xref>). The stimulus-based prediction errors observed here also differ from recordings of VTA DA neurons and their projections to ventral striatum in expert animals, which have been shown to integrate information across a wider variety of inputs and encode total, i.e. common currency, prediction errors, or even model-based prediction errors (<xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R47">47</xref>). The DLS DA signals we recorded modulate the strength of local cortico-striatal synapses (<xref ref-type="bibr" rid="R48">48</xref>). This, in turn, could rapidly regulate the size of stimulus-evoked responses in striatal neurons which form a functional closed loop by projecting back to midbrain DA neurons (<xref ref-type="bibr" rid="R49">49</xref>–<xref ref-type="bibr" rid="R51">51</xref>), causing the changes in DA stimulus and outcome responses we observed throughout learning.</p><p id="P28">Signatures of stimulus-choice association emerged in RTs before being evident in choice accuracy. DLS DA signals reflected this early signature, growing in initial days when mice had a flat but biased psychometric curve and choices in trials with the stimulus on the biased side were becoming faster. Unlike in other studies (<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R30">30</xref>), these DA signals were locked to stimulus onset and not choices. Nevertheless, these pre-outcome DA signals could contribute to reducing RTs by invigorating action (<xref ref-type="fig" rid="F8">Extended Data Fig.5h</xref>) (<xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R52">52</xref>). Our DLS DA signals also differed from previously reported DA novelty signals (<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R35">35</xref>) because they are absent in early days and grow over time. The absence of DA novelty signals could be because the visual stimuli we used were not salient enough in early days of learning, before their task-relevance was discovered. DLS DA stimulus-locked signals were similar in correct and error trials. Past studies measuring DA signals in the midbrain or ventral striatum of expert animals have shown smaller DA responses in error trials during perceptual decisions (<xref ref-type="bibr" rid="R52">52</xref>–<xref ref-type="bibr" rid="R54">54</xref>). This difference could be simply due to a difference in brain regions examined. Alternatively, this difference might emerge from different sources of errors: in our study errors reflect incomplete stimulus-choice association. However, in past studies of expert animals, errors are often caused by the limits of stimulus perception (<xref ref-type="bibr" rid="R55">55</xref>), leading to lower reward expectation in such trials.</p><p id="P29">We found that a hallmark of long-term learning to make perceptual decisions is the systematic progression through quasi-stages, connected by periods of more rapid change. Many other well-studied abilities–including face perception, semantic cognition, and numerical cognition–are characterised by similar structured transitions through strategies (<xref ref-type="bibr" rid="R56">56</xref>). Our model proposes a biological learning mechanism that exhibits such stage-like transitions, emerging from the hierarchy of saddle points that govern its learning dynamics. Interestingly, each saddle point corresponds to the network configuration that achieves the smallest loss using a subset of the available inputs or choices. Hence, the model explains the mice’s learning process as alternating between periods of discovering new inputs or choices, and periods of gradual association between the inputs and action value outputs (<italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>). The model also demonstrates that depth is a requirement of the circuit architecture, without which saddle points, and hence the characteristic learning stages, do not emerge. The same saddle point perspective has been offered as an explanation of stage-like transitions in semantic development (<xref ref-type="bibr" rid="R57">57</xref>). In contrast, ‘shallow’ reinforcement learning models commonly used in neuroscience can learn to perform the perceptual decision task (<xref ref-type="bibr" rid="R52">52</xref>), but do not capture the mouse learning trajectories.</p><p id="P30">Through the ‘tutor-executor’ learning rule, our neural network model reproduces the mouse learning trajectories with weight updates that are proportional to recorded DLS DA. A learning rule where different parts of the network minimise different loss functions, as used here, is unconventional in machine learning. However, this model accounts for both behavioural and DA data whereas standard gradient descent on a single total RPE<sup>2</sup> only accounts for the behaviour. Thus, while behavioural signatures of learning are consistent with multiple closely related models, the DA signals favour the tutor-executor model. Future studies are needed to investigate whether the DLS DA signal causally drive long-term learning to make perceptual decisions.</p><p id="P31">An intriguing feature of the ‘tutor-executor’ learning dynamics is that, with extensive training, weight magnitudes transfer from <italic>W</italic><sup><italic>1</italic></sup> (‘cortical’) to <italic>W</italic><sup><italic>2</italic></sup> (‘cortico-striatal’) while maintaining the value of their product <italic>W</italic><sup><italic>2</italic></sup><italic>W</italic><sup><italic>1</italic></sup> (<xref ref-type="fig" rid="F13">Extended Data Fig.10</xref>). This resembles the ‘transfer to striatum’ reported in past studies, where cortex was found to be crucial for task performance only in early stages of learning (<xref ref-type="bibr" rid="R58">58</xref>). Such a transfer can explain past results involving DLS in habitual behaviour (<xref ref-type="bibr" rid="R59">59</xref>), as the decrease in <italic>W</italic><sup><italic>1</italic></sup> weights leads to lower learning flexibility but could free the cortex for other tasks. This dynamic emerges from the difference in objective between <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> weights, since the total RPE teaching <italic>W</italic><sup><italic>1</italic></sup> reaches its minimum before the partial RPE’s in <italic>W</italic><sup><italic>2</italic></sup>, causing <italic>W</italic><sup><italic>2</italic></sup> to keep growing and <italic>W</italic><sup><italic>1</italic></sup> to compensate by decreasing.</p><p id="P32">In summary, we have identified behavioural, dopaminergic, and computational principles underlying long-term learning trajectories. Given its widespread nature, a saddle point view of stage-like learning could generalise beyond our task, and could be a signature of learning in the ‘deep’ cortico-striatal circuit investigated in this study. Building on our results, future studies can investigate the dynamics of neural activity in other dopaminergic circuits or other brain regions during long-term learning.</p></sec><sec id="S6" sec-type="methods"><title>Methods</title><sec id="S7"><title>Mice</title><p id="P33">The data presented in this paper was collected from 28 male wild-type C57/BL6J mice, with their age ranging between 9 to 30 weeks. All experiments were conducted according to the UK Animals Scientific Procedures Act (1986) under appropriate project and personal licences.</p></sec><sec id="S8"><title>Surgical procedures</title><p id="P34">Animals were anaesthetised with isoflurane and were kept on a feedback-controlled heating pad (Stoelting 53810). Hair overlying the skull was shaved and the skin and muscles over the central part of the skull were removed. The skull was thoroughly washed with sterile saline. A head plate was attached to the bone posterior to bregma using dental cement (Super-Bond C&amp;B). After the head plate fixation, we made craniotomies over the target areas and injected 300nl of AAV9-hsyn-DA2m into the right and/or left DLS (AP: +0.5 mm from bregma; ML: +/-2.5 mm from midline; DV: 2.8 mm from dura). This was followed by implantation of the optical fibre (core = 200 um, Neurophotometrics Ltd), which was secured to the head plate and skull using dental cement. Mice recovered for at least seven days following the surgery.</p></sec><sec id="S9"><title>Behavioural task</title><p id="P35">We trained mice in a complete psychometric visual decision making task from day 1 until expertise. Following surgery recovery, mice were first habituated to the experimenter for 2-3 days, followed by 2-3 days of habituation to the experimental rig and head-fixation. In each day of the experiment, mice were head-fixed with their body and hind-paws resting on a stable platform with a covering, and their forepaws resting on a steering wheel that could be rotated left and right. Each trial began after the wheel was held still for a short quiescence period (0.7-0.8s) (<xref ref-type="fig" rid="F4">Extended Data Fig.1a</xref>). A sinusoidal grating stimulus of varying contrast (0%, 25% and 50%) was presented on either the left or right side of a screen in front of the mouse, followed by an auditory go cue 0.2s after visual stimulus onset. The go cue indicated the start of the interactive period, during which wheel movements were coupled with movement of the visual stimulus on the screen. The mouse was required to indicate the position of the stimulus by steering the wheel in the correct direction to move the stimulus to the centre of the screen, causing a water reward (3ul drop) to be delivered via a spout positioned close to the mouth. A variable ‘feedback delay’ (0.1-0.3s) separated the time of choice completion from reward delivery. Subsequently, the next trial started following an ‘inter-trial delay’ (2-3s). When an incorrect choice was made, the mouse was presented with a 0.5s auditory stimulus of white noise via speakers positioned near each ear and had a brief timeout period (2s) before the next trial. When a mouse responded incorrectly to an ‘easy’ high-contrast stimulus (50% contrast), there was a 50% chance that the same stimulus was repeated in the next trials, until the mouse responded correctly. A response window of 30s after the go cue was provided for the mice to make their choice. The behavioural experiments were delivered by custom-made software written in MATLAB (MathWorks) which is freely available (<xref ref-type="bibr" rid="R60">60</xref>).</p></sec><sec id="S10"><title>Imaging dopamine release</title><p id="P36">To measure dopamine release in the dorsolateral striatum (DLS), we employed fibre photometry. Photometry and behavioural data were collected simultaneously. We used chronically implanted optical fibres to deliver excitation light and collect emitted fluorescence (Neurophotometrics FP3002). We used multiple excitation wavelengths (470 and 415 nm), delivered on alternating frames (sampling rate of 40 Hz), serving as target and isosbestic control wavelengths, respectively.</p><p id="P37">The recorded photometry signal was pre-processed following steps described previously (<xref ref-type="bibr" rid="R61">61</xref>). We began by de-interleaving the recorded signal at 470 nm and 415 nm wavelengths. Both signals were then de-noised to remove short-pulse artefacts using a median filter with kernel size 5 (medfilt from scipy.signal). Subsequently, the signals were detrended with a zero-phase low-pass filter with a 10Hz cutoff frequency (2nd-order butterworth filtfilt from scipy.signal). Next, a photobleaching correction was applied to remove slow changes in the signal likely coming from fluorophore degradation due to light exposure throughout the recording session. To do this, we used a scipy filtfilt zero-phase high-pass filter with a cutoff frequency of 0.001Hz, thus removing signals varying with a timescale slower than 16 minutes. We then corrected for motion signals by fitting the 415 nm isosbestic to the 470 nm signal with a least squares polynomial fit of degree 1 (linregress, scipy.stats) and the resulting fitted signal was then subtracted from the 470 nm signal. Finally, this quantity (ΔF) was normalised through division by the baseline fluorescence (F, defined as a low-pass filtering of the denoised 470 nm signal with a cutoff frequency of 0.001Hz) to obtain ΔF/F which was subsequently z-scored per session to enable more accurate comparisons across days of recording.</p><p id="P38">High data quality was ensured by removing sessions with weak DA signals. We plotted the relative amplitude of the raw 470nm and 415nm signal per session. If this ratio was smaller than 1 the session was discarded, since in such sessions most of the pre-processed ΔF/F fluctuations came from variation in the isosbestic signal instead of the informative 470nm channel. We also discarded sessions where the maximum fluctuations were smaller than one standard deviation (i.e., Z&lt;1).</p></sec><sec id="S11"><title>Video monitoring</title><p id="P39">The left eye was monitored with a camera (Teledyne Flir CM3-U3-13Y3M-CS) fitted with a zoom lens (Thorlabs MVL50M23) recording at 20 Hz. Front body movements were monitored with another camera (same model but different lens, Thorlabs MVL16M23) also recording at 20 Hz. Mice were illuminated with infrared light (850 nm, BW BWIR48) for the recording of eye and front body movements.</p></sec><sec id="S12"><title>Histology and fibre track quantifications</title><p id="P40">Histology after the experiment was performed to confirm successful fibre positioning. At the end of experiments, animals were deeply anaesthetised and perfused using 4% paraformaldehyde (PFA) and then decapitated. The brains were extracted, left in 4% PFA for 24h to post-fix in a refrigerator and then embedded in blocks of 1.5% agarose gel before collecting slices at 70 micrometre thickness using a vibratome (Leica VT1000 S). Slices were then stained with DAPI for 15 min (1:1000 solution), mounted onto glass, coverslipped, and imaged using an epifluorescence microscope (Leica).</p></sec><sec id="S13"><title>Behavioural data analyses</title><sec id="S14"><title>Behavioural data pre-processing</title><p id="P41">The behavioural data was pre-processed by removing the following trials: trials with response times more than 2 standard deviations above the mean per session, repeat trials (trials repeated after high contrast error trials) and trials where mice did not make a choice in less than 30s.</p></sec><sec id="S15"><title>Behavioural metrics</title><p id="P42">The main behavioural metrics we used to analyse the mouse trajectories were accuracy, psychometric slope and bias. Accuracy was defined as the proportion of rewarded choices in all trials except for those without stimuli (i.e., zero-contrast trials), where choices were rewarded randomly. Psychometric curves were calculated per session, and plotted the proportion of rightward choices (i.e., P(‘Right’)) for different stimulus positions (left, right) and contrast values (0, 0.25, 0.5). The value of psychometric slope we used in all our analyses is that of the simplified psychometric curve collapsing across contrast levels to give left stim, zero-contrast and right-stim x-axis values. Left (right) slope was defined as the absolute difference in P(‘Right’) for left (right) stimulus and zero-contrast trials. Bias was defined as the difference between the P(‘Right’) on zero-contrast trials and 0.5, thus representing the imbalance of choices on zero-contrast trials in left and right directions.</p></sec><sec id="S16"><title>Learning Trajectories</title><p id="P43">Each mouse was assigned a colour and cluster based on its learning trajectory. The rule for assigning colours used a weighted average of the difference between right and left per-session slopes over learning, where the weighting was equal to the sum of the left and right slopes on each session (<xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>). The resulting average slope asymmetry metric was used to determine a colour for each mouse on a spectrum ranging from purple (for negative values) to orange (for values around 0) to green (for positive values).</p><p id="P44">The trajectories of the behavioural metrics were smoothed for better visualisation, highlighting their slow variation over learning. To do this, we used scikit-learn’s Gaussian process regression package (<xref ref-type="bibr" rid="R62">62</xref>) to fit a gaussian process with an RBF kernel (with tuneable scaling and length-scale) to the session-by-session metrics. The predict method of the fit gaussian process could then be used to estimate the smoothed value of the metric at different time points over learning.</p><p id="P45">A cluster label was assigned to each mouse to obtain cluster averages that highlighted the main trends in the diversity across learning trajectories. A dynamic time warping clustering algorithm was used to obtain these clusters (<xref ref-type="bibr" rid="R63">63</xref>). This algorithm first looks for the time warping that best clusters the trajectories by shape. The cluster centres are then computed as the barycenters with respect to the time warped mouse trajectories, yielding cluster centres that are similar in shape to the individual trajectories, thus solving the problem of averaging across mice that learn at different speeds. This clustering was applied to the smoothed right vs. left slope trajectories in <xref ref-type="fig" rid="F1">Fig.1j</xref> and the resulting cluster labels were used to compute the cluster averages in all other behavioural and neural plots throughout the paper. The same colouring, smoothing, and clustering methods were applied to the model simulations to obtain plots similar to those produced for the experimental data.</p></sec><sec id="S17"><title>Pupil analysis</title><p id="P46">We used DeepLabCut (<xref ref-type="bibr" rid="R64">64</xref>) to track several points on the mice’s left pupil throughout each task trial. We selected 4 points in the top, bottom, left and right portions of each mouse’s pupil and recorded the x and y coordinates of each point over time. For our pupil motion analysis (<xref ref-type="fig" rid="F5">Extended Data Fig.2h</xref>), we defined the average x and y coordinate of these 4 points as the position of the pupil and investigated its horizontal (Δx) and vertical (Δy) motion. The average x and y coordinates were z-scored per session to enable more accurate comparisons across days. The alignment to stimulus onset was done as described in the neural analyses section. The analysis window used to compute pupil ‘stimulus responses’ was from 0.25-0.35s after the stimulus for the vertical motion and 0.5-0.6s for the horizontal motion.</p></sec></sec><sec id="S18"><title>Neural data analyses</title><sec id="S19"><title>Event alignment and time warping</title><p id="P47">DLS DA recordings were aligned to task events that caused significant DLS dopamine release, as determined by a kernel regression (see below). These events were visual stimulus onset, choice completion (i.e., correct trials: visual stimulus arriving in the centre of the screen, and error trials: stimulus moving out of the screen) and trial outcome (reward/no reward). To do this, a fixed time period around each event (-0.5s to +1s) was selected and a fixed number of elements for the resulting aligned neural trace were chosen (i.e., 100). The DA recording was then linearly interpolated to obtain a value for each of the desired time points in the chosen time period (using scipy.interp1d). The average value in the time period before the event (-0.5-0s) was used as a baseline and subtracted from the event-aligned traces.</p><p id="P48">Time warping was used to visualise DA signals in a single continuous trace including all trial events. This was achieved by warping the DA signals such that a fixed number of time points represented the time course between each event. In this way, varying time periods between events were accounted for by allowing different time intervals between data points. We chose to have 40 time points before stimulus onset, 30 between stimulus onset and choice completion, 12 between choice completion and outcome and 60 post-outcome. The recorded DA signal was then interpolated to obtain the fluorescence values at the corresponding time points. This time warping allowed us to compute and visualise average DA signals across trials with varying durations.</p></sec><sec id="S20"><title>Normalisation across sessions</title><p id="P49">In addition to the pre-processing steps described above, we further corrected for session-by-session variation in fluorescence levels by normalising the DA signals to the peak of the average DA response to reward delivery in zero-contrast (i.e., no visual stimulus) trials. To do this, we computed the average time warped DA trace per session, took the mean of the 3 largest values in the post-reward response (peak), and then subtracted the mean fluorescence levels 3 time points before and after the time of reward delivery (baseline) to obtain a single number (peak – baseline) that was used to divide the entire DA recording from that session by. This normalisation assumes that DA reward responses in zero-contrast trials should not vary session-by-session, as there are no cues that can predict the random reward delivery on these trials, meaning that the degree of ‘surprise’ (i.e., reward prediction error) should remain fairly constant throughout learning. This yielded normalised ΔF/F values that ranged from 0-1, expressing the signal as a proportion of the zero-contrast reward response in each session, thus resulting in more accurate averages across sessions and mice and more interpretable fluorescence values that could be compared with the model-derived DA signals.</p></sec><sec id="S21"><title>Analysis windows</title><p id="P50">We defined average DA responses to 3 events (stimulus onset, choice completion and reward) which we use in several analyses throughout the paper. These were defined as the average DA signal in a specific time window after each event, relative to the signal before the event.</p><p id="P51">For the ‘stimulus response’, we defined an analysis window from +0.2s to +0.35s post-stimulus onset and took the peak response of a moving average (window size = 10) of the signal in that time window. We then subtracted a baseline fluorescence value (defined as the mean ΔF/F in a window -0.1s to +0.1s around stimulus onset) from the peak estimate to obtain the ‘stimulus response’ on each trial.</p><p id="P52">For the DA response to choice completion (‘stim. centre’ on correct trials), we used the time warped traces to define the analysis windows for the peak and baseline estimates. We did this because the times between stimulus, choice completion and reward delivery/absence are variable and can be short, so using time warping is an accurate method to obtain isolated estimates of DA release values uniquely around this event. To do this, we used the entire 12 elements of the time warped trace between choice complete and reward delivery/absence and found the peak value of its moving average (window size = 6). From this, we subtracted the mean of the time warped DA signal one time point before and after the choice complete event.</p><p id="P53">For the DA response to reward delivery/absence we used the same procedure as for the ‘stimulus response’. We only changed the analysis window from which we calculated the peak response, i.e., 0-1s after the reward time. The baseline was similarly defined using the average signal in a window -0.1s to +0.1s around the event.</p><p id="P54">For most neural analyses we defined a combined ‘outcome response’ which is the addition of DA responses to choice completion and reward delivery/absence. This was done because in rewarded trials, the ‘stim. centre’ cue is a perfect predictor of upcoming reward and thus rapidly acquires value, causing the DA responses to reward to become smaller. We did not want to include this learning process in our analyses as it does not involve decision making (i.e., is Pavlovian). Hence, by adding the responses for the two events, we obtained an outcome signal that did not change until events before choice completion became associated with reward.</p></sec><sec id="S22"><title>Kernel regression</title><p id="P55">To find the events that caused significant variation in DLS DA release levels, we used a kernel regression algorithm. This algorithm works by regressing binary variables indicating the time period around events onto full trial DA signals. The design matrix (<italic>X</italic>) has a column per time point around the events of interest (regressors) and a row per time point of the DA signal, which was concatenated for all the trials used in the regression to form the dependent variable (<italic>y</italic>). The algorithm then finds the optimal scaling (<italic>β*</italic>) of the regressors in <italic>X</italic> which produces a prediction of the concatenated DA signal (<italic>ŷ*</italic>) that minimises the mean squared error Σ<sub>i</sub> <italic>(y</italic><sub><italic>i</italic></sub> <italic>– ŷ</italic><sub><italic>i</italic></sub><italic>)</italic><sup><italic>2</italic></sup>, <disp-formula id="FD1"><label>(Eq.1)</label><mml:math id="M3"><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mo>⋆</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mtext>X</mml:mtext><mml:mo>⊤</mml:mo></mml:msup><mml:mtext>X</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mtext>X</mml:mtext><mml:mo>⊤</mml:mo></mml:msup><mml:mtext>y,</mml:mtext></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD2"><label>(Eq.2)</label><mml:math id="M4"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>X</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula>
</p><p id="P56">The resulting elements in <italic>β*</italic> compose the ‘kernels’ for each event. The benefits of using kernel regression over an event-aligned average is that it isolates the effect of each event on the DA signal, removing the influence of other events occurring shortly before or after. This isolation is achieved as long as there is enough jitter between events across trials; if two events are separated by a fixed delay in all trials the regression will not find isolated kernels due to correlated regressors.</p><p id="P57">To account for changing DA signals over learning in our kernel regression, we split all trials into 4 bins with increasing psychometric slope. We also performed separate regressions for rewarded and unrewarded trials. The events we considered were stimulus onset, choice start, choice completion and reward delivery/absence, which resulted in a total of 32 kernels (<xref ref-type="fig" rid="F6">Extended Data Fig.3b</xref>). The explained variance for each kernel was computed by comparing the 5-fold cross-validated R<sup>2</sup> of a ‘full’ model with all the kernels against that of a model without the kernel being assessed.</p><p id="P58">We used a custom implementation of this algorithm written in Python, which can be found in our analysis code repository.</p></sec></sec><sec id="S23"><title>Deep linear RL model</title><sec id="S24"><title>Architecture</title><p id="P59">The model is a 3-layer deep neural network with linear activation functions (<xref ref-type="fig" rid="F3">Fig.3a</xref> Left). We denote the weight matrix connecting the input layer to the hidden layer <italic>W</italic><sup><italic>1</italic></sup>, and the matrix connecting the hidden layer to the output layer <italic>W</italic><sup><italic>2</italic></sup>, such that the function computed by the network is given by <disp-formula id="FD3"><label>(Eq.3)</label><mml:math id="M5"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mspace width="0.2em"/><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>x</italic> is the vector of inputs and <italic>y</italic> is the vector of outputs. The weights in both <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> are constrained to be nonnegative, and <italic>W</italic><sup><italic>1</italic></sup> is constrained to be a diagonal matrix. These constraints were chosen for simplicity and are not strictly necessary to capture the data. The network has two binary input neurons which encode the presence or absence of the right and left visual stimulus and one input neuron that has an activation of 1 for every trial, representing any non-stimulus cues that the mice may use to make choices on each trial, e.g. the auditory go cue. Thus, the network receives three different input vectors depending on the trial type, <disp-formula id="FD4"><label>(Eq.4)</label><mml:math id="M6"><mml:mrow><mml:mtext>Left</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>stimulus</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>Zero-contrast</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>Right</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>stimulus</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula></p><p id="P60">We opted not to model the different contrast levels on different trials (25% and 50%) because in most mice we did not see a significant difference in accuracy between these two levels.</p><p id="P61">In every trial, the input vectors are multiplied by the weights in <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> to obtain the activation of the two output neurons which encode the learned value of left and right choices, <italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>. These action values are used to determine choice through a softmax function with inverse temperature <italic>β</italic> that determines the choice probabilities, <disp-formula id="FD5"><label>(Eq.5)</label><mml:math id="M7"><mml:mrow><mml:mtext>P</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>'</mml:mo><mml:mtext>Right</mml:mtext><mml:mo>'</mml:mo><mml:mspace width="0.2em"/></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mtext>R</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="0.2em"/><mml:mtext>and</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>P</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mo>'</mml:mo><mml:mtext>Left</mml:mtext><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mtext>P</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mo>'</mml:mo><mml:mtext>Right</mml:mtext><mml:mo>'</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula> from which a choice is sampled on each trial.</p><p id="P62">For each simulation, the initialisation of <italic>W</italic><sup><italic>1</italic></sup> was sampled from a gaussian distribution centred on fit values of the initial stimulus input weights (identical for left/right) and the constant input weight. Similarly, the initialisation of <italic>W</italic><sup><italic>2</italic></sup> was sampled from a gaussian distribution centred on fit values of the stimulus pathway weights (<xref ref-type="fig" rid="F3">Fig.3a</xref> Left, pink – all identical) and the constant pathway weights (<xref ref-type="fig" rid="F3">Fig.3a</xref> Left, aqua – all identical). The softmax inverse temperature parameter <italic>β</italic> and learning rate <italic>α</italic> were also sampled from a gaussian centred on a value fit to the experimental data. The fitting procedure is described in the corresponding subsection. Simulations were run for 10,000 iterations (i.e., trials) and those that reached 70% accuracy in less than 8,500 iterations (approx. highest number of trials required for mice to learn) were included in our analyses.</p></sec><sec id="S25"><title>Learning rule</title><sec id="S26"><title>a. ‘Tutor-executor’ gradient descent</title><p id="P63">We refer to the model presented in <xref ref-type="fig" rid="F3">Fig.3</xref> as the ‘tutor-executor’ model due to its learning rule, which uses different reward prediction errors (RPEs) to train the weights in <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup>. The updates minimise three different losses through stochastic gradient descent (SGD): the ‘cortical’ loss for the weights in <italic>W</italic><sup><italic>1</italic></sup>, the ‘stimulus cortico-striatal’ loss for the weights in the stimulus pathway of <italic>W</italic><sup><italic>2</italic></sup> (<xref ref-type="fig" rid="F3">Fig 3a</xref> Left, pink), and the ‘constant cortico-striatal’ loss for the weights in the constant pathway of <italic>W</italic><sup><italic>2</italic></sup> (<xref ref-type="fig" rid="F3">Fig 3a</xref> Left, aqua). Each of these losses is a RPE<sup>2</sup> comparing predictions based on different subsets of inputs against trial outcome, <disp-formula id="FD6"><label>(Eq.6)</label><mml:math id="M8"><mml:mrow><mml:mo>'</mml:mo><mml:mtext>Cortical</mml:mtext><mml:mo>'</mml:mo><mml:mspace width="0.2em"/><mml:mtext>loss</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD7"><label>(Eq.7)</label><mml:math id="M9"><mml:mrow><mml:mspace width="0.2em"/><mml:mo>'</mml:mo><mml:mtext>Stimulus</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>cortico-striatal</mml:mtext><mml:mo>'</mml:mo><mml:mspace width="0.2em"/><mml:mtext>loss</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:msup><mml:mrow/><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mspace width="0.2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula> <disp-formula id="FD8"><label>(Eq.8)</label><mml:math id="M10"><mml:mrow><mml:mspace width="0.2em"/><mml:mtext>'Constant</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>cortico-striatal'</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>loss</mml:mtext><mml:mo>:</mml:mo><mml:mspace width="0.2em"/><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mspace width="0.2em"/><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mspace width="0.2em"/><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula> where <italic>Rew</italic> is a binary variable indicating whether the trial was rewarded or not, and the subscript <italic>ch</italic> indicates the choice made on each trial (left/right). Here <italic>Q</italic><sub><italic>ch</italic></sub> is the ‘total’ <italic>Q</italic>-value that uses all inputs to form its predictions, while <inline-formula><mml:math id="M11"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M12"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are ‘partial’ <italic>Q</italic>-values based on the stimulus and constant inputs in turn, <disp-formula id="FD9"><label>(Eq.9)</label><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD10"><label>(Eq.10)</label><mml:math id="M14"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mspace width="0.2em"/><mml:mspace width="0.2em"/><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD11"><label>(Eq.11)</label><mml:math id="M15"><mml:mrow><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>VSR</italic> and <italic>VSL</italic> are the binary inputs indicating the presence or absence of the right and left stimulus respectively and <italic>ch</italic> used as a subscript for the weights is 0 for left and 1 for right choices.</p><p id="P64">Gradient descent on these losses yields updates which depend on the trial outcome and choice, <disp-formula id="FD12"><label>(Eq.12)</label><mml:math id="M16"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD13"><label>(Eq.13)</label><mml:math id="M17"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfenced close="" open="{"><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>α</mml:mi><mml:mfenced close="]" open="["><mml:mrow><mml:mtable equalrows="true" equalcolumns="true"><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>if</mml:mtext><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math></disp-formula><disp-formula id="FD14"><label>(Eq.14)</label><mml:math id="M18"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula> where <italic>α</italic> is the learning rate and <italic>βW</italic><sup>2</sup> = <italic>βW</italic><sup>2,<italic>stim</italic></sup> + <italic>βW</italic><sup>2,<italic>const</italic></sup>. Notice how the updates for <italic>W</italic><sup><italic>1</italic></sup> are proportional to the total RPE, <italic>δ</italic><sup><italic>tot</italic></sup>; the updates for the stimulus pathway in <italic>W</italic><sup><italic>2</italic></sup> are proportional to the stimulus-based RPE, <italic>δ</italic><sup><italic>stim</italic></sup>; and lastly, the updates for <italic>W</italic><sup><italic>2</italic></sup>’s constant pathway are proportional to the constant-based RPE, <italic>δ</italic><sup><italic>const</italic></sup>. Interestingly, the general tendency of the learning rule is to minimise the total ‘cortical’ loss, <italic>ℒ</italic><sup><italic>tot</italic></sup>, as the learning in <italic>W</italic><sup><italic>1</italic></sup> <italic>tutors</italic> downstream learning in <italic>W</italic><sup><italic>2</italic></sup> by determining the relative salience of the inputs and balancing updates in the <italic>executor</italic> pathways.</p></sec></sec><sec id="S27"><title>Single-loss gradient descent</title><p id="P65">This learning rule updates <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> through stochastic gradient descent (SGD) to minimise a single total RPE<sup>2</sup>. This corresponds to the conventional method of training deep RL networks, where all parameters are updated to minimise a single loss and thus share the same objective (<xref ref-type="bibr" rid="R65">65</xref>). The loss we use for this learning rule is the same as the ‘cortical’ loss in the tutor-executor model, <disp-formula id="FD15"><label>(Eq.15)</label><mml:math id="M19"><mml:mrow><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>Rew</italic> is a binary variable indicating the outcome of a particular trial and <italic>Q</italic><sub><italic>ch</italic></sub> is a reward prediction calculated based on all inputs to the network. The updates for <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> can be written as follows <disp-formula id="FD16"><label>(Eq.16)</label><mml:math id="M20"><mml:mrow><mml:mtext>Δ</mml:mtext><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="FD17"><label>(Eq.17)</label><mml:math id="M21"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>α</mml:mi><mml:mo>[</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>R</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>V</mml:mi><mml:mi>S</mml:mi><mml:mi>L</mml:mi><mml:mspace width="0.2em"/><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>1</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula> where <italic>α</italic> is the learning rate and <italic>VSR</italic> and <italic>VSL</italic> are binary variables indicating the presence or absence of the right and left stimulus respectively. Here, all the updates are proportional to the same total RPE, <italic>δ</italic><sup>tot</sup>.</p></sec><sec id="S28"><title>Model-derived dopamine signals</title><p id="P66">Our neural network model captures empirical dorsolateral striatal dopamine (DLS DA) signals through the weights in its stimulus pathway. Over learning, the model reproduces DLS DA outcome responses with a stimulus-based reward prediction error (RPE), <disp-formula id="FD18"><label>(Eq.18)</label><mml:math id="M22"><mml:mrow><mml:mspace width="0.2em"/><mml:mtext>Outcome-time</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>DLS</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>DA</mml:mtext><mml:mspace width="0.2em"/><mml:mo>≡</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>w</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>Q</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>.</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula></p><p id="P67">This differs from conventional temporal difference reward prediction errors (TD-RPEs) commonly used in DA studies in that it does not use the full reward prediction based on all inputs to define the RPE, instead comparing trial outcome with a prediction based only on the stimulus inputs. This was motivated by our matched accuracy analysis in <xref ref-type="fig" rid="F2">Fig.2l,m</xref>, which showed that DLS DA at both stimulus and outcome time does not reflect reward predictions based on the constant input (i.e., the absence of a stimulus).</p><p id="P68">At stimulus time, in accordance with the TD-RPE hypothesis (<xref ref-type="bibr" rid="R15">15</xref>), the model captured DLS DA signals with a pre-choice reward prediction. Importantly, this prediction is also based only on the stimulus inputs, <disp-formula id="FD19"><label>(Eq.19)</label><mml:math id="M23"><mml:mrow><mml:mspace width="0.2em"/><mml:mtext>Stimulus-time</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>DLS</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>DA</mml:mtext><mml:mspace width="0.2em"/><mml:mo>≡</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>Q</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mspace width="0.2em"/></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M24"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>L</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M25"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>R</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are the left and right choice probabilities determined by the stimulus-based <italic>Q</italic>-values passed through the choice function in <xref ref-type="disp-formula" rid="FD5">Equation 5</xref>. Here, there is no need to subtract the value of the previous state as there is no cue before the stimulus that is predictive of reward.</p><p id="P69">To test whether the network could be trained with a learning signal analogous to our DLS DA recordings, the tutor-executor model uses a learning rule based on these ‘partial’ prediction errors to update its weights. Specifically, the updates of the <italic>W</italic><sup><italic>2</italic></sup> stimulus pathway weights are proportional to the stimulus-based RPE, <italic>δ</italic><sup><italic>stim</italic></sup> (<xref ref-type="disp-formula" rid="FD13">Equation 13</xref>). Comparing the evolution of <italic>δ</italic><sup><italic>stim</italic></sup> with the DLS DA outcome response over learning shows a striking similarity (<xref ref-type="fig" rid="F3">Fig.3o</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig.7f,i-k</xref>), suggesting that a similar dopamine-based learning mechanism could be governing the learning process of the mice.</p></sec><sec id="S29"><title>Average dynamics</title><p id="P70">Deriving the average dynamics of the model allowed us to obtain an analytical description of its learning process. We took the continuous time limit of the average gradient descent updates for <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> from the tutor-executor and single-loss learning rules, averaging over trial type (left stim., right stim., or zero-contrast) and choice (left or right). This yielded a 9-dimensional system of coupled differential equations describing the evolution of each weight in the network. For sufficiently small learning rates <italic>α</italic>, this ‘gradient flow’ limit provides a good description of the average dynamics for both learning rules. The resulting differential equations were numerically integrated to obtain average weight trajectories over training time. To capture the three main types of learning trajectory (i.e., left-associating, balanced and right-associating), we initialised the integration with a network configuration yielding different degrees of initial choice bias (i.e., imbalanced connections from const. to <italic>Q</italic><sub><italic>L</italic></sub> and <italic>Q</italic><sub><italic>R</italic></sub>). We then overlayed the resulting trajectories on those from trial-by-trial simulation (thick dashed lines in model figures).</p></sec><sec id="S30"><title>Tutor-executor average dynamics</title><p id="P71">The average dynamics of the network weights for the tutor-executor learning rule are governed by three main differential equations; one for each of the cortical, stimulus cortico-striatal and constant cortico-striatal weight subsets (black, pink, and aqua in <xref ref-type="fig" rid="F3">Fig.3a</xref> Left). These can be derived by taking the average over trial-types and choices of the gradient descent updates in <xref ref-type="disp-formula" rid="FD12">Equations 12</xref>-<xref ref-type="disp-formula" rid="FD14">14</xref>. Doing this for the cortical weights in <italic>W</italic><sup><italic>1</italic></sup> we obtain <disp-formula id="FD20"><label>(Eq.20)</label><mml:math id="M26"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mtext>⟨</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>⟨</mml:mtext><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mtext>⟩</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:msub><mml:mtext>⟩</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mtext>⟨⟨</mml:mtext><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mtext>⟩</mml:mtext><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:msub><mml:mtext>⟩</mml:mtext><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSL</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>L</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mtext>R</mml:mtext><mml:mo>,</mml:mo><mml:mtext>VSR</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic>t</italic> is a continuous time variable counting the number of trials; <italic>δ</italic><sup><italic>tot</italic></sup> represents the total RPE; <italic>p</italic><sub>VSL</sub> = <italic>p</italic><sub>VSR</sub> = 0.45 and <italic>p</italic><sub>0</sub> = 0.1 are the probabilities of there being a left vis. stim., right vis. stim., and zero-contrast trial; and lastly <italic>p</italic><sub>A,B</sub> and <italic>Q</italic><sub>A,B</sub> indicate the choice probabilities and total <italic>Q</italic>-values for choice A in a trial of type B. The choice probabilities are calculated using the sigmoidal choice rule in <xref ref-type="disp-formula" rid="FD5">Equation 5</xref>. The partial derivatives of the <italic>Q</italic>-values can then be expanded by writing them in terms of the network weights (<xref ref-type="disp-formula" rid="FD9">Equations 9</xref>-<xref ref-type="disp-formula" rid="FD11">11</xref>) and differentiating w.r.t. the <italic>W</italic><sup><italic>1</italic></sup> matrix.</p><p id="P72">The same procedure can be followed to find the differential equations for the weights in the stimulus and constant pathways of <italic>W</italic><sup><italic>2</italic></sup>, with gradient flow equations <disp-formula id="FD21"><label>(Eq.21)</label><mml:math id="M27"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>stim</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD22"><label>(Eq.22)</label><mml:math id="M28"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>⋅</mml:mo></mml:mrow></mml:math></disp-formula>
</p><sec id="S31"><title>b. Single-loss average dynamics</title><p id="P73">The single-loss average dynamics can be derived in a similar fashion, where now all the weights are minimising the same loss function (<xref ref-type="disp-formula" rid="FD15">Equation 15</xref>), yielding the following gradient flow equations <disp-formula id="FD23"><label>(Eq.23)</label><mml:math id="M29"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>〈</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mo>〉</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>〉</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD24"><label>(Eq.24)</label><mml:math id="M30"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>α</mml:mi></mml:mfrac><mml:mfrac><mml:mi>d</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mo>〈</mml:mo><mml:mo>〈</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>tot</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:msub><mml:mo>〉</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo>〉</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> which can be expanded as exemplified with the tutor-executor dynamics.</p></sec></sec><sec id="S32"><title>Saddle Points</title><p id="P74">Saddle points in the model’s learning dynamics provide an explanation for the systematic transitions between behavioural strategies and dopamine release patterns observed in the mice. We used the average dynamics derived in the sections above to demonstrate the existence of these saddle points. We first made informed guesses at stationary points by looking for network configurations where the average dynamics go to 0, and then investigated the dynamics around these points to verify their nature. These derivations can be found in the following Mathematica notebooks:</p><p id="P75"><ext-link ext-link-type="uri" xlink:href="https://www.wolframcloud.com/obj/samuelliebanagarcia/Published/tutor_executor_stationary_points.nb">https://www.wolframcloud.com/obj/samuelliebanagarcia/Published/tutor_executor_stationary_points.nb</ext-link></p><p id="P76"><ext-link ext-link-type="uri" xlink:href="https://www.wolframcloud.com/obj/samuelliebanagarcia/Published/single_loss_stationary_points.nb">https://www.wolframcloud.com/obj/samuelliebanagarcia/Published/single_loss_stationary_points.nb</ext-link></p><p id="P77">We also provide evidence for heteroclinic orbits connecting the saddle points, represented by the arrows in <xref ref-type="fig" rid="F3">Fig.3a</xref> and <xref ref-type="fig" rid="F11">Extended Data Fig.8a</xref>. To do this, we used the string method (<xref ref-type="bibr" rid="R66">66</xref>) to find the minimal energy path between each pair of saddle points. This allowed us to distinguish between points that are directly connected by such paths (i.e., heteroclinic orbits), and points that are only connected through another one of the saddle points. The orbits discovered by the string method are shown in the Mathematica notebooks, and their schematic form is shown in <xref ref-type="fig" rid="F3">Fig.3a</xref> and <xref ref-type="fig" rid="F11">Extended Data Fig.8a</xref>.</p></sec><sec id="S33"><title>Fitting procedure</title><p id="P78">To reproduce the learning trajectories in the data with our model, we fit the value of the network weights at initialisation and the <italic>β</italic> parameter of the choice function. Specifically, we fit the initial <italic>W</italic><sup><italic>1</italic></sup> weights for the constant and stimulus inputs (equal for left/right), and the initial <italic>W</italic><sup><italic>2</italic></sup> weights for the constant and stimulus pathways (all equal within each pathway). We took advantage of our expressions for the average dynamics of both the tutor-executor and single-loss learning rules, and minimised the mean squared error between the three trajectories emerging from integrating the dynamics starting from a small amount of left, right and no bias, and the three trajectory clusters from the data (derived from <xref ref-type="fig" rid="F1">Fig.1j</xref>). The parameters were fit using momentum-based gradient descent by comparing the learning trajectories from the data shown in <xref ref-type="fig" rid="F1">Fig.1j-m</xref> and <xref ref-type="fig" rid="F2">Fig.2d</xref> with the same trajectories derived from integrating the average dynamics equations. This proved much faster than simulation-based methods. Lastly, the learning rate <italic>α</italic> was fine-tuned by hand to obtain learning trajectories that were stable and learnt in a similar number of iterations (i.e., trials) as the mice. The resulting parameter values were used as the means of gaussians from which the parameters of each simulated network were sampled for the simulations shown in the figures across the paper.</p></sec></sec><sec id="S34"><title>Software packages</title><p id="P79">All data analyses were performed using custom code written in Python 3 using standard analysis and plotting libraries: numpy, scipy, matplotlib and seaborn. For the model, the JIT compilation and automatic differentiation capabilities of JAX were used to accelerate and simplify gradient calculations.</p></sec></sec><sec sec-type="extended-data" id="S35"><title>Extended Data</title><fig id="F4" position="anchor"><label>Extended Data Fig. 1</label><caption><title>Learning a visual decision task from naïve to expert.</title><p id="P80"><bold>a</bold>, The temporal structure of the task within a trial. Words in regular font indicate trial events, words in italics are labels for the traces in the timing diagram, and the solid vs dashed style of the traces indicate fixed vs variable time periods respectively. <bold>b</bold>, Histogram of the number of days that mice required to reach 70% accuracy. <bold>c</bold>, Histogram of the number of trials that mice required to reach 70% accuracy. <bold>d</bold>, Chronometric curve over quartiles per mouse (grey) and averaged across all mice (black). Negative (positive) contrast values indicate stimuli presented on the left (right) side of the screen. RT (s) indicates the mean response time (time from stimulus onset to choice completion) per contrast value in seconds, averaged first across trials in each session, and then across sessions in each quartile. Quartiles are defined per mouse by dividing days into 4 groups, with any remainder added to the last group. Error bars indicate the 95% confidence interval across mice. <bold>e</bold>, Z-scored RTs over training days per mouse (grey) and averaged across all mice (black/red). Red dots indicate data points significantly different from day 1 averages (<italic>p</italic>&lt;0.05, estimated via two-sided t-test). Z-scoring was performed by standardising each mouse’s mean RT per day using data from all its days. <bold>f</bold>, Accuracy over training days. <bold>g</bold>, Reward rate over training days, calculated as the choice accuracy divided by the mean (not z-scored) RT for each day.</p></caption><graphic xlink:href="EMS192844-f004"/></fig><fig id="F5" position="anchor"><label>Extended Data Fig. 2</label><caption><title>Learning from naïve to expert across mice.</title><p id="P81"><bold>a</bold>, Chronometric curves for the same days and mice as in <xref ref-type="fig" rid="F1">Fig.1d</xref>. Error bars indicate 95% confidence interval across trials in each day. <bold>b</bold>, Expert session chronometric curves per mouse (thin) and averaged per trajectory cluster (thick): right-associating (green), balanced (orange) and left-associating (purple). Cluster labels for each mouse obtained from <xref ref-type="fig" rid="F1">Fig.1j</xref>. Error bars indicate 95% confidence interval of the average across mice in each cluster. <bold>c</bold>, Colour scheme (left), scatter (middle), and histogram (right) of average psychometric slope asymmetry per mouse. Slope asymmetry was calculated as 0.5 times the weighted average of the difference in R-L slopes over all sessions in a mouse’s learning trajectory. The weightings in the average are the sum of both R+L slopes per session. <bold>d</bold>, Scatter of average right vs. left expert session psychometric slopes for each mouse. Stroke colour indicates the average slope asymmetry of each mouse. The fill of the left/right half of each circle (black/white) indicates the statistical significance of the left/right slopes (black means significant). <bold>e</bold>, Scatter of average right vs. left expert session chronometric slopes for each mouse. Chronometric slopes were defined as the difference between the median RT on zero-contrast trials and trials with right/left stimuli. <bold>f</bold>, Scatter plot of expert session right vs. left slopes, coloured by the average slope asymmetry of the mouse trajectory they belong to. <bold>g</bold>, Bar plots showing the average bias and slope difference in early (<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R8">8</xref>) and late (final 5) days for each mouse. Colours come from average psychometric slope asymmetry (see panel c). <bold>h</bold>, Average vertical and horizontal eye movements (pupil motion) aligned to stimulus onset in balanced and one-sided mice for stimuli contralateral (pink) and ipsilateral (blue) to the pupil, as well as on zero-contrast trials (grey). In one-sided mice, solid lines indicate pupil motion in response to ‘associated’ stimuli and dashed lines to ‘non-associated’ stimuli. Bar plots quantify pupil motion by taking the average in analysis windows from 0.25-0.35s after stimulus onset for the vertical motion and 0.5-0.6s for horizontal motion. <sup>*</sup> and <sup>**</sup> indicate <italic>p</italic> &lt; 0.05, <italic>p</italic> &lt; 0.005.</p></caption><graphic xlink:href="EMS192844-f005"/></fig><fig id="F6" position="anchor"><label>Extended Data Fig. 3</label><caption><title>Dopamine signals in dorsolateral striatum during task learning.</title><p id="P82"><bold>a</bold>, Top, the locations of the optic fibre estimated from post-mortem histological examinations overlaid on brain slice schematic. Bottom, example brain slice showing fibre tip. <bold>b</bold>, Regression coefficients of a kernel regression of DLS DA signals on different events in a trial: stimulus onset, choice start, choice completion, and outcome (reward/no reward). The regression uses the time points around each event as categorical regressors, and then finds the least squares regression coefficients predicting the original signal. The data was divided into correct and error trials, and was grouped into four bins of increasing psychometric slope. The kernel regression was performed independently for each subset of data. <bold>c</bold>, R<sup>2</sup> for each trial event used in the kernel regression, calculated independently for correct and error trials and for each level of psychometric slope.</p></caption><graphic xlink:href="EMS192844-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Fig. 4</label><caption><title>Dopamine signals during learning across mice.</title><p id="P83"><bold>a</bold>, Average time warped DLS DA signals across mice in each cluster, plotted over quartiles for correct trials with stimulus on the left (red) and right (blue), c.f. <xref ref-type="fig" rid="F2">Fig.2c</xref>. Vertical dashed lines indicate stim. onset, stim. centre, and reward delivery time. Insets show average psychometric curves across mice in each cluster for every quartile. Error bars in both neural and behavioural plots indicate 95% confidence interval of average across mice. <bold>b</bold>, Average stimulus and outcome DA responses over days in correct trials with stimulus on the left (red) and right (blue) for the three example mice from <xref ref-type="fig" rid="F1">Fig.1d</xref> and <xref ref-type="fig" rid="F2">Fig.2c</xref>. Error bars indicate +/- s.e.m. across trials. Data points fit with a 3rd degree spline to visualise trend (scipy.interpolate.UnivariateSpline), c.f. <xref ref-type="fig" rid="F2">Fig.2d</xref>. <bold>c</bold>, Quantification of DA signals from <xref ref-type="fig" rid="F2">Fig.2e</xref>. Left, bar plots showing average DA stimulus responses across mice in each cluster for correct trials with stimulus on the left (red) and right (blue) in early days (accuracy n.s. greater than 0.5) and late days (accuracy n.s. smaller than 0.7). Error bars indicate 95% confidence interval of average across mice. <italic>p</italic>-values calculated using two-sided t-test. Right, same as left but for outcome responses. <bold>d</bold>, Top left, average early days (accuracy n.s. greater than 0.5) stimulus-aligned DA signals in correct trials for one-sided animals. Solid and dashed black lines show signals in trials with associated and non-associated stimuli respectively. Solid and dashed grey lines show DA signals on zero-contrast trials (aligned to the time when the stimulus would have been presented, i.e., 0.2 s before the auditory go cue) for choices in and opposite to the direction of each mouse’s early bias. Bottom left, average early day time warped DA signals in correct trials. Top right, average early day DA stimulus responses in correct trials using the same legend as in top left. Bottom right, the sum of average early day DA responses to stimulus and outcome. <italic>p</italic>-values calculated using two-sided t-test. Error bars for stim.-aligned and time warped plots indicate +/- s.e.m., and bar plot error bars indicate 95% confidence interval, both across mice. <bold>e</bold>, Early day (accuracy n.s. greater than 0.5) chronometric curves for the three clusters of mice. Error bars indicate +/- s.e.m. across mice. <italic>p</italic>-values calculated using two-sided t-test. <sup>*, **</sup> and <sup>***</sup> indicate <italic>p</italic> &lt; 0.05, <italic>p</italic> &lt; 0.005 and <italic>p</italic> &lt; 0.0005. <bold>f</bold>, Left, first/second day of training average psychometric curves for the three clusters of mice. Error bars indicate 95% confidence interval of average across mice. Middle, first/second day of training time warped DA signals in correct trials. Error bars indicate +/- s.e.m. across mice. Right, first/second day of training DA stimulus responses in correct trials. Error bars indicate 95% confidence interval of average across mice. <bold>g</bold>, Regression of early difference in DA responses to R-L stimuli (average across days 4-8) against late difference in DA responses (average across final 5 days). Each point represents a mouse. <italic>p</italic>-value calculated from the exact distribution of r. <bold>h</bold>, Average difference in DA responses to right and left stimuli (R-L) and difference in DA rewarded outcome responses after R-L stimuli in early days (<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R8">8</xref>) and late days (final 5) for each mouse, c.f. <xref ref-type="fig" rid="F5">Extended Data Fig.2g</xref> for behavioural data. Colours come from average psychometric slope asymmetry (see <xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>). <bold>i</bold>, Difference in DA responses to rewarded outcomes after R-L stimuli over days per mouse (thin) and for the 3 clusters from <xref ref-type="fig" rid="F1">Fig.1j</xref> (thick). Number of days limited to 25 for better visualisation, c.f. <xref ref-type="fig" rid="F2">Fig.2f</xref>.</p></caption><graphic xlink:href="EMS192844-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Fig. 5</label><caption><title>Dopamine signals during learning split by stimulus laterality, response times and trial outcome.</title><p id="P84"><bold>a</bold>, Average DA stimulus-aligned signals (top), time warped signals (middle) and bar plots with average DA stimulus responses (bottom) for rewarded trials from each quartile of an example mouse with bilateral implants split by stimulus laterality w.r.t. implanted fibre. Error bars indicate +/- s.e.m. (top, middle) and 95% confidence interval (bottom) across days. <italic>p</italic>-value calculated from two-sided t-test. <bold>b</bold>, Average DA stimulus-aligned signals (top), time warped signals (middle) and bar plots with average DA stimulus responses (bottom) in rewarded trials for increasing psychometric slope values of all mice split by stimulus laterality with respect to the implanted fibre. Error bars indicate +/- s.e.m. (top, middle) and 95% confidence interval (bottom) across mice. <italic>p</italic>-value calculated from two-sided t-test. <bold>c</bold>, Left, balanced mice average accuracy in days with matched choice accuracy for trials with left (red) and right (blue) stimulus. Error bars indicate 95% confidence interval of average across days. Middle, balanced mice bar plots showing sum of average DA stimulus and rewarded outcome responses in trials with left and right stimuli in matched accuracy sessions. Error bars indicate 95% confidence interval of average across days. Right, balanced mice scatter plot of average DA stimulus responses to right (blue) and left (red) stimuli against accuracy. Each point represents a day. Data points fit with a 3rd degree spline to visualise trend (scipy.interpolate.UnivariateSpline). <bold>d</bold>, Same plots as in c reproduced for one-sided mice. <bold>e</bold>, Left, regression of difference in R-L psychometric slopes against difference in DA responses to R-L stimuli for matched accuracy sessions from one-sided mice. Right, regression of difference in choice accuracy in R-L stimulus trials against difference in DA responses to R-L stimuli for matched accuracy sessions from one-sided mice. Each point represents a recorded hemisphere. <italic>p</italic>-value calculated from the exact distribution of r. <bold>f</bold>, Average DA stimulus-aligned signals (top), time warped signals (middle) and bar plots with average DA stimulus responses (bottom) in rewarded trials for increasing RT values in expert sessions (accuracy &gt; 0.7) of one-sided mice split by trials with the associated or non-associated stimulus. Error bars indicate +/- s.e.m. (top, middle) and 95% confidence interval (bottom) of average across recordings. <italic>p</italic>-value calculated from two-sided t-test. <bold>g</bold>, Average DA stimulus-aligned signals (top), time warped signals (middle) and bar plots with average DA stimulus responses (bottom) across all mice for increasing psychometric slope and a fixed short RT range (0.5-2s) split by correct and error trials. <bold>h</bold>, Average DA stimulus-aligned signals (top), time warped signals (middle) and bar plots with average DA stimulus responses (bottom) across all mice for increasing RT values (same bins as in panel f) and a fixed large psychometric slope range (0.325-1) split by correct and error trials. <sup>*</sup> and <sup>**</sup> indicate <italic>p</italic> &lt; 0.05, <italic>p</italic> &lt; 0.005 in all panels.</p></caption><graphic xlink:href="EMS192844-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Fig. 6</label><caption><title>Further quantification of DLS DA responses to task events during learning.</title><p id="P85"><bold>a</bold>, Difference in DA responses to rewarded outcomes after right and left stimulus (R-L) vs. bias, c.f. <xref ref-type="fig" rid="F2">Fig.2i</xref>. <bold>b</bold>, Difference in DA responses to rewarded outcomes after right and left stimulus (R-L) vs. accuracy, c.f. <xref ref-type="fig" rid="F2">Fig.2j</xref>. <bold>c</bold>, Left, regressions of early (accuracy not significantly greater than 0.5) and late (accuracy not significantly smaller than 0.7) bias and slope difference vs. difference in DA responses to right and left stimuli (R-L). Right, regressions of early and late bias and slope difference vs. difference in DA rewarded outcome responses after right and left stimuli (R-L). Each point represents a day. <italic>p</italic>-value calculated from the exact distribution of r.</p></caption><graphic xlink:href="EMS192844-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Fig. 7</label><caption><title>The tutor-executor network accounts for mouse behavioural trajectories and dopamine signals throughout learning.</title><p id="P86"><bold>a</bold>, c.f. <xref ref-type="fig" rid="F5">Extended Data Fig.2g</xref>, The model’s average bias and slope difference early in training (average across iterations 1000-2000) and late in training (average across final 1000 iterations) for each simulation. Colours come from average psychometric slope asymmetry of each simulation (see <xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>). <bold>b</bold>, c.f. <xref ref-type="fig" rid="F4">Extended Data Fig.1c</xref>, Histogram of the number of iterations that simulations required to reach 70% accuracy. <bold>c</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1b</xref>, Accuracy over days per simulation (grey) and averaged across all simulations (black). Dashed lines indicate chance level (black) and 70% accuracy level (blue). Number of iterations limited to the average simulation length for better visualisation. <bold>d</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1c</xref>, Psychometric curve over quartiles per simulation (grey) and averaged across all simulations (black). Error bars indicate 95% confidence interval across simulations. <bold>e</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1d</xref>, Left, psychometric curves from 3 example simulations over quartiles throughout learning. Right, per simulation (thin) and average expert psychometric curves clustered by trajectory type (thick): right-associating (green), balanced (orange) and left-associating (purple). Cluster labels for each simulation obtained from <xref ref-type="fig" rid="F3">Fig.3f</xref>, colours obtained from average psychometric slope asymmetry (see <xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>). Error bars indicate 95% confidence interval of the average across simulations in each cluster. <bold>f</bold>, c.f. <xref ref-type="fig" rid="F2">Fig.2d</xref>, Average model-derived stimulus and outcome DA responses over deciles in correct trials with stimulus on the left (red) and right (blue) for the three clusters from <xref ref-type="fig" rid="F3">Fig.3f</xref>. Error bars indicate 95% confidence interval of average across simulations. Data points fit with a 3rd degree spline to visualise trends (scipy.interpolate.UnivariateSpline). <bold>g</bold>, c.f. <xref ref-type="fig" rid="F7">Extended Data Fig.4h</xref>, the average difference in model-derived DA responses to right and left stimuli (R-L) and difference in model-derived DA rewarded outcome responses after R-L stimuli early in training (average across iterations 1000-2000) and late in training (average across final 1000 iterations) for each simulation. Colours come from average psychometric slope asymmetry (see <xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>). <bold>h</bold>, c.f. <xref ref-type="fig" rid="F7">Extended Data Fig.4g</xref>, Regression of early difference in model-derived DA responses to R-L stimuli (average across iterations 1000-2000) against late difference in model-derived DA responses (average across final 1000 iterations). Each point represents a simulation. <italic>p</italic>-value is calculated from the exact distribution of r. <bold>i</bold>, c.f. <xref ref-type="fig" rid="F7">Extended Data Fig.4i</xref>, Difference in model-derived DA responses to rewarded outcomes after R-L stimuli over iterations per simulation (thin) and for the 3 clusters from <xref ref-type="fig" rid="F3">Fig.3f</xref> (thick). Number of iterations limited to 5000 for better visualisation. <bold>j</bold>, c.f. <xref ref-type="fig" rid="F9">Extended Data Fig.6a</xref>, Difference in model-derived DA responses to rewarded outcomes after R-L stimulus vs. bias. Stationary points here and in panel k are plotted using the average behaviour and DA predictions arising from their weight configurations. <bold>k</bold>, c.f. <xref ref-type="fig" rid="F9">Extended Data Fig.6b</xref>, Difference in model-derived DA responses to rewarded outcomes after R-L stimulus vs. accuracy.</p></caption><graphic xlink:href="EMS192844-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Fig. 8</label><caption><title>Single-loss gradient descent captures behavioural trajectories with a learning signal different from DLS DA.</title><p id="P87"><bold>a</bold>, Left, schematic of the deep linear ‘single-loss’ RL network architecture and learning rule (Methods). Right, schematic of the stationary point structure with behavioural predictions as well as corresponding network weight configurations. The connecting lines with arrows represent the steepest heteroclinic orbits into/out of each stationary point (Methods). All the stationary points are saddle points except for 4, which is the global minimum. <bold>b</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1f</xref>, Bias over iterations per simulation (thin), for the 3 clusters from panel f (thick), and for the average dynamics (thick dashed). Here, and in panels d and j, the number of iterations is limited to 4667 for better visualisation. Thick dashed lines in all panels indicate analytical trajectories derived from the average dynamics (Methods). <bold>c</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1g</xref>, Regression of early bias (average across iterations 1000-2000) against late bias (average across final 1000 iterations). Each point represents a simulation. <italic>P</italic>-value calculated from the exact distribution of r. <bold>d</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1h</xref>, Difference between right and left psychometric slopes over iterations per simulation (thin) and for the 3 clusters from panel f (thick). <bold>e</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1i</xref>, Regression of early bias (average across iterations 1000-2000) against late slope difference (average across final 1000 iterations). Each point represents a simulation. <italic>P</italic>-value calculated from the exact distribution of r. <bold>f</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1j</xref>, Left vs. right slope over iterations per simulation (thin) and for 3 clusters (thick). Clusters and colours obtained using the same procedure as for the behavioural data in <xref ref-type="fig" rid="F1">Fig.1j</xref>. The clusters from this analysis are used in all other panels. Stationary points here and in panels g-I are plotted using the average behaviour arising from their weight configurations. <bold>g-i</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1k-m</xref>, in order, difference in right and left (R-L) slope vs. bias, R-L slope vs. accuracy and bias vs. accuracy over iterations per simulation (thin) and for the 3 clusters from panel f (thick). <bold>j</bold>, c.f. <xref ref-type="fig" rid="F2">Fig.2f</xref>, Difference in total RPE signals after right and left stimuli (R-L) over iterations per simulation (thin) and for the 3 clusters from <xref ref-type="fig" rid="F3">Fig.3f</xref> (thick). <bold>k</bold>, c.f. <xref rid="F2" ref-type="fig">Fig 2g</xref>, Regression of early stimulus-evoked total RPE (average across iterations 1000-2000) against late slope difference (average across final 1000 iterations). Each point represents a simulation. <italic>p</italic>-value calculated from the exact distribution of r. <bold>l-o</bold>, c.f. <xref ref-type="fig" rid="F2">Fig.2h-k</xref>, in order, right vs. left stim.-evoked total RPEs; difference in total RPE evoked by right and left stimuli (R-L) vs. bias; difference in total RPE evoked by right and left stimuli (R-L) vs. accuracy; and outcome-evoked total RPE signals after right stimulus vs. after left stimulus per simulation (thin) and for the 3 clusters from <xref ref-type="fig" rid="F3">Fig.3f</xref>. Stationary points are plotted using the average total RPEs arising from their weight configurations.</p></caption><graphic xlink:href="EMS192844-f011"/></fig><fig id="F12" position="anchor"><label>Extended Data Fig. 9</label><caption><title>Depth is required for the model to capture the mice’s learning trajectories.</title><p id="P88"><bold>a</bold>, Comparison between the architecture, stationary points, and simulated behavioural and neural trajectories (row for each) of a shallow (left column) and deep (right column) version of the tutor-executor model. Note that <italic>0</italic> is not a stationary point of the shallow network, and is placed in the left panel of the second row for reference. The shallow network has three stationary points which have different network configurations to those of the deep model. They are hence labelled with the increasing numbers <italic>5L, 5R</italic> and <italic>6</italic>, accompanied by schematics of their associated behaviour and neural predictions. These correspond to states where the shallow network is only making left choices (<italic>5L</italic>), right choices (<italic>5R</italic>) and the global optimum (<italic>6</italic>). <bold>b</bold>, Comparison between the architecture, stationary points, behavioural and neural trajectories (row for each) of a shallow (left column) and deep (right column) version of the single-loss gradient descent model. <bold>c</bold>, Comparison of the accuracy over iterations for simulations from a shallow (top) and deep (bottom) version of the tutor-executor model. Thin grey lines show the accuracy curves for each simulation, and the thick black line indicates the average across simulations. <bold>d</bold>, Comparison of the accuracy over iterations for simulations from a shallow (top) and deep (bottom) version of the single-loss gradient descent model. The learning curve of both deep models better captures mice data (c.f. <xref ref-type="fig" rid="F1">Fig.1b</xref>) than the shallow models.</p></caption><graphic xlink:href="EMS192844-f012"/></fig><fig id="F13" position="anchor"><label>Extended Data Fig. 10</label><caption><title>Tutor-executor learning rule causes transfer to striatum.</title><p id="P89"><bold>a</bold>, <italic>W</italic><sup><italic>1</italic></sup> weights from naïve to expert in examples of right- (top), balanced (middle) and left-associating (bottom) analytical average dynamics of the tutor-executor network (same as dashed trajectories in <xref ref-type="fig" rid="F3">Fig.3</xref>). Right- and left-associating trajectories were obtained by initialising the average dynamics with a small degree of left and right bias, whereas the balanced trajectory comes from a network initialised without bias (Methods). <bold>b</bold>, <italic>W</italic><sup><italic>1</italic></sup> weights from naïve to expert in examples of right- (top), balanced (middle) and left-associating (bottom) analytical average dynamics of the single-loss gradient descent network (same as dashed trajectories in <xref ref-type="fig" rid="F11">Extended Data Fig.8</xref>). <bold>c</bold>, Left, stimulus and constant pathway Q-values (derived from the product of elements in <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup>, see <xref ref-type="sec" rid="S6">Methods</xref>) from naïve to overtrained in examples of right- (top), balanced (middle) and left-associating (bottom) analytical average dynamics of the tutor-executor network. Overtrained: trained 8 times longer than the training used for naïve to expert. We do not plot the Q-values of the ‘incorrect’ associations (i.e., left stimulus with right choice and right stimulus with left choice) as these remain around 0. Right, <italic>W</italic><sup><italic>1</italic></sup> and <italic>W</italic><sup><italic>2</italic></sup> weights from naïve to overtrained. Again, we do not plot the weights that connect the inputs with the wrong choices as these remain around 0. <bold>d</bold>, similar to c but for the single-loss gradient descent model.</p></caption><graphic xlink:href="EMS192844-f013"/></fig></sec></body><back><ack id="S36"><title>Acknowledgements</title><p>This work was supported by grants from the Wellcome Trust (213465/Z/18/Z) and ERC (funded by UKRI, EP/X026655/1) to A.L., grants from the Wellcome Trust (216386/Z/19/Z and 219627/Z/19/Z) and the Gatsby Charitable Foundation (GAT3755) to A.S., and grants from BBSRC (BB/S006338/1) and MRC (MC_UU_00003/1) to R.B. A.S. is a CIFAR Azrieli Global Scholar in the Learning in Machines &amp; Brains program. M.F. is supported by a HFSP long-term postdoctoral fellowship.</p></ack><sec id="S37" sec-type="data-availability"><title>Data and Code Availability</title><p id="P90">The data generated in this project, as well as computer codes for data analyses and computational modelling will be shared publicly upon peer-review publication.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P91"><bold>Author Contributions</bold></p><p id="P92">P.Z.H. and A.L. (Armin Lak) conceived and designed the experiment. P.Z.H., Ae.L. (Aeron Laffere), C.T., L.S., S.L.G. and A.L. performed the experiments. Y.L. shared viral constructs. S.L.G., Ae.L., P.Z.H., M.F., J.P., and A.L. analysed the data. S.L.G., R.B., A.L. and A.S. designed the model. S.L.G. and A.S. analysed the model. S.L.G., A.S. and A.L. wrote the manuscript with inputs from R.B., C.T. and M.F..</p></fn><fn fn-type="conflict" id="FN2"><p id="P93"><bold>Competing Interests</bold></p><p id="P94">The authors declare no competing interest.</p></fn><fn id="FN3"><p id="P95"><bold>Materials &amp; Correspondence</bold></p><p id="P96">Correspondence and requests for materials should be addressed to Samuel Liebana Garcia or Armin Lak.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Inhelder</surname><given-names>B</given-names></name><name><surname>Piaget</surname><given-names>J</given-names></name></person-group><source>The growth of logical thinking: From childhood to adolescence</source><publisher-loc>New York</publisher-loc><publisher-name>Basic Books</publisher-name><year>1958</year></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname><given-names>K</given-names></name><name><surname>Ueda</surname><given-names>Y</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name></person-group><article-title>Representation of Action-Specific Reward Values in the Striatum</article-title><source>Science</source><year>2005</year><volume>310</volume><issue>5752</issue><fpage>1337</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Seo</surname><given-names>H</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name></person-group><article-title>Neural Basis of Reinforcement Learning and Decision Making</article-title><source>Annual Review of Neuroscience</source><year>2012</year><volume>35</volume><issue>1</issue><fpage>287</fpage><lpage>308</lpage><pub-id pub-id-type="pmcid">PMC3490621</pub-id><pub-id pub-id-type="pmid">22462543</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150512</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>L</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>The basal ganglia’s contributions to perceptual decision making</article-title><source>Neuron</source><year>2013</year><volume>79</volume><issue>4</issue><fpage>640</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC3771079</pub-id><pub-id pub-id-type="pmid">23972593</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.042</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makino</surname><given-names>H</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><article-title>Learning enhances the relative impact of top-down processing in the visual cortex</article-title><source>Nat Neurosci</source><year>2015</year><month>Aug</month><volume>18</volume><issue>8</issue><fpage>1116</fpage><lpage>22</lpage><pub-id pub-id-type="pmcid">PMC4523093</pub-id><pub-id pub-id-type="pmid">26167904</pub-id><pub-id pub-id-type="doi">10.1038/nn.4061</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Khan</surname><given-names>AG</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Nemri</surname><given-names>A</given-names></name><name><surname>Orsolic</surname><given-names>I</given-names></name><name><surname>Krupic</surname><given-names>J</given-names></name><etal/></person-group><article-title>Learning Enhances Sensory and Multiple Non-sensory Representations in Primary Visual Cortex</article-title><source>Neuron</source><year>2015</year><volume>86</volume><issue>6</issue><fpage>1478</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC4503798</pub-id><pub-id pub-id-type="pmid">26051421</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.05.037</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname><given-names>BA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name></person-group><article-title>Neural Mechanisms of Post-error Adjustments of Decision Policy in Parietal Cortex</article-title><source>Neuron</source><year>2016</year><volume>89</volume><issue>3</issue><fpage>658</fpage><lpage>71</lpage><pub-id pub-id-type="pmcid">PMC4742416</pub-id><pub-id pub-id-type="pmid">26804992</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.027</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Merre</surname><given-names>P</given-names></name><name><surname>Esmaeili</surname><given-names>V</given-names></name><name><surname>Charriere</surname><given-names>E</given-names></name><name><surname>Galan</surname><given-names>K</given-names></name><name><surname>Salin</surname><given-names>PA</given-names></name><name><surname>Petersen</surname><given-names>CCH</given-names></name><etal/></person-group><article-title>Reward-Based Learning Drives Rapid Sensory Signals in Medial Prefrontal Cortex and Dorsal Hippocampus Necessary for Goal-Directed Behavior</article-title><source>Neuron</source><year>2018</year><volume>97</volume><issue>1</issue><fpage>83</fpage><lpage>91</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC5766832</pub-id><pub-id pub-id-type="pmid">29249287</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.031</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iigaya</surname><given-names>K</given-names></name><name><surname>Fonseca</surname><given-names>MS</given-names></name><name><surname>Murakami</surname><given-names>M</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>An effect of serotonergic stimulation on learning rates for rewards apparent after long intertrial intervals</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><issue>1</issue><elocation-id>2477</elocation-id><pub-id pub-id-type="pmcid">PMC6018802</pub-id><pub-id pub-id-type="pmid">29946069</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-04840-2</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinople</surname><given-names>CM</given-names></name><name><surname>Piet</surname><given-names>AT</given-names></name><name><surname>Bibawi</surname><given-names>P</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Kopec</surname><given-names>C</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Lateral orbitofrontal cortex promotes trial-by-trial learning of risky, but not spatial, biases</article-title><source>eLife</source><year>2019</year><month>Nov</month><day>6</day><volume>8</volume><pub-id pub-id-type="pmcid">PMC6834367</pub-id><pub-id pub-id-type="pmid">31692447</pub-id><pub-id pub-id-type="doi">10.7554/eLife.49744</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinert</surname><given-names>S</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Goltstein</surname><given-names>PM</given-names></name></person-group><article-title>Mouse prefrontal cortex represents learned rules for categorization</article-title><source>Nature</source><year>2021</year><volume>593</volume><issue>7859</issue><fpage>411</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC8131197</pub-id><pub-id pub-id-type="pmid">33883745</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-03452-z</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>Value representations in the rodent orbitofrontal cortex drive learning, not choice</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e64575</elocation-id><pub-id pub-id-type="pmcid">PMC9462853</pub-id><pub-id pub-id-type="pmid">35975792</pub-id><pub-id pub-id-type="doi">10.7554/eLife.64575</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Winnubst</surname><given-names>J</given-names></name><name><surname>Natrajan</surname><given-names>M</given-names></name><name><surname>Lai</surname><given-names>C</given-names></name><name><surname>Kajikawa</surname><given-names>K</given-names></name><name><surname>Michaelos</surname><given-names>M</given-names></name><etal/></person-group><article-title>Learning produces a hippocampal cognitive map in the form of an orthogonalized state machine</article-title><source>bioRxiv</source><year>2023</year></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT press</publisher-name><year>1998</year></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><year>1997</year><month>Mar</month><volume>275</volume><issue>5306</issue><fpage>1593</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Vong</surname><given-names>L</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>Nature</source><year>2012</year><volume>482</volume><issue>7383</issue><fpage>85</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC3271183</pub-id><pub-id pub-id-type="pmid">22258508</pub-id><pub-id pub-id-type="doi">10.1038/nature10754</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ljungberg</surname><given-names>T</given-names></name><name><surname>Apicella</surname><given-names>P</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Responses of Monkey Dopamine Neurons during Learning of Behavioral Reactions</article-title><source>J Neurophysiol</source><year>1992</year><volume>67</volume><issue>1</issue><fpage>145</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">1552316</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bayer</surname><given-names>HM</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><article-title>Midbrain Dopamine Neurons Encode a Quantitative Reward Prediction Error Signal</article-title><source>Neuron</source><year>2005</year><volume>47</volume><issue>1</issue><fpage>129</fpage><lpage>41</lpage><pub-id pub-id-type="pmcid">PMC1564381</pub-id><pub-id pub-id-type="pmid">15996553</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.020</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morris</surname><given-names>G</given-names></name><name><surname>Nevet</surname><given-names>A</given-names></name><name><surname>Arkadir</surname><given-names>D</given-names></name><name><surname>Vaadia</surname><given-names>E</given-names></name><name><surname>Bergman</surname><given-names>H</given-names></name></person-group><article-title>Midbrain dopamine neurons encode decisions for future action</article-title><source>Nature Neuroscience</source><year>2006</year><volume>9</volume><issue>8</issue><fpage>1057</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">16862149</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bromberg-Martin</surname><given-names>ES</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><article-title>Midbrain Dopamine Neurons Signal Preference for Advance Information about Upcoming Rewards</article-title><source>Neuron</source><year>2009</year><volume>63</volume><issue>1</issue><fpage>119</fpage><lpage>26</lpage><pub-id pub-id-type="pmcid">PMC2723053</pub-id><pub-id pub-id-type="pmid">19607797</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.06.009</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>X</given-names></name><name><surname>Costa</surname><given-names>RM</given-names></name></person-group><article-title>Start/stop signals emerge in nigrostriatal circuits during sequence learning</article-title><source>Nature</source><year>2010</year><volume>466</volume><issue>7305</issue><fpage>457</fpage><lpage>62</lpage><pub-id pub-id-type="pmcid">PMC3477867</pub-id><pub-id pub-id-type="pmid">20651684</pub-id><pub-id pub-id-type="doi">10.1038/nature09263</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flagel</surname><given-names>SB</given-names></name><name><surname>Clark</surname><given-names>JJ</given-names></name><name><surname>Robinson</surname><given-names>TE</given-names></name><name><surname>Mayo</surname><given-names>L</given-names></name><name><surname>Czuj</surname><given-names>A</given-names></name><name><surname>Willuhn</surname><given-names>I</given-names></name><etal/></person-group><article-title>A selective role for dopamine in stimulus– reward learning</article-title><source>Nature</source><year>2011</year><volume>469</volume><issue>7328</issue><fpage>53</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3058375</pub-id><pub-id pub-id-type="pmid">21150898</pub-id><pub-id pub-id-type="doi">10.1038/nature09588</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Stauffer</surname><given-names>WR</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Dopamine neurons learn relative chosen value from probabilistic rewards</article-title><source>eLife</source><year>2016</year><volume>27</volume><fpage>5</fpage><pub-id pub-id-type="pmcid">PMC5116238</pub-id><pub-id pub-id-type="pmid">27787196</pub-id><pub-id pub-id-type="doi">10.7554/eLife.18044</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parker</surname><given-names>NF</given-names></name><name><surname>Cameron</surname><given-names>CM</given-names></name><name><surname>Taliaferro</surname><given-names>JP</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Choi</surname><given-names>JY</given-names></name><name><surname>Davidson</surname><given-names>TJ</given-names></name><etal/></person-group><article-title>Reward and choice encoding in terminals of midbrain dopamine neurons depends on striatal target</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><issue>6</issue><fpage>845</fpage><lpage>54</lpage><pub-id pub-id-type="pmcid">PMC4882228</pub-id><pub-id pub-id-type="pmid">27110917</pub-id><pub-id pub-id-type="doi">10.1038/nn.4287</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><article-title>Midbrain dopamine neurons control judgment of time</article-title><source>Science</source><year>2016</year><day>9</day><volume>354</volume><issue>6317</issue><fpage>1273</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">27940870</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Babayan</surname><given-names>BM</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><article-title>Opposite initialization to novel cues in dopamine signaling in ventral and posterior striatum in mice</article-title><source>eLife</source><year>2017</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC5271609</pub-id><pub-id pub-id-type="pmid">28054919</pub-id><pub-id pub-id-type="doi">10.7554/eLife.21886</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>BT</given-names></name><name><surname>Richard</surname><given-names>JM</given-names></name><name><surname>Margolis</surname><given-names>EB</given-names></name><name><surname>Janak</surname><given-names>PH</given-names></name></person-group><article-title>Dopamine neurons create Pavlovian conditioned stimuli with circuit-defined motivational properties</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><issue>8</issue><fpage>1072</fpage><lpage>83</lpage><pub-id pub-id-type="pmcid">PMC6082399</pub-id><pub-id pub-id-type="pmid">30038277</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0191-4</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohebi</surname><given-names>A</given-names></name><name><surname>Pettibone</surname><given-names>JR</given-names></name><name><surname>Hamid</surname><given-names>AA</given-names></name><name><surname>Wong</surname><given-names>JT</given-names></name><name><surname>Vinson</surname><given-names>LT</given-names></name><name><surname>Patriarchi</surname><given-names>T</given-names></name><etal/></person-group><article-title>Dissociable dopamine dynamics for learning and motivation</article-title><source>Nature</source><year>2019</year><volume>570</volume><issue>7759</issue><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="pmcid">PMC6555489</pub-id><pub-id pub-id-type="pmid">31118513</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1235-y</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Finkelstein</surname><given-names>J</given-names></name><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Fleming</surname><given-names>W</given-names></name><name><surname>Jang</surname><given-names>HJ</given-names></name><name><surname>Ornelas</surname><given-names>S</given-names></name><etal/></person-group><article-title>Specialized coding of sensory, motor and cognitive variables in VTA dopamine neurons</article-title><source>Nature</source><year>2019</year><volume>570</volume><issue>7762</issue><fpage>509</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC7147811</pub-id><pub-id pub-id-type="pmid">31142844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1261-9</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coddington</surname><given-names>LT</given-names></name><name><surname>Lindo</surname><given-names>SE</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name></person-group><article-title>Mesolimbic dopamine adapts the rate of learning from action</article-title><source>Nature</source><year>2023</year><day>9</day><volume>614</volume><issue>7947</issue><fpage>294</fpage><lpage>302</lpage><pub-id pub-id-type="pmcid">PMC9908546</pub-id><pub-id pub-id-type="pmid">36653450</pub-id><pub-id pub-id-type="doi">10.1038/s41586-022-05614-z</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balleine</surname><given-names>BW</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name><name><surname>Hikosaka</surname><given-names>O</given-names></name></person-group><article-title>The Role of the Dorsal Striatum in Reward and Decision-Making: Figure 1</article-title><source>J Neurosci</source><year>2007</year><day>1</day><volume>27</volume><issue>31</issue><fpage>8161</fpage><lpage>5</lpage><pub-id pub-id-type="pmcid">PMC6673072</pub-id><pub-id pub-id-type="pmid">17670959</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1554-07.2007</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name></person-group><article-title>Striatal circuits for reward learning and decision-making</article-title><source>Nat Rev Neurosci</source><year>2019</year><volume>20</volume><issue>8</issue><fpage>482</fpage><lpage>94</lpage><pub-id pub-id-type="pmcid">PMC7231228</pub-id><pub-id pub-id-type="pmid">31171839</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0189-2</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burgess</surname><given-names>CP</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Steinmetz</surname><given-names>NA</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Reddy</surname><given-names>Bai</given-names></name><name><surname>Jacobs</surname><given-names>EAK</given-names></name><etal/></person-group><article-title>High-Yield Methods for Accurate Two-Alternative Visual Psychophysics in Head-Fixed Mice</article-title><source>Cell Rep</source><year>2017</year><volume>20</volume><issue>10</issue><fpage>2513</fpage><lpage>24</lpage><pub-id pub-id-type="pmcid">PMC5603732</pub-id><pub-id pub-id-type="pmid">28877482</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2017.08.047</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>F</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Dai</surname><given-names>B</given-names></name><name><surname>Qian</surname><given-names>T</given-names></name><name><surname>Zeng</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>Next-generation GRAB sensors for monitoring dopaminergic activity in vivo</article-title><source>Nat Methods</source><year>2020</year><volume>17</volume><issue>11</issue><fpage>1156</fpage><lpage>66</lpage><pub-id pub-id-type="pmcid">PMC7648260</pub-id><pub-id pub-id-type="pmid">33087905</pub-id><pub-id pub-id-type="doi">10.1038/s41592-020-00981-9</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrens</surname><given-names>J</given-names></name><name><surname>Aydin</surname><given-names>C</given-names></name><name><surname>Janse van Rensburg</surname><given-names>A</given-names></name><name><surname>Esquivelzeta Rabell</surname><given-names>J</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name></person-group><article-title>Cue-Evoked Dopamine Promotes Conditioned Responding during Learning</article-title><source>Neuron</source><year>2020</year><day>8</day><volume>106</volume><issue>1</issue><fpage>142</fpage><lpage>153</lpage><elocation-id>e7</elocation-id><pub-id pub-id-type="pmid">32027824</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldi</surname><given-names>P</given-names></name><name><surname>Hornik</surname><given-names>K</given-names></name></person-group><article-title>Neural networks and principal component analysis: Learning from examples without local minima</article-title><source>Neural Networks</source><year>1989</year><volume>2</volume><issue>1</issue><fpage>53</fpage><lpage>8</lpage></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>AM</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><article-title>Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</article-title><source>arXiv</source><year>2014</year></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>ZV</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Ophir</surname><given-names>E</given-names></name><name><surname>Gutnisky</surname><given-names>D</given-names></name><name><surname>Ting</surname><given-names>JT</given-names></name><etal/></person-group><article-title>Flow of cortical activity underlying a tactile decision in mice</article-title><source>Neuron</source><year>2014</year><day>8</day><volume>81</volume><issue>1</issue><fpage>179</fpage><lpage>94</lpage><pub-id pub-id-type="pmcid">PMC3984938</pub-id><pub-id pub-id-type="pmid">24361077</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2013.10.020</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Stone</surname><given-names>IR</given-names></name><collab>The International Brain Laboratory</collab><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><etal/></person-group><article-title>Mice alternate between discrete strategies during perceptual decision-making</article-title><source>Nat Neurosci</source><year>2022</year><volume>25</volume><issue>2</issue><fpage>201</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC8890994</pub-id><pub-id pub-id-type="pmid">35132235</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname><given-names>MW</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><article-title>Rapid signalling in distinct dopaminergic axons during locomotion and reward</article-title><source>Nature</source><year>2016</year><day>28</day><volume>535</volume><issue>7613</issue><fpage>505</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC4970879</pub-id><pub-id pub-id-type="pmid">27398617</pub-id><pub-id pub-id-type="doi">10.1038/nature18942</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><article-title>Dopamine role in learning and action inference</article-title><source>eLife</source><year>2020</year><day>7</day><volume>9</volume><elocation-id>e53262</elocation-id><pub-id pub-id-type="pmcid">PMC7392608</pub-id><pub-id pub-id-type="pmid">32633715</pub-id><pub-id pub-id-type="doi">10.7554/eLife.53262</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Elzelingen</surname><given-names>W</given-names></name><name><surname>Goedhoop</surname><given-names>J</given-names></name><name><surname>Warnaar</surname><given-names>P</given-names></name><name><surname>Denys</surname><given-names>D</given-names></name><name><surname>Arbab</surname><given-names>T</given-names></name><name><surname>Willuhn</surname><given-names>I</given-names></name></person-group><article-title>A unidirectional but not uniform striatal landscape of dopamine signaling for motivational stimuli</article-title><source>Proc Natl Acad Sci USA</source><year>2022</year><month>May</month><day>24</day><volume>119</volume><issue>21</issue><elocation-id>e2117270119</elocation-id><pub-id pub-id-type="pmcid">PMC9171911</pub-id><pub-id pub-id-type="pmid">35594399</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2117270119</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Sagiv</surname><given-names>Y</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A feature-specific prediction error model explains dopaminergic heterogeneity</article-title><source>bioRxiv</source><year>2022</year></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moss</surname><given-names>MM</given-names></name><name><surname>Zatka-Haas</surname><given-names>P</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name></person-group><article-title>Dopamine Axons in Dorsal Striatum Encode Contralateral Visual Stimuli and Choices</article-title><source>J Neurosci</source><year>2021</year><day>25</day><volume>41</volume><issue>34</issue><fpage>7197</fpage><lpage>205</lpage><pub-id pub-id-type="pmcid">PMC8387116</pub-id><pub-id pub-id-type="pmid">34253628</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0490-21.2021</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunnicutt</surname><given-names>BJ</given-names></name><name><surname>Jongbloets</surname><given-names>BC</given-names></name><name><surname>Birdsong</surname><given-names>WT</given-names></name><name><surname>Gertz</surname><given-names>KJ</given-names></name><name><surname>Zhong</surname><given-names>H</given-names></name><name><surname>Mao</surname><given-names>T</given-names></name></person-group><article-title>A comprehensive excitatory input map of the striatum reveals novel functional organization</article-title><source>eLife</source><year>2016</year><volume>28</volume><fpage>5</fpage><pub-id pub-id-type="pmcid">PMC5207773</pub-id><pub-id pub-id-type="pmid">27892854</pub-id><pub-id pub-id-type="doi">10.7554/eLife.19103</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Neuronal reward and decision signals: from theories to data</article-title><source>Physiological Review</source><year>2015</year><volume>95</volume><fpage>853</fpage><lpage>951</lpage><pub-id pub-id-type="pmcid">PMC4491543</pub-id><pub-id pub-id-type="pmid">26109341</pub-id><pub-id pub-id-type="doi">10.1152/physrev.00023.2014</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>AJ</given-names></name><name><surname>Sharpe</surname><given-names>MJ</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>Model-based predictions for dopamine</article-title><source>Current Opinion in Neurobiology</source><year>2018</year><volume>49</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC6034703</pub-id><pub-id pub-id-type="pmid">29096115</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2017.10.006</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JN</given-names></name><name><surname>Hyland</surname><given-names>BI</given-names></name><name><surname>Wickens</surname><given-names>JR</given-names></name></person-group><article-title>A cellular mechanism of reward-related learning</article-title><source>Nature</source><year>2001</year><day>6</day><volume>413</volume><issue>6851</issue><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">11544526</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haber</surname><given-names>SN</given-names></name><name><surname>Knutson</surname><given-names>B</given-names></name></person-group><article-title>The Reward Circuit: Linking Primate Anatomy and Human Imaging</article-title><source>Neuropsychopharmacology</source><year>2009</year><volume>35</volume><issue>1</issue><fpage>4</fpage><lpage>26</lpage><pub-id pub-id-type="pmcid">PMC3055449</pub-id><pub-id pub-id-type="pmid">19812543</pub-id><pub-id pub-id-type="doi">10.1038/npp.2009.129</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahiri</surname><given-names>AK</given-names></name><name><surname>Bevan</surname><given-names>MD</given-names></name></person-group><article-title>Dopaminergic Transmission Rapidly and Persistently Enhances Excitability of D1 Receptor-Expressing Striatal Projection Neurons</article-title><source>Neuron</source><year>2020</year><volume>106</volume><issue>2</issue><fpage>277</fpage><lpage>290</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC7182485</pub-id><pub-id pub-id-type="pmid">32075716</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.01.028</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ambrosi</surname><given-names>P</given-names></name><name><surname>Lerner</surname><given-names>TN</given-names></name></person-group><article-title>Striatonigrostriatal circuit architecture for disinhibition of dopamine signaling</article-title><source>Cell Reports</source><year>2022</year><volume>40</volume><issue>7</issue><elocation-id>111228</elocation-id><pub-id pub-id-type="pmcid">PMC9425427</pub-id><pub-id pub-id-type="pmid">35977498</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.111228</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Moss</surname><given-names>MM</given-names></name><name><surname>Gurnani</surname><given-names>H</given-names></name><name><surname>Farrell</surname><given-names>K</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Dopaminergic and Prefrontal Basis of Learning from Sensory Confidence and Reward Value</article-title><source>Neuron</source><year>2020</year><volume>105</volume><issue>4</issue><fpage>700</fpage><lpage>711</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC7031700</pub-id><pub-id pub-id-type="pmid">31859030</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.11.018</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarno</surname><given-names>S</given-names></name><name><surname>De Lafuente</surname><given-names>V</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name></person-group><article-title>Dopamine reward prediction error signal codes the temporal evaluation of a perceptual decision report</article-title><source>Proc Natl Acad Sci USA</source><year>2017</year><day>28</day><volume>114</volume><issue>48</issue><pub-id pub-id-type="pmcid">PMC5715768</pub-id><pub-id pub-id-type="pmid">29133424</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1712479114</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Nomoto</surname><given-names>K</given-names></name><name><surname>Keramati</surname><given-names>M</given-names></name><name><surname>Sakagami</surname><given-names>M</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><article-title>Midbrain Dopamine Neurons Signal Belief in Choice Accuracy during a Perceptual Decision</article-title><source>Curr Biol</source><year>2017</year><day>20</day><volume>27</volume><issue>6</issue><fpage>821</fpage><lpage>32</lpage><pub-id pub-id-type="pmcid">PMC5819757</pub-id><pub-id pub-id-type="pmid">28285994</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2017.02.026</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><article-title>Neural Basis of a Perceptual Decision in the Parietal Cortex (Area LIP) of the Rhesus Monkey</article-title><source>Journal of Neurophysiology</source><year>2001</year><day>1</day><volume>86</volume><issue>4</issue><fpage>1916</fpage><lpage>36</lpage><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>The parallel distributed processing approach to semantic cognition</article-title><source>Nat Rev Neurosci</source><year>2003</year><month>Apr</month><day>1</day><volume>4</volume><issue>4</issue><fpage>310</fpage><lpage>22</lpage><pub-id pub-id-type="pmid">12671647</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxe</surname><given-names>AM</given-names></name><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><article-title>A mathematical theory of semantic development in deep neural networks</article-title><source>Proc Natl Acad Sci U S A</source><year>2019</year><day>4</day><volume>116</volume><issue>23</issue><fpage>11537</fpage><lpage>46</lpage><pub-id pub-id-type="pmcid">PMC6561300</pub-id><pub-id pub-id-type="pmid">31101713</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1820226116</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawai</surname><given-names>R</given-names></name><name><surname>Markman</surname><given-names>T</given-names></name><name><surname>Poddar</surname><given-names>R</given-names></name><name><surname>Ko</surname><given-names>R</given-names></name><name><surname>Fantana</surname><given-names>AL</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><etal/></person-group><article-title>Motor Cortex Is Required for Learning but Not for Executing a Motor Skill</article-title><source>Neuron</source><year>2015</year><volume>86</volume><issue>3</issue><fpage>800</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC5939934</pub-id><pub-id pub-id-type="pmid">25892304</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.024</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graybiel</surname><given-names>AM</given-names></name><name><surname>Grafton</surname><given-names>ST</given-names></name></person-group><article-title>The Striatum: Where Skills and Habits Meet</article-title><source>Cold Spring Harb Perspect Biol</source><year>2015</year><volume>7</volume><issue>8</issue><elocation-id>a021691</elocation-id><pub-id pub-id-type="pmcid">PMC4526748</pub-id><pub-id pub-id-type="pmid">26238359</pub-id><pub-id pub-id-type="doi">10.1101/cshperspect.a021691</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhagat</surname><given-names>J</given-names></name><name><surname>Wells</surname><given-names>MJ</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Burgess</surname><given-names>CP</given-names></name></person-group><article-title>Rigbox: An Open-Source Toolbox for Probing Neurons and Behavior</article-title><source>eNeuro</source><year>2020</year><month>Jun</month><day>3</day><pub-id pub-id-type="pmcid">PMC7363478</pub-id><pub-id pub-id-type="pmid">32493756</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0406-19.2020</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akam</surname><given-names>T</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name></person-group><article-title>pyPhotometry: Open source Python based hardware and software for fiber photometry data acquisition</article-title><source>Sci Rep</source><year>2019</year><day>5</day><volume>9</volume><issue>1</issue><elocation-id>3521</elocation-id><pub-id pub-id-type="pmcid">PMC6401057</pub-id><pub-id pub-id-type="pmid">30837543</pub-id><pub-id pub-id-type="doi">10.1038/s41598-019-39724-y</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><etal/></person-group><article-title>Scikit-learn: Machine Learning in Python</article-title><source>arXiv</source><year>2012</year></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavenard</surname><given-names>R</given-names></name><name><surname>Faouzi</surname><given-names>J</given-names></name><name><surname>Vandewiele</surname><given-names>G</given-names></name></person-group><article-title>Tslearn,A Machine Learning Toolkit for Time Series Data</article-title><source>Journal of Machine Learning Research</source><year>2020</year><fpage>1</fpage><lpage>6</lpage></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><etal/></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><issue>9</issue><fpage>1281</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><etal/></person-group><article-title>A deep learning framework for neuroscience</article-title><source>Nat Neurosci</source><year>2019</year><month>Nov</month><volume>22</volume><issue>11</issue><fpage>1761</fpage><lpage>70</lpage><comment>2019/10/30</comment><pub-id pub-id-type="pmcid">PMC7115933</pub-id><pub-id pub-id-type="pmid">31659335</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinan</surname><given-names>E</given-names></name><name><surname>Weiqing</surname><given-names>R</given-names></name><name><surname>Vanden-Eijnden</surname><given-names>E</given-names></name></person-group><article-title>Minimum action method for the study of rare events</article-title><source>Comm Pure Appl Math</source><year>2004</year><volume>57</volume><issue>5</issue><fpage>637</fpage><lpage>56</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig 1</label><caption><title>Long-term learning trajectories are diverse yet systematic.</title><p><bold>a</bold>, The visual decision task for head-fixed mice (<italic>n</italic>=28 mice; Methods). <bold>b</bold>, Accuracy over days per mouse (grey) and averaged across all mice (black). Dashed lines indicate chance level (black) and 70% accuracy level (blue). Number of days limited to 30 for better visualisation. <bold>c</bold>, Psychometric curve over quartiles per mouse (grey) and averaged across all mice (black). Quartiles are defined per mouse by dividing days into 4 groups, with any remainder added to the last group. Negative (positive) contrast values indicate stimuli presented on the left (right) side of the screen. P(‘Right’) indicates the probability of reporting ‘right’ side stimulus position (i.e., turning the wheel anticlockwise). Error bars indicate the 95% confidence interval across mice. <bold>d</bold>, Left, psychometric curves from 3 example mice on 4 example days throughout learning. Error bars indicate the 95% confidence interval of a two-sided binomial test on P(‘Right’). See <xref ref-type="fig" rid="F5">Extended Data Fig.2a</xref> for corresponding chronometric curves. Right, per mouse (thin) and average expert psychometric curves clustered by trajectory type (thick): right-associating (green), balanced (orange) and left-associating (purple). See <xref ref-type="fig" rid="F5">Extended Data Fig.2b</xref> for corresponding chronometric curves. Cluster labels for each mouse were obtained from panel j, and colours were obtained from <xref ref-type="fig" rid="F5">Extended Data Fig.2c</xref>. Error bars indicate 95% confidence interval across mice in each cluster. <bold>e</bold>, Schematic explaining the metrics used to study behaviour. Left (right) slope is defined as the absolute difference between the P(‘Right’) for left (right) stimulus and zero-contrast trials. Bias is defined as the difference between zero-contrast P(‘Right’) and balanced choice probability (i.e., 0.5), thus indicating the choice preference on zero-contrast trials. <bold>f</bold>, Bias over days per mouse (thin) and for the 3 clusters from panel j (thick). In right- and left-associating mice, biases increase before reversing (<italic>p</italic>&lt;0.05; t-test first 2 days vs. days 5-6). Number of days limited to 25 for better visualisation. <bold>g</bold>, Regression of early bias (average across days 4-8) against late bias (average across final 5 days). Each point represents a mouse. <italic>p</italic>-value calculated from the exact distribution of r (stats.scipy.pearsonr). <bold>h</bold>, Difference between right and left psychometric slopes over days per mouse (thin) and for the 3 clusters from panel j (thick). Number of days limited to 25 for better visualisation. <bold>i</bold>, Regression of early bias (average across days 4-8) against late slope difference (average across final 5 days). Each point represents a mouse. <italic>p</italic>-value from the exact distribution of r (stats.scipy.pearsonr). <bold>j</bold>, Left vs. right slope across days per mouse (thin) and for 3 clusters (thick). Cyan dots indicate the beginning of learning and navy dots indicate the end. The hue of the cluster lines changes from dark to light indicating progress through learning. Left-associating (purple), balanced (orange) and right-associating (green) clusters are obtained from dynamic time warping clustering (Methods) applied to the mouse trajectories. The clusters from this analysis are used in all other panels. <bold>k-m</bold>, In order, difference in right and left (R-L) slope vs. bias; R-L slope vs. accuracy and bias vs. accuracy across days per mouse (thin) and for the 3 clusters from panel j (thick).</p></caption><graphic xlink:href="EMS192844-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><title>DLS DA encodes stimulus-choice association and reflects learning trajectories.</title><p><bold>a</bold>, Fibre photometry set-up for recording dopamine (DA) release in dorsolateral striatum (DLS). Recorded DA levels were normalised to correct for non-task-relevant day-by-day variations in fluorescence, see <xref ref-type="sec" rid="S6">Methods</xref>. <bold>b</bold>, Accuracy over days and simultaneous trial-wise stim.-aligned DA recordings from an example mouse (only correct trials). Blue lines indicate 5-trial moving average of the time when the stimulus is brought to the centre (i.e., choice completion, top) and stimulus onset time (bottom). <bold>c</bold>, Average time warped DA signals in correct trials with stimulus on the left (red) and right (blue) for the same days and mice as in <xref ref-type="fig" rid="F1">Fig.1d</xref> (psychometric curves reproduced as insets). Vertical dashed lines indicate stim. onset, stim. centre, and reward delivery time. Error bars indicate +/- s.e.m. across trials. <bold>d</bold>, Average stimulus and outcome DA responses over deciles in correct trials with stimulus on the left (red) and right (blue) for the three clusters from <xref ref-type="fig" rid="F1">Fig.1j</xref>. Stim. and outcome responses were calculated by computing the maximum of smoothened fluorescence in an analysis window (see <xref ref-type="sec" rid="S6">Methods</xref>). Outcome responses were defined as the sum of responses to stim. centre and reward delivery. Error bars indicate 95% confidence interval of average across mice. Data points fit with a 3rd degree spline to visualise trends (scipy.interpolate.UnivariateSpline). <bold>e</bold>, Left, average time warped DA signals in correct trials with stimulus on the left (red) and right (blue) and average psychometric curves for the three clusters from <xref ref-type="fig" rid="F1">Fig.1j</xref> in early days (accuracy n.s. greater than 0.5). Right, average time warped DA signals and psychometric curves in late days (accuracy n.s. smaller than 0.7). Error bars indicate s.e.m. across mice. Day-by-day regression analyses confirmed these results (<xref ref-type="fig" rid="F9">Extended Data Fig.6c</xref>). <bold>f</bold>, Difference in DA responses to right and left stimuli (R-L) over days per mouse (thin) and for the 3 clusters from <xref ref-type="fig" rid="F1">Fig.1j</xref> (thick). Number of days limited to 25 for better visualisation. <bold>g</bold>, Regression of early difference in DA responses to R-L stimuli (average across days 4-8) against late R-L slope difference (average across final 5 days). Each point represents a mouse. <italic>p</italic>-value is calculated from the exact distribution of r. <bold>h-k</bold>, In order, right vs. left DA stim. responses; difference in DA responses to right and left stimuli (R-L) vs. bias; difference in DA responses to right and left stimuli (R-L) vs. accuracy; and DA rewarded outcome responses after right stimulus vs. after left stimulus per mouse (thin) and for the 3 clusters from <xref ref-type="fig" rid="F1">Fig.1j</xref>. Cyan dots indicate the beginning of learning and navy dots indicate the end. The hue of the cluster lines changes from dark to light indicating progress through learning. <bold>l</bold>, Analysis of balanced mice DA responses in days where the difference in left and right stimulus trial choice accuracy is &lt;0.1; Left, average psychometric curve from matched accuracy days. Middle, average time warped DA signals for correct trials with stimulus on the left (red) and right (blue). Right, bar plot showing the average DA responses to stimuli for correct left and right stim trials; <italic>p</italic> &gt; 0.1, estimated using two-sided t-test. Error bars indicate 95% confidence interval of average across days. <bold>m</bold>, Same matched accuracy analysis as in <xref ref-type="fig" rid="F2">Fig.2l</xref> but applied to left- and right-associating mice. * indicates <italic>p &lt;</italic> 0.05.</p></caption><graphic xlink:href="EMS192844-f002"/></fig><fig id="F3" position="float"><label>Fig 3</label><caption><title>A deep linear neural network model accounts for behaviour and dopamine signals throughout learning.</title><p><bold>a</bold>, Left, schematic of the deep linear ‘tutor-executor’ RL network architecture and learning rule (Methods). Right, schematic of the stationary point structure including behavioural and neural predictions as well as corresponding network weight configurations. The connecting lines with arrows represent the steepest heteroclinic orbits into/out of each stationary point (Methods). All the stationary points are saddle points except for 4, which is the global minimum. <bold>b</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1f</xref>, Bias over iterations per simulation (thin), for the 3 clusters from panel f (thick), and for the average dynamics (thick dashed). Here, and in panels d and j, the number of iterations is limited to 5000 for better visualisation. Thick dashed lines in all panels indicate trajectories derived from the analytical average dynamics (Methods). <bold>c</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1g</xref>, Regression of early bias (average across iterations 1000-2000) against late bias (average across final 1000 iterations). Each point represents a simulation. <italic>p</italic>-value is calculated from the exact distribution of r. <bold>d</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1h</xref>, Difference between right and left psychometric slopes over iterations per simulation (thin) and for the 3 clusters from panel f (thick). <bold>e</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1i</xref>, Regression of early bias (average across iterations 1000-2000) against late slope difference (average across final 1000 iterations). Each point represents a simulation. <italic>p</italic>-value is calculated from the exact distribution of r. <bold>f</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1j</xref>, Left vs. right slope over iterations per simulation (thin) and for 3 clusters (thick). Clusters and colours obtained using the same procedure as for the behavioural data in <xref ref-type="fig" rid="F1">Fig.1j</xref>. The clusters from this analysis are used in all other panels. Stationary points here and in panels g-i are plotted using the average behaviour arising from their weight configurations. <bold>g-i</bold>, c.f. <xref ref-type="fig" rid="F1">Fig.1k-m</xref>, in order, difference in right and left (R-L) slope vs. bias, R-L slope vs. accuracy, and bias vs. accuracy over iterations per simulation (thin) and for the 3 clusters from panel f (thick). <bold>j</bold>, c.f. <xref ref-type="fig" rid="F2">Fig.2f</xref>, Difference in DA responses to right and left stimuli (R-L) over iterations per simulation (thin) and for the 3 clusters from panel f (thick). <bold>k</bold>, c.f. <xref ref-type="fig" rid="F2">Fig 2g</xref>, Regression of early difference in DA responses to R-L stimuli (average across iterations 1000-2000) against late slope difference (average across final 1000 iterations). Each point represents a simulation. <italic>p</italic>-value is calculated from the exact distribution of r. <bold>l-o</bold>, c.f. <xref ref-type="fig" rid="F2">Fig.2h-k</xref>, in order, right vs. left DA stim. responses; difference in DA responses to right and left stimuli (R-L) vs. bias; difference in DA responses to right and left stimuli (R-L) vs. accuracy; and DA rewarded outcome responses after right stimulus vs. after left stimulus per simulation (thin) and for the 3 clusters from panel f. Stationary points are plotted using the average DA responses arising from their weight configurations.</p></caption><graphic xlink:href="EMS192844-f003"/></fig></floats-group></article>