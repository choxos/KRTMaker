<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192491</article-id><article-id pub-id-type="doi">10.1101/2023.11.29.568856</article-id><article-id pub-id-type="archive">PPR767289</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Contentopic mapping in ventral and dorsal association cortex: the topographical organization of manipulable object information</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Almeida</surname><given-names>J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Kristensen</surname><given-names>S.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Tal</surname><given-names>Z.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Fracasso</surname><given-names>A.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Proaction Lab, Faculty of Psychology and Educational Sciences, University of Coimbra, Portugal</aff><aff id="A2"><label>2</label>CINEICC, Faculty of Psychology and Educational Sciences, University of Coimbra, Portugal</aff><aff id="A3"><label>3</label>School of Psychology and Neuroscience, University of Glasgow, UK</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author. <email>jorgecbalmeida@gmail.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>30</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Understanding how object information is neurally organized is fundamental to unravel object recognition<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>. The best-known neural organizational principle of information is topographical mapping of specific dimensions. Such maps have been shown mainly for sensorimotor information within sensorimotor cortices (e.g., retinotopy)<sup><xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. Thus, here we ask whether there are topographic maps – by analogy, contentopic maps – for mid-level object-related dimensions. We used functional magnetic resonance imaging and population receptive field analysis<sup><xref ref-type="bibr" rid="R7">7</xref></sup> to measure tuning of neural populations to selected manipulable object-related dimensions. Here we show maps in dorsal and ventral occipital cortex that code for the score of each object on each target dimension in a linear progression following a particular direction along the cortical surface. Maps for each dimension are distinct, and consistent across individuals. Thus, object information is coded in multiple topographical maps – i.e., contentopic maps. These contentopic maps refer to intermediate level visual and visuomotor representations, and are potentially computed from the grouping of lower-level information through non-linear transformation following gestalt principles<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R10">10</xref></sup>. This shows that topography is a widespread and non-incidental strategy for the organization of information in the brain that leads to greatly reduced connectivity-related metabolic costs and fast and efficient readouts of information for stimuli discrimination<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup>.</p></abstract></article-meta></front><body><p id="P2">A significant part of the life of an animal is spent on recognizing and identifying what is present in the environment in order to make informed decisions on essential issues such as foraging, mating, fighting, or fleeing. Object recognition became even more central for humans because our environment evolved beyond the natural world, and a myriad of human-made objects invaded our daily routines. A starting point for understanding how we recognize objects is focusing on how object-specific information is represented and organized in the brain<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>.</p><p id="P3">The best-known organizational principles of information in the brain are those present at primary sensorimotor cortices. For instance, visual cortex is organized by cortical columns that locally and continuously code for orientation of the visual stimulus<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. Moreover, individual cortical columns refer to particular locations in the visual field, with proximity in neural space mirroring proximity in the visual field. This organization musters a continuous and topographical map of the visual field across the different cortical columns that gradually follows retinal/visual field location of the stimulus (i.e., a retinotopic map<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>, for similar topographical maps in other sensorimotor cortices see<sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>).</p><p id="P4">Importantly, extant data on mid- and high-level object processing may point to a similar pattern of organization of object information in the brain. Work on non-human primates seems suggestive of a columnar organization within association cortex that is responsive to mid-level object-specific features such as complex shapes or combination of shapes, or object textures<sup><xref ref-type="bibr" rid="R13">13</xref></sup>. Moreover, adjacent columns code for ever so slightly different mid-level features<sup><xref ref-type="bibr" rid="R13">13</xref></sup>, suggesting similar topographical properties to those present in the columns within early visual cortex. Interestingly, recent work on object-driven human neural responses within association cortex showed that these regions code for object-related dimensions<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup>. Examples of the dimensions tested include, among others, real object size<sup><xref ref-type="bibr" rid="R15">15</xref></sup>, shape curvature and spikiness<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>, or visual field preferences and eye fixation patterns on the processing of different object domains<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>. These attempts have revealed very large organizational mosaics and extremely coarse topographies, whereby different regions dedicated to different dimensions or categories have been proposed (e.g., regions that code for large objects that are physically separated from regions that code for small objects<sup><xref ref-type="bibr" rid="R15">15</xref></sup>). It is not clear whether this level of explanation and degree of topography is sufficient to understand the organization of object knowledge in the brain, or whether we need to inspect finer-grained organizational levels and look for true topographical maps – i.e., look for finer-grain continuous mapping of cortical preferences within the cortical surface as a function of the (continuous) levels of an object-related dimension.</p><p id="P5">One aspect that may have deterred finding such topographic maps relates to the nature and precise content of object-related dimensions, which are considerably more elusive than those of the sensorimotor dimensions that rule the organization of sensorimotor cortices. The complexity of the world dictates that contrary to tone frequency of a sound, or the eccentricity of a visually presented stimulus in the visual field, an individual object-related dimension may be harder to isolate from other object-related dimensions. That is, in the real world, the position of a hammer within the periphery or the center of the visual field (i.e., its eccentricity) can easily be separated from its location in relation to the vertical meridian (i.e., polar angle) or its orientation. However, the typical grasp of a hammer (a power grasp) may be decodable also, to some extent, from the overall size of the object, potentially the amount of force needed to use the target object (e.g., precision grips may not be suited for high force grasps), or the weight of the object, among other mid-level (and potentially low-level) object properties. Nevertheless, and despite the fact that these dimensions are intertwined<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup>, we can still disentangle them – e.g., the size of an object and its typical grasp are different types of information that can be independently impaired in individuals with brain damage<sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R23">23</xref></sup>. This leads to the question of whether the organizational principles of sensorimotor cortices are also applicable to object-related information within association cortex. That is, are there true topographic maps for object-related dimensions – by analogy, contentopic maps – as there are for sensory dimensions (e.g., the eccentricity of a visual presented stimulus)?</p><sec id="S1"><title>Modeling of object-related dimensions</title><p id="P6">To test whether object-related dimensions are represented topographically in the same fashion as low-level visual dimensions are, we presented participants with manipulable object stimuli in sequences that followed the rank-order of those objects in their scores on a particular object-related dimension<sup><xref ref-type="bibr" rid="R1">1</xref></sup> while acquiring functional magnetic resonance imaging (fMRI) data. Specifically, we selected object-related dimensions that we obtained previously for a set of manipulable objects based on the subjective similarity between those manipulable objects on how they are manipulated in order to be used (see Methods)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. We chose the first two dimensions out of a 5-dimensional multidimensional scaling solution whose estimated distances between the tested objects were a good fit with the real distances (see Methods)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. That is, while these dimensions are still noisy and what they ultimately entail needs to be fully worked out, they are a relatively faithful description of the representational space we hold about how to manipulate objects. According to the labels provided by naïve participants, one of the selected dimensions refers to the type of grasp used when interacting with an object: power vs. precision grip (henceforth M1)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>; the other dimension refers to the amount of force that is necessary when interacting with an object: dexterity vs. force (henceforth M2)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Because we selected only the first two dimensions, perceived proximity between objects in this putative 2D space (M1 and M2) should be understood in the context of the full 5D space. Moreover, the names describing these dimensions, are, themselves, best approximations and may not fully capture the true underlying dimension, as they suffer from the noise inherent to the final multidimensional solution described above, as well as the necessarily subjective experience of the participants that labelled them. Nevertheless, at the level of analysis we are focusing here, these approximations are sufficiently detailed and have been shown to track, behaviorally and neurally, relevant object processing<sup><xref ref-type="bibr" rid="R1">1</xref></sup>.</p><p id="P7">Importantly, different objects have different scores for each dimension, and these scores refer to the “amount” each object has in relation to each dimension<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. Thus, objects can be rank-ordered in terms of their scores per dimension. We took advantage of this, and in an experimental run (e.g., referring to dimension M1), we cycled 6 times through the object stimuli rank-ordered by the target dimension (e.g., from objects that are grasped with a clear power grasp to objects that are grasped with a clear precision grip; see <xref ref-type="fig" rid="F1">Figure 1A &amp; 1B</xref>, <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>, and Methods for information about the distribution of stimuli through a cycle as a function of the dimension being tested). What we are cycling through, then, are the different levels of the object-related target dimension, via the presentation of different objects according to the rank-order of their dimensional scores. What we are modeling is how cycling through these different dimensional levels elicits ordered changes in preferred responses in contiguous parts of the cortical surface in a continuous way. Objects were presented continuously and at the center of the screen. Importantly, M1 and M2 are mathematically orthogonal<sup><xref ref-type="bibr" rid="R1">1</xref></sup>, and thus object sequences for M1 are unrelated to those for M2. Moreover, we controlled for low level visual properties of the selected stimuli by covarying out potential contributions of eccentricity and polar angle of the different stimuli (calculated based on center of fixation and center of mass of each stimulus; see Methods) from our data, and used the residuals for our analysis. This is especially important because stimulus eccentricity is a potential contributor to the organization of mid-to-high level organization of object knowledge<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>.</p><p id="P8">Given the two dimensions selected, potential contentopic maps should be obtained primarily within posterior parietal and dorsal occipital cortical regions, from the intraparietal sulcus posteriorly to V3d/V3A, as these areas code for shape and 3D representations of objects and grasp planning<sup><xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R28">28</xref></sup>. Moreover, aspects of object grasping that relate to typical object use (i.e., functional grasps)<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup> and to coding of grip force and object weight that allows for the derivation of the amount of force to apply to an object<sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>, have been shown to be associated with responses within lateral and ventral occipitotemporal cortex starting posteriorly within lingual gyrus and fusiform gyrus.</p><p id="P9">To capture the systematic variation of fMRI signal elicited by the sequence of objects presented, as a proxy to the varying dimensional scores, we adapted standard population receptive field analysis (pRF) typically used to unravel the organization of low-level visual information (e.g., eccentricity)<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, and applied it to our manipulable object dimensions. Specifically, we created one-dimensional gaussian pRFs to model neural signal (<xref ref-type="fig" rid="F1">Figure 1C</xref>). The model had two parameters: one that coded for the preferred location in the dimension (i.e., which values of the dimension are preferred by the neural population being inspected); and one that coded the tuning width of the response (i.e., the specificity and range of dimensional values the neural population responds to). That is, we modeled preferred level of the target dimension (i.e., preferred location) across the cortical surface using a 1D gaussian with different tuning widths. Importantly, we show that this model reliably captures signal fluctuations as a function of the dimensional scores of the objects that were presented (goodness-of-fit: R<sup>2</sup>), accounting for the time course of different voxels by different preferred dimensional tunings and width values (<xref ref-type="fig" rid="F2">Figure 2A</xref>, voxels in insets 1 to 3).</p><p id="P10">We then tested how our model fares in relation to other potential alternative models. Specifically, we tested a purely linear model that predicts monotonical increases/decreases of neural response as a function of the dimensional scores (i.e., a linear model), as well as a simpler gaussian model that exclusively modeled preferred location in the dimension, using a fixed narrow tuning width (σ=0.05; effectively resulting in a location-only model). Our original 1D gaussian model with 2 parameters (i.e., preferred location in the dimension and tuning width) outperformed the other models in explaining the data (see <xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>). As such, we focused exclusively on this model in all subsequent analysis exploring topographical representations of object knowledge.</p></sec><sec id="S2"><title>Comparing the tuning of the neural responses to the target object-related dimensions</title><p id="P11">In order to test for topography, we selected participant-specific cortical regions where our model robustly accounted for neural responses (R<sup>2</sup> &gt; 10%, approximately corresponding to p&lt;10<sup>-7</sup>, uncorrected). This led to the definition of two bilateral regions of interest (ROIs): a dorsal occipital area (d-occ) around the dorsal aspects of the middle occipital sulcus and gyrus, including voxels in the vicinity of V3A<sup><xref ref-type="bibr" rid="R28">28</xref></sup>; and a ventral area around lingual gyrus, inferior occipital gyrus, and occipitotemporal cortex (v-occ; <xref ref-type="fig" rid="F2">Figure 2B</xref>, <xref ref-type="supplementary-material" rid="SD1">Tables S1 and S2</xref>; <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>). Thus, the individual ROIs are generally within the regions expected to be tuned to these dimensions<sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R33">33</xref></sup>. However, other cortical locations were engaged by the continuous presentation of objects in the particular orders dictated by different dimensions. Specifically, variations in the scores of M2 were also accounted for in bilateral regions in lateral temporal cortex (MNI coordinates for the left ROI -40, -70, -7, and for the right ROI 43, -69, -15). Finally, inspection of individual maps for both M1 and M2 demonstrates that most participants (6 out of 8; <xref ref-type="supplementary-material" rid="SD1">Figures S4 &amp; S5</xref>) also showed responsive regions in posterior parietal cortex, but these cortical regions did not survive the strict inclusion criteria used.</p><p id="P12">We then focused on further understanding the topographical organization of the object-related dimensions of interest. M1 and M2 showed similar average goodness-of-fit of the pRF models across participants in d-occ and v-occ (<xref ref-type="fig" rid="F2">Figure 2C</xref>, top inset), and these average goodness-of-fit values were much higher than those observed in a control region (<xref ref-type="fig" rid="F2">Figure 2C</xref>, top inset; see Methods for the selection of the control region). Centrally, M1 and M2 differed in terms of average preferred location and width (i.e., the parameters of our pRF models) even though they engaged similar neural populations. This contrasts with the lack of a difference in location and width estimates for each of the dimensions in the control region (<xref ref-type="fig" rid="F2">Figure 2C</xref>, middle and bottom insets).</p><p id="P13">Importantly, differences in average preferred location for each dimension at the two ROIs could not be explained by an overlap in the objects whose scores are within these average preferred locations for each dimension – the amount of object overlap around the average location estimated for M1 and M2 does not differ from what would be obtained by chance (permutation testing; see Methods; see <xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>). Moreover, we observed a reliable difference in average width estimates between M1 and M2 within our ROIs. Specifically, M1 shows more specificity – i.e., its tuning is sharper – than M2. These patterns of results speak in favor of the reliability and sensitivity of our estimates.</p></sec><sec id="S3"><title>Object-related dimensions are arranged topographically</title><p id="P14">In the next step, we investigated the presence of contentopic maps – i.e., topographic maps that show a gradual and continuous progression of preferred dimensional scores over the cortical surface. We rendered preferred location maps for each dimension over the individual cortical surface (see <xref ref-type="fig" rid="F3">Figure 3A</xref> for an example of an individual contentopic map for M1). To estimate the topographic organization of the maps, and to define their main axis, we conducted the following analysis: for each participant, each ROI, and each hemisphere, we computed a cortical distance map over the ROI along its longest axis, where 0 represents the middle of the ROI along this axis (<xref ref-type="fig" rid="F3">Figure 3B</xref> left; see Methods for details). Each cortical distance map was rotated in 24 steps, yielding 24 cortical orientation maps (<xref ref-type="fig" rid="F3">Figure 3B</xref>). We iteratively fitted each cortical orientation map with the preferred location map, for each dimension, to obtain the best linear relationship between the two. (<xref ref-type="fig" rid="F3">Figure 3C</xref>). We then selected the orientations with the best fits per participant, dimension and ROI (<xref ref-type="fig" rid="F3">Figure 3C</xref>, inset 2). Thus, these orientation maps capture the main axis of the contentopic maps.</p><p id="P15"><xref ref-type="fig" rid="F4">Figure 4A</xref> shows the average topographic progression and the average best orientation of the linear fits for dimensions M1 and M2. In both dorsal and ventral ROIs, we observed reliable topographical progression of preferred location within the target dimensions across the cortical surface – i.e., we obtained contentopic maps (see <xref ref-type="supplementary-material" rid="SD1">Figure S4 &amp; S5</xref> for individual best orientations of the linear fit). To quantify contentopic organization, we first showed that the best fits across participants are significantly stronger in d-occ and v-occ for both M1 and M2, when compared to those in the control region where overall poor goodness-of-fit was observed (<xref ref-type="fig" rid="F4">Figure 4B</xref>). This demonstrates that there is a linear progression along the cortical surface as a function of the score of the objects in each of the dimensions.</p></sec><sec id="S4"><title>Object-related dimensions distinctively engage similar cortical locations</title><p id="P16">Finally, we tested the robustness of our fits for each dimension and each ROI via leave-one-participant-out cross-validation at the cortical surface level. If contentopic maps are consistent across participants, we should be able to predict the contentopic map of one of the participants from the contentopic maps of the remaining participants. <xref ref-type="fig" rid="F4">Figure 4C</xref> (top insets) shows that the topographic progression of M1 and M2 location maps were consistent across participants, such that we can reliably predict the map on the left-out individual based on the average of the remaining participants. Importantly, we also tested cross-validation across dimensions – that is, we tested whether we can predict the contentopic map of one of the participants for one dimension (e.g., M1) from the contentopic maps of the remaining participants on the other dimension (e.g., M2). Consistency between participants was not found between the dimensions (<xref ref-type="fig" rid="F4">Figure 4D</xref>). This shows, unequivocally, that these contentopic maps are dimension-specific – i.e., contentopic maps for M1 and M2 are distinct from one another.</p><p id="P17">However, the strongest test to the independence of the maps from M1 and M2 comes from analyzing the slope of the linear fits for the leave-one-participant-out cross-validation described above. This is because for the maps to be consistent across participants, they have to match in orientation, and most importantly, in the direction of the linear progression. In fact, simply matching in orientation could mean that the maps are in exact opposite directions. Thus, testing for positive slopes – i.e., when the to-be-predicted map and the maps of the remaining participants progress in the same direction through the cortical surface – will show that individual maps are consistent within dimension and independent across dimensions. The average slope of the within-dimension cross-validation linear fits within the ROIs for M1 and for M2 is positive (<xref ref-type="fig" rid="F4">Figure 4C</xref>, bottom insets) and it is clearly different from the slope in the control region. This shows that, within each dimension, individual maps can be predicted from the maps of the remaining individuals both in orientation and direction of its topographical progression. This is in stark contrast with the between-dimension analysis. Here, slopes hover around 0 and are no longer different from the slopes in the control region. This demonstrates the reliability of the maps obtained for each dimension at the individual participant level, and, importantly, the independence of the contentopic maps for the two different dimensions (for the results on both explained variance and slope for the analysis of width of the tuning curve, please see <xref ref-type="supplementary-material" rid="SD1">Figure S7</xref>).</p></sec><sec id="S5" sec-type="discussion"><title>Discussion</title><p id="P18">Previous studies have consistently demonstrated topographical mapping within sensorimotor cortices over particular dimensions that mirror response properties of the corresponding sensory receptor (e.g., location in the retina; sound frequency in the cochlea). This topographical organization is a central feature of sensory processing and dictates how we perceive and interact with the world<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. Here we show that topographical organization is also present for more abstract kinds of information such as those pertaining to objects. Particularly, we obtained contentopic maps over object-related dimensions (e.g., grasp type) – i.e., neural responses within dorsal and ventral occipital cortex are tuned, independently, to the two object-related dimensions tested. These topographic maps are continuous, in that they progress through the cortical surface gradually as function of the level of the target dimension; are consistent across participants, in that maps of individual participants are predictable from the maps of the remaining individuals; and are specific for each dimension, in that topographic maps for one of the dimensions are not predictable from the maps of the other dimension.</p><p id="P19">The ROIs that show contentopy for the two target dimensions are part of visual association cortex. Importantly, while the content of the dimensions used herein still needs to be fully fleshed out, and the labels used are but approximations based on subjective judgements of naïve participants, generally, these dimensions describe object manipulation space. In line with this, the ROIs obtained have also been shown to code for action-related information. For instance, voxels within the dorsal occipital ROI are involved in the processing of object-specific 3D shape<sup><xref ref-type="bibr" rid="R27">27</xref>,<xref ref-type="bibr" rid="R28">28</xref></sup>, and participate in object grasping, and in hand shaping and orientation in preparation for grasping<sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>. Moreover, voxels within the ventral occipital ROI have been shown to be also involved in object-specific (and potentially function-specific) action related processing<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R30">30</xref></sup>, as well as in the processing of object properties that impact action such as an object’s weight<sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>. Undoubtedly, one should expect parietal cortical regions more anterior to the ROIs herein to also show some degree of tuning to the object-related mid-level dimensions used, as these areas code for object grasping and manipulation<sup><xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R35">35</xref>–<xref ref-type="bibr" rid="R38">38</xref></sup>. In fact, inspection of individual maps (<xref ref-type="supplementary-material" rid="SD1">Figure S3 &amp; S4</xref>) shows that more posterior and superior parietal cortical regions do present neural responses tuned to both dimensions for most of the participants. Perhaps the weaker and less consistent responses in posterior and superior parietal cortex may be related with the format of stimuli presentation (exclusively visual) and the task required (unrelated to the object).</p><p id="P20">A remarkable aspect of our data is how independent the maps obtained for the two tested dimensions are. The way in which we elicited contentopic mapping reverted to using the same stimuli, with the only difference being that we presented these stimuli in different orders as dictated by the scores of each object in each dimension. Moreover, mapping was tested over the same neural population. Notwithstanding, pRF modeling of the neural responses (i.e., the modeling parameters of location and width) varied significantly (and non-accidently) by dimension. The computational separability of the neural tuning for these two dimensions suggests some level of topographic flexibility in how cortical regions process object-related information. Neural populations are able to pick up different informational properties of an object and code those independently in a superimposed way.</p><p id="P21">A major question relates to the nature of the representations within these contentopic maps. Our contentopic mapping data cannot be explained by low-level visual information. This is so because we regressed out stimuli-specific information about eccentricity (average pixel distance to center of fixation) and polar angle (orientation of the axis going from fixation to the center of mass of the object) from our data prior to the analysis reported here. Thus, contentopic maps were obtained independently of low-level stimulus variations of the kind that drive early visual cortex. This is not to say that low-level visual features, and especially eccentricity, are not informative and important for the definition of these maps and the dimensions that drive them (as they are of the response profiles of more anterior areas<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>) – for instance, eccentricity may well correlate, in the real world, with an object’s grasp. What we are showing, however, is that whatever is represented within these contentopic maps is not completely reducible to low-level properties.</p><p id="P22">Consequently, contentopy probably depends on intermediate representations that bridge between early visual properties and complex object representations<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Although these mid-level representations are important for visual and visuomotor object processing<sup><xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R42">42</xref></sup>, their nature is hard to determine<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Potentially, they no longer focus on spatial coding, but rather on object feature mapping<sup><xref ref-type="bibr" rid="R40">40</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup> and object-specific parameter spaces<sup><xref ref-type="bibr" rid="R44">44</xref></sup>. These spaces and maps should be dependent on the aggregation of information from upstream simpler cells (e.g., edge orientation) into more global representations (e.g., surface orientation). This would be done via highly non-linear transformations<sup><xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R39">39</xref></sup> that group lower-level information using gestalt principles applied to visual and visuomotor<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup> processing. Interestingly, and related to the dimensions tested, models for object grasp have been proposed that rely precisely on gestalt principles to determine the kind of grasps to be applied to a stimulus through the computation of visual surfaces from low-level visual properties and focusing on action as a composite of chunks that respects surface properties<sup><xref ref-type="bibr" rid="R45">45</xref></sup>.</p><p id="P23">Speculatively, transformations over the readouts of the (multiple and potentially correlated) contentopic maps of the kind shown here would lead to composite higher order maps<sup><xref ref-type="bibr" rid="R4">4</xref></sup> (within more anterior areas). In fact, it may be the case that domain selectivity within ventral temporal cortex follows precisely from the members of a domain satisfying a particular set of dimensions (and their associated contentopic maps; e.g., grasp type, elongation, presence of metal<sup><xref ref-type="bibr" rid="R1">1</xref></sup>) that relate strongly with that domain (e.g., manipulable objects). Thus, these downstream composite higher-level maps would arise dynamically from sampling upstream contentopic maps that are important for different domains of stimuli. This seems to be in line with there being finer parameter spaces within domain-specific regions<sup><xref ref-type="bibr" rid="R46">46</xref></sup>, and with the role of connectivity in the definition of the conceptual representation in ventral temporal cortex<sup><xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>.</p><p id="P24">But how does this possibility fit with many of the coarse topographies that have been suggested? Topography at more higher-level areas should emerge from the non-linear aggregation of upstream contentopy mid-level maps into composite maps<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. For instance, domain specificity (and potentially real object size, a topography<sup><xref ref-type="bibr" rid="R15">15</xref></sup> that is highly correlated with a domain organization)<sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref></sup> would emerge from the fact that members of a domain would evoke a large set of important dimensions, and thus mid-level contentopic maps, that are typical of that domain. In these domain-specific (or size-dependent) regions, traversing single dimensions (e.g., our M1 dimension) would lead to weak or no selectivity<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. However, evoking the right pattern of dimensions would lead to the specific response preferences observed. Similarly, because non-linear integration of contentopic maps into composite higher-level maps follows dynamically from the requirements of the different domains, then regions that do not show particular domain preferences (the so called “NoManLand” regions<sup><xref ref-type="bibr" rid="R17">17</xref></sup>) should show more unidimensional maps, potentially tuned towards highly informative mid-level properties such as curvature singularities<sup><xref ref-type="bibr" rid="R10">10</xref></sup>, as they do<sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>. Finally, this hypothesis is also compatible with the putative role of eccentricity at these higher-level topographies<sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R21">21</xref></sup>. Eccentricity maps would work as protomaps<sup><xref ref-type="bibr" rid="R16">16</xref></sup> to guide the dynamical integration of contentopies. Moreover, because contentopy is built out of from lower-level representations, eccentricity would be inherently passed to these higher-level maps.</p><p id="P25">Interestingly, this multidimensional flexibility in the topographical organization of object information may be central for the kinds of processes that underlie object recognition. The multidimensional flexibility that we show could be the mechanism by which object representations are built in the service of object recognition – neural populations would be tuned to different mid-level dimensions and flexibly superimpose different topographic maps. Potentially, these maps impose an object-centric reference frame to visual processing, just as retinotopy imposes a spatial reference to cognition<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. One potential consequence of such composite maps is that they allow for flexibility and discriminability when processing individual items: a particular hammer may have some properties that differ from a prototypical hammer, but because of the intersection of the readouts of multiple contentopy maps, in the end, a hammer will still be a hammer, even if differing from a prototypical hammer in some dimensions.</p><p id="P26">Importantly, here we show that topography is (at least) one of the central organization principles that the brain uses to represent and store information about objects. Note that although we show contentopy for one type of objects – i.e., manipulable objects – and a particular set of dimensions, we predict that this contentopic organization should be present for other types of stimuli (e.g., faces, animals, scenes; provided that we have structuring dimensions for the object categories to be tested) and for other object-related dimensions. For instance, mid-level dimensions such elongation<sup><xref ref-type="bibr" rid="R1">1</xref></sup>, or particular surface properties<sup><xref ref-type="bibr" rid="R48">48</xref></sup>, should also lead to relatively independent contentopic maps.</p><p id="P27">Most importantly, our results demonstrate the ubiquity and non-incidental nature of topographical organization of information in the brain. Irrespective of the kind of information – whether sensorimotor<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>, or more abstract such as numerosity<sup><xref ref-type="bibr" rid="R49">49</xref></sup> or, as demonstrated here, object information – the brain will organize information in continuous topographical maps that follow particularly relevant dimensional solutions for the kinds of information being coded. The parameters that may change for different kinds of information relate to the multidimensional flexibility that is implemented, and how this flexibility is rendered as a composite neural response. The benefits of such an organizational principle go from reducing metabolically costly long-distance connectivity between neurons processing similar information, to more cognitively focused aspects relating to processing specialization and stimuli discrimination<sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref></sup>. Overall, our results raise the question of whether all kinds of information are amenable to topographical mapping. Although our data cannot fully address this, it clearly brings a new layer of understanding to both the general question of how information is processed in the brain, as well as to the quest of understanding what underlies our formidable capacity to recognize objects.</p></sec><sec id="S6" sec-type="methods"><title>Methods</title><sec id="S7" sec-type="subjects"><title>Participants</title><p id="P28">8 individuals (5 women; average age: 24; age range: 19-36) from the community of the University of Coimbra participated in the experiment. All participants were right-handed, had normal or corrected-to-normal vision, and were naive as to the experimental manipulations. The experiment was approved by the ethics committee of the Faculty of Psychology and Educational Sciences of the University of Coimbra and followed all ethical guidelines. Moreover, participants provided written informed consent, and were compensated for their time by receiving either course credit on a major psychology course or financial compensation.</p></sec><sec id="S8"><title>Stimuli</title><p id="P29">We used a set of 80 common manipulable objects previously used to determine and explore object dimensionality and for which we have scores in object-related mid-level dimensions<sup><xref ref-type="bibr" rid="R1">1</xref></sup> (see Figure S1 for one exemplar of each object used). These were selected to be representative of the different types of objects that humans use routinely. We selected 3 exemplars per object type in a total of 240 images. Images were 400-by-400 pixel squares and subtended approximately 10° of the visual angle.</p></sec><sec id="S9"><title>Object-related dimensions</title><p id="P30">We selected two mid-level object-related dimensions from those that were obtained and tested in behavioral and neural experiments previously in our lab<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. To extract these dimensions, we analyzed how different objects relate to each other in a large conceptual space, and extracted the main axes that organize this conceptual multidimensional space. Specifically, we presented 60 participants with words referring to the target 80 manipulable objects and asked them to think about how similar these objects were in the manner in which we manipulated them. We used an object sorting task to derive dissimilarities between our set of objects – each participant was asked to sort all 80 objects into different piles such that objects in a pile were similar to each other, but different from objects in other piles, on the target knowledge type. This piling solutions were then entered in a participant-specific similarity matrix, and an averaged dissimilarity matrix was then created<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. We then applied non-metric multidimensional scaling (MDS)<sup><xref ref-type="bibr" rid="R50">50</xref></sup> to the dissimilarity matrices obtained in order to extract key object-related dimensions that structure our object representational space. For details on the selection of the dimensions, the similarity matrices, and the behavioral and neural testing please refer to Almeida and Colleagues<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. The two dimensions selected were the first dimensions, out of a 5-dimensional solution, that structured similarity in how we manipulate objects, and are the dimensions that explain most variance in these similarity judgements. The Kruskal stress value of the 5-dimensional solution was relatively low (Stress = 0,084) suggesting that the estimated distances between the tested objects in this multidimensional solution were a good fit with the real distances. These dimensions are by definition mathematically orthogonal. Moreover, these dimensions were labeled by another set of naïve participants as referring to the types of grasps applicable to the objects (M1), and to the amount of force that is necessary to interact with an object (M2; for details on all these experimental procedures and analysis please see Almeida and Colleagues<sup><xref ref-type="bibr" rid="R1">1</xref></sup>).</p></sec><sec id="S10"><title>fMRI Experimental design</title><p id="P31">The fMRI experiment consisted of 8 different sessions on different days. On day 1 and 2, dimension M1 was presented in a particular order. On day 3 and 4, dimension M1 was presented in reversed order in relation to sessions 1 and 2. On day 5 and 6, M2 was presented in a particular order, and on day 7 and 8 M2 was presented in reversed order in relation to sessions 5 and 6. Each session included 6 runs. Per run we presented 6 cycles of a dimension. For the first cycle we randomly selected 2 objects from each of the 20 bins (see <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). The remaining 2 objects from each bin were then used in the following cycle. This means that per cycle we presented a sample of 40 out the 80 objects used to determine the dimensions, and that all the levels of the dimensions were evenly sampled. Specifically, for instance in an experimental run referring to dimension M1, we cycled through the object stimuli rank-ordered by this dimension 6 times. In a cycle, we first presented objects (one at a time) that are clearly used with a power grasp (e.g., a shovel, followed by a weight, followed by a tennis racket, etc.; see <xref ref-type="fig" rid="F1">Figure 1A</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). These objects were then followed by objects whose grasp still falls closer to a power grasp than a precision grip (e.g., a grater, followed by a wooden spoon, followed by a computer mouse), and these were followed by objects whose grasp is increasingly closer to a precision grip (e.g., a fork, followed by a spray bottle, followed by a stamp). This cycle finished with objects that are clearly used with a precision grip (e.g., clothespin, tweezers and nail clipper).</p><p id="P32">Object images were presented for the duration of 1 TR (2 seconds). All runs were preceded and followed by 16 seconds of fixation. We used 3 different exemplar images per object which were counterbalanced across runs (exemplar 1 for run 1 and 3, exemplar 2 for run 2 and 5, and exemplar 3 for run 3 and 6). All participants completed all sessions and runs. Participants performed a simple task: they were instructed to maintain fixation on a yellow dot in the center of the image and press a button with index or middle finger when the dot changed color to red or green. Fingers and colors were counterbalanced across runs and sessions. The dot changed color on 10% of the trials.</p></sec><sec id="S11"><title>MRI acquisition</title><p id="P33">Scanning was performed with a Siemens MAGNETOM Prisma-fit 3T MRI Scanner (Siemens Healthineers) with a 64-channel head coil at the University of Coimbra (Portugal; BIN - National Brain Imaging Network). Functional images were acquired with the following parameters: T2* weighted (single-shot/GRAPPA) echo-planar imaging pulse sequence, repetition time (TR) = 2000 ms, echo time (TE) = 30 ms, flip angle = 75°, 37 axial slices, acquisition matrix = 70 x 64 with field of view of 210x192 mm, and voxel size of 3 mm3. Structural T1-weighted images were obtained using a magnetization prepared rapid gradient echo (MPRAGE) sequence with the following parameters: TR = 2530 ms, TE = 3.5 ms, total acquisition time = 136 s, FA = 7°, acquisition matrix = 256 x 256, with field of view of 256 mm, and voxel size of 1 mm3.</p></sec><sec id="S12"><title>Preprocessing of MRI data</title><p id="P34">Processing of the anatomical T1-weighted MR images was performed in Freesurfer image analysis suite (version 6.0, <ext-link ext-link-type="uri" xlink:href="http://surfer.nmr.mgh.harvard.edu/">http://surfer.nmr.mgh.harvard.edu/</ext-link>) using the recon-all pipeline, that included white and grey matter segmentation, skull removal, and cortical reconstruction. Pre-processing of functional MRI data was performed in AFNI<sup><xref ref-type="bibr" rid="R51">51</xref>,<xref ref-type="bibr" rid="R52">52</xref></sup> and included the following steps: slice acquisition time correction, head motion correction, detrending and scaling. Following the initial pre-processing steps, the time-series of each dimension (M1 and M2) and order of presentation (ascending and descending dimensional values) were averaged across repetitions for each participant. The averaged functional image was co-registered to the anatomical scans, using the function 3dAllineate, using local Pearson correlation as the cost function.</p></sec><sec id="S13"><title>Detrending low-level visual features</title><p id="P35">We calculated eccentricity and polar angle of the stimuli used in our experiment (3 sets of 80 exemplars) using Matlab (MathWorks). The image stimuli were first binarized to segregate the stimuli from the background. Eccentricity was calculated as the average distance of all pixels from the fixation point (corresponding also to the center of the image). To calculate polar angle, we first defined the center of mass of each object (using the regionprop function), and extracted the angle of the vector connecting the fixation point and the center of mass. The extracted values of eccentricity and polar angle were averaged across the three exemplars of each object, and across the 4 objects included in each bin. These averaging procedures were performed separately for each dimension. Thus, we obtained eccentricity and polar angle values for each bin of each dimension. These values were used to detrend the data, using the 3dDetrend function in AFNI, and the residuals of this detrending were then used in the pRF modelling procedure. By detrending these two dimensions we ensure that our maps are independent of the two major dimensions that lead to maps in visual cortex.</p></sec><sec id="S14"><title>Population receptive field analysis (pRF) and topography</title><p id="P36">The pRF analysis is a forward modelling approach aimed at accounting for the observed BOLD time series in each individual voxel by estimating parameters at the neuronal population level<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R53">53</xref>–<xref ref-type="bibr" rid="R55">55</xref></sup>. pRF parameters estimates were obtained from percent BOLD time series by fitting a linear model via an iterative procedure, similar to previous applications in the domains of vision, sensorimotor integration and quantity/numerosity perception The linear model can be described as follows: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>β</mml:mi><mml:mo>+</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:math></disp-formula></p><p id="P37">In <xref ref-type="disp-formula" rid="FD1">equation 1</xref>, <italic>p(t)</italic> represents the predicted BOLD signal, β is a scaling factor necessary to match the prediction to the observed data, defined in arbitrary units (A.U.), and <italic>e</italic> is measurement error, representing the departure from the estimated and the observed BOLD. The fitting was performed via the General Linear Model approach (GLM), using ordinary-least-square estimation (OLS).</p><p id="P38">We obtained the prediction <italic>p(t)</italic> starting from a parameterized model of the stimulus sequence. To parametrize the stimuli sequence, we used a Gaussian tuning function (see <xref ref-type="disp-formula" rid="FD2">equation 2</xref>) with the neuronal parameters (location, μ; width, σ) arranged in a two-dimensional grid. Gaussian location parameter (μ) was allowed to vary between -0.6 and 0.6 in 20 steps. Gaussian width (σ) was allowed to vary between 0.05 and 0.25 in 18 steps, for a total of 360 predictions. <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:math></disp-formula></p><p id="P39">For each voxel’s percent BOLD time series, the fit was performed with an iterative procedure. On each iteration the predictor was built based on the selected width and location parameters. The predictor was set as the independent variable of a general linear model with the percent BOLD time series as the dependent variable. For each iteration we computed model goodness-of-fit, measured with R<sup>2</sup>. The set of neuronal parameters (location, μ; width, σ) that maximized R<sup>2</sup> were assigned to the individual voxel.</p><p id="P40">It is important to note that in this modelling approach, the estimated parameters (Gaussian location - μ; and width - σ) are connected to local features of the percent BOLD time series. This approach facilitates the following analyses (statistical and topographical), allowing to assess the distribution of specific features of the percent BOLD time series across different experimental conditions and ROIs.</p></sec><sec id="S15"><title>Definition of ROIs</title><p id="P41">For each individual, we defined the regions of interest (ROIs) based on a leave-one-out procedure. For participant <italic>x,</italic> we projected pRF parameter maps (R<sup>2</sup>, location and width) on standard surface for each of the remaining (n-1) participants using the AFNI function 3dVol2Surf – i.e., we create 8 n-1 participant averages, one for each left-out participants thus defining independent ROIs.</p><p id="P42">Compared to standard volumetric approaches, the standard surface maps approach offers an advantage in terms of between participant co-registration at the cortical level<sup><xref ref-type="bibr" rid="R56">56</xref></sup>. Surface-based mapping relies on matching between common anatomical features such as sulci and gyri, and it preserves the local topology of the individual cortical sheet at a single hemisphere level. Thus, this procedure is more accurate than what could be obtained by classical volumetric-based registrations between individuals.</p><p id="P43">We averaged and visualized the maps obtained from the n-1 participants in the common MNI surface space. For each hemisphere and tested dimension (M1 and M2), we average R<sup>2</sup> across the n-1 participants and threshold the resulting map above 10% R<sup>2</sup>. We then ran a clustering algorithm at the surface level to remove small clusters of activation that survive the thresholding (minimum cluster size: 35mm<sup>2</sup>). The outcome of this operation yielded two ROIs per hemisphere: a dorsal-occipital ROI (d-occ) and a ventral occipital ROI (v-occ). For visualization purposes we then averaged these ROIs. Please see Tables S1 and S2, as well as <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref> for the location of these averaged ROIs.</p><p id="P44">The d-occ and v-occ rois were then projected back on the left out individual x for subsequent analysis. Please note that all the analyses presented in the current manuscript were performed on the native and topologically correct individual participant space.</p></sec><sec id="S16"><title>Permutation testing</title><p id="P45">Within the ROIs, the best modelling parameters across participants differed between object-related dimensions M1 and M2 (<xref ref-type="fig" rid="F2">Figure 2C</xref>, main manuscript). Specifically, M1 yielded more positive tuning location estimates and narrower tuning width compared to M2. From an fMRI perspective, the only difference between M1 and M2 is the temporal order in which the visual stimuli are presented. It might be argued that the observed differences between M1 and M2 modelling parameters could be simply due to a response to particular objects within the sequence, presented at different times during each dimension-specific sequence.</p><p id="P46">To test for this possibility, we extracted 12 objects around the average location estimate of M1 and M2 (i.e., 3 bins), separately. We observed an overlap of 2 out of 12 objects between M1 and M2 (16.6% of objects overlap). We asked whether this overlap would be sufficient to drive the observed difference between M1 and M2 average location estimates. To assess this, we run a permutation test, where we obtained the null distribution of the percentage of object overlap between 12 elements extracted separately from two shuffled sequences of 80 objects. If obtaining a 16.6% overlap would be extremely rare, then it could be argued that the difference between M1 and M2 average location estimates might be driven by object overlap per se. If 16.6% would fall within the expected range of overlap, then it would be less likely that the object overlap would account for the M1 and M2 location difference.</p><p id="P47">Permutation results indicate that the 95% confidence interval of the null distribution ranges between 0% and 33%, with a median of 16.6%. That is, in 95% of the repetitions, the number of overlapping objects between the two sequences ranged between 0% and 33% (see <xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>). In our data, we observe an overlap of 16.6% between the objects around the average location parameter of M1 and M2 – which falls within the 95% confidence interval of the null distribution. Thus, the number of objects overlap derived from our data is not different from what we would expect by chance. Hence, the difference in tuning parameters we observe is unlikely to be accounted for by the number of overlapping objects falling around the estimated mean tuning locations.</p></sec><sec id="S17"><title>Analysis of the orientation of the contentopic maps</title><p id="P48">A topographic map is a gradual and ordered representation of an estimated variable over the human cortical or subcortical location (e.g., visual eccentricity<sup><xref ref-type="bibr" rid="R12">12</xref></sup>). To test for the presence of topographical maps at the individual participant level we proceeded as follows: for each participant’s ROIs and hemisphere we computed the node-to- node geodesic distance from each node in one ROI – the adjacency matrix, derived from the distance metric, <italic>dist</italic> – geodesic distance, between two mesh vertices <italic>x<sub>i</sub></italic> and <italic>x<sub>j</sub>:</italic> <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P49">Then we computed the spectral decomposition of the adjacency matrix and selected the first eigenvector of the decomposition (vector 1 – <italic>v<sub>1</sub></italic>)<sup><xref ref-type="bibr" rid="R57">57</xref></sup>. This vector represents a map of cortical distance over the ROI along its longest axis, where each point in the map is the geodesic distance of a node along the selected axis with respect to 0 – the middle of the ROI along its longest axis. The second eigenvector of the decomposition (vector 2 – <italic>v2)</italic> represents a map of cortical distance over the ROI along its shorter axis.</p><p id="P50">Vector 1 and vector 2 effectively represent a 2D mapping over the cortical surface. These are used for example in surface-flattening approaches. In this application we iteratively rotated the 2D mapping with respect to its original axis in 24 steps using an 2D matrix rotation of the desired angle (in radians), multiplied by the original 2D mapping coordinates (<xref ref-type="disp-formula" rid="FD4">equation 4</xref>). Note that this operation does rotate the vectors in surface space but does not shift the center of the vectors. <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>sin</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>cos</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P51">For each iteration, this operation resulted in a rotated version of vector 1 and vector 2 (resulting in rotated vectors – <italic>v<sub>1r</sub> v<sub>2r</sub></italic>). We used vector 1 as our reference for the subsequent analysis steps.</p><p id="P52">For each participant, ROI, dependent variable (pRF location and pRF width) and dimension tested (M1 and M2), each of the 24 rotations of vector 1 map were projected in the single participant anatomical space using the AFNI function 3dSurf2Vol. We compared the linear progression between the rotated vector 1 map and the estimated dependent variable. The comparison between the rotated vector 1 and the variable of interest was performed using simple linear regression. The obtained t-statistic was stored for subsequent analysis.</p><p id="P53">For each participant, ROI, dependent variable (pRF location and pRF width) and experimental condition (M1 and M2) the best orientation characterizing the map was defined as the one yielding the highest t-statistic among the 24 possible rotations (linear fit – topography, see main manuscript, <xref ref-type="fig" rid="F4">Figure 4B</xref>). Following this procedure, we obtained the best rotations capturing the gradual representation of the given estimated variable (pRF location and pRF width) over each participant human cortical surface and ROI. We compared the obtained highest t-statistic for each participant and ROI with those obtained from a control ROI of similar size derived from each individual dorso-lateral cortex (ctrl ROI, main manuscript, <xref ref-type="fig" rid="F2">Figure 2B</xref> and <xref ref-type="fig" rid="F4">Figure 4B</xref>).</p><p id="P54">We opted for a linear model for theoretical (i) and computational (ii) reasons. (i) Theoretically, topographic maps are, for the most part, arranged linearly over the cortical surface, with a notable exception being tonotopic maps showing a non-linear progression along the Heschl’s gyrus (Da Costa et al., 2011). Somatotopic maps, although showing a discontinuity necessary to cover the two-dimensional body over a quasi-1D strip of cortex (pre-central and post-central gyrus), are largely arranged linearly when taking one effector at a time. For example, the hand shows an ordered progression of finger representation over the cortical surface<sup><xref ref-type="bibr" rid="R58">58</xref></sup>. Thus, at the very least, linear models would be best approximations for a first level analysis of contentopy; (ii) From a computational perspective, adding non-linear components would increase the likelihood of overfitting, thus potentially capturing noise variability instead of ‘true’ underlying map features.</p></sec><sec id="S18"><title>Cross validation of contentopic maps</title><p id="P55">We cross-validated the gradual representations for each ROI and each participant using a leave-one-participant-out procedure. For each iteration we averaged the map of N-1 participants (either location or width) in MNI surface space and compared the corresponding map (location or width) of the left-out participant. We assessed the similarity between the averaged map of N-1 participants and the left-out participant map using a linear regression model, storing the model goodness-of-fit (R<sup>2</sup> – explained variance) and the model slope (beta coefficient). We report the model goodness-of-fit and model slope, as the linear model could yield high explained variance (R<sup>2</sup>) estimates even in if the averaged map of N-1 participants and the left-out participant map were going in the opposite direction. Reporting the model slope resolves this issue, as the sign of the slope indicates the direction of the relation between maps. A negative slope indicates estimates in opposite direction, a positive slope indicates estimates in the same direction.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS192491-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d214aAdKbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S19"><title>Acknowledgements</title><p>This research was supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme Starting Grant 802553 “ContentMAP” to JA, and by European Research Executive Agency Widening programme under the European Union’s Horizon Europe Grant 101087584 “CogBooster” to JA. SK was supported by a Fundação para a Ciência e Tecnologia (FCT) Doctoral Grant SFRH/BD/145218/2019, ZT was supported by a Postodoctoral fellowship under the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme Starting Grant 802553 “ContentMAP”. AF was supported by a Biotechnology and Biology research council (BBSRC) grant BB/S006605/1, and a Bial Foundation Grant 203/2020.</p></ack><sec id="S20" sec-type="data-availability"><title>Code availability</title><p id="P56">Codes will be available at OSF before publication.</p></sec><sec id="S21" sec-type="data-availability"><title>Data availability</title><p id="P57">Data will be available at OSF before publication.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P58"><bold>Author contributions</bold></p><p id="P59">Conceptualization: JA</p><p id="P60">Methodology: SK, AF, ZT, JA</p><p id="P61">Investigation: SK, ZT</p><p id="P62">Funding acquisition: JA</p><p id="P63">Analysis: AF, SK, ZT, JA</p><p id="P64">Writing – original draft: JA</p><p id="P65">Writing – review &amp; editing: AF, SK, ZT, JA</p></fn><fn id="FN2" fn-type="conflict"><p id="P66"><bold>Competing interests:</bold> Authors declare that they have no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almeida</surname><given-names>J</given-names></name><etal/></person-group><article-title>Neural and behavioral signatures of the multidimensionality of manipulable object processing</article-title><source>Commun Biol</source><year>2023</year><volume>6</volume><fpage>940</fpage><pub-id pub-id-type="pmcid">PMC10502059</pub-id><pub-id pub-id-type="pmid">37709924</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-05323-x</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nat Rev Neurosci</source><year>2014</year><volume>15</volume><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC4143420</pub-id><pub-id pub-id-type="pmid">24962370</pub-id><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>What drives the organization of object knowledge in the brain?</article-title><source>Trends Cogn Sci</source><year>2011</year><volume>15</volume><fpage>97</fpage><lpage>103</lpage><pub-id pub-id-type="pmcid">PMC3056283</pub-id><pub-id pub-id-type="pmid">21317022</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2011.01.004</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Haushofer</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name></person-group><article-title>Interpreting fMRI data: maps, modules and dimensions</article-title><source>Nat Rev Neurosci</source><year>2008</year><volume>9</volume><fpage>123</fpage><lpage>135</lpage><pub-id pub-id-type="pmcid">PMC2731480</pub-id><pub-id pub-id-type="pmid">18200027</pub-id><pub-id pub-id-type="doi">10.1038/nrn2314</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name></person-group><article-title>High-field fMRI unveils orientation columns in humans</article-title><source>Proc Natl Acad Sci U S A</source><year>2008</year><volume>105</volume><fpage>10607</fpage><lpage>10612</lpage><pub-id pub-id-type="pmcid">PMC2492463</pub-id><pub-id pub-id-type="pmid">18641121</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0804110105</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sereno</surname><given-names>MI</given-names></name><etal/></person-group><article-title>Borders of multiple visual areas in humans revealed by functional magnetic resonance imaging</article-title><source>Science</source><year>1995</year><volume>268</volume><fpage>889</fpage><lpage>893</lpage><pub-id pub-id-type="pmid">7754376</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Wandell</surname><given-names>BA</given-names></name></person-group><article-title>Population receptive field estimates in human visual cortex</article-title><source>Neuroimage</source><year>2008</year><volume>39</volume><fpage>647</fpage><lpage>660</lpage><pub-id pub-id-type="pmcid">PMC3073038</pub-id><pub-id pub-id-type="pmid">17977024</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.09.034</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Formisano</surname><given-names>E</given-names></name><etal/></person-group><article-title>Mirror-symmetric tonotopic maps in human primary auditory cortex</article-title><source>Neuron</source><year>2003</year><volume>40</volume><fpage>859</fpage><lpage>869</lpage><pub-id pub-id-type="pmid">14622588</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Aflalo</surname><given-names>TN</given-names></name></person-group><article-title>Mapping behavioral repertoire onto the cortex</article-title><source>Neuron</source><year>2007</year><volume>56</volume><fpage>239</fpage><lpage>251</lpage><pub-id pub-id-type="pmid">17964243</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><source>Two-Dimensional Shape as a Mid-Level Vision Gestalt</source></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaas</surname><given-names>JH</given-names></name></person-group><article-title>Topographic maps are fundamental to sensory processing</article-title><source>Brain Res Bull</source><year>1997</year><volume>44</volume><fpage>107</fpage><lpage>112</lpage><pub-id pub-id-type="pmid">9292198</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wandell</surname><given-names>BA</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Brewer</surname><given-names>AA</given-names></name></person-group><article-title>Visual field maps in human cortex</article-title><source>Neuron</source><year>2007</year><volume>56</volume><fpage>366</fpage><lpage>383</lpage><pub-id pub-id-type="pmid">17964252</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><article-title>Inferotemporal cortex and object vision</article-title><source>Annu Rev Neurosci</source><year>1996</year><volume>19</volume><fpage>109</fpage><lpage>139</lpage><pub-id pub-id-type="pmid">8833438</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><etal/></person-group><article-title>THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior</article-title><source>Elife</source><year>2023</year><volume>12</volume><pub-id pub-id-type="pmcid">PMC10038662</pub-id><pub-id pub-id-type="pmid">36847339</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82580</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>Tripartite organization of the ventral stream by animacy and object size</article-title><source>J Neurosci</source><year>2013</year><volume>33</volume><fpage>10235</fpage><lpage>10242</lpage><pub-id pub-id-type="pmcid">PMC3755177</pub-id><pub-id pub-id-type="pmid">23785139</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0983-13.2013</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><article-title>Universal Mechanisms and the Development of the Face Network: What You See Is What You Get</article-title><source>Annu Rev Vis Sci</source><year>2019</year><volume>5</volume><fpage>341</fpage><lpage>372</lpage><pub-id pub-id-type="pmcid">PMC7568401</pub-id><pub-id pub-id-type="pmid">31226011</pub-id><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014917</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><year>2020</year><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC8088388</pub-id><pub-id pub-id-type="pmid">32494012</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><article-title>Spikiness and animacy as potential organizing principles of human ventral visual cortex</article-title><source>Cereb Cortex</source><year>2023</year><volume>33</volume><fpage>8194</fpage><lpage>8217</lpage><pub-id pub-id-type="pmcid">PMC10321104</pub-id><pub-id pub-id-type="pmid">36958809</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhad108</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname><given-names>J</given-names></name><name><surname>Natu</surname><given-names>V</given-names></name><name><surname>Jeska</surname><given-names>B</given-names></name><name><surname>Barnett</surname><given-names>M</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><article-title>Development differentially sculpts receptive fields across early and high-level human visual cortex</article-title><source>Nat Commun</source><year>2018</year><volume>9</volume><fpage>788</fpage><pub-id pub-id-type="pmcid">PMC5824941</pub-id><pub-id pub-id-type="pmid">29476135</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-03166-3</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Op de Beeck</surname><given-names>HP</given-names></name><name><surname>Torfs</surname><given-names>K</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name></person-group><article-title>Perceived shape similarity among unfamiliar objects and the organization of the human object vision pathway</article-title><source>J Neurosci</source><year>2008</year><volume>28</volume><fpage>10111</fpage><lpage>10123</lpage><pub-id pub-id-type="pmcid">PMC6671279</pub-id><pub-id pub-id-type="pmid">18829969</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2511-08.2008</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malach</surname><given-names>R</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>The topography of high-order human object areas</article-title><source>Trends Cogn Sci</source><year>2002</year><volume>6</volume><fpage>176</fpage><lpage>184</lpage><pub-id pub-id-type="pmid">11912041</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><etal/></person-group><article-title>Separate neural pathways for the visual analysis of object shape in perception and prehension</article-title><source>Curr Biol</source><year>1994</year><volume>4</volume><fpage>604</fpage><lpage>610</lpage><pub-id pub-id-type="pmid">7953534</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name><name><surname>Jakobson</surname><given-names>LS</given-names></name><name><surname>Carey</surname><given-names>DP</given-names></name></person-group><article-title>A neurological dissociation between perceiving objects and grasping them</article-title><source>Nature</source><year>1991</year><volume>349</volume><fpage>154</fpage><lpage>156</lpage><pub-id pub-id-type="pmid">1986306</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><etal/></person-group><article-title>Functional magnetic resonance imaging reveals the neural substrates of arm transport and grip formation in reach-to-grasp actions in humans</article-title><source>J Neurosci</source><year>2010</year><volume>30</volume><fpage>10306</fpage><lpage>10323</lpage><pub-id pub-id-type="pmcid">PMC6634677</pub-id><pub-id pub-id-type="pmid">20685975</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2023-10.2010</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Culham</surname><given-names>JC</given-names></name><etal/></person-group><article-title>Visually guided grasping produces fMRI activation in dorsal but not ventral stream brain areas</article-title><source>Exp Brain Res</source><year>2003</year><volume>153</volume><fpage>180</fpage><lpage>189</lpage><pub-id pub-id-type="pmid">12961051</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fattori</surname><given-names>P</given-names></name><name><surname>Breveglieri</surname><given-names>R</given-names></name><name><surname>Bosco</surname><given-names>A</given-names></name><name><surname>Gamberini</surname><given-names>M</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name></person-group><article-title>Vision for Prehension in the Medial Parietal Cortex</article-title><source>Cereb Cortex</source><year>2017</year><volume>27</volume><fpage>1149</fpage><lpage>1163</lpage><pub-id pub-id-type="pmid">26656999</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freud</surname><given-names>E</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title>‘What’ Is Happening in the Dorsal Visual Pathway</article-title><source>Trends Cogn Sci</source><year>2016</year><volume>20</volume><fpage>773</fpage><lpage>784</lpage><pub-id pub-id-type="pmid">27615805</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Georgieva</surname><given-names>S</given-names></name><name><surname>Peeters</surname><given-names>R</given-names></name><name><surname>Kolster</surname><given-names>H</given-names></name><name><surname>Todd</surname><given-names>JT</given-names></name><name><surname>Orban</surname><given-names>GA</given-names></name></person-group><article-title>The processing of threedimensional shape from disparity in the human brain</article-title><source>J Neurosci</source><year>2009</year><volume>29</volume><fpage>727</fpage><lpage>742</lpage><pub-id pub-id-type="pmcid">PMC6665151</pub-id><pub-id pub-id-type="pmid">19158299</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4753-08.2009</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Chapman</surname><given-names>CS</given-names></name><name><surname>McLean</surname><given-names>DA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><article-title>Activity patterns in the category-selective occipitotemporal cortex predict upcoming motor actions</article-title><source>Eur J Neurosci</source><year>2013</year><volume>38</volume><fpage>2408</fpage><lpage>2424</lpage><pub-id pub-id-type="pmid">23581683</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knights</surname><given-names>E</given-names></name><etal/></person-group><article-title>Hand-Selective Visual Regions Represent How to Grasp 3D Tools: Brain Decoding during Real Actions</article-title><source>J Neurosci</source><year>2021</year><volume>41</volume><fpage>5263</fpage><lpage>5273</lpage><pub-id pub-id-type="pmcid">PMC8211542</pub-id><pub-id pub-id-type="pmid">33972399</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0083-21.2021</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><article-title>Observing learned object-specific functional grasps preferentially activates the ventral stream</article-title><source>J Cogn Neurosci</source><year>2010</year><volume>22</volume><fpage>970</fpage><lpage>984</lpage><pub-id pub-id-type="pmid">19413481</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Cant</surname><given-names>JS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><article-title>Representation of object weight in human ventral visual cortex</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>1866</fpage><lpage>1873</lpage><pub-id pub-id-type="pmid">25065755</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keisker</surname><given-names>B</given-names></name><name><surname>Hepp-Reymond</surname><given-names>MC</given-names></name><name><surname>Blickenstorfer</surname><given-names>A</given-names></name><name><surname>Kollias</surname><given-names>SS</given-names></name></person-group><article-title>Differential representation of dynamic and static power grip force in the sensorimotor network</article-title><source>Eur J Neurosci</source><year>2010</year><volume>31</volume><fpage>1483</fpage><lpage>1491</lpage><pub-id pub-id-type="pmid">20384781</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groen</surname><given-names>IIA</given-names></name><name><surname>Dekker</surname><given-names>TM</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name></person-group><article-title>Visuospatial coding as ubiquitous scaffolding for human cognition</article-title><source>Trends Cogn Sci</source><year>2022</year><volume>26</volume><fpage>81</fpage><lpage>96</lpage><pub-id pub-id-type="pmid">34799253</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almeida</surname><given-names>J</given-names></name><name><surname>Fintzi</surname><given-names>AR</given-names></name><name><surname>Mahon</surname><given-names>BZ</given-names></name></person-group><article-title>Tool manipulation knowledge is retrieved by way of the ventral visual object processing pathway</article-title><source>Cortex</source><year>2013</year><volume>49</volume><fpage>2334</fpage><lpage>2344</lpage><pub-id pub-id-type="pmcid">PMC3795789</pub-id><pub-id pub-id-type="pmid">23810714</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2013.05.004</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergström</surname><given-names>F</given-names></name><name><surname>Wurm</surname><given-names>M</given-names></name><name><surname>Valério</surname><given-names>D</given-names></name><name><surname>Lingnau</surname><given-names>A</given-names></name><name><surname>Almeida</surname><given-names>J</given-names></name></person-group><article-title>Decoding stimuli (tool-hand) and viewpoint invariant grasp-type information</article-title><source>Cortex</source><year>2021</year><volume>139</volume><fpage>152</fpage><lpage>165</lpage><pub-id pub-id-type="pmid">33873036</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalénine</surname><given-names>S</given-names></name><name><surname>Buxbaum</surname><given-names>LJ</given-names></name><name><surname>Coslett</surname><given-names>HB</given-names></name></person-group><article-title>Critical brain regions for action recognition: lesion symptom mapping in left hemisphere stroke</article-title><source>Brain</source><year>2010</year><volume>133</volume><fpage>3269</fpage><lpage>3280</lpage><pub-id pub-id-type="pmcid">PMC2965423</pub-id><pub-id pub-id-type="pmid">20805101</pub-id><pub-id pub-id-type="doi">10.1093/brain/awq210</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahon</surname><given-names>BZ</given-names></name><etal/></person-group><article-title>Action-related properties shape object representations in the ventral stream</article-title><source>Neuron</source><year>2007</year><volume>55</volume><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="pmcid">PMC2000824</pub-id><pub-id pub-id-type="pmid">17678861</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2007.07.011</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Bonner</surname><given-names>M</given-names></name></person-group><conf-name>NeurIPS 2020 Workshop SVRHM</conf-name></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almeida</surname><given-names>J</given-names></name><etal/></person-group><article-title>Face-Specific Perceptual Distortions Reveal A View- and Orientation-Independent Face Template</article-title><source>Curr Biol</source><year>2020</year><volume>30</volume><fpage>4071</fpage><lpage>4077</lpage><elocation-id>e4074</elocation-id><pub-id pub-id-type="pmid">32795446</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Almeida</surname><given-names>J</given-names></name><name><surname>Mahon</surname><given-names>BZ</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>The role of the dorsal visual processing stream in tool identification</article-title><source>Psychol Sci</source><year>2010</year><volume>21</volume><fpage>772</fpage><lpage>778</lpage><pub-id pub-id-type="pmcid">PMC2908271</pub-id><pub-id pub-id-type="pmid">20483820</pub-id><pub-id pub-id-type="doi">10.1177/0956797610371343</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vannuscorps</surname><given-names>G</given-names></name><name><surname>Galaburda</surname><given-names>A</given-names></name><name><surname>Caramazza</surname><given-names>A</given-names></name></person-group><article-title>The form of reference frames in vision: The case of intermediate shape-centered representations</article-title><source>Neuropsychologia</source><year>2021</year><volume>162</volume><elocation-id>108053</elocation-id><pub-id pub-id-type="pmid">34624257</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name></person-group><article-title>Why have multiple cortical areas?</article-title><source>Vision Res</source><year>1986</year><volume>26</volume><fpage>81</fpage><lpage>90</lpage><pub-id pub-id-type="pmid">3716216</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><source>Readings in Computer Vision</source><person-group person-group-type="editor"><name><surname>Fischler</surname><given-names>Martin A</given-names></name><name><surname>Firschein</surname><given-names>Oscar</given-names></name></person-group><publisher-name>Morgan Kaufmann</publisher-name><year>1987</year><fpage>534</fpage><lpage>550</lpage></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kootstra</surname><given-names>G</given-names></name><etal/></person-group><article-title>Enabling grasping of unknown objects through a synergistic use of edge and surface information</article-title><source>The International Journal of Robotics Research</source><year>2012</year><volume>31</volume><fpage>1190</fpage><lpage>1213</lpage><pub-id pub-id-type="doi">10.1177/0278364912452621</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sato</surname><given-names>T</given-names></name><etal/></person-group><article-title>Object representation in inferior temporal cortex is organized hierarchically in a mosaic-like structure</article-title><source>J Neurosci</source><year>2013</year><volume>33</volume><fpage>16642</fpage><lpage>16656</lpage><pub-id pub-id-type="pmcid">PMC6618530</pub-id><pub-id pub-id-type="pmid">24133267</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5557-12.2013</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walbrin</surname><given-names>J</given-names></name><name><surname>Almeida</surname><given-names>J</given-names></name></person-group><article-title>High-Level Representations in Human Occipito-Temporal Cortex Are Indexed by Distal Connectivity</article-title><source>J Neurosci</source><year>2021</year><volume>41</volume><fpage>4678</fpage><lpage>4685</lpage><pub-id pub-id-type="pmcid">PMC8260247</pub-id><pub-id pub-id-type="pmid">33849949</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2857-20.2021</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Kentridge</surname><given-names>RW</given-names></name><name><surname>Heywood</surname><given-names>CA</given-names></name><name><surname>Milner</surname><given-names>AD</given-names></name></person-group><article-title>Separate channels for processing form, texture, and color: evidence from FMRI adaptation and visual object agnosia</article-title><source>Cereb Cortex</source><year>2010</year><volume>20</volume><fpage>2319</fpage><lpage>2332</lpage><pub-id pub-id-type="pmid">20100900</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname><given-names>BM</given-names></name><name><surname>Klein</surname><given-names>BP</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><article-title>Topographic representation of numerosity in the human parietal cortex</article-title><source>Science</source><year>2013</year><volume>341</volume><fpage>1123</fpage><lpage>1126</lpage><pub-id pub-id-type="pmid">24009396</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torgerson</surname><given-names>WS</given-names></name></person-group><article-title>Multidimensional scaling of similarity</article-title><source>Psychometrika</source><year>1965</year><volume>30</volume><fpage>379</fpage><lpage>393</lpage><pub-id pub-id-type="pmid">5217606</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name></person-group><article-title>AFNI: software for analysis and visualization of functional magnetic resonance neuroimages</article-title><source>Comput Biomed Res</source><year>1996</year><volume>29</volume><fpage>162</fpage><lpage>173</lpage><pub-id pub-id-type="pmid">8812068</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR Biomed</source><year>1997</year><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmid">9430344</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fabius</surname><given-names>JH</given-names></name><name><surname>Moravkova</surname><given-names>K</given-names></name><name><surname>Fracasso</surname><given-names>A</given-names></name></person-group><article-title>Topographic organization of eye-position dependent gain fields in human visual cortex</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>7925</elocation-id><pub-id pub-id-type="pmcid">PMC9789150</pub-id><pub-id pub-id-type="pmid">36564372</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-35488-8</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fracasso</surname><given-names>A</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name></person-group><article-title>Systematic variation of population receptive field properties across cortical depth in human visual cortex</article-title><source>Neuroimage</source><year>2016</year><volume>139</volume><fpage>427</fpage><lpage>438</lpage><pub-id pub-id-type="pmid">27374728</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kristensen</surname><given-names>S</given-names></name><name><surname>Fracasso</surname><given-names>A</given-names></name><name><surname>Dumoulin</surname><given-names>SO</given-names></name><name><surname>Almeida</surname><given-names>J</given-names></name><name><surname>Harvey</surname><given-names>BM</given-names></name></person-group><article-title>Size constancy affects the perception and parietal neural representation of object size</article-title><source>Neuroimage</source><year>2021</year><volume>232</volume><elocation-id>117909</elocation-id><pub-id pub-id-type="pmid">33652148</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Argall</surname><given-names>BD</given-names></name><name><surname>Saad</surname><given-names>ZS</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><article-title>Simplified intersubject averaging on the cortical surface using SUMA</article-title><source>Hum Brain Mapp</source><year>2006</year><volume>27</volume><fpage>14</fpage><lpage>27</lpage><pub-id pub-id-type="pmcid">PMC6871368</pub-id><pub-id pub-id-type="pmid">16035046</pub-id><pub-id pub-id-type="doi">10.1002/hbm.20158</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lombaert</surname><given-names>H</given-names></name><name><surname>Grady</surname><given-names>L</given-names></name><name><surname>Polimeni</surname><given-names>JR</given-names></name><name><surname>Cheriet</surname><given-names>F</given-names></name></person-group><article-title>Fast brain matching with spectral correspondence</article-title><source>Inf Process Med Imaging</source><year>2011</year><volume>22</volume><fpage>660</fpage><lpage>673</lpage><pub-id pub-id-type="pmid">21761694</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schellekens</surname><given-names>W</given-names></name><name><surname>Petridou</surname><given-names>N</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><article-title>Detailed somatotopy in primary motor and somatosensory cortex revealed by Gaussian population receptive fields</article-title><source>Neuroimage</source><year>2018</year><volume>179</volume><fpage>337</fpage><lpage>347</lpage><pub-id pub-id-type="pmcid">PMC6413921</pub-id><pub-id pub-id-type="pmid">29940282</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.06.062</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Mruczek</surname><given-names>RE</given-names></name><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><article-title>Probabilistic Maps of Visual Topography in Human Cortex</article-title><source>Cereb Cortex</source><year>2015</year><volume>25</volume><fpage>3911</fpage><lpage>3931</lpage><pub-id pub-id-type="pmcid">PMC4585523</pub-id><pub-id pub-id-type="pmid">25452571</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhu277</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destrieux</surname><given-names>C</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Dale</surname><given-names>A</given-names></name><name><surname>Halgren</surname><given-names>E</given-names></name></person-group><article-title>Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</article-title><source>Neuroimage</source><year>2010</year><volume>53</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC2937159</pub-id><pub-id pub-id-type="pmid">20547229</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.06.010</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><etal/></person-group><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><year>2016</year><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="pmcid">PMC4990127</pub-id><pub-id pub-id-type="pmid">27437579</pub-id><pub-id pub-id-type="doi">10.1038/nature18933</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Experimental design and population receptive field modeling.</title><p>Here we show <bold>(A)</bold> the progression of a cycle through different object types for each of the dimensions (M1 and M2). Forty stimuli were presented per cycle, sampling the different levels of the dimension evenly; <bold>(B)</bold> the distribution of the different cycles and orders per run and sessions. Participants went through 24 runs per dimension (in a total of 48 runs), 12 cycling the target dimension in one order (i.e., following ascending dimensional values), and 12 cycling in the reverse order. Each run cycled the dimension 6 times; <bold>(C)</bold> We adapted population receptive field modeling (pRF)<sup><xref ref-type="bibr" rid="R7">7</xref></sup> to object-related dimensions. In this flow chart, we describe an example neural gaussian model for a particular voxel with particular tuning properties (preferred location in the dimension – μ – and tuning width – σ). The predicted time series for the selected voxel was calculated by passing the dimension-related stimulus presentation through the gaussian model. The resulting time course was then convolved with the hemodynamic response function. This created different fMRI predicted time courses for the different neural models tested, per voxel. Final model parameters – μ and σ – are obtained by minimizing the difference between the predicted and the observed fMRI data.</p></caption><graphic xlink:href="EMS192491-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Population receptive field modeling of object dimensions, and neural population tuning.;</title><p><bold>(A)</bold> Insets 1 to 5 show example fMRI time courses for different voxels for one dimension (M1). Inset 6 shows the location of the sampled voxels, as presented on the inflated right hemisphere of one participant. Each dot in the fMRI time course plot corresponds to blood oxygen level–dependent (BOLD) percent signal change. The red line corresponds to the model with the best fit that was attributed to the voxel, with the corresponding variance explained. Insets 1 to 3 show voxels whose time courses are well explained by the neural models created, whereas voxels 4 to 5 show voxels whose time courses are poorly explained by the neural models tested. <bold>(B)</bold> Group averaged goodness-of-fit maps of M1 and M2 dimensions. We defined our individual ROIs based on the four main clusters that survived thresholding at R<sup>2</sup> = 0.10. We used a leave-one-participant-out approach to define the ROIs individually per participant and hemisphere. These include dorsal (d-occ) and ventral (v-occ) occipital association cortex. For visualization purposes, we delineated the ROIs based on the averaged individual ROIs. Moreover, we also created a control region of similar shape and size in fronto-lateral cortex (see Methods for details). <bold>(C)</bold> Distribution of goodness-of-fit (R<sup>2</sup>) and tuning parameters (preferred location in the dimension and tuning width) across participants for each dimension and each ROI. M1 and M2 yielded similar goodness-of-fit estimates across ROIs, that were systematically better than the goodness-of-fit estimates at the control ROI (ctrl, please note that data from the control ROI was not thresholded, as no voxels would survive a threshold of R<sup>2</sup> = 0.10). M1 yielded an average higher location parameter compared to M2 across ROIs. M1 tuning was significantly lower compared to M2, indicating narrower tuning for the former compared to the latter. Moreover, and in line with the fact that, in general, extremely sharp tuning widths in pRFs are associated with noise-dominated data<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, we obtained sharp tuning width estimates and low goodness-of-fit in the control region, in the context of width estimates in d-occ and v-occ that were considerably broader. Bonferroni corrected *<italic>p</italic>-values below 0.05; **<italic>p</italic>-values below 0.01; ***<italic>p</italic>-values below 0.001.</p></caption><graphic xlink:href="EMS192491-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Estimation of topographical maps.</title><p><bold>(A)</bold> Gradual progression of the estimated location parameter on the d-occ ROIs for one participant, M1 dimension. <bold>(B)</bold> Cortical distance along the main ROI axis for one hemisphere (d-occ ROI) and multiple rotations between 0 (the output of the spectral decomposition, see Methods) and 2π (6.28 radians). <bold>(C)</bold> Four examples of cortical distance rotations and their relationship with the estimated location parameter that are shown in panel A. Panel C1 shows the best linear fit (t-value = 38.6) compared to panels C2, C3 and C4. Blue lines show the same data, but with the dimensional scores binned and averaged along 10 equally sized bins and shown across the cortical distance.</p></caption><graphic xlink:href="EMS192491-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Contentopic mapping for location.</title><p><bold>(A)</bold> Average location parameter across 8 participants for M1 and M2 and average best orientations capturing the gradual representation of the location parameter. <bold>(B)</bold> Comparison of the t-statistic of the best orientation for each ROI (d-occ, v-occ and ctrl; please note that data from each ROI was thresholded based on the median R<sup>2</sup> to provide a similar proportion of voxels across ROIs. Median R<sup>2</sup> for d-occ and v-occ ~ 0.11, median R<sup>2</sup> for ctrl ~ 0.04; see Table S3) and each dimension. Results indicate a better linear fit between cortical distance and location parameter in d-occ and v-occ for M1 and M2, compared to the control ROI. <bold>(C)</bold> Within-dimension cross-validation of the best location maps using a leave-one-participant-out procedure (see Methods). We assessed the similarity between the averaged best location map of the N-1 participants, and the best location map of the single-out participant using a linear regression model. We extracted the model explained variance (R<sup>2</sup>) and slope (beta coefficient). Note that a negative slope indicates estimates in opposite topographical direction (i.e., similar orientation but different direction of the map), whereas a positive slope indicates estimates in the same direction. Results indicate that we can successfully predict the single-subject best location map from the average or the N-1 group, for each ROI (d-occ and v-occ) compared to the control (ctrl). <bold>(D)</bold> Between-dimension cross validation. Same procedure as in C, but applied between dimensions. Results indicate that we cannot predict the best location map of the singled-out participant for one of the dimensions (e.g., M1) from the average of the N-1 group when using the best location maps of the other dimension (e.g., M2). This is evidence for the specificity of M1 and M2 topography, which is further corroborated by the difference in average location and widths parameter shown in <xref ref-type="fig" rid="F2">Figure 2D</xref>.</p></caption><graphic xlink:href="EMS192491-f004"/></fig></floats-group></article>