<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS188417</article-id><article-id pub-id-type="doi">10.1101/2023.09.22.559020</article-id><article-id pub-id-type="archive">PPR728818</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dynamic vocal learning in adult marmoset monkeys</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Phaniraj</surname><given-names>Nikhil</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Wierucka</surname><given-names>Kaja</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Burkart</surname><given-names>Judith M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A5">5</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute of Evolutionary Anthropology, University of Zurich; Winterthurerstrasse 190, 8057 Zürich, Switzerland</aff><aff id="A2"><label>2</label>Neuroscience Center Zurich (ZNZ), University of Zurich and ETH Zurich; Winterthurerstrasse 190, 8057 Zürich, Switzerland</aff><aff id="A3"><label>3</label>Department of Biology, Indian Institute of Science Education and Research (IISER) Pune; Dr. Homi Bhabha Road, Pune 411008, India</aff><aff id="A4"><label>4</label>Behavioural Ecology and Sociobiology Unit, German Primate Center – Leibniz Institute for Primate Research, Kellnerweg 4, 37077 Göttingen, Germany</aff><aff id="A5"><label>5</label>Center for the Interdisciplinary Study of Language Evolution (ISLE), University of Zurich; Affolternstrasse 56, 8050 Zürich, Switzerland</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author. <bold>Materials and correspondence</bold> All material requests and correspondence should be addressed to Nikhil Phaniraj. <email>nikhil.phaniraj@uzh.ch</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>24</day><month>09</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>09</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license><license><ali:license_ref>https://europepmc.org/downloads/openaccess</ali:license_ref><license-p>This preprint is made available via the <ext-link ext-link-type="uri" xlink:href="https://europepmc.org/downloads/openaccess">Europe PMC open access subset</ext-link>, for unrestricted research re-use and secondary analysis in any form or by any means with acknowledgement of the original preprint source.</license-p></license></permissions><abstract><p id="P1">While vocal learning is vital to language acquisition in children, adults continue to adjust their speech while adapting to different social environments in the form of social vocal accommodation (SVA). Even though adult and infant vocal learning seemingly differ in their properties, whether the mechanisms underlying them differ remains unknown. The complex structure of language creates a challenge in quantifying vocal changes during SVA. Consequently, animals with simpler vocal communication systems are powerful tools for understanding the mechanisms underlying SVA. Here, we tracked acoustic changes in the vocalizations of adult common marmoset pairs, a highly vocal primate species known to show SVA, for up to 85 days after pairing with a new partner. We identified four properties of SVA in marmosets: (1) bidirectional learning, (2) exponential decrease in vocal distance with time, (3) sensitivity to initial vocal distance, and (4) dyadic acoustic feature synchrony. We developed a mathematical model that shows all four properties. The model suggests that marmosets continuously update the memory of their partners’ vocalizations and modify their own vocalizations to match them, a dynamic form of vocal learning. The model provides crucial insights into the mechanisms underlying SVA in adult animals and how they might differ from infant vocal learning.</p></abstract></article-meta></front><body><p id="P2">Human language, the most complex vocal communication system among animals, is grounded in vocal production learning (VPL) – the ability to alter one’s vocalization properties in response to hearing other vocalizations<sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref></sup>. The past few decades of VPL research has had a strong focus on songbirds, the first non-human clade in which this trait was discovered<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. Many juvenile songbirds learn their songs during the early days of their life from a tutor, similar to how human infants learn speech from adults<sup><xref ref-type="bibr" rid="R5">5</xref></sup>. It has been hypothesized that songbirds can do so by memorizing the tutor’s song, forming an auditory template, and matching their own songs to the template<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref></sup>. This hypothesis and the various neurobiological experiments that support it (reviewed in<sup><xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>) provide insights into the ways in which nervous systems can achieve VPL.</p><p id="P3">Even though the most striking example of VPL with the most drastic changes in vocalizations is speech development in human infants, human adults continue to show VPL as they modify their speech properties to adapt to changing social environments. This is visible as social vocal accommodation (SVA), a form of VPL where human interlocutors converge in speech properties such as fundamental frequency, speech rate, word choice and pronunciation, among others<sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>. A major feature that distinguishes SVA from VPL shown by human infants and juvenile songbirds is that learning is largely unidirectional in the latter, i.e., infants learn to imitate adult vocalizations. In contrast, learning during SVA can be bidirectional, where both isnterlocutors modify speech properties, leading to vocal convergence. This suggests differences in the nature of the processes underlying VPL in infants and adults.</p><p id="P4">The processes underlying SVA in humans remain largely unknown as the complex structure of language creates an enormous challenge for revealing the mechanisms. As in the case of speech development, SVA research would highly benefit from an animal model with a relatively simple vocal communication system eliciting a similar phenomenon. Common marmosets (<italic>Callithrix jacchus</italic>) stand out among non-human primates as a compelling model due to their remarkable vocal flexibility, which is based in their highly social nature and a cooperative breeding system<sup><xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R16">16</xref></sup>. Marmosets have been shown to possess dialects<sup><xref ref-type="bibr" rid="R17">17</xref></sup>, and translocation experiments have confirmed the presence of SVA in different call types<sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>. In this study, we took advantage of this system to comprehensively analyze the temporal dynamics of SVA and arrive at a mathematical model explaining its mechanisms. To study the temporal dynamics of SVA, we used the marmoset SVA dataset collected by Zürcher et al. <sup><xref ref-type="bibr" rid="R18">18</xref></sup> and chose 7 pairs for which recordings with high temporal resolution were available (n=14 marmosets: 7 males and 7 females). Adult marmoset individuals of the opposite sex, originally from groups possessing different dialects <sup><xref ref-type="bibr" rid="R17">17</xref></sup>, were paired. Their vocalizations were recorded before pairing, and repeatedly after pairing, until approximately 60 days after pairing (61 ± 16 days, median ± std.) and overall, all vocalizations (trills, phees and food calls) converged over time (18). However, as noted by Cohen-Priva and Sanker<sup><xref ref-type="bibr" rid="R20">20</xref></sup>, current methods for quantifying SVA may suffer from underestimation due to poor selection of acoustic features whose baseline levels are already similar in the beginning among interlocutors. To overcome this and provide a highly accurate description of the amount and temporal dynamic of marmoset SVA, we used the top 20 acoustic features that best distinguished the calls of the individuals out of a total of 3255 features, as determined in Phaniraj et al.<sup><xref ref-type="bibr" rid="R21">21</xref></sup> using a machine-learning classifier (<xref ref-type="supplementary-material" rid="SD1">Table S1</xref>). To quantify vocal accommodation, for each pair, we measured the Euclidean distance between the centroids (in the acoustic feature space) of all trill calls obtained from the two individuals before and after pair formation. To study the temporal dynamics of SVA we focused on trills because (1) the extent of SVA was highest in trills<sup><xref ref-type="bibr" rid="R19">19</xref></sup>, and (2) they are close-distance contact calls and are therefore always directed towards the partner. We analyzed a total of 5842 trills across the 14 individuals (1088 before pairing and 4754 after pairing).</p><p id="P5">In the first step, we analyzed the directionality of VPL in each dyad and how the vocal distance between the dyads changed over time. Moreover, preliminary analyses on a smaller trill dataset suggested that, over time, dyads show synchronized changes in some call parameters, such as fundamental frequency<sup><xref ref-type="bibr" rid="R22">22</xref></sup>. We, therefore, also analyzed to what extent changes in call structure are synchronized within a dyad using Fréchet distance<sup><xref ref-type="bibr" rid="R23">23</xref></sup> and Dynamic Time Warping measures<sup><xref ref-type="bibr" rid="R24">24</xref></sup>.</p><p id="P6">In the second step, we compiled four biologically plausible mathematical models that could explain the patterns of SVA in marmosets. The first model, the <italic>Initial Auditory Template Matching (IATM)</italic> was inspired by Konishi’s model for birdsong learning<sup><xref ref-type="bibr" rid="R25">25</xref></sup> and assumes that marmosets would form an initial, static auditory template of the partner’s call and match their own calls to the static template (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The second model, the <italic>Convergence to Intermediate Value (CIV)</italic> was inspired by the preactive and latent auditory templates hypothesized by Marler and Nelson<sup><xref ref-type="bibr" rid="R26">26</xref></sup> and assumes that in addition to the initial auditory template of the partner’s call (static acquired template), marmosets possess a static internal template of their own call, which may have previously played a role in vocal development, and both these templates drive VPL (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). We build the third model, the <italic>Dynamic Auditory Template Matching (DATM)</italic> over <italic>IATM</italic> and assume that the auditory template is dynamic and takes into account any changes in the partner’s vocalizations (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Similar to <italic>DATM</italic>, the fourth model, the <italic>Dynamic Convergence to Intermediate Value (DCIV)</italic> is built upon a previous model, the <italic>CIV</italic>, and assumes that both the internal and acquired templates are dynamic (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). We found the dynamic models <italic>(DATM and DCIV)</italic> to best explain the properties of marmoset SVA. We finally confirm this by simulating SVA in virtual marmoset pairs using the dynamic model and comparing its properties to the empirical data.</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>Properties of SVA in trills</title><p id="P7">We noted four characteristics of SVA in marmoset trill calls (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). <bold>(1) Bidirectional learning:</bold> males and females underwent equal amounts of vocal change during SVA (<xref ref-type="fig" rid="F2">Fig. 2D</xref>, n = 7 pairs, t = -0.094, df = 6, p = 0.928, two-tailed paired t-test), hence confirming that learning during marmoset SVA is bidirectional. Therefore, in this respect, VPL in adult marmosets is similar to adult humans and different from human infants and juvenile songbirds. <bold>(2) Exponential decrease of vocal distance with time:</bold> the vocal distance between pairs decayed exponentially after pairing (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). <bold>(3) Sensitivity to initial vocal distance:</bold> the amount of SVA undergone by the pair was significantly positively correlated to the initial vocal distance between them before pairing (<xref ref-type="fig" rid="F2">Fig. 2C</xref>, Spearman’s correlation coefficient r = 0.89, p = 0.01). <bold>(4) Dyadic acoustic feature synchrony:</bold> we developed two ways to measure synchronized changes in acoustic features of calls of dyads over time that were independent of the amount of convergence between individuals using Dynamic Time Warping (DTW) and Fréchet distance measures. Synchrony was found to be significantly higher in actual pairs than in control pairs (i.e. all possible combinations of different-sex dyads in the data set), visible as lower Fréchet and DTW distance values (n=7 actual pairs and 42 control pairs; <xref ref-type="fig" rid="F2">Fig. 2E</xref>, t-value = -4.64, df = 7.06, p = 0.0023 for Fréchet distance; <xref ref-type="supplementary-material" rid="SD1">Fig S1A</xref>, t-value = -3.02, df = 7.16, p = 0.0188 for DTW distance; two-tailed Welch’s t-test). We visualized this synchrony by reducing the 20-dimensional feature space to 3 principal components that explained most of the variation and plotting the vocal trajectories taken by the male and the female in a pair with time (<xref ref-type="supplementary-material" rid="SD1">Movie S1</xref>). The DTW and Fréchet distance values of pairs were highly correlated (<xref ref-type="supplementary-material" rid="SD1">Fig. S1B</xref>, n = 7 pairs, r = 0.94, p = 0.02, Pearson’s correlation). Note that this dyadic acoustic feature synchrony is also apparent when the amount of convergence between individuals is not accounted for (<xref ref-type="supplementary-material" rid="SD1">Fig. S2A</xref>, n = 7 actual pairs and 42 control pairs, t-value = -8.09, df = 6.42, p = 1.35e-04, two-tailed Welch’s t-test) and while looking at dyadic phase coherence of the principal component of commonly used spectral features for analyzing marmoset vocalizations (mean fundamental frequency, mean spectral entropy, frequency of amplitude modulation and call duration) (<xref ref-type="supplementary-material" rid="SD1">Fig. S2B</xref>, n = 7 actual pairs and 42 control pairs, t-value = 2.27, df = 9.61, p = 0.047, two-tailed Welch’s t-test) and fundamental frequency of the call alone (<xref ref-type="supplementary-material" rid="SD1">Fig. S2C</xref>, n = 7 actual pairs and 42 control pairs, t-value = 3.20, df = 8.70, p = 0.0113, two-tailed Welch’s t-test).</p><p id="P8">Finally, we visualized the dynamics of SVA in trills by plotting the acoustic feature values of females (v<sub>f,i</sub>) against their partner males (v<sub>m,i</sub>) on a phase portrait (<xref ref-type="fig" rid="F2">Fig. 2F</xref>). We note 3 attributes of the phase portrait. (1) The equilibrium points lie approximately within an ellipse whose major axis lies along v<sub>m,i</sub>=v<sub>f,i</sub>. (2) Most arrows point toward the ellipse suggesting the system moves towards v<sub>m,i</sub>=v<sub>f,i</sub>. (3) Arrows further away from the ellipse are longer, representing the higher rate of change of feature values when the vocal distance between the male and the female was larger.</p></sec><sec id="S3"><title>Modelling the temporal dynamics of SVA in trills</title><p id="P9">The convergence of trills can be achieved in four ways. We developed a mathematical model for each and compared the properties of the models to the real marmoset SVA data. Each call was represented in the model using 20 acoustic features, and all models assumed that marmosets could modify each acoustic feature (v<sub>m,i</sub> or v<sub>f,i</sub>, i<sup>th</sup> acoustic feature of male or female calls respectively) at every time step (t) independently of the other. Each model was characterized using a separate learning rate parameter for the male (α<sub>m</sub>) and the female (α<sub>f</sub>), which, when put together, determined the rate of convergence of the pair during SVA. We also separately modelled errors E<sub>m</sub>X<sub>t</sub> and E<sub>f</sub>X<sub>t</sub> for males and females, respectively, where the errors are a product of the error term E and a Gaussian noise term X at time t (X ~ N(0, 1)).</p><sec id="S4"><title>Initial Auditory Template Matching (IATM)</title><p id="P10">In this model, marmosets are assumed to follow the following steps to achieve vocal convergence (<xref ref-type="fig" rid="F1">Fig. 1A</xref>): <list list-type="order" id="L1"><list-item><p id="P11">When paired, marmosets initially form an auditory template of the trills of their partner.</p></list-item><list-item><p id="P12">Marmosets modify their trills to match the auditory template.</p></list-item><list-item><p id="P13">Marmosets continue to modify their trills to match the template. The template itself is static and is not updated to account for changes in the partner’s trills.</p></list-item></list></p><p id="P14">IATM is the most parsimonious of all models as it does not require updating the auditory template. It is represented by the following equations: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="S5"><title>Convergence to Intermediate Value (CIV)</title><p id="P15">In CIV, we assume that in addition to an acquired auditory template (from their partner), marmosets possess a template of their own trill (internal template), which might have previously played a role in vocal development. We build CIV as follows (<xref ref-type="fig" rid="F1">Fig. 1B</xref>): <list list-type="order" id="L2"><list-item><p id="P16">When paired, marmosets form an auditory template of the trills of their partner (acquired template) in addition to the internal template that they already possess.</p></list-item><list-item><p id="P17">Both the marmoset’s own internal template and the acquired template drive VPL. Marmosets modify their trills to match both templates.</p></list-item><list-item><p id="P18">Marmosets continue to modify their trills to match both templates. However, akin to IATM, the templates are not updated.</p></list-item></list></p><p id="P19">For the mathematical modelling of CIV, we chose the simplest case where both the internal and acquired templates equally affected the trills. The equations for CIV are as follows: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="S6"><title>Dynamic Auditory Template Matching (DATM)</title><p id="P20">Building on IATM, we add a layer of complexity by making the auditory template dynamic and sensitive to changes in the partner’s trills. This model can be summarized as follows (<xref ref-type="fig" rid="F1">Fig. 1C</xref>): <list list-type="order" id="L3"><list-item><p id="P21">When paired, marmosets form an auditory template of the trills of their partner.</p></list-item><list-item><p id="P22">Marmosets modify their trills to match the auditory template.</p></list-item><list-item><p id="P23">The template is continuously updated to account for changes in the partner’s trills, i.e., every time the trill structure of the partner changes, this changed trill acts as the new template. Marmosets modify their trills to match the updated template. Step 3 repeats.</p></list-item></list></p><p id="P24">In this model, marmosets will be able to account for errors or abrupt changes in the trills of the partner during SVA. DATM was modelled using the following equations: <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="S7"><title>Dynamic Convergence to Intermediate Value (DCIV)</title><p id="P25">Building upon CIV, we added a layer of complexity such that the internal and acquired templates are dynamic. According to DCIV (<xref ref-type="fig" rid="F1">Fig. 1D</xref>): <list list-type="order" id="L4"><list-item><p id="P26">Marmosets already have an internal template before pairing. When paired, marmosets additionally form the acquired template.</p></list-item><list-item><p id="P27">Trills are modified to match both templates.</p></list-item><list-item><p id="P28">Marmosets retain a memory of their own trill before modifying their trills. This becomes their new internal template. The acquired template is adjusted to account for the changes in the partner’s trills. Therefore, both the internal template and the acquired template are dynamic.</p></list-item></list></p><p id="P29">Similar to CIV, we chose the case where both the dynamic internal template and the dynamic acquired template are given equal weightage when marmosets match their trills to them. DCIV was modelled using the following equations: <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P30">We can show that DCIV displays the same dynamics as DATM with half the learning rate as rearranging the above equation gives: <disp-formula id="FD5"><mml:math id="M5"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P31">Therefore DATM/DCIV will be henceforth referred to as the “dynamic model”.</p><p id="P32">To be able to explain the synchronization of acoustic features between individuals in a pair, we added an error term to every model. Based on our observations of the nature of residuals during acoustic feature synchrony, where they decreased with time (<xref ref-type="supplementary-material" rid="SD1">Movie S1</xref>), we came up with 2 different error terms: <list list-type="order" id="L5"><list-item><p id="P33"><italic>E</italic> = <italic>f(t):</italic> “Error is a function of time”. The classic learning curve where the error rate exponentially decreases with time given by: <disp-formula id="FD6"><mml:math id="M6"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P34">Where x=m for males and x = f for females. β and γ are error coefficients and t is time.</p><p id="P35">To ensure that the dynamics shown by the model is invariant under the same error coefficients but linearly transformed initial acoustic feature values, the error term was modified as following: <disp-formula id="FD7"><mml:math id="M7"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P36"><italic>E</italic> = <italic>f(dv/dt):</italic> Arising from the possibility that the more the marmosets modify their calls, the more they are prone to making errors, we modelled the error to be a linear function of the amount of change in the acoustic feature value. This can be written as: <disp-formula id="FD8"><mml:math id="M8"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P37">Where x=m for males and x=f for females. δ and ∊ are error coefficients, and t is time.</p><p id="P38">To make the model invariant under linear transformation of the initial acoustic feature values, we modified it to: <disp-formula id="FD9"><mml:math id="M9"><mml:mrow><mml:msub><mml:mtext>E</mml:mtext><mml:mi>x</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p id="P39">We simulated 7 virtual pairs with the trills of the virtual marmosets characterized by 20 acoustic features so that we could compare the temporal dynamics shown by the model to SVA in trills of actual pairs. We examined which of the models showed significant synchrony in acoustic features. For simplicity, we set the learning rates (α) and error coefficients (β, γ, δ and ∊) of males and females to be equal. We varied these parameters within a suitable range (<xref ref-type="supplementary-material" rid="SD1">Table S2</xref>) and let the system evolve for 60 iterations (equivalent to 60 days). We found significant acoustic synchrony in a considerable range of parameter values in the case of the dynamic model with both types of error terms (<xref ref-type="fig" rid="F3">Fig. 3E and 3H</xref>). In contrast, acoustic synchrony was only found in sparse patches for IATM and CIV only in the case of the <italic>E</italic> = <italic>f(dv/dt)</italic> error term. These results were robust to changes in acoustic feature initialization and the seed of the random number generator (<xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref> and <xref ref-type="supplementary-material" rid="SD1">S4</xref>).</p><p id="P40">Within the range of learning rate and error coefficient values for which significant acoustic synchrony was found, we randomly chose a set of values for the model, performed simulations for 60 iterations (equivalent to 60 days), and visualized the temporal dynamics on a phase portrait. The goal was to find the closest match to the phase portrait obtained from actual data (in <xref ref-type="fig" rid="F2">Fig. 2F</xref>). Phase portraits of only the dynamic models were characterized by an approximately ellipsoid region of equilibrium points with the major axis along v<sub>m,i</sub> = v<sub>f,i</sub> (<xref ref-type="fig" rid="F3">Fig. 3E and 3H</xref>). Even though the equilibrium points in the dynamic model with <italic>E</italic> = <italic>f(dv/dt)</italic> error term lie within a strip along v<sub>m,i</sub> = v<sub>f,i</sub> (<xref ref-type="fig" rid="F3">Fig. 3F</xref>), we included the model for later analyses as it is possible that the strip is part of a larger ellipse. Based on the above observations, we concluded that the dynamic model best explains the dynamics of SVA in trills.</p></sec></sec><sec id="S8"><title>Properties of the dynamic model</title><sec id="S9"><title>Bidirectional learning</title><p id="P41">With the selected model, we sought to determine the nature of the error term that would provide the best fit of the dynamic model to the data. The <italic>E</italic> = <italic>f(t)</italic> error term gave a better fit with significantly higher adjusted R<sup>2</sup> values (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, n = 14, p = 1.2e-04, Wilcoxon signed-rank test) and substantially lower leave-one-out cross-validated normalized root mean square error (LOOCV NRMSE) values (<xref ref-type="fig" rid="F4">Fig. 4B</xref>, n = 14, p = 1.2e-04, Wilcoxon signed-rank test). Further, the learning rates obtained from fitting the dynamic model to the trill data were similar between males and females (<xref ref-type="fig" rid="F4">Fig. 4C</xref>, n = 7 pairs, t-value = -1.39, df = 6, p = 0.21, two-tailed paired t-test), confirming equal contributions of males and females towards vocal convergence during SVA.</p></sec><sec id="S10"><title>Exponential decrease of vocal distance with time</title><p id="P42">We fit a distribution over the learning rates (Gaussian) and error coefficients (gamma) from the dynamic model fit separately for males and females. From these distributions, we randomly extracted learning rates and error coefficients for the simulation. We performed simulations of 7 models (each symbolizing one actual marmoset pair in the data) for 60 iterations (equivalent to 60 days) and compared the temporal progression of SVA in the model to actual data. We repeated the same but this time using the median learning rates and error coefficients and simulated the “average model”. The nature of the decrease in vocal distance between virtual pairs in the models closely resembled the actual ones, both showing exponential decay (<xref ref-type="fig" rid="F4">Fig. 4D, E and F</xref>).</p></sec><sec id="S11"><title>Sensitivity to initial vocal distance</title><p id="P43">Along with studying the temporal progression of SVA in the dynamic models, we also examined the relationship between the total amount of SVA in a pair between t = 0 and t = 60 and the initial vocal distance. We found the amount of SVA in the dynamic models to be positively correlated with the initial vocal distance (<xref ref-type="fig" rid="F4">Fig. 4G</xref>, Spearman’s correlation coefficient r = 0.89, p = 0.01).</p></sec><sec id="S12"><title>Dyadic acoustic feature synchrony</title><p id="P44">Even though our model selection was based on the presence of acoustic feature synchrony for a wide range of model parameter values, the parameter values of males and females were tied to be equal during model simulations for <xref ref-type="fig" rid="F3">Fig. 3A</xref>-<xref ref-type="fig" rid="F2">2F</xref>. We sought to confirm whether acoustic feature synchrony remained in the simulated models. We found significant vocal synchrony in the 7 virtual pairs compared to virtual control pairs, confirming that acoustic feature synchrony can arise automatically in such models (<xref ref-type="fig" rid="F4">Fig. 4H</xref> for median parameters, t-value = -16.65, df = 9.05, p = 4.28e-08, two-tailed Welch’s t-test; <xref ref-type="fig" rid="F4">Fig. 4I</xref> for parameters taken from a distribution fit, t-value = -2.827, df = 6.117, p = 0.030, two-tailed Welch’s t-test). We visualized this acoustic synchrony by plotting the trajectories taken by the virtual male and the female in the acoustic feature space upon dimensional reduction (<xref ref-type="supplementary-material" rid="SD1">Movie S2</xref>).</p></sec></sec></sec><sec id="S13" sec-type="discussion"><title>Discussion</title><sec id="S14"><title>The properties of vocal learning in adult marmosets</title><p id="P45">When marmosets were paired with a new partner, their calls became more similar to each other. This accommodation was strongest in trills, which are close-distance contact calls<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Using highly accurate representations of trill calls<sup><xref ref-type="bibr" rid="R21">21</xref></sup>, we found four distinct patterns when pairs of marmosets engage in SVA. First, we were interested in who would accommodate to whom (i.e., the directionality). One hypothesis is that individuals more interested in forming a pair bond would accommodate more than their partner (10). This could arguably be the males because they are interested in siring offspring as soon as possible and also tend to show higher rates of affiliative behaviors early during pair formation<sup><xref ref-type="bibr" rid="R27">27</xref></sup>, or females, because as cooperative breeders, they are interested in forming a strong bond to ascertain future paternal contributions to infant care<sup><xref ref-type="bibr" rid="R28">28</xref></sup>. We neither found that males would vocally accommodate more to females nor vice versa, but both partners adjusted their own calls to approximately equal amounts (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). This is in line with the high levels of mutual affiliative behaviors in newly formed marmoset pairs<sup><xref ref-type="bibr" rid="R27">27</xref></sup> and a mutual interest in a strong bond since both partners mutually depend on a durable, lasting bond to successfully reproduce<sup><xref ref-type="bibr" rid="R28">28</xref></sup>.</p><p id="P46">Second, the decrease in vocal distance is a gradual, exponential process (<xref ref-type="fig" rid="F2">Fig. 2C</xref>). This contrasts with developmental changes in both human and marmoset infants, which are characterized by sudden transitions in the acoustic properties of vocalizations<sup><xref ref-type="bibr" rid="R29">29</xref></sup>. In contrast, our investigation of acoustic changes in adult marmosets suggests a gradual temporal progression with an exponential decrease in vocal distance within dyads. Such a gradual progression while matching a template is consistent with studies on pitch recovery in adult zebra finches<sup><xref ref-type="bibr" rid="R30">30</xref></sup> and indications of gradual temporal progression of SVA in humans<sup><xref ref-type="bibr" rid="R31">31</xref></sup>.</p><p id="P47">Third, the total amount of SVA undergone by a pair is positively correlated to the initial vocal distance between the pair (<xref ref-type="fig" rid="F2">Fig. 2D</xref>). This is logically coherent due to the fact that a large initial vocal distance provides more room for a pair to converge in the vocal space. It is in line with a previous finding on a smaller dataset of marmoset trills using spectral features<sup><xref ref-type="bibr" rid="R19">19</xref></sup> and suggests that VPL is sensitive to initial conditions that must be taken into account when comparing across pairs and developing models of VPL.</p><p id="P48">The fourth and most prominent pattern in marmoset SVA was that the calls of dyads show synchronized changes in acoustic features over time, i.e., they move together through the acoustic space (<xref ref-type="fig" rid="F2">Fig. 2E</xref>, <xref ref-type="supplementary-material" rid="SD1">Movie S1</xref>). Importantly, synchronization was higher in real dyads compared to all virtual control pairs composed of all possible different-sex dyads in the data set. These findings are consistent with previous observations based on traditional acoustic analyses<sup><xref ref-type="bibr" rid="R22">22</xref></sup>. Intriguingly, the tendency to synchronize has also been reported in other domains in marmosets, such as in oxytocin profiles of strongly bonded marmosets<sup><xref ref-type="bibr" rid="R32">32</xref></sup> or in behavioral alignment<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R34">34</xref></sup> and thus, may well be part of a general pattern of bio-behavioral synchrony<sup><xref ref-type="bibr" rid="R35">35</xref></sup>.</p><sec id="S15"><title>The dynamic model best describes adult marmoset vocal learning</title><p id="P49">The four observed properties of adult marmoset SVA were best instantiated by the dynamic model (<xref ref-type="fig" rid="F3">Fig. 3C</xref>, <xref ref-type="fig" rid="F4">Fig. 4C-I</xref>). The model has implications for understanding VPL in adult marmosets. In particular, it predicts the presence of mechanisms responsible for the formation of the dynamic auditory template and matching of trills to the template. Neural mechanisms for detecting differences between the partner’s trills and the marmoset’s own trills may lie in the auditory cortex. Neurons in the marmoset auditory cortex have been shown to be modulated in two different ways during self-initiated vocalizations; one population of neurons is excited during vocal production while the other is suppressed, with the latter being greater in number<sup><xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>. We hypothesize that excited neurons tuned to the marmoset’s own trills and the suppressed neurons tuned to the partner’s trills may together play a role in detecting differences between the structure of one’s own trill and the partner’s trill. Moreover, neurons in the marmoset auditory cortex suppressed by self-generated vocalizations have been shown to be tuned to particular values of frequency and other acoustic features of auditory stimuli, where stimuli included vocalizations of other marmosets in the colony<sup><xref ref-type="bibr" rid="R38">38</xref></sup>. Such neurons, along with a downstream circuit, may play a role in the dynamicity of the auditory template. A “comparison circuit” capable of detecting differences between the responses of the excited neurons and neurons encoding the dynamic template can drive marmosets to update their trills to match their partners’, ultimately leading to SVA.</p><p id="P50">The dynamic model suggests that synchronization in acoustic parameters during SVA can automatically arise when marmosets account for changes in their partner’s trills and modify their own trills accordingly. The model, therefore, provides direct evidence for the Interactive Alignment Model (IAM)<sup><xref ref-type="bibr" rid="R39">39</xref>–<xref ref-type="bibr" rid="R41">41</xref></sup> proposed for vocal accommodation in humans. IAM states that SVA arises automatically due to a tight link between speech production and perception in human interlocutors. Importantly, this does not exclude additional effects of social bond strength on the amount of SVA, as suggested by the Communication Accommodation Theory (CAT)<sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>. This is because the dynamic model cannot explain the variability in learning rates across individuals. We propose that while the mechanisms of SVA are fundamental in driving vocal convergence in adult marmosets and humans, social factors contribute to fine-tuning of the phenomena, which in extreme cases may also lead to vocal divergence as seen during negative social interactions in humans<sup><xref ref-type="bibr" rid="R44">44</xref></sup>. Future studies on the biological function of SVA, perhaps in particular of effects of social bond strengths on the amount of SVA in marmosets, will help disentangle the contributions of fundamental, possibly innate mechanisms and social factors towards SVA.</p><p id="P51">A limitation of the dynamic model is that it assumes that marmosets can vary their acoustic features independently of each other. This may not always be true, as the physiology of the vocal apparatus may add restrictions to how much a particular acoustic feature can be modified given the changes in other acoustic features. Detailed study of the structure of the marmoset vocal apparatus and associated organs, along with computational modelling of vocal production, will help refine the dynamic model.</p><p id="P52">Together, the above evidence gives way to the intriguing possibility that divergent mechanisms can underlie VPL in infant and adult animals. First, the transitions in the acoustic properties are gradual in adults but abrupt in immatures. Second, since infant VPL is directional with a stable set of target (adult) vocalizations, an updating of the auditory template is not necessary. Our model suggests that VPL in adult marmosets requires constant updating of the auditory template to account for changes in partners’ vocalizations. Importantly, to further explore this possibility, future studies will have to carefully compare different types of VPL (e.g., adjusting of acoustic structure as in SVA, acquisition of novel vocalizations and words, acquisition of call combinations and syntactic rules) in both immatures and adults of the same species to unveil which mechanisms are similar/dissimilar during immature vs adult vocal learning. Given the need for solid comparative evidence to understand the evolutionary origin of human language, such systematic comparisons are an effortful but very much needed next step.</p></sec></sec></sec><sec id="S16" sec-type="methods"><title>Methods</title><sec id="S17"><title>Experimental Subjects</title><p id="P53">20 common marmosets (<italic>Callithrix jacchus</italic>), 10 males and 10 females were used to obtain vocal recordings. Marmosets were housed in enclosures (1.8m x 2.4 m x 3.6m) along with at least one other group member. Each group had access to their own outdoor enclosure (1.8m x 2.4m x 4.6m) and a common experimental room. Marmosets were fed a predetermined amount of mush in the morning, cooked and raw vegetables during noon, and insects, cottage cheese or gum Arabic as snacks after 12:00 hours local time. Ad libitum access to water was provided at all times. All experiments were approved by Zürich’s cantonal veterinary office (license ZH223/16).</p></sec><sec id="S18"><title>Pair formation and vocal recordings</title><p id="P54">All experiments and vocal recordings were performed by Zürcher et al.<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. Marmoset vocalizations were recorded under two conditions: before pairing and after pairing. During the before pairing condition, individuals were recorded in their home enclosure on multiple days spread over two to three weeks. Each recording session being about 30 minutes long. For the after pairing condition, males and females were paired (each of the 10 males paired with a female), and vocalizations were recorded at regular intervals on about 15 days (15 ± 4 days, median ± std.) in the period starting from the day of pair formation to up to 60 days after pairing (61 ± 16 days, median ± std.). Individuals were recorded in their home enclosure or the experimental room.</p><p id="P55">Marmoset calls were obtained using a condenser Microphone (CM16/CMPA, Avisoft Bioacoustics, Germany) connected to Avisoft UltraSoundGate 116H (Avisoft Bioacoustics, Germany) at a sampling rate of 62500 Hz and source (individual) identity of all calls were simultaneously labeled using Avisoft Recorder (Avisoft Bioacoustics, Germany). Calls were visualized and segmented using Avisoft Pro (Avisoft Bioacoustics, Germany), and each call was saved in a separate file. See Zürcher et al.<sup><xref ref-type="bibr" rid="R18">18</xref></sup> for detailed information about pair formation, recording procedure, and processing.</p></sec><sec id="S19"><title>Properties of SVA in trills</title><p id="P56">To study the Properties of SVA, we chose 7 pairs (out of the 10) for which recordings with high temporal resolution were available (7 pairs: 7 males and 7 females). For acoustic feature extraction from animal vocalizations, Highly Comparative Time Series Analysis (HCTSA<sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup>) has been shown to be an effective method<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>. This method extracts numerous features directly from the raw acoustic waveform (pressure time series). We extracted features from 5842 marmoset trills recorded both before pairing (1088) and after pairing (4754) of individuals using HCTSA.</p><p id="P57">Previous studies have shown features from animal vocalizations extracted by HCTSA to be useful for supervised classification based on context and source properties<sup><xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R47">47</xref></sup>. In particular, tree-based boosting methods can classify marmoset calls based on the sex and identity of the source with high accuracies<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. Such classifiers also identify the acoustic features in which individual identity signatures are most likely to lie. The top 20 most important features (out of 3255) for classifying marmoset trills were determined. For exact details of the classifier, and classification accuracies, see Phaniraj et al. <sup><xref ref-type="bibr" rid="R21">21</xref></sup>. For a list of the top 20 features and their description, see <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>.</p><p id="P58">Using the top 20 feature set, we aimed to quantify vocal distances between the trills of individuals of every pair during SVA. For this, we calculated the Euclidean distance between centroids wherein the position of the centroids of the two clusters (one for each individual in a pair) in the 20-dimensional feature space (top 20 feature space) was determined, and the Euclidean distance between them was calculated.</p><p id="P59">To compare the amount of vocal change undergone by males and females during SVA, we selected the last day after pairing on which both individuals in a pair were recorded. Features were normalized (z-scored), and the distance between the calls of an individual on the last day and before that individual was paired with a partner was calculated. We used two-tailed paired t-tests to detect significant differences in the amount of vocal change undergone by males and females. To visualize the temporal progression of SVA, we first calculated the vocal distance between individuals using normalized features before they were paired to set a baseline. We then calculated the vocal distance between pairs on all days after pairing when both individuals were recorded. To determine the relationship between the amount of SVA and initial vocal distance, the amount of decrease in vocal distance between a pair from the first day of recording after pairing and the last day of recording was correlated with the vocal distance between the pair on the first day of recording after pairing.</p><p id="P60">To test for the presence of synchrony in vocal features during SVA, we first determined the centroid of the cluster (in the normalized feature space) of all calls given by an individual of a pair on the days they were recorded after pairing with a partner. Next, we performed linear interpolation and obtained centroid locations on the days with no recordings. This provided us with “vocal trajectories” of pairs when centroids of all days were connected. However, the amount of convergence between pairs would influence how similar the vocal trajectories of the individuals in the pair are. We, therefore, decomposed the vocal trajectories into a linear trend and the residuals to remove this influence. For this, multivariate regression was performed on the vocal trajectory (with day as the predictor variable), and residuals were obtained. To quantify vocal feature synchrony, we determined the trajectory similarity between residuals obtained for an individual and its partner. We used two measures for this: <list list-type="order" id="L6"><list-item><p id="P61"><italic>Dynamic Time Warping (DTW) distance</italic>: The DTW implementation on multivariate time series data<sup><xref ref-type="bibr" rid="R48">48</xref></sup> with a window parameter of 1 (maximum window size) was used to calculate the warping cost, which was reported as the distance. Large distance values represent low trajectory similarity values.</p></list-item><list-item><p id="P62"><italic>Fréchet distance</italic>: The discrete Fréchet distance (MATLAB file exchange #31922) between the two trajectories was calculated. Larger values indicate lower similarity.</p></list-item></list></p><p id="P63">As a control, we also calculated trajectory similarity between the residuals of all possible non-bonded male-female pairings in the data. For N pairs, N actual similarity values and N(N-1) control similarity values could be calculated. We used two-tailed Welch’s t-tests for samples with unequal variances to detect significant differences in the Fréchet and DTW distance measures of actual pairs and control pairs. Additionally, we repeated the above analyses without controlling for the amount of convergence between pairs.</p><p id="P64">Dyadic acoustic feature synchrony could also be measured using traditional spectral features. To test if traditional analyses would also detect synchrony, we extracted four commonly used spectral features for marmoset acoustic analyses<sup><xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R49">49</xref></sup> from all calls: mean fundamental frequency, mean spectral entropy, frequency of amplitude modulation and call duration. For this, all calls were filtered to be between the marmoset trill frequency range (4 – 10kHz). The MATLAB function ‘pitch’ was used to estimate the fundamental frequency, and its average over the entire call was used. For determining mean spectral entropy, the MATLAB function ‘spectralEntropy’ was used, and its average over the entire call was reported. To estimate the frequency of amplitude modulation, Hilbert transform was applied (using MATLAB function ‘hilbert’) to the waveform to first obtain the amplitude envelope and its frequency was determined. We performed Principal Component Analysis over z-scores of these four features and determined the Principal Acoustic Component (PAC). Then, from the time series of PAC values for individuals in a pair, we calculated the mean phase coherence. For this, we first bandpass filtered the two time series from a pair between the values [(0.125/length of time series) to 0.5 cycles per day] to remove both extremely low and extremely high frequencies that could contribute to these fluctuations, which are not of our interest. After filtering, we determined instantaneous phases by performing a Hilbert transform using the MATLAB function ‘hilbert’. Finally, we calculated the mean phase coherence using the formula: <disp-formula id="FD10"><mml:math id="M10"><mml:mrow><mml:mi>M</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P65">Where <italic>ϕ<sub>m</sub></italic>(<italic>t</italic>) and <italic>ϕ<sub>f</sub></italic>(<italic>t</italic>) are the instantaneous phases of the male’s and female’s PAC at instant t and T is the duration (days) between day of pairing and the last day of recording after pairing. The above analysis was also repeated with fundamental frequency alone, onto which PAC was heavily loaded.</p><p id="P66">We visualized the vocal trajectories taken during SVA of all pairs using a phase portrait. For a given day after pairing and a given pair, we determined the centroid of all calls in the normalized feature space for the male and the female separately. For every acoustic feature i, we plotted the value for the female (v<sub>f,i</sub> – y coordinate) against the value for the male (v<sub>m,i</sub> – x coordinate). Then, we determined the rate of change of the acoustic feature between that day and the next day of recording (ratio of the difference in acoustic feature value to the number of days passed) for both the male (dv<sub>m,i</sub>/dt) and the female (dv<sub>f,i</sub>/dt). Arrows originating from the points (v<sub>m,i</sub>, v<sub>f,i</sub>) with directional components (dv<sub>m,i</sub>/dt, dv<sub>f,i</sub>/dt) were plotted. We repeated this for all pairs and all days of recording after pairing. To prevent overcrowding of arrows in the phase portrait, we further chose a range of v<sub>m,i</sub> and v<sub>f,i</sub> symmetric around 0 that contained the majority of the data. Within this range, we divided the entire plot into 625 (25x25) squares. Within each square, the mean of all arrow vectors was calculated and the entire box was represented by 1 mean arrow vector.</p></sec><sec id="S20"><title>Modelling the temporal dynamics of SVA in trills</title><p id="P67">Our goal was to come up with the simplest set of mathematical equations that could show all the properties of the temporal dynamics of SVA in trills. Inspired by the auditory template hypothesis of vocal learning in songbirds<sup><xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R26">26</xref></sup>, we hypothesized four ways marmoset pairs could achieve convergence in trills and developed mathematical equations for modelling them. Additionally, to be able to explain the synchronization of acoustic features between individuals in a pair, based on our observations of the nature of residuals during acoustic feature synchrony, we added two types of error terms to every model. (1) As the classical learning curve, the error rate (E) decreases exponentially with time (“Error is a function of time”, <italic>E</italic> = <italic>f(t)</italic>), or (2) The more the individuals change the acoustic features of trills, the more prone they are to making errors (“Error is the function of the rate of change of acoustic feature value”, <italic>E</italic> = <italic>f(dv/dt)</italic>). Both the error terms are comprised of 2 error coefficients, β and γ for <italic>E</italic> = <italic>f(t)</italic>, and δ and ∊ for <italic>E</italic> = <italic>f(dv/dt).</italic> Virtual males and females were given error coefficient values separately. The error terms were constructed such that the model is invariant under changes in the linear transformation of initial conditions. The models are represented using the following equations: <list list-type="order" id="L7"><list-item><p id="P68"><italic>Initial Auditory Template Matching (IATM) with E = f(t) error term:</italic> <disp-formula id="FD11"><label>(1)</label><mml:math id="M11"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P69"><italic>Initial Auditory Template Matching (IATM) with E = f(dv/dt) error term:</italic> <disp-formula id="FD12"><label>(2)</label><mml:math id="M12"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P70"><italic>Convergence to Intermediate Value (CIV) with E = f(t) error term:</italic> <disp-formula id="FD13"><label>(3)</label><mml:math id="M13"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P71"><italic>Convergence to Intermediate Value (CIV) with E = f(dv/dt) error term:</italic> <disp-formula id="FD14"><label>(4)</label><mml:math id="M14"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P72"><italic>Dynamic Auditory Template Matching (DATM) with E = f(t) error term:</italic> <disp-formula id="FD15"><label>(5)</label><mml:math id="M15"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P73"><italic>Dynamic Auditory Template Matching (DATM) with E = f(dv/dt) error term:</italic> <disp-formula id="FD16"><label>(6)</label><mml:math id="M16"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P74"><italic>Dynamic Convergence to Intermediate Value (DCIV) with E = f(t) error term:</italic> <disp-formula id="FD17"><label>(7)</label><mml:math id="M17"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p id="P75"><italic>Dynamic Convergence to Intermediate Value (DCIV) with E = f(dv/dt) error term:</italic> <disp-formula id="FD18"><label>(8)</label><mml:math id="M18"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ε</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mtext>X</mml:mtext><mml:mi>t</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mtext>N</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></list-item></list></p><p id="P76">In all the above models, v<sub>m,i</sub> and v<sub>f,i</sub> are the values of the i<sup>th</sup> acoustic feature of male and female calls respectively, t is time, α<sub>m</sub> and α<sub>f</sub> are the learning rates of the male and the female respectively, β, γ, δ and ∊ are error coefficients, and X<sub><italic>t</italic></sub> is Gaussian noise at time t. The Error term was further modelled in 2 different ways:</p><p id="P77">We simulated 7 virtual marmoset pairs and 20 acoustic feature values (same as in the actual data). We performed numerical simulations of equations (<xref ref-type="disp-formula" rid="FD11">1</xref>-<xref ref-type="disp-formula" rid="FD18">8</xref>) using Euler’s method and the parameter value range mentioned in <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>. Gaussian noise was modelled using MATLAB’s ‘randn’ function, and acoustic feature values were initialized from a uniform distribution using MATLAB’s ‘rand’ function. Then, for each simulation, we tested for the presence of synchrony using the Fréchet distance measure, as described in the previous section. We compared the Fréchet distance between the trajectories (of the residuals of the acoustic feature values) of the virtual marmoset pairs with the virtual control pairs (similar to the case with original data, for 7 virtual pairs, 7 actual similarity values and 42 (7x(7-1)) control similarity values could be calculated). Fréchet distance was preferred over DTW distance as calculating DTW distance requires setting a window parameter which affects the output. Fréchet distance does not require setting any such parameter. Later, we examined if the pattern of synchronization was robust to changes in initial acoustic parameter values or the seed of the random number generators.</p><p id="P78">Finally, for each model, we selected a set of parameters for which synchronization was present, i.e., the Fréchet distance values for the 7 virtual pairs were significantly lower than the 42 (7x(7-1)) virtual control pairs. Then for those parameters, we visualized the temporal dynamics of the model by plotting a phase portrait. The steps followed are the same as those for the original data described in the previous section.</p></sec><sec id="S21"><title>Temporal dynamics of the best model</title><p id="P79">We studied the temporal dynamics of the models in which synchrony was present under a significant range of parameter values and whose phase portraits were qualitatively similar to the phase portrait obtained using actual marmoset data. First, we fit the model without the Gaussian noise term to the actual data using MATLAB’s ‘robustfit’ with the ‘cauchy’ weight function and its default tuning constant and obtained the learning rates (α<sub>m</sub> or α<sub>f</sub>) for every individual. The ‘cauchy’ weight function was chosen as it is smooth, differentiable, and does not completely exclude any outliers. Then, we inspected which kind of error term fit the residuals the best. For this, we discretized dv/dt values and binned all residuals into the relevant t (day) or dv/dt. The bin size for the <italic>E = f(t)</italic> model was 1 day, whereas the ideal bin size for the <italic>E = f(dv/dt)</italic> model was determined using the Freedman-Diaconis rule. We fit a Gaussian distribution on the residuals within each bin and obtained the standard deviation of the distribution for that bin. We then checked whether the standard deviation values followed <italic>E = f(t)</italic> error type or <italic>E = f(dv/dt)</italic>. To do so, we compared the adjusted R-square values and leave-one-out cross-validated normalized root mean square error (LOOCV-NRMSE) values of both the error equation fits. Wilcoxon signed-rank tests were used to detect significance during comparisons. The model with the higher adjusted R-square and lower LOOCV-NRMSE was chosen as the better error model.</p><p id="P80">For the virtual males in the best model overall, we first set the learning rate and error coefficients to the median of the values given by the model when fit on data from actual males. The same was done to obtain the coefficients for the virtual females. We studied the temporal progression of SVA of the best model by plotting the vocal distance between the virtual males and the virtual females in the model against the number of days. Here too, we simulated as many virtual marmoset pairs and acoustic parameter values as there were in the actual data (7). We compared the temporal progression plot to a similar plot on the data obtained from actual marmoset pairs. Then, for the particular coefficient values chosen for the best model, we checked whether the virtual pairs in the model were significantly more synchronized than control pairs using the procedure described in the previous sections for testing the presence of dyadic acoustic synchrony. Finally, we repeated this analysis, but this time by fitting a distribution on the parameters instead of taking the median. We fit a distribution over the learning rates (Gaussian) and error coefficients (gamma) from the dynamic model fit separately for males and females. From these distributions, we randomly extracted learning rates and error coefficients and performed the above-mentioned simulations.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary materials</label><media xlink:href="EMS188417-supplement-Supplementary_materials.pdf" mimetype="application" mime-subtype="pdf" id="d10aAdIbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Acknowledgements</title><p id="P81">We are thankful to Yvonne Zürcher for the marmoset vocal accommodation dataset and to Raghav Rajan and Richard Hahnloser for discussions at various stages of the project. This project was supported by the Swiss National Science Foundation (grant number 31003A_149796, the NCCR Evolving Language, agreement number 51NF40_180888) and the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 101001295). N.P. was a recipient of a grant from the A. H. Schultz Foundation.</p></ack><fn-group><fn id="FN1" fn-type="con"><p id="P82"><bold>Author contributions</bold></p><p id="P83">N.P., K.W. and J.M.B developed the concept and designed the study. N.P. developed the methods and models with inputs from J.M.B. N.P performed the analyses with inputs from K.W and J.M.B. N.P wrote the original draft. K.W. and J.M.B reviewed and edited the draft and provided supervision throughout the project. J.M.B acquired funding and administered the project.</p></fn><fn id="FN2" fn-type="conflict"><p id="P84"><bold>Competing interests</bold></p><p id="P85">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janik</surname><given-names>VM</given-names></name><name><surname>Slater</surname><given-names>PJB</given-names></name></person-group><article-title>The different roles of social learning in vocal communication</article-title><source>Animal Behaviour</source><year>2000</year><volume>60</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmid">10924198</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janik</surname><given-names>VM</given-names></name><name><surname>Knörnschild</surname><given-names>M</given-names></name></person-group><article-title>Vocal production learning in mammals revisited</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2021</year><volume>376</volume><elocation-id>20200244</elocation-id><pub-id pub-id-type="pmcid">PMC8419569</pub-id><pub-id pub-id-type="pmid">34482736</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0244</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vernes</surname><given-names>SC</given-names></name><etal/></person-group><article-title>The multi-dimensional nature of vocal learning</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2021</year><volume>376</volume><elocation-id>20200236</elocation-id><pub-id pub-id-type="pmcid">PMC8419582</pub-id><pub-id pub-id-type="pmid">34482723</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0236</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="book"><source>The Neuroethology of Birdsong</source><publisher-name>Springer International Publishing</publisher-name><year>2020</year><volume>71</volume></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Searcy</surname><given-names>WA</given-names></name><name><surname>Soha</surname><given-names>J</given-names></name><name><surname>Peters</surname><given-names>S</given-names></name><name><surname>Nowicki</surname><given-names>S</given-names></name></person-group><article-title>Variation in vocal production learning across songbirds</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2021</year><volume>376</volume><elocation-id>20200257</elocation-id><pub-id pub-id-type="pmcid">PMC8419578</pub-id><pub-id pub-id-type="pmid">34482719</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2020.0257</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soha</surname><given-names>J</given-names></name></person-group><article-title>The auditory template hypothesis: a review and comparative perspective</article-title><source>Animal Behaviour</source><year>2017</year><volume>124</volume><fpage>247</fpage><lpage>254</lpage></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marler</surname><given-names>P</given-names></name></person-group><article-title>A comparative approach to vocal learning: song development in white-crowned sparrows</article-title><source>Journal of comparative and physiological psychology</source><year>1970</year><volume>71</volume><fpage>1</fpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname><given-names>MZ</given-names></name><name><surname>Trusel</surname><given-names>M</given-names></name><name><surname>Roberts</surname><given-names>TF</given-names></name></person-group><article-title>Memory circuits for vocal imitation</article-title><source>Current Opinion in Neurobiology</source><year>2020</year><volume>60</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="pmcid">PMC7694441</pub-id><pub-id pub-id-type="pmid">31810009</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2019.11.002</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bolhuis</surname><given-names>JJ</given-names></name><name><surname>Moorman</surname><given-names>S</given-names></name></person-group><article-title>Birdsong memory and the brain: in search of the template</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2015</year><volume>50</volume><fpage>41</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">25459663</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruch</surname><given-names>H</given-names></name><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>The function and mechanism of vocal accommodation in humans and other primates</article-title><source>Biological Reviews</source><year>2018</year><volume>93</volume><fpage>996</fpage><lpage>1013</lpage><pub-id pub-id-type="pmid">29111610</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pardo</surname><given-names>JS</given-names></name><name><surname>Pellegrino</surname><given-names>E</given-names></name><name><surname>Dellwo</surname><given-names>V</given-names></name><name><surname>Möbius</surname><given-names>B</given-names></name></person-group><article-title>Special issue: Vocal accommodation in speech communication</article-title><source>Journal of Phonetics</source><year>2022</year><volume>95</volume><elocation-id>101196</elocation-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernhold</surname><given-names>QS</given-names></name><name><surname>Giles</surname><given-names>H</given-names></name></person-group><article-title>Vocal Accommodation and Mimicry</article-title><source>J Nonverbal Behav</source><year>2020</year><volume>44</volume><fpage>41</fpage><lpage>62</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><etal/></person-group><article-title>Marmosets: a neuroscientific model of human social behavior</article-title><source>Neuron</source><year>2016</year><volume>90</volume><fpage>219</fpage><lpage>233</lpage><pub-id pub-id-type="pmcid">PMC4840471</pub-id><pub-id pub-id-type="pmid">27100195</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.018</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>JM</given-names></name><name><surname>Finkenwirth</surname><given-names>C</given-names></name></person-group><article-title>Marmosets as model species in neuroscience and evolutionary anthropology</article-title><source>Neuroscience Research</source><year>2015</year><volume>93</volume><fpage>8</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">25242577</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burkart</surname><given-names>JM</given-names></name><etal/></person-group><article-title>A convergent interaction engine: vocal communication among marmoset monkeys</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2022</year><volume>377</volume><elocation-id>20210098</elocation-id><pub-id pub-id-type="pmcid">PMC9315454</pub-id><pub-id pub-id-type="pmid">35876206</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2021.0098</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliades</surname><given-names>SJ</given-names></name><name><surname>Miller</surname><given-names>CT</given-names></name></person-group><article-title>Marmoset vocal communication: behavior and neurobiology</article-title><source>Developmental neurobiology</source><year>2017</year><volume>77</volume><fpage>286</fpage><lpage>299</lpage><pub-id pub-id-type="pmid">27739195</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Evidence for dialects in three captive populations of common marmosets (Callithrix jacchus)</article-title><source>International Journal of Primatology</source><year>2017</year><volume>38</volume><fpage>780</fpage><lpage>793</lpage></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Willems</surname><given-names>EP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Are dialects socially learned in marmoset monkeys? Evidence from translocation experiments</article-title><source>PLOS ONE</source><year>2019</year><volume>14</volume><elocation-id>e0222486</elocation-id><pub-id pub-id-type="pmcid">PMC6808547</pub-id><pub-id pub-id-type="pmid">31644527</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0222486</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Willems</surname><given-names>EP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Trade-offs between vocal accommodation and individual recognisability in common marmoset vocalizations</article-title><source>Scientific Reports</source><year>2021</year><volume>11</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC8333328</pub-id><pub-id pub-id-type="pmid">34344939</pub-id><pub-id pub-id-type="doi">10.1038/s41598-021-95101-8</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Priva</surname><given-names>UC</given-names></name><name><surname>Sanker</surname><given-names>C</given-names></name></person-group><article-title>Limitations of difference-in-difference for measuring convergence</article-title><source>Laboratory Phonology</source><year>2019</year><volume>10</volume></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Phaniraj</surname><given-names>N</given-names></name><name><surname>Wierucka</surname><given-names>K</given-names></name><name><surname>Zürcher</surname><given-names>Y</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Optimising source identification from marmoset vocalizations with hierarchical machine learning classifiers</article-title><year>2022</year><elocation-id>2022.11.19.517179</elocation-id><pub-id pub-id-type="doi">10.1101/2022.11.19.517179</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zürcher</surname><given-names>Y</given-names></name></person-group><source>Vocal learning and flexibility in the communication of common marmosets (Callithrix jacchus)</source><publisher-name>University of Zurich</publisher-name><year>2020</year></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fréchet</surname><given-names>MM</given-names></name></person-group><article-title>Sur quelques points du calcul fonctionnel</article-title><source>Rendiconti del Circolo Matematico di Palermo (1884-1940)</source><year>1906</year><volume>22</volume><fpage>1</fpage><lpage>72</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakoe</surname><given-names>H</given-names></name><name><surname>Chiba</surname><given-names>S</given-names></name></person-group><article-title>Dynamic programming algorithm optimization for spoken word recognition</article-title><source>IEEE transactions on acoustics, speech, and signal processing</source><year>1978</year><volume>26</volume><fpage>43</fpage><lpage>49</lpage></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konishi</surname><given-names>M</given-names></name></person-group><article-title>The Role of Auditory Feedback in the Control of Vocalization in the White-Crowned Sparrow1</article-title><source>Zeitschrift für Tierpsychologie</source><year>1965</year><volume>22</volume><fpage>770</fpage><lpage>783</lpage><pub-id pub-id-type="pmid">5874921</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marler</surname><given-names>P</given-names></name><name><surname>Nelson</surname><given-names>D</given-names></name></person-group><article-title>Neuroselection and song learning in birds: species universals in a culturally transmitted behavior</article-title><source>Seminars in Neuroscience</source><year>1992</year><volume>4</volume><fpage>415</fpage><lpage>423</lpage></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>S</given-names></name><name><surname>Poole</surname><given-names>TB</given-names></name></person-group><article-title>Long-term changes and maintenance of the pair-bond in common marmosets, Callithrix jacchus jacchus</article-title><source>Folia primatologica</source><year>1984</year><volume>42</volume><fpage>33</fpage><lpage>41</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkenwirth</surname><given-names>C</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Why help? Relationship quality, not strategic grooming predicts infant-care in group-living marmosets</article-title><source>Physiology &amp; behavior</source><year>2018</year><volume>193</volume><fpage>108</fpage><lpage>116</lpage><pub-id pub-id-type="pmid">29730031</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varella</surname><given-names>TT</given-names></name><name><surname>Zhang</surname><given-names>YS</given-names></name><name><surname>Takahashi</surname><given-names>DY</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>A mechanism for punctuating equilibria during mammalian vocal development</article-title><source>PLOS Computational Biology</source><year>2022</year><volume>18</volume><elocation-id>e1010173</elocation-id><pub-id pub-id-type="pmcid">PMC9232141</pub-id><pub-id pub-id-type="pmid">35696441</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010173</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zai</surname><given-names>AT</given-names></name><name><surname>Stepien</surname><given-names>AE</given-names></name><name><surname>Giret</surname><given-names>N</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><article-title>Goal-directed vocal planning in a songbird</article-title><year>2023</year><elocation-id>2022.09.27.509747</elocation-id><pub-id pub-id-type="doi">10.1101/2022.09.27.509747</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobin</surname><given-names>SJ</given-names></name></person-group><article-title>Effects of native language and habituation in phonetic accommodation</article-title><source>Journal of Phonetics</source><year>2022</year><volume>93</volume><elocation-id>101148</elocation-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkenwirth</surname><given-names>C</given-names></name><name><surname>van Schaik</surname><given-names>C</given-names></name><name><surname>Ziegler</surname><given-names>TE</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Strongly bonded family members in common marmosets show synchronized fluctuations in oxytocin</article-title><source>Physiology &amp; Behavior</source><year>2015</year><volume>151</volume><fpage>246</fpage><lpage>251</lpage><pub-id pub-id-type="pmcid">PMC5916785</pub-id><pub-id pub-id-type="pmid">26232089</pub-id><pub-id pub-id-type="doi">10.1016/j.physbeh.2015.07.034</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koski</surname><given-names>SE</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Common marmosets show social plasticity and group-level similarity in personality</article-title><source>Scientific reports</source><year>2015</year><volume>5</volume><elocation-id>8878</elocation-id><pub-id pub-id-type="pmcid">PMC5155412</pub-id><pub-id pub-id-type="pmid">25743581</pub-id><pub-id pub-id-type="doi">10.1038/srep08878</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brügger</surname><given-names>RK</given-names></name><name><surname>Willems</surname><given-names>EP</given-names></name><name><surname>Burkart</surname><given-names>JM</given-names></name></person-group><article-title>Looking out for each other: Coordination and turn taking in common marmoset vigilance</article-title><source>Animal Behaviour</source><year>2023</year><volume>196</volume><fpage>183</fpage><lpage>199</lpage></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>R</given-names></name></person-group><article-title>The neurobiology of human attachments</article-title><source>Trends in cognitive sciences</source><year>2017</year><volume>21</volume><fpage>80</fpage><lpage>99</lpage><pub-id pub-id-type="pmid">28041836</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliades</surname><given-names>SJ</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Sensory-motor interaction in the primate auditory cortex during self-initiated vocalizations</article-title><source>Journal of neurophysiology</source><year>2003</year><volume>89</volume><fpage>2194</fpage><lpage>2207</lpage><pub-id pub-id-type="pmid">12612021</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliades</surname><given-names>SJ</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Comparison of auditory-vocal interactions across multiple types of vocalizations in marmoset auditory cortex</article-title><source>Journal of Neurophysiology</source><year>2013</year><volume>109</volume><fpage>1638</fpage><lpage>1657</lpage><pub-id pub-id-type="pmcid">PMC3602939</pub-id><pub-id pub-id-type="pmid">23274315</pub-id><pub-id pub-id-type="doi">10.1152/jn.00698.2012</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliades</surname><given-names>SJ</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Contributions of sensory tuning to auditory-vocal interactions in marmoset auditory cortex</article-title><source>Hearing Research</source><year>2017</year><volume>348</volume><fpage>98</fpage><lpage>111</lpage><pub-id pub-id-type="pmcid">PMC5392437</pub-id><pub-id pub-id-type="pmid">28284736</pub-id><pub-id pub-id-type="doi">10.1016/j.heares.2017.03.001</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><article-title>The interactive-alignment model: Developments and refinements</article-title><source>Behavioral and Brain Sciences</source><year>2004</year><volume>27</volume><fpage>212</fpage><lpage>225</lpage><pub-id pub-id-type="pmid">18241490</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><article-title>An integrated theory of language production and comprehension</article-title><source>Behavioral and brain sciences</source><year>2013</year><volume>36</volume><fpage>329</fpage><lpage>347</lpage><pub-id pub-id-type="pmid">23789620</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><source>Understanding dialogue: Language use and social interaction</source><publisher-name>Cambridge University Press</publisher-name><year>2021</year></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Giles</surname><given-names>H</given-names></name><name><surname>Coupland</surname><given-names>N</given-names></name><name><surname>Coupland</surname><given-names>I</given-names></name></person-group><source>Accommodation theory: Communication, context, and Contexts of accommodation: Developments in applied sociolinguistics</source><year>1991</year><volume>1</volume></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Giles</surname><given-names>H</given-names></name></person-group><source>Communication accommodation theory: Negotiating personal relationships and social identities across contexts</source><publisher-name>Cambridge University Press</publisher-name><year>2016</year></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourhis</surname><given-names>RY</given-names></name><name><surname>Giles</surname><given-names>H</given-names></name></person-group><article-title>The language of intergroup distinctiveness</article-title><source>Language, ethnicity and intergroup relations</source><year>1977</year><volume>13</volume><fpage>119</fpage></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Little</surname><given-names>MA</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name></person-group><article-title>Highly comparative time-series analysis: the empirical structure of time series and their methods</article-title><source>Journal of the Royal Society Interface</source><year>2013</year><volume>10</volume><elocation-id>20130048</elocation-id><pub-id pub-id-type="pmcid">PMC3645413</pub-id><pub-id pub-id-type="pmid">23554344</pub-id><pub-id pub-id-type="doi">10.1098/rsif.2013.0048</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fulcher</surname><given-names>BD</given-names></name><name><surname>Jones</surname><given-names>NS</given-names></name></person-group><article-title>hctsa: A computational framework for automated time-series phenotyping using massive feature extraction</article-title><source>Cell systems</source><year>2017</year><volume>5</volume><fpage>527</fpage><lpage>531</lpage><pub-id pub-id-type="pmid">29102608</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paul</surname><given-names>A</given-names></name><name><surname>McLendon</surname><given-names>H</given-names></name><name><surname>Rally</surname><given-names>V</given-names></name><name><surname>Sakata</surname><given-names>JT</given-names></name><name><surname>Woolley</surname><given-names>SC</given-names></name></person-group><article-title>Behavioral discrimination and time-series phenotyping of birdsong performance</article-title><source>PLOS Computational Biology</source><year>2021</year><volume>17</volume><elocation-id>e1008820</elocation-id><pub-id pub-id-type="pmcid">PMC8049717</pub-id><pub-id pub-id-type="pmid">33830995</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008820</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javed</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>BS</given-names></name><name><surname>Rizzo</surname><given-names>DM</given-names></name></person-group><article-title>A benchmark study on time series clustering</article-title><source>Machine Learning with Applications</source><year>2020</year><volume>1</volume><elocation-id>100001</elocation-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>DY</given-names></name><etal/></person-group><article-title>The developmental dynamics of marmoset monkey vocal production</article-title><source>Science</source><year>2015</year><volume>349</volume><fpage>734</fpage><lpage>738</lpage><pub-id pub-id-type="pmid">26273055</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Marmoset vocal accommodation and its models.</title><p>(<bold>A</bold>) Marmoset vocal accommodation. When marmosets are paired, the trill calls of the individuals become more similar over time. What drives these changes and how is not known. (<bold>B</bold>) Initial Auditory Template Matching (IATM). When paired, marmosets form a static auditory template of the trills of their partner. They compare their own trills to the static template, producing an error signal. This error signal drives VPL as the marmosets modify their trills to reduce the magnitude of the error signal. (<bold>C</bold>) Convergence to Intermediate Value (CIV). When paired, marmosets form a static auditory template of the trills of their partner (static acquired template) in addition to the static internal template that they already possess. Their trills are compared to both the templates and the resulting error signals drive changes in the marmoset’s own trills. (<bold>D</bold>) Dynamic Auditory Template Matching (DATM). When paired, marmosets form an auditory template of the trills of their partner. Marmosets compare their own trills to the template and the error signal drives changes in their own trills. Marmosets then continuously update the template (dynamic template) to account for changes in their partner’s trills, hence producing a dynamic error signal that drives VPL. (<bold>E</bold>) Dynamic Convergence to Intermediate Value (DCIV). Marmosets acquire a template of their partner’s latest trill in addition to the internal template when paired. Both these templates are dynamic, hence producing dynamic error signals when their own calls are compared to both templates, which then drives VPL. In all the panels, black dashed double arrows depict the comparison of the monkey’s current trill with the template(s). Grey arrows represent the error signals. Solid arrows represent the update or change occurring to the trills.</p></caption><graphic xlink:href="EMS188417-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Properties of SVA in trills.</title><p>(<bold>A</bold>) Example spectrogram of a trill call obtained using the Hann filter of size 512 samples, hop length of 256 samples, and DFT bin size of 512 samples. (<bold>B</bold>) Amount of vocal change undergone by males (n=7) and females (n=7) during SVA in trills. Each point is an individual. Male-female pairs are joined using grey lines. Black circles with error bars depict mean ± 95% bootstrapped CI. (<bold>C</bold>) Temporal progression of SVA in trills of n=7 pairs. Vocal distances are normalized to the before pairing condition. Green curves are spline fits for each pair, fit using MATLAB’s ‘smoothingspline’ with smoothing parameter of 0.1. Black curve depicts the first order exponential term fit to the population. (<bold>D</bold>) Correlation between the amount of SVA and Initial vocal distance for n = 7 pairs. ‘r’ is the Spearman’s correlation coefficient. (<bold>E</bold>) Dyadic acoustic feature synchrony in n=7 actual pairs (orange) compared to n=42 control pairings (grey) measured using Fréchet distance between the trajectories of the male and the female in a pair. Orange vertical lines depict the Fréchet distance values of actual pairs. Grey (normalized) histograms depict the distribution of Fréchet distance values of control pairs. Orange and grey curves are Gaussian probability distribution function fits to actual pairs and random pairs respectively. **p&lt;0.01, two-tailed Welch’s t-test. (<bold>F</bold>) Phase portrait of normalized acoustic feature values of n = 7 pairs (see materials and methods for more details). The region approximately containing equilibrium points is marked with a dashed outline.</p></caption><graphic xlink:href="EMS188417-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Behavior of the models.</title><p>Top plots show the model parameter space. Model parameter values for which the acoustic features of the n=7 virtual pairs are significantly more synchronized than the n=42 virtual control pairs are marked in yellow if p&lt;0.05, and in red if p&lt;0.01 (two-tailed Welch’s t-test, acoustic trajectory similarity calculated using Fréchet distance). The rest of the points are filled in grey. The fill colors are made slightly transparent such that points behind them are visible. α is the learning rate while β, γ, δ and ∊ are error coefficients. Bottom plots depict the phase portrait for the model parameter values marked using open blue circles in top panels. The region of the phase portrait approximately containing equilibrium points is outlined with a black dotted curve. See materials and methods for more details regarding model simulations, synchrony calculations and phase portrait visualizations. Abbreviations: IATM = Initial Auditory Template Matching, CIV = Convergence to Intermediate Value, DATM = Dynamic Auditory Template Matching, DCIV = Dynamic Convergence to Intermediate Value.</p></caption><graphic xlink:href="EMS188417-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Properties of the dynamic model.</title><p>(<bold>A, B</bold>) Box plots showing adjusted R<sup>2</sup> (<bold>A</bold>) and leave-one-out cross-validated normalized root mean square error (LOOCV NRMSE) (<bold>B</bold>) of <italic>E = f(t)</italic> and <italic>E = f(dv/dt)</italic> fits to the residuals for n=14 individuals (***p&lt;0.001, Wilcoxon signed-rank test). The residuals were obtained after a robust fit of the dynamic model without the error term to the data from each individual. (<bold>C</bold>) Learning rates (α) obtained after fitting the dynamic model to SVA data from each individual (n=14 individuals). Each point is an individual. Male-female pairs are joined using grey lines. Black circle with error bars depict mean ± 95% bootstrapped CI. (<bold>D</bold>) Temporal progression of SVA in actual data normalized to the vocal distance in the first recording after pairing. (<bold>E, F</bold>) Temporal progression of SVA in dynamic models with (<bold>E</bold>) parameters extracted from a distribution fit over actual parameters and (<bold>F</bold>) median of actual parameter values. (<bold>G</bold>) Correlation between the amount of SVA and initial vocal distance for n = 7 virtual pairs in the model. R is the Spearman’s correlation coefficient. (<bold>H, I)</bold> Acoustic feature synchrony in n=7 virtual pairs (purple) compared to n=42 virtual control pairings (grey) in the dynamic model simulated using (<bold>H</bold>) median of actual parameter values and (<bold>I</bold>) a distribution fit over actual parameter values, measured using Fréchet distance between the trajectories of the male and the female in a pair. Purple vertical lines depict the Fréchet distance values of virtual pairs. Grey (normalized) histograms depict the distribution of Fréchet distance values of virtual control pairs. Purple and grey curves are Gaussian probability distribution function fits to actual pairs and random pairs respectively. *p&lt;0.5, ***p&lt;0.001, two-tailed Welch’s t-test.</p></caption><graphic xlink:href="EMS188417-f004"/></fig></floats-group></article>