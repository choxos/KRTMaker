<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190913</article-id><article-id pub-id-type="doi">10.1101/2023.11.09.565503</article-id><article-id pub-id-type="archive">PPR757427</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">4</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Hippocampus facilitates reinforcement learning under partial observability</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Pedamonti</surname><given-names>Dabal</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Mohinta</surname><given-names>Samia</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Dimitrov</surname><given-names>Martin V.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Malagon-Vina</surname><given-names>Hugo</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Ciocchi</surname><given-names>Stephane</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Costa</surname><given-names>Rui Ponte</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Computational Neuroscience Unit, Intelligent Systems Labs Faculty of Engineering, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap>, <city>Bristol</city>, <country country="GB">United Kingdom</country></aff><aff id="A2"><label>2</label>Centre for Neural Circuits and Behaviour, Department of Physiology, Anatomy and Genetics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">United Kingdom</country></aff><aff id="A3"><label>3</label>Department of Physiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>University of Bern</institution></institution-wrap>, <city>Bern</city>, <country country="CH">Switzerland</country></aff><aff id="A4"><label>4</label>Laboratory of Systems Neuroscience, Department of Physiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>University of Bern</institution></institution-wrap>, <city>Bern</city>, <country country="CH">Switzerland</country></aff><aff id="A5"><label>5</label>Center for Brain Research, Division of Cognitive Neurobiology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05n3x4p02</institution-id><institution>Medical University of Vienna</institution></institution-wrap>, <city>Vienna</city>, <country country="AT">Austria</country></aff><aff id="A6"><label>6</label>Department of Physiology, Development and Neuroscience, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap>, <country country="GB">United Kingdom</country></aff><author-notes><corresp id="CR1">For correspondence: <email>rui.costa@dpag.ox.ac.uk</email> (RPC)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>11</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>10</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Mastering navigation in environments with limited visibility is crucial for survival. Although the hippocampus has been associated with goal-oriented navigation, its role in real-world behaviour remains unclear. To investigate this, we combined deep reinforcement learning (RL) modelling with behavioural and neural data analysis. First, we trained RL agents in partially observable environments using egocentric and allocentric tasks. We show that agents equipped with recurrent hippocampal circuitry, but not purely feedforward networks, learned the tasks in line with animal behaviour. Next, using dimensionality reduction, our agents predicted reward, strategy, and temporal representations, which we validated experimentally using hippocampal recordings. Moreover, hippocampal RL agents predicted state-specific trajectories, mirroring empirical findings. In contrast, agents trained in fully observable environments failed to capture experimental observations. Finally, we show that hippocampal-like RL agents demonstrated improved generalisation across novel task conditions. In summary, our findings suggest an important role of hippocampal networks in facilitating reinforcement learning in naturalistic environments.</p></abstract></article-meta></front><body><p id="P2">As we navigate new environments, we must learn to integrate incomplete sensory information towards desired goals. How biological neural networks perform this feat is not fully understood.</p><p id="P3">The hippocampus is classically associated with the building of a cognitive map of the environment and the storage of episodic memories (<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref>). However, growing evidence suggests that the hippocampus also supports goal-driven behaviour (<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R8">8</xref>). For example, Wikenheiser and Redish (<xref ref-type="bibr" rid="R4">4</xref>) showed that the hippocampus is indeed involved in planning routes towards desired goals. Moreover, their work suggests that hippocampal sequence events, known as “replay”, serve as a mechanism for goal-directed navigation, facilitating memory-based trajectory planning and guiding subsequent navigational behaviour. Other studies have shown that the hippocampus, and the hippocampal CA3 region in particular, is involved in maintaining information in working memory that is needed during navigational tasks when sensory cues are no longer present (<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>). Given that in most naturalistic conditions animals do not have continuous access to the full environment, we postulate that the hippocampus may have evolved to support goal-driven navigation in environments in which sensory information is not always present.</p><p id="P4">The hippocampus has been traditionally conceptualised using Hopfield neural networks, known for their capacity for autoassociative memory storage (<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R13">13</xref>). More recent studies have demonstrated that recurrent neural network models of the hippocampus, trained for navigation tasks, exhibit specific cell types tuned to spatial information (<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R17">17</xref>), including the commonly observed place cells and grid cells (<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R19">19</xref>). Another line of research has shown that model-based approaches, known as Successor Representation (SR), explain anticipatory features that have been observed experimentally (<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R21">21</xref>). A related approach combined model-free with model-based networks of the hippocampal-striatal system to reproduce a range of behavioural findings from spatial and non-spatial decision making tasks (<xref ref-type="bibr" rid="R22">22</xref>).</p><p id="P5">A complementary viewpoint has explored the involvement of the hippocampus in reward-centric navigational tasks (<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R26">26</xref>). For instance, Foster et al. (<xref ref-type="bibr" rid="R23">23</xref>) utilized an actor-critic network coupled with goal-independent representations to model water-maze tasks. Subsequently, this framework was expanded into a hierarchical model (<xref ref-type="bibr" rid="R25">25</xref>), thus accelerating learning, particularly in previously visited locations. Arleo and Gerstner (<xref ref-type="bibr" rid="R27">27</xref>), on the other hand, proposed a model that integrates unsupervised learning for spatial modelling with reward-based learning for goal-oriented behaviour. Nevertheless, such models did not explicitly differentiate the functional impact of CA3 and CA1 networks. Consequently, the contribution of the classical hippocampal layered structure for navigation of goal-driven environments under partially observable conditions, the underlying neural dynamics, and its implications for animal behaviour remain unclear.</p><p id="P6">Here, we posit that the hippocampal circuitry, particularly CA3 recurrence, is adept at navigating environments under realistic conditions, particularly those characterized by limited visibility and cue uncertainty. To test this hypothesis we combine behavioural and neural data analysis together with deep reinforcement learning (RL) modelling of goal-driven tasks. Both animals and agents were trained to perform ego-allocentric strategies on a T-maze. Our models consist of a hippocampal-like network trained to perform goal-driven tasks using reinforcement learning. By contrasting experimental observations with the model we show that hippocampal networks trained in partial, but not fully observable environments, provide a good match of neuronal and behavioural observations. Using task-relevant dimensionality reduction, we show that hippocampal neurons encode decision, strategy, and temporal population activity, which are better captured by a model incorporating CA3 recurrence. Moreover, our modelling shows that CA3 recurrence also captures key behavioural features commonly observed in animals and humans, and that it generalises to different task conditions. This is in contrast with non-recurrent models, which failed to capture experimental observations. In addition, our work shows that agents trained in fully observable environments also do not capture experimental observations, thus suggesting the need to reevaluate previous experimental findings that may have implicitly assumed full observability.</p><sec id="S1" sec-type="results"><title>Results</title><p id="P7">We were inspired by a standard behavioural setup in which animals are trained on plus-mazes (<xref ref-type="bibr" rid="R5">5</xref>). Next, we highlight the key elements of this experimental setup, which we model. First, rats were trained in a multi-task setting in which they had to perform a goal-driven navigational task while following two strategies, egocentric and allocentric (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). In the egocentric (self-centred) rule, the reward was always positioned in the same location with respect to the animal, i.e. regardless of the north or south starting location. For the allocentric (world-centred) rule, the reward is at the same location irrespective of the animal’s starting position, and the animal needs to turn left or right depending on whether they start from a north or south position. On a given trial rodents have to infer the task (ego versus allocentric) through trial-and-error. Second, rats were placed on a standard plus-maze defined by a 5cm wall with cues in the surrounding walls. In such setups, and in most navigational task setups rats are unlikely to be continuously accessing the full environment, implying a degree of partial observability (<xref ref-type="bibr" rid="R28">28</xref>). Furthermore, we analysed the speed and head direction of rats as they navigated the maze. This analysis suggested that rats did not have constant access to the full state of the environment (<xref ref-type="supplementary-material" rid="SD1">Fig. S1</xref>). Note that the maze setup was effectively transformed into a T-maze on each trial by blocking the opposite starting arm (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Finally, animals were trained using in a block-wise fashion with interleaved blocks containing allocentric and egocentric tasks (<xref ref-type="fig" rid="F1">Fig. 1a</xref>). Each block contains sub-blocks corresponding to different starting locations (i.e., north versus south). Despite the relatively complex nature of these tasks with multiple rules, rats can learn all tasks (<xref ref-type="fig" rid="F1">Fig. 1b</xref>).</p><p id="P8">Building on this task setup, we aimed to study the underlying architectural principles that enable such goal-driven navigation. To this end, we contrast animal behaviour and (hippocampal) neural data with artificial reinforcement learning (RL) agents equipped with a hippocampal-like architecture. To mimic the experimental setup described above, we simulated a 2D minigrid discrete environment (<xref ref-type="bibr" rid="R29">29</xref>), which consists of a starting state and two terminal states: rewarded and non-rewarded (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). To capture both north and south starting states, we use different sensory cues (<xref ref-type="fig" rid="F1">Fig. 1d</xref>). Closely following the experimental setup described above, we train the model in a <italic>multi-task</italic> (allocentric north/south and egocentric north/south) using the same <italic>block-wise</italic> training setup, as illustrated in <xref ref-type="fig" rid="F1">Fig. 1e</xref>. As in the experimental setting, RL agents need to figure out the task context through trial-and-error, but this computation in the model is simplified by assuming separate ego and allocentric output action networks (see <xref ref-type="sec" rid="S10">Methods</xref>). We provide a more detailed comparison between our modelling and the experimental setup in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>. Motivated by the fact that animals cannot continuously access all sensory information available in their environment (e.g. due to poor visual acuity and head direction), we test different degrees of observability of the environment.</p><sec id="S2"><title>Agents with recurrence learn ego-allocentric goal-driven tasks</title><p id="P9">Our hippocampal RL models are based on standard deep RL models, specifically, Deep Q-networks (DQN) (<xref ref-type="bibr" rid="R30">30</xref>) (see <xref ref-type="sec" rid="S10">Methods</xref>). Our models feature a three-layer hippocampal-like structure: the input layer emulates entorhinal input to the Dentate Gyrus (DG), the first layer represents CA3 and the second CA1. In addition, we also consider an output layer that encodes action-state values, denoted as <italic>Q</italic>(<italic>a, s</italic>) (<xref ref-type="fig" rid="F2">Fig. 2a,b</xref>). Environmental cues are provided around the start location of the maze and agents can take one of three actions (move forward, rotate left or rotate right). The entorhinal cortex (EC) is known to supply the hippocampus with a spatial map of the environment (<xref ref-type="bibr" rid="R19">19</xref>), which we approximate by providing our model with a 2D top-down spatial map obtained from a minigrid environment (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). The output layer, which encodes state-action Q-values, provides an abstraction of hippocampal-to-striatal networks (<xref ref-type="bibr" rid="R31">31</xref>) To prevent forgetting of a previously learned task, we use two separate output Q-heads in all models. Consequently, we switch between the allocentric or egocentric head depending on the current task being performed by the agent. However, other strategies are possible, we also demonstrate that the same tasks can be learned using a contextual task-specific signal (task ID; <xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>), as is commonly done in computational neuroscience (<xref ref-type="bibr" rid="R32">32</xref>).</p><p id="P10">Motivated by the existence of recurrent connectivity in CA3 (<xref ref-type="bibr" rid="R33">33</xref>) and in line with previous work in which brain areas are modelled as gated recurrent neural networks (<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R35">35</xref>) we model CA3 using a Gated Recurrent Unit (GRU) network (<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>), which we denote as hippocampal deep recurrent Q-Network (hcDRQN). In addition, we contrast this network with three other networks: a purely feedforward hippocampal Deep Q-Network (hcDQN) and two hcDQNs augmented with artificial continual learning algorithms. We considered two modern machine learning algorithms for as a strong control for what is achievable with feedforward networks in our multi-task setup. In particular, we included two of the most popular methods: Elastic Weight Consolidation (ML-EWC; (<xref ref-type="bibr" rid="R38">38</xref>)) and Synaptic Intelligence (ML-SI; (<xref ref-type="bibr" rid="R39">39</xref>)). Experience replay is often used to train DQN networks (<xref ref-type="bibr" rid="R30">30</xref>). We did not use replay in our model because of the lack of evidence of replay from the hippocampus to itself.</p><p id="P11">First, we compare the hippocampal deep RL models (<xref ref-type="fig" rid="F2">Fig. 2b</xref>) with animals by contrasting their task performance in a partially observable setting (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). We trained the models using the grid environment described above and following a similar training procedure (trial-by-trial) used to train animals with blocks of allocentric trials alternated with blocks of egocentric trials (<xref ref-type="fig" rid="F1">Fig. 1a</xref>, bottom). Within each block we alternate the two starting (north/south) positions. Our results show that the network with recurrence, hcDRQN, can successfully learn multiple tasks (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). Moreover, not only does hcDRQN yield the best performance on both tasks but it is also the only model that can learn allocentric tasks while other models perform around chance level. These results show that hcDRQN learns sufficiently generalisable representations across all tasks considered here. This is due to the RNN’s ability to integrate information over time, thus being able to link sensory cues with future outcomes. The action-value outputs must then decode this mixed information from the hippocampal network to attribute value to the appropriate actions. In contrast, hcDQN only learns a subset of the tasks. This is because models that fail to truly learn the tasks will default to memorising to always turn right at the decision point as this behaviour will work on 3 of the 4 sub-tasks (allo-south, ego-north and ego-south). This is in line with the performance observed experimentally, showing that animals can learn both strategies (<xref ref-type="fig" rid="F1">Fig. 1b</xref>). In addition, our results show that a model without plasticity in the recurrent CA3 layer is not sufficient to learn all tasks (fixed hcDRQN model), which further supports the importance of the RNN for learning these tasks. Studying how the performance evolves over trials within each allocentric and egocentric block, shows that allocentric performance drops for hcDQN after each switch between north vs. south scenarios (<xref ref-type="fig" rid="F2">Fig. 2e</xref>). This is a consequence of hcDRQN, but not hcDQN agents, being able to learn and thus remember previously encountered tasks. Finally, machine continual learning models perform similarly to hcDQN, thus not being able to learn all tasks (<xref ref-type="supplementary-material" rid="SD1">Fig. S3c</xref>).</p></sec><sec id="S3"><title>CA3 recurrence is needed in partially observable environments</title><p id="P12">CA3 recurrence enables the hcDRQN to integrate information over time, allowing it to retain initial cues for accurate action selection at decision points. Therefore, the hcDQN model should match hcDRQN performance in fully observable environments. To demonstrate this, we tested hcDRQN and hcDQN in environments with different degrees of visibility (3x3, 5x5, 7x7, and full view; <xref ref-type="fig" rid="F2">Fig. 2c</xref>). We expected a model without CA3 recurrence (i.e. DQN) to be able to solve all tasks in environments with full observability (i.e. all information continuously available). Our results show that the non-recurrent model, hcDQN, only succeeds to learn both allo and egocentric tasks when the full view is provided (<xref ref-type="fig" rid="F2">Fig. 2f</xref>). We expected that models learn to solve the task in these conditions by continuously rely on having access to the task-specific cues. To test this continuous reliance on sensory cues we removed the cues after the decision point (<xref ref-type="supplementary-material" rid="SD1">Fig. S5</xref>). Our results show that both models completely fail to complete the tasks.</p><p id="P13">Given that in most realistic environments animals are unlikely to have continuous access to the full environment our results suggest that CA3 recurrence plays an important role in supporting goal-driven behaviours under naturalistic conditions.</p></sec><sec id="S4"><title>Agent’s task-relevant neuronal dynamics are in align with experimental recordings</title><p id="P14">Next, we tested whether RL agents trained in partially environments best capture experimental task-specific neural dynamics. To this end, we contrasted the neural dynamics predicted by the model with those recorded from awake performing animals by using dimensionality reduction techniques. In particular, we used demixed PCA (dPCA), which enabled us to extract behaviourally-relevant dimensions.</p><p id="P15">We extracted task-specific neural activity from agents in an intermediate stage of learning as to have a more comparable performance to that observed in animals. (cf. <xref ref-type="fig" rid="F1">Fig. 1b</xref> and <xref ref-type="supplementary-material" rid="SD1">Fig. S4a</xref>). The simulated neural activity of the modelled CA1 layer was then used to perform dPCA (see <xref ref-type="sec" rid="S10">Methods</xref> for details). This analysis focused on three task-encoding components of interest in the hcDRQN agent (<xref ref-type="fig" rid="F3">Fig. 3a</xref>): strategy, time and decision. First, we find that the population dynamics for allo and egocentric tasks can be linearly separable when using the hcDRQN strategy demixed PCs, but not for hcDQN (<xref ref-type="fig" rid="F3">Fig. 3a</xref> left). The fact that hcDRQN, but not hcDQN, yields a clear separation of strategies is consistent with our results above showing that it can learn both allo and egocentric strategies (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). Next, we find that both hcDRQN and hcDQN exhibit a temporally decaying population dynamics (<xref ref-type="fig" rid="F3">Fig. 3a</xref> middle). These features of the hcDRQN highlight its unique ability to integrate input information over time, with the retained information gradually decaying as time elapses. Finally, we observe decision- or outcome-specific components (correct versus incorrect), which as expected collapse once the reward terminal point is reached. This predicts that the hippocampus should encode reward/no-reward information well before the terminal states. When sensory cues are removed, these task-specific features in the strategy and decision dPCA components are eliminated, aligning with the role of RNN sensory integration in achieving correct task outcomes (<xref ref-type="supplementary-material" rid="SD1">Fig. S6</xref>).</p><p id="P16">Next, to contrast the neural representations predicted by the RL agents we used tetrode recordings obtained from 612 CA1 neurons, as described in (<xref ref-type="bibr" rid="R5">5</xref>). To conduct this analysis, neural data were aligned to the moment when the reward zone was entered (see <xref ref-type="sec" rid="S10">Methods</xref>). The results of the dPCA show that the data qualitatively validate the results predicted by the hcDRQN agent for the strategy-specific components, but not the hcDQN agent (<xref ref-type="fig" rid="F3">Fig. 3a</xref> bottom). We also observe stronger neuronal activations in the allocentric tasks compared to the egocentric tasks, in line with experimental observations (<xref ref-type="supplementary-material" rid="SD1">Fig. S11</xref>). To better quantify model-data match we used a normalised mean-squared error metric (see <xref ref-type="sec" rid="S10">Methods</xref>). This metric shows that, indeed, hcDRQN better captures experimentally observed strategy neural dynamics (<xref ref-type="fig" rid="F3">Fig. 3b</xref> left). For the time-specific components, we observe a decaying component as predicted by the RL agents. Although both the hcDRQN and hcDQN look qualitatively very similar, the error metric shows that hcDRQN provides a better match with the data (<xref ref-type="fig" rid="F3">Fig. 3b</xref> middle). Finally, the decision component also reveals a separation between correct and incorrect trials as predicted by the models, but in contrast to the models this separation remains after the reward point. This discrepancy between model and animal reward-relevant neural dynamics is likely rooted in the distinction between reinforcement learning agents, which receive instantaneous rewards, and animals, which require several seconds to consume rewards. However, we should point out that reward point in experimental data simply means that a sensor close to the reward was triggered, thus there is likely some delay between triggering the sensor and actually perceiving the reward. As before, we used the model-data error metric on the decision components (up to the reward point) and found no differences between the hcDQN and hcDRQN (<xref ref-type="fig" rid="F3">Fig. 3b</xref> right).</p><p id="P17">Models trained under full observability conditions do not appear to provide a good match with experimental observations. To further test this point, we compared model neural dynamics in agents trained with full continuous access to the environment (<xref ref-type="supplementary-material" rid="SD1">Fig. S7</xref>). Our error metric shows that agents trained with full observability provide a poor match to neural dynamics when compared to agents trained under partial observability (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). These results provide further support for CA3 recurrence as being important to navigate environments under naturalistic conditions.</p><p id="P18">Finally, we contrast the degree of explained variance across models and data. hcDRQN captures explained variance across behavioural variables in a way that more closely matches awake tetrode recordings (<xref ref-type="fig" rid="F3">Fig. 3c</xref>). Of particular interest is the fact that hcDQN relies more on mixed (or interaction) components (37%) compared to hcDRQN (13%) and animal (17%), which is in line with its inability to fully solve all the tasks. On the other hand, for both hcDRQN and animal dynamics, but not hcDQN, strategy explains a large amount of the variance (hcDRQN: 27%; animal: 37%).</p><p id="P19">In summary, our neural dynamics analysis suggests that the hippocampus is indeed involved in task strategy, temporal integration and reward-based decision, in a way that are best matched by hcDRQN RL agents trained in partially observable environments.</p></sec><sec id="S5"><title>Hippocampal RL agents with recurrence better capture animal behaviour</title><p id="P20">Subsequently, we wondered whether RL agents could also capture animal behaviour during the task. To juxtapose the actions of RL agents against those of animals, we examined the trajectories observed during maze navigation post-learning. In order to facilitate a direct comparison with agent trajectories, we discretized the animal trajectories into a 9x9 grid. The behavioural trajectories show that hcDRQN better captures animal behaviour in terms of time spent at the starting point, decision and the terminal state (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). Interestingly, we find that both models tend to spend more time in the decision state compared to the initial state, which shows that agents take some time before committing to a final decision. This observation is related to vicarious trial-and-error and increased dwell times often found in rodents when approaching decision states (<xref ref-type="bibr" rid="R40">40</xref>). Note that agents can stay in a given cell because they can continue rotating left or right, before deciding to move forward. This behaviour is highlighted by cells with darker red (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). On the other hand, as expected, the hcDQN agent fails to discriminate between the two allocentric subtasks and instead learns only one policy (allocentric south; <xref ref-type="fig" rid="F4">Fig. 4a</xref>, bottom). Next, to quantify the time spent on each state we calculated the ratio between time spent in individual states and the final state. This state-to-end ratio shows that hcDRQN also better approximates animal behaviour at this finer level and for allocentric strategies in particular (<xref ref-type="fig" rid="F4">Fig. 4b,c</xref>). Next, to study whether the better fit of hcDRQN to animal behaviour is specific to a subtask we made this analysis across all possible subtasks (<xref ref-type="fig" rid="F4">Fig. 4d</xref>). Our results show that hcDRQN clearly outperforms hcDQN, except for a minor effect on the allocentric north when compared to allocentric south. When analysing RL agents trained with full observability we observed a poorer match to behavioural data, further supporting the notion that agents trained under partial observability provide a better description of animal behaviour(<xref ref-type="supplementary-material" rid="SD1">Fig. S12, S13</xref>).</p><p id="P21">Overall, hcDRQN trained with partial observability provides a better match to animal behaviour, further supporting the important role of CA3 recurrence in the hippocampal circuitry.</p></sec><sec id="S6"><title>Agent’s behaviour predicts state-dependent action values</title><p id="P22">Because the recurrent RL agent is able to solve all tasks we expected this agent to yield state-action value predictions with higher and more certain Q-values when compared to the non-recurrent agent. To examine this in more detail we analysed the action-values for each state. First, we highlight the sequence of actions that makes hcDQN take the wrong arm and the correct policy learned by hcDRQN for both allocentric tasks (<xref ref-type="fig" rid="F5">Fig. 5a</xref>). As expected, on average, hcDRQN has higher Q-values than hcDQN, which reflects the fact that it learns all tasks (<xref ref-type="fig" rid="F5">Fig. 5b</xref>, bottom). Next, we studied action selection certainty by calculating the relative Q-value variance across all possible Q-values for a given state (<xref ref-type="fig" rid="F5">Fig. 5b</xref>, top). This analysis shows that hcDRQN starts with lower action certainty but that it gradually increases over states until the terminal state. This reflects the effect of appropriate cue integration towards a decision, which can also be observed in models trained with full observability (<xref ref-type="supplementary-material" rid="SD1">Fig. S14 and S15</xref>). In contrast, hcDQN becomes more certain (i.e. lower variance) after the initial state, becoming again more certain once the decision is made. This result is explained by the relative variance obtained in allocentric tasks (<xref ref-type="fig" rid="F5">Fig. 5b</xref>, right), which is a consequence of the hcDQN’s inability to learn allocentric tasks (<xref ref-type="fig" rid="F2">Fig. 2d</xref>).</p><p id="P23">The observation that the hcDQN becomes overconfident in the allocentric task, which it fails to perform, is reminiscent of the Dunning–Kruger effect. This phenomenon suggests that expert individuals often exhibit less certainty than those who are less knowledgeable or skilled (<xref ref-type="bibr" rid="R41">41</xref>).</p></sec><sec id="S7"><title>Network recurrence enables generalisation to stochastic environments</title><p id="P24">Until now, we have trained RL agents in environments in which cues are always present. However, recurrent neural networks are well placed to deal with stochastic environments by integrating evidence over time (<xref ref-type="bibr" rid="R42">42</xref>, <xref ref-type="bibr" rid="R43">43</xref>). To test the effect of stochasticity on the different agents, we created environments in which cues randomly appear and disappear (<xref ref-type="fig" rid="F6">Fig. 6</xref>). We study two scenarios: (i) incremental random cue removal during inference (i.e. after learning) and (ii) effect of random cue removal on learning. During inference, the performance of hcDRQN gradually decreases as the likelihood of cue removal increases (<xref ref-type="fig" rid="F6">Fig. 6a</xref>). In contrast, for hcDQN, which lacks recurrence, its performance decreases as soon as the cues are removed, regardless of the degree of removal (<xref ref-type="fig" rid="F6">Fig. 6b</xref>). This highlights the lack of evidence integration of hcDQN, while hcDRQN can handle a high degree of removal 90% while maintaining performance above 80%. When comparing the hcDRQN trajectories to the model without cue removal, the agent switches to the default ‘turn left’ policy when little or no cues are present. In addition, the model spends more time in the start and middle corridors because it must observe cues before deciding which arm to turn onto. Next, we tested the idea that if the agent was trained in a stochastic environment, this should result in the model being more robust to cue removal (<xref ref-type="fig" rid="F6">Fig. 6c</xref>). Indeed, when trained under these conditions the model performs consistently better than a model trained without random cue removal (10% improvement). Finally, when analysing trajectory behaviour, our model shows that agents trained with probabilistic cues spend more time at the starting location to integrate sensory evidence for longer before committing to a decision (<xref ref-type="fig" rid="F6">Fig. 6d</xref>).</p><p id="P25">Taken together, these results suggest that CA3 recurrence also plays an important role in learning to navigate stochastic environments.</p></sec><sec id="S8"><title>hcDRQN agent generalises to different task conditions</title><p id="P26">Finally, we tested whether the RL agents considered here can generalise to different task conditions not experienced during training (<xref ref-type="fig" rid="F7">Fig. 7a</xref>). First, we tested different lengths of the initial maze corridor (<xref ref-type="fig" rid="F7">Fig. 7b</xref>). This allowed us to test whether the RL agents memorise the tasks or learn to integrate the cue information and maintain it in memory to trigger the right action at the decision point. Our results show that hcDRQN performance is very robust across a large range of lengths, whereas hcDQN defaults to chance level. This demonstrates that indeed the hcDRQN has successfully learned to integrate cue information, which is then maintained in its recurrent memory for action selection when required.</p><p id="P27">Next, we tested the models on the same environment it was trained on, but removing a set of cues at a time. Note that this is a complete removal of cues, rather than stochastic cues as in the previous section. When retaining all cues the performance obtained by all models is in line with the training performance, with hcDRQN being the best model and the only one doing better than chance in the allocentric tasks (<xref ref-type="fig" rid="F7">Fig. 7d</xref>, all cues). This demonstrates that the models were able to remember all the tasks on which they were trained. To test for generalisation, we gradually reduced the number of cues available in the environment. Our results show that hcDRQN is the only model that can handle half of the cues being removed (<xref ref-type="fig" rid="F7">Fig. 7d</xref> and S16 for the same test with longer maze). Interestingly, cue removal is more detrimental to allocentric navigation than egocentric navigation. This result is in line with experimental observations, in which allocentric but not egocentric task performance is impaired upon cue removal (<xref ref-type="bibr" rid="R44">44</xref>)(<xref ref-type="fig" rid="F7">Fig. 7c</xref>). In contrast, models trained with full observability cannot generalise, as they rely on the presence of specific cues (<xref ref-type="fig" rid="F7">Fig. 7g and h</xref>). Furthermore, the performance of both allocentric and egocentric tasks under full observability declines proportionally with the removal of cues. In both cases, the models fail to complete the tasks when no cues are present. This is in contrast to the partial observability version, where hcDRQN shows some degree of robustness to cue removal (cf. <xref ref-type="fig" rid="F7">Fig. 7d</xref>).</p><p id="P28">Finally, we repeated the original task on the T-maze adding a distractor cue and adding (Gaussian) white noise to the cues. When a distractor cue is introduced, hcDQN’s performance drops to chance level, whereas hcDRQN achieves a success rate of approximately 70% (<xref ref-type="fig" rid="F7">Fig. 7e</xref>). To test the robustness of both models to noise we tested a range of noise levels (<xref ref-type="fig" rid="F7">Fig. 7f</xref>). The hcDRQN model can handle a relatively large degree of cue noise without any changes in the overall performance, while hcDQN is more unstable and shows a faster decrease in performance as the noise is increased.</p><p id="P29">Taken together, our generalisation tests demonstrate that hcDRQN generalises better to different and realistic task conditions, in line with animal behaviour.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P30">Naturalistic behaviour almost always relies on navigating environments with limited visibility. Here, we have shown that recurrent hippocampal networks play a pivotal role in such environmental setups. Our investigation began by training RL agents to perform ego-allocentric tasks within partially observable environments. Remarkably, agents equipped with recurrent hippocampal circuitry successfully mastered these tasks, mirroring real-world animal behaviour. Additionally, our models predicted reward, strategy, and temporal neuronal representations, which we validated using tetrode hippocampal neuronal recordings. Furthermore, hippocampal-like RL agents predicted state-specific trajectories which resemble experimentally-observed animal behaviour. In stark contrast, agents trained in fully observable environments failed to replicate the experimental data. Importantly, hippocampal-like RL agents with CA3 recurrence demonstrated enhanced generalisation capabilities across novel task conditions.</p><p id="P31">Motivated by the challenging conditions that animals often face in the wild, we have focused on a task setup with partial observability. This is also supported by the lack of visual acuity in rodents (<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>). In addition, when our models were trained with full observability, they could not generalise (<xref ref-type="fig" rid="F7">Fig. 7h</xref>), which further suggests that partial observability provides a better model of animal behaviour. hcDRQN performs particularly well in partial environments, in line with previous research in artificial neural networks (<xref ref-type="bibr" rid="R47">47</xref>). Partially observable environments represent a more realistic setup, which together with our results suggests that hippocampal CA3 region may have evolved to support the ability to navigate in these conditions.</p><p id="P32">Our model shows that CA3 recurrence is needed to solve all the tasks we tested due to its ability to remember the relevant sensory cues. This is in line with experimental results showing that the CA3 region is involved in maintaining working memory representations in delayed-to-match sample tasks (<xref ref-type="bibr" rid="R9">9</xref>–<xref ref-type="bibr" rid="R11">11</xref>). Moreover, using low dimensionality analysis we have shown that hippocampal neuronal dynamics encodes a difference between correct and incorrect trials.</p><p id="P33">This could be a reflection of animals taking different trajectories on incorrect and correct trials or reward value transmitted from downstream brain areas as predicted by temporal different reinforcement learning algorithms (<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R49">49</xref>). Furthermore, our work suggests that CA3 recurrence enable hippocampal networks to develop representations that are context-invariant, which can then be used by downstream decoders for action selection and reinforcement learning.</p><p id="P34">Our study generates experimentally testable predictions on the role of CA3 recurrence in facilitating navigation in environments with limited visibility. In particular, our work predicts that perturbing CA3 recurrence should impair the ability of animals to generalise to new and stochastic task conditions. Furthermore, our findings suggest that plasticity in recurrent CA3 synapses is important in supporting these functions (<xref ref-type="fig" rid="F2">Fig. 2d</xref>).</p><p id="P35">Because our focus is on contrasting models with neuroscientific observations, we have used the same block-wise training of ego- and allocentric tasks as employed experimentally. In this setup, our work predicts that CA3 recurrence is particularly important to learn allocentric tasks (<xref ref-type="fig" rid="F2">Fig. 2d</xref>). Generalisation tests, show that our models are more robust to the removal of cues in egocentric tasks when compared to allocentric tasks (<xref ref-type="fig" rid="F7">Fig. 7c</xref>), consistent with experimental observations (<xref ref-type="bibr" rid="R44">44</xref>) and suggesting that the agents have learned the macrostructure of the task. However, the tasks that we have used here represent only a small subset of all the possible challenges that animals are faced with. This means that our model-animal comparison is relatively unfair as animals have to deal with much more than solving these two tasks. Relatedly, we have assumed the existence of action-value output networks that perform clean allo-egocentric task-switching. While such computations are believed to be executed by striatal networks (<xref ref-type="bibr" rid="R50">50</xref>), the precise mechanisms remain elusive. Finally, we have used artificial algorithms for credit assignment in hippocampal networks. All these elements remain to be explored in future work.</p><p id="P36">Our results show that in the multi-task block-training setting we tested, a model with a recurrent layer (hcDRQN) without experience replay outperforms alternative methods that were specifically designed for continual learning, consistent with recent machine learning findings (<xref ref-type="bibr" rid="R51">51</xref>). Given that hcDRQN is a systems-level approximation of the hippocampal system, it suggests that the brain relies on a combination of recurrent neural networks to continually adapt to new situations, at least in navigational tasks. It remains to be tested how general these principles are across other areas of the brain.</p><p id="P37">Classical hippocampal models suggest that CA3 recurrency enables pattern completion (<xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref>), while more recent computational models propose that the hippocampus creates a predictive map of the environment through successor representations (SR) (<xref ref-type="bibr" rid="R21">21</xref>). Our research aligns with this predictive view of the hippocampus, because in our model the hippocampus learns to predict the correct trajectory to take given sensory input, which is then reflected in simulated and experimental neuronal dynamics (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). Furthermore, our findings underscore the essential role of CA3 in constructing the hippocampal predictive map, consistent with the predictive view of hippocampal function (<xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R26">26</xref>). In another set of studies recurrent neural networks (RNNs) have been trained to support spatial navigation. They have shown that RNNs develop spatial representations similar to experimental findings (<xref ref-type="bibr" rid="R14">14</xref>–<xref ref-type="bibr" rid="R16">16</xref>). For instance, Cueva and Wei (<xref ref-type="bibr" rid="R15">15</xref>) demonstrated grid-like spatial response patterns, border cells, and band-like cells in trained RNNs. Similarly, Banino et al. (<xref ref-type="bibr" rid="R14">14</xref>) revealed grid cell-like representations when training deep recurrent reinforcement learning networks for 3D navigation. Uria et al. (<xref ref-type="bibr" rid="R16">16</xref>) trained a similar system, yielding neurons with spatial properties akin to those in Banino et al. (<xref ref-type="bibr" rid="R14">14</xref>), Cueva and Wei (<xref ref-type="bibr" rid="R15">15</xref>). While these studies emphasize the importance of memory (see also (<xref ref-type="bibr" rid="R17">17</xref>)) and recurrence in hippocampal networks, they do not assess their function in partial observability and its relationship to experimental observations in goal-driven tasks, which is our focus.</p><p id="P38">In comparison, other models have explicitly considered reward-based navigational tasks. For example, work by Foster et al. (<xref ref-type="bibr" rid="R23">23</xref>) utilises an actor-critic network and goal-independent representations for solving water-maze tasks, which was later extended hierarchically by Tessereau et al. (<xref ref-type="bibr" rid="R25">25</xref>). On the other hand, Arleo and Gerstner (<xref ref-type="bibr" rid="R27">27</xref>) combined external vision, path-integration and goal-driven reinforcement learning in a model evaluated on a physical robot. Whilst these models do connect various components to distinct brain structures, they have largely overlooked the significance of CA3 recurrence and its implications in real-world scenarios, including partial observability. Our research highlights the critical nature of these features in accurately modelling both behaviour and neuronal dynamics.</p><p id="P39">A limited number of models in neuroscience have considered environments with partial observability (<xref ref-type="bibr" rid="R54">54</xref>, <xref ref-type="bibr" rid="R55">55</xref>). A notable exception are Clone Structured Cognitive Graphs that create latent state-space maps for aliasing (i.e. partial observability) while capturing cognitive map phenomena (<xref ref-type="bibr" rid="R55">55</xref>, <xref ref-type="bibr" rid="R56">56</xref>). Our work adds to this body of literature supporting the importance of partial observability in explaining experimental observations.</p><p id="P40">Overall, our work suggests that hippocampal networks play a critical role in the ability of animals to continuously adapt to the environment under realistic conditions and with good generalisation properties.</p></sec><sec id="S10" sec-type="materials | methods"><title>Materials and Methods</title><p id="P41">We begin by outlining the deep reinforcement learning approaches employed in this study, followed by an explanation of the methods utilised for the analysis of neural data.</p><sec id="S11"><title>Reinforcement learning models</title><p id="P42">We developed a deep reinforcement learning model consistent with the hippocampal architecture. To train the models we designed a custom-built 2D gym-minigrid maze (<xref ref-type="bibr" rid="R29">29</xref>), mimicking the T-maze environment (<xref ref-type="fig" rid="F1">Fig. 1a</xref>) in line with common experimental setups, which allows us to compare our models with the behavioural and neural data (<xref ref-type="bibr" rid="R5">5</xref>). In order to capture cues commonly placed on external walls in experimental setups we placed four cues in the environment in both allocentric and egocentric trials, in line with (<xref ref-type="bibr" rid="R5">5</xref>). At the beginning of any given trial, the agent was placed at the start of the north or south arms of the maze following the same setup of the animal experiments, and we closed access to the opposite arm thus converting the maze into a T-maze. We considered two terminal states: one rewarded and one unrewarded. After reaching a terminal state (rewarded/unrewarded) we allowed the agent to continue exploring for three extra time steps which allowed us to model the animal behaviour right after reward consumption. During model training, we extracted neural activities which we used to contrast model and animal neural data. Note that direct sensory information about the terminal states was not given as input to the agent.</p><sec id="S12"><title>Deep RL agents:</title><p id="P43">Reinforcement learning (RL) models an agent that observes the environment and takes an action <italic>a</italic>. This action transitions the agent into a new state <italic>s</italic> of the environment which might give back a reward <italic>r</italic> according to the utility of the action selected. This can be formally defined by a Markov Decision Process as tuple of ⟨<italic>𝒮, 𝒜, P, ℛ, γ</italic>⟩ where <italic>𝒮</italic> is the set of all the states, <italic>𝒜</italic> is the action set, <italic>𝒫</italic> the transition matrix <italic>P</italic>(<italic>s</italic>′|<italic>s, a</italic>) from current state <italic>s</italic> to the next state <italic>s</italic>′ when taking action <italic>a</italic>. The objective is to maximise the expected total rewards, called return <italic>G</italic><sub><italic>t</italic></sub> defined as <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> where <italic>t</italic> is the current time step, <italic>R</italic><sub><italic>t</italic>+1</sub> is the reward obtained at time <italic>t</italic> + 1, <italic>γ</italic> is a discount factor such that 0 <italic>≤ γ &lt;</italic> 1 and <italic>T</italic> is the time at which the episode terminates.</p><p id="P44">For the hippocampal RL models, we build on the standard deep reinforcement learning models. In particular, we use Deep Q-networks (DQN) (<xref ref-type="bibr" rid="R30">30</xref>), in which states <italic>s</italic> are provided as input to an artificial neural network that are then mapped onto value-action pairs <italic>Q</italic>(<italic>s, a</italic>). The network is trained using state-outcome transition tuples, (<italic>s, a, s</italic>′, <italic>r</italic>), where <italic>s</italic> is the current state, <italic>a</italic> is the action, <italic>r</italic> denotes reward outcome and <italic>s</italic>′ the next state. The error function used to train the hippocampal network follows a Q-update function as <italic>E</italic><sub><italic>i</italic></sub> at step <italic>i</italic>: <disp-formula id="FD1"><label>(1)</label><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mo>∼</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>'</mml:mo></mml:mrow></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula> where <italic>θ</italic> denotes the network weights, <italic>γ</italic> is the discount factor and <italic>D</italic> is the dataset of past trajectories. As done by standard DQNs we use the concept of <italic>target network</italic> (<italic>θ</italic><sup>−</sup>). The target network is a frozen version of the primary neural network, which helps to stabilise reinforcement learning. It is updated at regular intervals to avoid inflated estimations of the Q-values. The hyperparameter <italic>target update counter</italic> controls the frequency of updates of the target Q-network. <xref ref-type="disp-formula" rid="FD1">Equation 1</xref> calculates expectation of the squared error between the target Q-value and the predicted Q-value over a batch of state-action experience tuples. We minimise this expected error by adjusting the parameters <italic>θ</italic><sub><italic>i</italic></sub> of the network using standard gradient descent and Adam optimiser (see parameters in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>).</p><p id="P45">To create a model that more closely captures the hippocampal circuitry, we consider a three-layered structure with the input layer modelling entorhinal input to Dentate Gyrus (DG), the first layer represents CA3, a second layer represents CA1, and the output layer encodes the value of a given action-state pair, <italic>Q</italic>(<italic>a, s</italic>) (<xref ref-type="fig" rid="F2">Fig. 2b</xref>). The DG input originates from the entorhinal cortex (EC) which is believed to provide the hippocampus with a spatial map of the environment (<xref ref-type="bibr" rid="R19">19</xref>). In our model, the EC spatial map is approximated by the 2D top-down spatial map provided by the minigrid environment (<xref ref-type="fig" rid="F1">Fig. 1c</xref>). The output layer encodes state-action Q values, which abstracts out hippocampal-to-striatum functional connectivity (<xref ref-type="bibr" rid="R31">31</xref>). A Q-value output action head was not sufficient to solve all tasks that we consider, even in the presence of replay mechanisms (<xref ref-type="supplementary-material" rid="SD1">Fig. S3a</xref>). Therefore, to ensure that the network could solve the two tasks (ego and allocentric) we use task-specific heads (i.e. two heads) at the output (see <xref ref-type="sec" rid="S10">Methods</xref>), for all the models we consider. In biological networks this could be implemented through task-specific contextual signals (<xref ref-type="bibr" rid="R57">57</xref>). Indeed, it is also possible to train agents with only one head, provided that a task-specific contextual signal is provided as input to the model (<xref ref-type="supplementary-material" rid="SD1">Fig. S2</xref>). Note that it is only at the output level that we assume task-separation (i.e. one output head for allocentric and another for egocentric tasks). This means that the representations developed by the hippocampus have to be generalisable across the different tasks. Given that we can capture key aspects of experimental data with this setup, this implies that task-specific Q-action values are encoded outside the hippocampus.</p><p id="P46">All our models have a three-layered (hippocampal-like) structure in which the input layer is of shape (<italic>RxC</italic>) where <italic>R</italic> is the number of rows in the input grid, <italic>C</italic> is the number of columns. The output layer has <italic>Nx</italic>1 shape (<italic>N</italic> = 3) denoting the Q-values for the 3 actions that the agent can take in the environment (left, right, forward). We use a standard discount factor (<italic>γ</italic> = 0.9), no replay memory (i.e. replay memory buffer size = 1, but see <xref ref-type="supplementary-material" rid="SD1">Fig. S3</xref> for different variants) and a batch size of 1 during training. Adam is used as the optimiser with a learning rate <italic>α</italic> of 0.001. We use a CA3 layer size of 50 for all models considered. In the case of hcDRQN these units are Gated Recurrent Units. A Gated Recurrent Unit (GRU) (<xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R37">37</xref>) network is a specially designed recurrent neural network that can capture long temporal dependencies in data. GRUs achieve this through a gating mechanism, which regulates the flow of information. This mechanism includes an update gate, controlling the extent to which a GRU unit keeps the previous state, and a reset gate, deciding how much past information to forget. The learning rate and <italic>ϵ</italic> (0.3 − 0.05) for <italic>ϵ</italic>-greedy, which controls the trade-off between exploration (choosing new actions) and exploitation (using known actions) have been selected using a grid-search. All the hyperparameters are given in <xref ref-type="supplementary-material" rid="SD1">Table S2</xref>.</p></sec><sec id="S13"><title>Partial observability</title><p id="P47">In our grid-based environment, models operate under conditions of limited environmental observability, mirroring the real-world challenges faced by navigating animals. Moreover, in reality, animals rarely possess complete access to all pertinent sensory data during navigation. For instance, they may initially focus on cue information but then shift their attention to executing motor commands to reach their destination. In experimental neuroscience, while cues are typically positioned along the outer walls of a room (<xref ref-type="bibr" rid="R5">5</xref>), animals do not continuously fixate on these cues. Additionally, maze setups often involve the incorporation of walls of varying heights, further restricting the visual input available to the animals.</p><p id="P48">To substantiate the importance of CA3 in navigation and its ability to better align with experimental findings in partially observable environments, we compare our models against those trained with full visibility. This comparison underscores the significance of CA3 and its capacity to more accurately capture experimental outcomes when navigating in environments with limited sensory input.</p></sec><sec id="S14"><title>Training details</title><p id="P49">The training phase consisted of a block of allocentric and egocentric trials. Specifically, each block contains 25 trials, and there were a total of 4 blocks (allocentric north/south, egocentric north/south). To model egocentric trials the reward location changed between north and south, but for allocentric trials it was kept constant. The agents were first exposed to blocks of allocentric trials in the north direction, which were then alternated with blocks of allocentric trials in the south direction. This alternating pattern was repeated four times before switching to egocentric trials. The same north/south combination was maintained throughout the duration of the egocentric trials. In total, the training consisted of 10,000 individual trials (200 blocks each with 25 trials for both ego and allocentric tasks).</p><p id="P50">All model parameters (weights and biases) were updated using BPTT within the trial and once the model reached the terminal state (i.e. there is no BPTT across trials and batch-size=1). Further, BPTT is applied at every two steps backwards in time until the first step is reached, as is commonly done in deep RL (<xref ref-type="bibr" rid="R58">58</xref>). The maximum number of steps that BPTT can go back in time is 128 steps, which is also the max number of steps during inference. hcDQN is trained in a similar fashion but we do not backpropagate the gradients in time towards the first step. During model testing, such as during the generalisation tests, we turn off learning/plasticity.</p><p id="P51">A two-head setup is utilised, where the final layer outputs two Q-values: Q-value-allo and Q-value-ego. The state input to the models are 2D matrices where cues, walls and no-walls were encoded as scalar values. For the partial view the observation size was 9 (3x3) while for the full view was 81 (9x9).</p></sec><sec id="S15"><title>Generalisation tests</title><p id="P52">We performed four types of generalisation tests (<xref ref-type="fig" rid="F7">Fig. 7</xref>): <list list-type="order" id="L1"><list-item><p id="P53"><italic>Longer maze</italic>: In this test, the length of the starting corridor was increased while keeping the length of the two terminal arms constant.</p></list-item><list-item><p id="P54"><italic>Cue removal</italic>: Different combinations of cue removal were performed, ranging from removing all cues to removing none. The cues were removed at the beginning of the trial, meaning that the agent had no access to the cue at any point during the task. This is in contrast to the experiments on probabilistic cue removal (<xref ref-type="fig" rid="F6">Fig. 6</xref>), where the agent still had access to these cues with a given probability.</p></list-item><list-item><p id="P55"><italic>Distractor</italic>: Another cue (represented by a scalar value) was added just next to (below) the existing cues.</p></list-item><list-item><p id="P56"><italic>Random noise</italic>: We added normally distributed noise, <italic>N</italic> (<italic>µ, σ</italic><sup>2</sup>) with <italic>µ</italic> = 0 and <italic>σ</italic><sup>2</sup> in the range of (<xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R15">15</xref>).</p></list-item></list></p></sec><sec id="S16"><title>Continual learning algorithms</title><p id="P57">Continual Learning, also known as Incremental or Life-long Learning, refers to the ability of a model to sequentially master multiple tasks without losing previously acquired knowledge, even when data from older tasks are no longer accessible during the training of new ones. We evaluated the performance of two state-of-the-art regularisation-based continual learning algorithms: Elastic Weight Consolidation (EWC) (<xref ref-type="bibr" rid="R38">38</xref>) and Synaptic Intelligence (SI) (<xref ref-type="bibr" rid="R39">39</xref>). These artificial algorithms were tested against our recurrent and non-recurrent DQN architectures in identical task settings. EWC adds a penalty term to the loss function, derived from a Fisher Information Matrix (also called weight importance), which guides the learning such that information pertaining to previous tasks remains preserved. On the other hand, SI determines the importance of each neural network weight throughout the learning process and uses this information to restrict weight updates, preserving prior task knowledge. Both importance measures, which are tunable configurations (hyperparameters), were selected through hyperparameter optimisation (the EWC weight importance was set to 800 and the SI weight importance to 30).</p></sec><sec id="S17"><title>Experiments with fully observable environments</title><p id="P58">We repeated the main results with a fully observable environment with both hcDRQN and hcDQN models. Although both models can learn all tasks in terms of performance, their dPCA analysis does not show a clear separation of all the components. In hcDRQN, decision and strategy follow the same trends with activity dropping to zero right after the reward point. This is the opposite of the partial view hcDRQN and animal activity where strategy components keep the separation even after the reward point. Moreover, hcDRQN fails to capture the time component. The hcDQN model completely fails to separate the strategy north components. Overall, given that full view model fails to capture dPCA components we argue that the fully observable environment does not provide a good match of the hippocampal data. We run further tests to analyse the animal trajectories and the generalisation capabilities of these full view RL models. Although most of the trajectory maps show a close match between the animal and hcDRQN, there are situations in which the hcDQN seems to be a better match to animal data. The generalisation tests highlight the limits of the fully observable models, as cue are gradually removed performance drops drastically, emphasising the dependency of these models on the cues. Animal performance and partial view RL models show evidence that egocentric task do not rely on cues, however, the fully observable models remain highly dependent on the cues. Overall our results suggest that hcDRQN trained with partial observability provides an overall best match with animal behaviour and neuronal encodings.</p></sec><sec id="S18"><title>Computing details</title><p id="P59">All experiments were conducted on the BluePebble supercomputer at Bristol; mostly on GPUs (GeForce RTX 2080 Ti) and some on CPUs (Intel(R) Xeon(R) Silver 4112 CPU @ 2.60GHz). We did not record the total computing time for the experimental results presented in this paper, but this can be estimated as follows. To train each model (one seed with all the task-specific trials) takes approx 1 hour and 30 min. For each of the models, we run 5 random seeds, resulting in approx 6 hours per model. When recording the activations, the total time is around 8 hours. Testing a single model for one seed takes approx 5 min. Overall total time it takes to run our models is 32 hours (8 x 4) for training with 5 seeds and 2 hours for the testing results with 5 seeds.</p></sec><sec id="S19"><title>Statistical analysis</title><p id="P60">Due to the inherent variability of the starting conditions on the learning path of these models, we trained our models across 5 different randomly selected seeds. To assess the significance of all relevant figures, we conducted a two-sided paired t-test on the relative alterations across the various seeds. Significance levels are denoted as follows: * (<italic>p</italic> &lt; 0.05), ** (<italic>p</italic> &lt; 0.01), *** (<italic>p</italic> &lt; 0.001), and **** (<italic>p</italic> &lt; 0.0001).</p></sec></sec><sec id="S20"><title>Neural and behavioural experimental data</title><sec id="S21"><title>Neural data analysis using demixed PCA</title><p id="P61">We used the neural activities of 612 hippocampal CA1 neurons from five behaving rats were recorded, which were obtained in the dorsal and ventral CA1 using multiple tetrodes (<xref ref-type="fig" rid="F3">Fig. 3a</xref>; see full details in Ciocchi et al. (<xref ref-type="bibr" rid="R5">5</xref>)). Spike sorting was used to assign spikes to different neurons (full details in (<xref ref-type="bibr" rid="R5">5</xref>)), which were then converted to firing rates of individual neurons using a bin size of 0.2s (<xref ref-type="bibr" rid="R5">5</xref>). Neural activity was aligned to the crossing of a reward-sensor. We considered neural activity 2s before and 4s after crossing the reward sensor for dPCA analysis. The reward sensor was located 10 cm from the end of the rewarded area. Animals were trained on a T-maze task in which rats had to follow both allocentric and egocentric navigational rules to reach reward points. We performed demixed Principal Component Analysis (dPCA) (<xref ref-type="bibr" rid="R59">59</xref>) on the neuronal firing rates using 3 behavioural variables – trial decision, strategy and time (dPCA <italic>λ</italic> = 2.919<italic>e</italic><sup>−05</sup> was found using grid-search as done by (<xref ref-type="bibr" rid="R59">59</xref>)). We used the dPCA code made available by the authors of (<xref ref-type="bibr" rid="R59">59</xref>) in <ext-link ext-link-type="uri" xlink:href="https://github.com/machenslab/dPCA">https://github.com/machenslab/dPCA</ext-link>. For models, we aligned data to the timestep in which a terminal state was reached and converted timesteps to seconds by making one timestep correspond to 200ms. To contrast with animal data, we continued recording the neural activity for 4s after reaching the terminal. The animal experiments used in our work were approved by the appropriate ethics committee as in Ciocchi et al. (<xref ref-type="bibr" rid="R5">5</xref>).</p><p id="P62">To quantitatively assess the differences in neural dynamics between observed data and model, we calculated the mean squared error (MSE) for the principal components derived from dPCA of both the animal and model CA1 neural activities. The MSE represents the average of the squared differences between the observed dPCs (<italic>Y<sub>i</sub></italic>) and the dPCs predicted by the model (<italic>Ŷ<sub>i</sub></italic>), as given by <inline-formula><mml:math id="M3"><mml:mrow><mml:mtext>MSE</mml:mtext><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and <italic>Ŷ<sub>i</sub></italic> were normalised by their max and min values before computing this error metric.</p></sec><sec id="S22"><title>Behavioural data</title><p id="P63">The behavioural data (i.e. animal task performance) consists of a total of 47067 trials recorded over multiple days (3 to 7) from a total of 5 animals (<xref ref-type="bibr" rid="R5">5</xref>). However, as some animals only had a maximum of 800 continuous trials we used a maximum of 800 continual trials per animal. The maze was cleaned with an odour-neutral solution every 10 trials to avoid odour-guided navigation as described in (<xref ref-type="bibr" rid="R44">44</xref>).</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS190913-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d59aAcIbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgements</title><p>We would like to thank the Neural &amp; Machine Learning group, Kim Stachenfeld and Chris Summerfield for useful feedback.</p><sec id="S24"><title>Funding</title><p>DP was funded by the EPSRC Centre for Doctoral Training in Future Autonomous and Robotic Systems (FARSCOPE), SM by the Wellcome Trust (PMAG/563) and BBSRC and RPC by the Medical Research Council (MR/X006107/1), BBSRC (BB/X013340/1) and a ERC-UKRI Frontier Research Guarantee Grant (EP/Y027841/1). HM was funded by a FWF grant (I 5458; part of the German Research Foundation Research Unit 5159) and a WWTF grant (CS18-039). SC was funded by a European Research Council starting grant 716761 and a Swiss National Science Foundation professorship grant 170654. This work made use of the HPC system Blue Pebble at the University of Bristol, UK. We would like to thank Dr Stewart for a donation that supported the purchase of GPU nodes embedded in the Blue Pebble HPC system.</p></sec></ack><sec id="S25" sec-type="data-availability"><title>Data and Materials Availability</title><p id="P64">All data needed to evaluate the conclusions in the paper are present in the paper and/or the Supplementary Materials. The animal data and code used in this study is available in Dryad (<ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5061/dryad.qv9s4mwnw">http://doi.org/10.5061/dryad.qv9s4mwnw</ext-link>). We used the PyTorch library for all reinforcement learning models. The code used in our simulations is also available at <ext-link ext-link-type="uri" xlink:href="http://github.com/neuralml/hcRL">http://github.com/neuralml/hcRL</ext-link>.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P65"><bold>Author contributions</bold></p><p id="P66">The author contributions are as follows:</p><p id="P67">Conceptualization: DP, SM, and RPC Methodology: DP, SM, HMV, SC and RPC Investigation: DP, SM, MVD, HMV, SC and RPC Visualization: DP, SM, HMV and RPC Supervision: RPC Writing—original draft: DP, SM, and RPC Writing—review and editing: DP, SM, HMV, SC and RPC</p></fn><fn id="FN2" fn-type="conflict"><p id="P68"><bold>Competing Interests</bold></p><p id="P69">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><article-title>Cognitive maps in rats and men</article-title><source>Psychological review</source><year>1948</year><volume>55</volume><issue>4</issue><fpage>189</fpage><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><source>The hippocampus as a cognitive map</source><publisher-name>Clarendon Press</publisher-name><publisher-loc>Oxford</publisher-loc><year>1978</year></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name></person-group><source>Organization of memory</source><publisher-name>Academic Press</publisher-name><year>1972</year></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wikenheiser</surname><given-names>AM</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Hippocampal place-cell sequences depict future paths to remembered goals</article-title><source>Nature</source><year>2013</year><volume>497</volume><issue>7447</issue><fpage>74</fpage><lpage>79</lpage><pub-id pub-id-type="pmcid">PMC3990408</pub-id><pub-id pub-id-type="pmid">23594744</pub-id><pub-id pub-id-type="doi">10.1038/nature12112</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciocchi</surname><given-names>S</given-names></name><name><surname>Passecker</surname><given-names>J</given-names></name><name><surname>Malagon-Vina</surname><given-names>H</given-names></name><name><surname>Mikus</surname><given-names>N</given-names></name><name><surname>Klausberger</surname><given-names>T</given-names></name></person-group><article-title>Brain computation. selective information routing by ventral hippocampal CA1 projection neurons</article-title><source>Science</source><year>2015</year><month>May</month><volume>348</volume><issue>6234</issue><fpage>560</fpage><lpage>563</lpage><pub-id pub-id-type="pmid">25931556</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sosa</surname><given-names>M</given-names></name><name><surname>Giocomo</surname><given-names>LM</given-names></name></person-group><article-title>Navigating for reward</article-title><source>Nature Reviews Neuroscience</source><year>2021</year><volume>22</volume><issue>8</issue><fpage>472</fpage><lpage>487</lpage><pub-id pub-id-type="pmcid">PMC9575993</pub-id><pub-id pub-id-type="pmid">34230644</pub-id><pub-id pub-id-type="doi">10.1038/s41583-021-00479-z</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyberg</surname><given-names>N</given-names></name><name><surname>Duvelle</surname><given-names>É</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Spiers</surname><given-names>HJ</given-names></name></person-group><article-title>Spatial goal coding in the hippocampal formation</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>3</issue><fpage>394</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">35032426</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelson</surname><given-names>MG</given-names></name><name><surname>Hare</surname><given-names>TA</given-names></name></person-group><article-title>Goal-dependent hippocampal representations facilitate self-control</article-title><source>Journal of Neuroscience</source><year>2023</year><volume>43</volume><issue>46</issue><fpage>7822</fpage><lpage>7830</lpage><pub-id pub-id-type="pmcid">PMC10648530</pub-id><pub-id pub-id-type="pmid">37714706</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0951-22.2023</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>I</given-names></name><name><surname>Kesner</surname><given-names>RP</given-names></name></person-group><article-title>Differential contribution of nmda receptors in hippocampal subregions to spatial working memory</article-title><source>Nature neuroscience</source><year>2002</year><volume>5</volume><issue>2</issue><fpage>162</fpage><lpage>168</lpage><pub-id pub-id-type="pmid">11780144</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>I</given-names></name><name><surname>Kesner</surname><given-names>RP</given-names></name></person-group><article-title>Differential roles of dorsal hippocampal subregions in spatial working memory with short versus intermediate delay</article-title><source>Behavioral neuroscience</source><year>2003</year><volume>117</volume><issue>5</issue><fpage>1044</fpage><pub-id pub-id-type="pmid">14570553</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>PE</given-names></name><name><surname>Kesner</surname><given-names>RP</given-names></name></person-group><article-title>The role of the dorsal ca3 hippocampal subregion in spatial working memory and pattern separation</article-title><source>Behavioural brain research</source><year>2006</year><volume>169</volume><issue>1</issue><fpage>142</fpage><lpage>149</lpage><pub-id pub-id-type="pmid">16455144</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>Proceedings of the national academy of sciences</source><year>1982</year><volume>79</volume><issue>8</issue><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="pmcid">PMC346238</pub-id><pub-id pub-id-type="pmid">6953413</pub-id><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name></person-group><article-title>A quantitative theory of the functions of the hippocampal ca3 network in memory</article-title><source>Frontiers in cellular neuroscience</source><year>2013</year><volume>7</volume><fpage>98</fpage><pub-id pub-id-type="pmcid">PMC3691555</pub-id><pub-id pub-id-type="pmid">23805074</pub-id><pub-id pub-id-type="doi">10.3389/fncel.2013.00098</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Mirowski</surname><given-names>P</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Degris</surname><given-names>T</given-names></name><name><surname>Modayil</surname><given-names>J</given-names></name><etal/></person-group><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><year>2018</year><volume>557</volume><issue>7705</issue><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="pmid">29743670</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wei</surname><given-names>X-X</given-names></name></person-group><source>Emergence of grid-like representations by training recurrent neural networks to perform spatial localization</source><conf-name>International Conference on Learning Representations</conf-name><year>2018</year></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Ibarz</surname><given-names>B</given-names></name><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Zambaldi</surname><given-names>V</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name></person-group><article-title>The spatial memory pipeline: a model of egocentric to allocentric understanding in mammalian brains</article-title><source>BioRxiv</source><year>2020</year><month>11</month><comment>2020</comment></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JC</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name></person-group><article-title>The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation</article-title><source>Cell</source><year>2020</year><volume>183</volume><issue>5</issue><fpage>1249</fpage><lpage>1263</lpage><pub-id pub-id-type="pmcid">PMC7707106</pub-id><pub-id pub-id-type="pmid">33181068</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.10.024</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><article-title>The hippocampus as a spatial map: preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain research</source><year>1971</year><volume>34</volume><issue>1</issue><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="pmid">5124915</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><year>2005</year><volume>436</volume><issue>7052</issue><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Improving generalization for temporal difference learning: The successor representation</article-title><source>Neural computation</source><year>1993</year><volume>5</volume><issue>4</issue><fpage>613</fpage><lpage>624</lpage></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The hippocampus as a predictive map</article-title><source>Nature neuroscience</source><year>2017</year><volume>20</volume><issue>11</issue><fpage>1643</fpage><pub-id pub-id-type="pmid">28967910</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geerts</surname><given-names>JP</given-names></name><name><surname>Chersi</surname><given-names>F</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Burgess</surname><given-names>N</given-names></name></person-group><article-title>A general model of hippocampal and dorsal striatal learning and decision making</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><issue>49</issue><fpage>31427</fpage><lpage>31437</lpage><pub-id pub-id-type="pmcid">PMC7733794</pub-id><pub-id pub-id-type="pmid">33229541</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2007981117</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname><given-names>DJ</given-names></name><name><surname>Morris</surname><given-names>RG</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>A model of hippocampally dependent navigation, using the temporal difference learning rule</article-title><source>Hippocampus</source><year>2000</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="pmid">10706212</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knudsen</surname><given-names>EB</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name></person-group><article-title>Hippocampal neurons construct a map of an abstract value space</article-title><source>Cell</source><year>2021</year><volume>184</volume><issue>18</issue><fpage>4640</fpage><lpage>4650</lpage><pub-id pub-id-type="pmcid">PMC8459666</pub-id><pub-id pub-id-type="pmid">34348112</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.07.010</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tessereau</surname><given-names>C</given-names></name><name><surname>Coombes</surname><given-names>S</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name></person-group><article-title>Reinforcement learning approaches to hippocampus-dependent flexible spatial navigation</article-title><source>Brain and Neuroscience Advances</source><year>2021</year><volume>5</volume><elocation-id>2398212820975634</elocation-id><pub-id pub-id-type="pmcid">PMC8042550</pub-id><pub-id pub-id-type="pmid">33954259</pub-id><pub-id pub-id-type="doi">10.1177/2398212820975634</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>C</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name></person-group><article-title>Predictive auxiliary objectives in deep rl mimic learning in the brain</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2310.06089</elocation-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arleo</surname><given-names>A</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Spatial cognition and neuro-mimetic navigation: a model of hippocampal place cell activity</article-title><source>Biological cybernetics</source><year>2000</year><volume>83</volume><issue>3</issue><fpage>287</fpage><lpage>299</lpage><pub-id pub-id-type="pmid">11007302</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoon</surname><given-names>T</given-names></name><name><surname>Okada</surname><given-names>J</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name><name><surname>Kim</surname><given-names>JJ</given-names></name></person-group><article-title>Prefrontal cortex and hippocampus subserve different components of working memory in rats</article-title><source>Learning &amp; memory</source><year>2008</year><volume>15</volume><issue>3</issue><fpage>97</fpage><lpage>105</lpage><pub-id pub-id-type="pmcid">PMC2275661</pub-id><pub-id pub-id-type="pmid">18285468</pub-id><pub-id pub-id-type="doi">10.1101/lm.850808</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Chevalier-Boisvert</surname><given-names>M</given-names></name><name><surname>Willems</surname><given-names>L</given-names></name><name><surname>Pal</surname><given-names>S</given-names></name></person-group><source>Minimalistic gridworld environment for openai gym</source><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="https://github.com/maximecb/gym-minigrid">https://github.com/maximecb/gym-minigrid</ext-link></comment></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><etal/></person-group><article-title>Human-level control through deep reinforcement learning</article-title><source>nature</source><year>2015</year><volume>518</volume><issue>7540</issue><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Devan</surname><given-names>BD</given-names></name><name><surname>White</surname><given-names>NM</given-names></name></person-group><article-title>Parallel information processing in the dorsal striatum: relation to hippocampal function</article-title><source>Journal of neuroscience</source><year>1999</year><volume>19</volume><issue>7</issue><fpage>2789</fpage><lpage>2798</lpage><pub-id pub-id-type="pmcid">PMC6786063</pub-id><pub-id pub-id-type="pmid">10087090</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.19-07-02789.1999</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>GR</given-names></name><name><surname>Joglekar</surname><given-names>MR</given-names></name><name><surname>Song</surname><given-names>HF</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><article-title>Task representations in neural networks trained to perform many cognitive tasks</article-title><source>Nature neuroscience</source><year>2019</year><volume>22</volume><issue>2</issue><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="pmcid">PMC11549734</pub-id><pub-id pub-id-type="pmid">30643294</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0310-2</pub-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherubini</surname><given-names>E</given-names></name><name><surname>Miles</surname><given-names>RM</given-names></name></person-group><article-title>The ca3 region of the hippocampus: how is it? what is it for? how does it do it?</article-title><source>Frontiers in cellular neuroscience</source><year>2015</year><volume>9</volume><fpage>19</fpage><pub-id pub-id-type="pmcid">PMC4318343</pub-id><pub-id pub-id-type="pmid">25698930</pub-id><pub-id pub-id-type="doi">10.3389/fncel.2015.00019</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>Assael</surname><given-names>IA</given-names></name><name><surname>Shillingford</surname><given-names>B</given-names></name><name><surname>de Freitas</surname><given-names>N</given-names></name><name><surname>Vogels</surname><given-names>T</given-names></name></person-group><source>Cortical microcircuits as gated-recurrent neural networks</source><conf-name>Advances in neural information processing systems</conf-name><year>2017</year><fpage>272</fpage><lpage>283</lpage></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Tirumala</surname><given-names>D</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><article-title>Prefrontal cortex as a meta-reinforcement learning system</article-title><source>Nature neuroscience</source><year>2018</year><volume>21</volume><issue>6</issue><fpage>860</fpage><lpage>868</lpage><pub-id pub-id-type="pmid">29760527</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>van Merriënboer</surname><given-names>B</given-names></name><name><surname>Bahdanau</surname><given-names>D</given-names></name><name><surname>Bougares</surname><given-names>F</given-names></name><name><surname>Schwenk</surname><given-names>H</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><source>Learning phrase representations using rnn encoder–decoder for statistical machine translation</source><conf-name>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</conf-name><year>2014</year><fpage>1724</fpage><lpage>1734</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>J</given-names></name><name><surname>Gulcehre</surname><given-names>C</given-names></name><name><surname>Cho</surname><given-names>K</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><source>Empirical evaluation of gated recurrent neural networks on sequence modeling</source><conf-name>NIPS 2014 Workshop on Deep Learning, December 2014</conf-name><year>2014</year></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>J</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Desjardins</surname><given-names>G</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Milan</surname><given-names>K</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Ramalho</surname><given-names>T</given-names></name><name><surname>Grabska-Barwinska</surname><given-names>A</given-names></name><etal/></person-group><article-title>Overcoming catastrophic forgetting in neural networks</article-title><source>Proceedings of the national academy of sciences</source><year>2017</year><volume>114</volume><issue>13</issue><fpage>3521</fpage><lpage>3526</lpage><pub-id pub-id-type="pmcid">PMC5380101</pub-id><pub-id pub-id-type="pmid">28292907</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1611835114</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name></person-group><source>Continual learning through synaptic intelligence</source><conf-name>International conference on machine learning</conf-name><conf-sponsor>PMLR</conf-sponsor><year>2017</year><fpage>3987</fpage><lpage>3995</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Vicarious trial and error</article-title><source>Nature Reviews Neuroscience</source><year>2016</year><volume>17</volume><issue>3</issue><fpage>147</fpage><lpage>159</lpage><pub-id pub-id-type="pmcid">PMC5029271</pub-id><pub-id pub-id-type="pmid">26891625</pub-id><pub-id pub-id-type="doi">10.1038/nrn.2015.30</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruger</surname><given-names>J</given-names></name><name><surname>Dunning</surname><given-names>D</given-names></name></person-group><article-title>Unskilled and unaware of it: how difficulties in recognizing one’s own incompetence lead to inflated self-assessments</article-title><source>Journal of personality and social psychology</source><year>1999</year><volume>77</volume><issue>6</issue><fpage>1121</fpage><pub-id pub-id-type="pmid">10626367</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Recurrent dynamics in the cerebral cortex: Integration of sensory evidence with stored knowledge</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>33</issue><elocation-id>e2101043118</elocation-id><pub-id pub-id-type="pmcid">PMC8379985</pub-id><pub-id pub-id-type="pmid">34362837</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2101043118</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pemberton</surname><given-names>J</given-names></name><name><surname>Chadderton</surname><given-names>P</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name></person-group><article-title>Cerebellar-driven cortical dynamics enable task acquisition, switching and con-solidation</article-title><source>bioRxiv</source><year>2022</year><month>11</month><comment>2022</comment><pub-id pub-id-type="pmcid">PMC11686095</pub-id><pub-id pub-id-type="pmid">39738061</pub-id><pub-id pub-id-type="doi">10.1038/s41467-024-55315-6</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malagon-Vina</surname><given-names>H</given-names></name><name><surname>Ciocchi</surname><given-names>S</given-names></name><name><surname>Passecker</surname><given-names>J</given-names></name><name><surname>Dorffner</surname><given-names>G</given-names></name><name><surname>Klausberger</surname><given-names>T</given-names></name></person-group><article-title>Fluid network dynamics in the prefrontal cortex during multiple strategy switching</article-title><source>Nat Commun</source><year>2018</year><month>January</month><volume>9</volume><issue>1</issue><fpage>309</fpage><pub-id pub-id-type="pmcid">PMC5778086</pub-id><pub-id pub-id-type="pmid">29358717</pub-id><pub-id pub-id-type="doi">10.1038/s41467-017-02764-x</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hughes</surname><given-names>A</given-names></name></person-group><chapter-title>The topography of vision in mammals of contrasting life style: comparative optics and retinal organisation</chapter-title><source>The visual system in vertebrates</source><publisher-name>Springer</publisher-name><year>1977</year><fpage>613</fpage><lpage>756</lpage></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prusky</surname><given-names>GT</given-names></name><name><surname>West</surname><given-names>PW</given-names></name><name><surname>Douglas</surname><given-names>RM</given-names></name></person-group><article-title>Behavioral assessment of visual acuity in mice and rats</article-title><source>Vision Res</source><year>2000</year><volume>40</volume><issue>16</issue><fpage>2201</fpage><lpage>2209</lpage><pub-id pub-id-type="pmid">10878281</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hausknecht</surname><given-names>M</given-names></name><name><surname>Stone</surname><given-names>P</given-names></name></person-group><source>Deep recurrent q-learning for partially observable mdps</source><conf-name>2015 AAAI Fall Symposium Series</conf-name><year>2015</year></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Xiang</surname><given-names>J-Z</given-names></name></person-group><article-title>Reward-spatial view representations and learning in the primate hippocampus</article-title><source>Journal of Neuroscience</source><year>2005</year><volume>25</volume><issue>26</issue><fpage>6167</fpage><lpage>6174</lpage><pub-id pub-id-type="pmcid">PMC6725063</pub-id><pub-id pub-id-type="pmid">15987946</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1481-05.2005</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>E</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Arano</surname><given-names>R</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>Y-T</given-names></name><name><surname>Thompson</surname><given-names>A</given-names></name><name><surname>Oh</surname><given-names>SJ</given-names></name><name><surname>Samuel</surname><given-names>A</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Directed stepwise tracing of polysynaptic neuronal circuits with replication-deficient pseudorabies virus</article-title><source>Cell Reports Methods</source><year>2023</year><volume>3</volume><issue>6</issue><pub-id pub-id-type="pmcid">PMC10326449</pub-id><pub-id pub-id-type="pmid">37426757</pub-id><pub-id pub-id-type="doi">10.1016/j.crmeth.2023.100506</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><article-title>Action selection and action value in frontal-striatal circuits</article-title><source>Neuron</source><year>2012</year><volume>74</volume><issue>5</issue><fpage>947</fpage><lpage>960</lpage><pub-id pub-id-type="pmcid">PMC3372873</pub-id><pub-id pub-id-type="pmid">22681697</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.03.037</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ehret</surname><given-names>B</given-names></name><name><surname>Henning</surname><given-names>C</given-names></name><name><surname>Cervera</surname><given-names>MR</given-names></name><name><surname>Meulemans</surname><given-names>A</given-names></name><name><surname>von Oswald</surname><given-names>J</given-names></name><name><surname>Grewe</surname><given-names>BF</given-names></name></person-group><source>Continual learning in recurrent neural networks</source><conf-name>International Conference on Learning Representations (ICLR 2021)</conf-name><year>2021</year></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leutgeb</surname><given-names>S</given-names></name><name><surname>Leutgeb</surname><given-names>JK</given-names></name></person-group><article-title>Pattern separation, pattern completion, and new neuronal codes within a continuous ca3 map</article-title><source>Learning &amp; memory</source><year>2007</year><volume>14</volume><issue>11</issue><fpage>745</fpage><lpage>757</lpage><pub-id pub-id-type="pmid">18007018</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>E</given-names></name></person-group><article-title>The mechanisms for pattern completion and pattern separation in the hippocampus</article-title><source>Frontiers in systems neuroscience</source><year>2013</year><volume>7</volume><fpage>74</fpage><pub-id pub-id-type="pmcid">PMC3812781</pub-id><pub-id pub-id-type="pmid">24198767</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2013.00074</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vértes</surname><given-names>E</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><article-title>A neurally plausible model learns successor representations in partially observable environments</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>George</surname><given-names>D</given-names></name><name><surname>Rikhye</surname><given-names>RV</given-names></name><name><surname>Gothoskar</surname><given-names>N</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Dedieu</surname><given-names>A</given-names></name><name><surname>Lázaro-Gredilla</surname><given-names>M</given-names></name></person-group><article-title>Clone-structured graph representations enable flexible learning and vicarious evaluation of cognitive maps</article-title><source>Nature communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>2392</elocation-id><pub-id pub-id-type="pmcid">PMC8062558</pub-id><pub-id pub-id-type="pmid">33888694</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-22559-5</pub-id></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lazaro-Gredilla</surname><given-names>M</given-names></name><name><surname>Deshpande</surname><given-names>I</given-names></name><name><surname>Swaminathan</surname><given-names>S</given-names></name><name><surname>Dave</surname><given-names>M</given-names></name><name><surname>George</surname><given-names>D</given-names></name></person-group><article-title>Fast exploration and learning of latent graphs with aliased observations</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2303.07397</elocation-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name><name><surname>Gill</surname><given-names>JV</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Papadoyannis</surname><given-names>ES</given-names></name><name><surname>Field</surname><given-names>RE</given-names></name><name><surname>Sten</surname><given-names>TAH</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Froemke</surname><given-names>RC</given-names></name></person-group><article-title>Parallel processing by cortical inhibition enables context-dependent behavior</article-title><source>Nature neuroscience</source><year>2017</year><volume>20</volume><issue>1</issue><fpage>62</fpage><lpage>71</lpage><pub-id pub-id-type="pmcid">PMC5191967</pub-id><pub-id pub-id-type="pmid">27798631</pub-id><pub-id pub-id-type="doi">10.1038/nn.4436</pub-id></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yarats</surname><given-names>D</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Lazaric</surname><given-names>A</given-names></name><name><surname>Pinto</surname><given-names>L</given-names></name></person-group><source>Mastering visual continuous control: Improved data-augmented reinforcement learning</source><conf-name>International Conference on Learning Representations</conf-name><year>2021</year></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobak</surname><given-names>D</given-names></name><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Constantinidis</surname><given-names>C</given-names></name><name><surname>Feierstein</surname><given-names>CE</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Qi</surname><given-names>X-L</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><article-title>Demixed principal component analysis of neural population data</article-title><source>Elife</source><year>2016</year><month>April</month><volume>5</volume><pub-id pub-id-type="pmcid">PMC4887222</pub-id><pub-id pub-id-type="pmid">27067378</pub-id><pub-id pub-id-type="doi">10.7554/eLife.10989</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Ego-allocentric task setup in animals and reinforcement learning agents.</title><p>(<bold>a</bold>) Top, experimental setup in which rats were placed in a plus-shaped maze, which was effectively transformed into a T-maze on each trial by blocking the opposite arm (<xref ref-type="bibr" rid="R5">5</xref>). The task consists of reaching the reward at the end of one of two arms following either egocentric (pink) or allocentric (purple) rules. Bottom: both animal and artificial agents were trained by interleaving blocks of allocentric and egocentric (see main text). (<bold>b</bold>) Animal performance on allocentric and egocentric tasks following the setup shown in (a) across 5 animals. There are no statistically significant differences in performance between allocentric and egocentric perspectives. (<bold>c</bold>) The experimental setup in (a) was simulated using a grid world environment. This setup was then used to train reinforcement learning agents in ego- and allocentric tasks. Environment observability was modelled by defining a visible range around the animal (light grey box), which is limited to the current cell alone when the agent enters the terminal arms. Specifically, the agent can only see the squares directly in front of it and to its sides, but not behind it. A total of four cues (cf. (d)) are placed in the environment, two for the north starting state and two for the south starting state. Trophy and red cross represent rewarding and non-rewarding terminal states, respectively (not made visible to the agent). (<bold>d</bold>,<bold>e</bold>) Schematic showing the cues used (d) and the four possible allocentric and egocentric rules (e). Dotted line represents the ideal path towards the reward starting north/south. Cues are presented near the starting positions where key/box refer to the north starting point and lava/ball to the south one. In the allocentric task, the reward is always on the same side regardless of the starting position. In the egocentric task, the reward is always on the right side of the starting position. ***: <italic>p &lt;</italic> 0.001, ****: <italic>p &lt;</italic> 0.0001 (two-sided paired t-test).</p></caption><graphic xlink:href="EMS190913-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Reinforcement learning agents with CA3 recurrence jointly learn ego and allocentric tasks.</title><p>(<bold>a</bold>) Classical hippocampal trisynaptic circuitry: entorhinal cortex (EC), dentate gyrus (DG), and hippocampus CA3 and CA1 layers. In addition, an output layer maps hippocampal representations to action Q-values (turn left, turn right, move forward). (<bold>b</bold>) Schematics of reinforcement learning (RL) agents with hippocampal-like architecture modelled as deep-Q-networks (DQN) used to learn the goal-driven tasks described in <xref ref-type="fig" rid="F1">Fig. 1</xref>. In our models, the DG receives a simplified (partially observable) map of the environment which is processed by the CA3-CA1 pathway and then CA1 projects to the reward system to compute the Q-value of state-action pairs, Q(s,a). We consider two main models: (i) with CA3 recurrence (hcDRQN, top) or (ii) with CA3 as a feedforward network (hcDQN, bottom). Both models consist of two hidden layers (CA3 and CA1) and an output action readout layer. (<bold>c</bold>) Minigrid environment showing 3x3 and full view size (orange outline). (<bold>d</bold>) Performance of all models for allocentric (left), egocentric (middle) or both (right) tasks. For comparison with modern machine learning solutions to multi-task learning we also consider two popular algorithms: elastic weight consolidation (ML-EWC) and synaptic intelligence (ML-SI). (<bold>e</bold>) Learning curves for both hcDQN and hcDRQN, showing that the former fails to learn allocentric tasks. Arrows represent switching points. (<bold>f</bold>) Task performance of RL agents as environment observability is progressively incremented. Both models achieve the same performance under full observability whereas only the hcDRQN agent can learn tasks under non-full observability. Error bars represent standard error of the mean over 5 different initial conditions.</p></caption><graphic xlink:href="EMS190913-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Strategy, temporal, and outcome neural dynamics in RL agents and animals.</title><p>(<bold>a</bold>) Demixed principal components corresponding to task strategy (allocentric and egocentric for both north and south start locations), time and decision (correct and incorrect). hcDRQN components show separate task strategies whereas hcDQN mixes task strategies. Note that RL agents do not show any egocentric incorrect dynamics because they learn these tasks perfectly (i.e. without incorrect trials). All components are provided in Figs. S8-S10. (<bold>b</bold>) Mean squared error between normalised model and animal data demixed components. We also contrast agents trained with full and partial observability (cf. <xref ref-type="supplementary-material" rid="SD1">Fig. S7</xref>). (<bold>c</bold>) Percentage of explained variance for all combined Decision, Strategy, Interaction and Time components (cf. with individual components in <xref ref-type="supplementary-material" rid="SD1">Fig. S8</xref>).</p></caption><graphic xlink:href="EMS190913-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Hippocampal RL agents with recurrence capture animal behaviour.</title><p>(<bold>a</bold>) Time spent on each maze state across the four strategies for rats, hcDRQN and hcDQN. Animals spend more time at the decision and rewarded terminal points. hcDRQN better captures animal behaviour and hcDQN fails to solve allocentric north task (cf. <xref ref-type="fig" rid="F2">Fig. 2</xref>). (<bold>b</bold>) Time spent on each state normalised to the time spent on the final (terminal) state in models and animal. The 2D maze is represented as a series of discrete states along the optimal path from the starting point to the terminal point. Specifically, each of the 7 states corresponds to a unique position along this optimal 1D trajectory within the maze. (<bold>c</bold>) Error of time spent across states between behaviour predicted by the models and animal behaviour. (<bold>d</bold>) Model-animal behavioural errors (as in c) for each possible task-pairs. hcDRQN shows overall closer match to animal behaviour when compared to hcDQN. Error bars represent standard error of the mean over 5 different initial conditions.</p></caption><graphic xlink:href="EMS190913-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Agent’s behaviour predicts state-dependent action values.</title><p>(<bold>a</bold>) Schematic illustrating state-action policy for hcDRQN and hcDQN for allocentric north subtask. It highlights that hcDRQN solves the task and that it is more uncertain about which action (i.e. different actions have similar values) until the decision point. Each state is represented by 3 coloured arrows corresponding to the state Q-values where darker colour means higher value. Note that the agent is capable of rotating left or right at any position, including the start of the maze. Rotating changes the agent’s orientation but not its position, which is why there are non-zero Q-values associated with the left and right actions even at the starting location. (<bold>b</bold>) Top: Relative Q-value variance, given by the variance over Q-values for each state divided by the mean Q-value for each state. It shows that hcDQN decreases its relative variance as it gets closer to the decision state (see arrows) and that it has higher overall relative variance compared to hcDRQN. Bottom: Average Q-values over environmental states. Error bars represent standard error of the mean over 5 different initial conditions.</p></caption><graphic xlink:href="EMS190913-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Recurrence enables better generalisability to stochastic environments.</title><p>(<bold>a</bold>) Change in state-occupancy (with probabilistic cues - without probabilistic cues) for the hcDRQN agent across different degrees of cue removal. (<bold>b</bold>) hcDRQN outperforms hcDQN across different degrees of probabilistic cues. (<bold>c</bold>) Performance of hcDRQN agents trained with probabilistic cues compared to without. (<bold>d</bold>) Change in state-occupancy of hcDRQN agents trained with probabilistic cues.</p></caption><graphic xlink:href="EMS190913-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>hcDRQN shows better generalisation to maze length, cue removal, distractors and sensory noise.</title><p>(<bold>a</bold>) T-maze setup for increased length of the middle corridor, cue removal and random noise. (<bold>b</bold>) Performance decrease over gradual increase of the maze length shows that hcDRQN can handle middle maze length being 32 steps. Black arrows on the X-axis represent maze corridor length of 4 steps utilised during training. (<bold>c</bold>,<bold>d</bold>) Animal and model performance when cues are removed from the environment. Both hcDRQN (light green) and animal (blue) allocentric navigation are highly dependent on cues while egocentric is not affected by cue removal. On the other hand, hcDQN fails to solve both allocentric tasks. (<bold>e</bold>) When adding a distractor cue, hcDQN drops to chance level while hcDRQN can still solve most of the tasks. (<bold>f</bold>) When adding white Gaussian noise to the cues hcDRQN is more stable and robust when compared to hcDQN. (<bold>g</bold>) T-maze setup with full observability. This setup was used to evaluate generalisation capabilities following cue removal (h). (<bold>h</bold>) Compared to models with partial observability, all models with full observability show a drop in performance when cues are removed. Error bars represent the standard error of the mean over 5 different initial conditions.</p></caption><graphic xlink:href="EMS190913-f007"/></fig></floats-group></article>