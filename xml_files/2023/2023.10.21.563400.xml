<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190121</article-id><article-id pub-id-type="doi">10.1101/2023.10.21.563400</article-id><article-id pub-id-type="archive">PPR747065</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Evidence for a role of synchrony but not common fate in the perception of biological group movements</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Cracco</surname><given-names>Emiel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Papeo</surname><given-names>Liuba</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Wiersema</surname><given-names>Jan R.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><aff id="A1"><label>1</label>Department of Experimental Clinical and Health Psychology, Ghent University, 9000, Gent Belgium</aff><aff id="A2"><label>2</label>Institut des Sciences Cognitives-Marc Jeannerod, UMR5229, Centre National de la Recherche Scientifique (CNRS) &amp; Université Claude Bernard Lyon 1, 69675 Bron, France</aff></contrib-group><pub-date pub-type="nihms-submitted"><day>26</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Extensive research has shown that observers are able to efficiently extract summary information from groups of people. However, little is known about the cues that determine whether multiple people are represented as a social group or as independent individuals. Initial research on this topic has primarily focused on the role of static cues. Here, we instead investigate the role of dynamic cues. In two experiments with male and female human participants, we use EEG frequency tagging to investigate the influence of two fundamental Gestalt principles - synchrony and common fate - on the grouping of biological movements. In Experiment 1, we find that brain responses coupled to four point-light figures walking together are enhanced when they move in sync vs. out of sync, but only when they are presented upright. In contrast, we found no effect of movement direction (i.e., common fate). In Experiment 2, we rule out that synchrony takes precedence over common fate by replicating the null effect of movement direction while keeping synchrony. These results put forward synchrony as an important driver of social grouping, consistent with the fact that it is an important feature of social interaction and an indicator of social cohesion. In contrast, the influence of common fate on social grouping is less clear and will require further research.</p></abstract><kwd-group><kwd>biological motion perception</kwd><kwd>Gestalt perception</kwd><kwd>social grouping</kwd><kwd>EEG frequency tagging</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Social interaction is at the heart of human life: much of what we do, we do with others. Understanding social interactions requires processing not only individual actions, but also how they relate to one another. Although the processing of biological movements (e.g., <xref ref-type="bibr" rid="R7">Blake &amp; Shiffrar, 2007</xref>; <xref ref-type="bibr" rid="R9">Caspers et al., 2010</xref>; <xref ref-type="bibr" rid="R45">Pitcher &amp; Ungerleider, 2021</xref>) and how this scales up from one to multiple agents (e.g., <xref ref-type="bibr" rid="R13">Cracco et al., 2015</xref>, <xref ref-type="bibr" rid="R14">2016</xref>, <xref ref-type="bibr" rid="R15">2019</xref>, <xref ref-type="bibr" rid="R16">2022</xref>; <xref ref-type="bibr" rid="R12">Cracco &amp; Brass, 2018</xref>) is well understood, relatively little is known about how we organize agents into groups.</p><p id="P3">What is known, is that observers can efficiently extract summary information from groups of people (e.g., <xref ref-type="bibr" rid="R35">Nguyen et al., 2021</xref>; <xref ref-type="bibr" rid="R54">Sweeny et al., 2013</xref>; <xref ref-type="bibr" rid="R55">Sweeny &amp; Whitney, 2014</xref>; <xref ref-type="bibr" rid="R67">Whitney &amp; Leib, 2018</xref>). However, an important open question is what determines whether multiple agents are seen as a group, or as rather as multiple individuals (<xref ref-type="bibr" rid="R24">Hafri &amp; Firestone, 2021</xref>; <xref ref-type="bibr" rid="R28">Kaiser et al., 2019</xref>; <xref ref-type="bibr" rid="R39">Papeo, 2020</xref>). Initial research into this question has focused on static cues and found that individuals who appear to interact by virtue of their relative spatial positioning (e.g., standing face-to-face) are grouped in working memory (e.g., <xref ref-type="bibr" rid="R19">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="R38">Paparella &amp; Papeo, 2022</xref>; <xref ref-type="bibr" rid="R64">Vestner et al., 2019</xref>) and visual perception (e.g., <xref ref-type="bibr" rid="R3">Adibpour et al., 2021</xref>; <xref ref-type="bibr" rid="R5">Bellot et al., 2021</xref>; <xref ref-type="bibr" rid="R27">Ji et al., 2020</xref>; <xref ref-type="bibr" rid="R39">Papeo, 2020</xref>; <xref ref-type="bibr" rid="R41">Papeo et al., 2017</xref>). Here, we instead investigate two movement cues: synchrony (i.e., temporal alignment) and common fate (i.e., spatial alignment). Both synchrony and common fate are important Gestalt principles that play a fundamental role in structuring basic motion input into coherent percepts (for reviews, see <xref ref-type="bibr" rid="R65">Wagemans, Elder, et al., 2012;</xref> <xref ref-type="bibr" rid="R66">Wagemans, Feldman, et al., 2012</xref>). Yet, whether they also contribute to organizing more complex scenes with multiple social agents remains to be explored.</p><p id="P4">Recent research has provided initial evidence that synchrony can indeed shape the processing of other people’s actions (<xref ref-type="bibr" rid="R4">Alp et al., 2017</xref>; <xref ref-type="bibr" rid="R10">Cheng et al., 2022</xref>; <xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>; <xref ref-type="bibr" rid="R60">Tsantani et al., 2022</xref>) or emotional expressions (<xref ref-type="bibr" rid="R20">Elias et al., 2017</xref>). However, in addition to being restricted to synchrony, this research has not yet established whether these effects are specific to biological agents or are more general in nature. Here, we address this question by simultaneously investigating how synchrony and common fate influence the perception of biological group movements. To do so, we adapted a recently developed frequency tagging task that measures the holistic processing of biological movement (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>). Specifically, we extracted brain responses coupled to the walking pace of 4 point-light figures moving at a fixed pace either in or out of synchrony (synchrony), in the same or different directions (common fate), upright or inverted (inversion). Experiment 1 manipulated both synchrony and common fate. Experiment 2 manipulated common fate alone.</p><p id="P5">If synchrony and common fate contribute to social grouping, brain responses at the figures’ walking pace should be modulated by these principles. Previous research has shown that neural activity in visual brain areas is increased when objects (<xref ref-type="bibr" rid="R29">Kim &amp; Biederman, 2011</xref>; <xref ref-type="bibr" rid="R51">Roberts &amp; Humphreys, 2010</xref>) or bodies (<xref ref-type="bibr" rid="R1">Abassi &amp; Papeo, 2020</xref>, <xref ref-type="bibr" rid="R2">2022</xref>) are organized in interactive vs. non-interactive constellations (e.g., facing), and these effects were found to correlate with behavioral indices of perceptual grouping (<xref ref-type="bibr" rid="R2">Abassi &amp; Papeo, 2022</xref>). As a result, processing multiple walkers as a single group should result in stronger brain responses coupled to those movements. If both cues influence perception through the same mechanism, we might furthermore expect them to interact (e.g., <xref ref-type="bibr" rid="R25">Han, 2004</xref>; <xref ref-type="bibr" rid="R32">Lee &amp; Blake, 2001</xref>; <xref ref-type="bibr" rid="R44">Peterson &amp; Kimchi, 2013</xref>).</p><p id="P6">In addition, if synchrony and common fate influence biological motion perception, rather than motion perception more generally, we should see that their effects are sensitive to inversion. Indeed, inversion is well-known to disrupt the configural processing of biological motion (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022;</xref> <xref ref-type="bibr" rid="R22">Giese &amp; Poggio, 2003</xref>) in that it triggers a local processing style where point-light figures are processed as local dots instead of global agents (<xref ref-type="bibr" rid="R6">Bertenthal &amp; Pinto, 1994</xref>; <xref ref-type="bibr" rid="R42">Pavlova &amp; Sokolov, 2000</xref>). Therefore, if synchrony and common fate influence social grouping, as opposed to a more unspecific grouping of objects moving in synchrony or in the same direction, their effects should be abolished when the walkers are inverted.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S3"><title>Open Science Statement</title><p id="P7">Both experiments were preregistered (<ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/NRY_639">https://aspredicted.org/NRY_639</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://aspredicted.org/SQM_V4G">https://aspredicted.org/SQM_V4G</ext-link>). The stimuli, data files, analysis script, and experimental program are available on the OSF together with example videos (<ext-link ext-link-type="uri" xlink:href="https://osf.io/67sn8/">https://osf.io/67sn8/</ext-link>).</p></sec><sec id="S4"><title>Experiment 1</title><sec id="S5" sec-type="subjects"><title>Participants</title><p id="P8">In line with our preregistration, we collected a sample of 33 volunteers who participated in the experiment in return for €25. Individuals could only participate if they were between 18 and 35 years old, had normal or corrected-to-normal vision, were right-handed, were fluent in Dutch, and did not have a history of neurological or psychiatric disorder. Age restrictions were imposed to ensure a homogenous sample and limit age-related effects. The sample size was based on a previous study in which we found a medium-sized effect (<italic>d</italic><sub>z</sub> = 0.56) of inversion (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>). Specifically, we conservatively rounded down this effect to <italic>d</italic><sub>z</sub> = 0.50 and calculated the sample size needed to obtain 80% power to detect such an effect at α = 0.05. This revealed a required sample size of N = 33. Following our preregistered approach, two participants were excluded due to bad data quality (&gt; 10% of electrodes requiring interpolation<sup><xref ref-type="fn" rid="FN3">1</xref></sup>), resulting in a final sample of 31 participants (24 female, 7 male, <italic>M</italic><sub>age</sub> = 22.90, range<sub>age</sub> = 18-32). The study was approved by the local ethics board of the Faculty of Psychological and Educational Sciences at Ghent University (2021/129).</p></sec><sec id="S6"><title>Task, Stimuli, and Procedure</title><p id="P9">Participants first signed the informed consent form. Next, they filled out the Dutch version of the Autism Quotient questionnaire (AQ; <xref ref-type="bibr" rid="R26">Hoekstra et al., 2008</xref>) while the experimenter prepared the EEG cap. The AQ was included for exploratory purposes, based on evidence that autism is associated with a decreased ability or propensity to extract socially relevant information from biological motion (<xref ref-type="bibr" rid="R21">Federici et al., 2020</xref>; <xref ref-type="bibr" rid="R57">Todorova et al., 2019</xref>). Internal consistency of the AQ in the current study was good (α = 0.78).</p><p id="P10">After preparing the EEG cap, the experiment was started. The experiment was programmed in PsychoPy (<xref ref-type="bibr" rid="R43">Peirce et al., 2019</xref>) and rendered on a 24-inch computer monitor with a 60 Hz refresh rate, positioned approximately 80–100 cm from the participant. During the experiment, participants watched videos of 4 point-light walkers, walking in place at a fixed pace of 2.4 Hz (1 step every ∼417 ms). Each walker extended an area of ∼2.33° x 5.24°, with a horizontal distance of 4.08° between the mid-point of two walkers and a vertical distance of 6.69°. In a previous study, we found that a walking pace of 2.4 Hz elicits a brain response at the frequency of walking and that this response disentangles biological motion perception from the perception of local motion or other lower-level features of the stimuli (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>). The walkers (2 male, 2 female) were created with the online <underline>BMLStimuli</underline> tool (<xref ref-type="bibr" rid="R58">Troje, 2002</xref>, <xref ref-type="bibr" rid="R59">2008</xref>) and were organized symmetrically around a salmon-colored fixation cross (see <xref ref-type="fig" rid="F1">Figure 1</xref>). The locations of the four walkers in the display were counterbalanced across participants, with the restriction that neighboring figures on both sides were never of the same sex to avoid horizontal or vertical grouping based on similarity.</p><p id="P11">Each participant watched 32 videos in total. Each video lasted exactly 124 walker steps (∼52 s), including a 4-step fade-in and a 4-step fade-out period (∼2 s each). Relatively long video durations were used to obtain a high spectral resolution (= 1/video duration). The 32 videos were randomly assigned to one condition of the movement direction (same vs. different) x synchrony (synchronous vs. asynchronous) x orientation (upright vs. inverted) design. Each of these 8 conditions was repeated 4 times. To minimize habituation, the walking direction of the 4 walkers differed between the different repetitions of the same condition.</p><p id="P12">In the <italic>same direction</italic> conditions, the azimuth of the 4 walkers (i.e., the rotation of the walker around the vertical axis) was either -135°, -45°, 45°, or 135°. In the <italic>different direction</italic> conditions, each walker was assigned one of those azimuths, with the restriction that walkers were never oriented toward each other and were never oriented to create an X-like pattern where, expressed in 2D terms, the top-left agent walked to the top-left corner, the top right agent to the top right corner, etc. This was done to prevent that a clear structure emerged from the way in which the walkers moved. Fixed azimuths were used to prevent accidental facingness or other social constellations (e.g., walking toward another walker) in the <italic>different direction</italic> condition. Importantly, to ensure that the azimuths were associated with unambiguous percepts, point-light stimuli were rendered using perspective projection (<xref ref-type="bibr" rid="R61">Vanrie et al., 2004</xref>).</p><p id="P13">In the <italic>synchrony</italic> conditions, all walkers started from the same position in the walking cycle. In the <italic>asynchrony</italic> conditions, all walkers started from different (random) positions in the walking cycle, which were at least 10 frames apart from each other. The start positions stayed the same across the experiment but differed across participants. All above stimuli were presented both upright and inverted. To maintain attention and control eye gaze, participants were instructed to look at the fixation cross around which the walkers were organized and to press the space bar every time it briefly (400 ms) turned red. This happened 2 to 4 times per video, at random time points. Accuracy of participants on this task was high and stable across the eight conditions (93–95%).</p></sec><sec id="S7"><title>Rating Task</title><p id="P14">At the end of the experiment, participants performed a rating task in which they saw shortened videos (10s) of the same stimuli and had to answer the following questions in random order after each video: (i) Did the figures move in the same direction? (ii) How synchronous did the figures move?, (iii) How pleasant did you find the video to look at?, (iv) How complex did you find the video? This rating task had two purposes. First, it was included to test if participants could detect whether the walkers moved in or out of synchrony and whether they moved in the same or in different directions. Second, it was included as a replication of <xref ref-type="bibr" rid="R16">Cracco, Lee, et al. (2022)</xref>, who found that synchronous videos were perceived as less complex and more aesthetically pleasing than non-synchronous videos. In contrast to the main task, each condition of the direction x synchrony x orientation design was shown only once in the rating task. The direction question was answered by clicking a ‘yes’ or ‘no’ button. The other three questions were answered by moving a cursor along a scale from 0 to 100.</p></sec><sec id="S8"><title>EEG Preprocessing</title><p id="P15">EEG was recorded using an ActiCHamp amplifier and BrainVisionRecorder software (version 1.21.0402, Brain Products, Gilching, Germany) at a sampling rate of 1000 Hz with 64 active Ag/AgCI electrodes. Electrode positions were based on the 10%-system, except that TP9 and TP10 were replaced with OI1h and OI2h according to the 5%-system to better cover posterior scalp sites. Fpz was used as the ground electrode and Fz as the online reference. FT9 and FT10 were used to record horizontal eye movements and two Ag/AgCI sintered ring electrodes placed above and below the left eye to record vertical eye movements.</p><p id="P16">The EEG signal was processed offline in Letswave 6 by band-pass filtering the raw data between 0.1 Hz and 100 Hz, segmenting the filtered data into 8 experimental conditions, and applying ICA (RUNICA algorithm, square mixing matrix) to the merged segmented data. The first 10 components were inspected, and those capturing eye blinks or horizontal eye movements were removed. Faulty or excessively noisy electrodes were interpolated from the 3 closest neighbors (1% on average, never more than 10%). Fz was then reinserted, and the data were re-referenced to the average signal across all electrodes. To ensure that the epoch length was a multiple of the presentation rate, epochs were cropped from the end of the fade-in to the start of the fade-out period. Finally, conditions were averaged, and the discrete Fourier transform of the signal was computed using a fast Fourier transform algorithm, resulting in normalized amplitudes (μV) in the frequency domain with a bin size of ∼0.02 Hz (∼1/48.33).</p></sec><sec id="S9"><title>Data Analysis</title><p id="P17">As frequency tagging elicits responses not only at the tagged frequency (2.4 Hz) but also at harmonics of that frequency (4.8 Hz, 7.2 Hz, …), we quantified the brain response as the sum of the baseline-subtracted amplitudes across all relevant harmonics (for further justification, see <xref ref-type="bibr" rid="R49">Retter et al., 2021</xref>; <xref ref-type="bibr" rid="R48">Retter &amp; Rossion, 2016</xref>). As preregistered, and following <xref ref-type="bibr" rid="R18">Cracco, Oomen, et al. (2022)</xref>, we identified the relevant harmonics by (i) calculating the grand-averaged amplitudes across participants, conditions, and electrodes, (ii) transforming the amplitudes at each frequency bin into z-scores, using as baseline the 10 surrounding bins on each side, excluding directly adjacent bins, and (iii) identifying the harmonics with a z-score &gt; 2.32 (i.e., p &lt; .01). This revealed 2 relevant harmonics, at 2.4 Hz and 4.8 Hz. For these two harmonics, we then calculated baseline-subtracted amplitudes using the same baseline as before and summed them to quantify the brain response (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p><p id="P18">In addition to a response at 2.4 Hz, we also preregistered to explore the presence of a response at 1.2 Hz, which is the frequency at which each individual dot repeats its trajectory. In a previous study, we had found responses at this frequency, but only for scrambled walkers (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>). In line with this finding, amplitudes at 1.2 Hz (and harmonics) did not exceed the predefined z-score threshold of 2.32. As a result, these frequencies were not considered further.</p><p id="P19">To select the electrodes to include into the analysis, we used a collapsed localizer approach (<xref ref-type="bibr" rid="R33">Luck &amp; Gaspelin, 2017</xref>). More precisely, we averaged the summed baseline-subtracted amplitudes across participants and conditions and chose the electrodes to include from the collapsed topography. This topography revealed activity over occipital and parieto-occipital electrodes. Based on this, we decided to include three electrode clusters in the analysis: left posterior (PO7, PO3, O1), middle posterior (OI1h, Oz, OI2h), and right posterior (O2, PO4, PO8). More specifically, we analyzed the brain response with a cluster (left vs. middle vs. right) x movement direction (same vs. different) x synchrony (synchronous vs. asynchronous) x orientation (upright vs. inverted) repeated measures ANOVA. In addition, as a preregistered exploratory analysis, we also correlated the brain response with the collected AQ scores. The rating data were analyzed using separate movement direction (same vs. different) x synchrony (synchronous vs. asynchronous) x orientation (upright vs. inverted) repeated measures ANOVAs for the four ratings (synchrony, direction, complexity, and liking).</p></sec></sec><sec id="S10"><title>Experiment 2</title><sec id="S11" sec-type="subjects"><title>Participants</title><p id="P20">Following our preregistration, we collected a new sample of 33 volunteers using the same procedure as in Experiment 1. However, one participant had to be excluded due to bad data quality (&gt; 10% of electrodes requiring interpolation), resulting in a final sample of 32 participants (27 female, 5 male, <italic>M</italic><sub>age</sub> = 22.16, <italic>range</italic><sub>age</sub> = 17-32).</p></sec><sec id="S12"><title>Task, Stimuli, and Procedure</title><p id="P21">All procedures were identical to Experiment 1, except that Experiment 2 only included videos where the walkers moved synchronously and no longer included the rating task at the end. The internal consistency of the AQ was good (α = 0.92). Accuracy of participants on the attention check was 97–98% in all conditions.</p></sec><sec id="S13"><title>EEG Preprocessing and Data Analysis</title><p id="P22">The preprocessing and analysis pipelines were identical to Experiment 1. In line with Experiment 1, only the first two harmonics (2.4 and 4.8 Hz) reached our predefined z-score cut-off (<xref ref-type="fig" rid="F3">Figure 3</xref>) and the baseline-subtracted amplitudes at these two frequencies were summed to quantify the brain response. The collapsed localizer of Experiment 2 revealed only a right posterior cluster. Though inconsistent with the collapsed topography of Experiment 1, such a right-lateralized scalp pattern matches the topography of the synchrony conditions in Experiment 1. For consistency, we therefore included not just the right posterior cluster (O2, PO4, PO8), but also the left (PO7, PO3, O1) and middle posterior (OI1h, Oz, OI2h) clusters in the analysis of Experiment 2. The resulting brain responses were analyzed with a cluster (left vs. middle vs. right) x movement direction (same vs. different) x orientation (upright vs. inverted) repeated measures ANOVA.</p></sec></sec></sec><sec id="S14" sec-type="results"><title>Results</title><sec id="S15"><title>Rating Task</title><p id="P23">Due to an oversight, the rating task was not administered to one participant. Hence, analyses were conducted on 30 out of 31 participants in Experiment 1. The rating task had two aims. First, we sought to confirm that our participants could differentiate between walkers moving in or out of synchrony and between walkers moving in the same or in different directions. Second, we sought to replicate a previous finding that groups moving in synchrony are perceived as less complex and as more aesthetically pleasing (<xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>). As such, we will focus on the contrasts answering those questions. However, a full overview of all rating results can be found on the OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/67sn8/">https://osf.io/67sn8/</ext-link>).</p><p id="P24">With respect to our first aim, we found significant effects of synchrony on the synchrony ratings, <italic>F</italic>(1, 29) = 41.91, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = 0.59, and of movement direction on the movement direction ratings, <italic>F</italic>(1, 29) = 333.39, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = 0.92. The synchrony effect indicated that participants rated the walkers as more synchronous when they moved in synchrony (<italic>M</italic> = 75, <italic>SD</italic> = 20) than out of synchrony (<italic>M</italic> = 49, <italic>SD</italic> = 18). The movement direction effect indicated that participants were more likely to rate the walkers as going in the same direction when they indeed did so (<italic>M</italic> = 91%, <italic>SD</italic> = 15%), as compared to when they did not (<italic>M</italic> = 17%, <italic>SD</italic> = 19%).</p><p id="P25">With respect to our second aim, we found that both complexity ratings, <italic>F</italic>(1, 29) = 16.74, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = 0.37, and liking ratings, <italic>F</italic>(1, 29) = 34.58, <italic>p</italic> &lt; .001, η<sup>2</sup><sub>p</sub> = 0.54, were influenced by synchrony. Whereas complexity was rated lower when the walkers moved in sync (<italic>M</italic> = 37, <italic>SD</italic> = 14) then when they moved out of sync (<italic>M</italic> = 49, <italic>SD</italic> = 16), liking was rated higher for synchronous (<italic>M</italic> = 63, <italic>SD</italic> = 16) than for asynchronous walkers (M = 46, SD = 15).</p></sec><sec id="S16"><title>Effects of Synchrony and Common Fate on Movement Processing</title><p id="P26">To investigate the effects of synchrony and common fate on the processing of biological group movements, Experiment 1 investigated the influence of both variables on the EEG response coupled to the walking frequency of the four point-light agents. As shown in <xref ref-type="fig" rid="F4">Figure 4</xref>, this revealed a main effect of synchrony, <italic>F</italic>(1, 30) = 8.86, <italic>p</italic> = .006, η<sup>2</sup><sub>p</sub> = 0.23, with larger amplitudes for synchronous than for asynchronous walkers, but no effect of movement direction, <italic>F</italic>(1, 30) = 3.07, <italic>p</italic> = .090, η<sup>2</sup><sub>p</sub> = 0.09. Though not statistically significant, the effect of movement direction was opposite to what we predicted, with slightly larger amplitudes when the walkers moved in different directions than when they moved in the same direction. In addition to the main effect of synchrony, we also found a cluster x synchrony interaction, <italic>F</italic>(2, 29) = 7.41, <italic>p</italic> = .003, η<sup>2</sup> <sub>p</sub> = 0.34, and a synchrony x orientation interaction, <italic>F</italic>(1, 30) = 9.88, <italic>p</italic> = .004, η<sup>2</sup><sub>p</sub> = 0.25. The cluster x synchrony interaction indicated that there was a synchrony effect in the middle, <italic>t</italic>(30) = 2.98, <italic>p</italic> = .006, <italic>d</italic><sub>z</sub> = 0.54, and right clusters, t(30) = 3.79, p &lt; .001, dz = 0.68, but not in the left cluster, <italic>t</italic>(30) = 1.08, <italic>p</italic> = .291, <italic>d</italic><sub>z</sub> = 0.19. The synchrony x orientation interaction indicated that there was a synchrony effect for upright, <italic>t</italic>(30) = 4.57, <italic>p</italic> &lt; .001, <italic>d</italic><sub>z</sub> = 0.82, but not for inverted walkers, <italic>t</italic>(30) = 0.48, <italic>p</italic> = .637, <italic>d</italic><sub>z</sub> = 0.09. None of the other effects reached significance, all <italic>ps</italic> ≥ .071. Finally, exploratory correlations with the AQ total scores revealed that neither the main effect of synchrony, <italic>r</italic> = .12, <italic>p</italic> = .507, nor the synchrony x orientation interaction, <italic>r</italic> = .25, <italic>p</italic> = .170, correlated significantly with autistic traits.</p><p id="P27">These results show that synchrony influenced the neural processing of biological group movements, while common fate did not. Importantly, this is true even though the results of the rating task showed that participants could reliably detect whether the walkers moved in the same or different directions. Our results therefore suggest that the movement synchrony but not common movement direction is used to organize individuals in groups. However, given that it is still unclear whether and how the brain integrates Gestalt cues (<xref ref-type="bibr" rid="R44">Peterson &amp; Kimchi, 2013</xref>), an alternative explanation could be that differences in synchrony take precedence over differences in common fate when organizing stimuli into groups. In support of this idea, we have previously found that image sequences eliciting fluent (i.e., spatiotemporally consistent) movement yield stronger brain responses than those eliciting non-fluent movement when there is only one agent (<xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>) or four agents who always move in synchrony (<xref ref-type="bibr" rid="R17">Cracco et al., 2023</xref>), but not when movement fluency is manipulated together with synchrony (<xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>). It is also consistent with evidence that individuals moving in the same direction are rated high on social cohesion, but not when they move asynchronously (<xref ref-type="bibr" rid="R68">Wilson &amp; Gos, 2019</xref>). In Experiment 2, we therefore investigated whether an effect of common fate would emerge when synchrony was kept constant.</p></sec><sec id="S17"><title>Effect of Common Fate on Movement Processing When Synchrony is Constant</title><p id="P28">Experiment 2 tested whether common fate would influence the processing of biological group movements when the walkers always moved in synchrony. As shown in <xref ref-type="fig" rid="F5">Figure 5</xref>, this was not the case: movement direction did not influence the EEG response coupled to the walking pace of the four point-light agents, <italic>F</italic>(1, 31) = 2.99, <italic>p</italic> = .094, η<sup>2</sup><sub>p</sub> = 0.09. If anything, the effect again went in the opposite direction relative to what we predicted, with a stronger response to agents moving in different directions <italic>vs</italic>. in the same direction. The effect of cluster was non-significant, <italic>F</italic>(2, 30) = 2.63, <italic>p</italic> = .089, η<sup>2</sup><sub>p</sub> = 0.15, but showed a trend for a right lateralized effect with larger response amplitudes in the right cluster than in the middle cluster and larger amplitudes in the middle cluster than in the left cluster. None of the remaining main effects or interactions were significant, all <italic>ps</italic> ≥ .180. Exploratory correlations with the AQ total scores revealed that neither the main effect of movement direction, <italic>r</italic> = .02, <italic>p</italic> = .933, nor the movement direction x orientation interaction, <italic>r</italic> = .07, <italic>p</italic> = .710, correlated with autistic traits. These results eliminate the possibility that the absence of a common fate effect in Experiment 1 was due to differences in synchrony taking precedence over differences in common fate. Instead, they suggest that only synchrony contributes to social grouping.</p></sec></sec><sec id="S18" sec-type="discussion"><title>Discussion</title><p id="P29">Observers are able to efficiently extract summary information from groups of people (e.g., <xref ref-type="bibr" rid="R35">Nguyen et al., 2021</xref>; <xref ref-type="bibr" rid="R54">Sweeny et al., 2013</xref>; <xref ref-type="bibr" rid="R55">Sweeny &amp; Whitney, 2014</xref>; <xref ref-type="bibr" rid="R67">Whitney &amp; Leib, 2018</xref>). However, not much is known about the cues that determine whether multiple individuals are represented as independent agents or as a group. In particular, even though motion is essential to social perception (<xref ref-type="bibr" rid="R45">Pitcher &amp; Ungerleider, 2021</xref>), most research so far has studied static cues contributing to social grouping (e.g., facing each other; <xref ref-type="bibr" rid="R39">Papeo, 2020</xref>). Here, we studied the role of dynamic cues instead. Inspired by Gestalt perception, we investigated in two experiments whether two well-known Gestalt principles – synchrony and common fate – influence the processing of four point-light figures walking in proximity to each other. As a marker of social grouping, we analyzed the extent to which a frequency-tagged EEG response, previously associated with the holistic perception of biological motion (<xref ref-type="bibr" rid="R16">Cracco, Oomen, et al., 2022</xref>), was facilitated when the walkers’ movements could be organized into a fluent, unified percept according to synchrony and/or common fate. The results revealed that brain responses coupled to the walkers’ movements were facilitated when the walkers moved in synchrony, but only when they were shown upright (Experiment 1). In contrast, common fate did not reliably alter the brain response, neither when it was manipulated together with synchrony (Experiment 1), nor when it was manipulated alone (Experiment 2).</p><sec id="S19"><title>The Role of Synchrony in Biological Motion Perception</title><p id="P30">Synchrony between the movements of upright walkers resulted in stronger brain responses. The finding that brain activity was enhanced is consistent with evidence that brain activity in visual brain areas is increased when objects (<xref ref-type="bibr" rid="R29">Kim &amp; Biederman, 2011</xref>; <xref ref-type="bibr" rid="R51">Roberts &amp; Humphreys, 2010</xref>) or bodies (<xref ref-type="bibr" rid="R1">Abassi &amp; Papeo, 2020</xref>, <xref ref-type="bibr" rid="R2">2022</xref>) can be represented as a single unit. The finding that this effect was restricted to upright walkers further indicates that it was driven by global, not local synchrony. Indeed, when using the same task with a single agent, we found that inversion specifically disrupted global processing of the walker’s movements, while leaving processing of the local dot trajectories untouched (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>). Together, our results thus indicate that synchrony influenced grouping at the level of the social agents.</p><p id="P31">These results add onto a burgeoning literature investigating synchrony in social perception (<xref ref-type="bibr" rid="R4">Alp et al., 2017</xref>; <xref ref-type="bibr" rid="R10">Cheng et al., 2022</xref>; <xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>; <xref ref-type="bibr" rid="R60">Tsantani et al., 2022</xref>). In a functional MRI study, <xref ref-type="bibr" rid="R60">Tsantani et al. (2022)</xref> showed that synchrony between the head movements of two agents could be decoded in brain areas of the social perception network, such as the posterior superior temporal cortex and the extrastriate body area. However, this study did not include a basic motion control, leaving open the question of whether synchrony influences social perception or motion perception more generally. Using EEG frequency-tagging, <xref ref-type="bibr" rid="R4">Alp et al. (2017)</xref> and <xref ref-type="bibr" rid="R11">Cracco et al. (2022)</xref> did include such a control in the form of inverted stimuli, a condition known to disrupt body representation (<xref ref-type="bibr" rid="R46">Reed et al., 2003</xref>). However, in contrast to the current study, where synchrony only affected perception of upright stimuli, those previous studies found independent effects of synchrony and inversion: stronger brain responses for upright vs. inverted stimuli and for synchronous vs. asynchronous stimuli.</p><p id="P32">The differences between previous work and the current study may be related to the measured brain response. Like the present work, both <xref ref-type="bibr" rid="R4">Alp et al. (2017)</xref> and <xref ref-type="bibr" rid="R11">Cracco et al. (2022)</xref> used frequency tagging to measure the response to biological motion perception. However, <xref ref-type="bibr" rid="R4">Alp et al. (2017)</xref> modulated the contrast of four point-light agents dancing in or out of synchrony and measured the brain responses coupled to the frequency of contrast modulation. As this frequency was independent of the movements performed by the agents, this means that they did not tag the movements themselves, but rather the visual appearance of the agents performing those movements. <xref ref-type="bibr" rid="R11">Cracco et al. (2022)</xref> did tag the movements themselves, but measured the top-down reconstruction of those movements from static input, rather than their bottom-up processing from motion kinematics. To be more precise, <xref ref-type="bibr" rid="R11">Cracco et al. (2022)</xref> did not show dynamic stimuli but instead showed sequences of static images. Within a range of inter-stimulus-intervals, such sequences can elicit an apparent movement percept in the absence of retinal motion (e.g., <xref ref-type="bibr" rid="R23">Grosjean et al., 2009</xref>; <xref ref-type="bibr" rid="R36">Orgs et al., 2011</xref>; <xref ref-type="bibr" rid="R52">Shiffrar &amp; Freyd, 1990</xref>). However, this type of motion perception is thought to rely on different neural processes (<xref ref-type="bibr" rid="R22">Giese &amp; Poggio, 2003</xref>) and recruits a different brain network than the network involved in perceiving biological motion from kinematics (<xref ref-type="bibr" rid="R37">Orgs et al., 2016</xref>; <xref ref-type="bibr" rid="R53">Stevens et al., 2006</xref>).</p><p id="P33">In line with the fact that synchrony describes the dynamic relationship between two or more movement paths, a comparison of the current work with earlier research thus indicates that synchrony may specifically interact with the processing of biological movements from motion kinematics, rather than the processing of the agent performing those movements (<xref ref-type="bibr" rid="R4">Alp et al., 2017</xref>) or the top-down reconstruction of movements from static input (<xref ref-type="bibr" rid="R16">Cracco, Lee, et al., 2022</xref>).</p></sec><sec id="S20"><title>The Role of Common Fate in Social Grouping</title><p id="P34">In contrast to synchrony, we found no evidence that common fate, another well-known Gestalt principle, influenced biological motion perception. There are several possible explanations for this finding. First, it could be that participants had difficulties to distinguish between walkers moving in the same vs. different directions. However, this seems unlikely based on the rating task results: participants could clearly differentiate between walkers based on whether they moved in one or in multiple directions. Importantly, while error rates were relatively high when the walkers moved in multiple directions (17%), a closer look at the data split up for the different conditions revealed that this was entirely driven by errors in the conditions where the walkers were inverted (34%), whereas not a single error was made when they were shown upright (<ext-link ext-link-type="uri" xlink:href="https://osf.io/67sn8/">https://osf.io/67sn8/</ext-link>). This is consistent with previous work showing that observers are highly accurate at differentiating between the walking directions of point-light figures, at least when perspective projection is used, like in the current study (<xref ref-type="bibr" rid="R61">Vanrie et al., 2004</xref>).</p><p id="P35">Another possible reason for why synchrony but not common fate influenced the brain response is that they operated at different levels. Indeed, albeit non-significant, both experiments found slightly <italic>stronger</italic> responses when the walkers moved in different directions than when they moved in the same direction. Though speculative, one interpretation for this pattern could be that common fate influences the processing of the individual dot movements (i.e. local processing of biological motion) and therefore that seeing multiple walkers move in the same direction increases focus on the individual dots while decreasing focus on the point-light agents (i.e., global processing of biological motion). This is consistent with evidence that motion coherency in random dot motion paradigms influences brain activity in brain areas typically associated with local processing of biological motion (e.g., V5/MT; <xref ref-type="bibr" rid="R8">Braddick et al., 2001</xref>; <xref ref-type="bibr" rid="R47">Rees et al., 2000</xref>; <xref ref-type="bibr" rid="R50">Rina et al., 2022</xref>), whereas the brain response measured here has been shown to specifically capture global processing of biological motion (<xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>).</p><p id="P36">The absence of a strong common effect could also be related to the method. More specifically, it is possible that EEG frequency tagging is inherently more sensitive to effects of synchrony. Indeed, whereas the degree of synchrony between brain waves directly influences the amplitude of the combined brain response, there is no such direct connection for common fate. As such, it is possible that the imbalance between synchrony and common fate here was caused by the method and that other methods may find an influence of common fate on social grouping. An interesting technique in this respect would be functional MRI, which has been successfully used to demonstrate effects of common fate on the processing of more basic, non-social stimuli (e.g., <xref ref-type="bibr" rid="R8">Braddick et al., 2001</xref>; <xref ref-type="bibr" rid="R47">Rees et al., 2000</xref>; <xref ref-type="bibr" rid="R50">Rina et al., 2022</xref>).</p><p id="P37">A fourth possible reason for the absence of a common fate effect is that common fate may be less relevant for social perception and for that reason did not influence the brain response. While the social relevance of synchrony has long been acknowledged, based on the fact that it is a common factor to many social interactions (<xref ref-type="bibr" rid="R56">Thurman &amp; Lu, 2014</xref>) and a consistent indicator of social cohesion (<xref ref-type="bibr" rid="R30">Lakens, 2010</xref>; <xref ref-type="bibr" rid="R31">Lakens &amp; Stel, 2011</xref>; <xref ref-type="bibr" rid="R68">Wilson &amp; Gos, 2019</xref>), the social function of common fate is less clear. On the one hand, one could argue that a common movement direction is indicative of a common goal and therefore of social cohesion. On the other hand, there is currently little evidence that common fate in and of itself influences social perception. To our knowledge, there is only one study so far that has investigated whether observers ascribe different social dynamics to groups moving in the same vs. different directions (<xref ref-type="bibr" rid="R69">Wilson &amp; Mansour, 2020</xref>). However, ‘collective movement’ in that study was operationalized as moving synchronously in the same direction, in close proximity of each other. In other words, collective movement in Wilson &amp; Manour (2020) combined common fate with synchrony and proximity. Moreover, when participants were presented with two dots that always moved in the same direction, but did so asynchronously and without maintaining close distance, social cohesion was rated only 2.36 out of 7 (<xref ref-type="bibr" rid="R69">Wilson &amp; Mansour, 2020</xref>). While more research is needed, this suggests a potential imbalance between synchrony and common fate, with the former but not the latter having an important social function.</p><p id="P38">A directional cue that is potentially more relevant for social grouping than common fate is whether individuals move towards or away from each other. In line with this idea, recent work has shown that people facing towards each other are detected more easily (<xref ref-type="bibr" rid="R41">Papeo et al., 2017</xref>, <xref ref-type="bibr" rid="R40">2019</xref>; <xref ref-type="bibr" rid="R64">Vestner et al., 2019</xref>) and remembered better (<xref ref-type="bibr" rid="R38">Paparella &amp; Papeo, 2022</xref>; <xref ref-type="bibr" rid="R64">Vestner et al., 2019</xref>) than people facing away from each other. In line with the synchrony effects found here, these effects were sensitive to inversion (<xref ref-type="bibr" rid="R41">Papeo et al., 2017</xref>, <xref ref-type="bibr" rid="R40">2019</xref>; <xref ref-type="bibr" rid="R64">Vestner et al., 2019</xref>) and were visible in the brain as increased integrative response (<xref ref-type="bibr" rid="R3">Adibpour et al., 2021</xref>) and enhanced activity in body- (<xref ref-type="bibr" rid="R1">Abassi &amp; Papeo, 2020</xref>) and movement-related brain areas (<xref ref-type="bibr" rid="R5">Bellot et al., 2021</xref>). Although there is an ongoing discussion as to whether these effects are social in nature (<xref ref-type="bibr" rid="R39">Papeo, 2020</xref>) or reflect a more general attentional mechanism (<xref ref-type="bibr" rid="R62">Vestner et al., 2020</xref>, <xref ref-type="bibr" rid="R63">2021</xref>), they suggest that organizing people into groups may depend not on whether they move in the same or in different directions, but on whether they move towards or away from each other.</p></sec></sec><sec id="S21" sec-type="conclusions"><title>Conclusion</title><p id="P39">The current study shows that social grouping is driven not only by static (e.g., facing each other; <xref ref-type="bibr" rid="R39">Papeo, 2020</xref>), but also by dynamic cues. Inspired by Gestalt perception (<xref ref-type="bibr" rid="R65">Wagemans, Elder, et al., 2012</xref>; <xref ref-type="bibr" rid="R66">Wagemans, Feldman, et al., 2012</xref>), we investigated the role of synchrony and common fate. The results revealed that synchrony influenced the perception of biological group movements, whereas common fate did not. An important question for future research will be to investigate whether such effects can be observed with other methods and/or stimuli, or that there is a more fundamental disparity between synchrony and common fate, perhaps related to differences in the extent to which they signal social cohesion or social interaction.</p></sec></body><back><ack id="S22"><title>Acknowledgements</title><p>This work was funded by a senior postdoctoral fellowship awarded to EC by the Research Foundation Flanders (12U0322N). LP was supported by a European Research Council Grant (Project THEMPO-758473).</p></ack><sec id="S23" sec-type="data-availability"><title>Data availability statement</title><p id="P40">The stimuli, data files, analysis script, and experimental program are available on the OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/67sn8/">https://osf.io/67sn8/</ext-link>).</p></sec><fn-group><fn fn-type="conflict" id="FN1"><p id="P41"><bold>Conflicts of interest disclosure:</bold> The authors declare no conflicts of interest.</p></fn><fn id="FN2"><p id="P42"><bold>Ethics statement:</bold> The study was approved by the local ethics board of the Faculty of Psychological and Educational Sciences at Ghent University (2021/129).</p></fn><fn id="FN3"><label>1</label><p id="P43">Note that this was an arbitrary threshold. However, it was preregistered and in line our previous work (<xref ref-type="bibr" rid="R17">Cracco et al., 2023</xref>; <xref ref-type="bibr" rid="R18">Cracco, Oomen, et al., 2022</xref>).</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>The Representation of Two-Body Shapes in the Human Visual Cortex</article-title><source>The Journal of Neuroscience</source><year>2020</year><volume>40</volume><issue>4</issue><fpage>852</fpage><lpage>863</lpage><pub-id pub-id-type="pmcid">PMC6975292</pub-id><pub-id pub-id-type="pmid">31801812</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1378-19.2019</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Behavioral and neural markers of visual configural processing in social scene perception</article-title><source>NeuroImage</source><year>2022</year><volume>260</volume><elocation-id>119506</elocation-id><pub-id pub-id-type="pmid">35878724</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adibpour</surname><given-names>P</given-names></name><name><surname>Hochmann</surname><given-names>J-R</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Spatial Relations Trigger Visual Binding of People</article-title><source>Journal of Cognitive Neuroscience</source><year>2021</year><volume>33</volume><issue>7</issue><fpage>1343</fpage><lpage>1353</lpage><pub-id pub-id-type="pmid">34496405</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alp</surname><given-names>N</given-names></name><name><surname>Nikolaev</surname><given-names>AR</given-names></name><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Kogo</surname><given-names>N</given-names></name></person-group><article-title>EEG frequency tagging dissociates between neural processing of motion synchrony and human quality of multiple point-light dancers</article-title><source>Scientific Reports</source><year>2017</year><volume>7</volume><comment>(August 2016), Article August 2016</comment><pub-id pub-id-type="pmcid">PMC5341056</pub-id><pub-id pub-id-type="pmid">28272421</pub-id><pub-id pub-id-type="doi">10.1038/srep44012</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellot</surname><given-names>E</given-names></name><name><surname>Abassi</surname><given-names>E</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Moving Toward versus Away from Another: How Body Motion Direction Changes the Representation of Bodies and Actions in the Visual Cortex</article-title><source>Cerebral Cortex</source><year>2021</year><volume>31</volume><issue>5</issue><fpage>2670</fpage><lpage>2685</lpage><pub-id pub-id-type="pmid">33401307</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertenthal</surname><given-names>BI</given-names></name><name><surname>Pinto</surname><given-names>J</given-names></name></person-group><article-title>Global Processing of Biological Motions</article-title><source>Psychological Science</source><year>1994</year><volume>5</volume><issue>4</issue><comment>Article 4</comment><pub-id pub-id-type="doi">10.1111/j.1467-9280.1994.tb00504.x</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>R</given-names></name><name><surname>Shiffrar</surname><given-names>M</given-names></name></person-group><article-title>Perception of human motion</article-title><source>Annual Review of Psychology</source><year>2007</year><volume>58</volume><fpage>47</fpage><lpage>73</lpage><pub-id pub-id-type="pmid">16903802</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braddick</surname><given-names>OJ</given-names></name><name><surname>O’Brien</surname><given-names>JMD</given-names></name><name><surname>Wattam-Bell</surname><given-names>J</given-names></name><name><surname>Atkinson</surname><given-names>J</given-names></name><name><surname>Hartley</surname><given-names>T</given-names></name><name><surname>Turner</surname><given-names>R</given-names></name></person-group><article-title>Brain Areas Sensitive to Coherent Visual Motion</article-title><source>Perception</source><year>2001</year><volume>30</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmid">11257978</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caspers</surname><given-names>S</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name></person-group><article-title>ALE meta-analysis of action observation and imitation in the human brain</article-title><source>NeuroImage</source><year>2010</year><volume>50</volume><issue>3</issue><fpage>1148</fpage><lpage>1167</lpage><pub-id pub-id-type="pmcid">PMC4981639</pub-id><pub-id pub-id-type="pmid">20056149</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.12.112</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Jiang</surname><given-names>Y</given-names></name></person-group><article-title>Following Other People’s Footsteps: A Contextual-Attraction Effect Induced by Biological Motion</article-title><source>Psychological Science</source><year>2022</year><volume>33</volume><issue>9</issue><fpage>1522</fpage><lpage>1531</lpage><pub-id pub-id-type="pmid">35985032</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Bernardet</surname><given-names>U</given-names></name><name><surname>Sevenhant</surname><given-names>R</given-names></name><name><surname>Vandenhouwe</surname><given-names>N</given-names></name><name><surname>Copman</surname><given-names>F</given-names></name><name><surname>Durnez</surname><given-names>W</given-names></name><name><surname>Bombeke</surname><given-names>K</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>Evidence for a two-step model of social group influence</article-title><source>iScience</source><year>2022</year><elocation-id>104891</elocation-id><pub-id pub-id-type="pmcid">PMC9424596</pub-id><pub-id pub-id-type="pmid">36051185</pub-id><pub-id pub-id-type="doi">10.1016/j.isci.2022.104891</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>The role of sensorimotor processes in social group contagion</article-title><source>Cognitive Psychology</source><year>2018</year><volume>103</volume><fpage>23</fpage><lpage>41</lpage><pub-id pub-id-type="pmid">29501776</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>De Coster</surname><given-names>L</given-names></name><name><surname>Andres</surname><given-names>M</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>Motor simulation beyond the dyad: Automatic imitation of multiple actors</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2015</year><volume>41</volume><issue>6</issue><comment>Article 6</comment><pub-id pub-id-type="pmid">26389616</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>De Coster</surname><given-names>L</given-names></name><name><surname>Andres</surname><given-names>M</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>Mirroring multiple agents: Motor resonance during action observation is modulated by the number of agents</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2016</year><volume>11</volume><issue>9</issue><comment>Article 9</comment><pub-id pub-id-type="pmcid">PMC5015808</pub-id><pub-id pub-id-type="pmid">27118879</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsw059</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Keysers</surname><given-names>C</given-names></name><name><surname>Clauwaert</surname><given-names>A</given-names></name><name><surname>Brass</surname><given-names>M</given-names></name></person-group><article-title>Representing Multiple Observed Actions in the Motor System</article-title><source>Cerebral Cortex</source><year>2019</year><volume>29</volume><issue>8</issue><comment>Article 8</comment><pub-id pub-id-type="pmcid">PMC6949134</pub-id><pub-id pub-id-type="pmid">30295709</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhy237</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>van Belle</surname><given-names>G</given-names></name><name><surname>Quenon</surname><given-names>L</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Orgs</surname><given-names>G</given-names></name></person-group><article-title>EEG Frequency Tagging Reveals the Integration of Form and Motion Cues into the Perception of Group Movement</article-title><source>Cerebral Cortex</source><year>2022</year><volume>32</volume><issue>13</issue><fpage>2843</fpage><lpage>2857</lpage><pub-id pub-id-type="pmcid">PMC9247417</pub-id><pub-id pub-id-type="pmid">34734972</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhab385</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Linthout</surname><given-names>T</given-names></name><name><surname>Orgs</surname><given-names>G</given-names></name></person-group><article-title>The Role of Objecthood and Animacy in Apparent Movement Processing</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2023</year><elocation-id>nsad014</elocation-id><pub-id pub-id-type="pmcid">PMC10032357</pub-id><pub-id pub-id-type="pmid">36905406</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsad014</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cracco</surname><given-names>E</given-names></name><name><surname>Oomen</surname><given-names>D</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name><name><surname>Wiersema</surname><given-names>JR</given-names></name></person-group><article-title>Using EEG movement tagging to isolate brain responses coupled to biological movements</article-title><source>Neuropsychologia</source><year>2022</year><volume>177</volume><elocation-id>108395</elocation-id><pub-id pub-id-type="pmid">36272677</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>X</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Shen</surname><given-names>M</given-names></name></person-group><article-title>Two Equals One: Two Human Actions During Social Interaction Are Grouped as One Unit in Working Memory</article-title><source>Psychological Science</source><year>2017</year><volume>28</volume><issue>9</issue><fpage>1311</fpage><lpage>1320</lpage><pub-id pub-id-type="pmid">28719763</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elias</surname><given-names>E</given-names></name><name><surname>Dyer</surname><given-names>M</given-names></name><name><surname>Sweeny</surname><given-names>TD</given-names></name></person-group><article-title>Ensemble Perception of Dynamic Emotional Groups</article-title><source>Psychological Science</source><year>2017</year><volume>28</volume><issue>2</issue><comment>Article 2</comment><pub-id pub-id-type="pmid">28036236</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Federici</surname><given-names>A</given-names></name><name><surname>Parma</surname><given-names>V</given-names></name><name><surname>Vicovaro</surname><given-names>M</given-names></name><name><surname>Radassao</surname><given-names>L</given-names></name><name><surname>Casartelli</surname><given-names>L</given-names></name><name><surname>Ronconi</surname><given-names>L</given-names></name></person-group><article-title>Anomalous Perception of Biological Motion in Autism: A Conceptual Review and Meta-Analysis</article-title><source>Scientific Reports</source><year>2020</year><volume>10</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC7067769</pub-id><pub-id pub-id-type="pmid">32165647</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-61252-3</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giese</surname><given-names>MA</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Neural mechanisms for the recognition of biological movements</article-title><source>Nature Reviews Neuroscience</source><year>2003</year><volume>4</volume><issue>3</issue><comment>Article 3</comment><pub-id pub-id-type="pmid">12612631</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosjean</surname><given-names>M</given-names></name><name><surname>Zwickel</surname><given-names>J</given-names></name><name><surname>Prinz</surname><given-names>W</given-names></name></person-group><article-title>Acting while perceiving: Assimilation precedes contrast</article-title><source>Psychological Research-Psychologische Forschung</source><year>2009</year><volume>73</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC2757607</pub-id><pub-id pub-id-type="pmid">18365250</pub-id><pub-id pub-id-type="doi">10.1007/s00426-008-0146-6</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafri</surname><given-names>A</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name></person-group><article-title>The Perception of Relations</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><volume>25</volume><issue>6</issue><fpage>475</fpage><lpage>492</lpage><pub-id pub-id-type="pmid">33812770</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>S</given-names></name></person-group><article-title>Interactions between proximity and similarity grouping: An event-related brain potential study in humans</article-title><source>Neuroscience Letters</source><year>2004</year><volume>367</volume><issue>1</issue><fpage>40</fpage><lpage>43</lpage><pub-id pub-id-type="pmid">15308293</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoekstra</surname><given-names>RA</given-names></name><name><surname>Bartels</surname><given-names>M</given-names></name><name><surname>Cath</surname><given-names>DC</given-names></name><name><surname>Boomsma</surname><given-names>DI</given-names></name></person-group><article-title>Factor structure, reliability and criterion validity of the Autism-Spectrum Quotient (AQ): A study in Dutch population and patient groups</article-title><source>Journal of Autism and Developmental Disorders</source><year>2008</year><volume>38</volume><issue>8</issue><fpage>1555</fpage><lpage>1566</lpage><pub-id pub-id-type="pmcid">PMC2516538</pub-id><pub-id pub-id-type="pmid">18302013</pub-id><pub-id pub-id-type="doi">10.1007/s10803-008-0538-x</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>H</given-names></name><name><surname>Yin</surname><given-names>J</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Ding</surname><given-names>X</given-names></name></person-group><article-title>Selective attention operates on the group level for interactive biological motion</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2020</year><volume>46</volume><issue>12</issue><fpage>1434</fpage><lpage>1442</lpage><pub-id pub-id-type="pmid">32969687</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Quek</surname><given-names>GL</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Object Vision in a Structured World</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><volume>23</volume><issue>8</issue><comment>Article 8</comment><pub-id pub-id-type="pmcid">PMC7612023</pub-id><pub-id pub-id-type="pmid">31147151</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2019.04.013</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>JG</given-names></name><name><surname>Biederman</surname><given-names>I</given-names></name></person-group><article-title>Where do objects become scenes?</article-title><source>Cerebral Cortex (New York, N Y: 1991)</source><year>2011</year><volume>21</volume><issue>8</issue><fpage>1738</fpage><lpage>1746</lpage><comment>1991</comment><pub-id pub-id-type="pmcid">PMC3138508</pub-id><pub-id pub-id-type="pmid">21148087</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhq240</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><article-title>Movement synchrony and perceived entitativity</article-title><source>Journal of Experimental Social Psychology</source><year>2010</year><volume>46</volume><issue>5</issue><fpage>701</fpage><lpage>708</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2010.03.015</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name><name><surname>Stel</surname><given-names>M</given-names></name></person-group><article-title>If They Move in Sync, They Must Feel in Sync: Movement Synchrony Leads to Attributions of Rapport and Entitativity</article-title><source>Social Cognition</source><year>2011</year><volume>29</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1521/soco.2011.29.1.1</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>S-H</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><article-title>Neural synergy in visual grouping: When good continuation meets common fate</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>16</issue><fpage>2057</fpage><lpage>2064</lpage><pub-id pub-id-type="pmid">11403790</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Gaspelin</surname><given-names>N</given-names></name></person-group><article-title>How to get statistically significant effects in any ERP experiment (and why you shouldn’t</article-title><source>Psychophysiology</source><year>2017</year><volume>54</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC5178877</pub-id><pub-id pub-id-type="pmid">28000253</pub-id><pub-id pub-id-type="doi">10.1111/psyp.12639</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morey</surname><given-names>RD</given-names></name></person-group><article-title>Confidence intervals from normalized data: A correction to Cousineau (2005</article-title><source>Tutorials in Quantitative Methods for Psychology</source><year>2008</year><volume>4</volume><issue>2</issue><comment>Article 2</comment><pub-id pub-id-type="doi">10.3758/s13414-012-0291-2</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>TTN</given-names></name><name><surname>Vuong</surname><given-names>QC</given-names></name><name><surname>Mather</surname><given-names>G</given-names></name><name><surname>Thornton</surname><given-names>IM</given-names></name></person-group><article-title>Ensemble coding of crowd speed using biological motion</article-title><source>Attention, Perception, &amp; Psychophysics</source><year>2021</year><volume>83</volume><issue>3</issue><comment>Article 3</comment><pub-id pub-id-type="pmid">33169330</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orgs</surname><given-names>G</given-names></name><name><surname>Bestmann</surname><given-names>S</given-names></name><name><surname>Schuur</surname><given-names>F</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name></person-group><article-title>From Body Form to Biological Motion: The Apparent Velocity of Human Movement Biases Subjective Time</article-title><source>Psychological Science</source><year>2011</year><volume>22</volume><issue>6</issue><comment>Article 6</comment><pub-id pub-id-type="pmcid">PMC3693441</pub-id><pub-id pub-id-type="pmid">21525378</pub-id><pub-id pub-id-type="doi">10.1177/0956797611406446</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orgs</surname><given-names>G</given-names></name><name><surname>Dovern</surname><given-names>A</given-names></name><name><surname>Hagura</surname><given-names>N</given-names></name><name><surname>Haggard</surname><given-names>P</given-names></name><name><surname>Fink</surname><given-names>GR</given-names></name><name><surname>Weiss</surname><given-names>PH</given-names></name></person-group><article-title>Constructing Visual Perception of Body Movement with the Motor Cortex</article-title><source>Cerebral Cortex</source><year>2016</year><volume>26</volume><issue>1</issue><fpage>440</fpage><lpage>449</lpage><pub-id pub-id-type="pmcid">PMC4677987</pub-id><pub-id pub-id-type="pmid">26534907</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhv262</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paparella</surname><given-names>I</given-names></name><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Chunking by social relationship in working memory</article-title><source>Visual Cognition</source><year>2022</year><volume>30</volume><issue>5</issue><fpage>354</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1080/13506285.2022.2064950</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname><given-names>L</given-names></name></person-group><article-title>Twos in human visual perception</article-title><source>Cortex</source><year>2020</year><volume>132</volume><fpage>473</fpage><lpage>478</lpage><pub-id pub-id-type="pmid">32698947</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname><given-names>L</given-names></name><name><surname>Goupil</surname><given-names>N</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name></person-group><article-title>Visual Search for People Among People</article-title><source>Psychological Science</source><year>2019</year><volume>30</volume><issue>10</issue><fpage>1483</fpage><lpage>1496</lpage><pub-id pub-id-type="pmid">31532709</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papeo</surname><given-names>L</given-names></name><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name></person-group><article-title>The Two-Body Inversion Effect</article-title><source>Psychological Science</source><year>2017</year><volume>28</volume><issue>3</issue><fpage>369</fpage><lpage>379</lpage><pub-id pub-id-type="pmid">28140764</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pavlova</surname><given-names>M</given-names></name><name><surname>Sokolov</surname><given-names>A</given-names></name></person-group><article-title>Orientation specificity in biological motion perception</article-title><source>Perception &amp; Psychophysics</source><year>2000</year><volume>62</volume><issue>5</issue><fpage>889</fpage><lpage>899</lpage><pub-id pub-id-type="pmid">10997036</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><year>2019</year><volume>51</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmcid">PMC6420413</pub-id><pub-id pub-id-type="pmid">30734206</pub-id><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>MA</given-names></name><name><surname>Kimchi</surname><given-names>R</given-names></name></person-group><source>Perceptual Organization in Vision</source><year>2013</year><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780195376746.013.0002</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitcher</surname><given-names>D</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name></person-group><article-title>Evidence for a Third Visual Pathway Specialized for Social Perception</article-title><source>Trends in Cognitive Sciences</source><year>2021</year><volume>25</volume><issue>2</issue><fpage>100</fpage><lpage>110</lpage><pub-id pub-id-type="pmcid">PMC7811363</pub-id><pub-id pub-id-type="pmid">33334693</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2020.11.006</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reed</surname><given-names>CL</given-names></name><name><surname>Stone</surname><given-names>VE</given-names></name><name><surname>Bozova</surname><given-names>S</given-names></name><name><surname>Tanaka</surname><given-names>J</given-names></name></person-group><article-title>The Body-Inversion Effect</article-title><source>Psychological Science</source><year>2003</year><volume>14</volume><issue>4</issue><comment>Article 4</comment><pub-id pub-id-type="pmid">12807401</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name></person-group><article-title>A direct quantitative relationship between the functional properties of human and macaque V5</article-title><source>Nature Neuroscience</source><year>2000</year><volume>3</volume><issue>7</issue><comment>Article 7</comment><pub-id pub-id-type="pmid">10862705</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name></person-group><article-title>Uncovering the neural magnitude and spatio-temporal dynamics of natural image categorization in a fast visual stream</article-title><source>Neuropsychologia</source><year>2016</year><volume>91</volume><fpage>9</fpage><lpage>28</lpage><pub-id pub-id-type="pmid">27461075</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Retter</surname><given-names>TL</given-names></name><name><surname>Rossion</surname><given-names>B</given-names></name><name><surname>Schiltz</surname><given-names>C</given-names></name></person-group><article-title>Harmonic Amplitude Summation for Frequency-tagging Analysis</article-title><source>Journal of Cognitive Neuroscience</source><year>2021</year><volume>33</volume><issue>11</issue><fpage>2372</fpage><lpage>2393</lpage><pub-id pub-id-type="pmid">34272961</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rina</surname><given-names>A</given-names></name><name><surname>Papanikolaou</surname><given-names>A</given-names></name><name><surname>Zong</surname><given-names>X</given-names></name><name><surname>Papageorgiou</surname><given-names>DT</given-names></name><name><surname>Keliris</surname><given-names>GA</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name></person-group><article-title>Visual Motion Coherence Responses in Human Visual Cortex</article-title><source>Frontiers in Neuroscience</source><year>2022</year><volume>16</volume><pub-id pub-id-type="pmcid">PMC8924467</pub-id><pub-id pub-id-type="pmid">35310109</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.719250</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>KL</given-names></name><name><surname>Humphreys</surname><given-names>GW</given-names></name></person-group><article-title>Action relationships concatenate representations of separate objects in the ventral visual system</article-title><source>NeuroImage</source><year>2010</year><volume>52</volume><issue>4</issue><fpage>1541</fpage><lpage>1548</lpage><pub-id pub-id-type="pmid">20580845</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiffrar</surname><given-names>M</given-names></name><name><surname>Freyd</surname><given-names>JJ</given-names></name></person-group><article-title>Apparent Motion of the Human Body</article-title><source>Psychological Science</source><year>1990</year><volume>1</volume><issue>4</issue><comment>Article 4</comment><pub-id pub-id-type="doi">10.1111/j.1467-9280.1990.tb00210.x</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevens</surname><given-names>JA</given-names></name><name><surname>Fonlupt</surname><given-names>P</given-names></name><name><surname>Shiffrar</surname><given-names>M</given-names></name><name><surname>Decety</surname><given-names>J</given-names></name></person-group><article-title>New aspects of motion perception: Selective neural encoding of apparent human movements</article-title><source>NeuroReport</source><year>2006</year><volume>11</volume><issue>1</issue><comment>Article 1</comment><pub-id pub-id-type="pmid">10683840</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweeny</surname><given-names>TD</given-names></name><name><surname>Haroz</surname><given-names>S</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>Perceiving group behavior: Sensitive ensemble coding mechanisms for biological motion of human crowds</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2013</year><volume>39</volume><issue>2</issue><fpage>329</fpage><lpage>337</lpage><pub-id pub-id-type="pmid">22708744</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sweeny</surname><given-names>TD</given-names></name><name><surname>Whitney</surname><given-names>D</given-names></name></person-group><article-title>Perceiving Crowd Attention: Ensemble Perception of a Crowd’s Gaze</article-title><source>Psychological Science</source><year>2014</year><volume>25</volume><issue>10</issue><fpage>1903</fpage><lpage>1913</lpage><pub-id pub-id-type="pmcid">PMC4192023</pub-id><pub-id pub-id-type="pmid">25125428</pub-id><pub-id pub-id-type="doi">10.1177/0956797614544510</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thurman</surname><given-names>SM</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name></person-group><article-title>Perception of Social Interactions for Spatially Scrambled Biological Motion</article-title><source>PLoS ONE</source><year>2014</year><volume>9</volume><issue>11</issue><comment>Article 11</comment><pub-id pub-id-type="pmcid">PMC4236114</pub-id><pub-id pub-id-type="pmid">25406075</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0112539</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todorova</surname><given-names>GK</given-names></name><name><surname>Hatton</surname><given-names>REM</given-names></name><name><surname>Pollick</surname><given-names>FE</given-names></name></person-group><article-title>Biological motion perception in autism spectrum disorder: A meta-analysis</article-title><source>Molecular Autism</source><year>2019</year><volume>10</volume><fpage>49</fpage><pub-id pub-id-type="pmcid">PMC6921539</pub-id><pub-id pub-id-type="pmid">31890147</pub-id><pub-id pub-id-type="doi">10.1186/s13229-019-0299-8</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>NF</given-names></name></person-group><article-title>Decomposing biological motion: A framework for analysis and synthesis of human gait patterns</article-title><source>Journal of Vision</source><year>2002</year><volume>2</volume><issue>5</issue><fpage>2</fpage><pub-id pub-id-type="pmid">12678652</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Troje</surname><given-names>NF</given-names></name></person-group><chapter-title>Retrieving information from human movement patterns</chapter-title><source>Understanding events: From perception to action</source><year>2008</year><publisher-name>Oxford University Press</publisher-name><fpage>308</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195188370.003.0014</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsantani</surname><given-names>M</given-names></name><name><surname>Yon</surname><given-names>D</given-names></name><name><surname>Cook</surname><given-names>R</given-names></name></person-group><article-title>Neural representations of observed interpersonal synchrony in the social perception network</article-title><source>PsyArXiv</source><year>2022</year><pub-id pub-id-type="doi">10.31234/osf.io/pjvke</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanrie</surname><given-names>J</given-names></name><name><surname>Dekeyser</surname><given-names>M</given-names></name><name><surname>Verfaillie</surname><given-names>K</given-names></name></person-group><article-title>Bistability and biasing effects in the perception of ambiguous point-light walkers</article-title><source>Perception</source><year>2004</year><volume>33</volume><issue>5</issue><fpage>547</fpage><lpage>560</lpage><pub-id pub-id-type="pmid">15250660</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vestner</surname><given-names>T</given-names></name><name><surname>Gray</surname><given-names>KLH</given-names></name><name><surname>Cook</surname><given-names>R</given-names></name></person-group><article-title>Why are social interactions found quickly in visual search tasks?</article-title><source>Cognition</source><year>2020</year><volume>200</volume><elocation-id>104270</elocation-id><pub-id pub-id-type="pmcid">PMC7315127</pub-id><pub-id pub-id-type="pmid">32220782</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104270</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vestner</surname><given-names>T</given-names></name><name><surname>Over</surname><given-names>H</given-names></name><name><surname>Gray</surname><given-names>KLH</given-names></name><name><surname>Cook</surname><given-names>R</given-names></name></person-group><article-title>Objects that direct visuospatial attention produce the search advantage for facing dyads</article-title><source>Journal of Experimental Psychology: General</source><year>2021</year><pub-id pub-id-type="pmid">34110891</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vestner</surname><given-names>T</given-names></name><name><surname>Tipper</surname><given-names>SP</given-names></name><name><surname>Hartley</surname><given-names>T</given-names></name><name><surname>Over</surname><given-names>H</given-names></name><name><surname>Rueschemeyer</surname><given-names>S-A</given-names></name></person-group><article-title>Bound Together: Social Binding Leads to Faster Processing, Spatial Distortion, and Enhanced Memory of Interacting Partners</article-title><source>Journal of Experimental Psychology: General</source><year>2019</year><volume>148</volume><issue>7</issue><fpage>1251</fpage><lpage>1268</lpage><pub-id pub-id-type="pmid">30652892</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Elder</surname><given-names>JH</given-names></name><name><surname>Kubovy</surname><given-names>M</given-names></name><name><surname>Palmer</surname><given-names>SE</given-names></name><name><surname>Peterson</surname><given-names>MA</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name><name><surname>von der Heydt</surname><given-names>R</given-names></name></person-group><article-title>A century of Gestalt psychology in visual perception: I. Perceptual grouping and figure–ground organization</article-title><source>Psychological Bulletin</source><year>2012</year><volume>138</volume><issue>6</issue><comment>Article 6</comment><pub-id pub-id-type="pmcid">PMC3482144</pub-id><pub-id pub-id-type="pmid">22845751</pub-id><pub-id pub-id-type="doi">10.1037/a0029333</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagemans</surname><given-names>J</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Gepshtein</surname><given-names>S</given-names></name><name><surname>Kimchi</surname><given-names>R</given-names></name><name><surname>Pomerantz</surname><given-names>JR</given-names></name><name><surname>van der Helm</surname><given-names>PA</given-names></name><name><surname>van Leeuwen</surname><given-names>C</given-names></name></person-group><article-title>A century of Gestalt psychology in visual perception: II. Conceptual and theoretical foundations</article-title><source>Psychological Bulletin</source><year>2012</year><volume>138</volume><issue>6</issue><comment>Article 6</comment><pub-id pub-id-type="pmcid">PMC3728284</pub-id><pub-id pub-id-type="pmid">22845750</pub-id><pub-id pub-id-type="doi">10.1037/a0029334</pub-id></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whitney</surname><given-names>D</given-names></name><name><surname>Leib</surname><given-names>AY</given-names></name></person-group><article-title>Ensemble Perception</article-title><source>Annual Review of Psychology</source><year>2018</year><volume>69</volume><fpage>105</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">28892638</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>S</given-names></name><name><surname>Gos</surname><given-names>C</given-names></name></person-group><article-title>Perceiving Social Cohesion: Movement Synchrony and Task Demands Both Matter</article-title><source>Perception</source><year>2019</year><volume>48</volume><issue>4</issue><fpage>316</fpage><lpage>329</lpage><pub-id pub-id-type="pmid">30871427</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>S</given-names></name><name><surname>Mansour</surname><given-names>JK</given-names></name></person-group><article-title>Collective directional movement and the perception of social cohesion</article-title><source>British Journal of Social Psychology</source><year>2020</year><volume>59</volume><issue>4</issue><fpage>819</fpage><lpage>838</lpage><pub-id pub-id-type="pmcid">PMC7586976</pub-id><pub-id pub-id-type="pmid">31900981</pub-id><pub-id pub-id-type="doi">10.1111/bjso.12361</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Still frames from two example stimuli used in Experiment 1.</title><p>The left example shows four inverted walkers. The right example shows four upright walkers. Example videos of each condition can be found on the Open Science Framework (OSF; <ext-link ext-link-type="uri" xlink:href="https://osf.io/67sn8/">https://osf.io/67sn8/</ext-link>).</p></caption><graphic xlink:href="EMS190121-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Collapsed topography and spectrum plot of Experiment 1.</title><p>The collapsed topography shows the sum of the baseline-subtracted amplitudes at 2.4 and 4.8 Hz, averaged across participants and conditions. Electrodes included in the analysis are marked in white. The topography is scaled from 0 to the maximum across electrodes (0.14 μV). The spectrum plot shows the baseline-subtracted amplitudes in the 8 conditions, averaged across participants and the electrodes included in the statistical analysis. INV: inverted, UP: upright, DIFF: different directions, SAME: same direction, NS: Non-synchronous, S: Synchronous.</p></caption><graphic xlink:href="EMS190121-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Collapsed topography and spectrum plot of Experiment 2.</title><p>The collapsed topography shows the sum of the baseline-subtracted amplitudes at 2.4 and 4.8 Hz, averaged across participants and conditions. Electrodes included in the analysis are marked in white. The topography is scaled from 0 to the maximum across electrodes (0.16 μV). The spectrum plot shows the baseline-subtracted amplitudes in the 4 conditions, across participants and the included electrodes. INV: inverted, UP: upright, DIFF: different directions, SAME: same direction.</p></caption><graphic xlink:href="EMS190121-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Baseline-subtracted amplitudes at the walking frequency (2.4 Hz) and its harmonics in Experiment 1.</title><p>Note that 0 is the baseline and hence that values below 0 reflect noise. The dots show the amplitude of the brain signal per condition and per participant. The diamonds show the amplitudes per condition across participants. Error bars are 95% confidence intervals corrected for within-subject designs (<xref ref-type="bibr" rid="R34">Morey, 2008</xref>). Topographies are scaled from 0 to the maximum across the scalp and conditions (0.24 μV). INV: inverted, UP: upright, DIFF: different directions, SAME: same direction, NS: Non-synchronous, S: Synchronous.</p></caption><graphic xlink:href="EMS190121-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Baseline-subtracted amplitudes at the walking frequency (2.4 Hz) and its harmonics in Experiment 2.</title><p>As 0 is the baseline, values below 0 reflect noise. The dots show amplitudes per condition and per participant. The diamonds show amplitudes per condition across participants. Error bars are 95% confidence intervals corrected for within-subject designs (<xref ref-type="bibr" rid="R34">Morey, 2008</xref>). Topographies are scaled from 0 to the maximum across the scalp and conditions (0.19 μV). INV: inverted, UP: upright, DIFF: different directions, SAME: same direction.</p></caption><graphic xlink:href="EMS190121-f005"/></fig></floats-group></article>