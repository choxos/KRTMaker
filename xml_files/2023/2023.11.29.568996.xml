<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192487</article-id><article-id pub-id-type="doi">10.1101/2023.11.29.568996</article-id><article-id pub-id-type="archive">PPR767334</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multiplexed tumor profiling with generative AI accelerates histopathology workflows and improves clinical predictions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pati</surname><given-names>Pushpak</given-names></name><email>pus@zurich.ibm.com</email><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Karkampouna</surname><given-names>Sofia</given-names></name><email>sofia.karkampouna@unibe.ch</email><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bonollo</surname><given-names>Francesco</given-names></name><email>francesco.bonollo@unibe.ch</email><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Compérat</surname><given-names>Eva</given-names></name><email>eva.comperat@meduniwien.ac.at</email><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Radic</surname><given-names>Martina</given-names></name><email>martina.radic@unibe.ch</email><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Spahn</surname><given-names>Martin</given-names></name><email>martin.spahn@hin.ch</email><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Martinelli</surname><given-names>Adriano</given-names></name><email>art@zurich.ibm.com</email><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Wartenberg</surname><given-names>Martin</given-names></name><email>martin.wartenberg@unibe.ch</email><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Julio</surname><given-names>Marianna Kruithof-de</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A8">8</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Rapsomaniki</surname><given-names>Maria Anna</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>IBM Research Europe, Säumerstrasse 4, Rüschlikon, 8803, Switzerland</aff><aff id="A2"><label>2</label>Department for BioMedical Research, Urology Research Laboratory, University of Bern, Murtenstrasse 24, Bern, 3008, Switzerland</aff><aff id="A3"><label>3</label>Department of Pathology, Medical University of Vienna, Währinger Gürtel 18-20, Vienna, 1090, Austria</aff><aff id="A4"><label>4</label>Department of Urology, Lindenhofspital Bern, Bremgartenstrasse 117, Bern, 3012, Switzerland</aff><aff id="A5"><label>5</label>Department of Urology, University Duisburg-Essen, Universitätsstrasse 2, Essen, 45141, Germany</aff><aff id="A6"><label>6</label>ETH Zürich, Rämistrasse 101, Zürich, 3010, Switzerland</aff><aff id="A7"><label>7</label>Institute of Tissue Medicine and Pathology, University of Bern, Murtenstrasse 31, Bern, 3008, Switzerland</aff><aff id="A8"><label>8</label>Department of Urology, Inselspital Bern, Freiburgstrasse 37, Bern, 3010, Switzerland</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author(s). <email>marianna.kruithofdejulio@unibe.ch</email>; <email>aap@zurich.ibm.com</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>02</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>01</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Understanding the spatial heterogeneity of tumors and its links to disease is a cornerstone of cancer biology. Emerging spatial technologies offer unprecedented capabilities towards this goal, but several limitations hinder their clinical adoption. To date, histopathology workflows still heavily depend on hematoxylin &amp; eosin (H&amp;E) and serial immunohistochemistry (IHC) staining, a cumbersome and tissue-exhaustive process that yields unaligned tissue images. We propose the VirtualMultiplexer, a generative AI toolkit that translates real H&amp;E images to matching IHC images for several markers based on contrastive learning. The VirtualMultiplexer learns from unpaired H&amp;E and IHC images and introduces a novel multi-scale loss to ensure consistent and biologically reliable stainings. The virtually multiplexed images enabled training a Graph Transformer that simultaneously learns from the joint spatial distribution of several markers to predict clinically relevant endpoints. Our results indicate that the VirtualMultiplexer achieves rapid, robust and precise generation of virtually multiplexed imaging datasets of high staining quality that are indistinguishable from the real ones. We successfully employed transfer learning to generate realistic virtual stainings across tissue scales, patient cohorts, and cancer types with no need for model fine-tuning. Crucially, the generated images are not only realistic but also clinically relevant, as they greatly improved the prediction of different clinical endpoints across patient cohorts and cancer types, speeding up histopathology workflows and accelerating spatial biology.</p></abstract><kwd-group><kwd>virtual staining</kwd><kwd>stain-to-stain translation</kwd><kwd>multiplexed imaging</kwd><kwd>generative modeling</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Tissues are spatially organized ecosystems, where cells of diverse phenotypes, morphologies and molecular profiles coexist with non-cellular compounds and interact to maintain homeostasis [<xref ref-type="bibr" rid="R1">1</xref>]. Several tissue staining technologies are used to interrogate this intricate tissue architecture and identify morphological and molecular patterns linked to disease. Among these technologies, H&amp;E staining is the undisputed workhorse, routinely used to assess aberrations in tissue morphology in histopathology workflows across diseases [<xref ref-type="bibr" rid="R2">2</xref>]. A notable example is cancer, where H&amp;E staining is routinely used to reveal abnormal cell proliferation, nuclear shape, lymphovascular invasion and immune cell infiltration, among others. Complementary to the morphological information available <italic>via</italic> H&amp;E staining, IHC [<xref ref-type="bibr" rid="R3">3</xref>], another staining technique routinely applied in histopathology laboratories, exploits antigen-antibody binding to detect and quantify the abundance of a single protein marker <italic>in situ</italic>. IHC visualizes the distribution and localization of specific markers within cell compartments (membranous, cytoplasmic and/or nuclear) and within their proper histological context, which is crucial for tumor subtyping, prognosis, and personalized treatment selection. As tissue re-staining in conventional IHC is limited, repeated serial tissue sections stained with different antibodies are required for in-depth tumor profiling. However, this is a time-consuming and tissue-exhaustive process, prohibitive in cases of limited tissue availability. At the same time, serial IHC staining yields unaligned, non-multiplexed tissue images occasionally of suboptimal quality in terms of tissue artifacts, and tissue unavailability may lead to missing stainings (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Recently, multiplexed imaging technologies (<italic>e.g</italic>., imaging mass cytometry - IMC [<xref ref-type="bibr" rid="R4">4</xref>], co-detection by indexing - CODEX [<xref ref-type="bibr" rid="R5">5</xref>], multiplexed ion beam imaging - MIBI [<xref ref-type="bibr" rid="R6">6</xref>]) have enabled the simultaneous quantification of dozens of markers on the same tissue, revolutionizing spatial biology [<xref ref-type="bibr" rid="R7">7</xref>]. Still, their high cost, cumbersome experimental process, tissue destructive nature, long turnaround time, and need for specialized personnel and equipment severely limit their clinical adoption.</p><p id="P3">Virtual staining, <italic>i.e</italic>., the generation of artificially stained tissue images using generative AI models, has emerged as a promising cost-effective, easily accessible and rapid alternative that addresses above limitations [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>]. A virtual staining model exploits two sets of images - a <italic>source</italic> set and a <italic>target</italic> set - to learn the source-to-target appearance mapping [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. During inference, the trained model takes as input a source image and produces a virtually stained target image by simulating the target staining on the source. Initial virtual staining models were based on different flavors of generative adversarial networks (GANs) and operated under a <italic>paired</italic> setting, <italic>i. e</italic>., precisely aligned source and target images, which allowed them to directly optimize a pixel-wise loss between the virtual and real images [<xref ref-type="bibr" rid="R12">12</xref>]. Successful examples of paired models include translating label-free microscopy tissue images to H&amp;E and specific stainings [<xref ref-type="bibr" rid="R13">13</xref>–<xref ref-type="bibr" rid="R16">16</xref>], H&amp;E to special stains [<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref>], H&amp;E to IHC [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>], and IHC to multiplex immunofluorescence [<xref ref-type="bibr" rid="R21">21</xref>]. However, as tissue re-staining is not routinely done in most cases, training paired models depends on aligning tissue slices via image registration, a time-consuming and error-prone process, which is often infeasible in practice because of substantial discrepancies even between consecutive slices. Additionally, as tissue architecture largely alters after the first set of slices, retrospective addition of new markers or multiplexing of several markers on a specific area/focal plane of interest is impossible. To circumvent these limitations, <italic>unpaired</italic> stain-to-stain (S2S) translation models have recently emerged, with early applications in translating from H&amp;E to IHC [<xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R26">26</xref>] and special staining [<xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R28">28</xref>] and from cryosections to Formalin-Fixed Paraffin-Embedded (FFPE) sections [<xref ref-type="bibr" rid="R29">29</xref>]. The vast majority of unpaired S2S translation models are inspired by CycleGAN [<xref ref-type="bibr" rid="R30">30</xref>]; they depend on an adversarial loss to preserve the source content (tissue architecture), and a cycle consistency loss to preserve the target style (staining pattern). Some employ additional constraints, <italic>e.g</italic>., domain-invariant content and domain-variant style [<xref ref-type="bibr" rid="R22">22</xref>], perceptual embeddings [<xref ref-type="bibr" rid="R24">24</xref>] or structural similarity [<xref ref-type="bibr" rid="R25">25</xref>].</p><p id="P4">However, an important limitation of CycleGAN-based models is that cycle consistency assumes a bijective mapping between the source and target domains [<xref ref-type="bibr" rid="R30">30</xref>], which does not necessarily hold for many S2S translation tasks. As a result, a persistent problem is staining unreliability, observed as incorrect mappings across domains, <italic>e.g</italic>., a positive signal from the source domain can get mapped to a negative signal from the target domain. To account for staining unreliability, recent works exploit expert annotations in the source and target domains to guide the translation. For example, Boyd <italic>et al.</italic> [<xref ref-type="bibr" rid="R26">26</xref>] translated H&amp;E to Cytokeratin (CK) stained IHC using aregion-based CycleGAN and expert annotations of positive and negative metastatic regions on the H&amp;E images. Similarly, Liu <italic>et al.</italic> [<xref ref-type="bibr" rid="R25">25</xref>] translated H&amp;E to Ki67 stained IHC by leveraging cancer or normal region annotations in both the H&amp;E and IHC images. Although these approaches show promising results for these specific translation problems, acquiring such annotations is impractical when translating to several IHC markers, and infeasible even for experienced pathologists for more specialized translation tasks (<italic>e.g</italic>., identifying p53+ cells in H&amp;E images). To circumvent the annotation challenge, Zheng <italic>et al.</italic> [<xref ref-type="bibr" rid="R31">31</xref>] recently introduced a semi-supervised approach, which however, again, depends on consecutive tissue sections and image registration. Consequently, there is a great need for S2S translation models that can learn from unpaired source and target images and preserve staining consistency without the need for consecutive tissue sections, image registration or extensive expert annotations on the source domain.</p><p id="P5">Regardless of the underlying modeling assumptions, an important limitation of the above S2S translation studies concerns model evaluation. As ground truth and virtually generated images are not pixel-wise aligned, S2S translation quality is typically quantified at a high-feature-level using inception-based scores [<xref ref-type="bibr" rid="R32">32</xref>]. However, these scores do not guarantee that the generated images accurately preserve complex and biologically meaningful spatial patterns [<xref ref-type="bibr" rid="R9">9</xref>]. To alleviate these concerns, a step in the right direction is expert qualitative assessment though pathological examination of the virtual images, employed by some studies [<xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R24">24</xref>]. Still, as with all generative AI approaches, a persistent concern is the presence of hallucinations in virtually stained images that might otherwise appear realistic even to experienced pathologists. Ultimately, to ensure that virtually stained images do not only appear visually realistic but are also useful and relevant from a clinical standpoint, using them as input to downstream deep learning models to predict diagnostic or prognostic endpoints could provide an unbiased and convincing validation [<xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P6">Here, we propose the VirtualMultiplexer, a generative toolkit that translates H&amp;E images (source) to matching IHC images (target) for a variety of molecular markers, generating virtually multiplexed tissue images (<xref ref-type="fig" rid="F1">Figure 1B</xref>). The VirtualMultiplexer is inspired by contrastive unpaired translation (CUT) [<xref ref-type="bibr" rid="R33">33</xref>], an appealing contrastive learning alternative to CycleGAN that achieves content preservation by maximizing the mutual information between target and source domains. Our toolkit does not depend on pixel-wise aligned H&amp;E and IHC images during training and, in contrast to existing approaches, requires minimal expert annotations only on the target IHC domain. To ensure consistent and biologically relevant virtual stainings, Virtual-Multiplexer introduces a novel architecture based on multi-scale constraints at the single-cell, cell-neighborhood and whole-image level that closely mimic human expert evaluation. Specifically, our multi-scale approach is designed to accurately capture the staining specificity at the individual cell level, while also ensuring content and style preservation at a cell neighborhood level and a global image level. We trained the VirtualMultiplexer on a prostate cancer tissue microarray (TMA) dataset containing unpaired H&amp;E and IHC stainings for six clinically relevant nuclear, cytoplasmic, and membrane-targeted markers, and evaluated the generated images using quantitative image fidelity metrics, expert pathological assessment and visual Turing tests.</p><p id="P7">Importantly, to evaluate the clinical relevance of the generated images, we devised a Graph Transformer model that combined a Graph Neural Network and Vision Transformer (ViT) to simultaneously learn from the joint spatial distribution of several markers and predict clinically relevant endpoints (<xref ref-type="fig" rid="F1">Figure 1C</xref>). As our toolkit is not limited by tissue availability or time constraints, we successfully transferred it across tissue image scales (TMAs to whole slide images (WSIs)), across two additional out-of-distribution large prostate cancer patient cohorts, and across tissue types (from prostate to pancreatic, breast and colorectal tissue) (<xref ref-type="fig" rid="F1">Figure 1C</xref>). Our results suggest that the VirtualMultiplexer generates realistic multiplexed IHC stainings of high staining quality which are indistinguishable from real IHC stainings, outperforming existing methods. Importantly, using the generated datasets to train early fusion Graph Transformers surpassed in performance models trained with real unaligned data when predicting clinically relevant endpoints (<italic>e.g</italic>., survival status, tumor grade and stage) not only in the training cohort, but also in two independent prostate cancer patient cohorts and a pancreatic ductal adenocarcinoma (PDAC) cohort. Overall, the VirtualMultiplexer is a cost-effective, rapid, and easily accessible toolkit that can be readily used to generate virtual multiplexed imaging datasets of high quality, alleviate issues caused by missing modalities and tissue artifacts, improve the prediction of clinical endpoints and generalize across image scales, patient cohorts, and cancer types, with important implications in histopathology.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>VirtualMultiplexer: a generative toolkit for virtually multiplexed staining</title><p id="P8">The VirtualMultiplexer is a generative toolkit for unpaired S2S translation, trained on unpaired real H&amp;E (source) and IHC (target) images. An overview of the model is presented in <xref ref-type="fig" rid="F2">Figure 2</xref> and a detailed description of its architecture and objective functions is provided in the <xref ref-type="sec" rid="S10">Methods</xref> (Section 8). During training, each image is split into patches of size 256×256 pixels at 10× resolution, which are in turn fed into a generator network <italic>G</italic> that conditions on input H&amp;E and IHC and learns to transfer the staining pattern, as captured from IHC images, to the tissue morphology, as captured by the H&amp;E images. The generated IHC patches are then stitched together to create a final virtually stained IHC image (<xref ref-type="fig" rid="F2">Figure 2A</xref>). During the S2S translation, we aim to ensure that the virtually stained IHC images are consistent with the real IHC images in terms of appearance, from the macroscopic to the microscopic scale. This implies that the generated IHC images should preserve the staining distribution of real IHC at a patch and tile scale, but also accurately learn the staining specificity of the different markers at the cellular scale. To achieve this, the model jointly optimizes three distinct loss functions, each one designed to preserve image consistency across different scales (<xref ref-type="fig" rid="F2">Figure 2B</xref>). The neighborhood loss (1) ensures that the generated IHC patches are indistinguishable from real IHC patches and consists of an adversarial and a multilayer contrastive loss (<xref ref-type="fig" rid="F2">Figure 2B</xref>), adopted from CUT [<xref ref-type="bibr" rid="R33">33</xref>]. The adversarial loss <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>adv</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (1a) is a standard GAN loss [<xref ref-type="bibr" rid="R34">34</xref>], where real and virtual IHC patches are used as input to a convolutional neural network (CNN) patch discriminator network <italic>D</italic> which attempts to classify them as either real or virtual, eliminating style differences between real and virtual patches. The multilayer contrastive loss (1b) is based on a patch-level noise contrastive estimation loss [<xref ref-type="bibr" rid="R33">33</xref>] <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>contrastive</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> that aims to ensure that the content of corresponding real H&amp;E and virtual IHC patches is preserved across multiple layers of <italic>G</italic><sub>enc</sub>, <italic>i.e</italic>., the encoder of the generator <italic>G</italic>. The VirtualMultiplexer introduces two novel losses, namely a global and a local consistency loss (<xref ref-type="fig" rid="F2">Figure 2B</xref>). The global consistency loss (2) uses a feature extractor <italic>F</italic>, a pre-trained VGG network [<xref ref-type="bibr" rid="R35">35</xref>], and enforces content consistency between the real H&amp;E and the virtual IHC image <inline-formula><mml:math id="M3"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>content</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>, and style consistency between the real and virtual IHC images <inline-formula><mml:math id="M4"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>style</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> at a tile level across multiple layers of F. In this way the model leverages a high-level sample information, <italic>i.e</italic>., the correspondence between real H&amp;E and real IHC pairs, to ensure global tissue composition consistency and mitigate the macroscopic appearance variability. Finally, the local consistency loss (3) consists of a cell discriminator loss <inline-formula><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and a cell classification loss <inline-formula><mml:math id="M6"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellClass</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> that together enable the model to capture a realistic appearance and staining pattern at the cellular level while alleviating the multi sub-domain mapping issue. This is achieved by leveraging prior knowledge on staining status via expert annotations, and training two separate networks: a cell discriminator <italic>D</italic><sub>cell</sub> that attempts to eliminate differences in the style of real and virtual cells, and a cell classifier <italic>F</italic><sub>cell</sub> that predicts the staining status and thus enforces staining consistency at a cell-level.</p></sec><sec id="S4"><title>Performance assessment of the VirtualMultiplexer</title><p id="P9">We first assessed the performance of VirtualMultiplexer in terms of the quality of the generated IHC stainings. We trained the model on a cohort of prostate cancer TMAs from the European Multicenter Prostate Cancer Clinical and Translational Research Group (EMPaCT) [<xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R38">38</xref>] (<xref ref-type="sec" rid="S10">Methods</xref>). The cohort contained unpaired H&amp;E and IHC stainings from 210 patients with 4 cores per patient for six clinically relevant markers, namely androgen receptor (AR), NK3 Homeobox 1 (NKX3.1), CD44, CD146, p53 and ERG. The VirtualMultiplexer was used to generate a series of virtual IHC stainings (<xref ref-type="fig" rid="F3">Figure 3C</xref>) that preserved the tissue morphology as seen in the real H&amp;E image (<xref ref-type="fig" rid="F3">Figure 3A</xref>) and the staining pattern as seen in the real IHC image (<xref ref-type="fig" rid="F3">Figure 3B</xref>). Additional examples for two more TMA cores and all IHC markers are presented in the <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A1</xref>. We quantitatively compared the results of the VirtualMultiplexer with four state-of-the-art unpaired S2S translation methods, namely CycleGAN [<xref ref-type="bibr" rid="R30">30</xref>], CUT [<xref ref-type="bibr" rid="R33">33</xref>], CUT with kernel instance normalization (KIN) [<xref ref-type="bibr" rid="R39">39</xref>], and AI-FFPE [<xref ref-type="bibr" rid="R29">29</xref>] using the Fréchet Inception Distance (FID), an established metric used to assess the proximity of images created by a generative model to a set of target domain images [<xref ref-type="bibr" rid="R40">40</xref>]. The VirtualMultiplexer resulted in the lowest FID score across all markers (<xref ref-type="fig" rid="F3">Figure 3D</xref>), with an average value of 29.2 (±3), which was consistently lower than the competing methods CycleGAN (49 ± 6), CUT (35.8 ± 4.5), CUT with KIN (37.8 ± 2.3), and AI-FFPE (35.9 ± 2.6). This result indicated that virtual stainings generated by the VirtualMultiplexer were the closest to the real IHC stainings in terms of distribution than any of the competing methods.</p><p id="P10">To further quantify the indistinguishability of real and synthetically stained images, we conducted a visual Turing test as follows: three independent evaluators with expertise in prostate tissue histopathology and one board-certified pathologist were shown 100 randomly selected patches per marker, with 50 of them originating from real and 50 from virtually stained IHC images, and were asked to classify each patch as virtual or real. We observed that our model was able to trick the experts, as it achieved an average sensitivity of 52.1% and specificity of 54.1% across all six markers (<xref ref-type="fig" rid="F3">Figure 3E</xref>) (note that a random classification into real or virtual would correspond to a sensitivity and specificity of 50%). Last, we performed a staining quality assessment: we provided the pathologist 50 real and 50 virtual images from 50 TMA cores per IHC marker, revealing which are real and which are virtual, who in turn performed a qualitative assessment of the staining, as judged by overall expression levels, overall background, staining pattern, cell type specificity and subcellular localization (<xref ref-type="fig" rid="F3">Figure 3F</xref>). Across all six markers, on average 70.7% of the virtually stained images reached an acceptable staining quality, as opposed to 78.3% of the real images. The results varied depending on the marker, with cores virtually stained for NKX3.1 and CD146 achieving the highest staining quality of 96%, surpassing even the real images. Conversely, virtually stained AR images had the lowest score of 46%, with an additional 10% exhibiting accurate staining but high background, and the remaining 42% rejected due to mostly heterogeneous staining, or falsely unstained cells. Background staining presented a challenge with CD44 and p53; the latter appeared to be further affected by border artifacts, <italic>i.e</italic>., presence of abnormally highly stained cells only in the border of the core, an artifact also occasionally present in real images. ERG achieved a higher staining quality assessment in virtual than in the real images, which both appeared to often face background issues. We concluded that, for most of the markers, the staining quality scores and the number of cores with staining artefacts (high background and/or border artefacts) were comparable in virtual versus real images.</p><p id="P11">Following these observations, we carefully examined the virtually generated images and assessed to which extend the VirtualMultiplexer captured the staining patterns of the real ones. The virtual images showed similar pattern and signal distribution to the real images for all six markers, with correct cell type and subcellular distribution (<xref ref-type="fig" rid="F4">Figure 4A</xref>). For instance, AR+ and NKX3.1+ cells were evaluated as having correct distribution in the luminal epithelial compartment of the prostatic glands and nuclear localization. Furthermore, a few NKX3.1+ cells in stromal regions (possibly stroma-invading tumor cells) were correctly predicted in the virtually stained cores. Similarities in specific, matched areas between virtual and real IHC images were also assessed mainly for staining pattern and overall intensity levels. To this end, we specifically assessed that the virtual expression of markers indicative of tumor-specific molecular profile, such as loss of TP53 and ERG overexpression, did not largely deviate from the real IHC at the overall TMA core level (<xref ref-type="fig" rid="F4">Figure 4A</xref>), which would be crucial for diagnostic applicability. Certain discrepancies in virtual versus real images were also found, such as non-specific signal in extra-cellular-matrix/stroma regions (NKX3.1, p53, ERG), false nuclear expression (CD44), and systematic lack of recognition of CD146+ vascular structures (<xref ref-type="fig" rid="F4">Figure 4B</xref>). Nonetheless, the more pathologically relevant staining patterns were correctly reconstructed, as described in <xref ref-type="fig" rid="F4">Figure 4A</xref>. We also performed an ablation study demonstrating the effects of training with different components of the objective function of the VirtualMultiplexer (<xref ref-type="supplementary-material" rid="SD1">Appendix Figure A2</xref>). Indeed, we observe that incorporating multi-scale loss terms, from the whole image to the cellular level, allows us to capture biologically consistent staining patterns that resemble the real IHC stainings. The mere imposition of the neighborhood consistency, the primary objective employed in competing methods, produces virtually stained tissue regions mimicking the overall staining distribution of real target IHC stainings, but clearly leads to staining unreliability, <italic>e.g</italic>., swapping of staining patterns between positive and negative cells. The introduction of the global consistency clearly mitigates this issue, and the further addition of the local consistency further optimizes the virtual staining at the cell-level.</p></sec><sec id="S5"><title>Transfer learning from TMAs to WSIs</title><p id="P12">To assess how well the model can be transferred across imaging scales, we fed the TMA-trained VirtualMultiplexer with five out-of-distribution prostate tissue WSIs stained with H&amp;E, and generated virtually stained IHC images for the NKX3.1, AR, and CD146 markers, as before. We then stained for the same markers by IHC on the direct serial sections, thus generating a real stained WSI that can be directly used to visually validate the model predictions. For marker NKX3.1 (<xref ref-type="fig" rid="F5">Figure 5</xref>), we observed that the virtually stained images largely captured the staining appearance of the real ones, both in terms of specific glandular luminal cell identification (positive signal) (examples 1 and 2 in <xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A3</xref>) and accurate non-annotation of stromal or vascular structures (absence of signal) (example 3 in <xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A3</xref>). In minority, virtual images did not highlight the rarer NKX3.1+ cell population that are not part of the epithelial gland, but rather in the periglandular stroma (example 4 in <xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A3</xref>). For both CD146 and AR, we observed staining intensity discrepancies between virtual and real images, which were more striking in the case of CD146 where the overall signal intensity and background is higher in the virtual versus the real images (<xref ref-type="fig" rid="F5">Figure 5</xref> and <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A3</xref>). These discrepancies can be reasoned to the fact that the VirtualMultiplexer has been trained on TMA images that have a different staining distribution than the WSIs. This might lead to false interpretation of the marker expression levels at a first visual inspection. However, when evaluating at higher magnification, the staining pattern in the matching regions of real and virtual stainings was effectively correct, <italic>e.g</italic>., no glandular signal (example 5 in <xref ref-type="fig" rid="F5">Figure 5</xref>) and appropriate stromal localization of CD146 (examples 6 and 7 in <xref ref-type="fig" rid="F5">Figure 5</xref>) and nuclear localization of AR in luminal epithelial cells (example 1 in <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A3</xref>). Lack of detection of vascular structures for CD146 was evident in both TMA cores and WSI (example 8 in <xref ref-type="fig" rid="F5">Figure 5</xref>).</p></sec><sec id="S6"><title>The VirtualMultiplexer leads to improved clinical predictions</title><p id="P13">We further substantiated the utility of the generated virtual IHC stainings in augmenting the performance of AI models when predicting clinically relevant endpoints. To this end, we benchmarked the classification performance of AI models that were trained using real H&amp;E, real IHC, or virtual IHC images. Specifically, we encoded the images as tissue-graph representations and employed a Graph-Transformer (GT) [<xref ref-type="bibr" rid="R41">41</xref>] to map the representations to downstream class labels. First, we extracted patches from the images and used a pretrained ResNet-50 network [<xref ref-type="bibr" rid="R42">42</xref>] to encode patch features (<xref ref-type="fig" rid="F6">Figure 6A</xref> and <xref ref-type="sec" rid="S10">Methods</xref>). The patches and their features formed the nodes and node representations of the tissue-graph, and the edges were formed using the spatial distribution of the patches (<xref ref-type="fig" rid="F6">Figure 6B</xref> and <xref ref-type="sec" rid="S10">Methods</xref>). The graph representation underwent graph convolutions to contextualize the node features of the local tissue neighborhood. Afterwards, the node features were pooled and fed to a transformer layer, trained to predict clinical endpoints. Depending on how the patch features were combined, we trained the GT model under the following three settings (<xref ref-type="fig" rid="F6">Figure 6C</xref>): (i) a unimodal setting, where independent GT models were trained for each H&amp;E and IHC marker, (ii) a multimodal late fusion setting, where the outputs of independent GT models were fused at the last embedding stage, and (iii) a multimodal early fusion setting, where the patch features were combined early in the tissue-graph and fed into the GT model. While the unimodal setting resulted in a separate prediction per marker, in both multimodal settings the patch features were combined, resulting in one prediction across all markers. However, in contrast to the late fusion multimodal setting that necessitated the training of several GT models, in the early fusion case only one model that learned from the joint spatial distribution across all markers was trained, mimicking a multiplexed imaging scenario. With the exception of the early fusion setting that was only feasible for virtual images, we tested all three settings with both real and virtual images as input, resulting in a total of five different combinations (<xref ref-type="fig" rid="F6">Figure 6D</xref>, legend).</p><p id="P14">We applied these settings to the EMPaCT dataset to predict two clinically relevant endpoints, namely the overall survival status and the disease progression of patients (<xref ref-type="fig" rid="F6">Figure 6D</xref>). We note that in the EMPaCT dataset few IHC marker stainings are missing, leading to small discrepancies in the number of real IHC images available across all six markers. To ensure a fair comparison between real and virtual unimodal models, we matched the number of virtual IHC images to the number of available real IHC images, which implies that the dark and light blue barplots in <xref ref-type="fig" rid="F6">Figure 6D</xref> are directly comparable. However, as H&amp;E images were available for all patients, this issue did not affect the unimodal model trained on H&amp;E images, which has a slight advantage over all other models in terms of number of samples used. Importantly, to compare all multimodal models, we again followed the same strategy of matching the number of virtual images used to the ones available in real data, and thus the last 3 bars in <xref ref-type="fig" rid="F6">Figure 6D</xref> are also directly comparable. More details of the dataset distribution and the GT training are presented in <xref ref-type="sec" rid="S10">Methods</xref> (Section 8). We observed that the unimodal models trained with virtual images are on par with unimodal ones trained with real images for both tasks, and the predictive performance of the unimodal models trained with virtual IHC images varied depending on the marker’s predictive ability towards the downstream task. In the case of overall survival status prediction, two interesting exceptions concern CD146 and p53: for CD146 the unimodal models trained on virtual data outperformed the ones trained on real data, which is in accordance to the previous observation that virtual CD146 images achieved a higher quality assessment than real CD146 images (<xref ref-type="fig" rid="F3">Figure 3F</xref>). The opposite is true for p53: virtual p53 images were of lower quality than real p53 images, and the corresponding unimodal - virtual prediction models achieved a lower performance than the unimodal - real ones. However, these observations were not replicated for disease progression prediction, which appeared to be an overall harder prediction task. In both prediction tasks, the multimodal settings outperformed the unimodal H&amp;E results, indicating the utility of combining information from complementary markers over individual stainings. Furthermore, the multimodal early fusion model trained with virtual images achieved the best weighted F1 score of 82.9% and 74.8% for overall survival status and disease progression, respectively, establishing the potential of multiplexed analysis via virtual staining for augmenting the efficacy of AI models. Overall, we concluded that using virtual images generated by the VirtualMultiplexer can boost the performance of state-of-the-art AI models for clinically relevant endpoints.</p></sec><sec id="S7"><title>Transferring the VirtualMultiplexer across patient cohorts and cancer types</title><p id="P15">We assessed the ability of the VirtualMultiplexer model to generalize to out-of-distribution data by employing two independent prostate cancer patient cohorts, namely SICAP [<xref ref-type="bibr" rid="R43">43</xref>] and PANDA [<xref ref-type="bibr" rid="R44">44</xref>], each one containing H&amp;E stained needle biopsies with associated Gleason scores (details in <xref ref-type="sec" rid="S10">Methods</xref>). We virtually stained the H&amp;E images for four IHC markers, namely NKX3.1, CD146, AR and ERG, using the pre-trained VirtualMultiplexer on the EMPaCT dataset (example needle biopsy for SICAP in <xref ref-type="fig" rid="F7">Figure 7A</xref>, additional examples for both SICAP and PANDA in <xref ref-type="supplementary-material" rid="SD1">Appendix Figure A4</xref>). The IHC markers were chosen due to their possible relevance towards Gleason score prediction. We observed that the virtual staining patterns of the IHC markers were overall correct and specific for each marker in terms of cell type and subcellular localization, with the only exception the occasional aspecific AR signal in the extracellular matrix areas. Other inconsistencies include the weak staining of interstitial tissue for CD146, and the heterogeneous staining of the glands for ERG. We also observed some recurring issues as in the quality assessment of the EMPaCT TMA (<xref ref-type="fig" rid="F3">Figure 3</xref>), namely background (<italic>e.g</italic>., stromal background occasionally present in NKX3.1 and ERG), border and tiling artifacts (<italic>e.g</italic>., for CD146). Subsequently, we trained GT models under the previously described settings to predict the Gleason grade for both SICAP and PANDA datasets, shown in <xref ref-type="fig" rid="F7">Figure 7B and C</xref>, respectively. We observe that the predictive performance of the unimodal models trained on virtual IHC stainings was close to or superior to the model using standalone H&amp;E stainings for both the SICAP and PANDA datasets. Further improvement in Gleason score prediction was attained by the multimodal models built on the virtual IHC stainings. The early fusion model trained with the virtually multiplexed data achieved the best weighted F1 score of 61.4% and 72.3% for Gleason scoring on SICAP and PANDA, respectively, which significantly outperformed the H&amp;E unimodal counterpart by 11.9% and 6.6% on SICAP and PANDA, respectively.</p><p id="P16">Finally, we evaluated the generalization ability of the VirtualMultiplexer to images of other cancer types. To this end, we first applied the pre-trained VirtualMultiplexer on the EMPaCT dataset to a PDAC TMA that was unseen by the model and used the available H&amp;E images to generate virtual IHC stainings for CD44, CD146 and p53 (<xref ref-type="fig" rid="F7">Figure 7D</xref>), three markers with expected expression in pancreatic tissue. The generated images appeared overall realistic, with no means of discriminating whether they were virtually or actually stained. We observed that the CD44 and CD146 staining pattern in the virtual images was allocated, as expected, to the extracellular matrix of presented tissue spots, without major staining in the epithelial tissue part. For p53, we again observed overall proper staining allocation to the nuclei of epithelial cells with expected distribution, with no major staining of other compartments. To quantify the utility of the virtual stainings for downstream applications, we followed the same process as before to predict PDAC tumor, node and metastasis (TNM) stage, leading, again, to increased performance of models trained with virtually multiplexed data, concluding that virtually multiplexed data offers a performance advantage to prediction models.</p><p id="P17">We also applied the pre-trained VirtualMultiplexer to generate virtual IHC stainings for CD44 and CD146 from colorectal [<xref ref-type="bibr" rid="R45">45</xref>] and breast cancer [<xref ref-type="bibr" rid="R46">46</xref>] H&amp;E-stained WSIs from The Cancer Genome Atlas (TCGA) [<xref ref-type="bibr" rid="R47">47</xref>]. Although the lack of normal tissue limited our ability to evaluate the staining quality in the generated images, we again observed an overall realistic virtual staining (<xref ref-type="supplementary-material" rid="SD1">Appendix Figure A5</xref>).</p></sec><sec id="S8"><title>The VirtualMultiplexer can greatly accelerate histopathology workflows</title><p id="P18">Lastly, we performed a runtime estimation of all components of the VirtualMultiplexer framework across imaging datasets of different scales, <italic>i.e</italic>., TMAs, needle biopsies and WSIs (<xref ref-type="supplementary-material" rid="SD1">Appendix Figure A6</xref>). We calculated that applying the trained VirtualMultiplexer on a single EMPaCT TMA core (6000×6000 pixels at 20X magnification-0.24<italic>μ</italic>m/pixel), an in-distribution sample, for one marker resulted in a total runtime of 2.81 seconds, and the same process for an out-of-distribution TMA core resulted in a runtime of 10.88 seconds, with the increase attributed to stain normalization. However, the stain normalization step is crucial as it alleviates the appearance disparity between the training EMPaCT H&amp;E TMAs and the out-of-distribution samples, and allows for a faithful application of the VirtualMultiplexer to unseen datasets. The above result implies that virtual staining of a hypothetical TMA slide containing 250 out-of-distribution TMA cores for 6 markers would be feasible in ≈65.8 minutes (preprocessing: ≈9.9 seconds per core, virtual staining and post-processing: ≈0.98 seconds per core and marker). Conversely, performing the IHC staining experiment for the same hypothetical TMA for 6 IHC markers could take an estimated time of approximately 1 day, when applied in a cutting-edge pathology laboratory using the latest protocols [<xref ref-type="bibr" rid="R48">48</xref>]. When applied in a biology lab that does not specialize in pathology, however, IHC staining could take up to 5 days per marker (sectioning: 1 day, staining: 2 days, slide drying: 1 day, imaging: 1 day), leading to a minimum of 5 days, if done simultaneously for all 6 markers, and more than 10 days, if performed mostly sequentially. Importantly, as our method scales linearly with the size of the tissue (TMA to WSI) and with the number of markers, similar time gains would be feasible for virtually staining needle biopsies and WSIs. We conclude that the VirtualMultiplexer leads to significant time gains when compared to a typical IHC staining, and can greatly accelerate histopathology workflows.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P19">Virtual staining has emerged as a promising direction in histopathology, with early attempts to apply GAN-based approaches on unpaired datasets across different stain-to-stain translation tasks. However, staining inconsistency issues limit its applications in translating marker-specific stains such as IHC. In this work, we proposed the VirtualMultiplexer, a generative toolkit that can effectively translate H&amp;E to IHC images for several markers, and is able to preserve staining consistency across image scales without requiring access to consecutive tissue sections, image registration or extensive expert annotations. This was achieved by proposing a novel architecture that includes the joint optimization of multi-scale loss functions that encode different biological priors to ensure biological consistency on a cellular, neighborhood, and global, whole-image scale. Our results indicated that the VirtualMultiplexer consistently out-performed in image fidelity state-of-the-art S2S translation methods and generated IHC images that were indistinguishable from the real ones to the expert human eye. Detailed histopathological evaluation suggested that the staining quality of the generated images was on par or even exceeded that of the real images in terms of staining pattern and distribution, with staining artifacts (<italic>e.g</italic>., high background, border effects) largely comparable in virtual versus real images for most markers. A thorough ablation study demonstrated that our novel local and global loss terms allowed us to mitigate staining unreliability and capture biologically consistent staining patterns, as opposed to solely using the adversarial and contrastive objectives employed in competing methods. When transferring the TMA-trained VirtualMultiplexer on prostate cancer WSIs and two unseen prostate cancer cohorts containing needle biopsies, we found that it generalized well to unseen prostate cancer images of different scales without any retraining or fine-tuning. Similar observations were made when generalizing on tissues of different origins, namely a PDAC TMA and TCGA breast and colorectal cancer WSIs.</p><p id="P20">While our results demonstrate a clear potential for the use of virtual multiplexed staining in histopathology, several limitations remain and can be addressed in future extensions of the model. Firstly, in some cases we observed elevated background levels in the EMPaCT TMA. Although this issue was also present in the real data, it appeared more pronounced for markers that had an overall more faint staining pattern (<italic>e.g</italic>., p53). More prominent background issues were present when we transferred the EMPaCT-trained model to the prostate cancer WSIs, which was expected considering that the EMPaCT IHC images and the prostate cancer WSIs were generated in two different institutions using different staining protocols and/or at different experiments and time points. Secondly, the patch-wise processing of the images induced tiling artifact in some cases, which is a well-known limitation of stain-to-stain translation approaches for large histopathology images [<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R39">39</xref>, <xref ref-type="bibr" rid="R49">49</xref>]. In our algorithm, the tiling artifact was more pronounced at the border of a core, visible as patches of high staining intensity. One possible underlying cause of this effect is that, when the model receives as input an H&amp;E patch at the border that contains very little tissue, the staining consistency losses “force” the model to stain the limited tissue with an overall higher intensity so as to match the staining distribution of the tissue-full patches that it has seen during training. Previous works [<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R39">39</xref>] have tried to address the tiling artifact, but it has been suggested that this caused less efficient translations [<xref ref-type="bibr" rid="R50">50</xref>]. As in our case the tiling artifact is observed in isolated, edge cases in the border, a straight-forward solution would be to simply discard a narrow border surrounding the tissue, as empirically done in regular IHC when border artifacts are present. Thirdly, certain discrepancies in staining specificity were occasionally observed, such as failing to stain CD146+ vascular structures and glandular NKX3.1+ cells that invaded periglandular stroma. This can be partially attributed to the fact that these patterns were observed more rarely in the training images, and can be mitigated by ensuring the inclusion of adequate representative examples in the labeling of the IHC images of the training set.</p><p id="P21">Crucially, despite their limitations, the generated virtual images enabled the training of early fusion Graph-Transformer (GT) models, which consistently outperformed models trained on real data in the prediction on clinically relevant endpoints. This improvement was not only observed in the training TMA dataset across two prediction tasks, but also further confirmed on both independent prostate cancer cohorts, and the PDAC TMA cohort. In our experiments, we ensured that the multimodal early fusion GT models did not have an advantage in terms of number of samples used over the models trained with real data. At the same time, multimodal early fusion models had a much smaller parameter space in comparison to late fusion ones, as late fusion necessitated the training of as many GT models as the number of markers. This suggests that the higher model complexity of late fusion methods does not guarantee a performance improvement. A potential explanation of the observed performance improvement could be the quality of the generated images that are not affected by partially missing tumor areas or other artifacts occasionally found in real images. This explanation is corroborated by the fact that, for markers where virtual images were of higher quality than real images, the corresponding unimodal - virtual models out-performed the unimodal - real ones, and vice versa. A more likely explanation could be that, as early fusion models were able to learn from the joint spatial distribution of several markers on the same tissue at once, they were able to pick up multimodal spatial relationships at the cellular level, mimicking data generated by advanced multiplexed imaging technologies. This explanation is further supported by the fact that in the early fusion case, a single GT model proved to have more learning capacity than the integration of several equivalently potent ones.</p><p id="P22">In conclusion, the current work establishes the potential of virtual multiplexed staining across images of different scales, patient cohorts and tissue types, with important implications towards AI-assisted histopathology. For example, the Virtual-Multiplexer could be directly used for data inpainting, <italic>i.e</italic>., filling out missing regions in a tissue image, or for sample imputation, <italic>i.e</italic>., generating from scratch missing samples in large patient cohorts. As IHC marker panels are not always standardized across labs, filling out the gaps via virtual multiplexed staining could open the door towards harmonizing datasets within or across research labs, which is particularly important in cases of archival tissue samples with limited availability [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R52">52</xref>]. As a result, the VirtualMultiplexer could enable the generation of comprehensive patient cohorts that could be used for clinically relevant predictions. An equally important application of our work is the use of virtual multiplexed staining for pre-histopathological experimental design: generating a large collection of IHC stains <italic>in silico</italic> and training AI models could support marker selection for actual experimentation, significantly reducing costs and preserving precious tissue. To reach its full potential, future work will be crucial to establish the use of the VirtualMultiplexer in real-world settings. From a technical standpoint, the generated virtual multiplexed stainings can enable the development of foundational models for IHC, as they have been successfully developed for brightfield H&amp;E images [<xref ref-type="bibr" rid="R53">53</xref>–<xref ref-type="bibr" rid="R55">55</xref>]. However, developing such models requires high volumes of data, which is potentially challenging to acquire for IHC. Virtual stainings can be beneficial to this end and can pave the way for multimodal foundational tissue characterization. Interestingly, the virtual multiplexed stainings can also be exploited as biologically conditioned data augmentations to boost the development and in turn the predictive performance of foundational models in histopathology applications. From an applications standpoint, although here we presented a first proof-of-concept for transferring the model across tissue types, more thorough evaluations are needed to solidify our encouraging results. Finally, although here we focused on H&amp;E-to-IHC translation, as our method is stain-agnostic, straight-forward adaptations of our work for S2S translation across cutting-edge multiplexed imaging technologies (<italic>e.g</italic>., IMC, CODEX, MIBI) could significantly reduce their costs and find important applications in dataset harmonization or antibody panel selection and optimization. Our vision is that future extensions of our work could lead to the generation of an ever-growing and readily available dictionary of virtual stainers for IHC and beyond, that would be essentially limitless with respect to how many markers could be incorporated, surpassing in multiplexing ability even the most cutting-edge technologies and significantly accelerating spatial biology.</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>Datasets</title><p id="P23">The VirtualMultiplexer was trained using the European Multicenter High Risk Prostate Cancer Clinical and Translational research group (EMPaCT) TMA dataset; an independent subset of EMPaCT was used for internal testing. The VirtualMultiplexer was further evaluated in a zero-shot fashion, <italic>i.e</italic>., without any retraining or fine-tuning, on three external prostate cancer datasets, namely prostate cancer WSIs, SICAP [<xref ref-type="bibr" rid="R43">43</xref>] and PANDA [<xref ref-type="bibr" rid="R44">44</xref>] needle biopsies, on an independent PDAC dataset (PDAC TMAs) and on TCGA data from breast and colorectal cancer. In all cases, independent Graph-Transformers are trained and tested for individual datasets, by using both real and virtually stained samples, to address various downstream classification tasks. Details on all datasets used follow.</p><sec id="S12"><title>EMPaCT</title><p id="P24">The dataset contains TMAs from 210 primary prostate tissues, as part of the European Multicenter High Risk Prostate Cancer Clinical and Translational research group (EMPaCT) and the Institute of Tissue Pathology in Bern. The study followed the guidelines of the World Medical Association Declaration of Helsinki 1964, updated in October 2013, and was conducted after approval by the Ethics Committees of Bern (CEC ID2015-00128). For each patient, four cores were selected, with two of them representing a low Gleason pattern and the other two a high Gleason pattern. Consecutive slices from each core were stained with H&amp;E and IHC using multiple antibodies against nuclear markers NK3 Homeobox 1 (NKX3.1) and Androgen receptor (AR), tumor markers Tumor Protein p53 (p53) and Erythroblast transformation-specific related gene (ERG) and membrane markers Cell Surface Glycoprotein (CD44) and Melanoma Cell Adhesion molecule (CD146/MCAM). TMA FFPE sections of 4 <italic>μ</italic>m were deparaffinized and used for heat-mediated antigen retrieval (citrate buffer, pH 6, Vector Labs or Tris-HCl pH 9). Sections were blocked for 10 min in 3% H<sub>2</sub>O<sub>2</sub>, followed by 30 minute room temperature incubation in 1% BSA in PBS–0.1%Tween 20. The following antibodies were used: anti-AR (Dako Agilent, M3562, 1:100 dilution), anti-NKX3.1 (Athena Enzyme Systems, 314, 1:200), anti-p53 (Dako Agilent, M7001, 1:800), anti-CD44 (Abcam, ab16728, 1:2000), anti-ERG (Abcam, ab133264, 1:500) and anti-CD146 (Abcam, ab75769 EPR3208, 1:500). Images were acquired using a 3D Histech Panoramic Flash II 250 scanner at 20× magnification (resolution 0.24<italic>μ</italic>m/pixel). The cores were annotated at patient-level by expert uro-pathologists with binary labels for overall survival status (0: alive/censored, 1: prostate cancer related death) and disease progression status (0: no recurrence, 1: recurrence). Clinical follow-up was recorded at a per-patient basis, with a maximum follow-up time of up to 12 years. For both the survival and disease progression clinical endpoints, the available data were imbalanced in terms of class distributions. Access information is possible upon request to the corresponding authors. The distribution of cores per clinical endpoint for the EMPaCT dataset is summarized in <xref ref-type="table" rid="T1">Table 1</xref>.</p></sec><sec id="S13"><title>Prostate cancer WSIs</title><p id="P25">Primary stage prostate cancer FFPE tissue sections (4 <italic>μ</italic>m) were deparaffinized and used for heat-mediated antigen retrieval (citrate buffer, pH 6, Vector Labs). Sections were blocked for 10 minutes in 3% H2O2, followed by 30 minute room temperature incubation in 1% BSA in PBS–0.1%Tween 20. The following primary antibodies were used: anti-CD146 (Abcam, ab75769 EPR3208, 1:500), anti-AR (Abcam, ab133273, EPR1535, 1:100) and anti-NKX3.1 (Cell Signaling, 83700T, 1:200). Secondary anti-rabbit antibody Envision HRP (DAKO, Agilent Technologies, Basel, Switzerland) for 30 minutes was used and signal detection was done using AEC substrate (DAKO, Agilent Technologies, Basel, Switzerland). Sections were counterstained with Hematoxylin and mounted with Aquatex). Images were acquired using a 3D Histech Panoramic Flash II 250 scanner at 20× magnification (resolution 0.24<italic>μ</italic>m/pixel).</p></sec><sec id="S14"><title>SICAP</title><p id="P26">The dataset contains 155 H&amp;E-stained WSIs from needle biopsies taken from 95 patients, split in 18,783 patches of size 512×512 [<xref ref-type="bibr" rid="R43">43</xref>]. The WSIs were reconstructed by stitching the patches. The WSIs were scanned at 40× magnification by Ventana iScan Coreo scanner and downsampled to 10× magnification. The WSIs were annotated by expert uro-pathologists for Gleason grades at the Hospital Clínico of Valencia, Spain.</p></sec><sec id="S15"><title>PANDA</title><p id="P27">The dataset includes 5,759 H&amp;E-stained needle biopsies from 1,243 patients at the Radboud University Medical Center, Netherlands [<xref ref-type="bibr" rid="R56">56</xref>], and 5,662 H&amp;E stained needle biopsies from 1,222 patients at various hospitals in Stockholm, Sweden [<xref ref-type="bibr" rid="R57">57</xref>]. The slides from Radboud were scanned with a 3D Histech Panoramic Flash II 250 scanner at 20× magnification (resolution 0.24<italic>μ</italic>m/pixel) and were downsampled to 10×. The slides from Sweden were scanned with a Hamamatsu C9600-12 and an Aperio Scan Scope AT2 scanner at 10× magnification with a pixel resolution of 0.45202<italic>μ</italic>m and 0.5032<italic>μ</italic>m, respectively. The Gleason grades of the biopsies were annotated by expert uro-pathologists and were released as part of the Prostate cANcer graDe Assessment (PANDA) challenge [<xref ref-type="bibr" rid="R44">44</xref>]. We removed the noisy and inconspicuously labeled biopsies from the dataset, resulting in 4,564 and 4,988 biopsies from the Radboud and the Swedish cohorts, respectively (9,552 biopsies in total). The distribution of WSIs across Gleason grades for both SICAP and PANDA datasets are shown in <xref ref-type="table" rid="T2">Table 2</xref>.</p></sec><sec id="S16"><title>PDAC</title><p id="P28">The PDAC TMA contained cancer tissue of 117 (50 female, 67 male) PDAC cases resected in a curative setting at the Department of Visceral Surgery of Inselspital Bern and diagnosed at Institute of Tissue Medicine and Pathology (ITMP) of the University of Bern between the years 2014 and 2020. The study followed the guidelines of the World Medical Association Declaration of Helsinki 1964, updated in October 2013, and was conducted after approval by the Ethics Committees of Bern (CEC ID2020-00498). All participants provided written general consent. The TMA contained three spots from each case (tumor front, tumor center, tumor stroma), leading to a total number of 351 tissue spots. Thirteen of these 117 cases were treated by neoadjuvant chemotherapy followed by surgical resection and adjuvant therapy, and the majority of the cases (104) were resected curatively and received adjuvant therapy. All cases were characterized comprehensively clinico-pathologically, including Tumour, Node, and Metastasis (TNM) stage during a master thesis of student Jessica Lisa Rohrbach at ITMP, supervised by Martin Wartenberg. All cases were UICC tumor stage I, stage II or stage III cases on pathologic examination, according to the Union for International Cancer Control (UICC) TNM Classification of Malignant Tumours, 8th edition [<xref ref-type="bibr" rid="R58">58</xref>]; the TMA did not comprise of UICC tumor stage IV cases. In all our analysis including the TNM prediction (<xref ref-type="fig" rid="F7">Figure 7D</xref>), we excluded the thirteen neoadjuvant cases and considered only the 104 cases that received adjuvant therapy. The distribution of cores across the three TNM stages is reported in <xref ref-type="table" rid="T3">Table 3</xref>.</p></sec><sec id="S17"><title>TCGA</title><p id="P29">The dataset includes example H&amp;E WSIs from breast (BRCA) and colorectal cancer (CRC) from The Cancer Genome Atlas (TCGA), available at the GDC data portal (<ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov">https://portal.gdc.cancer.gov</ext-link>).</p></sec></sec><sec id="S18"><title>Data Preprocessing</title><p id="P30">For all datasets used, we followed a tissue region detection and patch extraction preprocessing procedure. Specifically, the tissue region was segmented using the preprocessing tools in the HistoCartography library [<xref ref-type="bibr" rid="R59">59</xref>]. A binary tissue mask denoting the tissue and non-tissue regions was computed for each downsampled input image by iteratively applying Gaussian smoothing and Otsu thresholding until the mean of non-tissue pixels was below a threshold. The estimated contours of the denoted tissue and the cavities of tissue were then filtered depending on their area to generate the final segmentation mask. Subsequently, non-overlapping patches of size 256×256 were extracted from 10× magnification using the segmentation contours. The extracted H&amp;E and IHC patches of the EMPaCT dataset were used for training and internal validation of the VirtualMultiplexer. The extracted H&amp;E patches of all other unseen datasets (prostate cancer WSIs, SICAP, PANDA, PDAC, TCGA) were additionally first stain-normalized to mitigate the staining appearance variability with respect to the EMPaCT TMAs [<xref ref-type="bibr" rid="R60">60</xref>].</p></sec><sec id="S19"><title>VirtualMultiplexer Architecture</title><p id="P31">The VirtualMultiplexer is a generative AI toolkit that performs unpaired H&amp;E-to-IHC translation. An overview of the model’s architecture is shown in <xref ref-type="fig" rid="F2">Figure 2A</xref>. The VirtualMultiplexer is trained using two sets of images: source H&amp;E images, denoted as <inline-formula><mml:math id="M7"><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, and target IHC images, denoted as <inline-formula><mml:math id="M8"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. <italic>X</italic><sub>img</sub> and <italic>Y<sub>img</sub></italic> are unpaired images that originate from <bold>different sections of the same TMA core</bold> and thus belong to the same patient, but are pixel-wise unaligned, and thus unpaired. We train an independent one-to-one VirtualMultiplexer model for <bold>each IHC marker</bold> at a time. To train the VirtualMultiplexer, we use patches <italic>X<sub>p</sub></italic> = {<italic>x<sub>p</sub></italic> ∈ <italic>X</italic><sub>img</sub>} and <italic>Y<sub>p</sub></italic> = {<italic>y<sub>p</sub></italic> ∈ <italic>Y</italic><sub>img</sub>} extracted from a pair of images <italic>X</italic><sub>img</sub> and <italic>Y</italic><sub>img</sub>, respectively. The backbone of the VirtualMultiplexer is a GAN-based generator <italic>G</italic>, specifically a Contrastive Unpaired Translation (CUT) [<xref ref-type="bibr" rid="R33">33</xref>] model, that consists of two sequential components, an encoder <italic>G</italic><sub>enc</sub> and a decoder <italic>G</italic><sub>dec</sub>. Upon training, the generator takes as input a patch <italic>x<sub>p</sub></italic> and generates a virtual patch <inline-formula><mml:math id="M9"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>, <italic>i.e</italic>., <inline-formula><mml:math id="M10"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mtext>dec</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mtext>enc</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula>. Finally, the virtually generated patches are stitched together to produce a final virtual image <inline-formula><mml:math id="M11"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="script">Y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>. The VirtualMultiplexer is trained under the supervision of three levels of consistency objectives, namely local, neighborhood and global consistency (<xref ref-type="fig" rid="F2">Figure 2B</xref>). The neighborhood consistency enforces effective staining translation at a patch-level, where a patch captures the neighborhood of a cell. We introduce additional global and local consistency objectives, operating at an image-level and cell-level, respectively, to further constrain the unpaired S2S translation and alleviate the stain-specific inconsistencies.</p><sec id="S20"><title>Neighborhood consistency</title><p id="P32">The neighborhood objective is a combination of an adversarial loss and a patch-wise multilayer contrastive loss, implemented as previously described in CUT [<xref ref-type="bibr" rid="R33">33</xref>] (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 1). Briefly, the adversarial loss dictates the model to learn to eliminate style differences between real and virtual patches, and the multilayer contrastive loss guarantees the content preservation at patch-level [<xref ref-type="bibr" rid="R61">61</xref>]. The adversarial loss is a standard GAN min-max loss [<xref ref-type="bibr" rid="R34">34</xref>], where the discriminator D takes as input real IHC patches <italic>Y<sub>p</sub></italic> and IHC patches <inline-formula><mml:math id="M12"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> virtually generated by generator G and attempts to classify them as either real or virtual (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 1a). It is calculated as follows:
<disp-formula id="FD1"><label>(1)</label><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>adv </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P33">The patch-wise multilayer contrastive loss follows a noise contrastive estimation (NCE) concept as presented in [<xref ref-type="bibr" rid="R61">61</xref>, <xref ref-type="bibr" rid="R62">62</xref>] and reused in [<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. Specifically, it aims to maximize the resemblance between input H&amp;E patch <italic>x<sub>p</sub></italic> ∈ <italic>X<sub>p</sub></italic> and corresponding virtually synthesized IHC patch <inline-formula><mml:math id="M14"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 1b). We first extract a <italic>query</italic> sub-patch <inline-formula><mml:math id="M15"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>sp</mml:mi></mml:msub></mml:math></inline-formula> of size 64×64 from the target IHC domain patch <inline-formula><mml:math id="M16"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> (purple square in panel 1b, <xref ref-type="fig" rid="F2">Figure 2B</xref>) and match it to the corresponding sub-patch <italic>x<sub>sp</sub></italic>, <italic>i.e</italic>., a subpatch at the same spatial location as <inline-formula><mml:math id="M17"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>sp</mml:mi></mml:msub></mml:math></inline-formula> but from the H&amp;E source domain patch <italic>x<sub>p</sub></italic> (black square in panel 1b, <xref ref-type="fig" rid="F2">Figure 2B</xref>). Since both sub-patches originate from the exact same tissue neighborhood, we expect that <italic>x<sub>sp</sub></italic> and <inline-formula><mml:math id="M18"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>sp</mml:mi></mml:msub></mml:math></inline-formula> form a positive pair. We also sample <italic>N</italic> sub-patches <inline-formula><mml:math id="M19"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> at different spatial locations from <italic>x<sub>p</sub></italic> (red squares in panel 1b, <xref ref-type="fig" rid="F2">Figure 2B</xref>), and expect that they form dissimilar, negative pairs with <italic>x<sub>sp</sub>.</italic> In a standard contrastive learning scheme, we would map <italic>y<sub>sp</sub>, x<sub>sp</sub>,</italic> and <inline-formula><mml:math id="M20"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> to a <italic>d</italic>-dimensional embedding space <inline-formula><mml:math id="M21"><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> via <italic>G<sub>enc</sub></italic> and project them to a unit sphere, resulting in <italic>v, v</italic><sup>+</sup> and <inline-formula><mml:math id="M22"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively, and then estimate the probability of a positive pair (<italic>v, v</italic><sup>+</sup>) selected over negative pairs <inline-formula><mml:math id="M23"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, ∀n ∈ <italic>N</italic> as a cross-entropy loss with a temperature scaling parameter <italic>τ</italic>:
<disp-formula id="FD2"><label>(2)</label><mml:math id="M24"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>·</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>·</mml:mo><mml:msubsup><mml:mi>v</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P34">Here, we use a variation of the loss in <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>, specifically a <italic>patch-wise multilayer contrastive loss</italic> that extends <inline-formula><mml:math id="M25"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by computing it for feature maps extracted from <italic>L</italic>-layers of <italic>G</italic><sub>enc</sub> [<xref ref-type="bibr" rid="R29">29</xref>, <xref ref-type="bibr" rid="R33">33</xref>]. This is achieved by passing the <italic>L</italic> feature maps of x<sub>p</sub> and <inline-formula><mml:math id="M26"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> through a two-layer multilayer perceptron (MLP) <italic>H<sub>l</sub></italic>, resulting in a stack of features <inline-formula><mml:math id="M27"><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mtext>enc</mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M28"><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mtext>enc</mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mtext>enc</mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>, ∀<italic>l</italic> ∈ {1, 2, …,<italic>L</italic>}, respectively. We also iterate over each spatial location <italic>s</italic> ∈ {1, · · ·, <italic>S<sub>l</sub></italic>} and we leverage all <italic>S<sub>l</sub><bold>\</bold>s</italic> patches as negatives, ultimately resulting in <inline-formula><mml:math id="M29"><mml:msub><mml:msup><mml:mi>z</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, <italic>z<sub>l,s</sub></italic> and <italic>z<sub>l,Sl\s</sub></italic> for the query, positive and negative sub-patches, respectively (purple, black and red boxes in <xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 1b). The final patch-wise multilayer contrastive loss is computed as:
<disp-formula id="FD3"><label>(3)</label><mml:math id="M30"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>contrastive</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:mi>ℒ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>\</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P35">We also employ contrastive loss <inline-formula><mml:math id="M31"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>contrastive</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> on patches <italic>y<sub>p</sub> ∈ Y<sub>p</sub></italic>, a domainspecific version of the identity loss [<xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R64">64</xref>] that prevents the generator G from making unnecessary changes as proposed in [<xref ref-type="bibr" rid="R33">33</xref>]. Finally, the overall neighborhood consistency objective is computed as a weighted sum of the adversarial loss (1) and the multilayer contrastive loss (3) with regularization hyperparameter λ<sub>NCE</sub>:
<disp-formula id="FD4"><label>(4)</label><mml:math id="M32"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>neighborhood </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>adv </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>NCE</mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>contrastive </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>contrastive </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>H</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="S21"><title>Global consistency</title><p id="P36">Inspired by seminal work in neural style transfer [<xref ref-type="bibr" rid="R65">65</xref>], this objective consists of two loss functions, a content loss <inline-formula><mml:math id="M33"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>content</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and a style loss <inline-formula><mml:math id="M34"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>style</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> that together enforce biological consistency both in terms of tissue composition and staining pattern at the image (tile) level (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 2). Since the generated IHC images should be virtually paired to their corresponding input H&amp;E image in terms of tissue composition, the content loss aims to penalize the loss in content between H&amp;E and IHC images at a tile-level. First, real-patches <italic>X<sub>p</sub></italic> and synthesized patches <inline-formula><mml:math id="M35"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula> are stitched to create images <italic>X</italic><sub>img</sub> and <inline-formula><mml:math id="M36"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula>, respectively, and corresponding tiles of size 1024 × 1024 are extracted (boxes in <xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 2), denoted as <italic>X<sub>t</sub></italic> = {<italic>x<sub>t</sub></italic> ∈ <italic>X</italic><sub>img</sub>} and <inline-formula><mml:math id="M37"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, respectively. Then, the tiles are encoded by a pretrained feature extractor <italic>F</italic>, specifically VGG16 [<xref ref-type="bibr" rid="R35">35</xref>] pretrained on ImageNet [<xref ref-type="bibr" rid="R66">66</xref>]. The tile-level content loss at layer <italic>l</italic> of <italic>F</italic> is calculated as:
<disp-formula id="FD5"><label>(5)</label><mml:math id="M38"><mml:mrow><mml:msubsup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>content </mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mo>·</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mi>l</mml:mi></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P37">where, <italic>h, w</italic> and <italic>c</italic> are height, width, and channel dimensions of the feature map at <italic>l</italic><sup>th</sup> layer, respectively.</p><p id="P38">The style loss utilizes the synthesized image <inline-formula><mml:math id="M39"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula> and the available real image <italic>Y</italic><sub>img</sub> to match the style or overall staining distribution between real and virtual IHC images. Since <inline-formula><mml:math id="M40"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula> and <italic>Y</italic><sub>img</sub> do not have pixel-wise correspondence, large tiles <inline-formula><mml:math id="M41"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula> and <italic>Y<sub>t</sub></italic> = {<italic>y<sub>t</sub></italic> ∈ <italic>Y</italic><sub>img</sub>} are extracted at random such that each tile incorporates sufficient staining distribution. Next, <inline-formula><mml:math id="M42"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and <italic>Y<sub>t</sub></italic> are processed by <italic>F</italic> to produce feature maps across multiple layers. The style loss is computed as:
<disp-formula id="FD6"><label>(6)</label><mml:math id="M43"><mml:mrow><mml:msubsup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>style </mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mi mathvariant="script">G</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi>F</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P39">where <inline-formula><mml:math id="M44"><mml:mi mathvariant="script">G</mml:mi></mml:math></inline-formula> is the Gram matrix that measures the correlation between all the style in a feature map. The denominator is a normalization term which compensates for the under- or over-stylization of the tiles in a batch [<xref ref-type="bibr" rid="R67">67</xref>]. The overall global consistency loss is computed as:
<disp-formula id="FD7"><label>(7)</label><mml:math id="M45"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>global </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>content </mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>content </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>content </mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>style </mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>l</mml:mi><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mtext>style </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msubsup><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>style </mml:mtext></mml:mrow><mml:mi>l</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P40">where <italic>L</italic><sub>content</sub> and <italic>L</italic><sub>style</sub> are the lists of the content and style layers of <italic>F</italic>, respectively, used to extract the feature matrices, and λ<sub>content</sub> and λ<sub>style</sub> are regularization hyperparameters for the respective loss terms.</p></sec><sec id="S22"><title>Local consistency</title><p id="P41">The local consistency objective aims to enforce biological consistency at a local, cell-level and consists of two loss terms, namely a cell discriminator loss <inline-formula><mml:math id="M46"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and a cell classification loss <inline-formula><mml:math id="M47"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellClass</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 3). The cell discriminator loss is inspired by [<xref ref-type="bibr" rid="R26">26</xref>], and uses the cell discriminator <italic>D</italic><sub>cell</sub> to identify whether <italic>a cell</italic> is real or virtual, in the same way that the patch discriminator of <xref ref-type="disp-formula" rid="FD1">Eq.(1)</xref> attempts to classify <italic>patches</italic> as real or virtual. <inline-formula><mml:math id="M48"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> takes as input a real (<italic>Y<sub>p</sub></italic>) and a virtual <inline-formula><mml:math id="M49"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> target patch and their corresponding cell masks (<italic>M<sub>Y<sub>p</sub></sub></italic> and <inline-formula><mml:math id="M50"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>, respectively), which include bounding box demarcation around the cells (<xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 3). <italic>D</italic><sub>cell</sub> comprises a feature extractor followed by a RoIAlign layer [<xref ref-type="bibr" rid="R68">68</xref>], and a final discriminator. The goal of <italic>D</italic><sub>cell</sub> is to output, <italic>D</italic><sub>cell</sub>(<italic>Y<sub>p</sub>,M<sub>Y<sub>p</sub></sub></italic>) → <bold>1</bold>, and <inline-formula><mml:math id="M51"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula>, where <bold>1</bold> and <bold>0</bold> indicate real and virtual cells (indicated in black and purple, respectively, in <xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 3). The cell discriminator loss is defined as:
<disp-formula id="FD8"><label>(8)</label><mml:math id="M52"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P42">Although <italic>D</italic><sub>cell</sub> aims to enforce the generation of realistically looking cells, it is agnostic to their marker expression, as it does not explicitly capture which cells have a positive or a negative staining status. To account for this, we introduce an additional loss via a classifier <italic>F</italic><sub>cell</sub> that is trained to explicitly predict the cell staining status. This is achieved with the help of cell labels <inline-formula><mml:math id="M53"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>C<sub>Y<sub>p</sub></sub></italic>, <italic>i. e</italic>., binary variables depicting the positive or negative staining status of a cell (indicated as 1: yellow and 0: blue boxes in <xref ref-type="fig" rid="F2">Figure 2B</xref>, panel 3). The computation of cell masks and labels is described in detail in section <bold>Cell masking and labeling of IHC images</bold>. The cell-level classification loss can be easily computed as cross-entropy loss, calculated as:
<disp-formula id="FD9"><label>(9)</label><mml:math id="M54"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellClass </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mo>𝟙</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>×</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>×</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mo>{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mo>𝟙</mml:mo><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>×</mml:mo><mml:mi>log</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>×</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P43">where, |<italic>C<sub>y<sub>p</sub></sub></italic>| and <inline-formula><mml:math id="M55"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> are the number of cells in <italic>y<sub>p</sub></italic> and <inline-formula><mml:math id="M56"><mml:msub><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>p</mml:mi></mml:msub></mml:math></inline-formula>, respectively, <inline-formula><mml:math id="M57"><mml:mrow><mml:msub><mml:mn>𝟙</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the indicator function and <italic>p</italic>(.) is the cell-level probabilities predicted by <italic>F</italic><sub>cell</sub>.</p><p id="P44">The overall local consistency loss is computed as:
<disp-formula id="FD10"><label>(10)</label><mml:math id="M58"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>local </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>cellDisc </mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>​​​​​​​​​​​​​                                        </mml:mtext><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mtext>cellClass </mml:mtext></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellClass </mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>F</mml:mi><mml:mrow><mml:mtext>cell </mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mi>p</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msubsup><mml:mi>Y</mml:mi><mml:mi>p</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P45">where λ<sub>cellDisc</sub> and λ<sub>cellClass</sub> are the regularization hyperparameteres for the cell discriminator and classification loss terms, respectively. Importantly, the local consistency loss can be easily generalized to any other cellular or tissue component (<italic>e.g</italic>., nuclei, glands) that might be relevant to other S2S translation problems, provided that corresponding masks and labels are available.</p><p id="P46">The complete objective function for optimizing VirtualMultiplexer is given as,
<disp-formula id="FD11"><label>(11)</label><mml:math id="M59"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>VirtualMultiplexer </mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>neighborhood </mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>global </mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>local </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="S23"><title>Cell masking and labeling of IHC images</title><p id="P47">As already discussed, the local consistency loss of <xref ref-type="disp-formula" rid="FD11">Eq.11</xref> needs as input cell masks <italic>M<sub>X<sub>p</sub></sub>, M<sub>Y<sub>p</sub></sub></italic> and cell labels <italic>C<sub>X<sub>p</sub></sub></italic>, <italic>C<sub>Y<sub>p</sub></sub></italic>. However, acquiring these inputs manually for all patches across all antibodies is practically prohibitive, even for relatively small datasets. Automatic nuclei segmentation/detection using pretrained models (<italic>e.g</italic>., HoVerNet [<xref ref-type="bibr" rid="R69">69</xref>]) is a standard task for H&amp;E images, but no such model exists for IHC images. To circumvent this challenge, we use an attractive property of the VirtualMultiplexer, <italic>i. e</italic>., its ability to synthesize virtual images that are pixel-wise aligned <italic>in any direction</italic> between the source and target domain. Specifically, we train a separate instance of the VirtualMultiplexer that performs IHC → H&amp;E translation. The VirtualMultiplexer<sub>IHC→H&amp;E</sub> is trained using neighborhood consistency and global consistency objectives, as previously described. Once trained, it is used to synthesize a <bold>virtual</bold> H&amp;E image <inline-formula><mml:math id="M60"><mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula> from a real IHC image <italic>Y</italic><sub>img</sub>. At this point, we can leverage HoVerNet [<xref ref-type="bibr" rid="R69">69</xref>] to detect cell nuclei on real and virtual H&amp;E images (<italic>X</italic><sub>img</sub> and <inline-formula><mml:math id="M61"><mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula>) and simply transfer the corresponding cell masks (<italic>M</italic><sub><italic>X</italic><sub>img</sub></sub> and <inline-formula><mml:math id="M62"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula>) to their pixel-wise aligned IHC counterparts (<inline-formula><mml:math id="M63"><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula> and <italic>Y</italic><sub>img</sub>, respectively) to acquire <inline-formula><mml:math id="M64"><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>M</italic><sub><italic>Y</italic><sub>img</sub></sub>. This ”trick” eliminates the need to train individual cell detection models for each IHC antibody, and fully automates the cell masking process in the IHC domain. To acquire cell labels <inline-formula><mml:math id="M65"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:msub><mml:msup><mml:mi>Y</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>C</italic><sub><italic>Y</italic><sub>img</sub></sub>, we use only region annotations in <italic>Y</italic><sub>img</sub>, where the experts partially annotated areas as positive or negative stainings in a few representative images. Since IHC stainings are specialized in delineating positive or negative staining status, the annotation was easy and fast, and required approximately 2-3 minutes per image and per antibody marker. We also train cell detectors for the source and target domain, <italic>i. e</italic>., <inline-formula><mml:math id="M66"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell</mml:mtext></mml:mrow><mml:mrow><mml:mtext>source</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="M67"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell</mml:mtext></mml:mrow><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula>, respectively. Provided the annotations, <inline-formula><mml:math id="M68"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell</mml:mtext></mml:mrow><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> is trained as a CNN patch-classifier. The classifier predictions on <italic>Y</italic><sub>img</sub> combined with <italic>M<sub>Y<sub>p</sub></sub></italic> results in <italic>C<sub>Y<sup>p</sup></sub></italic>. The above region predictions on <italic>Y</italic><sub>img</sub> are transferred on to <inline-formula><mml:math id="M69"><mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula>. Afterwards, <inline-formula><mml:math id="M70"><mml:msub><mml:msup><mml:mi>X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mi>img</mml:mi></mml:msub></mml:math></inline-formula> and the transferred annotations are used to train <inline-formula><mml:math id="M71"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mtext>cell</mml:mtext></mml:mrow><mml:mrow><mml:mtext>source</mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> as a CNN patch-classifier. The classifier predictions on <italic>X</italic><sub>img</sub> combined with <italic>M<sub>X<sub>p</sub></sub></italic> results in <italic>C<sub>X<sub>p</sub></sub></italic>.</p></sec><sec id="S24"><title>Implementation and training details</title><p id="P48">The architectural choices of the VirtualMultiplexer were set as follows: <italic>G</italic> is a ResNet [<xref ref-type="bibr" rid="R42">42</xref>] with 9 residual blocks; <italic>D</italic> is a PatchGAN discriminator [<xref ref-type="bibr" rid="R12">12</xref>]; <italic>D</italic><sub>cell</sub> includes four stride-2 feature convolutions followed by a RoIAlign layer and a discrimination layer; and <italic>F</italic><sub>cell</sub> includes four stride-2 feature convolutions and a 2-layer MLP. We use Xavier weight initialization [<xref ref-type="bibr" rid="R70">70</xref>], instance normalization [<xref ref-type="bibr" rid="R71">71</xref>], and a batch size of 1 image. We use Least Square GAN loss [<xref ref-type="bibr" rid="R72">72</xref>] for <inline-formula><mml:math id="M72"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>adv</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula>. The model hyperparameters for the loss terms of the VirtualMultiplexer are set as: λ<sub>NCE</sub> is 1 with temperature <italic>τ</italic> equal to 0.08, λ<sub>content</sub> ∈ {0.01, 0.1}, λ<sub>style</sub> ∈ {5, 10}, λ<sub>cellDisc</sub> ∈ {0.5, 1}, and λ<sub>cellclass</sub> ∈ {0.1, 0.5}. VirtualMultiplexer is optimized for 125 epochs using Adam optimizer [<xref ref-type="bibr" rid="R73">73</xref>] with momentum parameters <italic>β</italic><sub>1</sub> = 0.5 and <italic>β</italic><sub>2</sub> = 0.999. Different learning rates lr are employed for different consistency objectives, i. e., for neighborhood consistency lr<sub><italic>G</italic></sub> and lr<sub><italic>D</italic></sub> is set to 0.0002, for global consistency learning rate lr<sub><italic>G</italic></sub> is chosen from {0.0001, 0.0002}, and for local consistency learning rates lr<sub><italic>D</italic><sub>cell</sub></sub> and lr<sub><italic>F</italic><sub>cell</sub></sub> are chosen from {0.00001, 0.0001, 0.0002}. Among other hyperparameters, the number of tiles extracted per image to compute <inline-formula><mml:math id="M73"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>content</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="M74"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>style</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is set to 8, the content layer in <italic>F</italic> is relu2_2, the style layers are relu1_2, relu2_2, relu3_3, relu4_3, and the number of cells per patch to compute <inline-formula><mml:math id="M75"><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mtext>cellDisc</mml:mtext></mml:mrow></mml:msub></mml:math></inline-formula> is set to 8.</p></sec></sec><sec id="S25"><title>Graph-Transformer Architecture</title><p id="P49">The Graph-Transformer (GT) architecture, proposed by [<xref ref-type="bibr" rid="R41">41</xref>], fuses a Graph Neural Network and a vision transformer (ViT) to process histopathology images. The GNN first operates on a graph-structured representation of a histopathology image, where the nodes and edges of the graph denote patches and inter-patch spatial connectivity,and the nodes encode patch features extracted from a pre-trained deep learning model. Specifically, GT employs a graph convolution layer [<xref ref-type="bibr" rid="R74">74</xref>] to learn contextualized node embeddings through propagating and aggregating neighborhood node information. Subsequently, a ViT layer operates on the contextualized node features, leverages self-attention to weigh the significance of the nodes and aggregates the node information to render an image-level feature representation. Finally, an MLP maps the image-level features to a downstream image label. To note, histopathology images can have different spatial dimensions, therefore their graph-representations can have varying number of nodes. Also, the number of nodes can be very high when operating on giga-pixel sized WSIs. These two factors can potentially hinder the integration of the graph convolution layer to the ViT layer. To address these challenges, GT introduces a mincut pooling layer [<xref ref-type="bibr" rid="R75">75</xref>], which reduces the number of nodes into a fixed number of tokens while preserving the local neighborhood information of the nodes.</p><sec id="S26"><title>Implementation and training details</title><p id="P50">The architecture of GT follows the official implementation on github<sup><xref ref-type="fn" rid="FN1">1</xref></sup>. Each input image was cropped to create a bag of 256×256 non-overlapping patches at 10× magnification and background patches with non-tissue area ¿10% were discarded. The patches were encoded using ResNet50 [<xref ref-type="bibr" rid="R42">42</xref>] model pretrained on ImageNet dataset [<xref ref-type="bibr" rid="R66">66</xref>]. A graph representation was constructed using the patches with a 8-node connectivity pattern. The GT network consisted of one graph convolutional layer, and set the ViT layer configurations as, number of ViT blocks = 3, MLP size = 128, embedding dimension of each patch = 32, and number of multi-head attention = 8. The model hyperparameters were set as: number of clusters in mincut pooling = {50, 100}, Adam optimizer with initial learning rate of {0.0001, 0.00001}, a cosine annealing scheme for scheduling, and a mini-batch size of 8. The GT models were trained for 400 epochs with early stopping.</p></sec></sec></sec><sec id="S27"><title>Method Evaluation</title><sec id="S28"><title>Patch-level evaluation</title><p id="P51">We use the FID score [<xref ref-type="bibr" rid="R76">76</xref>] to compare the distribution of the virtual IHC patches with the distribution of the real IHC patches, as shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. The computation begins with projecting the virtual and the real IHC patches to an embedding space using InceptionV3 [<xref ref-type="bibr" rid="R76">76</xref>] model, pretrained on ImageNet [<xref ref-type="bibr" rid="R66">66</xref>]. The extracted embeddings are used to estimate multivariate normal distributions <inline-formula><mml:math id="M76"><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for real data and <inline-formula><mml:math id="M77"><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mo>∑</mml:mo><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> for virtual data. Finally, the FID score is computed as:
<disp-formula id="FD12"><label>(12)</label><mml:math id="M78"><mml:mrow><mml:mtext>FID</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="italic">Tr</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:msub><mml:mi>Σ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P52">where, <italic>μ<sub>r</sub></italic> and <italic>μ<sub>v</sub></italic> are the feature-wise mean of the real and virtual patches, Σ<sub><italic>r</italic></sub> and Σ<sub><italic>v</italic></sub> are co-variance matrices for the real and virtual embeddings, and <italic>Tr</italic> is the trace function. A lower FID score indicates a lower disparity between the two distributions, thereby a higher staining efficacy of the VirtualMultiplexer. To ensure reproducibility, we ran each model three times with three independent initializations, and computed the mean and standard deviation for each model (barplot height and errorbar in <xref ref-type="fig" rid="F3">Figure 3</xref>). We used a 70%-30% ratio to split the data in train and test sets, respectively. As for each marker a different number of IHC stainings were available in the EMPaCT data, the exact number of cores used per marker are given in <xref ref-type="table" rid="T4">Table 4</xref>:</p></sec><sec id="S29"><title>Image-level evaluation</title><p id="P53">We used a number of downstream classification tasks to assess the discriminative ability of the virtually stained IHC images on the EMPaCT, SICAP, PANDA and PDAC datasets. We further used these tasks to depict the utility of leveraging virtually multiplexed staining in comparison to standalone real H&amp;E, real IHC, and virtual IHC staining. Specifically, provided the aforementioned images, we constructed graph representations as described in Section 8. Subsequently, Graph-Transformers [<xref ref-type="bibr" rid="R41">41</xref>] were trained under uni-modal and multi-modal settings using both real and virtually stained images, and evaluated on held-out independent test dataset. The final classification scores were reported using a weighted F1-metric, where a higher score depicts a better classification performance, thereby a higher discriminative power of the utilized images. As before, we ran each model three times with three independent initializations, and computed the mean and standard deviation for each model (barplot heights and errorbars in <xref ref-type="fig" rid="F6">Figures 6</xref> and <xref ref-type="fig" rid="F7">7</xref>). In all cases, we used a 60%-20%-20% ratio to split the data in train, validation and test sets, respectively. The exact number of train, validation and test samples used per task, marker and training setting in the EMPaCT dataset are given in <xref ref-type="table" rid="T5">Table 5</xref>.</p><p id="P54">For the SICAP, PANDA and PDAC datasets, the exact number of samples used in the train, validation and test splits coincide for all unimodal and multimodal models of <xref ref-type="fig" rid="F7">Figure 7</xref> and are reported in <xref ref-type="table" rid="T6">Table 6</xref>.</p></sec></sec><sec id="S30"><title>Computational Hardware and Software</title><p id="P55">The image datasets were preprocessed on POWER9 CPUs (Central Processing Units) and one NVIDIA Tesla A100 GPU (Graphics Processing Unit) using the Histocar-tography library [<xref ref-type="bibr" rid="R59">59</xref>]. The deep learning models were trained on NVIDIA Tesla P100 GPUs using PyTorch (version 1.13.1) [<xref ref-type="bibr" rid="R77">77</xref>] and PyTorch Geometric (version 2.3.0) [<xref ref-type="bibr" rid="R78">78</xref>]. The entire pipeline was implemented in Python (version 3.9.1).</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures</label><media xlink:href="EMS192487-supplement-Supplementary_Figures.pdf" mimetype="application" mime-subtype="pdf" id="d25aAdGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgments</title><p>We would like to thank Guillaume Jaume, Jannis Born and Mara Graziani for constructive comments, discussions and suggestions. The results published here are in part based upon data generated by the TCGA Research Network: <ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga">https://www.cancer.gov/tcga</ext-link>. This work was supported by the Swiss National Science Foundation (SNSF) Sinergia Grant (CRSII5_202297) to M.A.R. and M.K.dJ. The PDAC TMA construction has taken place at the Translational Research Unit (TRU) Platform of ITMP (<ext-link ext-link-type="uri" xlink:href="https://www.ngtma.com/">https://www.ngtma.com/</ext-link>) in the setting of a grant by the Foundation for Clinical-Experimental Cancer Research Bern to M.W.</p></ack><sec id="S32" sec-type="data-availability"><title>Data Availability</title><p id="P56">The main dataset used to support this study (EMPaCT) has been deposited in Zenodo, together with the prostate cancer WSIs (doi: 10.5281/zenodo.10066853). The SICAP dataset is available at Mendeley data (doi: 10.17632/9xxm58dvs3.1). The PANDA dataset is available at the Kaggle website (<ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/c/prostate-cancer-grade-assessment/data">https://www.kaggle.com/c/prostate-cancer-grade-assessment/data</ext-link>). The TCGA WSIs (BRCA and CRC) are available at the GDC data portal (<ext-link ext-link-type="uri" xlink:href="https://portal.gdc.cancer.gov">https://portal.gdc.cancer.gov</ext-link>). The PDAC dataset is available upon reasonable request from the authors. All clinical data associated with the EMPaCT and PDAC patient cohorts cannot be shared owing to patient-confidentiality obligations.</p></sec><sec id="S33" sec-type="code-availability"><title>Code Availability</title><p id="P57">All source code of the VirtualMultiplexer is available under an open-source license in <ext-link ext-link-type="uri" xlink:href="https://github.com/AI4SCR/VirtualMultiplexer">https://github.com/AI4SCR/VirtualMultiplexer</ext-link>.</p></sec><fn-group><fn id="FN1"><label>1</label><p id="P58">A Graph-Transformer for Whole Slide Image Classification: <ext-link ext-link-type="uri" xlink:href="https://github.com/vkola-lab/tmi2022">https://github.com/vkola-lab/tmi2022</ext-link></p></fn><fn id="FN2" fn-type="con"><p id="P59"><bold>Author contributions</bold></p><p id="P60">P.P. conceived and implemented the model. P.P., A.M. and M.A.R. designed and performed computational analyses. S.K., F.B. and M.R. performed the experiments. S.K., F.B., E.C., M.W. and M.K.d.J. assessed the virtual stainings. P.P., S.K., F.B., A.M. and M.A.R compiled the figures. M.S., M.W. and M.K.d.J. contributed materials for the experiments. P.P., S.K., F.B. and M.A.R. wrote the paper with inputs from all authors. M.K.d.J. and M.A.R. were responsible for the overall planning and supervision of the project.</p></fn><fn id="FN3" fn-type="conflict"><p id="P61"><bold>Competing interests</bold></p><p id="P62">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kashyap</surname><given-names>Aditya</given-names></name><name><surname>Rapsomaniki</surname><given-names>Maria Anna</given-names></name><name><surname>Barros</surname><given-names>Vesna</given-names></name><name><surname>Fomitcheva-Khartchenko</surname><given-names>Anna</given-names></name><name><surname>Martinelli</surname><given-names>Adriano Luca</given-names></name><name><surname>Rodriguez</surname><given-names>Antonio Foncubierta</given-names></name><name><surname>Gabrani</surname><given-names>Maria</given-names></name><name><surname>Rosen-Zvi</surname><given-names>Michal</given-names></name><name><surname>Kaigala</surname><given-names>Govind</given-names></name></person-group><article-title>Quantification of tumor heterogeneity: from data acquisition to metric generation</article-title><source>Trends in Biotechnology</source><year>2022</year><volume>40</volume><issue>6</issue><fpage>647</fpage><lpage>676</lpage><pub-id pub-id-type="pmid">34972597</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>John KC</given-names></name></person-group><article-title>The wonderful colors of the hematoxylin-eosin stain in diagnostic surgical pathology</article-title><source>International journal of surgical pathology</source><year>2014</year><volume>22</volume><issue>1</issue><fpage>12</fpage><lpage>32</lpage><pub-id pub-id-type="pmid">24406626</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Matos</surname><given-names>Leandro Luongo</given-names></name><name><surname>Trufelli</surname><given-names>Damila Cristina</given-names></name><name><surname>De Matos</surname><given-names>Maria Graciela Luongo</given-names></name><name><surname>da Silva Pinhal</surname><given-names>Maria Aparecida</given-names></name></person-group><article-title>Immunohistochemistry as an important tool in biomarkers detection and clinical practice</article-title><source>Biomarker insights</source><year>2010</year><volume>5</volume><elocation-id>BMI–S2185</elocation-id><pub-id pub-id-type="pmcid">PMC2832341</pub-id><pub-id pub-id-type="pmid">20212918</pub-id><pub-id pub-id-type="doi">10.4137/bmi.s2185</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giesen</surname><given-names>Charlotte</given-names></name><name><surname>Wang</surname><given-names>Hao AO</given-names></name><name><surname>Schapiro</surname><given-names>Denis</given-names></name><name><surname>Zivanovic</surname><given-names>Nevena</given-names></name><name><surname>Jacobs</surname><given-names>Andrea</given-names></name><name><surname>Hattendorf</surname><given-names>Bodo</given-names></name><name><surname>Schüffler</surname><given-names>Peter J</given-names></name><name><surname>Grolimund</surname><given-names>Daniel</given-names></name><name><surname>Buhmann</surname><given-names>Joachim M</given-names></name><name><surname>Brandt</surname><given-names>Simone</given-names></name><etal/></person-group><article-title>Highly multiplexed imaging of tumor tissues with subcellular resolution by mass cytometry</article-title><source>Nature methods</source><year>2014</year><volume>11</volume><issue>4</issue><fpage>417</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">24584193</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goltsev</surname><given-names>Yury</given-names></name><name><surname>Samusik</surname><given-names>Nikolay</given-names></name><name><surname>Kennedy-Darling</surname><given-names>Julia</given-names></name><name><surname>Bhate</surname><given-names>Salil</given-names></name><name><surname>Hale</surname><given-names>Matthew</given-names></name><name><surname>Vazquez</surname><given-names>Gustavo</given-names></name><name><surname>Black</surname><given-names>Sarah</given-names></name><name><surname>Nolan</surname><given-names>Garry P</given-names></name></person-group><article-title>Deep profiling of mouse splenic architecture with codex multiplexed imaging</article-title><source>Cell</source><year>2018</year><volume>174</volume><issue>4</issue><fpage>968</fpage><lpage>981</lpage><pub-id pub-id-type="pmcid">PMC6086938</pub-id><pub-id pub-id-type="pmid">30078711</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2018.07.010</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelo</surname><given-names>Michael</given-names></name><name><surname>Bendall</surname><given-names>Sean C</given-names></name><name><surname>Finck</surname><given-names>Rachel</given-names></name><name><surname>Hale</surname><given-names>Matthew B</given-names></name><name><surname>Hitzman</surname><given-names>Chuck</given-names></name><name><surname>Borowsky</surname><given-names>Alexander D</given-names></name><name><surname>Levenson</surname><given-names>Richard M</given-names></name><name><surname>Lowe</surname><given-names>John B</given-names></name><name><surname>Liu</surname><given-names>Scot D</given-names></name><name><surname>Zhao</surname><given-names>Shuchun</given-names></name><etal/></person-group><article-title>Multiplexed ion beam imaging of human breast tumors</article-title><source>Nature medicine</source><year>2014</year><volume>20</volume><issue>4</issue><fpage>436</fpage><lpage>442</lpage><pub-id pub-id-type="pmcid">PMC4110905</pub-id><pub-id pub-id-type="pmid">24584119</pub-id><pub-id pub-id-type="doi">10.1038/nm.3488</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>Sabrina M</given-names></name><name><surname>Asselin-Labat</surname><given-names>Marie-Liesse</given-names></name><name><surname>Nguyen</surname><given-names>Quan</given-names></name><name><surname>Berthelet</surname><given-names>Jean</given-names></name><name><surname>Tan</surname><given-names>Xiao</given-names></name><name><surname>Wimmer</surname><given-names>Verena C</given-names></name><name><surname>Merino</surname><given-names>Delphine</given-names></name><name><surname>Rogers</surname><given-names>Kelly L</given-names></name><name><surname>Naik</surname><given-names>Shalin H</given-names></name></person-group><article-title>Spatial omics and multiplexed imaging to explore cancer biology</article-title><source>Nature methods</source><year>2021</year><volume>18</volume><issue>9</issue><fpage>997</fpage><lpage>1012</lpage><pub-id pub-id-type="pmid">34341583</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillar</surname><given-names>Nir</given-names></name><name><surname>Ozcan</surname><given-names>Aydogan</given-names></name></person-group><article-title>Virtual tissue staining in pathology using machine learning</article-title><source>Expert Review of Molecular Diagnostics</source><year>2022</year><volume>22</volume><issue>11</issue><fpage>987</fpage><lpage>989</lpage><pub-id pub-id-type="pmid">36440487</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>Bijie</given-names></name><name><surname>Yang</surname><given-names>Xilin</given-names></name><name><surname>Li</surname><given-names>Yuzhu</given-names></name><name><surname>Zhang</surname><given-names>Yijie</given-names></name><name><surname>Pillar</surname><given-names>Nir</given-names></name><name><surname>Ozcan</surname><given-names>Aydogan</given-names></name></person-group><article-title>Deep learning-enabled virtual histological staining of biological samples</article-title><source>Light: Science &amp; Applications</source><year>2023</year><volume>12</volume><issue>1</issue><fpage>57</fpage><pub-id pub-id-type="pmcid">PMC9981740</pub-id><pub-id pub-id-type="pmid">36864032</pub-id><pub-id pub-id-type="doi">10.1038/s41377-023-01104-7</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschuchnig</surname><given-names>Maximilian E</given-names></name><name><surname>Oostingh</surname><given-names>Gertie J</given-names></name><name><surname>Gadermayr</surname><given-names>Michael</given-names></name></person-group><article-title>Generative adversarial networks in digital pathology: a survey on trends and future potential</article-title><source>Patterns</source><year>2020</year><volume>1</volume><issue>6</issue><elocation-id>100089</elocation-id><pub-id pub-id-type="pmcid">PMC7660380</pub-id><pub-id pub-id-type="pmid">33205132</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2020.100089</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jose</surname><given-names>Laya</given-names></name><name><surname>Liu</surname><given-names>Sidong</given-names></name><name><surname>Russo</surname><given-names>Carlo</given-names></name><name><surname>Nadort</surname><given-names>Annemarie</given-names></name><name><surname>Di Ieva</surname><given-names>Antonio</given-names></name></person-group><article-title>Generative Adversarial Networks in Digital Pathology and Histopathological Image Processing: A Review</article-title><source>Journal of Pathology Informatics</source><year>2021</year><volume>12</volume><issue>1</issue><fpage>43</fpage><pub-id pub-id-type="pmcid">PMC8609288</pub-id><pub-id pub-id-type="pmid">34881098</pub-id><pub-id pub-id-type="doi">10.4103/jpi.jpi_103_20</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isola</surname><given-names>Phillip</given-names></name><name><surname>Zhu</surname><given-names>Jun-Yan</given-names></name><name><surname>Zhou</surname><given-names>Tinghui</given-names></name><name><surname>Efros</surname><given-names>Alexei A</given-names></name></person-group><source>Image-to-image translation with conditional adversarial networks</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2017</year><fpage>1125</fpage><lpage>1134</lpage></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Jingxi</given-names></name><name><surname>Garfinkel</surname><given-names>Jason</given-names></name><name><surname>Zhang</surname><given-names>Xiaoran</given-names></name><name><surname>Wu</surname><given-names>Di</given-names></name><name><surname>Zhang</surname><given-names>Yijie</given-names></name><name><surname>De Haan</surname><given-names>Kevin</given-names></name><name><surname>Wang</surname><given-names>Hongda</given-names></name><name><surname>Liu</surname><given-names>Tairan</given-names></name><name><surname>Bai</surname><given-names>Bijie</given-names></name><name><surname>Rivenson</surname><given-names>Yair</given-names></name><etal/></person-group><article-title>Biopsy-free in vivo virtual histology of skin using deep learning</article-title><source>Light: Science &amp; Applications</source><year>2021</year><volume>10</volume><issue>1</issue><fpage>233</fpage><pub-id pub-id-type="pmcid">PMC8602311</pub-id><pub-id pub-id-type="pmid">34795202</pub-id><pub-id pub-id-type="doi">10.1038/s41377-021-00674-8</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname><given-names>Yair</given-names></name><name><surname>Liu</surname><given-names>Tairan</given-names></name><name><surname>Wei</surname><given-names>Zhensong</given-names></name><name><surname>Zhang</surname><given-names>Yibo</given-names></name><name><surname>de Haan</surname><given-names>Kevin</given-names></name><name><surname>Ozcan</surname><given-names>Aydogan</given-names></name></person-group><article-title>Phasestain: the digital staining of label-free quantitative phase microscopy images using deep learning</article-title><source>Light: Science &amp; Applications</source><year>2019</year><volume>8</volume><issue>1</issue><fpage>23</fpage><pub-id pub-id-type="pmcid">PMC6363787</pub-id><pub-id pub-id-type="pmid">30728961</pub-id><pub-id pub-id-type="doi">10.1038/s41377-019-0129-y</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname><given-names>Yair</given-names></name><name><surname>Wang</surname><given-names>Hongda</given-names></name><name><surname>Wei</surname><given-names>Zhensong</given-names></name><name><surname>de Haan</surname><given-names>Kevin</given-names></name><name><surname>Zhang</surname><given-names>Yibo</given-names></name><name><surname>Wu</surname><given-names>Yichen</given-names></name><name><surname>Günaydin</surname><given-names>Harun</given-names></name><name><surname>Zuckerman</surname><given-names>Jonathan E</given-names></name><name><surname>Chong</surname><given-names>Thomas</given-names></name><name><surname>Sisk</surname><given-names>Anthony E</given-names></name><etal/></person-group><article-title>Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning</article-title><source>Nature biomedical engineering</source><year>2019</year><volume>3</volume><issue>6</issue><fpage>466</fpage><lpage>477</lpage><pub-id pub-id-type="pmid">31142829</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rana</surname><given-names>Aman</given-names></name><name><surname>Lowe</surname><given-names>Alarice</given-names></name><name><surname>Lithgow</surname><given-names>Marie</given-names></name><name><surname>Horback</surname><given-names>Katharine</given-names></name><name><surname>Janovitz</surname><given-names>Tyler</given-names></name><name><surname>Da Silva</surname><given-names>Annacarolina</given-names></name><name><surname>Tsai</surname><given-names>Harrison</given-names></name><name><surname>Shanmugam</surname><given-names>Vignesh</given-names></name><name><surname>Bayat</surname><given-names>Akram</given-names></name><name><surname>Shah</surname><given-names>Pratik</given-names></name></person-group><article-title>Use of deep learning to develop and analyze computational hematoxylin and eosin staining of prostate core biopsy images for tumor diagnosis</article-title><source>JAMA network open</source><year>2020</year><volume>3</volume><issue>5</issue><fpage>e205111</fpage><pub-id pub-id-type="pmcid">PMC7240356</pub-id><pub-id pub-id-type="pmid">32432709</pub-id><pub-id pub-id-type="doi">10.1001/jamanetworkopen.2020.5111</pub-id></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>Kevin</given-names></name><name><surname>Zhang</surname><given-names>Yijie</given-names></name><name><surname>Zuckerman</surname><given-names>Jonathan E</given-names></name><name><surname>Liu</surname><given-names>Tairan</given-names></name><name><surname>Sisk</surname><given-names>Anthony E</given-names></name><name><surname>Diaz</surname><given-names>Miguel FP</given-names></name><name><surname>Jen</surname><given-names>Kuang-Yu</given-names></name><name><surname>Nobori</surname><given-names>Alexander</given-names></name><name><surname>Liou</surname><given-names>Sofia</given-names></name><name><surname>Zhang</surname><given-names>Sarah</given-names></name><etal/></person-group><article-title>Deep learning-based transformation of h&amp;e stained tissues into special stains</article-title><source>Nature communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>4884</elocation-id><pub-id pub-id-type="pmcid">PMC8361203</pub-id><pub-id pub-id-type="pmid">34385460</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25221-2</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Yijie</given-names></name><name><surname>de Haan</surname><given-names>Kevin</given-names></name><name><surname>Rivenson</surname><given-names>Yair</given-names></name><name><surname>Li</surname><given-names>Jingxi</given-names></name><name><surname>Delis</surname><given-names>Apostolos</given-names></name><name><surname>Ozcan</surname><given-names>Aydogan</given-names></name></person-group><article-title>Digital synthesis of histological stains using micro-structured and multiplexed virtual staining of label-free tissue</article-title><source>Light: Science &amp; Applications</source><year>2020</year><volume>9</volume><issue>1</issue><fpage>78</fpage><pub-id pub-id-type="pmcid">PMC7203145</pub-id><pub-id pub-id-type="pmid">32411363</pub-id><pub-id pub-id-type="doi">10.1038/s41377-020-0315-y</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Shengjie</given-names></name><name><surname>Zhu</surname><given-names>Chuang</given-names></name><name><surname>Xu</surname><given-names>Feng</given-names></name><name><surname>Jia</surname><given-names>Xinyu</given-names></name><name><surname>Shi</surname><given-names>Zhongyue</given-names></name><name><surname>Jin</surname><given-names>Mulan</given-names></name></person-group><source>Bci: Breast cancer immunohistochemical image generation through pyramid pix2pix</source><conf-name>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><year>2022</year><fpage>1815</fpage><lpage>1824</lpage></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Weisi</given-names></name><name><surname>Reder</surname><given-names>Nicholas P</given-names></name><name><surname>Koyuncu</surname><given-names>Can</given-names></name><name><surname>Leo</surname><given-names>Patrick</given-names></name><name><surname>Hawley</surname><given-names>Sarah</given-names></name><name><surname>Huang</surname><given-names>Hongyi</given-names></name><name><surname>Mao</surname><given-names>Chenyi</given-names></name><name><surname>Postupna</surname><given-names>Nadia</given-names></name><name><surname>Kang</surname><given-names>Soyoung</given-names></name><name><surname>Serafin</surname><given-names>Robert</given-names></name><etal/></person-group><article-title>Prostate cancer risk stratification via non-destructive 3d pathology with deep learning-assisted gland analysis</article-title><source>Cancer research</source><year>2022</year><volume>82</volume><issue>2</issue><fpage>334</fpage><pub-id pub-id-type="pmcid">PMC8803395</pub-id><pub-id pub-id-type="pmid">34853071</pub-id><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-21-2843</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghahremani</surname><given-names>Parmida</given-names></name><name><surname>Li</surname><given-names>Yanyun</given-names></name><name><surname>Kaufman</surname><given-names>Arie</given-names></name><name><surname>Vanguri</surname><given-names>Rami</given-names></name><name><surname>Greenwald</surname><given-names>Noah</given-names></name><name><surname>Angelo</surname><given-names>Michael</given-names></name><name><surname>Hollmann</surname><given-names>Travis J</given-names></name><name><surname>Nadeem</surname><given-names>Saad</given-names></name></person-group><article-title>Deep learning-inferred multiplex immunofluorescence for immunohistochemical image quantification</article-title><source>Nature machine intelligence</source><year>2022</year><volume>4</volume><issue>4</issue><fpage>401</fpage><lpage>412</lpage><pub-id pub-id-type="pmcid">PMC9477216</pub-id><pub-id pub-id-type="pmid">36118303</pub-id><pub-id pub-id-type="doi">10.1038/s42256-022-00471-x</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Ranran</given-names></name><name><surname>Cao</surname><given-names>Yankun</given-names></name><name><surname>Li</surname><given-names>Yujun</given-names></name><name><surname>Liu</surname><given-names>Zhi</given-names></name><name><surname>Wang</surname><given-names>Jianye</given-names></name><name><surname>He</surname><given-names>Jiahuan</given-names></name><name><surname>Zhang</surname><given-names>Chenyang</given-names></name><name><surname>Sui</surname><given-names>Xiaoyu</given-names></name><name><surname>Zhang</surname><given-names>Pengfei</given-names></name><name><surname>Cui</surname><given-names>Lizhen</given-names></name><etal/></person-group><article-title>Mvfstain: Multiple virtual functional stain histopathology images generation based on specific domain mapping</article-title><source>Medical Image Analysis</source><year>2022</year><volume>80</volume><elocation-id>102520</elocation-id><pub-id pub-id-type="pmid">35810588</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mercan</surname><given-names>Caner</given-names></name><name><surname>Mooij</surname><given-names>GCAM</given-names></name><name><surname>Tellez</surname><given-names>David</given-names></name><name><surname>Lotz</surname><given-names>Johannes</given-names></name><name><surname>Weiss</surname><given-names>Nick</given-names></name><name><surname>van Gerven</surname><given-names>Marcel</given-names></name><name><surname>Ciompi</surname><given-names>Francesco</given-names></name></person-group><source>Virtual staining for mitosis detection in breast histopathology</source><conf-name>2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI)</conf-name><conf-sponsor>IEEE</conf-sponsor><year>2020</year><fpage>1770</fpage><lpage>1774</lpage></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lahiani</surname><given-names>Amal</given-names></name><name><surname>Klaman</surname><given-names>Irina</given-names></name><name><surname>Navab</surname><given-names>Nassir</given-names></name><name><surname>Albarqouni</surname><given-names>Shadi</given-names></name><name><surname>Klaiman</surname><given-names>Eldad</given-names></name></person-group><article-title>Seamless virtual whole slide image synthesis and validation using perceptual embedding consistency</article-title><source>IEEE Journal of Biomedical and Health Informatics</source><year>2020</year><volume>25</volume><issue>2</issue><fpage>403</fpage><lpage>411</lpage><pub-id pub-id-type="pmid">32086223</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Shuting</given-names></name><name><surname>Zhang</surname><given-names>Baochang</given-names></name><name><surname>Liu</surname><given-names>Yiqing</given-names></name><name><surname>Han</surname><given-names>Anjia</given-names></name><name><surname>Shi</surname><given-names>Huijuan</given-names></name><name><surname>Guan</surname><given-names>Tian</given-names></name><name><surname>He</surname><given-names>Yonghong</given-names></name></person-group><article-title>Unpaired stain transfer using pathology-consistent constrained generative adversarial networks</article-title><source>IEEE transactions on medical imaging</source><year>2021</year><volume>40</volume><issue>8</issue><fpage>1977</fpage><lpage>1989</lpage><pub-id pub-id-type="pmid">33784619</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boyd</surname><given-names>Joseph</given-names></name><name><surname>Villa</surname><given-names>Irène</given-names></name><name><surname>Mathieu</surname><given-names>Marie-Christine</given-names></name><name><surname>Deutsch</surname><given-names>Eric</given-names></name><name><surname>Paragios</surname><given-names>Nikos</given-names></name><name><surname>Vakalopoulou</surname><given-names>Maria</given-names></name><name><surname>Christodoulidis</surname><given-names>Stergios</given-names></name></person-group><article-title>Region-guided cyclegans for stain transfer in whole slide images</article-title><source>Medical Image Computing and Computer Assisted Intervention (MICCAI)</source><publisher-name>Springer</publisher-name><year>2022</year><fpage>356</fpage><lpage>365</lpage></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Yiyang</given-names></name><name><surname>Zeng</surname><given-names>Bowei</given-names></name><name><surname>Wang</surname><given-names>Yifeng</given-names></name><name><surname>Chen</surname><given-names>Yang</given-names></name><name><surname>Fang</surname><given-names>Zijie</given-names></name><name><surname>Zhang</surname><given-names>Jian</given-names></name><name><surname>Ji</surname><given-names>Xiangyang</given-names></name><name><surname>Wang</surname><given-names>Haoqian</given-names></name><name><surname>Zhang</surname><given-names>Yongbing</given-names></name></person-group><source>Unpaired multi-domain stain transfer for kidney histopathological images</source><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><year>2022</year><volume>36</volume><fpage>1630</fpage><lpage>1637</lpage></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouteldja</surname><given-names>Nassim</given-names></name><name><surname>Klinkhammer</surname><given-names>Barbara M</given-names></name><name><surname>Schlaich</surname><given-names>Tarek</given-names></name><name><surname>Boor</surname><given-names>Peter</given-names></name><name><surname>Merhof</surname><given-names>Dorit</given-names></name></person-group><article-title>Improving unsupervised stain-to-stain translation using self-supervision and meta-learning</article-title><source>Journal of Pathology Informatics</source><year>2022</year><volume>13</volume><elocation-id>100107</elocation-id><pub-id pub-id-type="pmcid">PMC9577059</pub-id><pub-id pub-id-type="pmid">36268068</pub-id><pub-id pub-id-type="doi">10.1016/j.jpi.2022.100107</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ozyoruk</surname><given-names>Kutsev Bengisu</given-names></name><name><surname>Can</surname><given-names>Sermet</given-names></name><name><surname>Darbaz</surname><given-names>Berkan</given-names></name><name><surname>Başak</surname><given-names>Kayhan</given-names></name><name><surname>Demir</surname><given-names>Derya</given-names></name><name><surname>Gokceler</surname><given-names>Guliz Irem</given-names></name><name><surname>Serin</surname><given-names>Gurdeniz</given-names></name><name><surname>Hacisalihoglu</surname><given-names>Uguray Payam</given-names></name><name><surname>Kurtuluş</surname><given-names>Emirhan</given-names></name><name><surname>Lu</surname><given-names>Ming Y</given-names></name><etal/></person-group><article-title>A deep-learning model for transforming the style of tissue images from cryosectioned to formalin-fixed and paraffin-embedded</article-title><source>Nature Biomedical Engineering</source><year>2022</year><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">36564629</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>Jun-Yan</given-names></name><name><surname>Park</surname><given-names>Taesung</given-names></name><name><surname>Isola</surname><given-names>Phillip</given-names></name><name><surname>Efros</surname><given-names>Alexei A</given-names></name></person-group><source>Unpaired image-to-image translation using cycle-consistent adversarial networks</source><conf-name>Proceedings of the IEEE international conference on computer vision</conf-name><year>2017</year><fpage>2223</fpage><lpage>2232</lpage></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>Bowei</given-names></name><name><surname>Lin</surname><given-names>Yiyang</given-names></name><name><surname>Wang</surname><given-names>Yifeng</given-names></name><name><surname>Chen</surname><given-names>Yang</given-names></name><name><surname>Dong</surname><given-names>Jiuyang</given-names></name><name><surname>Li</surname><given-names>Xi</given-names></name><name><surname>Zhang</surname><given-names>Yongbing</given-names></name></person-group><article-title>Semi-supervised pr virtual staining for breast histopathological images</article-title><source>Medical Image Computing and Computer Assisted Intervention (MICCAI)</source><year>2022</year><fpage>232</fpage><lpage>241</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borji</surname><given-names>Ali</given-names></name></person-group><article-title>Pros and cons of gan evaluation measures: New developments</article-title><source>Computer Vision and Image Understanding</source><year>2022</year><volume>215</volume><elocation-id>103329</elocation-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>Taesung</given-names></name><name><surname>Efros</surname><given-names>Alexei A</given-names></name><name><surname>Zhang</surname><given-names>Richard</given-names></name><name><surname>Zhu</surname><given-names>Jun-Yan</given-names></name></person-group><article-title>Contrastive learning for unpaired image-to-image translation</article-title><source>ECCV</source><year>2020</year><fpage>319</fpage><lpage>345</lpage></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>Ian</given-names></name><name><surname>Pouget-Abadie</surname><given-names>Jean</given-names></name><name><surname>Mirza</surname><given-names>Mehdi</given-names></name><name><surname>Xu</surname><given-names>Bing</given-names></name><name><surname>Warde-Farley</surname><given-names>David</given-names></name><name><surname>Ozair</surname><given-names>Sherjil</given-names></name><name><surname>Courville</surname><given-names>Aaron</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group><article-title>Generative adversarial nets</article-title><source>Advances in neural information processing systems</source><year>2014</year><volume>27</volume></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>Karen</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></person-group><source>Very deep convolutional networks for large-scale image recognition</source><conf-name>International Conference on Learning Representations (ICLR</conf-name><year>2014</year></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Briganti</surname><given-names>Alberto</given-names></name><name><surname>Joniau</surname><given-names>Steven</given-names></name><name><surname>Gontero</surname><given-names>Paolo</given-names></name><name><surname>Abdollah</surname><given-names>Firas</given-names></name><name><surname>Passoni</surname><given-names>Niccolò M</given-names></name><name><surname>Tombal</surname><given-names>Bertrand</given-names></name><name><surname>Marchioro</surname><given-names>Giansilvio</given-names></name><name><surname>Kneitz</surname><given-names>Burkhard</given-names></name><name><surname>Walz</surname><given-names>Jochen</given-names></name><name><surname>Frohneberg</surname><given-names>Detlef</given-names></name><etal/></person-group><article-title>Identifying the best candidate for radical prostatectomy among patients with high-risk prostate cancer</article-title><source>European urology</source><year>2012</year><volume>61</volume><issue>3</issue><fpage>584</fpage><lpage>592</lpage><pub-id pub-id-type="pmid">22153925</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kneitz</surname><given-names>Burkhard</given-names></name><name><surname>Krebs</surname><given-names>Markus</given-names></name><name><surname>Kalogirou</surname><given-names>Charis</given-names></name><name><surname>Schubert</surname><given-names>Maria</given-names></name><name><surname>Joniau</surname><given-names>Steven</given-names></name><name><surname>van Poppel</surname><given-names>Hein</given-names></name><name><surname>Lerut</surname><given-names>Evelyne</given-names></name><name><surname>Kneitz</surname><given-names>Susanne</given-names></name><name><surname>Scholz</surname><given-names>Claus Jürgen</given-names></name><name><surname>Ströbel</surname><given-names>Philipp</given-names></name><etal/></person-group><article-title>Survival in patients with high-risk prostate cancer is predicted by mir-221, which regulates proliferation, apoptosis, and invasion of prostate cancer cells by inhibiting irf2 and socs3</article-title><source>Cancer research</source><year>2014</year><volume>74</volume><issue>9</issue><fpage>2591</fpage><lpage>2603</lpage><pub-id pub-id-type="pmid">24607843</pub-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tosco</surname><given-names>Lorenzo</given-names></name><name><surname>Laenen</surname><given-names>Annouschka</given-names></name><name><surname>Briganti</surname><given-names>Alberto</given-names></name><name><surname>Gontero</surname><given-names>Paolo</given-names></name><name><surname>Jeffrey Karnes</surname><given-names>R</given-names></name><name><surname>Bastian</surname><given-names>Patrick J</given-names></name><name><surname>Chlosta</surname><given-names>Piotr</given-names></name><name><surname>Claessens</surname><given-names>Frank</given-names></name><name><surname>Chun</surname><given-names>Felix K</given-names></name><name><surname>Everaerts</surname><given-names>Wouter</given-names></name><etal/></person-group><article-title>The empact classifier: a validated tool to predict postoperative prostate cancer-related death using competing-risk analysis</article-title><source>European urology focus</source><year>2018</year><volume>4</volume><issue>3</issue><fpage>369</fpage><lpage>375</lpage><pub-id pub-id-type="pmid">28753838</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>Ming-Yang</given-names></name><name><surname>Wu</surname><given-names>Min-Sheng</given-names></name><name><surname>Wu</surname><given-names>Che-Ming</given-names></name></person-group><source>Ultra-high-resolution unpaired stain transformation via kernelized instance normalization</source><conf-name>European Conference on Computer Vision</conf-name><publisher-name>Springer</publisher-name><year>2022</year><fpage>490</fpage><lpage>505</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heusel</surname><given-names>Martin</given-names></name><name><surname>Ramsauer</surname><given-names>Hubert</given-names></name><name><surname>Unterthiner</surname><given-names>Thomas</given-names></name><name><surname>Nessler</surname><given-names>Bernhard</given-names></name><name><surname>Hochreiter</surname><given-names>Sepp</given-names></name></person-group><article-title>Gans trained by a two time-scale update rule converge to a local nash equilibrium</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>Yi</given-names></name><name><surname>Gindra</surname><given-names>Rushin H</given-names></name><name><surname>Green</surname><given-names>Emily J</given-names></name><name><surname>Burks</surname><given-names>Eric J</given-names></name><name><surname>Betke</surname><given-names>Margrit</given-names></name><name><surname>Beane</surname><given-names>Jennifer E</given-names></name><name><surname>Kolachalama</surname><given-names>Vijaya B</given-names></name></person-group><article-title>A graph-transformer for whole slide image classification</article-title><source>IEEE transactions on medical imaging</source><year>2022</year><volume>41</volume><issue>11</issue><fpage>3003</fpage><lpage>3015</lpage><pub-id pub-id-type="pmcid">PMC9670036</pub-id><pub-id pub-id-type="pmid">35594209</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2022.3176598</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group><source>Deep residual learning for image recognition</source><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silva-Rodríguez</surname><given-names>Julio</given-names></name><name><surname>Colomer</surname><given-names>Adrián</given-names></name><name><surname>Sales</surname><given-names>María</given-names></name><name><surname>Molina</surname><given-names>Rafael</given-names></name><name><surname>Naranjo</surname><given-names>Valery</given-names></name></person-group><article-title>Going deeper through the gleason scoring scale: An automatic end-to-end system for histology prostate grading and cribriform pattern detection</article-title><source>Computer Methods and Programs in Biomedicine</source><year>2020</year><volume>195</volume><elocation-id>105637</elocation-id><pub-id pub-id-type="pmid">32653747</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bulten</surname><given-names>Wouter</given-names></name><name><surname>Kartasalo</surname><given-names>Kimmo</given-names></name><name><surname>Chen</surname><given-names>Po-Hsuan Cameron</given-names></name><name><surname>Ström</surname><given-names>Peter</given-names></name><name><surname>Pinckaers</surname><given-names>Hans</given-names></name><name><surname>Nagpal</surname><given-names>Kunal</given-names></name><name><surname>Cai</surname><given-names>Yuannan</given-names></name><name><surname>Steiner</surname><given-names>David F</given-names></name><name><surname>van Boven</surname><given-names>Hester</given-names></name><name><surname>Vink</surname><given-names>Robert</given-names></name><etal/></person-group><article-title>Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge</article-title><source>Nature medicine</source><year>2022</year><volume>28</volume><issue>1</issue><fpage>154</fpage><lpage>163</lpage><pub-id pub-id-type="pmcid">PMC8799467</pub-id><pub-id pub-id-type="pmid">35027755</pub-id><pub-id pub-id-type="doi">10.1038/s41591-021-01620-2</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Cancer Genome Atlas Network</collab><etal/></person-group><article-title>Comprehensive molecular characterization of human colon and rectal cancer</article-title><source>Nature</source><year>2012</year><volume>487</volume><issue>7407</issue><fpage>330</fpage><pub-id pub-id-type="pmcid">PMC3401966</pub-id><pub-id pub-id-type="pmid">22810696</pub-id><pub-id pub-id-type="doi">10.1038/nature11252</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><article-title>Comprehensive molecular portraits of human breast tumours</article-title><source>Nature</source><year>2012</year><volume>490</volume><issue>7418</issue><fpage>61</fpage><lpage>70</lpage><pub-id pub-id-type="pmcid">PMC3465532</pub-id><pub-id pub-id-type="pmid">23000897</pub-id><pub-id pub-id-type="doi">10.1038/nature11412</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="web"><source>The cancer genome atlas (tcga) research network</source><comment><ext-link ext-link-type="uri" xlink:href="https://www.cancer.gov/tcga">https://www.cancer.gov/tcga</ext-link></comment></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname><given-names>Annika</given-names></name><name><surname>Dawson</surname><given-names>Heather</given-names></name><name><surname>Hammer</surname><given-names>Caroline</given-names></name><name><surname>Perren</surname><given-names>Aurel</given-names></name><name><surname>Lugli</surname><given-names>Alessandro</given-names></name></person-group><article-title>Lean management in the pathology laboratory</article-title><source>Der Pathologe</source><year>2017</year><volume>38</volume><fpage>540</fpage><lpage>544</lpage><pub-id pub-id-type="pmid">29043447</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Bel</surname><given-names>Thomas</given-names></name><name><surname>Hermsen</surname><given-names>Meyke</given-names></name><name><surname>Kers</surname><given-names>Jesper</given-names></name><name><surname>van der Laak</surname><given-names>Jeroen</given-names></name><name><surname>Litjens</surname><given-names>Geert</given-names></name></person-group><article-title>Stain-transforming cycle-consistent generative adversarial networks for improved segmentation of renal histopathology</article-title><source>Medical Imaging with Deep Learning</source><year>2018</year><fpage>151</fpage><lpage>163</lpage></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siller</surname><given-names>Mario</given-names></name><name><surname>Stangassinger</surname><given-names>Lea Maria</given-names></name><name><surname>Kreutzer</surname><given-names>Christina</given-names></name><name><surname>Boor</surname><given-names>Peter</given-names></name><name><surname>Bulow</surname><given-names>Roman D</given-names></name><name><surname>Kraus</surname><given-names>Theo JF</given-names></name><name><surname>Von Stillfried</surname><given-names>Saskia</given-names></name><name><surname>Wolfl</surname><given-names>Soraya</given-names></name><name><surname>Couillard-Despres</surname><given-names>Sebastien</given-names></name><name><surname>Oostingh</surname><given-names>Gertie Janneke</given-names></name><etal/></person-group><article-title>On the acceptance of “fake” histopathology: A study on frozen sections optimized with deep learning</article-title><source>Journal of Pathology Informatics</source><year>2022</year><volume>13</volume><elocation-id>100168</elocation-id><pub-id pub-id-type="pmcid">PMC8794030</pub-id><pub-id pub-id-type="pmid">35136673</pub-id><pub-id pub-id-type="doi">10.4103/jpi.jpi_53_21</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nan</surname><given-names>Yang</given-names></name><name><surname>Del Ser</surname><given-names>Javier</given-names></name><name><surname>Walsh</surname><given-names>Simon</given-names></name><name><surname>Schönlieb</surname><given-names>Carola</given-names></name><name><surname>Roberts</surname><given-names>Michael</given-names></name><name><surname>Selby</surname><given-names>Ian</given-names></name><name><surname>Howard</surname><given-names>Kit</given-names></name><name><surname>Owen</surname><given-names>John</given-names></name><name><surname>Neville</surname><given-names>Jon</given-names></name><name><surname>Guiot</surname><given-names>Julien</given-names></name><etal/></person-group><article-title>Data harmonisation for information fusion in digital healthcare: A state-of-the-art systematic review, meta-analysis and future research directions</article-title><source>Information Fusion</source><year>2022</year><volume>82</volume><fpage>99</fpage><lpage>122</lpage><pub-id pub-id-type="pmcid">PMC8878813</pub-id><pub-id pub-id-type="pmid">35664012</pub-id><pub-id pub-id-type="doi">10.1016/j.inffus.2022.01.001</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vert</surname><given-names>Jean-Philippe</given-names></name></person-group><article-title>How will generative ai disrupt data science in drug discovery?</article-title><source>Nature Biotechnology</source><year>2023</year><fpage>1</fpage><lpage>2</lpage><pub-id pub-id-type="pmid">37156917</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Filiot</surname><given-names>Alexandre</given-names></name><name><surname>Ghermi</surname><given-names>Ridouane</given-names></name><name><surname>Olivier</surname><given-names>Antoine</given-names></name><name><surname>Jacob</surname><given-names>Paul</given-names></name><name><surname>Fidon</surname><given-names>Lucas</given-names></name><name><surname>Kain</surname><given-names>Alice Mac</given-names></name><name><surname>Saillard</surname><given-names>Charlie</given-names></name><name><surname>Schiratti</surname><given-names>Jean-Baptiste</given-names></name></person-group><article-title>Scaling self-supervised learning for histopathology with masked image modeling</article-title><source>medRxiv</source><year>2023</year><fpage>2023</fpage><lpage>07</lpage></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Richard J</given-names></name><name><surname>Ding</surname><given-names>Tong</given-names></name><name><surname>Lu</surname><given-names>Ming Y</given-names></name><name><surname>Williamson</surname><given-names>Drew FK</given-names></name><name><surname>Jaume</surname><given-names>Guillaume</given-names></name><name><surname>Chen</surname><given-names>Bowen</given-names></name><name><surname>Zhang</surname><given-names>Andrew</given-names></name><name><surname>Shao</surname><given-names>Daniel</given-names></name><name><surname>Song</surname><given-names>Andrew H</given-names></name><name><surname>Shaban</surname><given-names>Muhammad</given-names></name><etal/></person-group><article-title>A general-purpose self-supervised model for computational pathology</article-title><source>arXiv:2308.15474</source><year>2023</year></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vorontsov</surname><given-names>Eugene</given-names></name><name><surname>Bozkurt</surname><given-names>Alican</given-names></name><name><surname>Casson</surname><given-names>Adam</given-names></name><name><surname>Shaikovski</surname><given-names>George</given-names></name><name><surname>Zelechowski</surname><given-names>Michal</given-names></name><name><surname>Liu</surname><given-names>Siqi</given-names></name><name><surname>Mathieu</surname><given-names>Philippe</given-names></name><name><surname>van Eck</surname><given-names>Alexander</given-names></name><name><surname>Lee</surname><given-names>Donghun</given-names></name><name><surname>Viret</surname><given-names>Julian</given-names></name><etal/></person-group><article-title>Virchow: A million-slide digital pathology foundation model</article-title><source>arXiv:2309.07778</source><year>2023</year></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bulten</surname><given-names>Wouter</given-names></name><name><surname>Pinckaers</surname><given-names>Hans</given-names></name><name><surname>van Boven</surname><given-names>Hester</given-names></name><name><surname>Vink</surname><given-names>Robert</given-names></name><name><surname>de Bel</surname><given-names>Thomas</given-names></name><name><surname>van Ginneken</surname><given-names>Bram</given-names></name><name><surname>van der Laak</surname><given-names>Jeroen</given-names></name><name><surname>van de Kaa</surname><given-names>Christina Hulsbergen</given-names></name><name><surname>Litjens</surname><given-names>Geert</given-names></name></person-group><article-title>Automated deep-learning system for gleason grading of prostate cancer using biopsies: a diagnostic study</article-title><source>The Lancet Oncology</source><year>2020</year><volume>21</volume><issue>2</issue><fpage>233</fpage><lpage>241</lpage><pub-id pub-id-type="pmid">31926805</pub-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stroöm</surname><given-names>Peter</given-names></name><name><surname>Kartasalo</surname><given-names>Kimmo</given-names></name><name><surname>Olsson</surname><given-names>Henrik</given-names></name><name><surname>Solorzano</surname><given-names>Leslie</given-names></name><name><surname>Delahunt</surname><given-names>Brett</given-names></name><name><surname>Berney</surname><given-names>Daniel</given-names></name><name><surname>Bostwick</surname><given-names>David</given-names></name><name><surname>Evans</surname><given-names>Andrew</given-names></name><name><surname>Grignon</surname><given-names>David</given-names></name><name><surname>Humphrey</surname><given-names>Peter</given-names></name><name><surname>Iczkowski</surname><given-names>Kenneth</given-names></name><etal/></person-group><article-title>Pathologist-level grading of prostate biopsies with artificial intelligence</article-title><source>Bioinformatics</source><year>2019</year></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brierley</surname><given-names>James D</given-names></name><name><surname>Gospodarowicz</surname><given-names>Mary K</given-names></name><name><surname>Wittekind</surname><given-names>Christian</given-names></name></person-group><source>TNM classification of malignant tumours</source><publisher-name>John Wiley &amp; Sons</publisher-name><year>2017</year></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Jaume</surname><given-names>Guillaume</given-names></name><name><surname>Pati</surname><given-names>Pushpak</given-names></name><name><surname>Anklin</surname><given-names>Valentin</given-names></name><name><surname>Foncubierta</surname><given-names>Antonio</given-names></name><name><surname>Gabrani</surname><given-names>Maria</given-names></name></person-group><source>Histocartography: A toolkit for graph analytics in digital pathology</source><conf-name>MICCAI Workshop on Computational Pathology (MICCAI-W)</conf-name><year>2021</year><fpage>117</fpage><lpage>128</lpage></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vahadane</surname><given-names>Abhishek</given-names></name><name><surname>Peng</surname><given-names>Tingying</given-names></name><name><surname>Albarqouni</surname><given-names>Shadi</given-names></name><name><surname>Baust</surname><given-names>Maximilian</given-names></name><name><surname>Steiger</surname><given-names>Katja</given-names></name><name><surname>Schlitter</surname><given-names>Anna</given-names></name><name><surname>Sethi</surname><given-names>Amit</given-names></name><name><surname>Esposito</surname><given-names>Irene</given-names></name><name><surname>Navab</surname><given-names>Nassir</given-names></name></person-group><article-title>Structure-preserving color normalization and sparse stain separation for histological images</article-title><source>IEEE Transactions on Medical Imaging</source><year>2016</year><volume>35</volume><issue>8</issue><fpage>1962</fpage><lpage>1971</lpage><pub-id pub-id-type="pmid">27164577</pub-id></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutmann</surname><given-names>Michael</given-names></name><name><surname>Hyvärinen</surname><given-names>Aapo</given-names></name></person-group><article-title>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</article-title><source>Artificial Intelligence and Statistics</source><year>2010</year><fpage>297</fpage><lpage>304</lpage></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van den Oord</surname><given-names>Aaron</given-names></name><name><surname>Li</surname><given-names>Yazhe</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name></person-group><article-title>Representation learning with contrastive predictive coding</article-title><source>arXiv preprint arXiv</source><year>2018</year><elocation-id>1807.03748</elocation-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Taigman</surname><given-names>Yaniv</given-names></name><name><surname>Polyak</surname><given-names>Adam</given-names></name><name><surname>Wolf</surname><given-names>Lior</given-names></name></person-group><source>Unsupervised cross-domain image generation</source><conf-name>International Conference on Learning Representations (ICLR)</conf-name><year>2016</year></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Lin</given-names></name><name><surname>Zhang</surname><given-names>Lei</given-names></name><name><surname>Mou</surname><given-names>Xuanqin</given-names></name><name><surname>Zhang</surname><given-names>David</given-names></name></person-group><article-title>Fsim: A feature similarity index for image quality assessment</article-title><source>IEEE transactions on Image Processing</source><year>2011</year><volume>20</volume><issue>8</issue><fpage>2378</fpage><lpage>2386</lpage><pub-id pub-id-type="pmid">21292594</pub-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gatys</surname><given-names>Leon A</given-names></name><name><surname>Ecker</surname><given-names>Alexander S</given-names></name><name><surname>Bethge</surname><given-names>Matthias</given-names></name></person-group><source>Image style transfer using convolutional neural networks</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2016</year><fpage>2414</fpage><lpage>2423</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Jia</given-names></name><name><surname>Dong</surname><given-names>Wei</given-names></name><name><surname>Socher</surname><given-names>Richard</given-names></name><name><surname>Li</surname><given-names>Li-Jia</given-names></name><name><surname>Li</surname><given-names>Kai</given-names></name><name><surname>Fei-Fei</surname><given-names>Li</given-names></name></person-group><source>Imagenet: A large-scale hierarchical image database</source><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2009</year><fpage>248</fpage><lpage>255</lpage></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Jiaxin</given-names></name><name><surname>Jaiswal</surname><given-names>Ayush</given-names></name><name><surname>Wu</surname><given-names>Yue</given-names></name><name><surname>Natarajan</surname><given-names>Pradeep</given-names></name><name><surname>Natarajan</surname><given-names>Prem</given-names></name></person-group><source>Style-aware normalized loss for improving arbitrary style transfer</source><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2021</year><fpage>134</fpage><lpage>143</lpage></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Gkioxari</surname><given-names>Georgia</given-names></name><name><surname>Dollár</surname><given-names>Piotr</given-names></name><name><surname>Girshick</surname><given-names>Ross</given-names></name></person-group><source>Mask r-cnn</source><conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name><year>2017</year><fpage>2961</fpage><lpage>2969</lpage></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graham</surname><given-names>Simon</given-names></name><name><surname>Vu</surname><given-names>Quoc Dang</given-names></name><name><surname>Ahmed Raza</surname><given-names>Shan E</given-names></name><name><surname>Azam</surname><given-names>Ayesha</given-names></name><name><surname>Tsang</surname><given-names>Yee Wah</given-names></name><name><surname>Kwak</surname><given-names>Jin Tae</given-names></name><name><surname>Rajpoot</surname><given-names>Nasir</given-names></name></person-group><article-title>Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title><source>Medical image analysis</source><year>2019</year><volume>58</volume><elocation-id>101563</elocation-id><pub-id pub-id-type="pmid">31561183</pub-id></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>Xavier</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group><source>Understanding the difficulty of training deep feedforward neural networks</source><conf-name>International Conference on Artificial Intelligence and Statistics (AISTATS)</conf-name><year>2010</year><fpage>249</fpage><lpage>256</lpage></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>Dmitry</given-names></name><name><surname>Vedaldi</surname><given-names>Andrea</given-names></name><name><surname>Lempitsky</surname><given-names>Victor</given-names></name></person-group><article-title>Instance normalization: The missing ingredient for fast stylization</article-title><source>arXiv</source><year>2016</year><elocation-id>1607.08022</elocation-id></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>Xudong</given-names></name><name><surname>Li</surname><given-names>Qing</given-names></name><name><surname>Xie</surname><given-names>Haoran</given-names></name><name><surname>Lau</surname><given-names>Raymond YK</given-names></name><name><surname>Wang</surname><given-names>Zhen</given-names></name><name><surname>Smolley</surname><given-names>Stephen Paul</given-names></name></person-group><source>Least squares generative adversarial networks</source><conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name><year>2017</year><fpage>2794</fpage><lpage>2802</lpage></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>Diederik P</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><year>2014</year><elocation-id>1412.6980</elocation-id></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kipf</surname><given-names>Thomas N</given-names></name><name><surname>Welling</surname><given-names>Max</given-names></name></person-group><source>Semi-supervised classification with graph convolutional networks</source><conf-name>International Conference on Learning Representations (ICLR</conf-name><year>2017</year></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bianchi</surname><given-names>Filippo Maria</given-names></name><name><surname>Grattarola</surname><given-names>Daniele</given-names></name><name><surname>Alippi</surname><given-names>Cesare</given-names></name></person-group><source>Spectral clustering with graph neural networks for graph pooling</source><conf-name>International Conference on Machine Learning (ICML)</conf-name><year>2020</year><fpage>874</fpage><lpage>883</lpage></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Szegedy</surname><given-names>Christian</given-names></name><name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name><name><surname>Ioffe</surname><given-names>Sergey</given-names></name><name><surname>Shlens</surname><given-names>Jon</given-names></name><name><surname>Wojna</surname><given-names>Zbigniew</given-names></name></person-group><source>Rethinking the inception architecture for computer vision</source><conf-name>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</conf-name><year>2016</year><fpage>2818</fpage><lpage>2826</lpage></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><name><surname>Gross</surname><given-names>Sam</given-names></name><name><surname>Massa</surname><given-names>Francisco</given-names></name><name><surname>Lerer</surname><given-names>Adam</given-names></name><name><surname>Bradbury</surname><given-names>James</given-names></name><name><surname>Chanan</surname><given-names>Gregory</given-names></name><name><surname>Killeen</surname><given-names>Trevor</given-names></name><name><surname>Lin</surname><given-names>Zeming</given-names></name><name><surname>Gimelshein</surname><given-names>Natalia</given-names></name><name><surname>Antiga</surname><given-names>Luca</given-names></name><name><surname>Desmaison</surname><given-names>Alban</given-names></name><etal/></person-group><article-title>Pytorch: An imperative style, high-performance deep learning library</article-title><source>Neural Information Processing Systems (NeurIPS)</source><year>2019</year><fpage>8024</fpage><lpage>8035</lpage></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fey</surname><given-names>Matthias</given-names></name><name><surname>Lenssen</surname><given-names>Jan Eric</given-names></name></person-group><article-title>Fast graph representation learning with pytorch geometric</article-title><source>arxiv 2019</source><conf-name>ICLR Workshop on Representation Learning on Graphs and Manifolds</conf-name><year>2019</year></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>VirtualMultiplexer: a generative toolkit for synthesizing virtual multiplexed staining.</title><p><bold>(A)</bold> In a typical histopathology workflow, serial tissue sections from a tumor resection are stained with H&amp;E and IHC to highlight tissue morphology and molecular expression of several markers of interest. This time-consuming and tissue-exhaustive process yields unpaired tissue slides that bear the technical risk of suboptimal quality in terms of missing stainings, tissue artifacts, and unaligned tissues. <bold>(B)</bold> To mitigate these issues, the VirtualMultiplexer uses generative AI to rapidly render, from a real input H&amp;E image, consistent, reliable and pixel-wise aligned IHC stainings. <bold>(C)</bold> As the generated images are now virtually multiplexed, they are further exploited to train early-fusion Graph Transformers able to predict several clinically relevant endpoints. <bold>(D)</bold> The VirtualMultiplexer is transferable across image scales, patient cohorts and tissue types, accelerating clinical applications and discovery.</p></caption><graphic xlink:href="EMS192487-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Overview of VirtualMultiplexer architecture.</title><p><bold>(A)</bold> The VirtualMultiplexer consists of a generator <italic>G</italic> that takes as input real unpaired H&amp;E and IHC images and is trained to perform S2S translation by mapping the staining distribution of IHC onto H&amp;E while preserving tissue morphology, ultimately generating virtually multiplexed synthetic IHC images only from input H&amp;E images. <bold>(B)</bold> During training, the VirtualMultiplexer optimizes several losses that enforce consistent S2S translation at multiple scales, including a neighborhood consistency loss (1) that ensures indistinguishable translations at a neighborhood (patch) level, a global consistency loss (2) that ensures that the model accurately captures content and style constraints at a global tile-level, and a local consistency loss (3) that encodes biological priors on cell type classification and discriminator constraints at a cellular level.</p></caption><graphic xlink:href="EMS192487-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Performance evaluation of VirtualMultiplexer.</title><p><bold>(A)</bold> Example H&amp;E core from the EMPaCT TMA. <bold>(B)</bold> Real, unpaired IHC-stained cores for different antibody markers corresponding to the H&amp;E core in (A). <bold>(C)</bold> Virtually stained IHC cores, now paired with the H&amp;E core in (A). <bold>(D)</bold> Comparison of the VirtualMultiplexer with state-of-the-art S2S models. Barplots and errorbars indicate the mean and standard deviation of the FID score from 3 independent runs of each model. Number of test samples used varies per marker and is reported in each subplot. <bold>(E)</bold> Results of the visual Turing test, where circles indicate the guess of each one of the four experts, and barplots and errorbars indicate the corresponding mean and standard variation. <bold>(F)</bold> Assessment of staining quality of the virtual and real stainings, performed on 50 real and 50 virtual images.</p></caption><graphic xlink:href="EMS192487-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Visual quality assessment of virtually stained IHC images of the EMPaCT prostate cancer TMA.</title><p><bold>(A)</bold> Example virtual TMA cores across all 6 markers (left column) and selected zoomed in regions (middle column) that highlight accurate staining patterns. Real reference stainings for each markers are given on the right column. <bold>(B)</bold> Same as (A) but highlighting regions with inaccurate or inconclusive staining.</p></caption><graphic xlink:href="EMS192487-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Transfer learning from TMA to WSIs</title><p>of prostate cancer tissue. Example of H&amp;E (left image), virtual IHC (middle image), and real IHC (right image) staining for NKX3.1 (top) and CD146 (bottom) of prostate cancer tissue WSIs. Blue-framed zoomed-in regions display accurate staining pattern. Red-framed zoomed-in regions display examples of virtual staining mis-predictions.</p></caption><graphic xlink:href="EMS192487-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><title>Prediction of clinically relevant downstream tasks with virtually multiplexed data.</title><p><bold>(A)</bold> Patch extraction and computation of patch features with a frozen ResNet-50 model (blue trapezoid). <bold>(B)</bold> Overview of the Graph-Transformer model, implemented by first constructing a patch-level graph representation, followed by a transformer that processes the graph representation to predict clinically relevant endpoints. <bold>(C)</bold> Training of Graph-Transformer models (green trapezoid) under three different settings, depending on the integration strategy. <bold>(D)</bold> Prediction results of overall survival status (left, 0: alive/censored, 1: prostate cancer related death) and disease progression (right, 0: no recurrence, 1: recurrence). Barplot colors indicate one of the five combinations of training setting and input data used (see legend). For each combination, barplot heights and errorbars indicate the mean and standard deviation of the weighted F1 score, as computed in the held-out test set from 3 independent runs with different initializations. The exact number of training samples used in each cases is given on the top of the barplots. * For all multimodal models, the reported number refers to the union across all markers.</p></caption><graphic xlink:href="EMS192487-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><title>Transfer learning across scales, cohorts, and cancer types.</title><p><bold>(A)</bold> Real H&amp;E needle biopsy of the SICAP dataset (top) and matching virtual IHC stainings across 4 IHC markers (bottom), as generated from the EMPaCT-trained VirtualMultiplexer. <bold>(B)</bold> Prediction results of Gleason grading for the SICAP test set in terms of weighted F1 score and confusion matrix. Barplots and errorbars as in <xref ref-type="fig" rid="F3">Figure 3</xref>, confusion matrices correspond to the Multimodal - Virtual Early fusion model. Note that the setting unimodal - real (dark blue barplot) only includes training the model on H&amp;E, as no real IHC data are available here. <bold>(C)</bold> Same as in (B) but for the PANDA dataset. <bold>(D)</bold> Virtual IHC staining of a PDAC TMA dataset with corresponding prediction of TNM staging.</p></caption><graphic xlink:href="EMS192487-f007"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Distribution of cores per clinical endpoint for the EMPaCT dataset.</title></caption><table frame="hsides" rules="rows"><tbody><tr><td align="center" valign="middle" colspan="3"><bold>Overall survival status prediction</bold></td></tr><tr><td align="center" valign="middle">#cores 0: Alive/censored</td><td align="center" valign="middle">#cores 1: Prostate-cancer related death</td><td align="center" valign="middle">#total cores</td></tr><tr><td align="center" valign="middle">472</td><td align="center" valign="middle">205</td><td align="center" valign="middle">677</td></tr><tr><td align="center" valign="middle" colspan="3"><bold>Disease progression prediction</bold></td></tr><tr><td align="center" valign="middle">#cores 0: No recurrence</td><td align="center" valign="middle">#cores 1: Recurrence</td><td align="center" valign="middle">#total cores</td></tr><tr><td align="center" valign="middle">568</td><td align="center" valign="middle">109</td><td align="center" valign="middle">677</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><title>Distribution of WSIs across Gleason grades for both SICAP and PANDA datasets.</title></caption><table frame="hsides" rules="rows"><tbody><tr><td align="center" valign="middle"/><td align="center" valign="middle" style="border-left: solid thin;">Benign<break/>0+0</td><td align="center" valign="middle">Grade 6<break/>3+3</td><td align="center" valign="middle">Grade 7<break/>3+4/4+3</td><td align="center" valign="middle">Grade 8<break/>3+5/4+4/5+3</td><td align="center" valign="middle">Grade 9<break/>4+5/5+4</td><td align="center" valign="middle">Grade 10<break/>5+5</td><td align="center" valign="middle" style="border-left: solid thin;">Total</td></tr><tr><td align="center" valign="middle">SICAP</td><td align="center" valign="middle" style="border-left: solid thin;">36</td><td align="center" valign="middle">31</td><td align="center" valign="middle">29</td><td align="center" valign="middle">36</td><td align="center" valign="middle">16</td><td align="center" valign="middle">7</td><td align="center" valign="middle" style="border-left: solid thin;">155</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle" style="border-left: solid thin;">Benign<break/>0+0</td><td align="center" valign="middle">Grade 6<break/>3+3</td><td align="center" valign="middle">Grade 7<break/>3+4/4+3</td><td align="center" valign="middle">Grade 8<break/>3+5/4+4/5+3</td><td align="center" valign="middle">Grade 9<break/>4+5/5+4</td><td align="center" valign="middle">Grade 10<break/>5+5</td><td align="center" valign="middle" style="border-left: solid thin;">Total</td></tr><tr><td align="center" valign="middle">PANDA</td><td align="center" valign="middle" style="border-left: solid thin;">2603</td><td align="center" valign="middle">2397</td><td align="center" valign="middle">2327</td><td align="center" valign="middle">1124</td><td align="center" valign="middle">996</td><td align="center" valign="middle">105</td><td align="center" valign="middle" style="border-left: solid thin;">9552</td></tr></tbody></table></table-wrap><table-wrap id="T3" position="float" orientation="portrait"><label>Table 3</label><caption><title>Distribution of cores per TNM stage for the PDAC dataset.</title></caption><table frame="hsides" rules="rows"><thead><tr><th align="center" valign="middle">Stage I</th><th align="center" valign="middle">Stage II</th><th align="center" valign="middle">Stage III</th><th align="center" valign="middle" style="border-left: solid thin;">Total</th></tr></thead><tbody><tr><td align="center" valign="middle">24</td><td align="center" valign="middle">121</td><td align="center" valign="middle">115</td><td align="center" valign="middle" style="border-left: solid thin;">260</td></tr></tbody></table></table-wrap><table-wrap id="T4" position="float" orientation="portrait"><label>Table 4</label><caption><title>Distribution of cores per marker for training and testing the VirtualMultiplexer for the EMPaCT dataset.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle"/><th align="center" valign="middle" style="border-left: solid thin;">NKX3.1</th><th align="center" valign="middle">AR</th><th align="center" valign="middle">CD44</th><th align="center" valign="middle">CD146</th><th align="center" valign="middle">p53</th><th align="center" valign="middle">ERG</th></tr></thead><tbody><tr><td align="center" valign="middle">Train</td><td align="center" valign="middle" style="border-left: solid thin;">248</td><td align="center" valign="middle">359</td><td align="center" valign="middle">426</td><td align="center" valign="middle">390</td><td align="center" valign="middle">389</td><td align="center" valign="middle">251</td></tr><tr><td align="center" valign="middle">Test</td><td align="center" valign="middle" style="border-left: solid thin;">114</td><td align="center" valign="middle">153</td><td align="center" valign="middle">183</td><td align="center" valign="middle">159</td><td align="center" valign="middle">178</td><td align="center" valign="middle">122</td></tr><tr style="border-top: solid thin;"><td align="center" valign="middle" style="border-right: solid thin;">Total</td><td align="center" valign="middle">362</td><td align="center" valign="middle">512</td><td align="center" valign="middle">609</td><td align="center" valign="middle">549</td><td align="center" valign="middle">567</td><td align="center" valign="middle">373</td></tr></tbody></table></table-wrap><table-wrap id="T5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Distribution of cores for training, validation, and testing of Graph-Transformers per downstream task on the EMPaCT dataset, for all unimodal (first 7 columns) and multimodal models. * The number of cores reported under multimodal setting refers to the union across all markers used.</p></caption><table frame="hsides" rules="groups"><tbody><tr style="border-bottom: solid thin;"><td align="center" valign="middle" colspan="9">Task: Overall survival status prediction</td></tr><tr><td align="center" valign="middle"/><td align="center" valign="middle" style="border-left: solid thin;"><bold>NKX3.1</bold></td><td align="center" valign="middle"><bold>AR</bold></td><td align="center" valign="middle"><bold>CD44</bold></td><td align="center" valign="middle"><bold>CD146</bold></td><td align="center" valign="middle"><bold>p53</bold></td><td align="center" valign="middle"><bold>ERG</bold></td><td align="center" valign="middle"><bold>H&amp;E</bold></td><td align="center" valign="middle"><bold>Multimodal*</bold></td></tr><tr style="border-top: solid thin;"><td align="left" valign="middle">Train</td><td align="center" valign="middle" style="border-left: solid thin;">232</td><td align="center" valign="middle">333</td><td align="center" valign="middle">397</td><td align="center" valign="middle">343</td><td align="center" valign="middle">377</td><td align="center" valign="middle">230</td><td align="center" valign="middle">407</td><td align="center" valign="middle">407</td></tr><tr><td align="left" valign="middle">Validation</td><td align="center" valign="middle" style="border-left: solid thin;">74</td><td align="center" valign="middle">110</td><td align="center" valign="middle">133</td><td align="center" valign="middle">123</td><td align="center" valign="middle">120</td><td align="center" valign="middle">82</td><td align="center" valign="middle">134</td><td align="center" valign="middle">134</td></tr><tr><td align="left" valign="middle">Test</td><td align="center" valign="middle" style="border-left: solid thin;">85</td><td align="center" valign="middle">120</td><td align="center" valign="middle">141</td><td align="center" valign="middle">131</td><td align="center" valign="middle">126</td><td align="center" valign="middle">84</td><td align="center" valign="middle">136</td><td align="center" valign="middle">136</td></tr><tr style="border-top: solid thin;"><td align="center" valign="middle" colspan="9">Task: Disease progression prediction</td></tr><tr style="border-top: solid thin;"><td align="center" valign="middle"/><td align="center" valign="middle" style="border-left: solid thin;"><bold>NKX3.1</bold></td><td align="center" valign="middle"><bold>AR</bold></td><td align="center" valign="middle"><bold>CD44</bold></td><td align="center" valign="middle"><bold>CD146</bold></td><td align="center" valign="middle"><bold>p53</bold></td><td align="center" valign="middle"><bold>ERG</bold></td><td align="center" valign="middle"><bold>H&amp;E</bold></td><td align="center" valign="middle"><bold>Multimodal*</bold></td></tr><tr style="border-top: solid thin;"><td align="left" valign="middle">Train</td><td align="center" valign="middle" style="border-left: solid thin;">223</td><td align="center" valign="middle">341</td><td align="center" valign="middle">405</td><td align="center" valign="middle">348</td><td align="center" valign="middle">384</td><td align="center" valign="middle">225</td><td align="center" valign="middle">408</td><td align="center" valign="middle">408</td></tr><tr><td align="left" valign="middle">Validation</td><td align="center" valign="middle" style="border-left: solid thin;">87</td><td align="center" valign="middle">107</td><td align="center" valign="middle">127</td><td align="center" valign="middle">121</td><td align="center" valign="middle">121</td><td align="center" valign="middle">86</td><td align="center" valign="middle">135</td><td align="center" valign="middle">135</td></tr><tr><td align="left" valign="middle">Test</td><td align="center" valign="middle" style="border-left: solid thin;">81</td><td align="center" valign="middle">115</td><td align="center" valign="middle">139</td><td align="center" valign="middle">128</td><td align="center" valign="middle">128</td><td align="center" valign="middle">85</td><td align="center" valign="middle">134</td><td align="center" valign="middle">134</td></tr></tbody></table></table-wrap><table-wrap id="T6" position="float" orientation="portrait"><label>Table 6</label><caption><title>Number of available training, validation and test samples for the SICAP, PANDA and PDAC datasets.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle"/><th align="center" valign="middle">SICAP</th><th align="center" valign="middle">PANDA</th><th align="center" valign="middle">PDAC</th></tr></thead><tbody><tr><td align="center" valign="middle">Training</td><td align="center" valign="middle">95</td><td align="center" valign="middle">5736</td><td align="center" valign="middle">161</td></tr><tr><td align="center" valign="middle">Validation</td><td align="center" valign="middle">29</td><td align="center" valign="middle">1913</td><td align="center" valign="middle">48</td></tr><tr><td align="center" valign="middle">Test</td><td align="center" valign="middle">31</td><td align="center" valign="middle">1903</td><td align="center" valign="middle">51</td></tr><tr style="border-top: solid thin;"><td align="center" valign="middle">Total</td><td align="center" valign="middle">155</td><td align="center" valign="middle">9552</td><td align="center" valign="middle">260</td></tr></tbody></table></table-wrap></floats-group></article>