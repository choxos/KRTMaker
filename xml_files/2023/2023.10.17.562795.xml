<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189941</article-id><article-id pub-id-type="doi">10.1101/2023.10.17.562795</article-id><article-id pub-id-type="archive">PPR745399</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Do Domain-Specific Protein Language Models Outperform General Models on Immunology-Related Tasks?</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Deutschmann</surname><given-names>Nicolas</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Pelissier</surname><given-names>Aurelien</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Weber</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Gao</surname><given-names>Shuaijun</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Bogojeska</surname><given-names>Jasmina</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Martinez</surname><given-names>Maria Rodríguez</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A5">5</xref><xref ref-type="corresp" rid="CR1">†</xref></contrib></contrib-group><aff id="A1"><label>1</label>IBM Research Europe, 8803 Rüschlikon, Switzerland</aff><aff id="A2"><label>2</label>Department of Biosystems Science and Engineering, ETH Zurich, 4058 Basel, Switzerland</aff><aff id="A3"><label>3</label>Currently at FAIRTIQ AG, 3011 Bern, Switzerland</aff><aff id="A4"><label>4</label>Currently at ZHAW School of Engineering, 8400 Winterthur, Switzerland</aff><aff id="A5"><label>5</label>Currently at Yale School of Medicine, 06510 New Haven, United States</aff><author-notes><corresp id="CR1"><sup>†</sup><italic>Corresponding author</italic> <email>maria.rodriguezmartinez@yale.edu</email></corresp><fn id="FN1" fn-type="equal"><label>*</label><p id="P1">A. Pelissier contributed equally with N. Deutschmann</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>22</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>21</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Deciphering the antigen recognition capabilities by T cell and B cell receptors (antibodies) is essential for advancing our understanding of adaptive immune system responses. In recent years, the development of protein language models (PLMs) has facilitated the development of bioinformatic pipelines where complex amino acid sequences are transformed into vectorized embeddings, which are then applied to a range of downstream analytical tasks. With their success, we have witnessed the emergence of domain-specific PLMs tailored to specific proteins, such as immune receptors. Domain-specific models are often assumed to possess enhanced representation capabilities for targeted applications, however, this assumption has not been thoroughly evaluated. In this manuscript, we assess the efficacy of both generalist and domain-specific transformer-based embeddings in characterizing B and T cell receptors. Specifically, we assess the accuracy of models that leverage these embeddings to predict antigen specificity and elucidate the evolutionary changes that B cells undergo during an immune response. We demonstrate that the prevailing notion of domain-specific models outperforming general models requires a more nuanced examination. We also observe remarkable differences between generalist and domain-specific PLMs, not only in terms of performance but also in the manner they encode information. Finally, we observe that the choice of the size and the embedding layer in PLMs are essential model hyperparameters in different tasks. Overall, our analyzes reveal the promising potential of PLMs in modeling protein function while providing insights into their information-handling capabilities. We also discuss the crucial factors that should be taken into account when selecting a PLM tailored to a particular task.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P3">T cells and B cells are integral to the adaptive immune system, each executing critical functions to orchestrate robust defenses against invading pathogens and other internal challenges [<xref ref-type="bibr" rid="R1">1</xref>]. Both types of cells are pivotal for both acquired immunity and antigen-specific responses due to their unique capability to recognize and respond to specific antigens and epitopes. Namely, T cells are responsible for surveillance and cytotoxic activities against infected or aberrant cells, while B cells undergo affinity maturation and can produce highly specific antibodies (Abs) to neutralize antigens. Both cells are essential for allowing the immune system to distinguish between self and non-self entities. Regarding recognition, both T and B cells detect foreign antigens via hyper-variable B cell and T cell receptors (BCRs and TCRs), however, the mechanisms of recognition differ. B cells identify free, unprocessed antigens, while T cells detect antigens presented by the major histocompatibility complex (MHC), a complex of cell surface proteins expressed on the surface of antigen-presenting cells.</p><p id="P4">At the molecular level, BCRs and TCRs are sequences of amino acids that form complex 3D structures that can undergo conformational alterations. Adaptive immunity somatically produces extensive repertoires of TCRs and BCRs, potentially enabling the recognition of many different non-self molecules. During their development, each T and B cell assembles a unique receptor through the rearrangement of different V, D, and J gene segments. This process is accompanied by random nucleotide insertions and deletions at the <italic>junctions</italic> between these gene segments, specifically at the V-D and D-J boundaries [<xref ref-type="bibr" rid="R2">2</xref>]. The unique combination of segments and their junctions forms the complementary determining region 3 (CDR3), which is the most diverse part of the sequence. This region governs the binding specificity of B cell-derived immunoglobulins and T cell receptors, thus influencing their downstream functions [<xref ref-type="bibr" rid="R3">3</xref>]. Due to the stochastic nature of the recombination process, a large diversity of receptors can be generated, each one with unique antigen specificity. Estimates of the theoretical diversity vary, but it can be as large as 10<sup>20</sup> [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>]. While the theoretical diversity is huge, a smaller number of unique TCRs and BCRs is typically expressed in an individual, with estimates suggesting 10<sup>13</sup> unique proteins in the human antibody repertoire [<xref ref-type="bibr" rid="R6">6</xref>]. This high diversity allows the adaptive immune system to respond to the myriad of both seen and unseen threats.</p><p id="P5">The recent advent of high-throughput adaptive immune receptor repertoire (AIRR) sequencing has provided unprecedented insights into the diversity and adaptability of our immune system, but also raised significant challenges in terms of data analysis and interpretation. While many predictive algorithms for antibody-antigen and TCR-epitope interactions have been developed, most exhibit sub-optimal accuracy. This limits the rational design of antibodies [<xref ref-type="bibr" rid="R7">7</xref>], novel T-cell mediated therapies [<xref ref-type="bibr" rid="R8">8</xref>], and new forms of immunotherapy beyond oncological applications [<xref ref-type="bibr" rid="R9">9</xref>].</p><p id="P6">In recent years, self-supervised protein language models (PLMs) have emerged as a powerful paradigm for a large number of protein-related tasks, including biological and molecular property prediction [<xref ref-type="bibr" rid="R10">10</xref>]. At their core, these models treat amino acid sequences as a biological language. This language can be decoded using deep learning models trained on vast numbers of protein sequences (approximately 250 million), enabling the translation of specific sequences into meaningful vector representations of proteins (embeddings) in a high-dimensional latent space. Some of the best-known PLMs include protBERT (Bidirectional Encoder Representations from Transformers) [<xref ref-type="bibr" rid="R11">11</xref>], ESM (Evolutionary Scale Modeling) [<xref ref-type="bibr" rid="R12">12</xref>], aminoBERT [<xref ref-type="bibr" rid="R13">13</xref>] and ProGen [<xref ref-type="bibr" rid="R14">14</xref>]. Although neither of them was specifically trained for molecular property or structure prediction, their immense scale (15 billion parameters for the largest) allows them to distill fundamental qualities of the biological language. This is demonstrated by their ability to predict protein 3D-structures [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>], binding events [<xref ref-type="bibr" rid="R17">17</xref>], and identifying functional sites [<xref ref-type="bibr" rid="R18">18</xref>]. Importantly, the latent space representations generated by these models can be used as input for subsequent predictive models, reducing training time and model complexity, and boosting performance on downstream tasks. However, the size and complexity of these models raise substantial computational challenges, including the necessity of significant processing power and extensive training times. For instance, the largest among them, ESM2-15B, cannot operate on standard commercial machines and requires the use of costly clusters for embedding protein sequences. Therefore, when selecting a PLM, it is crucial to assess the model’s size in relation to expected performance in order to minimize computational costs while maintaining good accuracy.</p><sec id="S2"><title>Immune-specific PLMs</title><p id="P7">Given the success of these PLMs in many bioinformatics-related tasks, similar models have been trained on immune-specific protein sequences, such as antibodies (AbLang [<xref ref-type="bibr" rid="R19">19</xref>], Antibert [<xref ref-type="bibr" rid="R20">20</xref>], AbMAP [<xref ref-type="bibr" rid="R21">21</xref>]) and T-cell receptors (TCR-BERT [<xref ref-type="bibr" rid="R22">22</xref>], catELMO [<xref ref-type="bibr" rid="R23">23</xref>]). The rationale behind these models is that they might improve representation capabilities for immune-related applications [<xref ref-type="bibr" rid="R24">24</xref>]. Nevertheless, as they are usually trained on a narrower and less diverse dataset of proteins, there is a risk of missing general knowledge that might help the model learn and generalize better for unseen proteins and antigens [<xref ref-type="bibr" rid="R21">21</xref>].</p><p id="P8">In this article, we explore diverse use cases of PLMs (<xref ref-type="table" rid="T1">Table 1</xref>) for immune-related tasks. Specifically, we use embeddings directly derived from PLMs as input for basic multi-layer perceptrons trained for antigen and epitope-related tasks. We also discuss the implications of employing general models over specialized immune-specific PLMs. Our objective is not solely to ascertain the peak performance on specific tasks but to also comprehensively discuss the crucial factors that should be taken into account when selecting a protein embedding model tailored to a particular task. In this context, we demonstrate that the prevailing notion of "domain-specific models outperform general models" requires a more nuanced examination. Additionally, we highlight the critical importance of selecting the appropriate layer for extracting embeddings in optimizing performance on downstream tasks.</p></sec></sec><sec id="S3" sec-type="results"><label>2</label><title>Results</title><sec id="S4"><label>2.1</label><title>The choice of layer as embedding from PLMs matters</title><p id="P9">We began by examining the similarities and differences between embeddings-based metrics and other established distances for sequence analysis such as the Levenshtein distance. Levenshtein distance commonly referred to as the <italic>edit distance</italic>, quantifies sequence dissimilarity by determining the minimal number of single-character edits needed to transform one string into another [<xref ref-type="bibr" rid="R29">29</xref>].</p><p id="P10">We calculated pairwise distances between sequences by measuring the Euclidean distances within various embeddings, and compared them to the normalized Levenshtein pairwise distances (<xref ref-type="sec" rid="S15">Methods 4.2</xref>) using the Pearson and Kendal correlations (<xref ref-type="fig" rid="F1">Fig. 1</xref>). The Levenshtein distance is a measure of the similarity between two strings that can be computed both at the nucleotide or amino acid level [<xref ref-type="bibr" rid="R30">30</xref>]. To be consistent with PLMs that operate at the amino acid level, in this paper we always computed it at the amino acid level. While both Ablang and ESM-650M showed a significant correlation with the Levenshtein distance, there were also substantial differences, with ESM2-650M obtaining higher Pearson coefficients than Ablang (0.76 and 0.57, respectively, <xref ref-type="fig" rid="F1">Figure 1A &amp; B</xref>). Interestingly, the pattern was inverted when we examined the CDR3-<italic>β</italic> of TCRs, with Pearson correlation coefficients of 0.73 and 0.49 for TCR-BERT and ESM2-650M, respectively (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>).</p><p id="P11">Before moving forward, we make two clarifications. First, for the TCRs, our comparison focused only on CDR3-<italic>β</italic> chains since TCR-BERT cannot encode full sequences. Second, a comparison with the Levenshtein distance serves at best as an indirect measure of representation quality, as a single amino acid difference minimally changes the Levenshtein distance between two sequences, but can drastically alter the receptor’s functional properties. In the next sections, we will provide more meaningful evaluations focusing on affinity prediction tasks.</p><p id="P12">Several studies on PLMs outside of biology have already emphasized the importance of selecting the appropriate layer for embedding extraction as one of the model’s hyperparameters [<xref ref-type="bibr" rid="R32">32</xref>, <xref ref-type="bibr" rid="R33">33</xref>, <xref ref-type="bibr" rid="R34">34</xref>]. Therefore, we swept across different layers of PLMs and evaluated the changes in correlation to the Levenshtein distance. Denoting the layer from which the embedding was taken as LX, we observed significant differences for both B cell and T cell receptors (<xref ref-type="table" rid="T2">Table 2</xref>). For example, using the L6 instead of L33 of ESM2-650M increased the Pearson correlation to 0.76 from 0.63 on BCRs and 0.59 from 0.49 on TCRs CDR3-<italic>β</italic>. Similarly, L8 of TCR-BERT yielded a higher correlation with the Levenshtein distance compared to L12. As we were solely evaluating the correlation with the Levenshtein distance, we do not claim that earlier layers are inherently more expressive than deeper ones. Our main observation is that the choice of layer should be considered as a hyperparameter that requires optimization for each specific task and embedding.</p></sec><sec id="S5"><label>2.2</label><title>Generalist and domain-specific PLMs for the characterization of B cells evolutionary trajectories</title><p id="P13">In this section, we explored the use of PLMs to characterize B cell evolution during adaptive immune responses, and evaluated the evolutionary information captured by the different embeddings. Briefly, upon infection, B cells undergo rapid proliferation within the germinal centers (GCs), where they mutate their BCR genes to optimize the binding affinity to the invading pathogen. Hence, GCs are critical BCR evolutionary hubs that are shaped by intricate selection dynamics and inter-GC communication events. As a result, tightly intertwined BCR clones co-evolving together and potentially shared across GCs are often found in experimental studies.</p><p id="P14">Typical B cell repertoire analyzes start by grouping BCR sequences into clones descending from a common ancestor using the Levenshtein distance. Next, phylogenetic lineages of clonally related B cells are reconstructed to elucidate evolutionary trajectories, identify clonal interrelationships, and investigate dynamic immune responses over time. Such analyzes can shed light not only on the emergence of functional antibodies but also on the progression of conditions such as chronic infections, autoimmune disorders, and cancer.</p><p id="P15">However, these analyzes rely exclusively on multiple sequence alignments and quantify differences using the Levenshtein distance, which attributes equal importance to all amino acid changes in BCR sequences—be it substitutions, insertions, or deletions. While the Levenshtein distance assumes that all mutations influence BCR binding profiles equally, in contrast, the impact of a mutation on PLM embeddings is determined by both the site and nature of the change. In that regard, PLMs offer a new and potentially more expressive approach to analyze immune repertoires during an ongoing immune response.</p><p id="P16">To examine the extent to which various embeddings capture co-evolutionary relationships among sequences, we leveraged a repertoire of BCR heavy chain sequences originating from 10 GCs from the same lymph node [<xref ref-type="bibr" rid="R31">31</xref>]. After processing the sequences to obtain their alignment to germline V and J genes, we assigned them to the same clonal family using standard clonal identification methods [<xref ref-type="bibr" rid="R30">30</xref>], i.e. two sequences were grouped together if they had the same V and J genes as well as a CDR3 sequence similarity above 0.9 (<xref ref-type="sec" rid="S16">Methods Section 4.3</xref>). We categorized all sequences based on their V-genes, J-genes, CloneID, and GC of origin (<xref ref-type="fig" rid="F2">Figure 2A &amp; B</xref>). We then assessed the embeddings’ capability to determine whether two BCRs originated from the same or different classes, and we quantified the predictive accuracy in terms of AUC (<xref ref-type="fig" rid="F2">Figure 2C &amp; D</xref>, <xref ref-type="table" rid="T3">Table 3</xref>). As both the germline gene alignment and the clonal identification rely on variants of the Levenshtein distance, we anticipated that Levenshtein is likely to perform well in these tasks. Not surprisingly, as already hinted by the varying correlations between the studied embeddings and the Levenshtein distance (<xref ref-type="table" rid="T2">Table 2</xref>), the ability of various PLMs to differentiate B cells from distinct clones or GCs significantly differed across embeddings. While the Levenshtein distance slightly outperformed most embeddings, these differences were only significant when inferring the V-gene – this was expected as the V-gene accounts roughly for the 70% of the sequence, which provides it with an evident advantage for this task. Counter-intuitively, the AbLang model, despite being exclusively trained on antibody sequences, was less proficient in identifying clones and V-genes than the ESM2-based models. However, it excelled in differentiating GCs (<xref ref-type="table" rid="T3">Table 3</xref>). This once again indicates that each embedding captures distinct information when compared to one another and also when compared to the Levenshtein distance.</p><p id="P17">Next, we investigated and visualized individual B cell clones with Ablang and ESM embeddings. First, we selected a rare clone with a lineage shared among several GCs, typically signaling inter-GC communication events that occur when a B cell migrates from a GC to another [<xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R35">35</xref>]. We first constructed its root taking the unmutated V and J germlines and filling the remaining junction region with the consensus sequence of all unique sequences within the considered clone. Then, the phylogenetic evolutionary tree of this clone (<xref ref-type="sec" rid="S17">Method Section 4.4</xref>) revealed clearly distinguishable branches, which roughly correspond to the GC they belong to (<xref ref-type="fig" rid="F3">Figure 3A</xref>). While sequences from GC4 were distinctly separated from the two other GCs, GC10 and GC6 exhibit minor sequence intermixing, which can indicate further B cell recirculation events. An alternative and more plausible explanation might be experimental sequencing errors or sequence mislabeling.</p><p id="P18">As a comparison, we visualized the same phylogeny using a t-SNE map trained on the BCR Ablang-embedded sequences (<xref ref-type="fig" rid="F3">Figure 3B</xref>, <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref>), where nucleotide mutations were depicted with a line between neighbors BCR sequences. A simple visualization already revealed meaningful patterns, e.g. B cells from the same GC clustered together and showed correlation with their mutation count from the root, i.e. the number of nucleotide changes that separate a sequence from the root.</p><p id="P19">Interestingly, a subset of mutations led to larger displacements in the embedding space in terms of Euclidean distance compared to others. While this might be an artifact of t-SNE dimensionality reduction (as seen in <xref ref-type="supplementary-material" rid="SD1">Figure S6A</xref>, where the same clone is visualized using t-SNE on Levenshtein distances), a systematic analysis of all mutations in the phylogeny from the 50 most abundant clones confirmed that some mutations lead to significantly larger displacements in the embedding space than others. Furthermore, the displacement magnitude induced by a mutation differed significantly depending on whether it was randomly induced or selected during affinity maturation. Here, a random mutation is a random nucleotide change anywhere in the BCR heavy chain, while mutations shaping the inferred phylogenetic tree of expanded clones are referred to as selected. These were inferred during the reconstruction of the B-cell clone phylogeny (<xref ref-type="sec" rid="S17">Methods Section 4.4</xref>).</p><p id="P20">We observed that AbLang and ESM fundamentally differ in the way they handle random and selected mutations (<xref ref-type="fig" rid="F4">Figure 4A &amp; B</xref>, <xref ref-type="supplementary-material" rid="SD1">Figure S7</xref>). While AbLang led to a 2-fold increase in the displacement induced by selected mutations compared to random mutations, ESM2 showed the opposite behavior with a 3-fold decrease for the same selected mutations. A similar pattern was found between ESM2-35M, ESM2-150M, and ESM2-650M (<xref ref-type="fig" rid="F4">Figure 4C</xref>). Interestingly, ESM2-650M-L6 did not show a significant difference in terms of the displacement induced by random and selected mutations. Finally, it is worth noting that not all the displacements are equivalent with respect to the Levenshtein distance. For instance, a random mutation has approximately a 20% chance of being silent, resulting in a Levenshtein displacement of zero, whereas other non-silent mutations lead to a displacement of one. As silent mutations are less likely to be selected, this explains the observed slight increase of displacement by selected mutations over random mutations in the Levenshtein space.</p><p id="P21">While the interpretation of these results is subtle, it is worth noting that the AbLang training dataset comprised a large fraction of naive BCR sequences, i.e., sequences that have not undergone affinity maturation. Thus, we can assume that the embedding space naturally separated naive and mature B cells, as the authors have noted themselves [<xref ref-type="bibr" rid="R19">19</xref>]. If this hypothesis is correct, it seems reasonable that selected mutations should induce larger displacement than random ones, as selected mutations are more likely to result in displacements towards the regions of the embedding space where mature sequences lie. On the other hand, since ESM2 has been trained on heterogeneous collections of proteins, the embedding might group sequences by family affiliation rather than by harder-to-predict functional properties, such as affinity. However, further investigation into the properties of the embedded space coupled with data specifically generated to test the properties of interesting regions of this space is needed to better understand the functional implications of displacements in the embedded space.</p></sec><sec id="S6"><label>2.3</label><title>Comparing generalist and domain-specific embedders for BCR and TCR specificity prediction</title><p id="P22">It is expected that using machine learning to decode information in adaptive immune receptor repertoires can transform the prediction of immune responses and accelerate the development of vaccines, therapeutics, and diagnostics [<xref ref-type="bibr" rid="R36">36</xref>]. Having demonstrated that both Ablang and ESM2 capture important evolutionary information for both BCRs and TCRs, we shifted our focus to examining the predictive accuracy achieved by PLMs in epitope specificity predictions for TCRs and BCRs, both significant open challenges in immunoinformatics [<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P23">One of the primary limitations of current models for specificity prediction tasks is the scarcity of labeled training data. As a result, techniques leveraging transfer learning approaches [<xref ref-type="bibr" rid="R38">38</xref>] and, more recently, extracting embedded representations from extensive models like PLMs are under active exploration [<xref ref-type="bibr" rid="R24">24</xref>].</p></sec><sec id="S7"><title>TCR predictions</title><p id="P24">We compared different versions of the ESM2 generalist model and TCR-BERT as featurizers for downstream classification tasks. For this purpose, we borrowed a predictive task from the ImmRep TCR-specificity workshop benchmark, the first public benchmark that evaluated 23 predictive models [<xref ref-type="bibr" rid="R37">37</xref>]. The task entails predicting a binary label for binding to the GILGFVFTL epitope given the amino acid sequence of the TCR CDR3-<italic>β</italic> chain. While we focused here on a subset of the ImmRep dataset for a single epitope, we show in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Section A.1</xref> that a simple ESM2 embeddings-based classifier with minor modeling effort can compete with the best models from the ImmRep TCR-specificity workshop benchmark [<xref ref-type="bibr" rid="R37">37</xref>].</p><p id="P25">Here, for each embedding-based model, we used a multi-layer perceptron (MLP) with 1, 2, or 3 layers (<xref ref-type="sec" rid="S18">Methods 4.8</xref>) to predict the binary binding label. We report in <xref ref-type="table" rid="T4">Table 4</xref> the performance of models selected through hyperparameter optimization, measured with standard metrics for accuracy evaluation: AUC, Accuracy, F1, Precision, and Recall. The architectures and hyperparameters of the selected models are reported in <xref ref-type="table" rid="T7">table 7</xref>. Interestingly, after optimization of the classifier’s hyperparameters, there was no significant difference between the best ESM2- and TCR-BERT-based classifiers across all metrics considered.</p><p id="P26">As previously discussed [<xref ref-type="bibr" rid="R22">22</xref>], using early layers can lead to better performance in downstream tasks. For ESM2-based models, we observed that training on embeddings from an early layer of the largest model we considered (ESM2-650M-L6), yields significantly better performance for several metrics compared to classifiers trained on embeddings from the last layer (ESM-650M-L33). For TCR-BERT, however, we observed no significant difference between the last layer of TCR-BERT (L12) and the layer recommended for downstream affinity prediction (L8) [<xref ref-type="bibr" rid="R22">22</xref>], although the latter has a much lower AUC standard deviation which is a desirable feature in terms of performance guarantees. One final feature of note in <xref ref-type="table" rid="T4">Table 4</xref> is the lack of significant differences in recall among models based on six of the eight embeddings we considered, in stark contrast with the other metrics. This is explained by the empirical observation that a larger fraction of the positively labeled (binding) samples is close to the models’ decision boundaries, making them more sensitive to statistical noise. A potential reason for this observation is the design of the training set where the positive samples are those with certain properties that lead to the high binding affinity, while the negative samples are unrelated randomly chosen TCRs. This makes it less likely for individual negative samples to decisively define the decision boundary.</p><p id="P27">Finally, a possible factor affecting the embedding’s performance could be the size of the MLP used to classify TCR binders, with larger models potentially able to capture more relevant features for downstream tasks. Interestingly, in <xref ref-type="supplementary-material" rid="SD1">Supplementary Section A.2</xref>, we show that there was no visible dependence of optimized performance on parameter count for TCR binding predictions.</p></sec><sec id="S8"><title>BCR predictions</title><p id="P28">We turned next to BCR binding prediction tasks, which share similar biochemical underpinnings as TCR specificity prediction, but also present important differences from a machine learning point of view. We considered the AlphaSeq dataset [<xref ref-type="bibr" rid="R40">40</xref>], which provides continuous measurements of antibody affinity to an epitope of the SARS-CoV-2 spike protein. We defined the predicting affinity task as a continuous regression task, rather than a binary classification as we did for TCR specificity prediction. Another important difference is in the size of the sample, which comprises 154,029 samples including replicates, and 71,415 paired heavy and light chains. This dataset is thus approximately 37 times larger than the TCR dataset.</p><p id="P29">Similarly, as for the TCR binding prediction, we used ESM2 as the state-of-the-art general protein PLM, and compared its performance with AbLang, the best-in-class antibody-specific model [<xref ref-type="bibr" rid="R24">24</xref>]. For each BCR, we embedded the paired heavy and light chains separately. Then, we concatenated the embeddings and processed them with a shallow MLP (2 to 4 layers and 500K - 1.5M parameters, Methods Section 6) to obtain an affinity prediction. We optimized the regression models separately and selected the optimal hyperparameter set based on a held-out validation set.</p><p id="P30">We present the performance of the optimized classifiers in <xref ref-type="table" rid="T5">Table 5</xref>, whose architecture and hyperparameters are described in <xref ref-type="table" rid="T6">Table 6</xref>. Here, as for TCRs, the best ESM2 embedding was obtained from an early layer (layer 6) of the largest embedding (650M parameters). Regression models constructed using these embeddings significantly outperformed both the other ESM2 embeddings and the AbLang-based embeddings. The second-best-performing model on all metrics is ESM2-650M-L33, which is the same model as the best-performing model, but with embeddings extracted from the final layer. This second model also significantly outperformed AbLang in terms of MSE (Mean Squared Error) and R<sup>2</sup> (coefficient of determination), but did not statistically dominate it in terms of MAE (Mean Absolute Error). As we observed with TCRs, the size of the MLP did not affect the performance of the BCR binding predictions (<xref ref-type="supplementary-material" rid="SD1">Supplementary Section A.2</xref>).</p><p id="P31">In contrast with the analogous TCR binding prediction models where the best ESM2 models had comparable performance with domain-specific TCR-BERT, Ablang was clearly dominated by ESM2 here. A possible straightforward explanation could be that AbLang encodes less relevant information about antibody binding compared to the best ESM2 embedding, while for TCR-related tasks, a similar amount of information is captured by ESM2 and TCR-BERT embeddings. However, an alternative explanation might be that ESM2 encodes richer protein information compared to the domain-specific models. Yet, the limited sample size of the TCR classification task, which is 37 times smaller than AlphaSeq, might prevent the classifier from effectively differentiating relevant from non-task-specific features, limiting ESM2 accuracy.</p><p id="P32">To test this hypothesis, we considered multiple randomly down-sampled versions of the AlphaSeq dataset. For each dataset, we trained a regression model by selecting hyperparameters optimized specifically for each dataset size, for both AbLang-L12 and the superior ESM2 model, ESM2-650M-L6. As expected and shown in <xref ref-type="fig" rid="F5">Fig. 5</xref>, the performance of both PLM embedding-based regressors deteriorated with decreasing training data sizes. The figure shows that there is a clear gap between the two embeddings in the high-data regime, where ESM2-650M-L6 outperformed AbLang. Furthermore, ESM2-650M-L6 performance has an earlier inflection point while AbLang performance decreased more steadily with scarcer data, as a result of which, both models have comparable performances in the low-data regime. This observation supports the hypothesis that both PLMs encode similarly predictive <italic>learnable</italic> information when training data is limited. However, when more training data is available, the larger ESM2-650M-L6 encodes more relevant information than AbLang, at least, for the task we have inspected. This increase in disparity in the rich-data size regime could be attributed to the differences in their embeddings’ dimensions (1280 compared to 768 for Ablang, <xref ref-type="table" rid="T1">Table 1</xref>). ESM2, having higher dimensionality, may require more training data to reach its maximum disparity.</p></sec></sec><sec id="S9" sec-type="discussion"><label>3</label><title>Discussion</title><p id="P33">The profound impact and utility of Protein Language Models (PLMs) in bioinformatics [<xref ref-type="bibr" rid="R10">10</xref>] and immune-related applications [<xref ref-type="bibr" rid="R24">24</xref>] is beyond dispute. However, as new PLMs continuously emerge, it remains unclear which factors are essential to consider when utilizing them. Our study has shown that the model’s size, the magnitude of the training dataset, and the selection of layers, all play crucial roles in the model’s ability to capture relevant biological features in immunological tasks. For instance, ESM2-650M embeddings, with their higher dimensionality (1280 for all its layers compared to 768 for Ablang, <xref ref-type="table" rid="T1">Table 1</xref>), outperforms domain-specific PLMs only when a sufficient amount of training data is available. Concerning the model’s size, ESM2 consistently shows enhanced performance as the parameter size grows, aligning with the results seen in other related tasks where it was evaluated [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R15">15</xref>]. We also found that the last layer of PLMs is not necessarily the one yielding the highest performance, and thus, the layer number should be considered as a hyperparameter that needs to be optimized for each downstream task.</p><p id="P34">Regarding BCRs, their unique evolutionary processes to maximize the binding affinity to varying targets set them apart from most proteins [<xref ref-type="bibr" rid="R41">41</xref>]. Indeed, the close relationship between an antibody and the germline sequence of the naive B cell from which it has evolved, along with the subsequent random mutations it has undergone, collectively determine its specificity and binding profile. The distinct selective pressures experienced within germinal centers further shape BCR repertoires in a manner that presents challenges for generalist PLMs to accurately represent. For example, we observed that selected mutations had a greater impact on AbLang embeddings compared to random mutations, presumably because they enhanced the binding profile toward a specific target. However, this pattern was reversed for ESM2 embeddings (<xref ref-type="fig" rid="F4">Figure 4</xref>). While a comprehensive exploration of the meaning of displacements in the embedding space requires specialized techniques from interpretable deep learning and falls beyond the scope of this work, this different behavior suggests that AbLang and ESM2 organize BCR sequences in the embedding space in significantly distinct ways.</p><p id="P35">However, understanding how this different organization impacts the predictive performance of different embeddings on various downstream tasks is not trivial. While it might be naively expected that domain-specific models should outperform generalist models on tasks with high evolutionary specificity, it was demonstrated that ESM1 [<xref ref-type="bibr" rid="R12">12</xref>], the predecessor of ESM2, performed comparably to AbLang on various antibody-related tasks, including paratope and antigen binding prediction [<xref ref-type="bibr" rid="R24">24</xref>]. Adding to the comparison, we have shown here that ESM2 clearly outperforms AbLang on antigen-binding tasks. On the other hand, ESM1 underperformed AbLang on tasks requiring high specificity, such as antibody discovery and cell state prediction. Therefore, AbLang may still be a competitive choice compared to ESM2 for these tasks.</p><p id="P36">An intriguing question that our study does not address is to better understand which information gets selected in the low-data regime. Even without a thorough interpretation of the information captured by each embedding, assessing the generalization of their predictions on unseen data and the stability of predictions across subsampled training datasets could yield valuable insights into the information utilized by the downstream regressors. Of particular interest is whether there are specific ESM2 embedding features that are only reliably leveraged in the high data regime, leading to its improved performance, or whether the low-data regressors consistently make global but shallow use of the available ESM2 embedding features, suggesting a more robust and generalizable approach in scenarios with limited data.</p><p id="P37">Finally, in the context of modeling the evolution of antibodies during an immune response, the utilization of PLMs represents a promising approach for predicting the impact of mutations [<xref ref-type="bibr" rid="R42">42</xref>]. The limited accuracy of current antibody-antigen binding prediction models poses substantial challenges in the field of antibody design [<xref ref-type="bibr" rid="R7">7</xref>]. Rather than explicitly simulating the molecular binding between the antibody with its antigen, PLMs offer an alternative approach to train binding prediction models. Current modeling efforts to simulate an immune response to a known antigen cannot evaluate the influence of point mutations on BCR sequences [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R44">44</xref>]. This limitation hinders their applicability in vaccine design or infectious disease research, where faithful simulations of repertoire adaptation to external threats are needed. In this context, PLMs could facilitate more realistic germinal center simulations [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R46">46</xref>, <xref ref-type="bibr" rid="R47">47</xref>] by predicting which amino acids are more likely to undergo mutations.</p><p id="P38">Similar potential is expected in the context of T cells, where the development of T cell-based immunotherapies is currently hampered by our limited ability to predict cross-reactive binding events, i.e. events where natural or engineered T cells bind to off-target sites, potentially triggering inflammatory responses at healthy sites. Although current PLMs cannot predict the binding affinity to unseen epitopes and antigens, their predictive accuracy for numerous tasks is advancing rapidly. Aid by emerging high-throughput single-cell technologies that may enable the swift measurement of BCRs and TCRs binding affinity to extensive collections of epitopes and antigens, there is promising potential to revolutionize the clinical application of both B and T cell-based therapies.</p></sec><sec id="S10" sec-type="methods"><label>4</label><title>Methods</title><sec id="S11"><label>4.1</label><title>Datasets</title><sec id="S12"><title>TCR specificity</title><p id="P39">The TCR dataset was provided by the organizers of the ImmRep 2022 TCR-epitope specificity workshop [<xref ref-type="bibr" rid="R37">37</xref>], and was downloaded from <ext-link ext-link-type="uri" xlink:href="https://github.com/viragbioinfo/ImmRep_2022_TCRSpecificity">https://github.com/viragbioinfo/ImmRep_2022_TCRSpecificity</ext-link>. We restrict our focus to TCRs with binding information for the GILGFVFTL epitope. This selection deviates from the broader ImmRep goal to test various epitopes, and is attributed to the fact that TCR-BERT only embeds TCR sequences, not epitopes. A mix of positive and negative TCRs for each epitope was provided in the test data. By concentrating on a single epitope, our sample size is reduced to 4067 sequences, of which 680 are positively labeled. While the dataset contains paired information on the <italic>α</italic> and <italic>β</italic> chains, we retained only the CDR3-<italic>β</italic> chain to construct embeddings due to TCR-BERT limitations.</p></sec><sec id="S13"><title>Antibody specificity</title><p id="P40">The antibody covid specificity Dataset (AlphaSeq) [<xref ref-type="bibr" rid="R40">40</xref>] was downloaded from <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/5095284">https://zenodo.org/record/5095284</ext-link>. AlphaSeq consists of 71,415 paired heavy and light chains, with continuous measurements of their affinity to an epitope of the SARS-CoV-2 spike protein. The sequences were generated by introducing artificial mutations (up to three) in the CDRs of four known binders of the SARS-CoV-2 spike protein.</p></sec><sec id="S14"><title>Germinal center BCRs</title><p id="P41">The collection of B cell sequences originating from 10 individual germinal centers from a single human lymph node [<xref ref-type="bibr" rid="R31">31</xref>] was downloaded from <ext-link ext-link-type="uri" xlink:href="https://vdjserver.org/community/8899006209436478995-242ac118-0001-012">https://vdjserver.org/community/8899006209436478995-242ac118-0001-012</ext-link>. Germline V and J genes assignments were already included in the datasets, and the clone phylogenies were inferred following the approach described in <xref ref-type="sec" rid="S16">Method 4.3</xref>. Sequences with NaN values in V or J gene segments were removed from the study. Sequences are between 300 and 340 nucleotides long, and the V and J segments comprise roughly 70% and 15% of the sequences, respectively.</p></sec></sec><sec id="S15"><label>4.2</label><title>Levenshtein distance</title><p id="P42">The Levenshtein distance [<xref ref-type="bibr" rid="R29">29</xref>], defined as the minimum number of edits required to transform one sequence into another, is a common metric to quantify sequence similarity. To reduce the bias caused by differences in sequence length, we use the normalized Levenshtein distance [<xref ref-type="bibr" rid="R48">48</xref>] that incorporates the length of both sequences and satisfies the triangle inequality. Given two strings, <italic>s</italic><sub>1</sub> and <italic>s</italic><sub>2</sub>, and the Levenshtein distance between them, <italic>Lev</italic>(<italic>s</italic><sub>1</sub>, <italic>s</italic><sub>2</sub>), the normalized Levenshtein distance <italic>Lev</italic><sub>norm</sub> is defined as:
<disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mtext>norm</mml:mtext><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mo>|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:mo>+</mml:mo><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="S16"><label>4.3</label><title>Identifying germinal center B-cell clones</title><p id="P43">Following on the recommendations of [<xref ref-type="bibr" rid="R30">30</xref>], we first grouped B cells together only if they had the same V and J genes. For each obtained group, we computed the pairwise normalized Levenshtein distance between each junction sequence in that group, and applied the Hierarchical Agglomerative Clustering (HAC) algorithm [<xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R50">50</xref>] to cluster the BCRs into different clonal groups. We used the complete-linkage clustering criterion, which begins by putting each sequence on its own cluster, and then sequentially combines smaller clusters into larger ones until all elements are in one single cluster. This method generates a dendrogram, illustrating the sequence of cluster mergers and the distance at which each merger occurred. By setting an appropriate threshold, we can define individual clusters as all the clusters that have not been merged up to that distance. In this study, we chose as threshold the distance to the nearest distribution of negation sequences with a tolerance of 0.03% [<xref ref-type="bibr" rid="R30">30</xref>], corresponding to a threshold distance of 0.1. Here, negation sequences refer to BCR sequences with unrelated phylogenies, typically sampled from unrelated individuals.</p></sec><sec id="S17"><label>4.4</label><title>Inferring the mutation phylogeny of B-cell clones</title><p id="P44">We used mutation phylogenetic trees to visualize and reconstruct the evolution of the BCR sequences in a given clone. In such representation, each founder cell defines the unmutated germline of a new tree, newly acquired mutations are represented as downstream nodes (<xref ref-type="supplementary-material" rid="SD1">Figure S4A</xref>), and leaves represent observed sequences [<xref ref-type="bibr" rid="R43">43</xref>].</p><p id="P45">To construct the tree, we first defined the root by taking the unmutated V and J germlines and filling the remaining junction region with the consensus sequence of all unique sequences within the considered clone. Then, the grouped sequences from each clone were aligned with ClustalW [<xref ref-type="bibr" rid="R51">51</xref>]. The alignment was necessary to define distances between each sequence, from which we inferred a tree with a hierarchical clustering approach. The most similar sequences were grouped together and progressively aggregated with other groups until the root node (maximum distance) was reached (Neighbour Joining method in ClustalW). Then, we associated a sequence to each bifurcation in the dendrogram, defined as the consensus sequence of all the leaves below it. Finally, the final tree was constructed by linking bifurcations together and by introducing mutations between them, or by merging them when having the same sequence. These inferred sequences, or intermediate nodes in the tree, represent the BCR of parent cells that existed but later underwent additional mutations and are not present in the repertoire at the sequencing snapshot. The compilation of mutations linking these inferred sequences together defines a set of <italic>selected</italic> mutations that we can leverage to evaluate the sensitivity of PLMs to these mutations.</p><p id="P46">This method to infer sequences was preferred over Bayesian Maximum likelihood estimation approaches [<xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R53">53</xref>] or Markov chain Monte Carlo sampling [<xref ref-type="bibr" rid="R54">54</xref>], as these heavily rely on the infinite sites assumptions, according to which every mutation occurs at a previously not mutated site. While this assumption is reasonable at a genome-wide scale, in the context of shorter junctions and somatic hypermutation rates, it is often not verified, as we have found in our data (<xref ref-type="supplementary-material" rid="SD1">Figure S4B</xref>).</p></sec><sec id="S18"><label>4.5</label><title>Visualizing phylogenetic trees</title><p id="P47">Circular visualization of Hierarchical dendogram were implemented with pyCirclize [<xref ref-type="bibr" rid="R55">55</xref>] (<ext-link ext-link-type="uri" xlink:href="https://github.com/moshi4/pyCirclize">https://github.com/moshi4/pyCirclize</ext-link>). Mutation phylogenetic trees were plotted with Graphviz (<ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/graphviz/">https://pypi.org/project/graphviz/</ext-link>).</p></sec><sec id="S19"><label>4.6</label><title>Implementation details of PLMs</title><p id="P48">All embeddings are extracted using PyTorch [<xref ref-type="bibr" rid="R56">56</xref>] implementations of the aforementioned PLMs. For each model, we tokenize the protein sequences using the tokenizers provided by the authors and obtain token-wise vector representations using a forward pass of their encoders. These embedded vector representations of variable-length amino acid sequences are aggregated to produce fixed-size representations using mean pooling, which are then used as global sequence representations. Models-pecific details follow.</p><sec id="S20"><title>ESM2</title><p id="P49">We use the PyTorch-Hub implementation of ESM2 models [<xref ref-type="bibr" rid="R15">15</xref>] (<ext-link ext-link-type="uri" xlink:href="https://github.com/facebookresearch/esm">https://github.com/facebookresearch/esm</ext-link>) and rely on the readily-available <monospace>repr_layers</monospace> parameter of the <monospace>esm.ESM2.forward</monospace> method to collect the token embeddings, i.e. the amino acid-based embeddings, at any given internal layer of the model. Mean pooling is achieved by truncating each padded vector sequence at its correct length before computing the per-sequence mean.</p></sec><sec id="S21"><title>TCR-BERT</title><p id="P50">We use the HuggingFace [<xref ref-type="bibr" rid="R57">57</xref>] implementation of the base version of TCR-BERT (trained exclusively using amino acid masked language modeling), which is available under the label <monospace>wukevin/tcr-bert-mlm-only</monospace> on the HuggingFace Hub. Another version of the model, fine-tuned on a TCR binding prediction task, is available but raises the risk of having part of our test set exposed as its training data. We obtain mean-pooled embeddings with the SentenceTransformers library [<xref ref-type="bibr" rid="R58">58</xref>], which handles mean pooling automatically. Early layers were obtained by editing the HuggingFace model internals (truncating the <monospace>BertModel.encoder.layer</monospace> implementing the sequence of 12 <monospace>BertLayer</monospace>) before being passed to the <monospace>sentence_transformers.SentenceTransformer</monospace> object.</p></sec><sec id="S22"><title>AbLang</title><p id="P51">The AbLang [<xref ref-type="bibr" rid="R19">19</xref>] authors have made their model available as a standalone Python package(<ext-link ext-link-type="uri" xlink:href="https://github.com/oxpig/AbLang">https://github.com/oxpig/AbLang</ext-link>) which readily provides embedding computation through its main interface. We use this interface directly (calling ablang.pretrained objects with <monospace>mode=‘seqcoding’</monospace>). AbLang comprises two models, optimized for modeling antibody heavy and light chains, respectively. We use each model to obtain a mean-pooled encoding of each BCR separately, which we then concatenate to obtain a global antibody representation.</p></sec></sec><sec id="S23"><label>4.7</label><title>Model architecture for antibody specificity prediction</title><p id="P52">We evaluate PLM-based antibody affinity predictions through the lens of the AlphaSeq dataset [<xref ref-type="bibr" rid="R40">40</xref>], which provides a continuous label for the estimated binding affinity of antibodies to a specific SARS-CoV-2 epitope, including several replicate measurements for many sequences. We use PLMs to compute fixed-size embeddings of the antibody protein sequences, which reduces the task of predicting the affinity to a supervised regression problem, mapping embedding vectors to a continuous number. Given that AbLang provides separate embeddings for the antibody heavy and light chains, we obtain antibody-level representation by concatenating the two chain vector representations. To ensure a fair comparison with ESM2-based embeddings, we replicate this process for ESM2 models. Specifically, we individually embed each chain and then concatenate them both into a single vector.</p><p id="P53">We use multi-layer perceptrons of 2, 3, or 4 layers to map the high-dimensional sequence representations to a single continuous label. We use dropout regularization after the internal layers and before the non-linear ReLU activation functions, but neither dropout nor an activation function is applied to the output of the final layer. The model parameters are trained by optimizing the mean-squared error (MSE) loss using the Adam optimizer [<xref ref-type="bibr" rid="R59">59</xref>] until a minimum validation loss is achieved, based on an early-stopping algorithm with a patience of 5 steps. Given the large size of the dataset, we set the batch size to a fixed value of 128 to accelerate training and do not consider lower values.</p><p id="P54">Model hyperparameters are tuned using a random search with Monte-Carlo cross-validation with six repetitions. For each Monte Carlo replicate, 20% of the data is held out for testing and 20% of the remaining data is held out for validation. Hence, the data is split as follows, 64% for training, 20% for testing, and 16% for validation. For each training, model weights are randomly initialized and we use the training data to estimate the coordinate-wise mean and standard deviation of the embedding distribution and standardize training, validation, and test data with these estimates.</p><p id="P55">In each model, we specify the architecture by specifying the dimensions of the internal layers, while the input dimension is determined by the dimensionality of the embedding vectors used (see <xref ref-type="table" rid="T1">Table 1</xref>), multiplied by two due to the concatenation, and the output dimension is always 1. The explicit range of random parameters is listed below.<list list-type="bullet" id="L1"><list-item><p id="P56">Learning Rate: [10<sup>−4</sup>, 10<sup>−3</sup>, 10<sup>−2</sup>].</p></list-item><list-item><p id="P57">Dropout Rate: [0, 0.05, 0.1, 0.3].</p></list-item><list-item><p id="P58">Batch size: fixed to 128.</p></list-item><list-item><p id="P59">Hidden Layers: [(128, 32), (512, 128), (1024, 512, 512), (512, 512, 512, 512)].</p></list-item></list></p><p id="P60">In <xref ref-type="fig" rid="F6">Figure 6</xref>, we show the ten best hyperparameter (HP) configurations of each test PLMs with a box-plot generated with six samplings [<xref ref-type="bibr" rid="R60">60</xref>], ranked by their mean <italic>R<sup>2</sup></italic> test score. Strikingly, the best AbLang hyperparameter configuration comes at the 11th position, reinforcing our confidence in the superiority of ESM2 embeddings.</p></sec><sec id="S24"><label>4.8</label><title>Model architecture for TCR binding prediction</title><p id="P61">The ImmRep benchmark [<xref ref-type="bibr" rid="R37">37</xref>] sets up TCR-epitope binding prediction as a binary classification task where a TCR-epitope pair is labeled as positive if the TCR recognizes the epitope, while negatively labeled pairs indicate non-recognition. In the TCR task, we consider in this manuscript, we identify the individual epitope with the most positively labeled samples and simplify the task to a binary TCR classification problem. We take the CDR3-<italic>β</italic> segment of the TCR and use protein-PLMs as sequence featurizers, mapping variable-length amino-acid sequences to fixed-size vectors. The task then reduces to a standard supervised binary classification problem, mapping a vector of features to a binary label. Given the lack of obvious internal structure or invariances in the embedding spaces we consider, we use multi-layer perceptrons to map the high-dimensional sequence representations to a single binary label, normalized with a sigmoid function. We use dropout regularization after internal layers and before non-linear ReLU activation functions. The model parameters are trained by optimizing the binary cross-entropy loss using the Adam optimizer [<xref ref-type="bibr" rid="R59">59</xref>]. Training continues until a minimum validation loss is achieved. We assume that a minimum has been reached if there is no observed improvement after training for 20 epochs.</p><p id="P62">Model hyperparameters are tuned using a random search with Monte-Carlo cross-validation with six repetitions. For each Monte Carlo replicate, 20% of the data is held out for testing and 20% of the remaining data is held out for validation. Hence, the data is split as follows, 64% for training, 20% for testing, and 16% for validation. For each training fold, model weights are randomly initialized and we use the training data to estimate the coordinate-wise mean and standard deviation of the embedding distribution and standardize training, validation, and test data with these estimates.</p><p id="P63">In each model, the input dimension is determined by the dimensionality of the embedding vectors used and the output dimension is always 1. We specify the architecture by specifying the dimensions of the internal layers which we tune by defining a scaling factor <italic>λ</italic>. The explicit range of random hyperparameters is listed below.<list list-type="bullet" id="L2"><list-item><p id="P64">Learning Rate: [10<sup>−4</sup>, 5 × 10<sup>−4</sup>, 10<sup>−3</sup>, 5 × 10<sup>−3</sup>, 10<sup>−2</sup>].</p></list-item><list-item><p id="P65">Dropout Rate: [0, 0.05, 0.1, 0.15, 0.2, 0.3].</p></list-item><list-item><p id="P66">Batch Size: [8, 16, 32, 64, 128].</p></list-item><list-item><p id="P67">Hidden Layers: [(<italic>λ</italic> × 64), (<italic>λ</italic> × 32, <italic>λ ×</italic> 8), (<italic>λ</italic> × 64, <italic>λ ×</italic> 8), (<italic>λ</italic> × 256, <italic>λ ×</italic> 8), (<italic>λ</italic> × 32, <italic>λ ×</italic> 32, <italic>λ ×</italic> 8), (<italic>λ</italic> × 64, <italic>λ ×</italic> 32, <italic>λ ×</italic> 8)].</p></list-item></list></p><p id="P68">For each value of the hidden-layer-scale factor <italic>λ</italic> = 1, 2, 3, we sample 32 random choices of hyperparameters on which we run our six Monte-Carlo cross-validation replicates. The best models used to report scores are selected based on the mean AUC validation score across Monte Carlo replicates. We report test AUC score distributions among the 10 best hyperparameter choices in <xref ref-type="fig" rid="F7">Figure 7</xref>. The actual top-performing models’ hyperparameter choices are specified in <xref ref-type="table" rid="T7">Table 7</xref>.</p><p id="P69">Note that the range of parameters scanned in this experiment is more comprehensive than for antibody affinity regression. This is the result of our observation of the lack of significant difference between the best TCR-BERT- and ESM2-based models, which led us to perform a second random search with expanded parameter ranges and to introduce the layer-width scaling factor <italic>λ</italic>.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS189941-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d45aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S25"><title>Acknowledgments And Funding</title><p>This work was supported by the Swiss National Science Foundation Grant No 192128, and by the European Union’s Horizon 2020 research and innovation program under grant agreements No 765158 (COSMIC), No 813545 (HELICAL), and No 826121 (iPC).</p></ack><fn-group><fn id="FN2" fn-type="conflict"><p id="P70"><bold>Competing Interests</bold></p><p id="P71">There is NO Competing Interest.</p></fn><fn id="FN3" fn-type="con"><p id="P72"><bold>Author Contributions Statement</bold></p><p id="P73">A.P. and N.D. performed computations and wrote the manuscript, under the supervision of M.R.M. A.W. and S.G. performed computation of supplementary section A, under the supervision of M.R.M. and J.B. All authors have read, reviewed, and agreed to the published version of the manuscript.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parkin</surname><given-names>Jacqueline</given-names></name><name><surname>Cohen</surname><given-names>Bryony</given-names></name></person-group><article-title>An overview of the immune system</article-title><source>The Lancet</source><year>2001</year><volume>357</volume><issue>9270</issue><fpage>1777</fpage><lpage>1789</lpage><pub-id pub-id-type="pmid">11403834</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kovaltsuk</surname><given-names>Aleksandr</given-names></name><etal/></person-group><article-title>How B-cell receptor repertoire sequencing can be enriched with structural antibody data</article-title><source>Frontiers in immunology</source><year>2017</year><volume>8</volume><elocation-id>1753</elocation-id><pub-id pub-id-type="pmcid">PMC5727015</pub-id><pub-id pub-id-type="pmid">29276518</pub-id><pub-id pub-id-type="doi">10.3389/fimmu.2017.01753</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akbar</surname><given-names>Rahmad</given-names></name><etal/></person-group><article-title>A compact vocabulary of paratope-epitope interactions enables predictability of antibody-antigen binding</article-title><source>Cell Reports</source><year>2021</year><volume>34</volume><issue>11</issue><elocation-id>108856</elocation-id><pub-id pub-id-type="pmid">33730590</pub-id></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zarnitsyna</surname><given-names>Veronika I</given-names></name><etal/></person-group><article-title>Estimating the diversity, completeness, and cross-reactivity of the T cell repertoire</article-title><source>Frontiers in immunology</source><year>2013</year><volume>4</volume><fpage>485</fpage><pub-id pub-id-type="pmcid">PMC3872652</pub-id><pub-id pub-id-type="pmid">24421780</pub-id><pub-id pub-id-type="doi">10.3389/fimmu.2013.00485</pub-id></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhanati</surname><given-names>Yuval</given-names></name><etal/></person-group><article-title>Inferring processes underlying B-cell repertoire diversity</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><year>2015</year><volume>370</volume><issue>1676</issue><elocation-id>20140243</elocation-id><pub-id pub-id-type="pmcid">PMC4528420</pub-id><pub-id pub-id-type="pmid">26194757</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2014.0243</pub-id></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greiff</surname><given-names>Victor</given-names></name><etal/></person-group><article-title>Bioinformatic and statistical analysis of adaptive immune repertoires</article-title><source>Trends in immunology</source><year>2015</year><volume>36</volume><issue>11</issue><fpage>738</fpage><lpage>749</lpage><pub-id pub-id-type="pmid">26508293</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mason</surname><given-names>DerekM</given-names></name><etal/></person-group><article-title>Optimization of therapeutic antibodies by predicting antigen specificity from antibody sequence via deep learning</article-title><source>Nature Biomedical Engineering</source><year>2021</year><volume>5</volume><issue>6</issue><fpage>600</fpage><lpage>612</lpage><pub-id pub-id-type="pmid">33859386</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jogalekar</surname><given-names>ManasiP</given-names></name><etal/></person-group><article-title>CAR T-cell-based gene therapy for cancers: new perspectives, challenges, and clinical developments</article-title><source>Frontiers in immunology</source><year>2022</year><volume>13</volume><elocation-id>925985</elocation-id><pub-id pub-id-type="pmcid">PMC9355792</pub-id><pub-id pub-id-type="pmid">35936003</pub-id><pub-id pub-id-type="doi">10.3389/fimmu.2022.925985</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aghajanian</surname><given-names>Haig</given-names></name><name><surname>Rurik</surname><given-names>JoelG</given-names></name><name><surname>Epstein</surname><given-names>JonathanA</given-names></name></person-group><article-title>CAR-based therapies: opportunities for immuno-medicine beyond cancer</article-title><source>Nature metabolism</source><year>2022</year><volume>4</volume><issue>2</issue><fpage>163</fpage><lpage>169</lpage><pub-id pub-id-type="pmcid">PMC9947862</pub-id><pub-id pub-id-type="pmid">35228742</pub-id><pub-id pub-id-type="doi">10.1038/s42255-022-00537-5</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bepler</surname><given-names>Tristan</given-names></name><name><surname>Berger</surname><given-names>Bonnie</given-names></name></person-group><article-title>Learning the protein language: Evolution, structure, and function</article-title><source>Cell systems</source><year>2021</year><volume>12</volume><issue>6</issue><fpage>654</fpage><lpage>669</lpage><pub-id pub-id-type="pmcid">PMC8238390</pub-id><pub-id pub-id-type="pmid">34139171</pub-id><pub-id pub-id-type="doi">10.1016/j.cels.2021.05.017</pub-id></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandes</surname><given-names>Nadav</given-names></name><etal/></person-group><article-title>ProteinBERT: a universal deep-learning model of protein sequence and function</article-title><source>Bioinformatics</source><year>2022</year><volume>38</volume><issue>8</issue><fpage>2102</fpage><lpage>2110</lpage><pub-id pub-id-type="pmcid">PMC9386727</pub-id><pub-id pub-id-type="pmid">35020807</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btac020</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rives</surname><given-names>Alexander</given-names></name><etal/></person-group><article-title>Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</article-title><source>Proceedings of the National Academy of Sciences</source><year>2021</year><volume>118</volume><issue>15</issue><elocation-id>e2016239118</elocation-id><pub-id pub-id-type="pmcid">PMC8053943</pub-id><pub-id pub-id-type="pmid">33876751</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2016239118</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chowdhury</surname><given-names>Ratul</given-names></name><etal/></person-group><article-title>Single-sequence protein structure prediction using a language model and deep learning</article-title><source>Nature Biotechnology</source><year>2022</year><volume>40</volume><issue>11</issue><fpage>1617</fpage><lpage>1623</lpage><pub-id pub-id-type="pmcid">PMC10440047</pub-id><pub-id pub-id-type="pmid">36192636</pub-id><pub-id pub-id-type="doi">10.1038/s41587-022-01432-w</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madani</surname><given-names>Ali</given-names></name><etal/></person-group><article-title>Large language models generate functional protein sequences across diverse families</article-title><source>Nature Biotechnology</source><year>2023</year><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC10400306</pub-id><pub-id pub-id-type="pmid">36702895</pub-id><pub-id pub-id-type="doi">10.1038/s41587-022-01618-2</pub-id></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>Zeming</given-names></name><etal/></person-group><article-title>Evolutionary-scale prediction of atomic-level protein structure with a language model</article-title><source>Science</source><year>2023</year><volume>379</volume><issue>6637</issue><fpage>1123</fpage><lpage>1130</lpage><pub-id pub-id-type="pmid">36927031</pub-id></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bepler</surname><given-names>Tristan</given-names></name><name><surname>Berger</surname><given-names>Bonnie</given-names></name></person-group><source>Learning protein sequence embeddings using information from structure</source><conf-name>International Conference on Learning Representations</conf-name><year>2019</year></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hou</surname><given-names>Zilong</given-names></name><etal/></person-group><article-title>Learning the protein language of proteome-wide protein-protein binding sites via explainable ensemble deep learning</article-title><source>Communications Biology</source><year>2023</year><volume>6</volume><issue>1</issue><fpage>73</fpage><pub-id pub-id-type="pmcid">PMC9849350</pub-id><pub-id pub-id-type="pmid">36653447</pub-id><pub-id pub-id-type="doi">10.1038/s42003-023-04462-5</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>Wayland</given-names></name><etal/></person-group><article-title>Alignment-free estimation of sequence conservation for identifying functional sites using protein sequence embeddings</article-title><source>Briefings in Bioinformatics</source><year>2023</year><volume>24</volume><issue>1</issue><elocation-id>bbac599</elocation-id><pub-id pub-id-type="pmcid">PMC9851297</pub-id><pub-id pub-id-type="pmid">36631405</pub-id><pub-id pub-id-type="doi">10.1093/bib/bbac599</pub-id></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsen</surname><given-names>TobiasH</given-names></name><name><surname>Moal</surname><given-names>IainH</given-names></name><name><surname>Deane</surname><given-names>CharlotteM</given-names></name></person-group><article-title>AbLang: an antibody language model for completing antibody sequences</article-title><source>Bioinformatics Advances</source><year>2022</year><volume>2</volume><issue>1</issue><elocation-id>vbac046</elocation-id><pub-id pub-id-type="pmcid">PMC9710568</pub-id><pub-id pub-id-type="pmid">36699403</pub-id><pub-id pub-id-type="doi">10.1093/bioadv/vbac046</pub-id></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leem</surname><given-names>Jinwoo</given-names></name><etal/></person-group><article-title>Deciphering the language of antibodies using self-supervised learning</article-title><source>Patterns</source><year>2022</year><volume>3</volume><issue>7</issue><pub-id pub-id-type="pmcid">PMC9278498</pub-id><pub-id pub-id-type="pmid">35845836</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2022.100513</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>Rohit</given-names></name><etal/></person-group><article-title>Learning the Language of Antibody Hypervariability</article-title><source>bioRxiv</source><year>2023</year><fpage>2023</fpage><lpage>04</lpage></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Kevin</given-names></name><etal/></person-group><article-title>TCR-BERT: learning the grammar of T-cell receptors for flexible antigen-xbinding analyses</article-title><source>bioRxiv</source><year>2021</year><fpage>2021</fpage><lpage>11</lpage></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Pengfei</given-names></name><etal/></person-group><article-title>Context-Aware Amino Acid Embedding Advances Analysis of TCR-Epitope Interactions</article-title><year>2023</year><month>July</month></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Danqing</given-names></name><name><surname>Fei</surname><given-names>YE</given-names></name><name><surname>Zhou</surname><given-names>Hao</given-names></name></person-group><source>On pre-training language model for antibody</source><conf-name>The Eleventh International Conference on Learning Representations</conf-name><year>2023</year></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzek</surname><given-names>BarisE</given-names></name><etal/></person-group><article-title>UniRef: comprehensive and non-redundant UniProt reference clusters</article-title><source>Bioinformatics</source><year>2007</year><volume>23</volume><issue>10</issue><fpage>1282</fpage><lpage>1288</lpage><pub-id pub-id-type="pmid">17379688</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olsen</surname><given-names>TobiasH</given-names></name><name><surname>Boyles</surname><given-names>Fergus</given-names></name><name><surname>Deane</surname><given-names>CharlotteM</given-names></name></person-group><article-title>Observed Antibody Space: A diverse database of cleaned, annotated, and translated unpaired and paired antibody sequences</article-title><source>Protein Science</source><year>2022</year><volume>31</volume><issue>1</issue><fpage>141</fpage><lpage>146</lpage><pub-id pub-id-type="pmcid">PMC8740823</pub-id><pub-id pub-id-type="pmid">34655133</pub-id><pub-id pub-id-type="doi">10.1002/pro.4205</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Wei</given-names></name><etal/></person-group><article-title>PIRD: pan immune repertoire database</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><issue>3</issue><fpage>897</fpage><lpage>903</lpage><pub-id pub-id-type="pmid">31373607</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shugay</surname><given-names>Mikhail</given-names></name><etal/></person-group><article-title>VDJdb: a curated database of T-cell receptor sequences with known antigen specificity</article-title><source>Nucleic acids research</source><year>2018</year><volume>46</volume><issue>D1</issue><fpage>D419</fpage><lpage>D427</lpage><pub-id pub-id-type="pmcid">PMC5753233</pub-id><pub-id pub-id-type="pmid">28977646</pub-id><pub-id pub-id-type="doi">10.1093/nar/gkx760</pub-id></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Levenshtein</surname><given-names>VladimirI</given-names></name></person-group><article-title>Binary codes capable of correcting deletions, insertions, and reversals</article-title><source>Soviet physics doklady</source><publisher-loc>Soviet Union</publisher-loc><year>1966</year><volume>10</volume><issue>8</issue><fpage>707</fpage><lpage>710</lpage></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelissier</surname><given-names>Aurelien</given-names></name><etal/></person-group><article-title>Exploring the impact of clonal definition on B-cell diversity: implications for the analysis of immune repertoires</article-title><source>Frontiers in Immunology</source><year>2023</year><volume>14</volume><pub-id pub-id-type="pmcid">PMC10150052</pub-id><pub-id pub-id-type="pmid">37138881</pub-id><pub-id pub-id-type="doi">10.3389/fimmu.2023.1123968</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelissier</surname><given-names>Aurelien</given-names></name><etal/></person-group><article-title>Convergent evolution and B-cell recirculation in germinal centers in a human lymph node</article-title><source>Life Science Alliance</source><year>2023</year><volume>6</volume><issue>11</issue><pub-id pub-id-type="pmcid">PMC10462906</pub-id><pub-id pub-id-type="pmid">37640448</pub-id><pub-id pub-id-type="doi">10.26508/lsa.202301959</pub-id></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azaria</surname><given-names>Amos</given-names></name><name><surname>Mitchell</surname><given-names>Tom</given-names></name></person-group><article-title>The internal state of an llm knows when its lying</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2304.13734</elocation-id></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>Chi</given-names></name><etal/></person-group><article-title>In-Context Learning of Large Language Models Explained as Kernel Regression</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2305.12766</elocation-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Kenneth</given-names></name><etal/></person-group><article-title>Emergent world representations: Exploring a sequence model trained on a synthetic task</article-title><source>arXiv preprint</source><year>2022</year><elocation-id>arXiv:2210.13382</elocation-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Firl</surname><given-names>DanielJ</given-names></name><etal/></person-group><article-title>Capturing change in clonal composition amongst single mouse germinal centers</article-title><source>Elife</source><year>2018</year><volume>7</volume><elocation-id>e33051</elocation-id><pub-id pub-id-type="pmcid">PMC6070335</pub-id><pub-id pub-id-type="pmid">30066671</pub-id><pub-id pub-id-type="doi">10.7554/eLife.33051</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greiff</surname><given-names>Victor</given-names></name><name><surname>Yaari</surname><given-names>Gur</given-names></name><name><surname>Cowell</surname><given-names>LindsayG</given-names></name></person-group><article-title>Mining adaptive immune receptor repertoires for biological and clinical information using machine learning</article-title><source>Current Opinion in Systems Biology</source><year>2020</year><volume>24</volume><fpage>109</fpage><lpage>119</lpage></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meysman</surname><given-names>Pieter</given-names></name><etal/></person-group><article-title>Benchmarking solutions to the T-cell receptor epitope prediction problem: IMMREP22 workshop report</article-title><source>ImmunoInformatics</source><year>2023</year><volume>9</volume><elocation-id>100024</elocation-id></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>Anna</given-names></name><name><surname>Born</surname><given-names>Jannis</given-names></name><name><surname>Martinez</surname><given-names>MariaRodriguez</given-names></name></person-group><article-title>TITAN: T-cell receptor specificity prediction with bimodal attention networks</article-title><source>Bioinformatics</source><year>2021</year><volume>37</volume><issue>Supplement_1</issue><fpage>i237</fpage><lpage>i244</lpage><pub-id pub-id-type="pmcid">PMC8275323</pub-id><pub-id pub-id-type="pmid">34252922</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btab294</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mann</surname><given-names>HB</given-names></name><name><surname>Whitney</surname><given-names>DR</given-names></name></person-group><article-title>On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other</article-title><source>The Annals of Mathematical Statistics</source><year>1947</year><volume>18</volume><issue>1</issue><fpage>50</fpage><lpage>60</lpage><comment>url: https://doi.org/10.1214/aoms/1177730491</comment><pub-id pub-id-type="doi">10.1214/aoms/1177730491</pub-id></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelhart</surname><given-names>Emily</given-names></name><etal/></person-group><article-title>A dataset comprised of binding interactions for 104,972 antibodies against a SARS-CoV-2 peptide</article-title><source>Scientific Data</source><year>2022</year><volume>9</volume><issue>1</issue><fpage>653</fpage><pub-id pub-id-type="pmcid">PMC9606274</pub-id><pub-id pub-id-type="pmid">36289234</pub-id><pub-id pub-id-type="doi">10.1038/s41597-022-01779-4</pub-id></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oreste</surname><given-names>Umberto</given-names></name><name><surname>Ametrano</surname><given-names>Alessia</given-names></name><name><surname>Coscia</surname><given-names>MariaRosaria</given-names></name></person-group><article-title>On origin and evolution of the antibody molecule</article-title><source>Biology</source><year>2021</year><volume>10</volume><issue>2</issue><fpage>140</fpage><pub-id pub-id-type="pmcid">PMC7916673</pub-id><pub-id pub-id-type="pmid">33578914</pub-id><pub-id pub-id-type="doi">10.3390/biology10020140</pub-id></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hie</surname><given-names>BrianL</given-names></name><etal/></person-group><article-title>Efficient evolution of human antibodies from general protein language models</article-title><source>Nature Biotechnology</source><year>2023</year><pub-id pub-id-type="pmid">37095349</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pélissier</surname><given-names>Aurélien</given-names></name><etal/></person-group><article-title>Computational Model Reveals a Stochastic Mechanism behind Germinal Center Clonal Bursts</article-title><source>Cells</source><year>2020</year><volume>9</volume><issue>6</issue><fpage>1448</fpage><pub-id pub-id-type="pmcid">PMC7349200</pub-id><pub-id pub-id-type="pmid">32532145</pub-id><pub-id pub-id-type="doi">10.3390/cells9061448</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conti</surname><given-names>Simone</given-names></name><name><surname>Lau</surname><given-names>EdmondY</given-names></name><name><surname>Ovchinnikov</surname><given-names>Victor</given-names></name></person-group><article-title>On the rapid calculation of binding affinities for antigen and antibody design and affinity maturation simulations</article-title><source>Antibodies</source><year>2022</year><volume>11</volume><issue>3</issue><fpage>51</fpage><pub-id pub-id-type="pmcid">PMC9397028</pub-id><pub-id pub-id-type="pmid">35997345</pub-id><pub-id pub-id-type="doi">10.3390/antib11030051</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia-Valiente</surname><given-names>Rodrigo</given-names></name><etal/></person-group><article-title>Understanding repertoire sequencing data through a multiscale computational model of the germinal center</article-title><source>npj Systems Biology and Applications</source><year>2023</year><volume>9</volume><issue>1</issue><fpage>8</fpage><pub-id pub-id-type="pmcid">PMC10019394</pub-id><pub-id pub-id-type="pmid">36927990</pub-id><pub-id pub-id-type="doi">10.1038/s41540-023-00271-y</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Conti</surname><given-names>Simone</given-names></name><etal/></person-group><article-title>Multiscale affinity maturation simulations to elicit broadly neutralizing antibodies against HIV</article-title><source>PLoS Computational Biology</source><year>2022</year><volume>18</volume><issue>4</issue><elocation-id>e1009391</elocation-id><pub-id pub-id-type="pmcid">PMC9020693</pub-id><pub-id pub-id-type="pmid">35442968</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009391</pub-id></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faris</surname><given-names>JonathanG</given-names></name><etal/></person-group><article-title>Moving the needle: Employing deep reinforcement learning to push the boundaries of coarse-grained vaccine models</article-title><source>Frontiers in Immunology</source><year>2022</year><volume>13</volume><elocation-id>1029167</elocation-id><pub-id pub-id-type="pmcid">PMC9670804</pub-id><pub-id pub-id-type="pmid">36405722</pub-id><pub-id pub-id-type="doi">10.3389/fimmu.2022.1029167</pub-id></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yujian</surname><given-names>Li</given-names></name><name><surname>Bo</surname><given-names>Liu</given-names></name></person-group><article-title>A normalized Levenshtein distance metric</article-title><source>IEEE transactions on pattern analysis and machine intelligence</source><year>2007</year><volume>29</volume><issue>6</issue><fpage>1091</fpage><lpage>1095</lpage><pub-id pub-id-type="pmid">17431306</pub-id></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullner</surname><given-names>Daniel</given-names></name></person-group><article-title>Modern hierarchical, agglomerative clustering algorithms</article-title><source>arXiv preprint</source><year>2011</year><elocation-id>arXiv:1109.2378</elocation-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>NamitaT</given-names></name><etal/></person-group><article-title>Hierarchical clustering can identify B cell clones with high confidence in Ig repertoire sequencing data</article-title><source>The Journal of Immunology</source><year>2017</year><volume>198</volume><issue>6</issue><fpage>2489</fpage><lpage>2499</lpage><pub-id pub-id-type="pmcid">PMC5340603</pub-id><pub-id pub-id-type="pmid">28179494</pub-id><pub-id pub-id-type="doi">10.4049/jimmunol.1601850</pub-id></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>JulieD</given-names></name><name><surname>Higgins</surname><given-names>DesmondG</given-names></name><name><surname>Gibson</surname><given-names>TobyJ</given-names></name></person-group><article-title>CLUSTAL W: improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position-specific gap penalties and weight matrix choice</article-title><source>Nucleic acids research</source><year>1994</year><volume>22</volume><issue>22</issue><fpage>4673</fpage><lpage>4680</lpage><pub-id pub-id-type="pmcid">PMC308517</pub-id><pub-id pub-id-type="pmid">7984417</pub-id><pub-id pub-id-type="doi">10.1093/nar/22.22.4673</pub-id></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeWitt</surname><given-names>WilliamS</given-names><suffix>III</suffix></name><etal/></person-group><article-title>Using genotype abundance to improve phylogenetic inference</article-title><source>Molecular biology and evolution</source><year>2018</year><volume>35</volume><issue>5</issue><fpage>1253</fpage><lpage>1265</lpage><pub-id pub-id-type="pmcid">PMC5913685</pub-id><pub-id pub-id-type="pmid">29474671</pub-id><pub-id pub-id-type="doi">10.1093/molbev/msy020</pub-id></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdollahi</surname><given-names>Nika</given-names></name><etal/></person-group><article-title>Reconstructing B cell lineage trees with minimum spanning tree and genotype abundances</article-title><source>BMC bioinformatics</source><year>2023</year><volume>24</volume><issue>1</issue><fpage>70</fpage><pub-id pub-id-type="pmcid">PMC9972711</pub-id><pub-id pub-id-type="pmid">36849917</pub-id><pub-id pub-id-type="doi">10.1186/s12859-022-05112-z</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jahn</surname><given-names>Katharina</given-names></name><name><surname>Kuipers</surname><given-names>Jack</given-names></name><name><surname>Beerenwinkel</surname><given-names>Niko</given-names></name></person-group><article-title>Tree inference for single-cell data</article-title><source>Genome biology</source><year>2016</year><volume>17</volume><issue>1</issue><fpage>86</fpage><pub-id pub-id-type="pmcid">PMC4858868</pub-id><pub-id pub-id-type="pmid">27149953</pub-id><pub-id pub-id-type="doi">10.1186/s13059-016-0936-x</pub-id></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Shimoyama</surname><given-names>Yuki</given-names></name></person-group><source>pyCirclize: Circular visualization in Python</source><year>2022</year><month>Dec</month><comment>url: <ext-link ext-link-type="uri" xlink:href="https://github.com/moshi4/pyCirclize">https://github.com/moshi4/pyCirclize</ext-link></comment></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>Adam</given-names></name><etal/></person-group><chapter-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</chapter-title><source>Advances in Neural Information Processing Systems 32</source><publisher-name>Curran Associates, Inc</publisher-name><year>2019</year><fpage>8024</fpage><lpage>8035</lpage><comment>url: <ext-link ext-link-type="uri" xlink:href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</ext-link></comment></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>Thomas</given-names></name><etal/></person-group><source>Transformers: State-of-the-Art Natural Language Processing</source><conf-name>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2020</year><month>Oct</month><fpage>38</fpage><lpage>45</lpage><comment>Online: url: <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/2020.emnlp-demos.6">https://www.aclweb.org/anthology/2020.emnlp-demos.6</ext-link></comment></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Reimers</surname><given-names>Nils</given-names></name><name><surname>Gurevych</surname><given-names>Iryna</given-names></name></person-group><source>Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</source><conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2019</year><month>Nov</month><comment>url: <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1908.10084">https://arxiv.org/abs/1908.10084</ext-link></comment></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DiederikP</given-names></name><name><surname>Ba</surname><given-names>Jimmy</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Bengio</surname><given-names>Yoshua</given-names></name><name><surname>LeCun</surname><given-names>Yann</given-names></name></person-group><source>Adam: A Method for Stochastic Optimization</source><conf-name>3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</conf-name><year>2015</year><comment>url: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></comment></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larsen</surname><given-names>RussellD</given-names></name></person-group><article-title>Box-and-whisker plots</article-title><source>Journal of Chemical Education</source><year>1985</year><volume>62</volume><issue>4</issue><fpage>302</fpage></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Advani</surname><given-names>MadhuS</given-names></name><name><surname>Saxe</surname><given-names>AndrewM</given-names></name><name><surname>Sompolinsky</surname><given-names>Haim</given-names></name></person-group><article-title>High-dimensional dynamics of generalization error in neural networks</article-title><source>Neural Networks</source><year>2020</year><volume>132</volume><fpage>428</fpage><lpage>446</lpage><comment>issn: 0893-6080 url: <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0893608020303117">https://www.sciencedirect.com/science/article/pii/S0893608020303117</ext-link></comment><pub-id pub-id-type="pmcid">PMC7685244</pub-id><pub-id pub-id-type="pmid">33022471</pub-id><pub-id pub-id-type="doi">10.1016/j.neunet.2020.08.022</pub-id></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belkin</surname><given-names>Mikhail</given-names></name><etal/></person-group><article-title>Reconciling modern machine-learning practice and the classical bias–variance trade-off</article-title><source>Proceedings of the National Academy of Sciences</source><year>2019</year><volume>116</volume><issue>32</issue><fpage>15849</fpage><lpage>15854</lpage><pub-id pub-id-type="pmcid">PMC6689936</pub-id><pub-id pub-id-type="pmid">31341078</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1903070116</pub-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakkiran</surname><given-names>Preetum</given-names></name><etal/></person-group><article-title>Deep double descent: Where bigger models and more data hurt</article-title><source>Journal of Statistical Mechanics: Theory and Experiment</source><year>2021</year><volume>2021</volume><issue>12</issue><elocation-id>124003</elocation-id></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bommasani</surname><given-names>Rishi</given-names></name><etal/></person-group><source>On the Opportunities and Risks of Foundation Models</source><publisher-name>arXiv 2108.0 7258 [cs.LG]</publisher-name><year>2022</year><pub-id pub-id-type="pmcid">PMC9344208</pub-id><pub-id pub-id-type="pmid">35923379</pub-id><pub-id pub-id-type="doi">10.1148/ryai.220119</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>Relationship between Levenshtein and Euclidean distance in the (A) AbLang and (B) ESM2-650M-L6 embedding space. The sequences are BCR heavy chains taken from 10 germinal center B cells in the same human lymph node [<xref ref-type="bibr" rid="R31">31</xref>].</p></caption><graphic xlink:href="EMS189941-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>ESM2-650M-L33 t-SNE representation of BCR heavy chains of the GC dataset [<xref ref-type="bibr" rid="R31">31</xref>] colored according to (A) their V-gene assignment and (B) their GC of origin. In (C) and (D), we show the distribution of pairwise Euclidean distances in the ESM2 space for BCRs within the same class and across different classes.</p></caption><graphic xlink:href="EMS189941-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Phylogeny analysis of a B cell clone shared among three germinal centers.</title><p>(A) Circular phylogenetic tree constructed from a dendrogram of Levenshtein distances between sequences. (B) Visualization of the same B cell phylogeny in the AbLang space. Observed sequences are represented by circled points, while inferred intermediate sequences in the phylogeny are denoted by small points and colored based on their nearest neighbor sequence. Each line represents a single nucleotide mutation between two sequences.</p></caption><graphic xlink:href="EMS189941-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Quantification of displacement in the embedding space after one nucleotide mutation.</title><p>(A, B) Distribution of Euclidean distance displacements induced by selected and random mutations in the AbLang and ESM2-650M embeddings respectively. (C) log-ratio between the displacements induced by selected and random mutations for different embeddings. The Levenshtein distance is also shown for reference. The values are averaged over the 50 most abundant clones with error bars representing one standard deviation. If no layers are indicated on the PLM <italic>x</italic>-axis, the last layer was used.</p></caption><graphic xlink:href="EMS189941-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Test performance of hyperparameter-tuned regression models taking AbLang-L12 and ESM2-650M-L6 embeddings as input as a function of the training set size.</title><p>(a) Test R<sup>2</sup> scores of models with the best validation R<sup>2</sup>. (b) Test MAE scores of models with the best validation MAE.</p></caption><graphic xlink:href="EMS189941-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Test <italic>R</italic><sup>2</sup> score distributions of the 10 best TCR binding classifier hyperparameter (HP) configurations for each PLM we tested.</title><p>We rank hyperparameter choices based on their mean validation <italic>R</italic><sup>2</sup> score across six Monte Carlo replicates, which we visualize with a Box-plot [<xref ref-type="bibr" rid="R60">60</xref>]. Each position on the horizontal axis corresponds to a choice of PLM and HP.</p></caption><graphic xlink:href="EMS189941-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Test AUC score distributions of the 10 best TCR binding classifier hyperparameter (HP) configurations for each embedding we tested.</title><p>We rank hyperparameter choices based on their mean validation AUC score across six Monte Carlo replicates, which we visualize with a Box-plot [<xref ref-type="bibr" rid="R60">60</xref>]</p></caption><graphic xlink:href="EMS189941-f007"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Characteristics of PLMs used to embed immune sequences in this article. All layers of a given PLM have the same dimension.</title></caption><table frame="box" rules="cols"><thead><tr><th align="center" valign="middle" style="border-top: hidden; border-left: hidden"/><th align="center" valign="middle" colspan="4">ESM2 [<xref ref-type="bibr" rid="R15">15</xref>]</th><th align="center" valign="middle">AbLang [<xref ref-type="bibr" rid="R19">19</xref>]</th><th align="center" valign="middle">TCR-BERT [<xref ref-type="bibr" rid="R22">22</xref>]</th></tr><tr style="border-top: solid thin"><th align="center" valign="middle">Input</th><th align="center" valign="middle" colspan="4">Any protein</th><th align="center" valign="middle">BCR / Antibody</th><th align="center" valign="middle">T-cell CDR3-<italic>β</italic></th></tr><tr style="border-top: solid thin"><th align="center" valign="middle">Trained on</th><th align="center" valign="middle" colspan="4">UniRef [<xref ref-type="bibr" rid="R25">25</xref>]</th><th align="center" valign="middle">OAS [<xref ref-type="bibr" rid="R26">26</xref>]</th><th align="center" valign="middle">PIRD[<xref ref-type="bibr" rid="R27">27</xref>], VDJdb[<xref ref-type="bibr" rid="R28">28</xref>]</th></tr><tr style="border-top: solid thin"><th align="center" valign="middle">Training set size</th><th align="center" valign="middle" colspan="4">60M</th><th align="center" valign="middle">14M</th><th align="center" valign="middle">100K</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="center" valign="middle">Nr. Parameters</td><td align="center" valign="middle">8M</td><td align="center" valign="middle">35M</td><td align="center" valign="middle">150M</td><td align="center" valign="middle">650M</td><td align="center" valign="middle">~250M</td><td align="center" valign="middle">~250M</td></tr><tr><td align="center" valign="middle">Nr. Layers</td><td align="center" valign="middle">6</td><td align="center" valign="middle">12</td><td align="center" valign="middle">30</td><td align="center" valign="middle">33</td><td align="center" valign="middle">12</td><td align="center" valign="middle">12</td></tr><tr><td align="center" valign="middle">Attention Heads</td><td align="center" valign="middle">20</td><td align="center" valign="middle">20</td><td align="center" valign="middle">20</td><td align="center" valign="middle">20</td><td align="center" valign="middle">12</td><td align="center" valign="middle">12</td></tr><tr><td align="center" valign="middle">Embedding dim.</td><td align="center" valign="middle">320</td><td align="center" valign="middle">480</td><td align="center" valign="middle">640</td><td align="center" valign="middle">1280</td><td align="center" valign="middle">768</td><td align="center" valign="middle">768</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Relationship of distance in various embeddings to Levenshtein distance of B cell receptors and T cell receptors sequences.</title><p>Note that for compatibility with TCR-BERT, only CDR3-<italic>β</italic> sequences are considered for T cells.</p></caption><table frame="box" rules="cols"><thead><tr><th align="center" valign="middle" style="border-top: hidden; border-left: hidden"/><th align="center" valign="middle">Pearson (B cells)</th><th align="center" valign="middle">Kendall (B cells)</th><th align="center" valign="middle">Pearson (T cells)</th><th align="center" valign="middle">Kendall (T cells)</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="center" valign="middle">AbLang-L12</td><td align="center" valign="middle">0.58</td><td align="center" valign="middle">0.34</td><td align="center" valign="middle">—</td><td align="center" valign="middle">—</td></tr><tr><td align="center" valign="middle">TCR-BERT-L8</td><td align="center" valign="middle">—</td><td align="center" valign="middle">—</td><td align="center" valign="middle"><bold>0.75</bold></td><td align="center" valign="middle"><bold>0.52</bold></td></tr><tr><td align="center" valign="middle">TCR-BERT-L12</td><td align="center" valign="middle">—</td><td align="center" valign="middle">—</td><td align="center" valign="middle">0.73</td><td align="center" valign="middle">0.50</td></tr><tr><td align="center" valign="middle">ESM2-650M-L6</td><td align="center" valign="middle"><bold>0.76</bold></td><td align="center" valign="middle"><bold>0.58</bold></td><td align="center" valign="middle">0.59</td><td align="center" valign="middle">0.39</td></tr><tr><td align="center" valign="middle">ESM2-650M-L33</td><td align="center" valign="middle">0.63</td><td align="center" valign="middle">0.41</td><td align="center" valign="middle">0.49</td><td align="center" valign="middle">0.32</td></tr><tr><td align="center" valign="middle">ESM2-150M-L30</td><td align="center" valign="middle">0.63</td><td align="center" valign="middle">0.42</td><td align="center" valign="middle">0.44</td><td align="center" valign="middle">0.23</td></tr><tr><td align="center" valign="middle">ESM2-35M-L12</td><td align="center" valign="middle">0.60</td><td align="center" valign="middle">0.47</td><td align="center" valign="middle">0.42</td><td align="center" valign="middle">0.30</td></tr><tr><td align="center" valign="middle">ESM2-8M-L6</td><td align="center" valign="middle">0.40</td><td align="center" valign="middle">0.35</td><td align="center" valign="middle">0.44</td><td align="center" valign="middle">0.30</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>AUC of different paired class differentiation tasks (i.e. do 2 BCRs belong to the same class?) with the Euclidean distance between the embeddings.</title><p>The classification performance using the Levenshtein distance is also provided for comparison.</p></caption><table frame="box" rules="cols"><thead><tr><th align="center" valign="middle" style="border-top: hidden; border-left: hidden"/><th align="center" valign="middle">V-gene</th><th align="center" valign="middle">J-gene</th><th align="center" valign="middle">GC</th><th align="center" valign="middle">Clone</th></tr></thead><tbody><tr style="border-top: solid thin"><td align="center" valign="middle">Levenshtein</td><td align="center" valign="middle"><bold>0.98</bold></td><td align="center" valign="middle"><bold>0.60</bold></td><td align="center" valign="middle">0.55</td><td align="center" valign="middle"><bold>0.99</bold></td></tr><tr><td align="center" valign="middle">AbLang-L12</td><td align="center" valign="middle">0.84</td><td align="center" valign="middle">0.59</td><td align="center" valign="middle"><bold>0.57</bold></td><td align="center" valign="middle">0.96</td></tr><tr><td align="center" valign="middle">ESM2-650M-L6</td><td align="center" valign="middle">0.90</td><td align="center" valign="middle">0.58</td><td align="center" valign="middle">0.55</td><td align="center" valign="middle"><bold>0.99</bold></td></tr><tr><td align="center" valign="middle">ESM2-650M-L33</td><td align="center" valign="middle">0.90</td><td align="center" valign="middle">0.58</td><td align="center" valign="middle">0.55</td><td align="center" valign="middle">0.97</td></tr><tr><td align="center" valign="middle">ESM2-150M-L30</td><td align="center" valign="middle">0.89</td><td align="center" valign="middle">0.56</td><td align="center" valign="middle">0.54</td><td align="center" valign="middle">0.97</td></tr><tr><td align="center" valign="middle">ESM2-35M-L12</td><td align="center" valign="middle">0.87</td><td align="center" valign="middle">0.58</td><td align="center" valign="middle">0.55</td><td align="center" valign="middle">0.97</td></tr><tr><td align="center" valign="middle">ESM2-8M-L6</td><td align="center" valign="middle">0.81</td><td align="center" valign="middle">0.57</td><td align="center" valign="middle">0.56</td><td align="center" valign="middle">0.96</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="float"><label>Table 4</label><caption><title>Test metrics evaluated on binary predictions of TCR binding to the GILGFVFTL epitope for hyperparameter-tuned classifiers (selecting on validation AUC) taking different PLM embeddings as input.</title><p>The highest mean value is indicated with an asterisk, and the standard deviation across Monte Carlo repetitions is indicated in parentheses as the uncertainty on the last significant digit. Values indicated in bold are non-dominated by the best score according to a multiple-testing-corrected Mann-Whitney U-test [<xref ref-type="bibr" rid="R39">39</xref>]. Arrows indicate whether higher (↑) or lower (↓) is better.</p></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="middle" style="border-right: solid thin">Embedder</th><th align="center" valign="middle">AUC(↑)</th><th align="center" valign="middle">Accuracy(↑)</th><th align="center" valign="middle">F1(↑)</th><th align="center" valign="middle">Precision(↑)</th><th align="center" valign="middle">Recall(↑)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right: solid thin">TCR-BERT-L8</td><td align="center" valign="middle"><bold>0</bold>.<bold>934</bold>(<bold>3</bold>)*</td><td align="center" valign="middle"><bold>0</bold>.<bold>891</bold>(<bold>6</bold>)*</td><td align="center" valign="middle"><bold>0</bold>.<bold>884</bold>(<bold>7</bold>)*</td><td align="center" valign="middle"><bold>0</bold>.<bold>941</bold>(<bold>8</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>83</bold>(<bold>1</bold>)<sup>*</sup></td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">TCR-BERT-L12</td><td align="center" valign="middle"><bold>0</bold>.<bold>930</bold>(<bold>1</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>890</bold>(<bold>1</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>880</bold>(<bold>1</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>942</bold>(<bold>3</bold>)<sup>*</sup></td><td align="center" valign="middle"><bold>0</bold>.<bold>83</bold>(<bold>2</bold>)<sup>*</sup></td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L6</td><td align="center" valign="middle"><bold>0</bold>.<bold>92β</bold>(<bold>?</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>881</bold>(<bold>8</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>874</bold>(<bold>8</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>931</bold>(<bold>8</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>82</bold>(<bold>1</bold>)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L5</td><td align="center" valign="middle">0.917(7)</td><td align="center" valign="middle">0.873(7)</td><td align="center" valign="middle">0.865(8)</td><td align="center" valign="middle">0.924(7)</td><td align="center" valign="middle"><bold>0</bold>.<bold>81</bold>(<bold>1</bold>)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L33</td><td align="center" valign="middle"><bold>0</bold>.<bold>916</bold>(<bold>6</bold>)</td><td align="center" valign="middle">0.866(7)</td><td align="center" valign="middle">0.857(8)</td><td align="center" valign="middle">0.916(8)</td><td align="center" valign="middle"><bold>0</bold>.<bold>80</bold>(<bold>1</bold>)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-35M-L12</td><td align="center" valign="middle">0.910(4)</td><td align="center" valign="middle">0.854(5)</td><td align="center" valign="middle">0.847(6)</td><td align="center" valign="middle">0.889(4)</td><td align="center" valign="middle"><bold>0</bold>.<bold>81</bold>(<bold>1</bold>)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L30</td><td align="center" valign="middle">0.904(7)</td><td align="center" valign="middle">0.850(8)</td><td align="center" valign="middle">0.84(1)</td><td align="center" valign="middle">0.894(7)</td><td align="center" valign="middle">0.79(1)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-8M-L6</td><td align="center" valign="middle">0.904(5)</td><td align="center" valign="middle">0.848(8)</td><td align="center" valign="middle">0.84(1)</td><td align="center" valign="middle">0.90(1)</td><td align="center" valign="middle">0.78(1)</td></tr></tbody></table></table-wrap><table-wrap id="T5" orientation="portrait" position="float"><label>Table 5</label><caption><title>Test metrics evaluated on predictions of antibody binding to a SARS-CoV-2 epitope for hyperparameter-tuned regression models (selecting on validation R<sup>2</sup>) taking different PLM embeddings as input.</title><p>Values indicated in bold are non-dominated by the best score according to a multiple-testing-corrected WMU test. Arrows indicate whether higher (↑) or lower (↓) is better.</p></caption><table frame="void" rules="groups"><thead><tr><th align="center" valign="middle" style="border-right: solid thin">Embedder</th><th align="center" valign="middle">MSE (↓)</th><th align="center" valign="middle">MAE (↓)</th><th align="center" valign="middle">R<sup>2</sup> (↑)</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L6</td><td align="center" valign="middle"><bold>0</bold>.<bold>119</bold>(<bold>1</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>2659</bold>(<bold>9</bold>)</td><td align="center" valign="middle"><bold>0</bold>.<bold>455</bold>(<bold>7</bold>)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L33</td><td align="center" valign="middle">0.123(1)</td><td align="center" valign="middle">0.271(2)</td><td align="center" valign="middle">0.438(6)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">AbLang-L12</td><td align="center" valign="middle">0.127(1)</td><td align="center" valign="middle">0.276(3)</td><td align="center" valign="middle">0.419(4)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-35M-L12</td><td align="center" valign="middle">0.129(1)</td><td align="center" valign="middle">0.279(2)</td><td align="center" valign="middle">0.413(5)</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L30</td><td align="center" valign="middle">0.1305(9)</td><td align="center" valign="middle">0.283(2)</td><td align="center" valign="middle">0.404(4)</td></tr></tbody></table></table-wrap><table-wrap id="T6" orientation="portrait" position="float"><label>Table 6</label><caption><title>Optimal hyperparameter choices for BCR affinity regression models (Multilayer perceptron) taking embeddings from each PLM tested.</title><p>The layers correspond to the output dimension of each hidden layer and the number of parameters (#Param.) takes into account each linear layer matrix and bias term.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-right: solid thin">PLM</th><th align="center" valign="middle" colspan="5">Best Hyperparameter Choice for Multilayer Perceptron</th></tr><tr><th align="center" valign="middle" style="border-right: solid thin">PLM Dim.</th><th align="center" valign="middle">Layers</th><th align="center" valign="middle">#Param.</th><th align="center" valign="middle">Learning Rate</th><th align="center" valign="middle">Dropout Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L6</td><td align="center" valign="middle">2560</td><td align="center" valign="middle">[128, 32]</td><td align="center" valign="middle">331,936</td><td align="center" valign="middle">1 × 10<sup>–4</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L33</td><td align="center" valign="middle">2560</td><td align="center" valign="middle">[512, 128]</td><td align="center" valign="middle">1,376,896</td><td align="center" valign="middle">1 × 10<sup>–4</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-35M-L12</td><td align="center" valign="middle">960</td><td align="center" valign="middle">[512, 128]</td><td align="center" valign="middle">557,696</td><td align="center" valign="middle">1 × 10<sup>–4</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L30</td><td align="center" valign="middle">1280</td><td align="center" valign="middle">[512, 128]</td><td align="center" valign="middle">721,536</td><td align="center" valign="middle">1 × 10<sup>–3</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">AbLang-L12</td><td align="center" valign="middle">1536</td><td align="center" valign="middle">[512, 128]</td><td align="center" valign="middle">852,608</td><td align="center" valign="middle">1 × 10<sup>–4</sup></td><td align="center" valign="middle">0.05</td></tr></tbody></table></table-wrap><table-wrap id="T7" orientation="portrait" position="float"><label>Table 7</label><caption><title>Optimal hyperparameter choices for TCR binding classifiers (Multilayer perceptron) taking embeddings from each PLM tested.</title><p>The layers correspond to the output dimension of each hidden layer and the number of parameters (#Param.) takes into account each linear layer matrix and bias term.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2" style="border-right: solid thin">PLM</th><th align="center" valign="middle" colspan="5">Best Hyperparameter Choice for Multilayer Perceptron</th></tr><tr><th align="center" valign="middle" style="border-right: solid thin">PLM Dim.</th><th align="center" valign="middle">Layers</th><th align="center" valign="middle">#Param.</th><th align="center" valign="middle">Learning Rate</th><th align="center" valign="middle">Dropout Rate</th></tr></thead><tbody><tr><td align="center" valign="middle" style="border-right: solid thin">TCR-Bert-L8</td><td align="center" valign="middle">768</td><td align="center" valign="middle">[192]</td><td align="center" valign="middle">147,648</td><td align="center" valign="middle">5 × 10<sup>–4</sup></td><td align="center" valign="middle">0.05</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">TCR-Bert-L12</td><td align="center" valign="middle">768</td><td align="center" valign="middle">[192]</td><td align="center" valign="middle">147,648</td><td align="center" valign="middle">1 × 10<sup>–3</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-8M-L6</td><td align="center" valign="middle">320</td><td align="center" valign="middle">[512, 16]</td><td align="center" valign="middle">172,560</td><td align="center" valign="middle">1 × 10<sup>–3</sup></td><td align="center" valign="middle">0.05</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-35M-L12</td><td align="center" valign="middle">480</td><td align="center" valign="middle">[128]</td><td align="center" valign="middle">61,568</td><td align="center" valign="middle">1 × 10<sup>–3</sup></td><td align="center" valign="middle">0.0</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L5</td><td align="center" valign="middle">640</td><td align="center" valign="middle">[192]</td><td align="center" valign="middle">123,072</td><td align="center" valign="middle">5 × 10<sup>–4</sup></td><td align="center" valign="middle">0.05</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-150M-L30</td><td align="center" valign="middle">640</td><td align="center" valign="middle">[192]</td><td align="center" valign="middle">123,072</td><td align="center" valign="middle">5 × 10<sup>–4</sup></td><td align="center" valign="middle">0.05</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L6</td><td align="center" valign="middle">1280</td><td align="center" valign="middle">[192]</td><td align="center" valign="middle">245,952</td><td align="center" valign="middle">5 × 10<sup>–4</sup></td><td align="center" valign="middle">0.05</td></tr><tr><td align="center" valign="middle" style="border-right: solid thin">ESM2-650M-L33</td><td align="center" valign="middle">1280</td><td align="center" valign="middle">[96, 24]</td><td align="center" valign="middle">125,304</td><td align="center" valign="middle">5 × 10<sup>–4</sup></td><td align="center" valign="middle">0.0</td></tr></tbody></table></table-wrap></floats-group></article>