<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190954</article-id><article-id pub-id-type="doi">10.1101/2023.11.12.566727</article-id><article-id pub-id-type="archive">PPR758304</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dopamine Neurons Encode a Multidimensional Probabilistic Map of Future Reward</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sousa</surname><given-names>Margarida</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Bujalski</surname><given-names>Pawel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Cruz</surname><given-names>Bruno F.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Louie</surname><given-names>Kenway</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>McNamee</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Paton</surname><given-names>Joseph J.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Champalimaud Neuroscience Programme, Champalimaud Foundation, Lisbon, PT</aff><aff id="A2"><label>2</label>Center for Neural Science, New York University, New York, NY, USA</aff><aff id="A3"><label>3</label>Present address: Allen Institute for Neural Dynamics, Seattle, WA, USA</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding authors: <email>joe.paton@neuro.fchampalimaud.org</email>,<email>margarida.sousa@research.fchampalimaud.org</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>14</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>13</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Learning to predict rewards is a fundamental driver of adaptive behavior. Midbrain dopamine neurons (DANs) play a key role in such learning by signaling reward prediction errors (RPEs) that teach recipient circuits about expected rewards given current circumstances and actions. However, the algorithm that DANs are thought to provide a substrate for, temporal difference (TD) reinforcement learning (RL), learns the mean of temporally discounted expected future rewards, discarding useful information concerning experienced distributions of reward amounts and delays. Here we present time-magnitude RL (TMRL), a multidimensional variant of distributional reinforcement learning that learns the joint distribution of future rewards over time and magnitude using an efficient code that adapts to environmental statistics. In addition, we discovered signatures of TMRL-like computations in the activity of optogenetically identified DANs in mice during a classical conditioning task. Specifically, we found significant diversity in both temporal discounting and tuning for the magnitude of rewards across DANs, features that allow the computation of a two dimensional, probabilistic map of future rewards from just 450ms of neural activity recorded from a population of DANs in response to a reward-predictive cue. In addition, reward time predictions derived from this population code correlated with the timing of anticipatory behavior, suggesting the information is used to guide decisions regarding when to act. Finally, by simulating behavior in a foraging environment, we highlight benefits of access to a joint probability distribution of reward over time and magnitude in the face of dynamic reward landscapes and internal physiological need states. These findings demonstrate surprisingly rich probabilistic reward information that is learned and communicated to DANs, and suggest a simple, local-in-time extension of TD learning algorithms that explains how such information may be acquired and computed.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The field of reinforcement learning (RL) provides a normative theoretical framework for adaptive animal behavior<sup><xref ref-type="bibr" rid="R2">2</xref></sup>. A core tenet of RL is that behaviors producing maximal expected future reward are the target of learning through interaction with the environment. Relatedly, associative learning, including learning to associate states and actions with future reward as is required by RL, can be viewed through the lens of statistical inference<sup><xref ref-type="bibr" rid="R3">3</xref></sup>. Whether a given observation or action should be associated with future reward depends on the degree to which its taking place reduces uncertainty about when rewards will occur. Such an account for associative learning would seem to require the brain to operate on distributions of events in time<sup><xref ref-type="bibr" rid="R4">4</xref></sup>. Furthermore, predicting when, and not just whether, behaviorally relevant events such as rewards will occur is often critical for survival. For example, crossing a desert to reach an oasis is only advisable if you won't perish before you get there. However, the RL algorithms that have driven startling progress in the neuroscience of learned behavioral control and in artificial intelligence alike, do not generally learn value representations that encode distributions of rewards over time.</p><p id="P3">Midbrain dopamine neurons have figured prominently in theories of how RL-like functions may be performed within neural circuits. Specifically, the phasic activity of midbrain DANs is thought to encode temporal difference (TD) reward prediction errors (RPEs), which serve as teaching signals that are used to update the value of states or actions so as to inform appropriate programs, or policies, for behavioral control<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R5">5</xref></sup>. However, standard TD formulations learn value functions that encode expectations of, temporally discounted, future reward: delayed rewards are weighted less relative to immediate ones. This produces ambiguity in value representations regarding when future rewards are expected to arrive. To illustrate this ambiguity one may observe the same value corresponding to either a large magnitude but delayed reward, or a smaller magnitude but imminent reward (<xref ref-type="fig" rid="F1">Figure 1A</xref>). In addition, because simple TD learning algorithms learn the average of temporally discounted future reward, they do not learn about the distribution of reward magnitudes. Recently, TD algorithms have been elaborated to include a set of value learning channels that differ in their sensitivity to positive and negative RPEs, leading to value estimates that converge to distinct statistics of the expected cumulative reward distribution, an innovation termed distributional RL<sup><xref ref-type="bibr" rid="R6">6</xref></sup> (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Such innovations have been shown to improve performance of deep RL agents on benchmark tasks due to improved statistical robustness, and evidence of distributional RL-like computations has been reported in midbrain DANs of both mice and primates<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>, however the direct functional relevance of such distributional mechanisms and representations to behavior is unknown. In the engineering setting, deep RL agents vary in whether and how they make use of knowledge about the distribution over future rewards when selecting actions<sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R12">12</xref></sup>, and decoding of reward distributions from DAN activity has only been demonstrated at the time of reward delivery <sup><xref ref-type="bibr" rid="R7">7</xref></sup>. In principle, knowing in advance, at the start of an episode, about the range and likelihood of rewards available and when they are likely to occur could be highly useful for planning and flexible behavior, particularly in the face of non-stationary in either the environment or internal state (e.g. hunger) of the animal <sup><xref ref-type="bibr" rid="R13">13</xref></sup>.</p><p id="P4">Here, we develop a computational model of efficient multidimensional distributional RL that learns to predict distributions of rewards over both time and magnitude or distributional time-magnitude RL (TMRL). We then test predictions of the model <italic>in vivo</italic> by recording from optogenetically identified midbrain DANs in mice during behavior. Specifically, in addition to previously demonstrated variability in the degree to which individual neurons respond to positive and negative RPEs <sup><xref ref-type="bibr" rid="R7">7</xref></sup> the model predicts variability in the degree to which individual neurons discount future rewards. We discovered evidence of both types of heterogeneity in DANs. This enabled decoding of future reward times that correlated with the variability in the temporal evolution of behavior, suggesting that decoded estimates correspond with animals’ temporal expectations. Furthermore, we show that taking into account the variability in both sensitivity to reward magnitude and delay allows decoding of a two-dimensional distribution, or map, of future reward amount over time from a set of DAN’ responses to a predictive cue, at the start of an episode. Strikingly, the tuning of individual dopamine neurons for reward time and magnitude was dynamic, adapting to changes in reward statistics, in accordance with the principles of an efficient code that is optimized for encoding information about these two important dimensions of reward. We propose that these data reflect a mechanism by which the brain may use a local-in-time algorithm akin to TD learning to build information-maximising, predictive, and probabilistic models of the environment for use in behavioral control.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Learning and encoding a two-dimensional probabilistic map of future reward</title><p id="P5">We begin by defining an adaptive distributional code for reward time and magnitude (TMRL). This code takes inspiration from several threads of research on temporal discounting<sup><xref ref-type="bibr" rid="R14">14</xref></sup>, temporal coding in general<sup><xref ref-type="bibr" rid="R15">15</xref></sup>, distributional value codes, and more recent work that extends distributional value codes to the time domain<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R17">17</xref></sup>.</p><p id="P6">Classical TD learning produces a global value function that encodes the average of expected future rewards. Temporal discounting of future rewards arises at the computation of the TD RPE <italic>δ</italic>(<italic>t</italic>) - the temporal difference between the value at the current timestep <italic>V</italic>(<italic>t</italic>) and the discounted value, parameterized by a discount factor <italic>γ</italic>, at the subsequent step <italic>V</italic>(<italic>t</italic>+1), plus any incoming reward <italic>r</italic>(<italic>t</italic>), <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi mathvariant="bold">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P7">Though expected reward delays are indeed reflected in the value function because of temporal discounting, delay and magnitude information are compressed into a single scalar value, producing ambiguity between these two dimensions in the value code (<xref ref-type="fig" rid="F1">Figure 1A</xref> bottom). Instead of learning a single value function, TD algorithms have recently been elaborated to learn a set of value functions <italic>V<sub>i</sub></italic>(<italic>t</italic>) that systematically differ in their sensitivity to positive and negative RPEs (<italic>α</italic><sup>+</sup> and <italic>α</italic><sup>−</sup><xref ref-type="fig" rid="F1">Figure 1B</xref>), <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P8">This causes each value function to converge to a different statistic of the observed distribution of reward magnitudes. Viewed collectively, this set of value functions encodes not just the average magnitude of expected future reward, but the distribution over their magnitudes (distributional TD learning in magnitude, <xref ref-type="fig" rid="F1">Figure 1B</xref>). A central insight of our model is that this approach may be generalized to both learn and encode information about the distribution of reward times by assuming that a set of value functions <italic>V<sub>j</sub></italic>(<italic>t</italic>) are learned that differ in their sensitivity to reward delays, parameterized as the standard temporal discount factor <italic>γ<sub>j</sub></italic> within the computation of a TD RPE (distributional TD learning in time, <xref ref-type="fig" rid="F1">Figure 1C</xref>), <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P9">Multiple timescales for discounting across a set of parallel learning channels alone resolves ambiguity between reward delays and the average reward magnitude that is present in a system with a single temporal discount factor (<xref ref-type="fig" rid="F1">Figure 1C</xref>). However, when distributional learning in time is combined with distributional learning of reward magnitude, <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">V</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> the resulting system learns to encode a probabilistic map of future rewards over both dimensions (<xref ref-type="fig" rid="F1">Figure 1D</xref>). In such a two-dimensional distributional reward coding system, correcting for tuning diversity across one dimension should reveal the remaining tuning diversity for the other that might otherwise be obscured (<xref ref-type="fig" rid="F1">Figure 1D</xref> bottom left).</p><p id="P10">Critically, because it specifies how temporal and magnitude parameters are learned, the TMRL model adapts to the rewards it experiences. The brain contains a finite number of neurons, and thus faces constraints in its information encoding capacity<sup><xref ref-type="bibr" rid="R18">18</xref></sup>. In the face of such constraints, efficient coding theory prescribes that the tuning properties of neurons adapt to the statistics of the variable they aim to represent in a manner that maximises overall information content of encoded signals<sup><xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. In the current context, this predicts that the discount factors and value parameters should be adapted so as to maximise the encoding of reward time-magnitude information with respect to the current environment<sup><xref ref-type="bibr" rid="R21">21</xref></sup>. We return to this aspect of the TMRL model in more detail below.</p></sec><sec id="S4"><title>Temporal discount rates vary among dopamine neurons and carry information about the distribution of future reward times</title><p id="P11">Does the brain use an algorithm similar to TMRL to learn about distributions of rewards along multiple dimensions? We tested for this by recording from midbrain DANs of mice (<xref ref-type="fig" rid="F7">Ext. Data Fig. 1</xref> and <xref ref-type="fig" rid="F8">2</xref>) during a simple behavioral task, trace odor conditioning, designed to induce predictions of reward at different delays and magnitudes. Four odor cues (conditioned stimuli, CSs) predicted the same reward amount but with a distinct delay (0,1.5, 3 or 6 seconds<sup>a</sup>, respectively, <xref ref-type="fig" rid="F2">Figure 2A</xref>). A fifth CS predicted, at a delay of 3s, a reward amount sampled from a bimodal probability distribution (<xref ref-type="fig" rid="F2">Figure 2B</xref>). Importantly the mean of the probability distribution of rewards associated with this fifth CS was equal to the fixed reward amount delivered following presentation of the other CSs. We focus our analyses on data from 43 optogenetically identified DANs collected from 6 trained mice. By considering the response of each dopamine neuron to CSs with different delays to reward, we estimated how single neurons discounted rewards over time, parametrizing this function with a temporal discount factor (<italic>γ</italic>), and a gain parameter (<xref ref-type="fig" rid="F2">Figure 2C</xref>). In addition, by examining the responses to different reward amounts, we estimated the value expected by each neuron (reversal point) and the slopes for negative (<italic>α</italic><sup>−</sup>, represented in blue) and positive (<italic>α</italic><sup>+</sup>, represented in red) RPEs<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, <xref ref-type="fig" rid="F2">Figure 2D</xref>).</p><p id="P12">We tested for the diversity in temporal discounts in optogenetically identified dopamine neurons and in putative dopamine neurons (see <xref ref-type="fig" rid="F9">Ext. Data Fig. 3</xref> and <xref ref-type="fig" rid="F10">4</xref>). We identified significant diversity in temporal discounts (<xref ref-type="fig" rid="F3">Figure 3A-C</xref>). This diversity did not reflect noise in the estimation of <italic>γ</italic>, as estimates were highly correlated across random partitioning of trials (<xref ref-type="fig" rid="F3">Figure 3B</xref>). Furthermore, neurons exhibited significantly different temporal discounts (<xref ref-type="fig" rid="F3">Figure 3C</xref>). We note that five cells possessed estimated discount factors that were greater than, and with 99% confidence intervals that were non-inclusive of, one. This may reflect limitations due to a limited number of reward delays, since three of these neurons were recorded for the mice only subject to three (instead of four) reward delays; by considering an underestimate of the responses to the missing delay, we obtain temporal discount factors that do not significantly differ from one (see <xref ref-type="fig" rid="F11">Ext. Data Fig. 5</xref>). For completeness we include these neurons in all analyses.</p><p id="P13">A key prediction of our distributional code for reward timing is that the probability distribution over future reward time can be decoded from the population responses to a CS (<xref ref-type="fig" rid="F1">Figure 1C</xref>). We focus on the population responses at the CSs that predict a certain reward at fixed delays and assume the system has knowledge of the temporal discount rate of each neuron. Since the population response at the CS reveal the encoded values, assumed to reflect the sum of temporally discounted rewards over time, determining the distribution over future reward time presents as a linear regression problem (<xref ref-type="fig" rid="F3">Figure 3D</xref> top). The independent variable is a matrix of temporal discounts to the power of the discretized time, the dependent variable is the mean responses at the CS and the regression coefficients are the probabilities of rewards over time (<xref ref-type="fig" rid="F3">Figure 3D</xref> top). The resultant densities capture the differences in the timing of rewards for the four CSs (<xref ref-type="fig" rid="F3">Figure 3D</xref> bottom).</p><p id="P14">We next asked if the decoded estimates correspond with animals’ temporal expectations, by comparing trial-by-trial variability in anticipatory licking behavior with the future reward time predicted by the population of DANs. We observed that in trials wherein animals commenced licking earlier or later, the decoded distribution over future rewards exhibited a qualitatively similar shift in time (<xref ref-type="fig" rid="F3">Figure 3E</xref>). These data suggest that estimates of future reward times decoded from a dopamine neuron population reflected temporal expectations that animals used to guide behavior.</p></sec><sec id="S5"><title>Dopamine neuron cue responses reflect distributional value information encoded in responses to reward</title><p id="P15">Next, we sought to combine the distributional time code with the previously proposed distributional code in amount<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, focusing first on the CS predicting a variable reward amount. The distributional RL theory in amount predicts that neurons with asymmetric linear functions for positive and negative RPEs possess reversal points (the reward amount for each neuron that produces zero net change in activity) that correspond to the expectiles of the probability distribution of rewards<sup><xref ref-type="bibr" rid="R7">7</xref></sup>. Previous work linking distributional codes for reward to the activity of midbrain dopamine neurons has largely focused on reward responses, and not the responses to cues that predict rewards. However, in principle, distributional information should be propagated backward in time from reward and thus decodable from the responses to reward predictive cues (<xref ref-type="fig" rid="F1">Figure 1D</xref>). One reason this may not have been observed in previously reported data is that the diversity in temporal discounting across neurons that we describe in the previous section can occlude distributional reward magnitude information (<xref ref-type="fig" rid="F1">Figure 1A,D</xref>). Because we measured diversity in temporal discounting across neurons, we were able to correct for it. Indeed, we found that correcting for the diversity in temporal discounts and gains revealed residual CS responses that are significantly correlated with the reversal points estimated at the time of rewards (<xref ref-type="fig" rid="F4">Figure 4A</xref>), indicating stable but mixed selectivity for reward delay and magnitude in single neurons. We then computed probability distributions over reward magnitude from dopamine responses at both the time of CS and the time of reward delivery. The distributions from reward and cue responses were nearly identical (<xref ref-type="fig" rid="F4">Figure 4B</xref>), indicating that the consequences of systematic variability in sensitivity to positive and negative of errors at reward delivery, previously identified as evidence that the dopaminergic system implements a distributional code for value, are transmitted to the cue response. This establishes the presence, at the time of cue presentation, of information required for the system to foresee impending variability in the magnitude of rewards ahead of time, when it might be used to guide future behavior.</p><p id="P16">We then tested if the joint distribution over future reward amounts and times can be decoded, a key prediction of the distributional TMRL code, from the responses of the DAN population at the CS. Conditioned on the CS, the joint probability distribution of reward on each trial can be factorized as the product of the marginal distributions of reward over time, giving a 2D map of future reward magnitude over time (<xref ref-type="fig" rid="F4">Figure 4C</xref>) that closely matches the true distributions. Thus, a multi-dimensional probabilistic map of future rewards may be estimated from just 450ms of dopaminergic neural activity at the onset of an episode.</p></sec><sec id="S6"><title>Value and temporal sensitivity efficiently adapt to environment statistics</title><p id="P17">We have identified diverse temporal discounting and sensitivity to value in midbrain DANs that may be used to estimate the joint probability of future reward time and magnitude. However, does parameter diversity across neurons that enables such a code reflect variability that we as experimenters exploit? Or are the parameters collectively regulated to maximize information about rewards along the hypothesized target dimensions? Evidence of such regulation would not only indicate efficient representational codes for reward within the dopamine system, but by extension, also provide strong evidence that the diversity in parameter tuning is present for the purpose of representing distributional reward information.</p><p id="P18">We used an efficient population coding framework<sup><xref ref-type="bibr" rid="R22">22</xref>–<xref ref-type="bibr" rid="R24">24</xref></sup> to derive temporal discount functions that optimally represent the reward times <italic>t<sub>r</sub></italic> in the environment. Inspired by previous work, we propose that this distribution is efficiently encoded in an expectile code<sup><xref ref-type="bibr" rid="R7">7</xref></sup>. In particular, we sought to maximize the <italic>mutual information</italic> between the true expectile reward times predicted at the cue <inline-formula><mml:math id="M5"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and those encoded by the dopamine neural population <inline-formula><mml:math id="M6"><mml:mrow><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, constrained by the number of neurons <italic>N</italic> and by the population expected firing rate <italic>R</italic>. We assume the cue response of each dopamine neuron decays exponentially as a function of reward delay with a time scale <italic>τ</italic> and a gain parameter <italic>a</italic>. Parameterising the population with the density of tuning curves <italic>d</italic> and a gain <italic>g</italic>, the solution is that neuron’s timescales <italic>τ</italic> should distribute according to the probability distribution of the current environment expectile reward times <inline-formula><mml:math id="M7"><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>∝</mml:mo><mml:mi>N</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (see <xref ref-type="sec" rid="S9">Methods</xref> and <xref ref-type="fig" rid="F16">Ext. Data Fig. 10</xref>). Intuitively, if rewards at short delays occur infrequently in a given environment, the system should not waste coding capacity to encode expected future rewards at short timescales, and vice versa if the environment only rarely emits reliable rewards at long delays. Furthermore, for each neuron the gain should be inversely proportional to the probability that a randomly chosen reward time will be smaller than it’s time scale <italic>P</italic>(<italic>τ</italic>), <italic>g</italic>(<italic>t<sub>r</sub></italic>) = <italic>R</italic>/(<italic>NP</italic>(<italic>τ</italic>)) (see <xref ref-type="sec" rid="S9">Methods</xref>). For low reward times, the entire population is active, incurring a large metabolic cost for encoding these values. Intuitively, this metabolic penalty can be reduced by lowering the gains tuned to late reward time scales, while maintaining optimized coding.</p><p id="P19">In order to optimize this efficient population code online, we generalize the distributional learning rules (<xref ref-type="fig" rid="F5">Figure 5A</xref><sup>6</sup>) to the time domain, considering multiple channels with different relative scaling for over and underestimation of reward times, that generate a diversity of learnt reward time scales (<xref ref-type="fig" rid="F5">Figure 5E</xref>), <disp-formula id="FD5"><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P20">Importantly, these parameters converge to the efficient code that optimally adapts to the statistics of expected reward times in the environment (see <xref ref-type="sec" rid="S9">Methods</xref>).</p><p id="P21">A critical but untested prediction of the distributional code for value is that the value each neuron expects (as defined by its reversal point) should adapt to changes in the probability distribution of reward magnitudes (<xref ref-type="fig" rid="F5">Figure 5A,B</xref>). In addition, the ordering of reversal points in the population should be preserved for different probability distributions. We found that the reversal point ordering is preserved across the two CSs (<xref ref-type="fig" rid="F5">Figure 5C</xref>) and that the variance of the reversal points for the variable CS is significantly greater than for the certain CS (<xref ref-type="fig" rid="F5">Figure 5D</xref>).</p><p id="P22">The mapping from time scales to temporal discount factors is an exponential mapping that takes into account the fact that steep temporal discounting (ie. small temporal discount factors) gives rise to very different learned value estimates for rewards that will occur at distinct short delays, but discriminates poorly between rewards occuring at later delays. Conversely, shallow temporal discounting (ie. large temporal discount factors) discriminate between rewards occurring far apart in time, and can thus support encoding of rewards at long time delays. Therefore, the discount factor is a monotonically increasing, exponentially decelerating function of the time scales at which rewards are observed <inline-formula><mml:math id="M9"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="F5">Figure 5E</xref> inset).</p><p id="P23">To test if the dopamine code adapts to the temporal statistics of reward, at the end of each session we modified the temporal reward distributions by removing the CS corresponding to either the shortest or longest reward delay. As predicted by the theory (<xref ref-type="fig" rid="F5">Figure 5F</xref>), when removing the longest delay, DANs adapted to improve encoding accuracy on short reward times by decreasing their discount factor (<xref ref-type="fig" rid="F5">Figure 5G</xref>). While adaptation when removing the shortest delay was not statistically significant in this dataset (<xref ref-type="fig" rid="F5">Figure 5G</xref>), the magnitude of the theoretically predicted adaptation is smaller than that predicted when removing the longest delay, and the data exhibited a trend in the correct direction, and thus a larger data set may reveal a shift in this condition as well. Alternatively, asymmetry in neural adaptation may be due to a rational bias in the neural code towards ensuring that predictions for the very near future are accurately encoded regardless of the temporal reward distribution in the environment, given that passage through intervening delays en route to later rewards is unavoidable. We also tested whether discounting by DANs was updated more continuously by comparing the trial-to-trial adaptation in temporal discount when the rate of reward occurrence is relatively high or low. Indeed, the temporal discounts for low rates were larger than for high rates (<xref ref-type="fig" rid="F15">Ext. Data Fig. 9</xref>). Regarding the gain, we indeed observe as predicted that individual neuron gains are negatively correlated to temporal discount factors (<xref ref-type="fig" rid="F5">Figure 5H</xref>).</p><p id="P24">The lawful, dynamic regulation of temporal discounting in individual DANs that we describe here indicates that principles of efficient coding likely apply to how the brain regulates the time constants over which rewards are predicted. However such lawful adaptation also strengthens our confidence that the dimension over which we as experimenters are decoding expected rewards - time - is indeed an encoding target for the system.</p></sec><sec id="S7"><title>How might information about future reward distribution be used to guide behavior?</title><p id="P25">The TMRL algorithm is relatively simple and “model-free”, meaning that the algorithm itself does not have access to knowledge of how the world transitions between states when learning to assign value to them. However, we reasoned that the very nature of the information it learns can naturally allow for complex computations such as planning. To examine the potential behavioral benefits of the joint distributional reward representation produced by TMRL in relation to pre-existing algorithms, we performed a series of simulations in which model agents forage for rewards (<xref ref-type="fig" rid="F6">Figure 6</xref>). For comparison, we model agents making use of either TMRL, standard TDRL, or the successor representation (SR). Similar to TMRL and TDRL, SR is a predictive model learned using a temporal-difference algorithm. However, SR instead caches expectations of future state visits, which can facilitate heuristic planning strategies<sup><xref ref-type="bibr" rid="R25">25</xref></sup>. Interestingly, the basic mechanism underlying the SR - TD learning of expected future state occupancy - has recently been extended to include a set of SRs that vary in their temporal horizon, a conceptually similar innovation to that reflected in TMRL, except that it focuses on creating multi-scale temporal predictions of future states instead of future rewards<sup><xref ref-type="bibr" rid="R26">26</xref></sup>.</p><p id="P26">Our simulations took the following form. In a simulated stationary environment where a mouse can choose between patches that deliver different but stable amounts of reward (eg. acorns), the TDRL, SR and TMRL agents perform similarly (<xref ref-type="fig" rid="F6">Figure 6A,C</xref>). However, in a dynamic environment where different patches have different amounts of available reward at different periods of time during the day, mice can collect significantly more reward when given access to knowledge of the timing of reward availability (in addition to the average amount of reward each patch contains). In naturalistic settings, dynamics of effective reward availability may arise due to resource production itself, foraging strategies of competitors, or dynamic predation<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. By forward planning over the map generated by the distributional TMRL algorithm, we find that mice can adopt a strategy where they harvest from a lower value, but more immediate, reward patch (patch one and two) before moving on to a higher value, but delayed, patch (patch three) that would be persistently selected by a single temporal discount factor model that only learns about the mean reward of each patch (<xref ref-type="fig" rid="F6">Figure 6B,C</xref>). The TMRL also outperforms the SR agent, since the standard SR algorithm also estimates a scalar value. These simulations provide a simple demonstration of the potential benefit of using information about the distribution of rewards in time to guide action selection over existing models that do not possess such information, specifically in the context of an environment with temporally delimited reward availability.</p><p id="P27">The previous example focused on a case where the external world possesses reward dynamics, but biological agents are also subject to internal state dynamics that incentivise using knowledge about reward distributions. For example, in response to changes in wealth or physiological need state, humans and other animals shift their preferences with respect to the delay and variability in the amount of reward, which can be expressed as a utility function<sup><xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R32">32</xref></sup>. The distributional TMRL code generates a representation that may be used to flexibly adapt policies to these preferences, as has been shown previously for model based implementations<sup><xref ref-type="bibr" rid="R33">33</xref></sup>. For example, if the mouse is hungry and urgently needs a significant amount of reward, they might reweight different regions of the TMRL reward map, akin to creating a dynamic utility function that heavily weights large, imminent rewards (<xref ref-type="fig" rid="F6">Figure 6D</xref>). This might lead the mouse to employ a policy where they select patch two, predicted to grant the largest reward at a short delay. When sated, the mouse may want to devise a different utility function that does not vary as a function of predicted reward delay, initially selecting patch three, which gives the largest reward at the longest delay (<xref ref-type="fig" rid="F6">Figure 6D</xref>). The distributional TMRL algorithm and the SR allow for a faster adaptation of policies from the hungry to the sated state than the TDRL algorithm, which needs to learn through experience to adapt the policy (<xref ref-type="fig" rid="F6">Figure 6E</xref>), and thus may present interesting potential computational mechanisms for how internal states modulate decision-making.</p><p id="P28">While not intended as exhaustive, the two scenarios we simulate here in a foraging environment - temporally delimited and variable magnitude rewards, and internal state dependent modulation of utility - are chosen to illustrate the broad potential importance of the information provided by distributional TMRL for animal behavior. They reflect a small, but illustrative, sample of the rich opportunities for future work to examine how the reward representations generated by a multidimensional distributional algorithm like TMRL can benefit adaptive behavior across a range of scenarios.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P29">A fundamental facet of intelligence is the ability to use past experience to predict the future<sup><xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Predictions may incorporate detailed information about how the environment will develop depending on a course of action, constituting models of the world that can be operated on flexibly but laboriously. Alternatively, predictions may take the form of efficient, compressed representations that discard detailed features of environmental structure and are specific to particular, behaviorally relevant events, such as rewards. Within RL, model-based learning algorithms that involve the former, detailed predictions enable more flexible behavior at the cost of computational complexity, while model-free algorithms that target the latter, simpler predictions allow for efficiency at the expense of flexibility. The field of RL provides a growing set of tools for learning simpler and more complex predictions alike in service of adaptive behavior, and there is ample evidence that the brain employs strategies resembling both algorithmic classes depending on, for example, whether behavior is under more explicit, goal-directed or automatic, habitual control<sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R36">36</xref></sup>.</p><p id="P30">Though often described in categorical terms, recent work has highlighted how predictive representations of intermediate complexity can enable more flexible behavior that tends to characterize model-based algorithms, while using computationally efficient model-free learning algorithms<sup><xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>. Here we present distributional TMRL, a theoretical extension of TD learning that learns an efficient, multi-dimensional, probabilistic map of future reward value over time. This probabilistic map constitutes a kind of ‘model’ of the world, and yet, the mechanics by which it is learned are less complex than those used to learn the full structure of possible state transitions in an environment. Furthermore, we show how distributional TMRL learns representations that may enable better, ie. more rewarding, policies for behavioral control when in an environment where exploiting knowledge about the time course of available rewards confers a benefit, or when changes in internal state might favor dynamic attitudes toward risk. Importantly, we present evidence that the brain may make use of TMRL-like computations. Midbrain DANs, a core component of neural systems involved in learning policies for behavioral control, displayed signatures of a multidimensional distributional code over reward time and value; furthermore, animals appear to use the reward timing information reflected in DAN population activity to guide temporal control of behavior. Lastly, we show that the distributional TMRL code adapts to changes in reward statistics in accordance with information theoretic principles of efficient coding. This provides not only strong evidence that the joint distribution of reward over time and value that we as experimenters decode from neural activity is indeed a functional target for encoding within the system, but potentially opens a fresh perspective on how to interpret individual variability in temporal discounting at a behavioral level. Steep temporal discounting that heavily favors immediate over delayed rewards, leading to apparently maladaptive, impulsive behavior, can be reframed as the optimal solution to a volatile environment. Conversely, shallow discounting, leading to consideration of delayed rewards, is ideal for stable environments that are predictable over long time scales. Thus, changes in environmental volatility may be compensated by adaptive changes in discounting, complementing evidence for volatility-induced changes in learning rates<sup><xref ref-type="bibr" rid="R38">38</xref>–<xref ref-type="bibr" rid="R40">40</xref></sup>. This suggests that therapeutic attempts to improve impulse control might fruitfully target the environment, by intervening to lengthen the timescale over which predictions are valid, or reorient individuals to the longer timescale structure of rewards that may already exist within an environment.</p><p id="P31">More than two decades ago, midbrain DANs were first hypothesized to emit a TD RPE to teach recipient circuits accurate reward expectations<sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R5">5</xref></sup>. This proposal drove the development of a new field of research into whether the brain may be using algorithms like those contained within computational RL models to learn programs for behavioral control, and if so, what form they take. That a compact algorithm like TD learning can estimate an overarching objective around which much of learned behavior can be shaped is not only practically useful in engineering settings, but explains a wide range of neural and behavioral data. However, in its simplest form, TD learning does not capture behaviorally relevant dimensions that characterize rewards because it compresses information regarding reward time, magnitude, and quality into one scalar value that represents, in a common currency, the expected average sum of temporally discounted future rewards. Sensory characteristics, and unambiguous knowledge about magnitude and timing of rewards is lost. Furthermore, experimental data has shown that DANs can exhibit sensitivity to the sensory characteristics of rewards <sup><xref ref-type="bibr" rid="R41">41</xref></sup>, actions or features of the environment<sup><xref ref-type="bibr" rid="R42">42</xref></sup>, and appear to access internal models regarding environmental structure<sup><xref ref-type="bibr" rid="R43">43</xref></sup>. It is thus becoming ever more clear that the basic model of DANs emitting a unitary, model free TD RPE requires refinement, at the very least, and some have argued for its replacement altogether<sup><xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R45">45</xref></sup>.</p><p id="P32">However, the space of possible TD learning algorithms is large and continually growing. Assumptions about the way the state of the environment is encoded, the space of possible actions in TD methods for control, and parameter determination or regulation are just a few of many ways in which TD methods can differ <sup><xref ref-type="bibr" rid="R46">46</xref></sup>. Given the parallel and modular nature of brain circuits, including those embedding DANs, one class of TDRL extensions that would seem particularly attractive for deriving hypotheses about neural systems posits parallel learning of multiple reward expectations that systematically differ, either quantitatively or qualitatively. For example, multi-system models, where separate model-free and model-based RL mechanisms both contribute to behavioral control have been mapped to distinct, parallel circuitry in the basal ganglia <sup><xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R48">48</xref></sup>, the main target of dopaminergic innervation, and multi-agent and mixture of experts models have been used to explain recent data regarding the functional role of neurons in different regions of the striatum, a major target of dopamine, and spatial heterogeneity in striatal dopamine signaling itself, respectively<sup><xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R50">50</xref></sup>. In addition, recently described heterogeneity in DAN encoding of task variables has been hypothesized to reflect a vector, as opposed to a scalar, prediction error, still within a TD framework <sup><xref ref-type="bibr" rid="R51">51</xref></sup>, and signatures of multiple separate, qualitatively distinct predictions have been observed in DANs<sup><xref ref-type="bibr" rid="R52">52</xref></sup>. Here we provide direct evidence that diverse sensitivity to a fundamental dimension along which rewards are distributed, time, can explain another source of variability in response properties across DANs, without abandoning a computational framework for understanding dopaminergic function for which the accumulated experimental support is large.</p><p id="P33">Temporal information is critical for learning systems. Reliable temporal structure in the world forms the basis for identifying predictive relationships that drive associative learning and contribute to causal inference. To extract such structure, the brain must somehow register when certain events occur in relation to each other, creating something akin to a temporal map<sup><xref ref-type="bibr" rid="R53">53</xref></sup>. We describe the existence of just such a temporal map, specific to reward, that is reflected in a core component of reward circuitry long known to be critical for behavioral control: midbrain DANs. It will be important to determine in future work whether and in what manner this information is distributed within other brain circuits, and how this information is brought to bear on the problems faced by organisms seeking to thrive within complex and dynamic natural environments.</p></sec><sec id="S9" sec-type="methods"><title>Methods</title><sec id="S10"><label>1</label><title>Mice</title><p id="P34">Young adult (2-7 months old at time of experiments), male DAT-Cre mice expressing Channelrhodopsin-2 (ChR2) in midbrain dopamine cells were used in this study. For this, Ai32(RCL-ChR2(H134R)/EYFP) mice (IMSR<sub><italic>JAX</italic></sub>: 012569) were crossed with DAT-IRES-Cre mice (IMSR<sub><italic>JAX</italic></sub>: 006660). Mice were group-housed (up to 3 mice per cage) until the first of two craniotomies were performed, after which they were single-housed. A temperature (21°C) and humidity-controlled (50%) housing room was maintained with a 12-hour light/dark cycle. Mice were maintained on PicoLab Rodent Diet 20 (5053), and under water deprivation for all behavioural experiments (&gt;85% body weight from baseline <italic>ad</italic> libitum period before deprivation). All experiments and procedures followed guidelines set and approved by the relevant national and international authorities (Champalimaud Foundation Animal Welfare Body (protocol number: 2017/013), Portuguese Veterinary General Board (Direcção-Geral de Veterinária, project approval 0421/000/000/2018) and European Union Directive 2010/63/EEC).</p></sec><sec id="S11"><label>2</label><title>Surgical Procedures</title><p id="P35">Each mouse received three surgeries, first one for headpost implantation and then two unilateral craniotomies (performed 1 week apart) above the targeted regions (VTA/SNc). All surgical procedures were carried out under anesthesia with isoflurane (3% for induction; 1–2% for surgery at 0.8 l min <sup>−1</sup>). Mice were then fixed in a stereotaxic frame and their eyes were protected with a small amount of ophthalmic ointment.</p><p id="P36">The headpost implantation surgery was performed following the procedure outlined in<sup><xref ref-type="bibr" rid="R61">61</xref></sup>. Briefly, the hair was shaved down to skin which was then disinfected and incised. After exposing the skull, the soft tissue and periosteum were carefully removed and cleared. Then bregma and lambda were identified, the skull was leveled, and the locations for the craniotomies were marked. Subsequently, the skull surface was prepared for headpost implantation following the procedure suggested in the protocol. The headpost was then placed in a desired location (anterior to bregma and slightly above the skull surface) and cemented in place with dental adhesive (C&amp;B Metabond, Parkell). Following the surgery, mice received carprofen subcutaneously (s.c.) for pain management. Mice were then placed back in their original cages and monitored for 7 days post-surgery to ensure well-being and a full recovery.</p><p id="P37">Prior to any electrophysical recordings (24-48 hours), a small craniotomy (1.5mm) was performed and sealed with a removable silicone sealant (Kwik-Sil, World Precision Instrument). Two separate craniotomies, one in the left and the other in the right parietal bones, were performed (coordinates: AP: -3.0 mm; ML: ±0.6 mm from the Bregma). Prior to surgery, all mice received carprofen (s.c.), enrofloxacin (s.c.), and dexamethasone (i.m.).</p></sec><sec id="S12"><label>3</label><title>Behaviour &amp; Training</title><sec id="S13"><label>3.1</label><title>Behavioural apparatus</title><p id="P38">The behavioral setup consisted of an infrared light source, an IR camera, a head-fixation system, a water delivery tube, a custom-built olfactometer<sup><xref ref-type="bibr" rid="R62">62</xref></sup> with an odor delivery tube, and an air ventilation system that was running during the whole experiment to prevent odor accumulation. The task logic was implemented in a real-time operating system using an Arduino microcontroller (Arduino Mega 2560, Arduino). The behavior of the animal was also monitored via an IR camera (FL3-U3-13S2, FLIR). The videos were acquired with Bonsai<sup><xref ref-type="bibr" rid="R63">63</xref></sup> at 120fps (640 x 480 pixels) for online licking detection and further offline processing. Briefly, a small area of the image was selected, on each session. For each frame, the sum of all pixels’ luminance was computed. The resulting trace was then manually thresholded and lick events were detected as frames with average luminance above the background.</p><p id="P39">The odor cues were delivered through the tube of a custom-built olfactometer placed approximately 1cm from the mouse snout, and the odor delivery was controlled via two-way micro-solenoid valves (model LHDA1233115H, Lee Company, CT, USA). Similarly, calibrated water reward was delivered through a lick spout using a two-way micro-solenoid valve.</p></sec><sec id="S14"><label>3.2</label><title>Odor stimuli</title><p id="P40">During each trial an odor cue was presented for 1 second approximately 1 cm from the snout. Odors were delivered via a computer-controlled olfactometer with a 1,000 ml/minute constant flow. Each odor was dissolved in mineral oil at 1:10 dilution and 15<italic>μ</italic>l of diluted odor solution was applied to the syringe filter (2.7<italic>μ</italic> m pore, 13mm; Whatman, 6823-1327). Odors were: Cuminaldehyde, (S)-(+)-2-Octanol, (R)-(-)-Carvone, Pentyl acetate and Hexanoic acid.</p></sec><sec id="S15"><label>3.3</label><title>Behavioural task</title><p id="P41">Mice were water restricted 7 days after head-posting and habituated to head-restrainment for 2-3 days (10-30 min sessions). Within these sessions, mice were allowed to voluntarily lick the spout for a water reward. Following the habituation sessions, mice were trained in an odor-cued classical conditioning task, where an odor cue predicts reward with distinct delay and/or magnitude (<italic>i</italic>.<italic>e</italic>. volume of water).</p><p id="P42">The task included five trial types, randomly intermixed. Trial types 1-4 began with a 1-s odor delivery, followed by a delay of 0, 1.5, 3, 6 seconds, respectively, and a fixed water reward amount of 4.5 microliters. For one of the animals, the delay of 0s wasn’t included in the task. Trial type 5 began with a 1s odor delivery, followed by a 3 seconds delay and a reward sampled from a probability distribution with five possible outcomes: 1, 2.75, 4.5, 6.25, 8 microliters with respective probabilities: 0.25, 0.167, 0.167, 0.167, 0.25, such that the mean of this distribution was 4.5 microliters and thus matching the average reward amount delivery across all trial-types. The odor identity associated with each trial type was shuffled for different individual mice. Importantly, at the end of the session, when 200 trials had passed, there was a context switch and either the cue predicting the shortest or the longest delay was removed. Trial duration was drawn from an exponential distribution (minimum 11s, mean 4.5s, truncated at 21s), resulting in an approximately flat hazard function and an approximately constant reward rate throughout the session.</p></sec></sec><sec id="S16"><label>4</label><title>Electrophysiology</title><sec id="S17"><label>4.1</label><title>Acute recordings</title><p id="P43">All electrophysiological experiments were conducted while mice were head-restrained. Recordings were performed for up to 6 days following a 24-48 hours recovery period from the craniotomy surgery. Between recording sessions, the craniotomies were covered in the same manner as described above. A two-shank 64-channel silicon probe (ASSY 77-H6, Cambridge NeuroTech) with a tapered optical fibre (Lambda-B fibre 100-μm core NA=0.48, Cambridge NeuroTech) glued to the back, was lowered into the ventral tegmental area (VTA) and substancia nigra pars compacta (SNc). Probe tracks were reconstructed from three different sessions by dipping the probes in DiD, DiO and DiI solutions before insertion. Electrophysiology and laser/LED modulation data were digitized at 30kHz with the Open Ephys acquisition board<sup><xref ref-type="bibr" rid="R64">64</xref></sup> and recorded with Bonsai<sup><xref ref-type="bibr" rid="R63">63</xref></sup>.</p></sec><sec id="S18"><label>4.2</label><title>Spike sorting and data processing</title><p id="P44">In order to remove light artifacts, independent component analysis (ICA) was performed<sup><xref ref-type="bibr" rid="R65">65</xref></sup>. In particular, we used chuncks of recorded signals from the beginning and end of each session where pulses of light were given, and used fastICA<sup><xref ref-type="bibr" rid="R66">66</xref></sup> to obtain the independent components and the mixing matrix. Light artifacts were present simultaneously in the majority of the channels, hence we considered as artifacts the two components with highest entropy of the mixing matrix weights, and after visual inspection, removed these components and reconstructed the signals. Data were sorted offline with Kilosort 2.5 spike sorting software (<ext-link ext-link-type="uri" xlink:href="https://github.com/MouseLand/Kilosort">https://github.com/MouseLand/Kilosort</ext-link>), and manually curated using Phy (<ext-link ext-link-type="uri" xlink:href="https://github.com/cortex-lab/phy">https://github.com/cortex-lab/phy</ext-link>).</p></sec><sec id="S19"><label>4.3</label><title>Light identification of dopamine neurons</title><p id="P45">The light evoked photoactivation of midbrain dopaminergic neurons with ChR2 was used to identify neurons as dopaminergic. The protocol consisted of trains of 10 blue (473 nm) light pulses, each 10 ms long, at 1, 5 and 20 Hz (5s inter-train interval) followed by 3 consecutive long pulses, lasting for 1s each. Laser power at the tip of the fiber was on average 20 mW (measured at the tip of the fiber, before each experiment). Optogenetic stimulation was delivered twice at the beginning and twice at the end of the recording session. Units were considered photo-identified using an intersection of different criteria: SALT test<sup><xref ref-type="bibr" rid="R67">67</xref></sup> p-value &lt;0.001, paired t-test comparing baseline and post-laser onset (1–10ms) firing rate yielding a p-value&lt;0.01, correlation coefficient between laser-triggered waveform, non-evoked waveform &gt;0.9 and probability of eliciting &gt;1 spikes within 1-10ms of each pulse &gt;0.1. To be included in the data set, a neuron had to be well-isolated (inter-spike-interval (ISI) violation &lt;0.04<sup><xref ref-type="bibr" rid="R68">68</xref></sup>).</p></sec></sec><sec id="S20"><label>5</label><title>Immunohistochemistry and microscopy</title><p id="P46">Histology was performed to verify placement of the recording electrodes and expression patterns of transgenes. The mice received an overdose of pentobarbital (Eutasil, 100 mgkg<sup>−1</sup> intraperitoneally) and, once deeply anesthetized, were perfused transcardially with 4% paraformaldehyde. The brains were removed from the skull, stored for 24h in 4% paraformaldehyde and then kept in PBS until sectioning. Coronal brain slices were obtained using vibratome sectioning (80 μm), and immunostained with antibodies against GFP: rabbit anti-GFP (A-6455, 1:1,000) and goat anti-rabbit AF488 (Invitrogen, A-11008, 1:1,000). The sections were incubated in DAPI and mounted In Mowiol. Imaging was carried out using a slide scanner (Axio Scan Z1, Zeiss). For electrophysiological recordings, shank placement was confirmed using DiD, DiO and DiI cell-labeling solutions (V22889, Thermo Fisher Scientific).</p></sec><sec id="S21"><label>6</label><title>Probe trajectories &amp; location of the recording sites</title><p id="P47">Probe trajectories were reconstructed from histology data by alligning the histological slices with the Allen common coordinate framework (CCF) atlas and manually tracing the dye track using code that is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/petersaj/AP_histology">https://github.com/petersaj/AP_histology</ext-link>.</p><p id="P48">CCF coordinates were transformed into stereotaxic coordinates using the method described in <ext-link ext-link-type="uri" xlink:href="https://community.brain-map.org/t/how-to-transform-ccf-x-y-z-coordinates-into-stereotactic-coordinates/1858">https://community.brain-map.org/t/how-to-transform-ccf-x-y-z-coordinates-into-stereotactic-coordinates/1858</ext-link>. The AP, ML and DV coordinates were estimated for the probe’s tip (visual guess) and 400 points that lie within 400 μm (step k = 1 μm) from the tip. In total, 54 cells (18 photo-identified and 36 putative) were recorded from the sessions with the dyed probe. The coordinates were estimated for the channels these cells were recorded from with the depth of each cell taken from the Kilosort results.</p></sec><sec id="S22"><label>7</label><title>Distributional code for reward time model</title><p id="P49">We describe our model in the context of a class of Markov decision processes (MDPs) which use a deterministic chain of states to model the temporal evolution of a reward function across time. This is known as the complete serial compound representation<sup><xref ref-type="bibr" rid="R69">69</xref></sup>. Such MDPs <inline-formula><mml:math id="M10"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="script">T</mml:mi><mml:mo>,</mml:mo><mml:mi>𝒫</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are composed of a set of time states <inline-formula><mml:math id="M11"><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></inline-formula>, a reward random variable <inline-formula><mml:math id="M12"><mml:mrow><mml:mi>r</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> indexed by time states and a deterministic transition function between time states <inline-formula><mml:math id="M13"><mml:mrow><mml:mi>𝒫</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The value at time step <italic>t</italic> is the expected sum of discounted future rewards over a time range from 0 to <italic>T</italic>, <disp-formula id="FD6"><label>(1)</label><mml:math id="M14"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M15"><mml:mrow><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>]</mml:mo></mml:mrow></mml:math></inline-formula> and <italic>γ</italic> ∈ [0, 1] is the temporal discount factor that determines how much the utility of delayed rewards is decreased relative to immediate reward.</p><sec id="S23"><label>7.1</label><title>Efficient population coding and the expectiles of reward times</title><p id="P50">In our experiment, reward time and amount is manipulated across conditions and thus our dopamine population model describes adaptive neural responses to both these features of the environment reward function. Initially, we describe a dopamine neuron’s tuning function with respect to reward time <italic>t<sub>r</sub></italic>. It is proposed that, across the dopamine neuron population, reward is heterogeneously discounted over time, and reflect this assumption in exponentially decaying neural tuning function parametrized by reward time scale <italic>τ</italic>, with a gain parameter <italic>a</italic> <disp-formula id="FD7"><label>(2)</label><mml:math id="M16"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mfrac><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>τ</italic> is the time at which the neural response is reduced to <inline-formula><mml:math id="M17"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>e</mml:mi></mml:mfrac></mml:mrow></mml:math></inline-formula> times its initial value. Considering this parametrization, the corresponding temporal discount factors (see <xref ref-type="disp-formula" rid="FD6">Eqn. 1</xref>) arise naturally as a function of the reward time scales, specifically <inline-formula><mml:math id="M18"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>τ</mml:mi></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. We consider a mapping <inline-formula><mml:math id="M19"><mml:mrow><mml:mi>ι</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>→</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> from the set of neuron’s reward time constants <inline-formula><mml:math id="M20"><mml:mrow><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></inline-formula>, to the space of reward times <italic>T</italic>.</p><p id="P51">In order to predict optimal variability across the reward time scales estimated from dopamine neuron responses, we develop an efficient population coding model<sup><xref ref-type="bibr" rid="R22">22</xref></sup> for the distribution <italic>p</italic>(<italic>t<sub>r</sub></italic>|<italic>s</italic>) := <italic>p</italic>(<italic>t</italic> = <italic>t<sub>r</sub></italic>|<italic>r</italic> ≠ 0, <italic>s</italic>) of reward times <italic>t<sub>r</sub></italic>, where <italic>s</italic> is the stimulus presentation at time 0. That is, the distribution of possible future reward times <italic>t<sub>r</sub></italic> after the stimulus has been observed. We propose that this distribution is efficiently encoded in an expectile code, as opposed to the quantile code. Indeed, previous experimental work has shown that the tuning functions of dopamine neurons of reward magnitudes are more consistent with bi-linear functions, that would be predicted for an expectile code, then with heaviside functions, that would be predicted for a quantile code<sup><xref ref-type="bibr" rid="R7">7</xref></sup>. Thus, for consistency with these previous results, we use an expectile code model based on expectations of reward times. Theoretically, we accomplish this by optimizing our dopamine population model to efficiently encode the distribution of expectations over reward times. In contrast, the quantile code may be computed in our formalism by efficiently encoding the distribution over reward times<sup><xref ref-type="bibr" rid="R70">70</xref></sup>. Importantly, though our theoretical formalism is general and integrative over both expectile and quantile codes, our use of the expectile code in particular does not qualitatively change our model predictions.</p><p id="P52">The expectiles of a distribution generalize the mean statistic analogously to how quantiles generalise the median<sup><xref ref-type="bibr" rid="R70">70</xref></sup>. Given reward times <italic>t<sub>r</sub></italic> with probability distribution <italic>p</italic>(<italic>t<sub>r</sub></italic>|<italic>s</italic>), the <italic>η</italic>-expectile <inline-formula><mml:math id="M21"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> satisfies<sup><xref ref-type="bibr" rid="R71">71</xref></sup> : <disp-formula id="FD8"><mml:math id="M22"><mml:mrow><mml:mi>η</mml:mi><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo>)</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P53">For example, the mean corresponds to the expectile with level <italic>η</italic> = 0.5. The expectiles are distributed according to the cumulative distribution, <disp-formula id="FD9"><mml:math id="M23"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula> An explicit representation of this cumulative distribution has also been derived previously<sup><xref ref-type="bibr" rid="R72">72</xref></sup>.</p><p id="P54">Given a population activity vector of <italic>N</italic> dopamine neurons <inline-formula><mml:math id="M24"><mml:mrow><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, we sought to maximize the mutual information <inline-formula><mml:math id="M25"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>r</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between the true expectile reward times <inline-formula><mml:math id="M26"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and those encoded by the population <inline-formula><mml:math id="M27"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>r</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, constraining on the number of neurons and on the population expected firing rate <italic>R</italic>. We consider a population with homogeneous derivatives, <disp-formula id="FD10"><label>(3)</label><mml:math id="M28"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>δ</mml:mi><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mstyle><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> that approximately tiles the range of possible reward times from 0 to <italic>T</italic>, have constant Fisher information and where each neuron is subject to independent Poisson noise. A population that linearly tiles the exponential decay half-lives approximately satisfies these conditions, however this construction has edge effects at 0 which does not affect the capability of this model to encode future (non-zero) reward times. Since computing the mutual information is analytically intractable, a lower bound is optimized instead, namely the Fisher information<sup><xref ref-type="bibr" rid="R73">73</xref></sup>. We parameterize the optimized dopamine tuning curves <inline-formula><mml:math id="M29"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with an invertible <italic>density</italic> function <inline-formula><mml:math id="M30"><mml:mrow><mml:mi>d</mml:mi><mml:mo>:</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>→</mml:mo><mml:mi mathvariant="script">T</mml:mi></mml:mrow></mml:math></inline-formula> which characterizes the heterogeneous allocation of neurons to reward time scales <inline-formula><mml:math id="M31"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">T</mml:mi><mml:mo>≡</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula> and a <italic>gain</italic> function <italic>g</italic> : <italic>T</italic> → <italic>T</italic> which characterizes the mean firing rate across reward times <inline-formula><mml:math id="M32"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mi>T</mml:mi><mml:mo>≡</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></inline-formula>, <disp-formula id="FD11"><mml:math id="M33"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Population mapping</mml:mtext><mml:mo>:</mml:mo><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>→</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> <disp-formula id="FD12"><label>(4)</label><mml:math id="M34"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mtext>Neuron mapping</mml:mtext><mml:mo>:</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>→</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>d</italic>(<italic>τ</italic>) is the density of tuning curves at reward time scale <italic>τ</italic> in the optimized population and <italic>g</italic>(<italic>t<sub>r</sub></italic>) is the population mean firing rate for reward time <italic>t<sub>r</sub></italic>. The Fisher information of the optimized population is given by<sup><xref ref-type="bibr" rid="R22">22</xref></sup>, <disp-formula id="FD13"><mml:math id="M35"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>f</mml:mi><mml:mi>d</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>g</mml:mi><mml:msup><mml:mi>d</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P55">After constraining on the number of neurons <italic>N</italic>, the solution for the density function <italic>d</italic> is given by, <disp-formula id="FD14"><label>(5)</label><mml:math id="M36"><mml:mrow><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>∝</mml:mo><mml:mi>N</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>ι</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> i.e., the population reward time scales should distribute proportionally to the expectile rewards times in the environment (see <xref ref-type="fig" rid="F16">Ext. Data Fig. 10</xref>). On the other hand, additionally constraining on the mean population firing rate <italic>R</italic>, considering the population defined in <xref ref-type="disp-formula" rid="FD10">Eqn. 3</xref>, the population gain function <italic>g</italic> should satisfy, <disp-formula id="FD15"><label>(6)</label><mml:math id="M37"><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>p</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo></mml:mrow></mml:mstyle><mml:mi>d</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P56">Integrating by parts to obtain the mean population firing rate as, <disp-formula id="FD16"><label>(7)</label><mml:math id="M38"><mml:mrow><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:msubsup><mml:mi>P</mml:mi></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>P</italic>(<italic>t<sub>r</sub></italic>) is the cumulative distribution of <italic>P</italic>(<italic>t<sub>r</sub></italic>). Therefore the solution for the population gain is given by, <disp-formula id="FD17"><label>(8)</label><mml:math id="M39"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>P</mml:mi><mml:mo>(</mml:mo><mml:mi>ι</mml:mi><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> i.e., for each neuron the gain should be inversely proportional to the probability that a randomly chosen reward time will be smaller than it’s time scale. For small reward times, the entire population is active, incurring a large metabolic cost for encoding these values. Intuitively, this metabolic penalty can be reduced by lowering the gains tuned to long reward times and therefore large cumulative probabilities <italic>P</italic>(<italic>ι<sub>t</sub></italic>(<italic>τ</italic>)) in <xref ref-type="disp-formula" rid="FD17">Eqn. 8</xref> (see <xref ref-type="fig" rid="F16">Ext. Data Fig 10</xref>).</p></sec><sec id="S24"><label>7.2</label><title>Distributional learning mechanism</title><p id="P57">In our analysis thus far, we have identified a particular density profile of neural tuning functions that optimally encode the distribution of reward times following a stimulus presentation. In this section, we describe how such a neural population may, in principle, be optimized via an online learning process.</p><p id="P58">We pursue an algorithmic strategy inspired by the distributional learning of reward magnitudes as proposed previously<sup><xref ref-type="bibr" rid="R7">7</xref></sup> In this approach, multiple channels with different relative scaling for over-or under-estimation of reward magnitude (<italic>α</italic><sup>+</sup>, <italic>α</italic><sup>−</sup>) leads to a diversity of learned values, <disp-formula id="FD18"><mml:math id="M40"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>δ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula> which collectively encode the entire reward magnitude distribution.</p><p id="P59">With respect to the distribution of reward times <italic>p</italic>(<italic>t<sub>r</sub></italic>|<italic>s</italic>), we consider multiple channels with different relative scaling for over- and under-estimation of reward times thus generating a heterogeneous set of time scales. If <italic>t<sub>r</sub></italic> is the time of reward on a given trial, the corresponding prediction error is given by <italic>δ</italic> = <italic>t<sub>r</sub></italic> − <italic>τ</italic> and the update rules are <disp-formula id="FD19"><mml:math id="M41"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>δ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mo>−</mml:mo></mml:msup><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>δ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P60">These learning rules converge to the <italic>quantiles</italic> of the probability distribution over expectile reward times <inline-formula><mml:math id="M42"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula><sup><xref ref-type="bibr" rid="R6">6</xref></sup>. This is because these quantiles are uniformly distributed in the cumulative probability space (i.e. the domain of the cumulative distribution function <inline-formula><mml:math id="M43"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> over expectile reward times, and satisfy the optimal information-theoretic condition defined above (<xref ref-type="disp-formula" rid="FD14">Eqn. 5</xref>).</p></sec><sec id="S25"><label>7.3</label><title>Multi-dimensional integration over reward magnitude and time</title><p id="P61">Importantly, while in the distributional code for magnitude, the slope asymmetry <inline-formula><mml:math id="M44"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mo>−</mml:mo></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> controls the level of <italic>optimism</italic> in individual units <sup><xref ref-type="bibr" rid="R7">7</xref></sup>, in the temporal coding model developed here it controls the reward time scale and therefore the temporal discount factor, also known as <italic>impatience</italic><sup><xref ref-type="bibr" rid="R74">74</xref></sup>. Integrating the distributional models in reward magnitude and time, leading to the time magnitude reinforcement learning (TMRL), each neuron is characterized by an optimism level <italic>κ</italic> and an impatience level <italic>η</italic>, corresponding to the temporal discount factor <italic>γ<sub>j</sub></italic> (which is induced from a corresponding reward time scale <italic>τ<sub>j</sub></italic>). Therefore the temporal-difference vector RPEs take the form, <disp-formula id="FD20"><mml:math id="M45"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mover accent="true"><mml:mi mathvariant="bold">V</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M46"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">V</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> is a random sample from the value distribution at time <italic>t</italic>+1. This is denoted by the <italic>imputation</italic> step, that implies a non-local update rule, as shown in previous work<sup><xref ref-type="bibr" rid="R70">70</xref>,<xref ref-type="bibr" rid="R75">75</xref></sup>. Understanding if, in the population of dopamine neurons, this manifests as a non-local update rule or may be implemented locally remains an open question. Finally, the multi-dimensional distributional value update rule is given by, <disp-formula id="FD21"><mml:math id="M47"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec></sec><sec id="S26"><label>8</label><title>Data analysis</title><p id="P62">Spike counts were binned in 2-ms windows and smoothed by convolving with a gamma probability distribution kernel (shape parameter <italic>k</italic> =2 and scale parameter <italic>θ</italic> =25ms).</p><p id="P63">We identified putative dopamine neurons based on their firing rate patterns in a window around the cue and reward using an unsupervised clustering approach as in previous studies<sup><xref ref-type="bibr" rid="R76">76</xref>–<xref ref-type="bibr" rid="R18">18</xref></sup>. In summary, for each neuron and each 200ms time bin, the area under the receiver operator curve (auROC) was calculated between the distribution of firing rates across trials for that bin and the distribution of baseline firing rates (1s before cue). PCA of the auROC was calculated and then hierarchical clustering was done using the first three PCs using the Euclidean distance metric and the complete agglomeration method. As described before, three clusters were found: one revealed sustained inhibition at rewards (Type I), one had phasic responses to cue and reward, where the vast majority of photo-identified neurons were included (Type II) and the last had sustained excitation to reward (Type III). Type II neurons were classified as putative dopamine neurons. We focus our analysis on both light-identified dopamine neurons (n=43) and putative dopamine neurons (n=131).</p><p id="P64">Responses to reward were defined as the average activity from 200 to 650 ms after cue and reward onset, baseline subtracted by the mean activity over trials from -1000 ms to 0 ms relative to cue onset. Responses to cue were defined as the average activity from 200 to 650 ms after cue onset. This window was selected in order to exclude the initial response to the solenoid valve opening, that was in the majority of the neurons not selective to reward amount or delay, as shown in previous literature<sup><xref ref-type="bibr" rid="R79">79</xref></sup>. For each dopamine neuron <italic>j</italic>, we assumed the tuning function of reward time (<italic>t<sub>r</sub></italic>) was given by an exponential decaying function<sup><xref ref-type="bibr" rid="R80">80</xref></sup>, <disp-formula id="FD22"><label>(9)</label><mml:math id="M48"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>γ<sub>j</sub></italic> is the temporal discount factor and <italic>a<sub>j</sub></italic> is a gain parameter. The parameters were estimated minimizing the mean squared error between <italic>δ<sub>j</sub></italic> (<italic>t<sub>r</sub></italic>) and the mean responses at the cue for each reward time.</p><p id="P65">To test whether the estimated temporal discount factors did not reflect noise we divided the trials in two random partitions, such that each partition contained the same number of trials for each reward delay. Then we estimated the temporal discounts for each partition and measured the linear regression slope, correlation coefficient and p-value for these sets of estimates. We repeated 10,000 times this procedure, and computed the mean correlation coefficient and the geometric mean of the obtained p-values.</p><p id="P66">In order to test if the single neuron’s estimated temporal discounts are significantly different from the population mean temporal discount, we took randomly selected 50% of trials per delay, estimated the temporal discounts and repeated 1000 times, to obtain confidence intervals.</p><p id="P67">To test if changes in the reward time statistics would lead to an adaptation of the temporal discounts, we removed either the longest or shortest delay and estimated the temporal discount before and after this context switch. The responses to the same delays were used to estimate the temporal discounts before and after the manipulation. After the context switch, we did not consider the first 5 trials. To assess the degree of relative adaptation of the population activity when removing the shortest or longest delay at the end of the session, we bootstrapped considering 10,000 resamples and computed the p-value and confidence interval (for the null hypothesis that there is no adaptation in the mean population temporal discounts). We also tested if there was adaptation of the population temporal discounts separately, for each type of context switch, bootstrapping using 10,000 resamples and computing the p-value and confidence interval (for the null hypothesis that there is no adaptation).</p><p id="P68">To test whether discounting by dopamine neurons was updated more continuously, we started by computing the rate of reward occurrence, by convolving the occurrence of rewards with exponential discounting kernels with different time scales. For different time scales we observed the lower the rates, the larger the population temporal discount factors. We represent the adaptation in temporal discounts for the time scale that minimized the bootstrapped p-value (considering 10,000 resamples) comparing the update in temporal discounts for low and high rates (<xref ref-type="fig" rid="F15">Ext. Data Fig 9</xref>). Since for low reward rates the number of small reward delay trials was lower, and for high reward rates the number of high delay trials was lower, when fitting the temporal discounts factors, we weighted the errors by the normalized variance of the responses.</p><p id="P69">On the other hand, for each dopamine neuron <italic>i</italic>, we assumed the tuning function of reward magnitude (<italic>r</italic>) was given by a bilinear function, <disp-formula id="FD23"><mml:math id="M49"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>r</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mstyle><mml:mi>i</mml:mi><mml:mi>f</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M50"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> if the slope for the positive RPEs, <inline-formula><mml:math id="M51"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> the slope for negative RPEs, <inline-formula><mml:math id="M52"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the asymmetry in slopes for positive and negative RPEs and <inline-formula><mml:math id="M53"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the reversal point. As in previous work<sup><xref ref-type="bibr" rid="R7">7</xref></sup>, the reversal point was defined as the magnitude <italic>M</italic> that maximized the number of positive responses to rewards greater than <italic>M</italic> plus the number of negative responses to rewards less than <italic>M</italic>. After measuring reversal points, we fit linear functions separately to the positive (<inline-formula><mml:math id="M54"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>) and negative (<inline-formula><mml:math id="M55"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>) domains of each cell.</p><p id="P70">We measure the correlation between the firing rate at the cue corrected for the temporal discount and gain (<xref ref-type="disp-formula" rid="FD22">Equation 9</xref>) and report the correlation coefficient, p-value and 95% confidence interval.</p><p id="P71">To test if the variance of estimated reversal points for the variable cues was significantly greater than for the certain one, we boostrapped considering 10,000 resamples and computed the p-value 95% confidence interval.</p><p id="P72">To classify an event as a lick it had to have a minimum duration of 0.015s. Licks were binned in 0.13s windows and smoothed by convolving with a exponential decaying function with a time scale of 0.77s to obtain lick rates. The anticipatory licking slope, described in <xref ref-type="fig" rid="F3">Figure 3G</xref>, was defined as the linear regression slope in licking rate from <italic>t</italic> =0.01s to <italic>t</italic> = reward time.</p></sec><sec id="S27"><label>9</label><title>Future reward distribution decoding</title><sec id="S28"><label>9.1</label><title>Decoding future distribution of reward times</title><p id="P73">As defined before in <xref ref-type="disp-formula" rid="FD24">Equation 10</xref>, the value considering a time horizon of T is given by, <disp-formula id="FD24"><label>(10)</label><mml:math id="M56"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P74">The problem of determining the expected future rewards over time <inline-formula><mml:math id="M57"><mml:mrow><mml:mover accent="true"><mml:mstyle><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mi>r</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> can be seen as a linear regression problem. The data points correspond to the population temporal discounts {<italic>γ</italic><sub>1</sub>,⋯,<italic>γ<sub>N</sub></italic>}, the basis functions are given by <italic>ϕ</italic>(<italic>γ</italic>) = (<italic>γ</italic>,<italic>γ</italic><sup>2</sup>,⋯,γ<sup><italic>T</italic></sup>) and the targets are <bold>V</bold><sub><italic>γ</italic></sub> = {<italic>V</italic><sub><italic>γ</italic><sub>1</sub></sub>,…<italic>V<sub>γ<sub>N</sub></sub></italic>}, which are the single neuron’s responses at the cue. Assuming a uniform prior and a Gaussian likelihood functions with inverse variance <italic>β<sub>j</sub></italic> the log of the posterior is given by, <disp-formula id="FD25"><mml:math id="M58"><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mstyle><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>−</mml:mo></mml:mstyle><mml:mfrac><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mstyle><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mstyle><mml:mn>1</mml:mn></mml:mstyle><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P75">When the reward is either 0 or 1 the expected rewards at a given time-step <italic>t</italic> corresponds to the probability of rewards at that time-step, therefore the log posterior becomes, <disp-formula id="FD26"><label>(11)</label><mml:math id="M59"><mml:mrow><mml:mi>ln</mml:mi><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>−</mml:mo></mml:mstyle><mml:mfrac><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mstyle><mml:mi>r</mml:mi></mml:mstyle><mml:mo stretchy="true">¯</mml:mo></mml:mover><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mstyle><mml:mn>1</mml:mn></mml:mstyle><mml:mi>T</mml:mi></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P76">In practice, for each neuron we considered <italic>V<sub>j</sub></italic> the responses of each neuron normalized by the estimated gain and <italic>β<sub>j</sub></italic> was the estimated inverse of the variance of <inline-formula><mml:math id="M60"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The solution that maximized (11) was determined analytically using singular value decomposition (SVD), as proposed in<sup><xref ref-type="bibr" rid="R75">75</xref>,<xref ref-type="bibr" rid="R81">81</xref></sup>. To obtain a probability distribution, the solution was normalized. A qualitative search was done on the smoothing parameter, to maximize the similarity between the estimated and the true probability distribution.</p><p id="P77">For <xref ref-type="fig" rid="F3">Figure 3E</xref>, we computed the mean decoded densities using the population responses in trials with anticipatory licking slope greater or smaller than the median.</p></sec><sec id="S29"><label>9.2</label><title>Decoding future distribution of rewards amounts</title><p id="P78">The population firing rate at the cue corrected for the gain and temporal discount factor <inline-formula><mml:math id="M61"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is significantly correlated with the reversal points estimated at the reward delivery. We therefore assume an additive Gaussian noise model, <disp-formula id="FD27"><label>(12)</label><mml:math id="M62"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P79">In practice, for each neuron, <italic>β<sub>i</sub></italic> was the inverse of the estimated variance of <inline-formula><mml:math id="M63"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Each dopamine neuron <italic>i</italic> has a different level of optimism <italic>κ<sub>i</sub></italic>, by weighting asymmetrically positive (<inline-formula><mml:math id="M64"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>) and negative (<inline-formula><mml:math id="M65"><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula>) RPEs, with <inline-formula><mml:math id="M66"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, predicting different values, <inline-formula><mml:math id="M67"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We measure the responses of each neuron at only five different reward amounts in a small range of rewards, from 1<italic>μ</italic>l-8<italic>μ</italic>l. We use Monte Carlo simulations to estimate the bias and variance we are inducing in the estimation of the <italic>κ</italic> parameter. For these simulations we assume the noise in the responses is Gaussian and use the variance of each neuron’s responses for each reward amount to generate responses. We observe that there is a sistematic relationship in the induced bias and variance: the variance increases quadratically with the distance to the mean of the reward magnitudes and the absolute bias increases linearly with the distance to the mean of the reward amounts, and it is positive or negative for reversal points smaller or greater than the mean reward amounts, respectively (<xref ref-type="fig" rid="F14">Ext. Data Fig. 8</xref>). We therefore subtract the induced bias to the estimated <italic>κ<sub>i</sub></italic>’s. We use a isotonic regression to model the relationship between the reversals <inline-formula><mml:math id="M68"><mml:mrow><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>’s and <italic>κ<sub>i</sub></italic>’s<sup><xref ref-type="bibr" rid="R82">82</xref></sup>, which only assumes this is an increasing function. To estimate the piecewise linear functions in isotonic regression we take in account the variance in the estimation of each <italic>κ<sub>i</sub></italic>. We assume the population of <italic>N</italic> neurons is minimizing the loss function<sup><xref ref-type="bibr" rid="R7">7</xref></sup> <disp-formula id="FD28"><mml:math id="M69"><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>~</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>~</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <disp-formula id="FD29"><label>(13)</label><mml:math id="M70"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>κ</mml:mi></mml:msub><mml:mo>(</mml:mo><mml:mi>u</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>κ</mml:mi><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mtext> if </mml:mtext><mml:mi>u</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>κ</mml:mi><mml:mo>)</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mtext> else</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P80">Considering asymmetric Gaussian likelihood functions<sup><xref ref-type="bibr" rid="R83">83</xref></sup> with inverse variance <italic>β<sub>i</sub></italic>, we obtain the posterior over reward amounts, <disp-formula id="FD30"><label>(14)</label><mml:math id="M71"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>δ</mml:mi><mml:mo>→</mml:mo></mml:mover><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold">V</mml:mi><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>κ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="bold">V</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p></sec><sec id="S30"><label>9.3</label><title>Decoding future rewards over time and amount</title><p id="P81">Taking in account the dopamine population diversity in value prediction in terms of temporal discount <bold>V</bold><sub><italic>γ</italic></sub> and optimism level <bold>V</bold><sub><italic>κ</italic></sub>, the posterior over reward magnitude and time is given by, <disp-formula id="FD31"><mml:math id="M72"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mo>(</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P82">There is evidence that the diversity in dopamine tuning functions to reward time is independent of the diversity to reward magnitude (see <xref ref-type="fig" rid="F17">Ext. Data Fig. 11A</xref>), hence we can factorize the above equation, <disp-formula id="FD32"><mml:math id="M73"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mo>∝</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow/></mml:mtd><mml:mtd columnalign="left"><mml:mo>∝</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P83">Conditioned on the cues, the amounts and times of reward are independent in our behavioral task, hence we assume the joint distribution over reward amounts and times can be factorized, <disp-formula id="FD33"><label>(15)</label><mml:math id="M74"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>κ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mstyle><mml:mi mathvariant="bold">V</mml:mi></mml:mstyle><mml:mi>γ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P84">Thus we derive a probabilistic decoding framework for independently decoding reward time and magnitude. This, despite the limited numbers of neurons in our dataset, allows for the decoding of the reward distribution over these two dimensions in practice. In principle, with a sufficiently large number of neurons this assumption can be relaxed (see <xref ref-type="fig" rid="F17">Ext. Data Fig. 11B,C</xref>).</p></sec></sec><sec id="S31"><label>10</label><title>Foraging simulations</title><p id="P85">In the foraging simulations in <xref ref-type="fig" rid="F6">Figure 6</xref> we implemented a temporal-difference learning algorithm for estimating the value of each patch (value RL). Simultaneously, we implemented a temporal-difference algorithm for estimating the occupancy matrix, that was multiplied by the reward, to obtain the sucessor representation (SR) estimation of value for each patch. For the TMRL we considered a set of values with different temporal discounts, and with these, we decoded the probability distribution over future reward for each patch. If the time of rewards in one patch was shorter, the agent selected that patch first. Initially the values for all algorithms were set to zero. The policy was the softmax over the value of each patch, with temperature parameter set to 0.2, however the simulations results are qualitatively independent of this parameter. A reference temporal discount was used to obtain a value and get a policy for TMRL. The learning rate was set to 0.02. For the stationary environment, patch one always retrieved a reward of 6 and patch two a reward of 2. For the non-stationary environment, patch one retrieved a reward of 2 after 2 time steps, patch two a reward of 0 or 4 after 2 time steps and patch three a reward of 6 after 6 time steps. Early in learning corresponds to 1000 time-steps and late in learning corresponds to 10,000 time-steps.</p><p id="P86">In the foraging simulations in <xref ref-type="fig" rid="F6">Figure 6E</xref>, the change in internal state from hungry to sated was modelled as a change in the intrinsic value or utility of reward. We model the utility function in time as a decaying function in time parametrised by the temporal discount factor (<italic>γ<sup>t</sup></italic>). When the mice was hungry the temporal discount was set to <italic>γ</italic> = 0.6 and the utility function in reward magnitude was considered convex<sup><xref ref-type="bibr" rid="R84">84</xref></sup>. When the mice became sated, the temporal discount factor was increased to <italic>γ</italic> =1<sup><xref ref-type="bibr" rid="R85">85</xref></sup> and the utility function in reward magnitudes was considered to be linear. We consider the utility matrix <italic>U</italic> defined over a discretized time range, {1,…,<italic>j</italic>,…,<italic>T</italic>}, and magnitude range, {<italic>u</italic><sub>1</sub>,…,<italic>u<sub>i</sub></italic>,…,<italic>u<sub>N</sub></italic>}, <disp-formula id="FD34"><label>(16)</label><mml:math id="M75"><mml:mi>U</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>…</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:msup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:msup><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mrow/></mml:mtd><mml:mtd><mml:mrow><mml:mo>…</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:msup><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:math></disp-formula></p><p id="P87">Since the TMRL allows for the construction of a probability distribution over future reward time and magnitude, once the agent has experienced reward in the three different patches for the sated state, it can recompute the value of each patch for the new state, using the distributional map. In particular, for each time-step in the future, the probability distribution over reward magnitudes <italic>p<sub>ij</sub></italic> is normalized, such that for all <italic>j</italic>, <disp-formula id="FD35"><mml:math id="M76"><mml:mrow><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>=</mml:mo><mml:mn>1.</mml:mn></mml:mrow></mml:math></disp-formula></p><p id="P88">Then, each entry of this matrix is multiplied by the respective element in the utility matrix and summed to obtain the value for each patch, <disp-formula id="FD36"><label>(17)</label><mml:math id="M77"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P89">The SR agent can also flexibly recompute the value function for the sated state using the occupancy matrix. However, the standard value RL agent needs to learn through experience to compute the value of each patch for the new state.</p></sec></sec><sec id="S32" sec-type="extended-data"><title>Extended Data</title><fig id="F7" position="anchor"><label>Extended Data Figure 1</label><caption><title>Histological reconstruction of recording sites.</title><p id="P90">Recording locations of the probe tracks. Recording sites from all mice in seven coronal sections from the rostrocaudal axis (AP -2.68, -2.76, -2.84, -2.92, -3.05, -3.13,, –3.17). The approximate location of the photo-ided neurons is depicted in green, the VTA has a red outline and the SNc a blue outline.</p></caption><graphic xlink:href="EMS190954-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Figure 2</label><caption><title>Photo-identification of dopamine neurons.</title><p id="P91"><bold>(A)</bold> Three example photo-ided neurons. <bold>Top)</bold> Raster plot with single spikes aligned to laser pulse onset (10 ms duration). <bold>Bottom)</bold> Distribution of latencies to first spike after laser pulse observed in a 1-20ms window Bottom-inset) mean waveform (black) and mean laser-triggered waveform (blue). Distribution of: <bold>(B)</bold> probability of observing a spike xsbetween 1 and 10 ms after laser onset pulse. <bold>(C)</bold> median latency to first spike in a window between 1ms and 20ms. <bold>(D)</bold> Differences in firing rate between the baseline and 1-10ms post-pulse window. <bold>(E)</bold> Log of the p-value of the salt test<sup><xref ref-type="bibr" rid="R54">54</xref></sup>. <bold>(F)</bold> Correlation coefficient (<italic>ρ</italic>) between the mean waveform and the mean laser-triggered waveform. <bold>(G)</bold> Log of t-test for the difference in firing rates between baseline and pos-pulse firing rate in a window 1-10ms.</p></caption><graphic xlink:href="EMS190954-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Figure 3</label><caption><title>Hierarchical clustering of VTA/SNc neurons.</title><p id="P92"><bold>(A)</bold> Area under the receiver operator curve (auROC) aligned to cue and reward delivery for the recorded population of neurons. Pink corresponds to increase relative to baseline activity, green to decrease and black to no change. The blue dots correspond to photo-ided neurons. <bold>(B)</bold> Projection coefficient for the three first principal components (PCs) of the auRoc. <bold>(C)</bold> Using these projection coefficients, hierarchical clustering with the euclidean distance was performed using the complete agglomeration method. <bold>Top:</bold> Each dot corresponds to a recorded neuron in the 3 first PC space. <bold>(D) Left:</bold> PSTH for each cluster for the different reward delays aligned to cue. <bold>Right:</bold> PSTH for each cluster for the different reward amounts aligned to reward delivery.</p></caption><graphic xlink:href="EMS190954-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Figure 4</label><caption><title>Heterogeneity in temporal discounting in putative dopamine neurons.</title><p id="P93"><bold>(A,B,C,D,F)</bold> Analysis from <xref ref-type="fig" rid="F3">Figure 3</xref> with putative dopamine neurons. <bold>(E)</bold> Estimated single neuron temporal discount factors as a function of the position in the medial-lateral axis. Dots are color coded by animal identity and dots with a blue outer circle correspond to the photo-identified neurons. The line represents the Huber Regression fit. Spearman rank correlation <italic>ρ</italic> = −0.12, two-tailed test p-value=0.26.</p></caption><graphic xlink:href="EMS190954-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Figure 5</label><caption><title>Testing the influence of number of experienced delays on estimated temporal discounts.</title><p id="P94"><bold>(A)</bold> Responses at the 1.5s delay cue as a function of the responses at the 0s cue. The dashed line represents the unitary line. <bold>(B)</bold> Similar to <xref ref-type="fig" rid="F3">Figure 3</xref> (C) but considering the responses at the 1.5s reward as an (under) estimator of the 0s cue responses, for neurons recorded from the mice only subject to three delays.</p></caption><graphic xlink:href="EMS190954-f011"/></fig><fig id="F12" position="anchor"><label>Extended Data Figure 6</label><caption><title>Analysis from <xref ref-type="fig" rid="F4">Figure 4</xref> with putative dopamine neurons.</title></caption><graphic xlink:href="EMS190954-f012"/></fig><fig id="F13" position="anchor"><label>Extended Data Figure 7</label><caption><title>Analysis from <xref ref-type="fig" rid="F5">Figure 5</xref> with putative dopamine neurons.</title></caption><graphic xlink:href="EMS190954-f013"/></fig><fig id="F14" position="anchor"><label>Extended Data Figure 8</label><caption><title>Decoding distribution over reward magnitudes.</title><p id="P95"><bold>(A)</bold> Responses for different reward amounts for individual dopamine neurons sorted by estimated reversal point. <bold>(B)</bold> Bias in the estimation of <italic>κ</italic> as a function of reversal point, computed with monte carlo simulations. <bold>(C)</bold> Variance in the estimation of <italic>κ</italic> as a function of reversal point, computed with monte carlo simulations. <bold>(D)</bold> Distribution of the difference between the reversal point estimated at reward and at the cue, using the linear regression depicted in <xref ref-type="fig" rid="F4">Figure 4A</xref>. <bold>(E)</bold> Corrected asymmetry as a function of the reversal point estimated at the reward, color coded by the confidence. The dashed line represents the isotonic regression fit. <bold>(F)</bold> Corrected asymmetry as a function of the reversal point estimated at the cue, color coded by the confidence. The dashed line corresponds to the isotonic regression fit.</p></caption><graphic xlink:href="EMS190954-f014"/></fig><fig id="F15" position="anchor"><label>Extended Data Figure 9</label><caption><title>Adaptation of dopamine neuron population temporal discount factors with reward occurrence rate.</title><p id="P96"><bold>(A)</bold> Considering exponential decaying kernels with different time constants to compute the rate of reward occurrence, we compute the p-value bootstrapping 10000 times, comparing the update in the estimated temporal discount factor for relatively high or low rate of reward occurrence. Vertical blue line: time scale of 39s that minimizes the p-value. <bold>(B)</bold> The estimated temporal discounts updates for relatively low or high rate of reward occurrence, computed using a exponential decaying kernel with the time scale that minimizes the p-value in (A). The correspondent 95% CI=(0.02,0.24). The vertical lines are the mean update in temporal discount rates.</p></caption><graphic xlink:href="EMS190954-f015"/></fig><fig id="F16" position="anchor"><label>Extended Data Figure 10</label><caption><title>Efficient coding predictions.</title><p id="P97"><bold>(A,B)</bold> Densities after removing the shortest (green) or longest (pink) reward delay. <bold>(C)</bold> The optimized tuning functions for the densities of reward time depicted in A and B. <bold>(D)</bold> The Fisher Information for the optimized populations for the range of 2-8 seconds.</p></caption><graphic xlink:href="EMS190954-f016"/></fig><fig id="F17" position="anchor"><label>Extended Data Figure 11</label><caption><title>At a population level, the distribution over temporal discounts and reversal points are statistically independent. However, in principle, the joint distribution can be decoded without assuming factorization over time and magnitude.</title><p id="P98"><bold>(A)</bold> Estimated reversal points as a function of estimated temporal discount factors, for photo-identified and putative dopamine neurons. Only neurons with reversal points in the range of reward magnitudes given in the experiment (1 μl-8μl) were included. The p-value refers to the chi-squared test, considering the null hypothesis that the joint distribution over the temporal discounts and reversal points is equal to the product of the marginals, and considering 16 degrees of freedom. <bold>(B)</bold> We simulate a population of n=100 units with a diverse set of temporal discount (<italic>γ</italic>) and reversal points uniformly sampled and decode the reward distribution over magnitude and time (not assuming these features are independent), from the responses at the cue. Importantly, these simulations were done for the cue that predicts a variable amount after a delay of three seconds, as described in <xref ref-type="fig" rid="F2">Figure 2</xref>. We first apply the inverse Laplace for each reversal point and get the temporal evolution of reversal points (middle). Then, for each time we decode the distribution over reward magnitudes (lower), by scaling the probability for each time point by one minus of probability of reward=0. <bold>(C)</bold> The same simulations but for all cues described in <xref ref-type="fig" rid="F4">Figure 4</xref>.</p></caption><graphic xlink:href="EMS190954-f017"/></fig></sec></body><back><ack id="S33"><title>Acknowledgements</title><p>We thank H. Schuett, F. Rodrigues, T. Duarte and C. Haimerl for comments on versions of the manuscript and the entire Paton laboratory, past and present, for feedback during the course of this project. The work was funded by an HHMI International Research Scholar Award to J.J.P. (55008745), a European Research Council Consolidator grant (DYCOCIRC - REP-772339-1) to J.J.P., a Bial bursary for scientific research to J.J.P. (193/2016), internal support from the Champalimaud Foundation, and PhD fellowships from FCT to M.S. (PD/BD/141552/2018) and B.F.C. (PD/BD/105945/2014). The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript.</p></ack><sec id="S34" sec-type="data-availability"><title>Data Availability</title><p id="P99">All data generated or analyzed during this study (and its supplementary information files) will be made available upon publication.</p></sec><sec id="S35" sec-type="data-availability"><title>Code Availability</title><p id="P100">The analysis code and code used to generate the theoretical predictions and simulations will be published in an online repository.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P101"><bold>Author contributions</bold></p><p id="P102">M.S. developed the theory, together with J.J.P and D.M and with input from K.L. Experiments were designed by J.J.P, M.S. and B.F.C. The experimental apparatus was constructed by. B.F.C., P.B. and M.S. All behavioral and electrophysiological experiments that provided data for the study were performed by P.B., and B.F.C. performed pilot experiments to establish protocols. P.B. and M.S. analysed histological data. M.S. analysed the neural and behavioral data, with input from B.F.C., K.L., and D.M. M.S. performed the foraging simulations with input from D.M and K.L. J.J.P. and M.S. wrote the paper, and D.M., K.L., and B.F.C. edited the paper. J.J.P. supervised all aspects of the project.</p></fn><fn id="FN2" fn-type="conflict"><p id="P103"><bold>Declaration of Interests</bold></p><p id="P104">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><year>1997</year><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Introduction to reinforcement learning</source><publisher-name>MIT Press Cambridge</publisher-name><year>1998</year><volume>135</volume></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Long</surname><given-names>T</given-names></name></person-group><article-title>Statistical models of conditioning</article-title><source>Adv Neural Inf Process Syst</source><year>1997</year><volume>10</volume></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name><name><surname>Gibbon</surname><given-names>J</given-names></name></person-group><article-title>Time, rate, and conditioning</article-title><source>Psychol Rev</source><year>2000</year><volume>107</volume><fpage>289</fpage><lpage>344</lpage><pub-id pub-id-type="pmid">10789198</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname><given-names>PR</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title><source>J Neurosci</source><year>1996</year><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="pmcid">PMC6578666</pub-id><pub-id pub-id-type="pmid">8774460</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Rowland</surname><given-names>M</given-names></name><name><surname>Bellemare</surname><given-names>M</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name></person-group><source>Distributional Reinforcement Learning With Quantile Regression</source><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><year>2018</year><volume>32</volume><pub-id pub-id-type="doi">10.1609/aaai.v32i1.11791</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dabney</surname><given-names>W</given-names></name><etal/></person-group><article-title>A distributional code for value in dopamine-based reinforcement learning</article-title><source>Nature</source><year>2020</year><volume>577</volume><fpage>671</fpage><lpage>675</lpage><pub-id pub-id-type="pmcid">PMC7476215</pub-id><pub-id pub-id-type="pmid">31942076</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1924-6</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avvisati</surname><given-names>R</given-names></name><etal/></person-group><article-title>Distributional coding of associative learning within projection-defined populations of midbrain dopamine neurons</article-title><source>bio Rxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.07.18.500429</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muller</surname><given-names>TH</given-names></name><etal/></person-group><article-title>Distributional reinforcement learning in prefrontal cortex</article-title><source>bio Rxiv</source><year>2021</year><comment>2021.06.14.448422</comment><pub-id pub-id-type="doi">10.1101/2021.06.14.448422</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Munos</surname><given-names>R</given-names></name></person-group><chapter-title>A Distributional Perspective on Reinforcement Learning</chapter-title><person-group person-group-type="editor"><name><surname>Precup</surname><given-names>D</given-names></name><name><surname>Teh</surname><given-names>YW</given-names></name></person-group><source>Proceedings of the 34th International Conference on Machine Learning</source><publisher-name>PMLR</publisher-name><year>2017</year><volume>70</volume><fpage>449</fpage><lpage>458</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Lyskawinski</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Englot</surname><given-names>B</given-names></name></person-group><chapter-title>Stochastically Dominant Distributional Reinforcement Learning</chapter-title><person-group person-group-type="editor"><name><surname>Iii</surname><given-names>HD</given-names></name><name><surname>Singh</surname><given-names>A</given-names></name></person-group><source>Proceedings of the 37th International Conference on Machine Learning</source><publisher-name>PMLR</publisher-name><year>2020</year><month>Jul</month><day>13--18</day><volume>119</volume><fpage>6745</fpage><lpage>6754</lpage></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Théate</surname><given-names>T</given-names></name><name><surname>Ernst</surname><given-names>D</given-names></name></person-group><article-title>Risk-Sensitive Policy with Distributional Reinforcement Learning</article-title><source>Algorithms</source><year>2023</year><volume>16</volume><fpage>325</fpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Puterman</surname><given-names>ML</given-names></name></person-group><source>Markov Decision Processes: Discrete Stochastic Dynamic Programming</source><publisher-name>John Wiley &amp; Sons</publisher-name><year>2014</year></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Temporal-difference reinforcement learning with distributed representations</article-title><source>PLoS One</source><year>2009</year><volume>4</volume><elocation-id>e7362</elocation-id><pub-id pub-id-type="pmcid">PMC2760757</pub-id><pub-id pub-id-type="pmid">19841749</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007362</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shankar</surname><given-names>KH</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>A scale-invariant internal representation of time</article-title><source>Neural Comput</source><year>2012</year><volume>24</volume><fpage>134</fpage><lpage>193</lpage><pub-id pub-id-type="pmid">21919782</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tano</surname><given-names>P</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><chapter-title>A Local Temporal Difference Code for Distributional Reinforcement Learning</chapter-title><person-group person-group-type="editor"><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Ranzato</surname><given-names>M</given-names></name><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Balcan</surname><given-names>MF</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2020</year><volume>33</volume><fpage>13662</fpage><lpage>13673</lpage></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tiganj</surname><given-names>Z</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>Estimating scale-invariant future in continuous time</article-title><source>Neural Comput</source><year>2019</year><volume>31</volume><fpage>681</fpage><lpage>709</lpage><pub-id pub-id-type="pmcid">PMC6959535</pub-id><pub-id pub-id-type="pmid">30764739</pub-id><pub-id pub-id-type="doi">10.1162/neco_a_01171</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><etal/></person-group><article-title>Possible principles underlying the transformation of sensory messages</article-title><source>Sensory communication</source><year>1961</year><volume>1</volume></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>S</given-names></name></person-group><article-title>A simple coding procedure enhances a neuron’s information capacity</article-title><source>Z Naturforsch C</source><year>1981</year><volume>36</volume><fpage>910</fpage><lpage>912</lpage><pub-id pub-id-type="pmid">7303823</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><article-title>Natural image statistics and neural representation</article-title><source>Annu Rev Neurosci</source><year>2001</year><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="pmid">11520932</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Lewen</surname><given-names>GD</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name><name><surname>de Ruyter Van Steveninck</surname><given-names>RR</given-names></name></person-group><article-title>Efficiency and ambiguity in an adaptive neural code</article-title><source>Nature</source><year>2001</year><volume>412</volume><fpage>787</fpage><lpage>792</lpage><pub-id pub-id-type="pmid">11518957</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguli</surname><given-names>D</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Efficient sensory encoding and Bayesian inference with heterogeneous neural populations</article-title><source>Neural Comput</source><year>2014</year><volume>26</volume><fpage>2103</fpage><lpage>2134</lpage><pub-id pub-id-type="pmcid">PMC4167880</pub-id><pub-id pub-id-type="pmid">25058702</pub-id><pub-id pub-id-type="doi">10.1162/NECO_a_00638</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Schütt</surname><given-names>HH</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><article-title>Reward prediction error neurons implement an efficientcode for reward</article-title><source>bio Rxiv</source><year>2022</year><elocation-id>2022.11.03.515104</elocation-id><pub-id pub-id-type="doi">10.1101/2022.11.03.515104</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name></person-group><article-title>Asymmetric and adaptive reward coding via normalized reinforcement learning</article-title><source>PLoS Comput Biol</source><year>2022</year><volume>18</volume><elocation-id>e1010350</elocation-id><pub-id pub-id-type="pmcid">PMC9345478</pub-id><pub-id pub-id-type="pmid">35862443</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010350</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Improving Generalization for Temporal Difference Learning: The Successor Representation</article-title><source>Neural Comput</source><year>1993</year><volume>5</volume><fpage>613</fpage><lpage>624</lpage></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>Predicting the Future with Multi-scale Successor Representations</article-title><source>bio Rxiv</source><year>2018</year><elocation-id>449470</elocation-id><pub-id pub-id-type="doi">10.1101/449470</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>MJYM</given-names></name></person-group><article-title>Coexistence of temporally partitioned spiny mice: roles of habitat structure and foraging behavior</article-title><source>Ecology</source><year>2001</year><volume>82</volume><fpage>2164</fpage><lpage>2176</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamada</surname><given-names>H</given-names></name><name><surname>Tymula</surname><given-names>A</given-names></name><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name></person-group><article-title>Thirst-dependent risk preferences in monkeys identify a primitive form of wealth</article-title><source>Proc Natl Acad Sci U S A</source><year>2013</year><volume>110</volume><fpage>15788</fpage><lpage>15793</lpage><pub-id pub-id-type="pmcid">PMC3785724</pub-id><pub-id pub-id-type="pmid">24019461</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1308718110</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stauffer</surname><given-names>WR</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Dopamine reward prediction error responses reflect marginal utility</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>2491</fpage><lpage>2500</lpage><pub-id pub-id-type="pmcid">PMC4228052</pub-id><pub-id pub-id-type="pmid">25283778</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2014.08.064</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kacelnik</surname><given-names>A</given-names></name><name><surname>Bateson</surname><given-names>M</given-names></name></person-group><article-title>Risky theories—the effects of variance on foraging decisions</article-title><source>Am Zool</source><year>1996</year><volume>36</volume><fpage>402</fpage><lpage>434</lpage></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshimura</surname><given-names>J</given-names></name><name><surname>Ito</surname><given-names>H</given-names></name><name><surname>Miller</surname><given-names>DG</given-names><suffix>III</suffix></name><name><surname>Tainaka</surname><given-names>K-I</given-names></name></person-group><article-title>Dynamic decision-making in uncertain environments I The principle of dynamic utility</article-title><source>J Ethol</source><year>2013</year><volume>31</volume><fpage>101</fpage><lpage>105</lpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kagel</surname><given-names>JH</given-names></name><name><surname>Green</surname><given-names>L</given-names></name><name><surname>Caraco</surname><given-names>T</given-names></name></person-group><article-title>When foragers discount the future: constraint or adaptation?</article-title><source>Anim Behav</source><year>1986</year><volume>34</volume><fpage>271</fpage><lpage>283</lpage></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Uncertainty-based competition between prefrontal and dorsolateral striatal systems for behavioral control</article-title><source>Nat Neurosci</source><year>2005</year><volume>8</volume><fpage>1704</fpage><lpage>1711</lpage><pub-id pub-id-type="pmid">16286932</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craik</surname><given-names>KJW</given-names></name></person-group><source>The Nature of Explanation by K J W Craik</source><year>1943</year></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNamee</surname><given-names>D</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><article-title>Internal Models in Biological Control</article-title><source>Annu Rev Control Robot Auton Syst</source><year>2019</year><volume>2</volume><fpage>339</fpage><lpage>364</lpage><pub-id pub-id-type="pmcid">PMC6520231</pub-id><pub-id pub-id-type="pmid">31106294</pub-id><pub-id pub-id-type="doi">10.1146/annurev-control-060117-105206</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balleine</surname><given-names>BW</given-names></name><name><surname>O’Doherty</surname><given-names>JP</given-names></name></person-group><article-title>Human and rodent homologies in action control:corticostriatal determinants of goal-directed and habitual action</article-title><source>Neuropsychopharmacology</source><year>2010</year><volume>35</volume><fpage>48</fpage><lpage>69</lpage><pub-id pub-id-type="pmcid">PMC3055420</pub-id><pub-id pub-id-type="pmid">19776734</pub-id><pub-id pub-id-type="doi">10.1038/npp.2009.131</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russek</surname><given-names>EM</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Predictive representations can link model-based reinforcement learning to model-free mechanisms</article-title><source>PLoS Comput Biol</source><year>2017</year><volume>13</volume><elocation-id>e1005768</elocation-id><pub-id pub-id-type="pmcid">PMC5628940</pub-id><pub-id pub-id-type="pmid">28945743</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005768</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Learning the value of information in an uncertain world</article-title><source>Nat Neurosci</source><year>2007</year><volume>10</volume><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Izquierdo</surname><given-names>A</given-names></name></person-group><article-title>Adaptive learning under expected and unexpected uncertainty</article-title><source>Nat Rev Neurosci</source><year>2019</year><volume>20</volume><fpage>635</fpage><lpage>644</lpage><pub-id pub-id-type="pmcid">PMC6752962</pub-id><pub-id pub-id-type="pmid">31147631</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0180-y</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><etal/></person-group><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nat Neurosci</source><year>2012</year><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="pmcid">PMC3386464</pub-id><pub-id pub-id-type="pmid">22660479</pub-id><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpe</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Dopamine transients do not act as model-free prediction errors during associative learning</article-title><source>Nat Commun</source><year>2020</year><volume>11</volume><fpage>106</fpage><pub-id pub-id-type="pmcid">PMC6949299</pub-id><pub-id pub-id-type="pmid">31913274</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13953-1</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelhard</surname><given-names>B</given-names></name><etal/></person-group><article-title>Specialized coding of sensory, motor and cognitive variables in VTA dopamine neurons</article-title><source>Nature</source><year>2019</year><volume>570</volume><fpage>509</fpage><lpage>513</lpage><pub-id pub-id-type="pmcid">PMC7147811</pub-id><pub-id pub-id-type="pmid">31142844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1261-9</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpe</surname><given-names>MJ</given-names></name><etal/></person-group><article-title>Dopamine transients are sufficient and necessary for acquisition of model-based associations</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>735</fpage><lpage>742</lpage><pub-id pub-id-type="pmcid">PMC5413864</pub-id><pub-id pub-id-type="pmid">28368385</pub-id><pub-id pub-id-type="doi">10.1038/nn.4538</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeong</surname><given-names>H</given-names></name><etal/></person-group><article-title>Mesolimbic dopamine release conveys causal associations</article-title><source>Science</source><year>2022</year><volume>378</volume><elocation-id>eabq6740</elocation-id><pub-id pub-id-type="pmcid">PMC9910357</pub-id><pub-id pub-id-type="pmid">36480599</pub-id><pub-id pub-id-type="doi">10.1126/science.abq6740</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coddington</surname><given-names>LT</given-names></name><name><surname>Dudman</surname><given-names>JT</given-names></name></person-group><article-title>The timing of action determines reward prediction signals in identified midbrain dopamine neurons</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1563</fpage><lpage>1573</lpage><pub-id pub-id-type="pmcid">PMC6226028</pub-id><pub-id pub-id-type="pmid">30323275</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0245-7</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tesauro</surname><given-names>G</given-names></name></person-group><article-title>Practical issues in temporal difference learning</article-title><source>Mach Learn</source><year>1992</year><volume>8</volume><fpage>257</fpage><lpage>277</lpage></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname><given-names>AM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>Multiplicity of control in the basal ganglia: computational roles of striatal subregions</article-title><source>Curr Opin Neurobiol</source><year>2011</year><volume>21</volume><fpage>374</fpage><lpage>380</lpage><pub-id pub-id-type="pmcid">PMC3269306</pub-id><pub-id pub-id-type="pmid">21429734</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2011.02.009</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>HH</given-names></name><etal/></person-group><article-title>Dynamic reorganization of striatal circuits during the acquisition and consolidation of a skill</article-title><source>Nat Neurosci</source><year>2009</year><volume>12</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="pmcid">PMC2774785</pub-id><pub-id pub-id-type="pmid">19198605</pub-id><pub-id pub-id-type="doi">10.1038/nn.2261</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamid</surname><given-names>AA</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name><name><surname>Moore</surname><given-names>CI</given-names></name></person-group><article-title>Wave-like dopamine dynamics as a mechanism for spatiotemporal credit assignment</article-title><source>Cell</source><year>2021</year><volume>184</volume><fpage>2733</fpage><lpage>2749</lpage><elocation-id>e16</elocation-id><pub-id pub-id-type="pmcid">PMC8122079</pub-id><pub-id pub-id-type="pmid">33861952</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2021.03.046</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruz</surname><given-names>BF</given-names></name><etal/></person-group><article-title>Action suppression reveals opponent parallel control via striatal circuits</article-title><source>Nature</source><year>2022</year><volume>607</volume><fpage>521</fpage><lpage>526</lpage><pub-id pub-id-type="pmid">35794480</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A vector reward prediction error model explains dopaminergic heterogeneity</article-title><source>bio Rxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.02.28.482379</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>YK</given-names></name><etal/></person-group><article-title>Dopaminergic prediction errors in the ventral tegmental area reflect a multithreaded predictive model</article-title><source>Nat Neurosci</source><year>2023</year><volume>26</volume><fpage>830</fpage><lpage>839</lpage><pub-id pub-id-type="pmcid">PMC10646487</pub-id><pub-id pub-id-type="pmid">37081296</pub-id><pub-id pub-id-type="doi">10.1038/s41593-023-01310-x</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balsam</surname><given-names>PD</given-names></name><name><surname>Gallistel</surname><given-names>CR</given-names></name></person-group><article-title>Temporal maps and informativeness in associative learning</article-title><source>Trends Neurosci</source><year>2009</year><volume>32</volume><fpage>73</fpage><lpage>78</lpage><pub-id pub-id-type="pmcid">PMC2727677</pub-id><pub-id pub-id-type="pmid">19136158</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2008.10.004</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kvitsiani</surname><given-names>D</given-names></name><etal/></person-group><article-title>Distinct behavioural and network correlates of two interneuron types in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>498</volume><fpage>363</fpage><lpage>366</lpage><pub-id pub-id-type="pmcid">PMC4349584</pub-id><pub-id pub-id-type="pmid">23708967</pub-id><pub-id pub-id-type="doi">10.1038/nature12176</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reynolds</surname><given-names>JN</given-names></name><name><surname>Hyland</surname><given-names>BI</given-names></name><name><surname>Wickens</surname><given-names>JR</given-names></name></person-group><article-title>A cellular mechanism of reward-related learning</article-title><source>Nature</source><year>2001</year><volume>413</volume><fpage>67</fpage><lpage>70</lpage><pub-id pub-id-type="pmid">11544526</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stauffer</surname><given-names>WR</given-names></name><etal/></person-group><article-title>Dopamine Neuron-Specific Optogenetic Stimulation in Rhesus Macaques</article-title><source>Cell</source><year>2016</year><volume>166</volume><fpage>1564</fpage><lpage>1571</lpage><elocation-id>e6</elocation-id><pub-id pub-id-type="pmcid">PMC5018252</pub-id><pub-id pub-id-type="pmid">27610576</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2016.08.024</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunnicutt</surname><given-names>BJ</given-names></name><etal/></person-group><article-title>A comprehensive excitatory input map of the striatum reveals novel functional organization</article-title><source>Elife</source><year>2016</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC5207773</pub-id><pub-id pub-id-type="pmid">27892854</pub-id><pub-id pub-id-type="doi">10.7554/eLife.19103</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>M</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name></person-group><article-title>Multiple representations and algorithms for reinforcement learning in the cortico-basal ganglia circuit</article-title><source>Curr Opin Neurobiol</source><year>2011</year><volume>21</volume><fpage>368</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">21531544</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname><given-names>B</given-names></name><name><surname>Monteiro</surname><given-names>T</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><article-title>The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia</article-title><source>Curr Opin Neurobiol</source><year>2017</year><volume>46</volume><fpage>241</fpage><lpage>247</lpage><pub-id pub-id-type="pmid">28985550</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsutsui-Kimura</surname><given-names>I</given-names></name><etal/></person-group><article-title>Distinct temporal difference error signals in dopamine axons in three regions of the striatum in a decision-making task</article-title><source>Elife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7771962</pub-id><pub-id pub-id-type="pmid">33345774</pub-id><pub-id pub-id-type="doi">10.7554/eLife.62390</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="other"><source>Appendix 1: IBL protocol for headbar implant surgery in mice</source><year>2020</year></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><article-title>Speed and accuracy of olfactory discrimination in the rat</article-title><source>Nature neuroscience</source><year>2003</year><volume>6</volume><fpage>1224</fpage><lpage>1229</lpage><pub-id pub-id-type="pmid">14566341</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><etal/></person-group><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in neuroinformatics</source><year>2015</year><volume>9</volume><fpage>7</fpage><pub-id pub-id-type="pmcid">PMC4389726</pub-id><pub-id pub-id-type="pmid">25904861</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Open Ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>Journal of neural engineering</source><year>2017</year><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="pmid">28169219</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>AJ</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title>An information-maximization approach to blind separation and blind deconvolution</article-title><source>Neural computation</source><year>1995</year><volume>7</volume><fpage>1129</fpage><lpage>1159</lpage><pub-id pub-id-type="pmid">7584893</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyvarinen</surname><given-names>A</given-names></name></person-group><article-title>Fast and robust fixed-point algorithms for independent component analysis</article-title><source>IEEE transactions on Neural Networks</source><year>1999</year><volume>10</volume><fpage>626</fpage><lpage>634</lpage><pub-id pub-id-type="pmid">18252563</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kvitsiani</surname><given-names>D</given-names></name><etal/></person-group><article-title>Distinct behavioural and network correlates of two interneuron types in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>498</volume><fpage>363</fpage><lpage>366</lpage></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>DN</given-names></name><name><surname>Mehta</surname><given-names>SB</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><article-title>Quality metrics to accompany spike sorting of extracellular signals</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><fpage>8699</fpage><lpage>8705</lpage><pub-id pub-id-type="pmcid">PMC3123734</pub-id><pub-id pub-id-type="pmid">21677152</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0971-11.2011</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ludvig</surname><given-names>EA</given-names></name><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Kehoe</surname><given-names>EJ</given-names></name></person-group><article-title>Stimulus representation and the timing of reward-prediction errors in models of the dopamine system</article-title><source>Neural computation</source><year>2008</year><volume>20</volume><fpage>3034</fpage><lpage>3054</lpage><pub-id pub-id-type="pmid">18624657</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rowland</surname><given-names>M</given-names></name><etal/></person-group><source>Statistics and samples in distributional reinforcement learning</source><conf-name>International Conference on Machine Learning</conf-name><year>2019</year><fpage>5528</fpage><lpage>5536</lpage></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Newey</surname><given-names>WK</given-names></name><name><surname>Powell</surname><given-names>JL</given-names></name></person-group><article-title>Asymmetric least squares estimation and testing</article-title><source>Econometrica: Journal of the Econometric Society</source><year>1987</year><fpage>819</fpage><lpage>847</lpage></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MC</given-names></name></person-group><article-title>Expectiles and M-quantiles are quantiles</article-title><source>Statistics &amp; Probability Letters</source><year>1994</year><volume>20</volume><fpage>149</fpage><lpage>153</lpage></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Nadal</surname><given-names>J-P</given-names></name></person-group><article-title>Mutual information, Fisher information, and population coding</article-title><source>Neural computation</source><year>1998</year><volume>10</volume><fpage>1731</fpage><lpage>1757</lpage><pub-id pub-id-type="pmid">9744895</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><source>Neuroeconomics: Decision making and the brain</source><publisher-name>Academic Press</publisher-name><year>2013</year></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tano</surname><given-names>P</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>A local temporal difference code for distributional reinforcement learning</article-title><source>Advances in neural information processing systems</source><year>2020</year><volume>33</volume><fpage>13662</fpage><lpage>13673</lpage></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Bukwich</surname><given-names>M</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Dopamine neurons share common response function for reward prediction error</article-title><source>Nature neuroscience</source><year>2016</year><volume>19</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="pmcid">PMC4767554</pub-id><pub-id pub-id-type="pmid">26854803</pub-id><pub-id pub-id-type="doi">10.1038/nn.4239</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Vong</surname><given-names>L</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>nature</source><year>2012</year><volume>482</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="pmcid">PMC3271183</pub-id><pub-id pub-id-type="pmid">22258508</pub-id><pub-id pub-id-type="doi">10.1038/nature10754</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>K</given-names></name><etal/></person-group><article-title>Temporally restricted dopaminergic control of reward-conditioned movements</article-title><source>Nature neuroscience</source><year>2020</year><volume>23</volume><fpage>209</fpage><lpage>216</lpage><pub-id pub-id-type="pmcid">PMC7007363</pub-id><pub-id pub-id-type="pmid">31932769</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0567-0</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stauffer</surname><given-names>WR</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Components and characteristics of the dopamine reward utility signal</article-title><source>Journal of Comparative Neurology</source><year>2016</year><volume>524</volume><fpage>1699</fpage><lpage>1711</lpage><pub-id pub-id-type="pmcid">PMC4753145</pub-id><pub-id pub-id-type="pmid">26272220</pub-id><pub-id pub-id-type="doi">10.1002/cne.23880</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Influence of reward delays on responses of dopamine neurons</article-title><source>Journal of neuroscience</source><year>2008</year><volume>28</volume><fpage>7837</fpage><lpage>7846</lpage><pub-id pub-id-type="pmcid">PMC3844811</pub-id><pub-id pub-id-type="pmid">18667616</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1600-08.2008</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yagle</surname><given-names>AE</given-names></name></person-group><article-title>Regularized matrix computations</article-title><source>matrix</source><year>2005</year><volume>500</volume><fpage>10</fpage></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chakravarti</surname><given-names>N</given-names></name></person-group><article-title>Isotonic median regression: a linear programming approach</article-title><source>Mathematics of operations research</source><year>1989</year><volume>14</volume><fpage>303</fpage><lpage>308</lpage></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Picheny</surname><given-names>V</given-names></name><name><surname>Moss</surname><given-names>H</given-names></name><name><surname>Torossian</surname><given-names>L</given-names></name><name><surname>Durrande</surname><given-names>N</given-names></name></person-group><source>Bayesian quantile and expectile optimisation in Uncertainty in Artificial Intelligence</source><year>2022</year><fpage>1623</fpage><lpage>1633</lpage></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kacelnik</surname><given-names>A</given-names></name><name><surname>Bateson</surname><given-names>M</given-names></name></person-group><article-title>Risky theories—the effects of variance on foraging decisions</article-title><source>American zoologist</source><year>1996</year><volume>36</volume><fpage>402</fpage><lpage>434</lpage></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>HE</given-names></name><etal/></person-group><article-title>Hypothalamic-extended amygdala circuit regulates temporal discounting</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><fpage>1928</fpage><lpage>1940</lpage><pub-id pub-id-type="pmcid">PMC7939087</pub-id><pub-id pub-id-type="pmid">33441435</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1836-20.2020</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Diversity in temporal discounting and relative scaling for positive and negative RPEs facilitates the construction of a distributional map of future reward in time and magnitude.</title><p><bold>(A)</bold> The green cue predicts a certain reward amount after a short delay, the orange a variable amount after a short delay and the purple a big amount after a long delay. In classic temporal-difference (TD) learning all units are computing the RPE relative to the same value, and hence at the beginning of the episode the population cannot distinguish between the three cues. <bold>Bottom left:</bold> The value at the cue for different reward magnitudes and times. Different combinations of reward magnitude and time map to one value. <bold>Bottom right:</bold> The temporally discounted value for the three different cues as a function of time since cue. <bold>(B)</bold> In value distributional TD learning, units asymmetrically scale positive and negative RPEs and hence learn a diverse set of values. We use the learnt values (expectiles) and the asymmetries to decode at the time of reward the distribution over reward magnitudes represented on the bottom. <bold>(C)</bold> A population with diversity in temporal discount factors allows for decoding the distribution over future rewards at the cue. <bold>Bottom left:</bold> multiple temporally discounted values, color coded by temporal discount factor. The orange, green and purple blocks represent the population responses at the three different cues. <bold>Bottom right:</bold> using the population responses at the cue, and having knowledge of the temporal discount factor of each unit, we decoded the future reward distribution over reward time for the three different cues. <bold>(D)</bold> Considering a population with diverse temporal discount factors and values allows for decoding the map of future reward in time and amount at the cue. <bold>Bottom left:</bold> Simulated values encoded at reward time as a function of values encoded at the cue corrected for the diversity in temporal discount factor. Points are color coded by value. <bold>Bottom right:</bold> We use the asymmetries for positive and negative RPEs, the temporal discount factors and the responses at the cue to decode the probability of reward over time and magnitude.</p></caption><graphic xlink:href="EMS190954-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Dopamine neurons are modulated by reward magnitude and time.</title><p><bold>(A)</bold> Variable time CSs, trial types 1-4, odor cues are sampled to produce a uniform distribution of reward times over trials, reward magnitude = 4.5 μl. <bold>(B)</bold> Variable magnitude CSs, trial type 5. 3s after CS onset, a reward amount sampled from a bimodal distribution is delivered. <bold>(C)</bold> Raster and mean peristimulus histogram (PSTH) aligned to odor onset for different reward times for two example neurons. The shaded area depicts the standard error of the mean. Black line: window used to compute responses (200-650ms). The inset depicts responses to the different delays, gray line: fitted discount function. <bold>(D)</bold> Raster and PSTH aligned to reward delivery for different reward magnitudes. The inset for the PSTH depicts the baseline subtracted responses for the five different reward amounts, blue line: fitted line for the negative responses, red line: fitted line for the positive responses. <bold>(E)</bold> Mean lick rate for all sessions and all animals (n=6) aligned to odor onset for different reward times, the shaded area depicts the standard error of the mean. <bold>(F)</bold> Mean population PSTH aligned to odor onset, the shaded area depicts the standard error of the mean. <bold>(G)</bold> Mean licking rate for all sessions and all animals (n=6) aligned to reward delivery for different reward magnitudes, the shaded area depicts the standard error of the mean. <bold>(H)</bold> Mean population PSTH aligned to reward delivery, the shaded area depicts the standard error of the mean.</p></caption><graphic xlink:href="EMS190954-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Dopamine neurons discount future reward heterogeneously, reflecting information about the timing of future rewards that correlates with licking behavior.</title><p><bold>(A)</bold> The dots are single neuron responses to CSs predicting different reward times, normalized by the responses at delay=0s and the lines are the fitted temporal discount functions, color code: temporal discount factor. The population mean temporal discount function is plotted in blue. <bold>(B)</bold> Temporal discounts estimated using two random disjoint partitions of the total number of trials. This procedure was repeated 10 000 times, and the mean correlation coefficient and the geometric mean of the p-values (two-tailed test) is depicted. In the inset, the histogram of the regression slopes for each run is depicted. <bold>(C)</bold> Cross validation of temporal discount factors of single neurons using 50% of the total number of trials per delay. The error bars: 99% confidence interval. Dashed blue line: population mean temporal discount factor, dashed gray line: temporal discount factor equal to one. One-way ANOVA for the difference in population temporal discounts F(42,252)=1498.02, p-value=1.96e-284. <bold>(D) Top:</bold> We use the responses at the cue (normalized by the gain) and the estimated temporal discount for each neuron to decode the distribution of future reward by inverting a linear regression. <bold>Middle:</bold> A representation of a possible decoded density over a discretized range of times. <bold>Bottom:</bold> decoded density using the dopamine population responses aligned to odor cue onsets predicting rewards after 0s, 1.5, 3s and 6s respectively. The light lines represent decoded densities using the responses of 70% of randomly selected trials and the dark lines represent the mean decoded density. <bold>(E) Top:</bold> Mean licking rate for all mice (n=6), for the trials in which the mice started licking earlier (red) or later (blue). The shaded area depicts the standard error of the mean and the horizontal black line the window used to compute the licking slopes. <bold>Bottom:</bold> Decoded density using the trials for which the mice started licking earlier (red) or later (blue). The dashed lines depict the maximum of the decoded reward time.</p></caption><graphic xlink:href="EMS190954-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Dopamine neurons reflect information about the distribution of future rewards at cue presentation.</title><p><bold>(A)</bold> Reversal points of individual neurons (calculated from the response to rewards of differing amount) as a function of the response to the cue associated with variable reward, corrected for the estimated discount function for each neuron (calculated from the response to cues associated with rewards of differing delay), color code: reversal point. Pearson correlation coefficient r=0.6, two-tailed test p-value=0.0002, 95% CI=(0.33,0.79). <bold>(B)</bold> The actual smoothed distribution over reward amounts predicted by the variable cue (black) and the density over reward amounts decoded from the DAN population response to reward (green) and the cue predicting variable reward amounts (yellow). The reversal points at the cue were estimated using the linear regression depicted in (A). <bold>(C)</bold> Decoded joint density of reward over magnitude and time, using the population responses at the variable and certain cues, and the single neuron estimated tuning functions for reward magnitude and time.</p></caption><graphic xlink:href="EMS190954-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Value and temporal sensitivity adapt to changes in reward statistics, in accordance with principles of efficient coding.</title><p><bold>(A)</bold> The distributional value code predicts that units have different asymmetries for positive <inline-formula><mml:math id="M78"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and negative <inline-formula><mml:math id="M79"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>α</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> RPEs, generating a set of values <italic>V<sub>i</sub></italic>, color code: reversal point. <bold>(B)</bold> The distributional value code predicts that variability in reversal points for the cue predicting a bimodal distribution is greater than for the cue that predicts a certain reward amount. <bold>(C)</bold> DAN responses to the certain reward amount delivered at a 3s delay, as a function of the reversal point estimated using the responses for the variable reward amounts delivered at a delay of 3s. Pearson correlation coefficient r=-0.75, two-tailed test p-value&lt;1e-5, 95% CI=(-0.87,-0.54). <bold>(D)</bold> Reversal points estimated at the cue predicting variable and certain reward magnitudes at the same delay. The reversals were estimated using the firing rate at the cue corrected for the diversity in temporal discounting and the linear regression depicted in <xref ref-type="fig" rid="F4">Figure 4A</xref>. The variance is depicted alongside double-ended arrows. Bootstrapping 10000 times to test if variance of reversal points for variable cue is greater than for certain cue: p-value=0.02, mean difference in the variances=1.71, 95% CI=(0.15,6.9). <bold>(E)</bold> Considering asymmetric weights for under and over-estimation of reward times generates a diversity of time constants that are mapped to temporal discount factors using the function depicted in the inset, color code: temporal discount factor. <bold>(F)</bold> Predicted adaptation in temporally discounted values when the reward time distribution is manipulated, by removing the shortest (green curves) or longest delay (magenta curves). The black curve depicts the smoothed distribution of reward times in the first phase of each experimental session. The inset depicts predicted adaptation in temporal discount factors. <bold>(G)</bold> Experimentally observed adaptation in temporal discount factor estimated from the recorded DANs. Responses to the same delays were used to estimate the temporal discounts before and after the manipulation. The inset depicts the histogram of the update in temporal discounts for the two different manipulations. The vertical lines depict the mean update. Bootstrapping 10,000 times testing if update in temporal discount when removing the shortest or longest delay is different: p-value=0.01, mean absolute difference=0.13, 95% CI=(0.008, 0.27). Bootstrapping 10,000 times testing if update when taking longest delay is negative: p=0.04, 95% CI=(-0.34,-0.0070). Bootstrapping 10,000 times testing if update when taking the shortest delay is positive: p-value=0.3, 95% CI=(-0.07,0.22). <bold>(H)</bold> Single neuron gains as a function of temporal discount factors, the dashed line represents the fitted linear regression. Pearson correlation coefficient r=-0.37, two-tailed test p-value=0.02.</p></caption><graphic xlink:href="EMS190954-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>A distributional code allows for planning and flexible adaptation to temporal dynamics and preferences of reward using a model-free RL algorithm.</title><p><bold>(A)</bold> A foraging mouse must decide which patch to choose to maximize cumulative collected rewards in a stationary environment. Axes indicate the learned joint probability distribution of reward time and magnitude associated with each patch. Rewards available from the two patches are stable during the day, with patch two providing a larger quantity of acorns than patch one. <bold>(B)</bold> In the non-stationary environment patch one provides fewer acorns early in the day. Patch two provides a variable (represented as competitors) number of acorns with the same mean as patch one early in the day. Patch three provides more acorns than either patch one or patch two, but later in the day. The reward variability in magnitude can arise due to the presence of competitors (represented as other mice). On the other hand, temporal dynamics can arise from environmental factors such as the foraging strategies of predators (represented as eagles, which preclude acorn availability). The standard TDRL and SR only keep track of a scalar value V (represented as vertical lines below the axes), whereas the TMRL keeps track of the future probability over reward time and magnitude. <bold>(C) Left:</bold> Cumulative rewards obtained when simulating the behavior of the three different algorithms in the stationary environment. <bold>Right:</bold> Cumulative rewards obtained when simulating behavior of the three algorithms in the non-stationary environment when fewer (early in learning) or greater (late in learning) numbers of time steps have elapsed. <bold>(D)</bold> In the agent making use of TMRL, the probability distribution over future reward time and magnitude is additionally weighted by a utility function to obtain an estimate that depends on internal state (see <xref ref-type="sec" rid="S9">Methods</xref> for a detailed description). The utility function when the mouse is hungry (center) or sated (right). <bold>(E)</bold> Probability of choosing patch two (left) and patch three (right) when the mouse becomes sated early or late in learning, color coded for the three different algorithms.</p></caption><graphic xlink:href="EMS190954-f006"/></fig></floats-group></article>