<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189894</article-id><article-id pub-id-type="doi">10.1101/2023.10.18.562860</article-id><article-id pub-id-type="archive">PPR744770</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dogs’ sensory-motor tuning shapes dog-human vocal interactions</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Déaux</surname><given-names>E. C.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Piette</surname><given-names>T.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Gaunet</surname><given-names>F.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Legou</surname><given-names>T.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Arnal</surname><given-names>L.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Giraud</surname><given-names>A-L.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Basic Neurosciences, Faculty of Medicine, University of Geneva, Geneva, Switzerland</aff><aff id="A2"><label>2</label>Aix-Marseille University and CNRS, Laboratoire de Psychologie Cognitive (UMR 7290), Fédération de recherche 3C, Marseille, France</aff><aff id="A3"><label>3</label>Aix Marseille University and CNRS, Laboratoire Parole et Langage (UMR 6057), 13 100, Aix-en-Provence, France</aff><aff id="A4"><label>4</label>Institut Pasteur, Université Paris Cité, Hearing Institute, Paris, France</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author: <email>eloise.deaux@unige.ch</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>21</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>19</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Within species, vocal and auditory systems co-evolve to converge on a critical temporal acoustic structure that can be best produced and perceived. While dogs cannot produce articulated sounds, they respond to speech, raising the question as to whether this heterospecific receptive ability is shaped by exposure to speech or bounded by their own sensorimotor capacity. Acoustic analyses of vocalisations show that dogs’ main production rhythm is slower than the dominant (syllabic) speech rate, and that human dog-directed speech falls halfway in between. Comparative exploration of neural (electroencephalography) and behavioural responses to speech reveals that comprehension in dogs relies on a slower speech rhythm tracking (delta) than humans’ (theta), even though dogs are equally sensitive to human speech content and prosody. Thus, the dog audio-motor tuning differs from humans’, who vocally adjust their speech rate to this shared temporal channel.</p></abstract></article-meta></front><body><p id="P2">Acoustic communication evolves through a dynamic closed-loop phenomenon in which auditory systems are tuned to vocal signals, while in turn vocal production adapts to exploit the capacity of sensory systems <sup><xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R4">4</xref></sup>. In this fine audio-vocal tuning, temporal acoustic features have a universal ecological relevance, being essential for e.g. vocal recognition <sup><xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref></sup> predator avoidance <sup><xref ref-type="bibr" rid="R7">7</xref></sup> or mate choice <sup><xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R10">10</xref></sup>.</p><p id="P3">Exploration of the speech system has provided capital insight into the neural bases of this perception/production tuning. Speech rhythms are mechanically constrained by the motor effectors, but also operated within a certain dynamic range to best match perception-action neural rhythms. Thus, the dominant speech rhythm, the syllable rate, is cross-culturally stable <sup><xref ref-type="bibr" rid="R11">11</xref></sup> because it both arises from the interplay of the different articulators <sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref></sup> and corresponds to neural theta oscillations, involved in active sensing across species <sup><xref ref-type="bibr" rid="R14">14</xref></sup>. In speech perception, the auditory theta rhythm serves to actively interface the acoustics with endogenous neural processes, and the closer the acoustics to this rhythm the more efficient the information transfer. Crucially, the neural theta rhythm can <italic>flexibly</italic> adapt to speech quasiperiodicity via a mechanism referred to as “speech tracking” <sup><xref ref-type="bibr" rid="R15">15</xref></sup>, and comprehension critically depends on its precision <sup><xref ref-type="bibr" rid="R16">16</xref>–<xref ref-type="bibr" rid="R21">21</xref></sup>. Thus speech production and reception tuning has led to a common temporal window of analysis centred on the 4-8Hz range <sup><xref ref-type="bibr" rid="R22">22</xref></sup>.</p><p id="P4">Among animals, dogs, <italic>Canis familiaris</italic>, are one of the most adept at responding to human signals, arguably because of their long co-evolutionary history with us, and are particularly receptive to auditory cues <sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup>. They can learn upwards of hundreds of words <sup><xref ref-type="bibr" rid="R25">25</xref></sup> and may exhibit fast mapping <sup><xref ref-type="bibr" rid="R26">26</xref></sup> and statistical learning abilities <sup><xref ref-type="bibr" rid="R27">27</xref></sup>. Behavioural data consistently show their sensitivity to speech temporal regularities <sup><xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref></sup>.Yet, dogs lack the vocal/neural system necessary for articulated communication <sup><xref ref-type="bibr" rid="R30">30</xref>–<xref ref-type="bibr" rid="R32">32</xref></sup>.</p><p id="P5">This thus begs the question as to how dogs and humans have adapted to each other’s production/perception constraints to produce successful vocal interactions. It may be that the dog neural system has adapted to human speech, or conversely that humans have adjusted their vocal production to exploit the dogs’ neural (auditory) capacity. Interestingly, like parents with infants, dog owners spontaneously use accented speech modulations, referred to as dog-directed speech <sup><xref ref-type="bibr" rid="R33">33</xref>–<xref ref-type="bibr" rid="R35">35</xref></sup>. Infant-directed speech also accentuates slow rhythm (i.e. delta band) over the syllabic rhythm (i.e. theta band)<sup><xref ref-type="bibr" rid="R36">36</xref></sup>, which has a positive influence on infant speech tracking <sup><xref ref-type="bibr" rid="R37">37</xref></sup>, although a similar analysis of dog-directed speech rhythm is lacking. To address these questions, we first analysed dog vocalisations, as well as adult- (ADS) and dog-directed speech (DDS), to probe whether dogs vocalise at the same or at a different rate than humans, and whether the temporal properties of DDS differ from those of ADS. Second, we compared speech neural processing in dogs and humans using non-invasive electroencephalography (EEG), to investigate whether dogs track speech modulations in sync with their own production system or syllabify speech as humans do. In other words, we explore whether dogs could have evolved a discordance between the capacity of their auditory and vocal systems through phylo- and/or onto-genetic exposure to speech.</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>Natural vocal rate in dogs and humans</title><p id="P6">Using 143 vocal sequences (30 dogs) including all major vocal classes (barks, growls, howls, snarls and whines <sup><xref ref-type="bibr" rid="R38">38</xref></sup>), 106 adult-directed (27 individuals, 10 women) and 149 dog-directed speech sequences (22 individuals, 16 women) spanning five different languages, we found that dogs vocalise at a slower rate than humans (Dogs mean ± SD: 2 ± 1.1 vocalisations/s, ADS: 4 ± 1.9 syllables/s; Tukey-corrected post-hoc pairwise comparison: t=6.8, p&lt;0.001, <xref ref-type="fig" rid="F1">Figure 1A and 1B</xref>). Further, DDS has a slower rate (3 ± 1.6Hz) than ADS (t=3.1, p=0.007), but faster rate than the average dog vocal rate (t=3.9, p=0.006). For a subset of individuals that produced DDS, we found duration-matched ADS sentences, confirming that pet owners slow their speech rate when talking to their dogs (paired t-test: t=2.7, df=11, p=0.02, Cohen’s d= 0.8, <xref ref-type="fig" rid="F1">Figure 1C</xref>). DDS also has higher mean F0 than ADS (t=-2.2, df=11, p=0.05, Cohen’s d= 0.6) confirming previous results <sup><xref ref-type="bibr" rid="R33">33</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Further analyses of vocal sequences find no significant differences in vocal rate among vocal classes in dogs (F<sub>4,16.1</sub>=1.4, p=0.28) nor among languages in ADS (F<sub>4,19.8</sub>=1.8, p=0.17) or DDS (F<sub>4,9.5</sub>=2, p=0.17, <xref ref-type="fig" rid="F1">Figure 1E</xref>). Thus, the dog’s vocal rate is overall slower than human speech and importantly, pet owners modify not only the spectral but also the temporal feature of their output when speaking to their dogs, in a direction that brings them closer to the natural vocal rate of the latter.</p><p id="P7">Furthermore, when exploring other factors known to influence the structure of animal vocal signals <sup><xref ref-type="bibr" rid="R2">2</xref></sup>, we found no evidence of large inter-individual differences in vocal rate unlike for the dominant acoustic frequency (<xref ref-type="supplementary-material" rid="SD1">table S1</xref>) confirming the latter’s functional significance in individual discrimination <sup><xref ref-type="bibr" rid="R39">39</xref>,<xref ref-type="bibr" rid="R40">40</xref></sup> and speaking against such selection effects in the former. Concurrently, body weight had no explanatory effect on vocal rate variation (F<sub>1,8.17</sub>=0.03, p=0.88) while it had a strong negative effect on dominant acoustic frequency (F<sub>1,12.07</sub>=6.03, p=0.03), again confirming the known acoustic allometric relationship between the latter two <sup><xref ref-type="bibr" rid="R41">41</xref></sup> and speaking to other types of constraints on vocal rate (<xref ref-type="fig" rid="F1">Figure 1D</xref>)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>.</p></sec><sec id="S3"><title>Neural tracking and speech ‘intelligibility’</title><p id="P8">To investigate auditory neural processes in dogs, we adapted typical human protocols e.g. <sup><xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>, where speech intelligibility is altered using spectral and temporal modifications of speech stimuli and neural tracking strength is correlated to behavioural measures of intelligibility (<xref ref-type="fig" rid="F2">Figure 2</xref>). Speech streams were composed of words that the dogs had learned to respond to, i.e. command words (e.g. “sit”, “come”) allowing us to use their behavioural responses as an index of ‘intelligibility’ in the sense of a successful stimulus-action relationship. In humans, comprehension was assessed by asking participants to rate word streams on an intelligibility scale. We performed EEG and behavioural experiments on 12 dogs (1-13 years old, seven females) and 12 paired human participants (18-65 years old, six women) with no reported hearing deficits. Four dogs and one human participant were excluded from analyses due to poor EEG signal quality.</p><p id="P9">We first confirmed that modifying speech spectral and temporal features altered both species’ perceptual performances (<xref ref-type="fig" rid="F3">Figure 3A</xref>). Modifying speech rate (main effect: F<sub>2,80</sub>=46.6, p&lt;0.001) and type (main effect: F<sub>2,80</sub>=112, p&lt;0.001) affected speech intelligibility in humans, in an interactive way (rate by type interaction: F<sub>4,80</sub>=10.5, p&lt;0.001). Increasing speech rate decreased speech intelligibility, with a stronger effect in the Normal speech and Content-only conditions. Concurrently, altering speech features had a strong impact on comprehension, particularly when content was removed i.e. the Prosody-only condition. In dogs, speech rate (main effect: F<sub>2,64</sub>=4.9, p=0.01) and type (main effect: F<sub>2,64</sub>=6.4, p=0.003) also impacted speech intelligibility, again interactively (rate by type interaction: F<sub>4,64</sub>=6.9; p&lt;0.001), with intelligibility dropping as speech rate increased, but only in the normal speech type condition.</p><p id="P10">We then quantified the two species’ neural responses, restricting the EEG analyses to the FCz electrode in humans and Cz in dogs, as they showed the strongest response to the acoustic stimuli (<xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>, <sup><xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref></sup>). Both dogs and humans showed increased power activity in the low frequency range (&lt;10Hz, <xref ref-type="fig" rid="F3">Figure 3B</xref>), confirming and characterising the auditory cortex activity reported in fMRI studies of dog speech processing <sup><xref ref-type="bibr" rid="R44">44</xref>–<xref ref-type="bibr" rid="R46">46</xref></sup>. However, we noted a first difference between the two species’ neural responses in this frequency range. Dogs showed a predominant power increase in the delta band (1-3 Hz), as opposed to the theta band (4-7Hz) in humans (unpaired t-test: t=-2.47, df=17, p=0.02, Cohen’s d=1.1, <xref ref-type="fig" rid="F3">Figure 3C</xref>), speaking to possible divergent auditory processes.</p><p id="P11">Given the presence of a stimulus-related and sustained neural response, we then established whether dogs display evidence of a speech tracking response under normal speech conditions. Cerebro-acoustic coherence, a measure that quantifies the phase-locking of neural signals to speech envelope <sup><xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>, was above the mean random coherence value throughout the 1-10 Hz range in humans, but restricted to a 1-3 Hz peak in dogs (<xref ref-type="fig" rid="F4">Figure 4A</xref>). Averaged coherence values in the delta band were significantly higher in the real pairings than the random ones in humans (paired t-test: t=-5, df=10, p&lt;0.001, d=1.5) and in dogs (t=-3, df=7, p=0.02, d=1.1). However, theta coherence was significantly higher in humans (t=-2.87, df=10, p=0.02, d=0.9) but not in dogs (t=-0.6, df=7, p=0.5, d=0.2). In other words, dogs show evidence of auditory tracking capabilities, as do other species <sup><xref ref-type="bibr" rid="R47">47</xref>–<xref ref-type="bibr" rid="R49">49</xref></sup>, however, in the context of speech stimulation, such tracking is restricted to the delta band (<xref ref-type="fig" rid="F4">Figure 4B</xref>).</p><p id="P12">Given that dogs and humans differ with regards to the primary frequency band that responds most strongly to speech stimuli (i.e. the delta band for dogs, and theta band for humans), we subsequently extracted the peak rhythm of each speech stimulus in these two bands, corresponding to the word and syllable rate, respectively, and the individual cerebro-acoustic coherence value for these specific rhythms (hereafter ‘word coherence’ and ‘syllable coherence’). We confirmed that increasing speech rate had a negative effect on cerebro-acoustic coherence in both species and at both granularity levels (<xref ref-type="supplementary-material" rid="SD1">fig. S2</xref>). In humans, both syllable and word coherence decreased as syllable rate (F<sub>1,90.5</sub>=9.2, p=0.003, <xref ref-type="supplementary-material" rid="SD1">fig. S2A</xref>) and word rate (F<sub>1,84</sub>=4.2, p=0.04, <xref ref-type="supplementary-material" rid="SD1">fig. S2B</xref>) increased respectively, but speech type had no effect in either model (Syllable model: F<sub>2,83</sub>=0.27, p=0.8; Word model: F<sub>2,83</sub>=0.5, p=0.6). The same pattern was found for dogs, with both syllable and word coherence dropping with increasing syllable rate (F<sub>1,65.5</sub>=5, p=0.03, <xref ref-type="supplementary-material" rid="SD1">fig. S2C</xref>) and word rate (F<sub>1,61</sub>=9.1, p=0.003, <xref ref-type="supplementary-material" rid="SD1">fig. S2D</xref>) respectively, while speech type (Syllable model: F<sub>2,59</sub>=1.5, p=0.2; Word model: F<sub>2,59</sub>=2.9, p=0.07) had no effect.</p><p id="P13">Remarkably however, the two species differed with regards to the granularity level at which tracking was most strongly related to behavioural outputs (<xref ref-type="fig" rid="F5">Fig. 5A</xref>). Specifically, in humans, word coherence did not explain intelligibility (F<sub>1,58.3</sub>=1.3, p=0.3) while increased syllable coherence led to increased intelligibility (F<sub>1,89.5</sub>= 5.5, p=0.02). Conversely, in dogs syllable coherence had no impact on speech intelligibility (F<sub>1,62</sub>=0.21, p=0.6), while intelligibility increased with increasing word coherence (F<sub>1,62</sub>= 4.69, p=0.03). Interestingly, in both species the speech type main effect remained (humans syllable model: F<sub>2,85</sub>=49.9, p&lt;0.001; dogs word model: F<sub>2, 61</sub>=3.2, p=0.05, <xref ref-type="fig" rid="F5">Figure 5B</xref>), with significant differences among the intercepts of all speech types in humans (all pairwise comparisons: p&lt;0.001) and higher intelligibility in the normal speech condition compared to the prosody-only condition (Norm. speech – Prosody-only est=0.1, df=69, t=2.4, p=0.05) in dogs (all other pairwise comparisons: p&gt;0.05). In other words, like humans, dogs’ comprehension of speech appears to involve more than stimulus-driven auditory processes <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R43">43</xref>,<xref ref-type="bibr" rid="R50">50</xref></sup>.</p></sec></sec><sec id="S4" sec-type="discussion"><title>Discussion</title><p id="P14">While humans’ speaking rate corresponds to the frequency range of theta neural oscillations, dogs’ vocal rate aligns instead with the delta oscillation range. Further, dogs’ natural vocalisation rate is conserved across vocalisation types, not influenced by body weight and shows only limited inter-individual differences, suggesting that selective pressures favoured the maintenance of this regime. This is consistent with what is known from the speech production system, namely that despite wide linguistic variations, speech rate shows remarkable consistency cross-culturally <sup><xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R51">51</xref></sup>. Thus, we propose that dogs, like humans, are subject to evolutionary factors that have kept production constrained within their species-specific ranges (<xref ref-type="fig" rid="F1">Figure 1</xref>), possibly due to inherent differences in the function(s) of their respective communication systems <sup><xref ref-type="bibr" rid="R4">4</xref></sup>. Interestingly, the theta vocal rhythm has been shown to be present in many primate species including both closely and more distantly-related ones <sup><xref ref-type="bibr" rid="R52">52</xref>–<xref ref-type="bibr" rid="R54">54</xref></sup> leading to hypotheses of an exaptation from masticatory movements <sup><xref ref-type="bibr" rid="R55">55</xref></sup>. Yet, despite being masticators, dogs do not vocalize in that range and instead exhibit a lower rate, suggesting that the theta rhythm evolved sometime after the split between the Laurasiatheria and Euarchontoglires and begging for a more thorough characterization of the phylogeny of the theta rhythm and of the possible selective forces behind its emergence.</p><p id="P15">Importantly, we found three key pieces of evidence that lend support to the view that receiver sensory and neural systems are a primary selective force driving signal design <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R57">57</xref></sup>. Dogs’ vocal rate is range-constrained in the delta band and during auditory perception of human speech, dogs primarily track slow amplitude modulations in speech that correspond to delta oscillations and only word-level predicts intelligibility (<xref ref-type="fig" rid="F4">Figure 4</xref> and <xref ref-type="fig" rid="F5">5</xref>). At this point, it should be noted that there is currently no data available on dogs’ processing of conspecific vocal sequences. Yet, it seems highly probable that dogs would similarly rely on delta tracking to process conspecifics signals given that cortical tracking is a basal processing mechanism <sup><xref ref-type="bibr" rid="R47">47</xref>,<xref ref-type="bibr" rid="R48">48</xref>,<xref ref-type="bibr" rid="R56">56</xref></sup> and that delta oscillations best match this species’ vocal temporal patterning as shown in this study. Furthermore, we found that rather than dogs syllabifying speech, humans modify their vocal output to approach dogs’ temporal channel. Thus, while the exact rhythm at which production systems align to reception systems is species-specific, such a process appears to also regulate adaptive heterospecific communication.</p><p id="P16">Further, whether when looking at the behavioural or neural level, we found that there existed differences relating to the type of speech broadcasted (<xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="fig" rid="F5">5</xref>). Indeed, contrary to popular beliefs, we found no evidence that dogs primarily rely on ‘prosody’ rather than ‘content’ to respond to human vocal cues. Instead, successful responses require the fully integrated signal (<xref ref-type="fig" rid="F3">Figure 3</xref>), confirming and extending previous results that used a preference-looking paradigm to investigate responses to ADS and DDS in dogs <sup><xref ref-type="bibr" rid="R58">58</xref></sup>. To borrow a term from the multi-modal communication literature, while in humans, prosody and content have a modulatory effect, for dogs we see an emergence effect of the composite signal <sup><xref ref-type="bibr" rid="R59">59</xref></sup>.</p><p id="P17">Finally, we found that speech tracking was not sufficient to fully explain intelligibility in both species, as differences in intercepts among speech types remained (<xref ref-type="fig" rid="F5">Figure 5</xref>). In humans, speech comprehension relies on both bottom-up and top-down processes distributed across a wide range of cortical regions <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R61">61</xref></sup>. Most notably, the former involves the hierarchical phase-amplitude coupling of theta and gamma frequency bands allowing for phoneme encoding <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup> while the later involving motor cortex responses <sup><xref ref-type="bibr" rid="R63">63</xref></sup> that are causally related to perception <sup><xref ref-type="bibr" rid="R64">64</xref></sup>. Arguably, both the need for and possibility to encode phonemes and the role of the motor cortex in predictive processes during auditory tasks likely mechanistically reflects within-species audio-motor tuning <sup><xref ref-type="bibr" rid="R22">22</xref></sup>, and as such need not be speech specific <sup><xref ref-type="bibr" rid="R65">65</xref>–<xref ref-type="bibr" rid="R67">67</xref></sup>. Yet, the dogs’ data also suggest that their processing of human speech entails more than just low-level auditory processing and while there is no data on dog neural coupling, a recent study mentioned differential activation in a cortical premotor region when comparing familiar vs unfamiliar language processing <sup><xref ref-type="bibr" rid="R50">50</xref></sup>. Thus, it will be greatly interesting for future research to investigate whether and to what extend hierarchical bottom-up and predictive processes initially linked to within-species acoustic processing can adapt to or constrain inter-species communication.</p><p id="P18">Overall these results reveal that dogs’ auditory and vocal systems have aligned on a single temporal processing window that differs from that of humans, and which remains predominant even when dogs process and appropriately respond to human speech. In parallel, we show that humans who speak to their dogs adopt a speech rhythm that differs from adult-directed speech and more closely aligns with the dog’s neural delta oscillatory capacity. Thus, in the history of the dog-human relationship, it appears that the neural constraints of the dogs’ reception system have limited this heterospecific communication to a temporal structure falling midway between the natural speech rate and a slower rate that would perfectly match the dog’s analysis capacity.</p></sec><sec id="S5" sec-type="methods"><title>Methods</title><sec id="S6"><title>Ethics statement</title><p id="P19">All the dogs used in this study were pet dogs who lived with their caregivers. As the tests took place in France, involved non-invasive EEG recordings and behavioural tests, no ethical approval was required under the French law. The human participants all provided informed consent prior to the experiments and the procedures were approved by the ethics commission of Geneva University CUREG.202011.18.</p></sec><sec id="S7" sec-type="subjects"><title>Subjects</title><sec id="S8"><title>Dogs</title><p id="P20">Dog owners were recruited by contacting canine clubs located in France. After initial contact with potential participants, dogs were recruited if they met the following inclusion criteria: being 1 year or older, having no hearing deficits, a good sociability level, high trainability and a good level of education. This recruitment process resulted in a pool of 12 dogs (seven females) aged 1-13 years old being included, all being medium to large dog breeds, the smallest being Shetland sheepdogs and the biggest being the Beauceron.</p></sec><sec id="S9"><title>Humans</title><p id="P21">We also recruited the same number of human participants (six women) from the clubs who served as paired controls. Inclusion criteria for human participants were: being aged 18-65 years old, having no hearing deficit, no psychiatric or motor disorders and speaking French fluently.</p></sec></sec><sec id="S10" sec-type="methods"><title>Procedure</title><sec id="S11"><title>Perception experiment</title><sec id="S12"><title>Pre-experiment dog training</title><p id="P22">We developed a training protocol using positive reinforcement and behavioural shaping, to condition dogs to wear the EEG equipment while remaining still. First, dogs were clicker-trained to lay down while resting their head. Once dogs could maintain this position for at least 15s, they were habituated to wear a headband (happy hoodie ®) normally used during toileting, to which we made holes to let the ears out (<xref ref-type="fig" rid="F2">Figure 2</xref>, photo inserts). Then they were finally conditioned to maintain the position, while wearing the headband and listening to a variety of noises including music, environmental noises and voices. Dogs were judged sufficiently experienced once they could maintain this position regardless of noise or other environmental disturbances for at least 15s.</p></sec><sec id="S13"><title>Acoustic stimuli</title><p id="P23">Typically, comprehension is assessed in humans by asking participants to rate word sequences on an intelligibility scale. As this was not possible for dogs, we selected words that the dogs had learned to respond to, i.e. command words (e.g. “sit”, “come”), allowing us to use behavioural responses to these words as an index of ‘comprehension’ in the sense of a successful stimulus-action relationship. For each dog, we recorded five command words spoken by their owners during a typical training session to obtain original, naturalistic DDS. The words were a mix of mono- and disyllabic words (<xref ref-type="fig" rid="F2">Figure 2</xref>). Each dog listened (EEG task) and responded (behavioural task) to their specific set of command words. Their matched control human participant also listened to the same stimuli. Recordings were made with a Sennheiser ME64 microphone and a K6 module mounted onto a FOSTEX FR-2LE field recorder in 44.1kHz-16 bit wav format. One exemplar of each command word was selected based on the sound quality and on whether that occurrence resulted in a clear, successful behavioural response. The selected command words were first high-pass filtered at 100hz, independently normalised at -2dB and then concatenated into one word stream with 300 ± 50ms silent intervals in between command words. We then used PRAAT and the VocalToolkit plug-in to construct the acoustic stimuli. In total we constructed nine word stream stimuli using a fully crossed design of the three levels of speech type and the three levels of speech rate (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p><p id="P24">In the <underline>Content-only</underline> condition, we first changed the pitch median of the original dog-directed word sequence to match that of the owner’s adult-directed speech pitch, and to remove all pitch modulations. In a second step, we altered the intensity component of prosody, by reversing the natural intensity contour, while keeping the speech forward. To create the <underline>Prosody-only</underline> condition, we reversed each individual word rendering the speech unintelligible, while keeping their order in the word stream. Because this process also reversed pitch modulation and intensity, we then copied the pitch and intensity modulation patterns from the original speech word stream, effectively reinstating the original prosody. Finally, given that these procedures resulted in undesired contingent effects, such as slightly robotized voice effects, we also created a control <underline>Normal speech</underline> condition by first making the pitch monotone and recopying the original pitch contour from the original recording. This ensured that these contingent effects were also present in the Normal word stream and thus controlled for.</p><p id="P25">To accelerate the speech rate, we used the ‘change tempo’ function in Audacity®, <ext-link ext-link-type="uri" xlink:href="https://audacityteam.org/">https://audacityteam.org/</ext-link>, which accelerates the rate without impacting the pitch. Each stream was compressed by a factor of 2 (twice as fast) and a factor of 4 (four times as fast). This resulted in a total of nine word streams to which we affixed the dog’s name in its original form, as a way of capturing the dog’s attention throughout the experimental session. We then created experimental tracks that included all 9 word streams repeated 40 times each, presented in a random order and separated by an inter-stimulus silent interval of 1.5 ± 0.5s. Experimental tracks lasted on average 23 min.</p></sec><sec id="S14"><title>Experimental location</title><p id="P26">All tests took place at the owner’s home whenever possible, or at another place familiar to the dog (such as another participant’s house belonging to the same canine club). This avoided having to familiarise animals with new locations and gave us more flexibility during the COVID-19 situation. Typically, this involved using the living room area of the house, with the dog being either positioned on a bedding or a couch, depending on its usual place during the pre-experiment training.</p></sec><sec id="S15"><title>EEG listening task</title><p id="P27">On the day of the EEG test, dogs were fitted with 1 to 4 golden cup electrodes (at least Cz and if possible, C3, C4 and POz) using gel and a conductive paste, and connected to a g.Nautilus amplifier (g.tec ©) secured on the back of the dog, which wirelessly transmitted data to a receiver connected to a recording DELL laptop. The reference electrode was placed at the nap of the neck (<xref ref-type="fig" rid="F2">Figure 2</xref>, photo inserts). Electrodes were then secured by the headband to prevent any movement during the experiment. Electrode impedance was kept under 30kΩ and data were recorded at a 500 Hz sampling rate. Dogs layed down facing a PREMIO 8 speaker (T.A.G Montpellier, France) placed 2m away. The experimental track was then broadcast at 60 ± 5dBC. The experiment was paused regularly to reward the dog for maintaining the position or when the dog became restless. On average the dog EEG listening task lasted 39.6 ± 15.8min.</p><p id="P28">Human recordings were made as similar as possible, using the same recording device and the same set-up. The only differences being that we used 7-8 gel-based g.SCARABEO (g.tec ©) active electrodes inserted in a cap and ear-referenced, and that the participants were asked to sit in a chair and instructed to avoid movements and blinking during the stimulus presentation. No breaks were given during the presentation.</p></sec></sec></sec><sec id="S16"><title>Behavioural task</title><sec id="S17"><title>Humans</title><p id="P29">Participants were asked before the EEG listening task, to score the linguistic material. For that, they were equipped with headphones and listened to each stimulus and were prompted to score on a scale of 0 to 5 how many words they understood. The word streams were randomly ordered but only presented once to avoid learning effects.</p></sec><sec id="S18"><title>Dogs</title><p id="P30">To obtain a comparable index for dogs, we used a playback experiment where dogs were made to listen to each command word separately (45 words in total) and scored on how well they responded to the command. To do so, we installed the speaker at the mouth level of the dog owner, who stood quietly next to it while wearing sunglasses and a face mask, holding their arms along their body or behind their back. This procedure ensured that the experiment was as realistic as possible while preventing dogs from using visual cues to answer the command. Prior to each command word, the dog was positioned in front of the speaker 1-2 away in a position that allowed it to display the appropriate behavioural response (e.g. standing up if the next command was a ‘sit’). Each command word was played a maximum of three times with 10s of silent interval in between. The first time the word command was played, it was preceded by the name of the dog, to grab her attention and replicate typical training settings. After the command word was played, we scored on a scale of 1 to 5 how accurately the dog responded to the command (<xref ref-type="supplementary-material" rid="SD1">Table S2</xref>). If the dog obtained a score of 4 or 5 (i.e. perfect response within the 10s scoring interval), she was rewarded with her usual treat, the playback series for that command was stopped and we moved on to the next command word series, again mimicking a typical training session. Scoring was performed by the experimenter and the dog owner. If the two disagreed, the lower response score was given. If the dog became restless and/or inattentive, the experiment was interrupted by a play and/or walk session and then resumed. On average the task lasted 53.5 +/-20.3min.</p></sec></sec><sec id="S19"><title>Production experiment</title><sec id="S20"><title>Dogs</title><p id="P31">We collected vocal sequences from youtube videos using the freely available Audio Set database <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. A total of 143 sequences (30 individuals) lasting &gt;1.5s were extracted, spanning the range of basic vocalisations in canids (barks, growls, whines, snarls and howls <sup><xref ref-type="bibr" rid="R38">38</xref></sup>). We categorised the dogs according to their body size as either small (a terrier-like dog or below) or large and to their age class (adult vs juvenile). Whenever available, we recorded the breed of the dog and obtained the corresponding mean breed weight using the American Kennel Club website (<ext-link ext-link-type="uri" xlink:href="https://www.akc.org/">https://www.akc.org/</ext-link>). For the Cane corso, data were unavailable on the AKC website, so we used the French equivalent, the Societé Centrale Canine website (<ext-link ext-link-type="uri" xlink:href="https://www.centrale-canine.fr">https://www.centrale-canine.fr</ext-link>). Finally, for those three individuals whose breed was known and who were pups, we first estimated the age (in months) of the pup from the video and then used the weight curve of the corresponding weight category provided in <sup><xref ref-type="bibr" rid="R69">69</xref></sup> to obtain the mean weight (50% centile) at that age.</p></sec></sec><sec id="S21"><title>Humans</title><p id="P32">To keep datasets as comparable as possible, we extracted ADS and DDS sequences from youtube videos (ADS: 106 sequences, 27 individuals, 10 women; DDS: 149 sequences, 22 individuals, 16 women). We selected speech sequences from five different languages: English, French, Italian, Japanese and Vietnamese to cover the range of stress-, syllable- and mora-timed speech patterns. DDS sentences included both typical praising and command sentences. For 12 individuals (nine women) that produced DDS sequences, we were able to match one DDS and one ADS exemplar (matched for duration), either extracted from the same video or by looking at other videos published by that user. For this analysis, we were not able to find matching ADS and DDS sequences in Vietnamese.</p></sec><sec id="S22"><title>Measurements</title><sec id="S23"><title>Perception experiment</title><sec id="S24"><title>Intelligibility index</title><p id="P33">For humans, the intelligibility score corresponded to the proportion of correctly comprehended words. To obtain a comparable index for dogs, we calculated the mean response score from the maximum behavioural score obtained in response to each command word of a given condition (<xref ref-type="supplementary-material" rid="SD1">Table S2</xref>), and then scaled this variable between 0 and 1.</p></sec><sec id="S25"><title>Audio signal</title><p id="P34">We computed the speech envelope (from the onset of the first command word) using the Hilbert transform, low-pass filtered below 30Hz using an 8<sup>th</sup> order butterworth filter, to extract, for each participant, the word and syllable rate for each of the nine word streams from the power spectrum of the envelope. These word and syllable rate variables were then z-scored and subsequently used as regressors in the statistical analyses investigating the relationships between neural, acoustic and behavioural data.</p></sec><sec id="S26"><title>EEG data</title><p id="P35">All EEG pre-processing steps were done in MATLAB using the fieldtrip toolbox <sup><xref ref-type="bibr" rid="R70">70</xref></sup> and custom-written scripts. EEG data were bandpass filtered between 1 and 70 Hz and a DFT filter was applied at 50, 100 and 150 Hz. Signals were then epoched from 1s pre-stimulus to the end of the word sequence. Human data were re-referenced to average and an independent component analysis (ICA) was used to remove eye blink data. ICA was not used on dog data, as for most subjects (8 out of 12), we only had the Cz recording electrode. Artefact rejection (eye blinks, muscle and jumps) was automatically done using fieldtrip functions, with species-specific cut-off z-values (more stringent for humans). A final visual inspection of all trials was used to remove any other trial that failed the rejection procedure. During these initial procedures, we had to exclude three dogs and one human participant due to poor signal quality, leaving nine dogs and 11 humans.</p><sec id="S27"><title>Electrode selection</title><p id="P36">For most dogs we could only analyse EEG data from Cz, as from the nine dogs remaining after the initial pre-processing steps, seven were equipped with only Cz. Thus, we decided to similarly restrict further data analyses to one electrode for humans. To select which electrode to keep we used a decoding approach, using the mTRF model <sup><xref ref-type="bibr" rid="R71">71</xref></sup>. Briefly, mTRF models use regularised linear regression to find the latent relationships between the stimulus features (in our case the speech envelope) and the neural response. We ran mTRF models for each subject and each electrode separately, restricting the shifting lag from 100ms pre-stimulus onset to 500ms post-stimulus onset. We then calculated the correlation between the reconstructed and the actual stimulus and saved the mTRF r value obtained as our measure of how well each electrode responded to the task. A linear-mixed model with electrode as a fixed effect and subject ID as a random term, followed by post-hoc analyses showed that FCz had significantly higher correlation value compared to the other electrodes, and was thus selected for further analyses (<xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>).</p></sec><sec id="S28"><title>Cerebro-acoustic coherence</title><p id="P37">To assess the extent of cortical phase-locking to the speech temporal structure, we used the cerebro-acoustic coherence index. Focusing on the control normal speech condition, we first obtained the cross-spectral density between neural signal and the speech envelope using a wavelet method between 1-20Hz in 0.1 Hz frequency steps and 0.01 ms time steps, from 0.6 s post stimulus onset to 1.3s. This time window was selected to exclude ERP components resulting from the first word of the sentence, which was always the dog’s name, and to allow keeping trial length equal across subjects. We then used the coherence function in fieldtrip to compute the phase coherence between the speech envelope and neural signal. To evaluate how well subjects tracked the speech signal we compared the actual coherence to random coherence values obtained from the pairings of neural data with randomised acoustic envelopes averaged over 100 runs. To further characterise neural tracking in the two most relevant auditory frequency bands, i.e. delta and theta bands, we extracted mean coherence values (delta: 1-3Hz; theta: 4-7Hz) in both real and random datasets and compared them using paired t-tests. Then, to explore how tracking was influenced by speech type and rate and how it related to behavioural data, we calculated, for each subject in each condition, the mean word and syllable coherence (time window: 0.6-1.3s post-stimulus onset, time steps: 0.01s, frequency steps: 0.5Hz) value centred around the subject-specific stimulus word and syllable rate (+/-0.5Hz).</p></sec></sec></sec></sec><sec id="S29"><title>Production experiment</title><sec id="S30"><title>Vocal rate and dominant acoustic frequency</title><p id="P38">Acoustic analyses were performed using the seewave package in R (Sueur et al. 2008). To extract the peak vocal rate, i.e. the predominant rhythm at which vocalisations in a sequence are produced, we first bandpass filtered the sequence between 0.1-10kHz and then computed the signal’s envelope using the Hilbert transform. This envelope was further low-pass filtered below 20Hz using a 4th order butterworth filter and a wavelet method was used to obtain the frequency decomposition of the signal and extract the frequency of the highest peak.</p><p id="P39">As a control analysis, we also extracted the dominant acoustic frequency of one vocalisation per sequence (selected based on its signal-to-noise ratio) for the dogs, and the sentence’s mean fundamental frequency (F0) for humans. For the dogs, the vocal unit was first bandpass filtered between 50Hz-2kHz, and the averaged frequency spectrum was then computed to extract the frequency of the peak amplitude. We focused on the dominant acoustic frequency rather than the fundamental frequency, because the latter is not always quantifiable, particularly in noisy and chaotic vocalisations such as barks. For humans, we used PRAAT (with standard settings) to extract the mean F0 in each sequence, as in human speech, F0 is both easy to compute and a better characterisation of pitch than dominant frequency.</p></sec><sec id="S31"><title>Potential for individual coding (PIC)</title><p id="P40">To assess potential individual-related variations in the acoustic parameters, we calculated the within- and between-individual coefficients of variation (CVw and CVb respectively) using the formula for small samples (Sokal and Rohlf 1995) and obtained the feature’s PIC value, which is the CVb/meanCVw ratio, where meanCVw is the mean value of the CVw for all individuals <sup><xref ref-type="bibr" rid="R72">72</xref></sup>.</p></sec></sec><sec id="S32"><title>Statistical analyses</title><sec id="S33"><title>Behavioural and EEG data</title><p id="P41">To investigate how the experimental conditions influenced neural and behavioural responses, we used Linear-Mixed Models (LMMs). Models always included participant ID as a random term. Fixed effects varied depending on the question being addressed and were always first specified as the full model, then interaction terms were dropped if they did not reach significance. Statistical significance of fixed effects was assessed using F tests and the Kenward-Roger method of degrees-of-freedom approximation, as it has been shown to be a reliable method when LMMs are balanced <sup><xref ref-type="bibr" rid="R73">73</xref></sup>. Post-hoc pairwise comparisons were Tukey corrected. Visual inspection of plots showed that the normality and homoscedasticity of residuals and random effects assumptions were met in all cases. For full reporting of these models and all other statistical analyses, refer to the sup. mat. Rmd document. Unpaired or paired t-tests were used in comparisons when they were warranted.</p></sec><sec id="S34"><title>Vocal production data</title><p id="P42">We tested for differences in vocal rates between dogs and humans, using a LMM with species as a fixed effect and subjects within call/language types as random effects. We used LMMs to assess whether the acoustic measurements varied with call/language type adding weight class and subject ID as random terms in dogs, while in humans, the random effects were subject ID and sex. Finally, to assess whether acoustic parameters were allometrically related to body weight in dogs, we first log-transformed the variables and then used an LMM with vocal class and subject ID as random terms. Unpaired or paired t-tests were used in comparisons where they were warranted.</p></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>supplementary files</label><media xlink:href="EMS189894-supplement-supplementary_files.pdf" mimetype="application" mime-subtype="pdf" id="d15aAdHbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S35"><title>Acknowledgements</title><p>We are thankful to Silvia Marchesotti and Johanna Nicolle for their help in the initial piloting stage of this work. This research received funding from the NCCR Evolving Language, Swiss National Science Foundation Agreement #51NF40_180888.</p></ack><sec id="S36" sec-type="data-availability"><title>Data Availability</title><p id="P43">Data and codes are available here: figshare DOI (DOI number will be made available at the time of publication)</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P44"><bold>Authors Contributions:</bold></p><p id="P45"><italic>Experimental design</italic>: ED, FG &amp; ALG, <italic>Experimental setup:</italic> ED &amp; TL, <italic>Data collection and experiments:</italic> ED &amp; TP, <italic>Data analysis:</italic> EC, LA, TP &amp; ALG, <italic>Manuscript preparation:</italic> ED, LA &amp; ALG, <italic>Manuscript revision: all co-authors</italic>.</p></fn><fn fn-type="conflict" id="FN2"><p id="P46"><bold>Competing Interests:</bold></p><p id="P47">The authors declare no conflicts of interest.</p></fn><fn id="FN3"><p id="P48"><bold>Materials &amp; correspondance:</bold></p><p id="P49">All requests should be sent to E. Déaux.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charlton</surname><given-names>BD</given-names></name><name><surname>Reby</surname><given-names>D</given-names></name></person-group><article-title>The evolution of acoustic size exaggeration in terrestrial mammals</article-title><source>Nature communications</source><year>2016</year><volume>7</volume><elocation-id>12739</elocation-id><pub-id pub-id-type="pmcid">PMC5025854</pub-id><pub-id pub-id-type="pmid">27598835</pub-id><pub-id pub-id-type="doi">10.1038/ncomms12739</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname><given-names>AM</given-names></name><name><surname>Reby</surname><given-names>D</given-names></name></person-group><article-title>The contribution of source–filter theory to mammal vocal communication research</article-title><source>Journal of Zoology</source><year>2010</year><volume>280</volume><fpage>221</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1111/j.1469-7998.2009.00661.x</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>MJ</given-names></name><name><surname>Fox</surname><given-names>JH</given-names></name><name><surname>Wilczynski</surname><given-names>W</given-names></name><name><surname>Rand</surname><given-names>AS</given-names></name></person-group><article-title>Sexual selection for sensory exploitation in the frog <italic>Physalaemus pustulosus</italic></article-title><source>Nature</source><year>1990</year><volume>343</volume><fpage>66</fpage><lpage>67</lpage><pub-id pub-id-type="pmid">2296291</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Kleinschmidt</surname><given-names>A</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Human screams occupy a privileged niche in the communication soundscape</article-title><source>Curr Biol</source><year>2015</year><volume>25</volume><fpage>2051</fpage><lpage>2056</lpage><pub-id pub-id-type="pmcid">PMC4562283</pub-id><pub-id pub-id-type="pmid">26190070</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2015.06.043</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoy</surname><given-names>RR</given-names></name><name><surname>Pollack</surname><given-names>GS</given-names></name><name><surname>Moiseff</surname><given-names>A</given-names></name></person-group><article-title>Species-Recognition in the Field Cricket, <italic>Teleogryllus oceanicus</italic>: Behavioral and Neural Mechanisms</article-title><source>American Zoologist</source><year>2015</year><volume>22</volume><fpage>597</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1093/icb/22.3.597</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Smith-Rohrberg</surname><given-names>D</given-names></name><name><surname>Hauser</surname><given-names>MD</given-names></name></person-group><article-title>The role of temporal cues in rhesus monkey vocal recognition: Orienting asymmetries to reversed calls</article-title><source>Brain, Behavior and Evolution</source><year>2001</year><volume>58</volume><fpage>163</fpage><lpage>172</lpage><pub-id pub-id-type="pmid">11910173</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumstein</surname><given-names>DT</given-names></name><name><surname>Armitage</surname><given-names>KB</given-names></name></person-group><article-title>Alarm calling in yellow-bellied marmots: I. The meaning of situationally variable alarm calls</article-title><source>Animal Behaviour</source><year>1997</year><volume>53</volume><fpage>143</fpage><lpage>171</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draganoiu</surname><given-names>TI</given-names></name><name><surname>Nagle</surname><given-names>L</given-names></name><name><surname>Kreutzer</surname><given-names>M</given-names></name></person-group><article-title>Directional female preference for an exaggerated male trait in canary (Serinus canaria) song</article-title><source>Proceedings of the Royal Society of London Series B: Biological Sciences</source><year>2002</year><volume>269</volume><fpage>2525</fpage><lpage>2531</lpage><pub-id pub-id-type="pmcid">PMC1691196</pub-id><pub-id pub-id-type="pmid">12573066</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2002.2192</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galeotti</surname><given-names>P</given-names></name><name><surname>Sacchi</surname><given-names>R</given-names></name><name><surname>Rosa</surname><given-names>DP</given-names></name><name><surname>Fasola</surname><given-names>M</given-names></name></person-group><article-title>Female preference for fast-rate, high-pitched calls in Hermann’s tortoises <italic>Testudo hermanni</italic></article-title><source>Behavioral Ecology</source><year>2005</year><volume>16</volume><fpage>301</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1093/beheco/arh165</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pauly</surname><given-names>GB</given-names></name><name><surname>Bernal</surname><given-names>XE</given-names></name><name><surname>Rand</surname><given-names>AS</given-names></name><name><surname>Ryan</surname><given-names>MJ</given-names></name></person-group><article-title>The vocal sac increases call rate in the Túngara frog <italic>Physalaemus pustulosus</italic></article-title><source>Physiological and Biochemical Zoology</source><year>2006</year><volume>79</volume><fpage>708</fpage><lpage>719</lpage><pub-id pub-id-type="pmid">16826497</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coupé</surname><given-names>C</given-names></name><name><surname>Oh</surname><given-names>YM</given-names></name><name><surname>Dediu</surname><given-names>D</given-names></name><name><surname>Pellegrino</surname><given-names>F</given-names></name></person-group><article-title>Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</article-title><source>Science Advances</source><year>2019</year><volume>5</volume><elocation-id>eaaw2594</elocation-id><pub-id pub-id-type="pmcid">PMC6984970</pub-id><pub-id pub-id-type="pmid">32047854</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aaw2594</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name></person-group><article-title>Speech rhythms and their neural foundations</article-title><source>Nature Reviews Neuroscience</source><year>2020</year><volume>21</volume><fpage>322</fpage><lpage>334</lpage><pub-id pub-id-type="pmid">32376899</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacNeilage</surname><given-names>PF</given-names></name><name><surname>Davis</surname><given-names>BL</given-names></name><name><surname>Kinney</surname><given-names>A</given-names></name><name><surname>Matyear</surname><given-names>CL</given-names></name></person-group><article-title>The motor core of speech: A comparison of serial organization patterns in infants and languages</article-title><source>Child development</source><year>2000</year><volume>71</volume><fpage>153</fpage><lpage>163</lpage><pub-id pub-id-type="pmid">10836569</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Radman</surname><given-names>T</given-names></name><name><surname>Scharfman</surname><given-names>H</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><article-title>Dynamics of Active Sensing and perceptual selection</article-title><source>Current Opinion in Neurobiology</source><year>2010</year><volume>20</volume><fpage>172</fpage><lpage>176</lpage><pub-id pub-id-type="pmcid">PMC2963579</pub-id><pub-id pub-id-type="pmid">20307966</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hyafil</surname><given-names>A</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Kabdebon</surname><given-names>C</given-names></name><name><surname>Gutkin</surname><given-names>B</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Speech encoding by coupled cortical theta and gamma oscillations</article-title><source>Elife</source><year>2015</year><volume>4</volume><elocation-id>e06213</elocation-id><pub-id pub-id-type="pmcid">PMC4480273</pub-id><pub-id pub-id-type="pmid">26023831</pub-id><pub-id pub-id-type="doi">10.7554/eLife.06213</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahissar</surname><given-names>E</given-names></name><etal/></person-group><article-title>Speech comprehension is correlated with temporal response patterns recorded from auditory cortex</article-title><source>Proceedings of the National Academy of Sciences</source><year>2001</year><volume>98</volume><fpage>13367</fpage><lpage>13372</lpage><pub-id pub-id-type="pmcid">PMC60877</pub-id><pub-id pub-id-type="pmid">11698688</pub-id><pub-id pub-id-type="doi">10.1073/pnas.201400998</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><year>2007</year><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="pmcid">PMC2703451</pub-id><pub-id pub-id-type="pmid">17582338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title><source>Cereb Cortex</source><year>2013</year><volume>23</volume><fpage>1378</fpage><lpage>1387</lpage><pub-id pub-id-type="pmcid">PMC3643716</pub-id><pub-id pub-id-type="pmid">22610394</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs118</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>Neuroimage</source><year>2014</year><volume>85</volume><issue>Pt 2</issue><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="pmcid">PMC3839250</pub-id><pub-id pub-id-type="pmid">23791839</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pefkou</surname><given-names>M</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><article-title>theta-band and beta-band neural activity reflects independent syllable tracking and comprehension of time-compressed speech</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>7930</fpage><lpage>7938</lpage><pub-id pub-id-type="pmcid">PMC6596908</pub-id><pub-id pub-id-type="pmid">28729443</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2882-16.2017</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name></person-group><article-title>Adaptive oscillators provide a hard-coded Bayesian mechanism for rhythmic inference</article-title><source>bioRxiv</source><year>2022</year><elocation-id>2022.2006.2018.496664</elocation-id><pub-id pub-id-type="doi">10.1101/2022.06.18.496664</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nat Neurosci</source><year>2012</year><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="pmcid">PMC4461038</pub-id><pub-id pub-id-type="pmid">22426255</pub-id><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hare</surname><given-names>B</given-names></name><name><surname>Tomasello</surname><given-names>M</given-names></name></person-group><article-title>Human-like social skills in dogs?</article-title><source>Trends in Cognitive Sciences</source><year>2005</year><volume>9</volume><fpage>439</fpage><lpage>444</lpage><pub-id pub-id-type="pmid">16061417</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Gacsi</surname><given-names>M</given-names></name><name><surname>Farago</surname><given-names>T</given-names></name><name><surname>Kis</surname><given-names>A</given-names></name><name><surname>Miklosi</surname><given-names>A</given-names></name></person-group><article-title>Voice-sensitive regions in the dog and human brain are revealed by comparative fMRI</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>574</fpage><lpage>578</lpage><pub-id pub-id-type="pmid">24560578</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pilley</surname><given-names>JW</given-names></name><name><surname>Reid</surname><given-names>AK</given-names></name></person-group><article-title>Border collie comprehends object names as verbal referents</article-title><source>Behav Processes</source><year>2011</year><volume>86</volume><fpage>184</fpage><lpage>195</lpage><pub-id pub-id-type="pmid">21145379</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaminski</surname><given-names>J</given-names></name><name><surname>Call</surname><given-names>J</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><article-title>Word learning in a domestic dog: evidence for fast mapping</article-title><source>Science</source><year>2004</year><volume>304</volume><fpage>1682</fpage><lpage>1683</lpage><pub-id pub-id-type="pmid">15192233</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boros</surname><given-names>M</given-names></name><etal/></person-group><article-title>Neural processes underlying statistical learning for speech segmentation in dogs</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><fpage>5512</fpage><lpage>5521</lpage><elocation-id>e5515</elocation-id><pub-id pub-id-type="pmcid">PMC7612233</pub-id><pub-id pub-id-type="pmid">34717832</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.10.017</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliffe</surname><given-names>VF</given-names></name><name><surname>Reby</surname><given-names>D</given-names></name></person-group><article-title>Orienting asymmetries in dogs’ responses to different communicatory components of human speech</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>2908</fpage><lpage>2912</lpage><pub-id pub-id-type="pmid">25454584</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fukuzawa</surname><given-names>M</given-names></name><name><surname>Mills</surname><given-names>DS</given-names></name><name><surname>Cooper</surname><given-names>JJ</given-names></name></person-group><article-title>The effect of human command phonetic characteristics on auditory cognition in dogs (Canis familiaris</article-title><source>J Comp Psychol</source><year>2005</year><volume>119</volume><fpage>117</fpage><lpage>120</lpage><pub-id pub-id-type="pmid">15740436</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitch</surname><given-names>T</given-names></name></person-group><article-title>The phonetic potential of nonhuman vocal tracts: Comparative cineradiographic observations of vocalizing animals</article-title><source>Phonetica</source><year>2000</year><volume>57</volume><fpage>205</fpage><lpage>218</lpage><pub-id pub-id-type="pmid">10992141</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lieberman</surname><given-names>P</given-names></name></person-group><article-title>Vocal tract anatomy and the neural bases of talking</article-title><source>Journal of Phonetics</source><year>2012</year><volume>40</volume><fpage>608</fpage><lpage>622</lpage></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boë</surname><given-names>L-J</given-names></name><etal/></person-group><article-title>Which way to the dawn of speech?: Reanalyzing half a century of debates and data in light of speech science</article-title><source>Science Advances</source><year>2019</year><volume>5</volume><elocation-id>eaaw3916</elocation-id><pub-id pub-id-type="pmcid">PMC7000245</pub-id><pub-id pub-id-type="pmid">32076631</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aaw3916</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnham</surname><given-names>D</given-names></name><name><surname>Kitamura</surname><given-names>C</given-names></name><name><surname>Vollmer-Conna</surname><given-names>U</given-names></name></person-group><article-title>What’s new, pussycat? On talking to babies and animals</article-title><source>Science</source><year>2002</year><volume>296</volume><fpage>1435</fpage><pub-id pub-id-type="pmid">12029126</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirsh-Pasek</surname><given-names>K</given-names></name><name><surname>Treiman</surname><given-names>R</given-names></name></person-group><article-title>Doggerel: Motherese in a new context</article-title><source>Journal of Child Language</source><year>1982</year><volume>9</volume><fpage>229</fpage><lpage>237</lpage><pub-id pub-id-type="pmid">7061632</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Aderet</surname><given-names>T</given-names></name><name><surname>Gallego-Abenza</surname><given-names>M</given-names></name><name><surname>Reby</surname><given-names>D</given-names></name><name><surname>Mathevon</surname><given-names>N</given-names></name></person-group><article-title>Dog-directed speech: why do we use it and do dogs pay attention to it?</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>2017</year><volume>284</volume><elocation-id>20162429</elocation-id><pub-id pub-id-type="pmcid">PMC5247504</pub-id><pub-id pub-id-type="pmid">28077769</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2016.2429</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>V</given-names></name><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name><name><surname>Goswami</surname><given-names>U</given-names></name></person-group><article-title>The temporal modulation structure of infant-directed speech</article-title><source>Open Mind</source><year>2017</year><volume>1</volume><fpage>78</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1162/OPMI_a_00008</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalashnikova</surname><given-names>M</given-names></name><name><surname>Peter</surname><given-names>V</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Burnham</surname><given-names>D</given-names></name></person-group><article-title>Infant-directed speech facilitates seven-month-old infants’ cortical tracking of speech</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><elocation-id>13745</elocation-id><pub-id pub-id-type="pmcid">PMC6137049</pub-id><pub-id pub-id-type="pmid">30214000</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-32150-6</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name><name><surname>Fox</surname><given-names>M</given-names></name></person-group><article-title>Vocalizations in wild canids and possible effects of domestication</article-title><source>Behavioural Processes</source><year>1976</year><volume>1</volume><fpage>77</fpage><lpage>92</lpage><pub-id pub-id-type="pmid">24923546</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yin</surname><given-names>S</given-names></name><name><surname>McCowan</surname><given-names>B</given-names></name></person-group><article-title>Barking in domestic dogs: context specificity and individual identification</article-title><source>Animal behaviour</source><year>2004</year><volume>68</volume><fpage>343</fpage><lpage>355</lpage></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Molnár</surname><given-names>C</given-names></name><name><surname>Pongrácz</surname><given-names>P</given-names></name><name><surname>Faragó</surname><given-names>T</given-names></name><name><surname>Dóka</surname><given-names>A</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name></person-group><article-title>Dogs discriminate between barks: The effect of context and identity of the caller</article-title><source>Behavioural Processes</source><year>2009</year><volume>82</volume><fpage>198</fpage><lpage>201</lpage><pub-id pub-id-type="pmid">19596426</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bowling</surname><given-names>D</given-names></name><etal/></person-group><article-title>Body size and vocalization in primates and carnivores</article-title><source>Scientific reports</source><year>2017</year><volume>7</volume><elocation-id>41070</elocation-id><pub-id pub-id-type="pmcid">PMC5259760</pub-id><pub-id pub-id-type="pmid">28117380</pub-id><pub-id pub-id-type="doi">10.1038/srep41070</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howell</surname><given-names>TJ</given-names></name><name><surname>Conduit</surname><given-names>R</given-names></name><name><surname>Toukhsati</surname><given-names>S</given-names></name><name><surname>Bennett</surname><given-names>P</given-names></name></person-group><article-title>Auditory stimulus discrimination recorded in dogs, as indicated by mismatch negativity (MMN)</article-title><source>Behav Processes</source><year>2012</year><volume>89</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">22001730</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magyari</surname><given-names>L</given-names></name><name><surname>Huszár</surname><given-names>Z</given-names></name><name><surname>Turzó</surname><given-names>A</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name></person-group><article-title>Event-related potentials reveal limited readiness to access phonetic details during word processing in dogs</article-title><source>Royal Society Open Science</source><year>2020</year><volume>7</volume><elocation-id>200851</elocation-id><pub-id pub-id-type="pmcid">PMC7813267</pub-id><pub-id pub-id-type="pmid">33489257</pub-id><pub-id pub-id-type="doi">10.1098/rsos.200851</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andics</surname><given-names>A</given-names></name><etal/></person-group><article-title>Neural mechanisms for lexical processing in dogs</article-title><source>Science</source><year>2016</year><volume>353</volume><fpage>1030</fpage><lpage>1032</lpage><pub-id pub-id-type="pmid">27576923</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gábor</surname><given-names>A</given-names></name><etal/></person-group><article-title>Multilevel fMRI adaptation for spoken word processing in the awake dog brain</article-title><source>Scientific Reports</source><year>2020</year><volume>10</volume><elocation-id>11968</elocation-id><pub-id pub-id-type="pmcid">PMC7398925</pub-id><pub-id pub-id-type="pmid">32747731</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-68821-6</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bálint</surname><given-names>A</given-names></name><name><surname>Szabó</surname><given-names>Á</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name><name><surname>Gácsi</surname><given-names>M</given-names></name></person-group><article-title>Dog and human neural sensitivity to voicelikeness: A comparative fMRI study</article-title><source>NeuroImage</source><year>2023</year><volume>265</volume><elocation-id>119791</elocation-id><pub-id pub-id-type="pmid">36476565</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name></person-group><article-title>A new unifying account of the roles of neuronal entrainment</article-title><source>Current Biology</source><year>2019</year><volume>29</volume><fpage>R890</fpage><lpage>R905</lpage><pub-id pub-id-type="pmcid">PMC6769420</pub-id><pub-id pub-id-type="pmid">31550478</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2019.07.075</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boari</surname><given-names>S</given-names></name><name><surname>Mindlin</surname><given-names>GB</given-names></name><name><surname>Amador</surname><given-names>A</given-names></name></person-group><article-title>Neural oscillations are locked to birdsong rhythms in canaries</article-title><source>European Journal of Neuroscience</source><year>2022</year><volume>55</volume><fpage>549</fpage><lpage>565</lpage><pub-id pub-id-type="pmid">34852183</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Shaevitz</surname><given-names>SS</given-names></name></person-group><article-title>Auditory processing of vocal sounds in birds</article-title><source>Current opinion in neurobiology</source><year>2006</year><volume>16</volume><fpage>400</fpage><lpage>407</lpage><pub-id pub-id-type="pmid">16842993</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuaya</surname><given-names>LV</given-names></name><name><surname>Hernández-Pérez</surname><given-names>R</given-names></name><name><surname>Boros</surname><given-names>M</given-names></name><name><surname>Deme</surname><given-names>A</given-names></name><name><surname>Andics</surname><given-names>A</given-names></name></person-group><article-title>Speech naturalness detection and language representation in the dog brain</article-title><source>NeuroImage</source><year>2022</year><volume>248</volume><elocation-id>118811</elocation-id><pub-id pub-id-type="pmid">34906714</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrino</surname><given-names>F</given-names></name><name><surname>Coupé</surname><given-names>C</given-names></name><name><surname>Marsico</surname><given-names>E</given-names></name></person-group><article-title>A cross-language perspective on speech information rate</article-title><source>Language</source><year>2011</year><fpage>539</fpage><lpage>558</lpage></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Paukner</surname><given-names>A</given-names></name><name><surname>Ferrari</surname><given-names>PF</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><article-title>Monkey lipsmacking develops like the human speech rhythm</article-title><source>Developmental Science</source><year>2012</year><volume>15</volume><fpage>557</fpage><lpage>568</lpage><pub-id pub-id-type="pmcid">PMC3383808</pub-id><pub-id pub-id-type="pmid">22709404</pub-id><pub-id pub-id-type="doi">10.1111/j.1467-7687.2012.01149.x</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Risueno-Segovia</surname><given-names>C</given-names></name><name><surname>Hage</surname><given-names>SR</given-names></name></person-group><article-title>Theta Synchronization of Phonatory and Articulatory Systems in Marmoset Monkey Vocal Production</article-title><source>Current Biology</source><year>2020</year><pub-id pub-id-type="pmid">32888481</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>AS</given-names></name><name><surname>Kavanagh</surname><given-names>E</given-names></name><name><surname>Hobaiter</surname><given-names>C</given-names></name><name><surname>Slocombe</surname><given-names>KE</given-names></name><name><surname>Lameira</surname><given-names>AR</given-names></name></person-group><article-title>Chimpanzee lip-smacks confirm primate continuity for speech-rhythm evolution</article-title><source>Biology Letters</source><year>2020</year><volume>16</volume><elocation-id>20200232</elocation-id><pub-id pub-id-type="pmcid">PMC7280036</pub-id><pub-id pub-id-type="pmid">32453963</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2020.0232</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacNeilage</surname><given-names>PF</given-names></name></person-group><article-title>The frame/content theory of evolution of speech production</article-title><source>Behavioral and Brain Sciences</source><year>1998</year><volume>21</volume><fpage>499</fpage><lpage>511</lpage><pub-id pub-id-type="pmid">10097020</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical entrainment to music and its modulation by expertise</article-title><source>Proc Natl Acad Sci U S A</source><year>2015</year><volume>112</volume><fpage>E6233</fpage><lpage>6242</lpage><pub-id pub-id-type="pmcid">PMC4653203</pub-id><pub-id pub-id-type="pmid">26504238</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1508431112</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><article-title>The theta-syllable: a unit of speech information defined by cortical function</article-title><source>Frontiers in Psychology</source><year>2013</year><volume>4</volume><pub-id pub-id-type="pmcid">PMC3602725</pub-id><pub-id pub-id-type="pmid">23519170</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00138</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>A</given-names></name><name><surname>Slocombe</surname><given-names>K</given-names></name></person-group><article-title>Who’s a good boy?! Dogs prefer naturalistic dog-directed speech</article-title><source>Animal Cognition</source><year>2018</year><volume>21</volume><fpage>353</fpage><lpage>364</lpage><pub-id pub-id-type="pmcid">PMC5908831</pub-id><pub-id pub-id-type="pmid">29500713</pub-id><pub-id pub-id-type="doi">10.1007/s10071-018-1172-4</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Partan</surname><given-names>SR</given-names></name><name><surname>Marler</surname><given-names>P</given-names></name></person-group><article-title>Issues in the classification of multimodal communication signals</article-title><source>The American Naturalist</source><year>2005</year><volume>166</volume><fpage>231</fpage><lpage>245</lpage><pub-id pub-id-type="pmid">16032576</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name></person-group><article-title>Linking Speech Perception and Neurophysiology: Speech Decoding Guided by Cascaded Oscillators Locked to the Input Rhythm</article-title><source>Frontiers in Psychology</source><year>2011</year><volume>2</volume><pub-id pub-id-type="pmcid">PMC3127251</pub-id><pub-id pub-id-type="pmid">21743809</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00130</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hovsepyan</surname><given-names>S</given-names></name><name><surname>Olasagasti</surname><given-names>I</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Combining predictive coding and neural oscillations enables online syllable recognition in natural speech</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><elocation-id>3117</elocation-id><pub-id pub-id-type="pmcid">PMC7305192</pub-id><pub-id pub-id-type="pmid">32561726</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-16956-5</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lizarazu</surname><given-names>M</given-names></name><name><surname>Carreiras</surname><given-names>M</given-names></name><name><surname>Molinaro</surname><given-names>N</given-names></name></person-group><article-title>Theta-gamma phase-amplitude coupling in auditory cortex is modulated by language proficiency</article-title><source>Human Brain Mapping</source><year>2023</year><volume>44</volume><fpage>2862</fpage><lpage>2872</lpage><pub-id pub-id-type="pmcid">PMC10089097</pub-id><pub-id pub-id-type="pmid">36852454</pub-id><pub-id pub-id-type="doi">10.1002/hbm.26250</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><article-title>The auditory representation of speech sounds in human motor cortex</article-title><source>elife</source><year>2016</year><volume>5</volume><elocation-id>e12577</elocation-id><pub-id pub-id-type="pmcid">PMC4786411</pub-id><pub-id pub-id-type="pmid">26943778</pub-id><pub-id pub-id-type="doi">10.7554/eLife.12577</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meister</surname><given-names>IG</given-names></name><name><surname>Wilson</surname><given-names>SM</given-names></name><name><surname>Deblieck</surname><given-names>C</given-names></name><name><surname>Wu</surname><given-names>AD</given-names></name><name><surname>Iacoboni</surname><given-names>M</given-names></name></person-group><article-title>The Essential Role of Premotor Cortex in Speech Perception</article-title><source>Current Biology</source><year>2007</year><volume>17</volume><fpage>1692</fpage><lpage>1696</lpage><pub-id pub-id-type="pmcid">PMC5536895</pub-id><pub-id pub-id-type="pmid">17900904</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2007.08.064</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>H</given-names></name><name><surname>Nottebohm</surname><given-names>F</given-names></name></person-group><article-title>Auditory responses in avian vocal motor neurons: a motor theory for song perception in birds</article-title><source>Science</source><year>1985</year><volume>229</volume><fpage>279</fpage><lpage>282</lpage><pub-id pub-id-type="pmid">4012321</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Archakov</surname><given-names>D</given-names></name><etal/></person-group><article-title>Auditory representation of learned sound sequences in motor regions of the macaque brain</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><volume>117</volume><fpage>15242</fpage><lpage>15252</lpage><pub-id pub-id-type="pmcid">PMC7334521</pub-id><pub-id pub-id-type="pmid">32541016</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1915610117</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikuchi</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Sequence learning modulates neural responses and oscillatory coupling in human and monkey auditory cortex</article-title><source>PLOS Biology</source><year>2017</year><volume>15</volume><elocation-id>e2000219</elocation-id><pub-id pub-id-type="pmcid">PMC5404755</pub-id><pub-id pub-id-type="pmid">28441393</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000219</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gemmeke</surname><given-names>JF</given-names></name><etal/></person-group><conf-name>2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</conf-name><fpage>776</fpage><lpage>780</lpage><conf-sponsor>IEEE</conf-sponsor></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salt</surname><given-names>C</given-names></name><etal/></person-group><article-title>Growth standard charts for monitoring bodyweight in dogs of different sizes</article-title><source>PLOS ONE</source><year>2017</year><volume>12</volume><elocation-id>e0182064</elocation-id><pub-id pub-id-type="pmcid">PMC5584974</pub-id><pub-id pub-id-type="pmid">28873413</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0182064</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Computational intelligence and neuroscience</source><year>2011</year><volume>2011</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Bednar</surname><given-names>A</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>The Multivariate Temporal Response Function (mTRF) Toolbox: A MATLAB Toolbox for Relating Neural Signals to Continuous Stimuli</article-title><source>Frontiers in Human Neuroscience</source><year>2016</year><volume>10</volume><pub-id pub-id-type="pmcid">PMC5127806</pub-id><pub-id pub-id-type="pmid">27965557</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00604</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robisson</surname><given-names>P</given-names></name><name><surname>Aubin</surname><given-names>T</given-names></name><name><surname>Bremond</surname><given-names>J-C</given-names></name></person-group><article-title>Individuality in the voice of the emperor penguin Aptenodytes forsteri: adaptation to a noisy environment</article-title><source>Ethology</source><year>1993</year><volume>94</volume><fpage>279</fpage><lpage>290</lpage></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaalje</surname><given-names>GB</given-names></name><name><surname>McBride</surname><given-names>JB</given-names></name><name><surname>Fellingham</surname><given-names>GW</given-names></name></person-group><article-title>Adequacy of approximations to distributions of test statistics in complex mixed linear models</article-title><source>Journal of Agricultural, Biological, and Environmental Statistics</source><year>2002</year><volume>7</volume><fpage>512</fpage><lpage>524</lpage><pub-id pub-id-type="doi">10.1198/108571102726</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Comparison of dog/human vocal production.</title><p>A) Oscillograms and, overlaid, envelopes used to compute the vocal rate. B) Model estimates and their 95% CI of vocal rate in dog and human sequences. Black dots are the original observations. C) Vocal rate (Hz) and mean F0 (Hz) for matched ADS and DDS speech sentences. D) Model slope and 95% CI of weight effect on dog vocal rate (VR) and dominant acoustic frequency (DF). E) Density distribution of vocal rate according to vocal classes for dogs and languages for humans. Overall mean (thick dashed line) and SD (thin dashed lines) statistics are displayed.</p></caption><graphic xlink:href="EMS189894-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Schematic of the perception study.</title><p>Word stream stimuli were first constructed by recording dog-specific command words (mostly disyllabic and monosyllabic, cf. small insert) that were appended into a 5-word stream with ∼300 ± 50ms silence intervals. These word streams were altered with regards to 1) speech type: by either removing content (reversed words) or prosodic information (flatten pitch modulation and reversed energy contour) and 2) speech rate: compression by a factor of 2 or 4; amounting to nine word-stream conditions in total. The behavioural experiment consisted of an intelligibility scoring task for humans who listened to the full word stream, and of a playback task for dogs, who heard each word command separately (45 in total) a maximum of 3 times each, while the experimenter and the owner agreed on a behavioural response score. For the EEG experiment dogs were first fitted with 1 to 4 electrodes covered by a headband and linked to an amplifier strapped on their back (photo inserts). They were then instructed to lie down and passively listen to an audio track (broadcasted via a speaker) containing 40 repetitions of each word stream condition. For comparability purposes, human EEG recordings were made under the same experimental conditions.</p></caption><graphic xlink:href="EMS189894-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Speech stimulus alteration effects on intelligibility and characterisation of neural responses.</title><p>A) Mean (± SE) behavioural responses according to speech type and rate in humans and dogs. Tukey-corrected, post-hoc pairwise comparisons are shown. *** p&lt;0.001, ** p&lt;0.01, * p&lt;0.05. B) Time-frequency plots averaged across all conditions and individuals within species. Z-score transformed relative power is plotted to ease visual comparison across species. C) Power spectra (peak normalised and averaged between 0-1.3s) and unpaired t-test between species on frequency of highest power (range=1-7Hz).</p></caption><graphic xlink:href="EMS189894-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Speech tracking in both species and both the delta and the theta bands.</title><p>A) Mean and SD cerebro-acoustic coherence over the 1-20 Hz range, calculated from the normal speech condition. Black dashed line shows mean and SD random coherence values for pairings of neural signals with randomised acoustic envelopes. B) Paired t-test of coherence in the delta and theta range between the real cerebro-acoustic and cerebro-randomised acoustic pairings.</p></caption><graphic xlink:href="EMS189894-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Increased speech neural tracking (cerebro-acoustic coherence) increases comprehension.</title><p>A) Model slopes and 95% CI for the syllable and word coherence effect on intelligibility in dogs and humans. B) Mean and S.E. intercepts for each speech types in humans and dogs, showing that beyond speech tracking, additional processes must be present to explain the differences.</p></caption><graphic xlink:href="EMS189894-f005"/></fig></floats-group></article>