<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS188425</article-id><article-id pub-id-type="doi">10.1101/2023.09.20.558624</article-id><article-id pub-id-type="archive">PPR728859</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Generalization of deep learning models for predicting spatial gene expression profiles using histology images: A breast cancer case study</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Yuanhao</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Xie</surname><given-names>Jacky</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Tan</surname><given-names>Xiao</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ye</surname><given-names>Nan</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Nguyen</surname><given-names>Quan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute for Molecular Bioscience, The University of Queensland, 306 Carmody Road, 4072, QLD, Australia</aff><aff id="A2"><label>2</label>School of Mathematics and Physics, The University of Queensland, St Lucia, 4067, QLD, Australia</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding authors. <email>quan.nguyen@uq.edu.au</email> and <email>nan.ye@uq.edu.au</email></corresp><fn id="FN1"><p id="P1">FOR PUBLISHER ONLY Received on Date Month Year; revised on Date Month Year; accepted on Date Month Year</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>24</day><month>09</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>09</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">Spatial transcriptomics is a breakthrough technology that enables spatially-resolved measurement of molecular profiles in tissues, opening the opportunity for integrated analyses of morphology and transcriptional profiles through paired imaging and gene expression data. However, the high cost of generating data has limited its widespread adoption. Predicting gene expression profiles from histology images only can be an effective and cost-efficient <italic>in-silico spatial transcriptomics</italic> solution but is computationally challenging and current methods are limited in model performance. To advance research in this emerging and important field, this study makes the following contributions. We first provide a systematic review of deep learning methods for predicting gene expression profiles from histology images, highlighting similarities and differences in algorithm, model architecture, and data processing pipelines. Second, we performed extensive experiments to evaluate the generalization performance of the reviewed methods on several spatial transcriptomics datasets for breast cancer, where the datasets are generated using different technologies. Lastly, we propose several ideas for model improvement and empirically investigate their effectiveness. Our results shed insight on key features in a neural network model that either improve or not the performance of <italic>in-silico spatial transcriptomics</italic>, and we highlight challenges in developing algorithms with strong generalization performance.</p></abstract><kwd-group><kwd>Deep learning</kwd><kwd>Spatial transcriptomics</kwd><kwd>Histological images</kwd><kwd>Digital Pathology</kwd><kwd>Diagnosis</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">Spatial transcriptomics (ST) is a rapidly developing technology for producing histopathological images paired with gene expression profiles of thousands of individual spots/cells, providing information on both the unseen molecular signatures and imaging morphological features [<xref ref-type="bibr" rid="R1">1</xref>]. Technology such as the 10x Genomics Visium platform [<xref ref-type="bibr" rid="R2">2</xref>] measures the gene expression on spots of resolutions 55 to 100 micrometers in diameter that are distributed close together at fixed locations across the tissue, which can then be mapped back to the underlying high-resolution stained tissue image. ST has been used to characterize the spatial heterogeneity of cancers, providing new insights in cancer research, and opening an unprecedented potential for novel capabilities in the diagnosis and prognosis of cancer (e.g., see the survey paper [<xref ref-type="bibr" rid="R3">3</xref>]). However, the wide adoption of ST has been limited by its high cost.</p><p id="P4">Several deep learning methods have been developed for predicting gene expression profiles from Hematoxylin and Eosin (H&amp;E) stained histology images. These include methods that predict bulk or single-cell RNA profiles which do not provide spatial information [<xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R5">5</xref>], and methods that are able to predict spatially resolved profiles [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R11">11</xref>]. In this work, we focus on the latter class of methods that are emerging with the potential to bring about new abilities to predict spatial transcriptomic data for histopathological image assessments. While the published results support the potential of the deep learning approach, the performance of these existing methods still requires significant improvement. At the same time, there are several limitations in existing studies that prevent a good understanding of the state of the field: (a) existing studies often use different datasets to compare a subset of the algorithms, instead of comparing existing algorithms on the same datasets; (b) existing studies focus on in-distribution (ID) generalization performance (where the training and test sets are sampled from the same distribution), while in practice, it is even more important to assess the generalization for out-of-distribution (OOD) (where the training and test sets may follow different distributions); (c) the methods often differ in multiple aspects, including the model architectures, preprocessing techniques, and data augmentation strategies, but it is often unclear which of the differences account for the performance differences.</p><p id="P5">Our paper aims to address the above limitations and advance research in the field by performing a comprehensive assessment of the generalization performance of existing approaches and experimenting some new ideas for improving models. Specifically, we identify six existing methods and we study the following questions: <list list-type="bullet" id="L1"><list-item><p id="P6">How do the methods compare with each other on datasets generated by different technologies?</p></list-item><list-item><p id="P7">How well do the methods generalize on OOD data?</p></list-item><list-item><p id="P8">How effective are the different preprocessing and data augmentation techniques?</p></list-item><list-item><p id="P9">How effective are some new ideas for improving deep learning models (e.g., the use of pre-trained models)?</p></list-item></list></p><p id="P10">We focus on breast cancer in this study, because of the availability of relatively large datasets generated by a legacy spatial transcriptomics protocol (100 μm resolution spots) and a recent version of 10x Genomics Visium protocol (55 μm spots), which have different resolutions and detection sensitivities, and would allow us to study the above questions. In fact, some previous work has partially compared different methods using breast cancer tissues [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R10">10</xref>], but the important questions and limitations as mentioned above remained unaddressed. For example, several methods [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R8">8</xref>] have not been quantitatively evaluated on datasets generated by newer, higher-resolution technologies, and their performance on OOD data has not been evaluated.</p><p id="P11">Our paper contributes a systematic review of existing methods, and results in several interesting empirical insights, with some highlighted below.</p><list list-type="bullet" id="L2"><list-item><p id="P12">Top-performing methods: Hist2ST, BLEEP, and STimage consistently achieved higher test set performance for both the in-distribution and OOD settings.</p></list-item><list-item><p id="P13">General performance: All the evaluated methods predicted overall variable genes or cancer markers with relatively low accuracy, resulting in low-performance metrics across the diverse datasets. This raises the need for assessing which genes can be predicted and at which performance ranges.</p></list-item><list-item><p id="P14">Limited generalization: Most methods exhibited limited generalization capability, with low performance when applied to OOD data. Some methods exhibit overfitting to the data, and we highlight differences in models that may account for improved robustness.</p></list-item><list-item><p id="P15">Negative transfer: Using pre-trained image encoders that have been trained on general images led to negative transfer when applied to H&amp;E images, resulting in lower performance compared to training from the ground up.</p></list-item><list-item><p id="P16">Impact of image augmentation and expression preprocessing: Various image augmentation strategies did not have a discernible impact on overall performance. In addition, different preprocessing transformations for gene expression values did not lead to significantly different results.</p></list-item></list><p id="P17">The remainder of this paper is structured as follows. First, we review and compare six existing deep-learning algorithms for predicting spatial gene expression profiles from histology images. We then describe our benchmark methodology, with details on the datasets used, the performance metrics, and variants of a top-performing algorithm Hist2ST [<xref ref-type="bibr" rid="R8">8</xref>]. This is followed by assessing the ID and OOD generalization performances of the existing methods and the variants of Hist2ST. We then discuss the benchmarking results and critically review the differences between methods and model architectures in light of the observed results. We also describe experimental results from testing several new ideas for improving the performance. Finally, we address the limitations of this benchmarking work and highlight the potential challenges of applying deep learning models for the prediction of spatial transcriptomic profiles.</p><sec id="S2"><title>Review of existing methods</title><p id="P18">We identify and review six existing methods in this section. All methods extract image patches from the whole-slide histology images based on the spatial coordinates of the measured gene expression. The transformer-based methods (HisToGene, Hist2ST) treat all spots on a single tissue section as an independent training sample, making use of the global relationship between spots; the other methods treat all spots (both within and between tissue samples) as independent training samples, with the number of input images determined by the batch size. Most methods (ST-Net, HisToGene, Hist2ST, STimage, DeepSpaCE) learn to predict gene expression directly by multivariate regression; the exception is BLEEP, which infers the gene expression by querying through nearest-neighbors in a learned joint embedding space for image and gene expression. The architectures used in the first five methods all share the same high-level structure: an image encoder is used to extract features from the image patches, then a prediction head is used to predict the gene expression profiles using the extracted features. Some methods additionally include a distribution module for predicting the distribution of the gene expression values rather than a fixed value. This general structure is illustrated in the top figure in <xref ref-type="fig" rid="F1">Figure 1 (a)</xref>. The five methods differ in the specific networks used for the encoder, the prediction head, and the distribution module, as will be discussed below. In addition to differences in model architecture, these methods employ different techniques for processing image and gene expression data; summarized in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>. The methods we have identified have publicly available code and have been individually evaluated previously on different datasets, including breast cancer data; we provide a summary in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table S4</xref>. In this section, we focus on comparing the model architectures and loss functions used and review each method separately below.</p><p id="P19">ST-Net [<xref ref-type="bibr" rid="R6">6</xref>] is a CNN model that employs a DenseNet-121 pre-trained on ImageNet [<xref ref-type="bibr" rid="R13">13</xref>] as an image encoder. The whole-slide histology images are divided into 224×224-pixels patches for each spot, which are transformed into features by the image encoder. The prediction head is a single fully connected output layer, which directly predicts the vector of gene expression. ST-Net is trained by minimizing the mean squared error (MSE).</p><p id="P20">HisToGene [<xref ref-type="bibr" rid="R7">7</xref>] utilizes a modified vision transformer (ViT) model as an image encoder, which is able to incorporate the spatial relation of image spots. 112×112-pixel image patches are flattened and passed into a linear layer to produce the patch embeddings of dimension 1024. In addition, a learnable linear layer is used to map the spatial coordinates of each spot into a positional embedding vector of the same dimension. The patch embeddings and position embeddings are aggregated via summation and then input into eight multi-headed attention layers to generate latent embeddings. The prediction head is a single feed-forward layer that directly outputs the gene expression values. The model is trained to minimize the MSE.</p><p id="P21">Hist2ST [<xref ref-type="bibr" rid="R8">8</xref>] consists of three main modules – Convmixer, Transformer, and Graph Convolutional Network (GCN) – to learn a global feature representation for all image spots on a single tissue section. 112×112-pixel patches are extracted around each spot. The initial image encoder is a Convmixer, a variant of CNN, that produces image features for each spot. The transformer module encodes spatial locations and fuses them with the image features, employing eight multi-head attention layers to learn global spatial dependencies. The aggregated features from the transformer module are then input into the GCN module to explicitly learn local spatial dependencies. The prediction head is a fully connected layer. In addition, there is a distribution module that estimates parameters for Zero-Inflated Negative Binomial (ZINB) [<xref ref-type="bibr" rid="R14">14</xref>] distributions for each gene; this is learned by minimizing the negative log-likelihood. Self-distillation [<xref ref-type="bibr" rid="R15">15</xref>] is used to learn from augmented image patches. The loss function combines the MSE from the prediction head, the ZINB loss, and the self-distillation loss.</p><p id="P22">STimage [<xref ref-type="bibr" rid="R10">10</xref>] is a CNN model that uses a ResNet50 image encoder to extract features from 299×229-pixel image patches. The features are input into a distribution module, which consists of two fully connected output layers to estimate the parameters of a Negative Binomial (NB) distribution for each gene. This is achieved by minimizing the negative log-likelihood. The estimated mean of the NB distribution for each gene is used as the prediction, and the parameters can be used to quantify the uncertainty in the prediction. STimage additionally employs an ensemble approach to account for variation across independent training runs to improve performance and robustness.</p><p id="P23">DeepSpaCE [<xref ref-type="bibr" rid="R9">9</xref>] is a CNN model that utilizes the VGG16 architecture as an image encoder. Image patches of size 150% times the original spot dimensions are extracted from the histology image. The prediction head consists of two fully connected layers for predicting the gene expressions. The loss function used is the smooth L1 loss.</p><p id="P24">BLEEP [<xref ref-type="bibr" rid="R11">11</xref>] learns a bimodal embedding for image and gene expression data. Its architecture is illustrated by the bottom figure in <xref ref-type="fig" rid="F1">Figure 1 (a)</xref>. The process involves extracting features from the image tiles and expression vectors using separate encoders. The image encoder is a pre-trained ResNet50, while a fully connected network serves as the expression encoder. The image and gene expression features are then separately projected into image embeddings and expression embeddings, with a shared dimension of 256. Contrastive learning [<xref ref-type="bibr" rid="R16">16</xref>] is employed to align the latent space for the two embeddings by minimizing the cross entropy. During inference, the image patches are mapped to embeddings by the encoder, and the k-nearest expression profiles from the reference dataset are selected based on their proximity, measured by Euclidean distance, to each patch in the joint embedding space. The prediction of expression profiles for the query patches is then performed by taking a linear combination of the selected expression profiles.</p></sec></sec><sec id="S3" sec-type="methods"><title>Methods</title><p id="P25">We evaluate the ID and OOD generalization performance of all six models and multiple variants of Hist2ST using two data settings, as detailed below.</p><sec id="S4"><title>Datasets</title><p id="P26">In this study, we utilized two spatial transcriptomic breast cancer datasets generated by the Visium technology and the legacy spatial transcriptomics technology. The two datasets, summarized in <xref ref-type="table" rid="T1">Table 1</xref> and described in detail below, differ in terms of the technology, the number of spots, the spot resolution, the number of genes detected per spot, and the number of patients per datatype. Patient samples were collected in different cohorts and data was generated by different laboratories, representing technical variations likely observed in the real application settings. This approach allowed us to assess how various models perform across datasets with different characteristics.</p><p id="P27">The lower resolution (100 μm per spot) HER2+ breast tumour ST dataset [<xref ref-type="bibr" rid="R17">17</xref>] comprises 36 tissue sections from 36 HER2+ patients, with data generated using the initial version of the spatial transcriptomics protocol before the method was further developed by 10x Genomics to increase the resolution to 55 μm per spot. The term legacy ST is referred to for historical reasons, without implying about quality of the data. Among the 36 samples, there are annotated cell type labels for 8 tissues, by trained pathologists. Each sample consists of around 300 to 600 spots, with each spot containing the measurements for 11,871 genes. The images are low resolution, less than 9000 pixels in height and width.</p><p id="P28">The higher resolution Visium breast cancer dataset, introduced and described in [<xref ref-type="bibr" rid="R10">10</xref>], consists of nine breast cancer tissue samples, which includes 3 samples (2 fresh-frozen and 1 formalin-fixed-paraffin-embedded, FFPE, tissue) from 10x Genomics [<xref ref-type="bibr" rid="R18">18</xref>] and six samples obtained from Swarbrick’s laboratory [<xref ref-type="bibr" rid="R19">19</xref>]. Each sample is measured on the 10x Visium platform and contains around 1300 to 4900 spots, with each spot containing the measurements for up to 36,601 genes. The images are high resolution, ranging from approximately 9000 to 40000 pixels in height and width.</p><sec id="S5"><title>Genes selected for evaluation</title><p id="P29">Although spatial transcriptomic data contains measurements for thousands of genes, the methods considered have not been designed to scale to predict more than a small subset of usually 200 to 1000 genes. As the performance is likely dependent on the gene targets for prediction, we considered the prediction of two sets of genes: highly variable genes (HVGs), and a smaller set of 12 cancer-associated genes (marker genes). For the HER2+ dataset, the performance was assessed on the prediction of the 769 shared HVGs and the Visium dataset was assessed on both the HVGs (685 shared), and the 12 marker genes. Of note is that the selection of HVGs for prediction is a common practice across all methods, although the rationale for why the HVGs are suitable or not is not clearly justified, except for the fact that these methods are not scalable to predict all genes.</p><p id="P30">For the BLEEP method, the model appears to be more scalable and the selected gene panel may incorporate a larger set of highly variable genes for both training and inference. Therefore, we also assess the model performance when a larger set of genes are used, and so we have benchmarked the BLEEP method in two ways for comparison: for the HVGs, the gene set was fixed which is the same case as the other methods; and another gene set was union of the 12 marker genes and 1000 HVGs across all samples, resulting in an enlarged set.</p></sec></sec><sec id="S6"><title>Performance measures</title><p id="P31">We describe how we evaluate a method’s ID and OOD generalization performance below. We measure the predictive performance of a method for a gene using the Pearson correlation coefficient (PCC), defined by <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtext>PCC</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>Cov</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>X</italic><sub>pred</sub>, <italic>X</italic><sub>target</sub> is the predicted and measured target gene expression for all spots in the sample, respectively. PCC is a better measure than the mean squared error or the mean absolute error, because it is not dependent on the differences in the absolute scale between abundant and lowly-expressed genes.</p><p id="P32">The ID generalization performance is measured using LOOCV (leave-one-out cross-validation) as the number of samples is small in the datasets used. Specifically, we leave one sample out at a time and train a model on the remaining samples. We then make predictions on the hold-out sample and measure the PCC for each gene.</p><p id="P33">The OOD generalization performance is measured only on the Visium dataset, using the marker genes. Specifically, we partitioned the Visium dataset (illustrated in 2a) into a training set, a validation set, and a test set. The training set and the validation set contain 4 samples and 2 samples respectively from the Swarbrick data [<xref ref-type="bibr" rid="R19">19</xref>]. The test set contains samples from 10x Genomics. We used the validation set for measuring the ID generalization performance instead of for hyperparameter tuning. The test set is used for measuring the OOD generalization performance. The partition was motivated by the observation that data produced from two different laboratories are likely to have distribution shifts in the image and gene expression data. To verify this, <xref ref-type="fig" rid="F2">Figure 2a</xref> shows that the whole slide images for the training and validation dataset are separated into two groups with different color profiles, which are further different from the test set. <xref ref-type="fig" rid="F2">Figure 2b</xref> verifies that the color distribution of the RGB channels is distinctive and varies across individual samples and laboratories. In terms of gene expression, <xref ref-type="fig" rid="F2">Figure 2c</xref> shows that the distribution of gene expression has similar distinctiveness between the training and validation sets and the test set samples.</p></sec><sec id="S7"><title>Algorithm settings</title><p id="P34">All the methods were first trained and tested with their default hyperparameters, pre-and post-processing steps, and data augmentation methods. We provide a concise overview of the processing steps applied to both gene expression data and image data for each method in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref> and a detailed summary in <xref ref-type="supplementary-material" rid="SD1">Supplementary section 7.1</xref>. Each model was trained on a single NVIDIA Tesla V100 SMX2 GPU with 32 GB RAM.</p><p id="P35">When testing Hist2ST and HisToGene with the Visium high-resolution data, we faced out-of-memory issues. Since these methods include transformer modules that process all the spots in tissue samples at once, processing Visium samples that usually contain more than 2000 spots exceeds the available memory. To address this, we divided each whole image into smaller, non-overlapping square windows of 4000×4000-pixel sections, resulting in multiple training instances for each sample, and each instance can be trained in memory successfully, with each batch containing hundreds of spots rather than thousands.</p></sec><sec id="S8"><title>Proposed modifications for Hist2ST</title><p id="P36">To explore possible approaches to improve predictive performance and robustness, we adopted Hist2ST as a baseline model and experimented with the following modifications, which are illustrated in the <xref ref-type="supplementary-material" rid="SD1">Supplementary section 7.1</xref>. We also empirically compare different augmentation techniques and transformations for gene preprocessing to determine if there is an augmentation and preprocessing method that can optimally improve the model performance.</p><sec id="S9"><title>Image augmentation and color normalization</title><p id="P37">Existing methods employ different data augmentation methods to improve model robustness to variation in image data and improve generalization. These techniques are summarized in <xref ref-type="supplementary-material" rid="SD1">Figure S2a</xref>. To investigate whether different augmentation techniques result in better performance, we benchmarked each technique using the Hist2ST backbone model.</p><p id="P38">H&amp;E images produced by different equipment and laboratories can vary greatly in stain levels and color distribution. We observe such variations in the Visium dataset (<xref ref-type="fig" rid="F2">Figure 2b</xref>). Although some existing techniques apply random color augmentation such as color jitter (see <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>), many do not employ processing steps that take into consideration the unique characteristics of H&amp;E images, which possess a color distribution greatly distinct from natural images. For example, only STimage applies stain normalization to the images prior to training. Thus, we also systematically benchmarked the effects of applying dedicated stain color augmentation and normalization methods on model performance and robustness. This included stainlib [<xref ref-type="bibr" rid="R20">20</xref>], which offers H&amp;E-intensity color augmentation and Reinhard color normalization, and RandStainNA [<xref ref-type="bibr" rid="R21">21</xref>], which integrates stain normalization and stain augmentation in a combined fashion to constrain the variability of stain styles within a practicable range.</p></sec><sec id="S10"><title>Preprocessing for gene expression</title><p id="P39">Different methods utilize different transformations for the gene expression values; this is summarized in <xref ref-type="supplementary-material" rid="SD1">Table S1</xref>. Here, log-transformation refers to a log-transformation on the counts with a pseudocount addition. To determine whether there is a transformation that results in better predictive performance, we benchmarked the effect of the following transformations: no transformation (raw), log-transformation, log-transformation on normalized counts (log counts per million), and MinMax scaling (counts scaled to the range [0, 1]).</p></sec><sec id="S11"><title>Auxiliary classification loss</title><p id="P40">We investigated whether the addition of an auxiliary classification task in training could improve the performance of gene expression prediction. As a subset of the Visium and HER2+ datasets contain spot annotations by pathologists for the same tissue section with the spatial data, employing this information by introducing a classifier module during training could potentially lend the model to learn more informative features for each image spot. Furthermore, the classification loss could act as a regularization term to improve the generalization of the model to images of similar tissue characteristics in new data. Here, we proposed Regclass, a variant of Hist2ST, which introduces an auxiliary classification head in the model and is trained to jointly optimize the original loss for regression and the cross-entropy loss for classification.</p><p id="P41">The workflow of Regclass is shown in <xref ref-type="boxed-text" rid="BX1">Algorithm 1</xref>. The classification head is a 4-layer multilayer perceptron, and the remaining components are the same as in Hist2ST, including the Convmixer, position encoder, transformer, graph convolution network, and regression head. <boxed-text id="BX2" position="anchor" content-type="below"><caption><title>Algorithm 1 Regclass model</title></caption><p id="P42"><bold>Input:</bold> Image patches <italic>I</italic>, Spatial coordinates <italic>S</italic>, Adjacency matrix <italic>A</italic>, Convmixer CONV, Position encoder POS, Transformer T, Graph convolutional network GCN, ZINB layer ZINB, Classification head CLS, Regression head REG</p><p id="P43"><bold>Output:</bold> Gene expression <italic>X</italic><sub>pred</sub>, Cell types <italic>Y</italic><sub>pred</sub></p><p id="P44">  1: <italic>H</italic><sub>0</sub> ← CONV(<italic>I</italic>)               ▹ Image embeddings</p><p id="P45">  2: <italic>P</italic> ← POS(<italic>S</italic>)                 ▹ Position embeddings</p><p id="P46">  3: <italic>H</italic><sub>1</sub> ← T(<italic>H</italic><sub>0</sub> + <italic>P</italic>)                    ▹ Self-attention</p><p id="P47">  4: <italic>H</italic><sub>2</sub> ← GCN(<italic>H</italic><sub>1</sub>, <italic>A</italic>)                 ▹ Message passing</p><p id="P48">  5: <italic>m, d, p</italic> ← ZINB(<italic>H</italic><sub>2</sub>)              ▹ ZINB parameters</p><p id="P49">  6: <italic>Y</italic><sub>pred</sub> ← CLS(<italic>H</italic><sub>2</sub>)                   ▹ Classification</p><p id="P50">  7: <italic>X</italic><sub>pred</sub> ← REG(<italic>H</italic><sub>2</sub>)                   ▹ Regression</p><p id="P51">  8: <bold>Return:</bold> <italic>X</italic><sub>pred</sub>, <italic>Y</italic><sub>pred</sub></p></boxed-text></p><p id="P52">The loss is calculated as follows. <disp-formula id="FD2"><label>(1)</label><mml:math id="M2"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Loss</mml:mtext><mml:mo>=</mml:mo><mml:mtext>MSE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>X</mml:mtext><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mtext>L</mml:mtext><mml:mrow><mml:mtext>ZINB</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mspace width="1.5em"/><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mtext>MSE</mml:mtext></mml:mrow><mml:mrow><mml:mtext>self-distillation</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo>⋅</mml:mo><mml:mtext>CE</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mtext>pred</mml:mtext></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mtext>target</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> where <italic>X</italic><sub>pred</sub>, <italic>X</italic><sub>target</sub> are the predicted and ground truth gene expression values, respectively; <italic>Y</italic><sub>pred</sub>, <italic>Y</italic><sub>target</sub> are the predicted and ground truth tissue classes, respectively; and <italic>γ</italic> is a hyperparameter to scale the cross-entropy loss CE. The MSE, L<sub>ZINB</sub>, and MSE<sub>self-distillation</sub> losses are the same as in Hist2ST. We investigated different values of <italic>γ</italic> to determine an optimal value for both accuracy and generalization.</p></sec><sec id="S12"><title>Pre-trained image encoders</title><p id="P53">Here, we investigated whether using transfer learning with different image encoders trained on ImageNet could improve the performance of the baseline Hist2ST model. As the datasets are small, using pre-trained image encoders could be advantageous as it allows the model to leverage the knowledge gained from broader training data, enabling it to extract better features from a model initialized with meaningful weights. We benchmarked the original Hist2ST model, which used a Convmixer as the initial image encoder, with the following models pre-trained on ImageNet: ResNet50 [<xref ref-type="bibr" rid="R12">12</xref>], EfficientNet V2 [<xref ref-type="bibr" rid="R22">22</xref>] and Swin Transformer [<xref ref-type="bibr" rid="R23">23</xref>]. Only the last layer of each backbone was fine-tuned.</p></sec><sec id="S13"><title>Simplification with graph attention network</title><p id="P54">The original Hist2ST model includes a transformer and GCN module to learn spatially aware representations. However, we have found that the transformer does not scale well to newer ST data, such as for the Visium samples, which contain thousands of spots on one tissue section. This is due to the quadratic complexity of the transformer in terms of the input sequence length from global self-attention.</p><p id="P55">On the other hand, the graph attention network (GAT) uses the attention mechanism to learn node embeddings by considering only the neighbourhood information of each node. Attending to only a subset of neighbouring nodes for each node reduces the overall computation when compared to the transformer. We hypothesized that a simpler model, which consists of replacing the transformer and GCN modules in Hist2ST with a 4-layer GAT, could perform similarly to the original. This was modified in conjunction with the replacement of the initial feature extractor and benchmarked for comparison.</p></sec></sec></sec><sec id="S14" sec-type="results"><title>Results</title><sec id="S15"><title>Performance of existing methods</title><sec id="S16"><title>ID generalization performance</title><p id="P56"><xref ref-type="fig" rid="F1">Figures 1b,c</xref> shows the results of the performance comparison between the six methods across the two datasets. In general, the best-performing methods are STimage, BLEEP, and Hist2ST, which consistently outperform other methods based on average PCC values across all tissues from different datasets. HisToGene had a slightly lower performance on average, with a median accuracy of around 0.1 correlation. In contrast, ST-Net and DeepSpaCE were unable to consistently predict HVGs and markers with median accuracy above 0.1 correlation. The relative performance ranking between methods is similar across HER2+ and Visium datasets, suggesting that the performance of methods remains consistent and does not change much with an increase in image resolution and number of spots. On the HER2+ dataset, the performance of BLEEP and STimage is significantly better than that of the other models. On the Visium dataset, the performance of Hist2ST consistently outperforms other models. We find that the ranking between methods is consistent across predicting the smaller set of marker genes and the larger set of HVGs on the Visium dataset, with slightly higher average PCC on the markers.</p></sec><sec id="S17"><title>OOD generalization performance</title><p id="P57"><xref ref-type="fig" rid="F2">Figure 2d</xref> shows that in general, the average PCC on the validation (ID samples) and test sets (OOD) are similar. STimage has the highest median performance on the OOD test data. For Hist2ST, there is a large discrepancy between the high performance on the validation dataset and the lower performance on the test dataset, suggesting possible overfitting to the training data and less generalizability.</p></sec></sec><sec id="S18"><title>Performance of Hist2ST modifications</title><p id="P58">We empirically assessed the performance changes due to each of the new modifications in model architecture, regularization, and data augmentation to the backbone Hist2ST model. We applied the data preprocessing methods as shown in <xref ref-type="supplementary-material" rid="SD1">Figure S1a</xref> to the images in the training set and the trained models were tested on the unseen, unprocessed image data. The Hist2ST’s performance results were compared between different augmentation techniques and gene expression transformation methods. Our comparisons suggest that the data augmentation techniques resulted in similar performance (<xref ref-type="supplementary-material" rid="SD1">Figure S1c</xref>). Comparing stain augmentation and color normalization we also observed that both steps did not improve the performance (<xref ref-type="supplementary-material" rid="SD1">Figure S1d</xref>). In addition, we found that the various transformations for the gene expression values resulted in similar performance in terms of PCC, where the only significant difference is that the variance for the log-normalized transformation is smaller than the others (<xref ref-type="supplementary-material" rid="SD1">Figure S2e</xref>).</p><p id="P59">To assess the usefulness of the auxiliary classification module in a modified Regclass model as illustrated in <xref ref-type="supplementary-material" rid="SD1">Figure S2</xref>, we found that the model can be trained to classify tissue types successfully in most of the samples, with both the F1 score and Area under the ROC Curve (AUC) generally above 0.8 <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>. However, this did not translate to performance improvements for predicting gene expression, with the performance being similar to the original Hist2ST (<xref ref-type="supplementary-material" rid="SD1">Figure S3a</xref>). <xref ref-type="supplementary-material" rid="SD1">Figure S3b</xref> shows that Regclass slightly improves the performance on OOD data, for values of <italic>γ</italic> above 0.</p><p id="P60">Comparing the performance of the pre-trained feature extractors as shown in <xref ref-type="supplementary-material" rid="SD1">Figure S3a</xref>, we found that alternative extractors show a large performance reduction compared to the method used in Hist2ST. Similarly, the use of the graph attention network did not result in any improvements in performance on both datasets.</p></sec></sec><sec id="S19" sec-type="discussion"><title>Discussion</title><p id="P61">We benchmarked six methods to assess the prediction accuracy and generalization across two datasets. The comparison demonstrates that, on average, existing methods do not predict expression to a high correlation for both HVGs and marker genes. For top-performing methods – Hist2ST, BLEEP, and STimage – the majority of genes are predicted with a correlation between 0.1 and 0.3. We observe slightly higher average performance when predicting the set of marker genes, indicating that useful biological markers may be predicted to a somewhat higher degree of accuracy. Thus, prior selection of useful genes and use of a smaller subset of genes may be beneficial in general to improve performance and reduce computational cost.</p><p id="P62">We did not observe substantial differences between the overall performance of the model when applying to the Visium and HER2+ datasets, despite the fact that the Visium dataset has an increased resolution and spot density. Although the total number of Visium spots is nearly double that in the HER2+ dataset, the number of independent Visium samples is lower (36 vs 9 patients), possibly reducing the generalization capability as spots from the same sample are correlated [<xref ref-type="bibr" rid="R24">24</xref>].</p><p id="P63">Our assessment of the generalizability and robustness of each method on ID validation and OOD test sets indicate that each method has similar performance on both sets, with the exception of Hist2ST, which shows a drop-off in performance in the OOD set. This may be in part due to the higher model complexity of Hist2ST. However, the relationship between the number of parameters and overfitting is not simple [<xref ref-type="bibr" rid="R25">25</xref>], and the performance and generalization may be in part limited by the lack of training data, which only consisted of 4 samples. The other top-performing methods BLEEP and STimage demonstrate robust and consistent performance in the OOD set, which may be due to their different methods of inference that contrast them from the other methods that directly produce point-wise predictions. For STimage, learning the negative binomial distributions for each gene allows the model to account for noise and variance present in the data. For BLEEP, performing inference by querying and using a weighted combination of existing values may be more robust than using a parameterized function, which only indirectly contains information about the true distributions of genes in the training data. These results and observations can provide insight into how deep learning models can be designed to take into account noise and uncertainty in the data and improve robustness.</p><p id="P64">Our results provide evidence that current methods that transform gene expression in different ways do not have a discernible impact on the performance of the Hist2ST model. Thus, although prior normalization of counts and variance stabilizing transformations such as log-transformation are usually applied to reduce technical variation [<xref ref-type="bibr" rid="R26">26</xref>], we find that there are no significant benefits for improving predictive performance for the currently existing models and data. We observe that normalization of the counts with log-transformation results in a smaller spread in the accuracy across predicted genes, which may be beneficial for stabilising predictions.</p><p id="P65">Although we have tested various methods for improving the state-of-the-art model Hist2ST, such as stain augmentation and normalization, no improvement in the average LOOCV performance or improved generalization to OOD data was observed. This may be due to the complexity of the relationship between stain levels and gene expression, making the effect of color perturbations on the generalization of models unclear and possibly less effective. We did not find improvement by using an auxiliary classification loss, meaning that the use of tissue type information this way did not improve the model at the task of predicting gene expression. Moreover, using pre-trained feature extractors, or simplifying the architecture with a graph attention network, resulted in worse performance compared to the original model. This suggests that models pre-trained on natural image datasets do not transfer well to the domain of histology slide images, and that the global attention layers in the transformer are important for higher performance.</p><p id="P66">In terms of scalability, we note that the transformer-based methods Hist2ST and HisToGene do not scale well to data with thousands of spots, such as the Visium data, due to the computation of global attention between spots. Although a workaround was implemented in this work, this sacrificed global attention between all the spots in a sample. In contrast, the other methods that treat spots as independent training samples are more scalable as the batch size is independent. Improvements in efficiency and scalability for deep learning methods should be considered for future developments and will be crucial as ST technology develops, providing increases in spot resolution and total spot counts.</p><sec id="S20"><title>Limitations</title><p id="P67">Our benchmarking comparison shows that there is still a large gap between the performance of existing models and effectively usable outcomes. However, there are a number of limitations that should be considered when drawing conclusions from our results.</p><p id="P68">Firstly, the limited size of the spatial transcriptomic datasets used, which was due to limited availability of publicly accessible data, poses a challenge in drawing comprehensive benchmarking conclusions. In this work, the higher resolution Visium data comprised only 9 independent samples, and the HER2+ dataset only 36. Deep learning methods require a large training and diverse training set to avoid overfitting and generalize effectively [<xref ref-type="bibr" rid="R24">24</xref>]. Consequently, caution should be exercised when generalizing the findings and extrapolating the performance of these models to datasets of different sizes. In addition, we focused on datasets consisting of breast cancer tissue samples but did not include data from other types of tissues due to the limited amount of training samples from available data. Thus, it may be possible that our results and conclusions are not generalizable to other cancer or tissue types, where the relationship between morphological features and gene expression in the data is different.</p><p id="P69">In addition, we note that there could be variation in performance due to the set of genes chosen to train and evaluate the models. Although we have focused on a larger set of highly variable genes and a smaller set of cancer markers and found consistent results between methods, this is only a subset of the available expression panel which consists of over 30,000 genes in total for the Visium data. For current methods, it is infeasible to predict all the possible genes, and is computationally expensive to train or re-train models to predict different or larger sets of genes. Therefore, further work on finding a useful biologically relevant set of genes or a set of genes that can be consistently predicted to high accuracy is highly desirable.</p></sec><sec id="S21"><title>Conclusions and perspectives</title><p id="P70">While the idea of cost-effective in-silico transcriptomics is appealing, several intricate challenges hinder the potential for an effective and reliable solution. As described by some authors [<xref ref-type="bibr" rid="R11">11</xref>], a fundamental challenge lies in the ill-posed nature of the problem. Histology images and spatial transcriptomics data offer complementary views of tissue composition and gene expression patterns. However, expecting image features to predict the expression of all genes is ambitious, and difficult due to the complex, multifaceted nature of gene regulation. Nevertheless, this challenge prompts researchers to identify and prioritize specific gene categories that are biologically relevant for the intended applications. A collection of genes as signatures for cell types or subtypes or those that are associated with the morphological changes and spatial distribution during cancer progression emerge as crucial candidates, as they hold significant diagnostic and therapeutic implications.</p><p id="P71">However, the joint relationship between gene distribution and tissue image features is still unclear for the majority of genes, and thus there is a large component of uncertainty in both the choice of genes to train on and the model predictions, with many genes not being able to be predicted reliably by current methods. Most existing methods do not have a way of quantifying the uncertainty in the model predictions, which is important for establishing reliability and robustness when testing on new data [<xref ref-type="bibr" rid="R27">27</xref>]. STimage is among the first methods to quantify uncertainty prediction for each gene in each spot [<xref ref-type="bibr" rid="R10">10</xref>]. More work and improvements in this area and a better understanding of spatial gene expression and tissue relationships will be important to improve these methods and support their use in practice.</p><p id="P72">Furthermore, the rapidly evolving landscape of spatial transcriptomic methods introduces additional complexities [<xref ref-type="bibr" rid="R28">28</xref>]. New technologies generate data that are different in resolution and fidelity to older data. Experimental artifacts, batch effects, and variations within and across samples can confound prediction models, potentially leading to unreliable results. Addressing these challenges requires robust preprocessing techniques and machine-learning algorithms that can properly separate biological signals from noise [<xref ref-type="bibr" rid="R29">29</xref>]. In addition, the development of deep learning methods would benefit greatly from having larger amounts of available data across a diverse range of sample types; efforts to promote the open sharing of spatial transcriptomic data would greatly enhance the robustness and reproducibility of experimental outcomes in this field.</p><p id="P73">In conclusion, predicting gene expression from histology images and spatial transcriptomics data is a formidable challenge that is still largely unsolved by existing methods. Overcoming the challenging nature of the problem by focusing on a subset of predictable genes, dealing with scarce training data, and navigating the complexities introduced by experimental variability are crucial steps toward achieving an effective solution. Success in this endeavour holds the promise of providing cost-effective solutions for digital pathology.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS188425-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d15aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S22"><title>Funding</title><p>This research was supported by the Australian Research Council (ARC DECRA grant DE190100116), the National Health &amp; Medical Research Council (NHMRC Project Grant 2001514), and the NHMRC Investigator Grant (GNT2008928).</p></ack><sec id="S23" sec-type="data-availability"><title>Data and code availability</title><p id="P74">The datasets analysed in this study are available here: human HER2-positive breast tumor ST data <ext-link ext-link-type="uri" xlink:href="https://github.com/almaan/her2st">https://github.com/almaan/her2st</ext-link>, 10x Genomics Visium data and Swarbrick’s Laboratory Visium data <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48610/4fb74a9">https://doi.org/10.48610/4fb74a9</ext-link>.</p><p id="P75">Our code used to produce the data reported here is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/BiomedicalMachineLearning/DeepHis2Exp">https://github.com/BiomedicalMachineLearning/DeepHis2Exp</ext-link>.</p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>Vivien</given-names></name></person-group><article-title>Method of the Year: spatially resolved transcriptomics</article-title><source>Nature Methods</source><year>2021</year><month>January</month><volume>18</volume><issue>1</issue><fpage>9</fpage><lpage>14</lpage><pub-id pub-id-type="pmid">33408395</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ståhl</surname><given-names>Patrik L</given-names></name><name><surname>Salmén</surname><given-names>Fredrik</given-names></name><name><surname>Vickovic</surname><given-names>Sanja</given-names></name><name><surname>Lundmark</surname><given-names>Anna</given-names></name><name><surname>Navarro</surname><given-names>José Fernández</given-names></name><name><surname>Magnusson</surname><given-names>Jens</given-names></name><name><surname>Giacomello</surname><given-names>Stefania</given-names></name><name><surname>Asp</surname><given-names>Michaela</given-names></name><name><surname>Westholm</surname><given-names>Jakub O</given-names></name><name><surname>Huss</surname><given-names>Mikael</given-names></name><etal/></person-group><article-title>Visualization and analysis of gene expression in tissue sections by spatial transcriptomics</article-title><source>Science (New York, NY)</source><year>2016</year><month>July</month><volume>353</volume><issue>6294</issue><fpage>78</fpage><lpage>82</lpage><pub-id pub-id-type="pmid">27365449</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Qichao</given-names></name><name><surname>Jiang</surname><given-names>Miaomiao</given-names></name><name><surname>Wu</surname><given-names>Liang</given-names></name></person-group><article-title>Spatial transcriptomics technology in cancer research</article-title><source>Frontiers in Oncology</source><year>2022</year><elocation-id>5486</elocation-id><pub-id pub-id-type="pmcid">PMC9606570</pub-id><pub-id pub-id-type="pmid">36313703</pub-id><pub-id pub-id-type="doi">10.3389/fonc.2022.1019111</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmauch</surname><given-names>Benoît</given-names></name><name><surname>Romagnoni</surname><given-names>Alberto</given-names></name><name><surname>Pronier</surname><given-names>Elodie</given-names></name><name><surname>Saillard</surname><given-names>Charlie</given-names></name><name><surname>Maillé</surname><given-names>Pascale</given-names></name><name><surname>Calderaro</surname><given-names>Julien</given-names></name><name><surname>Kamoun</surname><given-names>Aurélie</given-names></name><name><surname>Sefta</surname><given-names>Meriem</given-names></name><name><surname>Toldo</surname><given-names>Sylvain</given-names></name><name><surname>Zaslavskiy</surname><given-names>Mikhail</given-names></name><etal/></person-group><article-title>A deep learning model to predict RNA-Seq expression of tumours from whole slide images</article-title><source>Nature Communications</source><year>2020</year><month>August</month><volume>11</volume><issue>1</issue><elocation-id>3877</elocation-id><pub-id pub-id-type="pmcid">PMC7400514</pub-id><pub-id pub-id-type="pmid">32747659</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-17678-4</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Yinxi</given-names></name><name><surname>Kartasalo</surname><given-names>Kimmo</given-names></name><name><surname>Weitz</surname><given-names>Philippe</given-names></name><name><surname>Ács</surname><given-names>Balázs</given-names></name><name><surname>Valkonen</surname><given-names>Masi</given-names></name><name><surname>Larsson</surname><given-names>Christer</given-names></name><name><surname>Ruusuvuori</surname><given-names>Pekka</given-names></name><name><surname>Hartman</surname><given-names>Johan</given-names></name><name><surname>Rantalainen</surname><given-names>Mattias</given-names></name></person-group><article-title>Predicting Molecular Phenotypes from Histopathology Images: A Transcriptome-Wide Expression-Morphology Analysis in Breast Cancer</article-title><source>Cancer Research</source><year>2021</year><month>October</month><volume>81</volume><issue>19</issue><fpage>5115</fpage><lpage>5126</lpage><pub-id pub-id-type="pmcid">PMC9397635</pub-id><pub-id pub-id-type="pmid">34341074</pub-id><pub-id pub-id-type="doi">10.1158/0008-5472.CAN-21-0482</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>Bryan</given-names></name><name><surname>Bergenstråhle</surname><given-names>Ludvig</given-names></name><name><surname>Stenbeck</surname><given-names>Linnea</given-names></name><name><surname>Abid</surname><given-names>Abubakar</given-names></name><name><surname>Andersson</surname><given-names>Alma</given-names></name><name><surname>Borg</surname><given-names>Åke</given-names></name><name><surname>Maaskola</surname><given-names>Jonas</given-names></name><name><surname>Lundeberg</surname><given-names>Joakim</given-names></name><name><surname>Zou</surname><given-names>James</given-names></name></person-group><article-title>Integrating spatial gene expression and breast tumour morphology via deep learning</article-title><source>Nature biomedical engineering</source><year>2020</year><volume>4</volume><issue>8</issue><fpage>827</fpage><lpage>834</lpage><pub-id pub-id-type="pmid">32572199</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>Minxing</given-names></name><name><surname>Su</surname><given-names>Kenong</given-names></name><name><surname>Li</surname><given-names>Mingyao</given-names></name></person-group><article-title>Leveraging information in spatial transcriptomics to predict super-resolution gene expression from histology images in tumors</article-title><source>bioRxiv</source><year>2021</year><fpage>2021</fpage><lpage>2111</lpage></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>Yuansong</given-names></name><name><surname>Wei</surname><given-names>Zhuoyi</given-names></name><name><surname>Yu</surname><given-names>Weijiang</given-names></name><name><surname>Yin</surname><given-names>Rui</given-names></name><name><surname>Yuan</surname><given-names>Yuchen</given-names></name><name><surname>Li</surname><given-names>Bingling</given-names></name><name><surname>Tang</surname><given-names>Zhonghui</given-names></name><name><surname>Lu</surname><given-names>Yutong</given-names></name><name><surname>Yang</surname><given-names>Yuedong</given-names></name></person-group><article-title>Spatial transcriptomics prediction from histology jointly through transformer and graph neural networks</article-title><source>Briefings in Bioinformatics</source><year>2022</year><volume>23</volume><issue>5</issue><elocation-id>bbac297</elocation-id><pub-id pub-id-type="pmid">35849101</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monjo</surname><given-names>Taku</given-names></name><name><surname>Koido</surname><given-names>Masaru</given-names></name><name><surname>Nagasawa</surname><given-names>Satoi</given-names></name><name><surname>Suzuki</surname><given-names>Yutaka</given-names></name><name><surname>Kamatani</surname><given-names>Yoichiro</given-names></name></person-group><article-title>Efficient prediction of a spatial transcriptomics profile better characterizes breast cancer tissue sections without costly experimentation</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><elocation-id>4133</elocation-id><pub-id pub-id-type="pmcid">PMC8904587</pub-id><pub-id pub-id-type="pmid">35260632</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-07685-4</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Xiao</given-names></name><name><surname>Mulay</surname><given-names>Onkar</given-names></name><name><surname>MacDonald</surname><given-names>Samual</given-names></name><name><surname>Kim</surname><given-names>Taehyun</given-names></name><name><surname>Werry</surname><given-names>Jason</given-names></name><name><surname>Simpson</surname><given-names>Peter T</given-names></name><name><surname>Roosta</surname><given-names>Fred</given-names></name><name><surname>Trzaskowski</surname><given-names>Maciej</given-names></name><name><surname>Nguyen</surname><given-names>Quan</given-names></name></person-group><article-title>Stimage: robust, confident and interpretable models for predicting gene markers from cancer histopathological images</article-title><source>bioRxiv</source><year>2023</year><fpage>2023</fpage><lpage>2105</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Ronald</given-names></name><name><surname>Pang</surname><given-names>Kuan</given-names></name><name><surname>Bader</surname><given-names>Gary D</given-names></name><name><surname>Wang</surname><given-names>Bo</given-names></name></person-group><article-title>Spatially resolved gene expression prediction from h&amp;e histology images via bi-modal contrastive learning</article-title><source>arXiv preprint</source><year>2023</year><elocation-id>arXiv:2306.01859</elocation-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>Kaiming</given-names></name><name><surname>Zhang</surname><given-names>Xiangyu</given-names></name><name><surname>Ren</surname><given-names>Shaoqing</given-names></name><name><surname>Sun</surname><given-names>Jian</given-names></name></person-group><source>Deep residual learning for image recognition</source><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><year>2016</year><fpage>770</fpage><lpage>778</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>Jia</given-names></name><name><surname>Dong</surname><given-names>Wei</given-names></name><name><surname>Socher</surname><given-names>Richard</given-names></name><name><surname>Li</surname><given-names>Li-Jia</given-names></name><name><surname>Li</surname><given-names>Kai</given-names></name><name><surname>Fei-Fei</surname><given-names>Li</given-names></name></person-group><source>Imagenet: A large-scale hierarchical image database</source><conf-name>2009 IEEE conference on computer vision and pattern recognition</conf-name><year>2009</year><fpage>248</fpage><lpage>255</lpage><conf-sponsor>Ieee</conf-sponsor></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eraslan</surname><given-names>Gökcen</given-names></name><name><surname>Simon</surname><given-names>Lukas M</given-names></name><name><surname>Mircea</surname><given-names>Maria</given-names></name><name><surname>Mueller</surname><given-names>Nikola S</given-names></name><name><surname>Theis</surname><given-names>Fabian J</given-names></name></person-group><article-title>Single-cell rna-seq denoising using a deep count autoencoder</article-title><source>Nature communications</source><year>2019</year><volume>10</volume><issue>1</issue><fpage>390</fpage><pub-id pub-id-type="pmcid">PMC6344535</pub-id><pub-id pub-id-type="pmid">30674886</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-07931-2</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>Yixiao</given-names></name><name><surname>Zhang</surname><given-names>Xiao</given-names></name><name><surname>Choi</surname><given-names>Ching Lam</given-names></name><name><surname>Cheung</surname><given-names>Ka Chun</given-names></name><name><surname>Zhao</surname><given-names>Peipei</given-names></name><name><surname>Zhu</surname><given-names>Feng</given-names></name><name><surname>Wang</surname><given-names>Xiaogang</given-names></name><name><surname>Zhao</surname><given-names>Rui</given-names></name><name><surname>Li</surname><given-names>Hongsheng</given-names></name></person-group><article-title>Self-distillation with batch knowledge ensembling improves imagenet classification</article-title><source>arXiv preprint</source><year>2021</year><elocation-id>arXiv:2104.13298</elocation-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Ting</given-names></name><name><surname>Kornblith</surname><given-names>Simon</given-names></name><name><surname>Norouzi</surname><given-names>Mohammad</given-names></name><name><surname>Hinton</surname><given-names>Geoffrey</given-names></name></person-group><source>A simple framework for contrastive learning of visual representations</source><conf-name>International conference on machine learning</conf-name><year>2020</year><fpage>1597</fpage><lpage>1607</lpage><conf-sponsor>PMLR</conf-sponsor></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>Alma</given-names></name><name><surname>Larsson</surname><given-names>Ludvig</given-names></name><name><surname>Stenbeck</surname><given-names>Linnea</given-names></name><name><surname>Salmén</surname><given-names>Fredrik</given-names></name><name><surname>Ehinger</surname><given-names>Anna</given-names></name><name><surname>Wu</surname><given-names>Sunny Z</given-names></name><name><surname>Al-Eryani</surname><given-names>Ghamdan</given-names></name><name><surname>Roden</surname><given-names>Daniel</given-names></name><name><surname>Swarbrick</surname><given-names>Alex</given-names></name><name><surname>Borg</surname><given-names>Åke</given-names></name><etal/></person-group><article-title>Spatial deconvolution of her2-positive breast cancer delineates tumor-associated cell type interactions</article-title><source>Nature communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>6012</elocation-id><pub-id pub-id-type="pmcid">PMC8516894</pub-id><pub-id pub-id-type="pmid">34650042</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-26271-2</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Janesick</surname><given-names>Amanda</given-names></name><name><surname>Shelansky</surname><given-names>Robert</given-names></name><name><surname>Gottscho</surname><given-names>Andrew D</given-names></name><name><surname>Wagner</surname><given-names>Florian</given-names></name><name><surname>Rouault</surname><given-names>Morgane</given-names></name><name><surname>Beliakoff</surname><given-names>Ghezal</given-names></name><name><surname>de Oliveira</surname><given-names>Michelli Faria</given-names></name><name><surname>Kohlway</surname><given-names>Andrew</given-names></name><name><surname>Abousoud</surname><given-names>Jawad</given-names></name><name><surname>Morrison</surname><given-names>Carolyn A</given-names></name><etal/></person-group><source>High resolution mapping of the breast cancer tumor microenvironment using integrated single cell, spatial and in situ analysis of FFPE tissue</source><year>2022</year><month>November</month></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Sunny Z</given-names></name><name><surname>Al-Eryani</surname><given-names>Ghamdan</given-names></name><name><surname>Roden</surname><given-names>Daniel</given-names></name><name><surname>Junankar</surname><given-names>Simon</given-names></name><name><surname>Harvey</surname><given-names>Kate</given-names></name><name><surname>Andersson</surname><given-names>Alma</given-names></name><name><surname>Thennavan</surname><given-names>Aatish</given-names></name><name><surname>Wang</surname><given-names>Chenfei</given-names></name><name><surname>Torpy</surname><given-names>James</given-names></name><name><surname>Bartonicek</surname><given-names>Nenad</given-names></name><etal/></person-group><article-title>A single-cell and spatially resolved atlas of human breast cancers</article-title><source>Nature genetics</source><year>2021</year><month>September</month><volume>53</volume><issue>9</issue><fpage>1334</fpage><lpage>1347</lpage><pub-id pub-id-type="pmcid">PMC9044823</pub-id><pub-id pub-id-type="pmid">34493872</pub-id><pub-id pub-id-type="doi">10.1038/s41588-021-00911-1</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otálora</surname><given-names>Sebastian</given-names></name><name><surname>Marini</surname><given-names>Niccoló</given-names></name><name><surname>Podareanu</surname><given-names>Damian</given-names></name><name><surname>Hekster</surname><given-names>Ruben</given-names></name><name><surname>Tellez</surname><given-names>David</given-names></name><name><surname>Van Der Laak</surname><given-names>Jeroen</given-names></name><name><surname>Müller</surname><given-names>Henning</given-names></name><name><surname>Atzori</surname><given-names>Manfredo</given-names></name></person-group><article-title>Stainlib: a python library for augmentation and normalization of histopathology h&amp;e images</article-title><source>bioRxiv</source><year>2022</year><fpage>2022</fpage><lpage>2105</lpage></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>Yiqing</given-names></name><name><surname>Luo</surname><given-names>Yulin</given-names></name><name><surname>Shen</surname><given-names>Dinggang</given-names></name><name><surname>Ke</surname><given-names>Jing</given-names></name></person-group><source>Randstainna: Learning stain-agnostic features from histology slides by bridging stain augmentation and normalization</source><conf-name>International Conference on Medical Image Computing and Computer-Assisted Intervention</conf-name><year>2022</year><fpage>212</fpage><lpage>221</lpage><publisher-name>Springer</publisher-name></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Mingxing</given-names></name><name><surname>Le</surname><given-names>Quoc</given-names></name></person-group><source>Efficientnet: Rethinking model scaling for convolutional neural networks</source><conf-name>International conference on machine learning</conf-name><year>2019</year><fpage>6105</fpage><lpage>6114</lpage><conf-sponsor>PMLR</conf-sponsor></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Ze</given-names></name><name><surname>Lin</surname><given-names>Yutong</given-names></name><name><surname>Cao</surname><given-names>Yue</given-names></name><name><surname>Hu</surname><given-names>Han</given-names></name><name><surname>Wei</surname><given-names>Yixuan</given-names></name><name><surname>Zhang</surname><given-names>Zheng</given-names></name><name><surname>Lin</surname><given-names>Stephen</given-names></name><name><surname>Guo</surname><given-names>Baining</given-names></name></person-group><source>Swin transformer: Hierarchical vision transformer using shifted windows</source><conf-name>Proceedings of the IEEE/CVF international conference on computer vision</conf-name><year>2021</year><fpage>10012</fpage><lpage>10022</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawaguchi</surname><given-names>Kenji</given-names></name><name><surname>Kaelbling</surname><given-names>Leslie Pack</given-names></name><name><surname>Bengio</surname><given-names>Yoshua</given-names></name></person-group><source>Generalization in Deep Learning</source><year>2022</year><month>December</month><fpage>112</fpage><lpage>148</lpage><elocation-id>arXiv:1710.05468 [cs, stat]</elocation-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Chiyuan</given-names></name><name><surname>Bengio</surname><given-names>Samy</given-names></name><name><surname>Hardt</surname><given-names>Moritz</given-names></name><name><surname>Recht</surname><given-names>Benjamin</given-names></name><name><surname>Vinyals</surname><given-names>Oriol</given-names></name></person-group><article-title>Understanding deep learning requires rethinking generalization</article-title><source>CoRR</source><year>2016</year><elocation-id>abs/1611.03530</elocation-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Yingdong</given-names></name><name><surname>Li</surname><given-names>Ming-Chung</given-names></name><name><surname>Konaté</surname><given-names>Mariam M</given-names></name><name><surname>Chen</surname><given-names>Li</given-names></name><name><surname>Das</surname><given-names>Biswajit</given-names></name><name><surname>Karlovich</surname><given-names>Chris</given-names></name><name><surname>Williams</surname><given-names>P Mickey</given-names></name><name><surname>Evrard</surname><given-names>Yvonne A</given-names></name><name><surname>Doroshow</surname><given-names>James H</given-names></name><name><surname>McShane</surname><given-names>Lisa M</given-names></name></person-group><article-title>Tpm, fpkm, or normalized counts? A comparative study of quantification measures for the analysis of rna-seq data from the nci patient-derived models repository</article-title><source>Journal of Translational Medicine</source><year>2021</year><month>June</month><volume>19</volume><issue>1</issue><fpage>269</fpage><pub-id pub-id-type="pmcid">PMC8220791</pub-id><pub-id pub-id-type="pmid">34158060</pub-id><pub-id pub-id-type="doi">10.1186/s12967-021-02936-w</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdar</surname><given-names>Moloud</given-names></name><name><surname>Pourpanah</surname><given-names>Farhad</given-names></name><name><surname>Hussain</surname><given-names>Sadiq</given-names></name><name><surname>Rezazadegan</surname><given-names>Dana</given-names></name><name><surname>Liu</surname><given-names>Li</given-names></name><name><surname>Ghavamzadeh</surname><given-names>Mohammad</given-names></name><name><surname>Fieguth</surname><given-names>Paul</given-names></name><name><surname>Cao</surname><given-names>Xiaochun</given-names></name><name><surname>Khosravi</surname><given-names>Abbas</given-names></name><name><surname>Acharya</surname><given-names>U Rajendra</given-names></name><etal/></person-group><article-title>A review of uncertainty quantification in deep learning: Techniques, applications and challenges</article-title><source>Information Fusion</source><year>2021</year><month>December</month><volume>76</volume><fpage>243</fpage><lpage>297</lpage></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Jun</given-names></name><name><surname>Yang</surname><given-names>Yu-Chen</given-names></name><name><surname>An</surname><given-names>Zhi-Jie</given-names></name><name><surname>Zhang</surname><given-names>Ming-Hui</given-names></name><name><surname>Fu</surname><given-names>Xue-Hang</given-names></name><name><surname>Huang</surname><given-names>Zou-Fang</given-names></name><name><surname>Yuan</surname><given-names>Ye</given-names></name><name><surname>Hou</surname><given-names>Jian</given-names></name></person-group><article-title>Advances in spatial transcriptomics and related data analysis strategies</article-title><source>Journal of Translational Medicine</source><year>2023</year><month>May</month><volume>21</volume><issue>1</issue><fpage>330</fpage><pub-id pub-id-type="pmcid">PMC10193345</pub-id><pub-id pub-id-type="pmid">37202762</pub-id><pub-id pub-id-type="doi">10.1186/s12967-023-04150-2</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fang</surname><given-names>Shuangsang</given-names></name><name><surname>Chen</surname><given-names>Bichao</given-names></name><name><surname>Zhang</surname><given-names>Yong</given-names></name><name><surname>Sun</surname><given-names>Haixi</given-names></name><name><surname>Liu</surname><given-names>Longqi</given-names></name><name><surname>Liu</surname><given-names>Shiping</given-names></name><name><surname>Li</surname><given-names>Yuxiang</given-names></name><name><surname>Xu</surname><given-names>Xun</given-names></name></person-group><article-title>Computational Approaches and Challenges in Spatial Transcriptomics</article-title><source>Genomics, Proteomics &amp; Bioinformatics</source><year>2023</year><month>February</month><volume>21</volume><issue>1</issue><fpage>24</fpage><lpage>47</lpage><pub-id pub-id-type="pmcid">PMC10372921</pub-id><pub-id pub-id-type="pmid">36252814</pub-id><pub-id pub-id-type="doi">10.1016/j.gpb.2022.10.001</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vahadane</surname><given-names>Abhishek</given-names></name><name><surname>Peng</surname><given-names>Tingying</given-names></name><name><surname>Sethi</surname><given-names>Amit</given-names></name><name><surname>Albarqouni</surname><given-names>Shadi</given-names></name><name><surname>Wang</surname><given-names>Lichao</given-names></name><name><surname>Baust</surname><given-names>Maximilian</given-names></name><name><surname>Steiger</surname><given-names>Katja</given-names></name><name><surname>Schlitter</surname><given-names>Anna Melissa</given-names></name><name><surname>Esposito</surname><given-names>Irene</given-names></name><name><surname>Navab</surname><given-names>Nassir</given-names></name></person-group><article-title>Structure-preserving color normalization and sparse stain separation for histological images</article-title><source>IEEE transactions on medical imaging</source><year>2016</year><volume>35</volume><issue>8</issue><fpage>1962</fpage><lpage>1971</lpage><pub-id pub-id-type="pmid">27164577</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafemeister</surname><given-names>Christoph</given-names></name><name><surname>Satija</surname><given-names>Rahul</given-names></name></person-group><article-title>Normalization and variance stabilization of single-cell rna-seq data using regularized negative binomial regression</article-title><source>Genome biology</source><year>2019</year><volume>20</volume><issue>1</issue><fpage>296</fpage><pub-id pub-id-type="pmcid">PMC6927181</pub-id><pub-id pub-id-type="pmid">31870423</pub-id><pub-id pub-id-type="doi">10.1186/s13059-019-1874-1</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Key Messages</title></caption><list list-type="bullet" id="L3"><list-item><p>We comprehensively compared the performance of existing methods for predicting spatial gene expression profiles from histology images</p></list-item><list-item><p>We assessed the roles of different algorithms, model architectures, and data processing pipelines to model performance</p></list-item><list-item><p>We performed extensive experiments to evaluate the generalization of the models on in-distribution and out-of-distribution spatial transcriptomics datasets</p></list-item><list-item><p>We proposed several strategies for improving existing models and empirically investigated their effectiveness</p></list-item></list></boxed-text><fig id="F1" position="float"><label>Fig. 1</label><caption><title><bold>a)</bold> Schematic overview of the general model architecture shared by the six existing methods that predict gene expression from H&amp;E images: ST-Net, HisToGene, Hist2ST, STimage, DeepSpaCE and BLEEP. The top diagram illustrates the common feed-forward architecture of regression-based methods, and the bottom displays the query-reference method BLEEP. <bold>b)</bold> Model performances on the Visium breast cancer and HER2+ datasets, showing the averaged LOOCV results. Box plots indicate the Pearson correlation coefficient (PCC) for highly variable genes (leftmost and rightmost), and marker genes (middle). <bold>c)</bold> Individual results of the LOOCV for each hold-out test sample. The top panel corresponds to HVGs for the HER2+ dataset; the bottom left and right, marker genes and HVGs for the Visium dataset, respectively.</title></caption><graphic xlink:href="EMS188425-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title><bold>a)</bold> Sample images in the Visium dataset and the dataset splits for assessing the generalization of methods. <bold>b)</bold> Color distributions in the Visium dataset, where the training set refers to the combined training and validation sets. <bold>c)</bold> Distributions of gene expression in the Visium dataset, showing the density (y-axis, frequency of the spots) corresponding to the number of genes detected per spot (x-axis). <bold>d)</bold> Assessment of generalization ability on in-distribution validation set and out-of-distribution (OOD) test set. Box plots show the correlation between predicted and true gene expression for each gene predicted (marker genes).</title></caption><graphic xlink:href="EMS188425-f002"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Summary of samples from the HER2+ Legacy and Visium datasets.</title></caption><table frame="box" rules="all"><thead><tr><th align="center" valign="top">Dataset</th><th align="center" valign="top">Patients</th><th align="center" valign="top">Tissues</th><th align="center" valign="top">Cohorts</th><th align="center" valign="top">Resolution</th><th align="center" valign="top">Spots</th><th align="center" valign="top">Genes</th></tr></thead><tbody><tr><td align="center" valign="top">HER2+</td><td align="center" valign="top">36</td><td align="center" valign="top">Frozen</td><td align="center" valign="top">He et al.</td><td align="center" valign="top">100 μm</td><td align="center" valign="top">13,620</td><td align="center" valign="top">11,871</td></tr><tr><td align="center" valign="top">Visium</td><td align="center" valign="top">6</td><td align="center" valign="top">Frozen</td><td align="center" valign="top">Wu et al.</td><td align="center" valign="top">55 μm</td><td align="center" valign="top">16,456</td><td align="center" valign="top">36,601</td></tr><tr><td align="center" valign="top">Visium</td><td align="center" valign="top">2</td><td align="center" valign="top">Frozen</td><td align="center" valign="top">10x</td><td align="center" valign="top">55 μm</td><td align="center" valign="top">7,785</td><td align="center" valign="top">36,601</td></tr><tr><td align="center" valign="top">Visium</td><td align="center" valign="top">1</td><td align="center" valign="top">FFPE</td><td align="center" valign="top">10x</td><td align="center" valign="top">55 μm</td><td align="center" valign="top">2,518</td><td align="center" valign="top">36,601</td></tr></tbody></table></table-wrap></floats-group></article>