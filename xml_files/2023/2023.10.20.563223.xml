<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190071</article-id><article-id pub-id-type="doi">10.1101/2023.10.20.563223</article-id><article-id pub-id-type="archive">PPR746254</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>LAVASET: Latent Variable Stochastic Ensemble of Trees. A novel ensemble method for correlated datasets</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kasapi</surname><given-names>Melpomeni</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Kexin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Ebbels</surname><given-names>Timothy M.D.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>O’Regan</surname><given-names>Declan P.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ware</surname><given-names>James S.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Posma</surname><given-names>Joram M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib></contrib-group><aff id="A1"><label>1</label>Section of Bioinformatics, Division of Systems Medicine, Department of Metabolism, Digestion, and Reproduction, Faculty of Medicine, Imperial College London, London, W12 0NN, UK</aff><aff id="A2"><label>2</label>National Heart &amp; Lung Institute, Imperial College London, London, W12 0NN, UK</aff><aff id="A3"><label>3</label>MRC London Institute of Medical Sciences, Imperial College London, London, W12 0HS, UK</aff><aff id="A4"><label>4</label>Royal Brompton &amp; Harefield Hospitals, Guy’s and St. Thomas’ NHS Foundation Trust, London, SW3 6NP, UK</aff><aff id="A5"><label>5</label>Program in Medical &amp; Population Genetics, Broad Institute of MIT &amp; Harvard, Cambridge, MA, US</aff><author-notes><corresp id="CR1"><bold>Correspondence:</bold> <email>jmp111@ic.ac.uk</email>, <email>mk218@imperial.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>25</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><sec id="S1"><title>Motivation</title><p id="P1">Random Forests (RFs) can deal with a large number of variables, achieve reasonable prediction scores, and yield highly interpretable feature importance values. As such, RFs are appropriate models for feature selection and further dimension reduction (DR). However, RFs are often not appropriate for correlated datasets due to their mode of selecting individual features for splitting. Addressing correlation relationships in high dimensional datasets is imperative for reducing the number of variables that are assigned high importance, hence making the DR most efficient. Here, we propose the LAtent VAriable Stochastic Ensemble of Trees (LAVASET) method that derives latent variables based on the distance characteristics of each feature and aims to incorporate the correlation factor in the splitting step.</p></sec><sec id="S2"><title>Results</title><p id="P2">Without compromising on performance in the majority of examples, LAVASET outperforms RF by accurately determining feature importance across all correlated variables and ensuring proper distribution of importance values. LAVASET yields mostly non-inferior prediction accuracies to traditional RFs when tested in simulated and real 1D datasets, as well as more complex and high-dimensional 3D datatypes. Unlike traditional RFs, LAVASET is unaffected by single ‘important’ noisy features (false positives), as it considers the local neighbourhood. LAVASET, therefore, highlights neighbourhoods of features, reflecting real signals that collectively impact the model’s predictive ability.</p><p id="P3">LAVASET is freely available as a standalone package from <ext-link ext-link-type="uri" xlink:href="https://github.com/melkasapi/LAVASET">https://github.com/melkasapi/LAVASET</ext-link>.</p></sec></abstract><kwd-group><kwd>random forest</kwd><kwd>machine learning</kwd><kwd>correlation</kwd><kwd>omics data</kwd></kwd-group></article-meta></front><body><sec id="S3" sec-type="intro"><title>Introduction</title><p id="P4">Random Forest classifiers (RFs) are frequently used to analyse biological data for prediction and feature selection tasks. RFs can deal with a large number of variables, achieve reasonable prediction scores, and yield highly interpretable feature importance values (<xref ref-type="bibr" rid="R1">1</xref>). As such, they are appropriate models for feature selection and further dimension reduction (DR) for integrated datasets. The premise of the original RF algorithm is to assemble an ensemble of trees that complement each other and increase variability of predictor selection (<xref ref-type="bibr" rid="R2">2</xref>). However, each node and subsequent split still only consider one predictor variable, limiting both the predictive ability and correct feature importance assignment in complex biological settings that include correlated features.</p><p id="P5">Nguyen et al.(<xref ref-type="bibr" rid="R2">2</xref>) have recently developed a new information criterion statistic to evaluate the contribution of features to the predictive ability of the model. It comprises of different categories of probabilities that assess the feature’s proximity to the target class and the complexity of the relationship with the given class. In addition, permutation-based feature importance has been extensively studied in RFs. It has been demonstrated that there is some level of bias in the assignment of feature importance when there exists collinearity between features that are both associated with the target outcome (<xref ref-type="bibr" rid="R3">3</xref>).</p><p id="P6">Few techniques have been proposed for enhancing feature importance calculations in datasets with highly correlated variables. The Boruta algorithm (<xref ref-type="bibr" rid="R4">4</xref>) uses a ‘shadow’ feature approach where it duplicates the original dataset and shuffles the feature values. Boruta trains a classifier on the enhanced dataset and assigns a value of importance to each of the features. Then, the algorithm performs a number of iterations where it compares the importance of the real features to the shadow (shuffled) features, while recording how many times the original features outperform the shadow ones (hits). A threshold is set and depending on the number of hits, the respective feature is assigned an importance or removed from the matrix of further iterations. Boruta, in reality, combines permutation importance, by shuffling the original features, with recursive feature elimination (<xref ref-type="bibr" rid="R5">5</xref>), by iteratively considering and removing features that do not reach a threshold.</p><p id="P7">These methods are efficient in removing noisy features that might not reflect real signals, especially by eliminating these through iterations. However, they are not sensitive in picking all relevant features when these are collinear. Addressing relationships between collinear features in high dimensional datasets is imperative for reducing the number of features that are assigned high importance and thereby making the DR more efficient. Here, we propose a novel method termed LAtent VAriable Stochastic Ensemble of Trees (LAVASET) that derives latent variables based on the distance characteristics of each feature and thereby incorporates the correlation factor in the splitting step. Hence, it inherently groups correlated features and ensures similar importance assignment for these. Distance characteristics for the features can include the feature’s adjacent points in a 1D spectrum, adjacent features of time-series data, or spatial distance in 3D structures among other examples. In this context, LAVASET addresses a major limitation in the interpretation of feature importance of RFs when the data are collinear, such as is the case for spectroscopic and imaging data.</p></sec><sec id="S4" sec-type="methods"><title>Methods</title><sec id="S5"><label>A</label><title>Datasets</title><p id="P8">We demonstrate the LAVASET algorithm (detailed below) on four different datasets with feature importances calculated as a result of a prediction/classification problem between disease and healthy control groups or simulated groups.</p><sec id="S6"><label>A.1</label><title>Irritable Bowel Syndrome - faecal metabolomics (1D)</title><p id="P9">The Maastricht University Irritable Bowel Syndrome (MIBS) cohort includes human faecal water samples analysed with <sup>1</sup>H Nuclear Magnetic Resonance (NMR) spectroscopy for 267 participants (146 IBS patients; 121 healthy controls (HCs)). Details on demographics, sample collection, and data acquisition can be found in 6. Unlike the original publication, we used the full NMR spectrum (digitised to a total of 18,000 features) following the removal of the internal standard and the water region, and baseline correction.</p><p id="P10">To test LAVASET’s ability in capturing relevant features, we also created simulated groups from the MIBS cohort dataset. The groups were generated by using 2 (uncorrelated) compounds that each have 2 multiplets, with signals spread out across the length of the spectrum. The compounds used are metabolites ethanol, with peaks at 1.17-1.20 and 3.64-3.68 ppm, and uracil with peaks at 5.79-5.81 and 7.53-7.56 ppm.</p></sec><sec id="S7"><label>A.2</label><title>Diabetes - urinary metabolomics (1D)</title><p id="P11">Human urinary metabolomics data from individuals with type-2 diabetes mellitus (T2DM), freely available from Metabolights (MT-BLS1), were used as an additional test cohort. Prior work on this dataset has shown higher classification accuracy compared to IBS data. A total of 84 samples were collected, consisting of 12 healthy volunteers with data at 7 time points, and 30 individuals with T2DM with data collected at 1-3 time points (total of 50 spectra). These were analysed by <sup>1</sup>H-NMR spectroscopy to evaluate the urine profiles between T2DM and HCs. Metabolite identification was performed by PLS-DA models previously, as described by the authors in 7. The raw data were downloaded from Metabolights and processed to standardise each spectrum to 18,000 data points. The water and internal standard regions were removed from the spectrum with the remaining points used for the modelling.</p></sec><sec id="S8"><label>A.3</label><title>Acute myocardial infarction - electrocardiogram (1D)</title><p id="P12">Electrocardiogram (ECG) data from the Physikalisch-Technische Bundesanstalt (PTB) dataset (<xref ref-type="bibr" rid="R8">8</xref>) were downloaded from PhysioNet (<xref ref-type="bibr" rid="R9">9</xref>). We extracted ECG data from individuals with acute myocardial infarction (MI) and compared this against no acute MI. We excluded all data without a reason for admission or with an unknown diagnosis. This resulted in 175 control and 346 acute MI ECGs. The individual ECGs were processed to correct signal drifts. An average cardiac cycle was extracted for each individual that was normalized to 750 data points (0.75s). We used the 3 Frank leads (vector cardiogram, VCG) as input to the algorithm and visualise the feature importance in the conventional 12-leads by making use of the (absolute) Kors regression transformation (<xref ref-type="bibr" rid="R10">10</xref>) for the 8 independent leads.</p></sec><sec id="S9"><label>A.4</label><title>Hypertrophic Cardiomyopathy - CMR imaging (3D)</title><p id="P13">To test LAVASET’s performance on high dimensional, spatial, 3D datasets, we used data meshes derived from cardiac magnetic resonance (CMR) imaging of the human left ventricle. Segmentation of these images (to produce the meshes) was performed using a deep learning framework developed by collaborators (<xref ref-type="bibr" rid="R11">11</xref>). Measurements of myocardial wall thickness were calculated along radial segments that connected the inner endocardial and outer epicardial surfaces (<xref ref-type="bibr" rid="R12">12</xref>). This process produced a 46,808 <italic>×</italic> 4 matrix, where for each of the 46,808 points there is a value for the x, y, and z coordinates of the ventricle, and the wall thickness at that point. These segmentations were performed on images in the UK Biobank dataset (<xref ref-type="bibr" rid="R13">13</xref>) and an in-house Hypertrophic Cardiomyopathy (HCM) cohort (Royal Brompton Hospital Cardiovascular Biobank (<xref ref-type="bibr" rid="R14">14</xref>)). The total number of samples used here was 1,273, consisting of 634 hypertrophic cardiomyopathy (HCM) patients and 639 demographically-matched HCs.</p></sec></sec><sec id="S10"><label>B</label><title>Approach</title><p id="P14">LAVASET operates given a number of prerequisites and hyperparameters that can be optimised (<xref ref-type="fig" rid="F1">Figure 1</xref>). A user-specified distance matrix is calculated to select the k closest feature points to the feature of interest (FOI), which form the FOI neighbourhood. This FOI neighbourhood submatrix is defined as: <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>F</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> where <italic>M</italic> denotes the matrix containing all the input features and the set 𝒩(1, <italic>k</italic>) represents the indices of the <italic>k</italic> features in <italic>M</italic> that are closest to FOI (based on the user-specified distance matrix), including the index for FOI itself. The user specifies the maximum number of features to consider for each split (with default the square root of the total number of features), and these are randomly selected from the entire dataset in each step. For each selected FOI, the first left singular vector (PC1) of the respective FOI neighbourhood is calculated. We calculate this via Singular Value Decomposition (SVD) of the sub-matrix <italic>FOIn</italic>: <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>F</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mtext>Σ</mml:mtext><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P15">Here, <italic>FOIn</italic> is the scaled FOI neighbourhood for the selected VOI, <italic>U</italic> is the matrix of left singular vectors, Σ is a diagonal matrix containing singular values, and <italic>V</italic> <sup><italic>T</italic></sup> (or <italic>V</italic> transposed) is the matrix of right singular vectors. The PC1 or first left singular vector is the first vector in matrix <italic>U</italic>, and this is now the latent variable for the FOIn. Loadings for each latent variable are calculated as: <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mi>F</mml:mi><mml:mi>O</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:math></disp-formula> where the first left singular vector <italic>U</italic><sub>1</sub> is multiplied by the original <italic>FOIn</italic> sub-matrix. The input matrix for determining the best split will now consist of the latent variable values instead of the original feature values. The best-split variable and value are evaluated by the traditional Gini index method by deducting the sum of the squared probabilities of each class from one. Once the split occurs, the Gini gain is calculated for the selected latent variable and node by subtracting the sum of the Gini index weights of the two child nodes from the parent node. This is repeated recursively until all samples are split into pure leaf nodes, similar to the classic CART algorithm (<xref ref-type="bibr" rid="R15">15</xref>).</p></sec><sec id="S11"><label>C</label><title>Feature importance calculation</title><p id="P16">Feature importance scores are calculated for each feature by weighing its contribution to the PC score (left singular vector). Specifically, for every selected feature where the PC score is calculated, the loadings vector of the score (with a shape equaling the number of neighbours considered) is calculated as shown in <xref ref-type="disp-formula" rid="FD3">Equation 3</xref>. The loadings vector <italic>L</italic> is then multiplied by the Gini gain value assigned to the selected latent variable. This results in a feature importance score not only for the FOI but also for its neighbours.</p><p id="P17">The LAVASET algorithm is designed to allow for multiple variations in the calculation of a feature’s importance after model construction. LAVASET outputs for each feature<sub><italic>i</italic></sub> a vector of values shown in <xref ref-type="disp-formula" rid="FD4">Equation 4</xref>, where <inline-formula><mml:math id="M4"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> represents the summation over all trees from 1 to <italic>T</italic> of the count of times a feature is evaluated in each tree. Similarly, <inline-formula><mml:math id="M5"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M6"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> represent the total counts of times a feature is selected for a split and the accumulated Gini values in each tree, respectively. All features of the original matrix <italic>M</italic> are equally <italic>evaluated</italic> for a split with a frequency that follows a Gaussian distribution. The subset of features <italic>selected</italic> for splitting a node are those assigned a feature importance. <disp-formula id="FD4"><label>(4)</label><mml:math id="M7"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mspace width="0.2em"/><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P18">From the values in <xref ref-type="disp-formula" rid="FD4">Equation 4</xref> we can calculate a normalised Gini feature importance by dividing by the sum of all importances. We also evaluate the ratio of the number of times a feature is selected over the count of times it is considered for a split, and incorporate this in our feature importance final value. Using the same output, another use case would be to compare the ratio of Gini values over the count of times a feature is selected for split, which can give an idea of the value magnitude assigned to a feature at a given split. Results presented in this paper utilise the normalised feature importance multiplied by the ratio of <inline-formula><mml:math id="M8"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> over the <inline-formula><mml:math id="M9"><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> for each feature. This is shown in <xref ref-type="disp-formula" rid="FD2">equation 2</xref> where <italic>importance</italic>(i) is the importance of the feature i, T is the total number of trees, <italic>importance</italic>(i, t) represents the Gini importance of the feature <italic>i</italic> in the tree <italic>t</italic>, and <italic>F</italic> is the total number of features. The sum in the numerator of the first fraction goes over all the trees from 1 to <italic>T</italic> for the feature <italic>i</italic>. The sum in the denominator of the first fraction goes over all the trees and all the features (<italic>f</italic> represents each feature in the feature set), which normalises the Gini of the feature <italic>i</italic>. The ratio of the sums in the second fraction measures the frequency with which feature <italic>i</italic> was selected when it was evaluated. <disp-formula id="FD5"><label>(5)</label><mml:math id="M10"><mml:mrow><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>F</mml:mi></mml:msubsup><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>g</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p></sec><sec id="S12"><label>D</label><title>Model evaluation</title><p id="P19">Standard metrics (accuracy, precision, recall, F1-score) are used to compare the classification performance of RFs and LAVASET. For both the simulated and MIBS cohort inputs, LAVASET and RF were run 20 distinct times (20 run pairs with 100 and 1,000 trees, respectively). Identical random state seeds were assigned per run pair for the sample bootstrapping, to account for the randomness between the comparisons. The data were split into training and test sets (80% and 20% respectively), with the sets always being kept identical across the runs. Mean accuracy, precision, recall and F1-scores are reported for the 20 runs.</p><p id="P20">We performed a grid search optimising the number of trees and neighbours for the MIBS cohort. Tree values ranged from 100 to 10,000 and neighbours from 1 to 50, and all possible combinations were evaluated. The optimal value for the number of trees for LAVASET was 1,000, this model is referred to as LAVASET-1K. We compare LAVASET-1K to two RF models: the first is an RF with the same number of 1,000 trees, referred to RF-1K hereafter, and the second is an RF that runs for the same amount of computational time as LAVASET. We evaluated the number of RF trees that can be fit in the same amount of time as LAVASET-1K required. LAVASET-1K fits 1,000 trees for 18 neighbours (optimal for MIBS cohort) in approximately 11 minutes (on an HP Z6 G4 workstation with 16-core Intel(R) Xeon(R) Silver 4110 CPU @ 2.10GHz with 128GB RAM), a classic RF can include approximately 41,000 trees in the same amount of time (referred to as the RF-41K model).</p><p id="P21">Precision and recall scores are also calculated as metrics for evaluating the peak coverage of the feature importance performance. In the simulated dataset, the peak points used to create the two distinct groups are assigned as the ground truth (positive designation) for the peak coverage. To evaluate the specificity of LAVASET, we include points outside the peak (equal to the k neighbours assigned) in the calculations of precision and recall. These serve as the true negative designations. The threshold for positive or negative designation in these calculations is whether the point has been assigned a feature importance value (feature importance&gt;0) or not (feature importance=0).</p><p id="P22">The neighbours for the VCG data were calculated on the basis of the time of the cardiac cycle. I.e. selecting Frank’s lead x for variable (time) i also includes the other 2 leads (y, z) at time i. Including more neighbours takes place in steps of 6 (x, y and z for both i ± 1). Where the first and last time points are also considered neighbours at i ± 1.</p><p id="P23">Finally, we assessed LAVASET’s performance on feature extraction for DR, specifically for the 3D CMR dataset. Due to computational constraints, the 3D dataset was tested on 100 and 200 neighbours and 150,000 trees. The neighbours were decided by the x, y, and z coordinates (spatial distance) in an iterative manner by considering the 100 neighbours (for each point) that remain neighbours for over half of the 1,273 samples. The most important features (above the 50% importance value threshold) were selected for LAVASET and RF models (both 150,000 trees), and for each set, we performed further DR and clustering to evaluate how the clusters separated the HCM and HC samples. Uniform manifold approximation and projection (UMAP) and k-means clustering with k=2 (expected number of groups) were used in both cases (<xref ref-type="bibr" rid="R16">16</xref>). UMAP components and k-means transformations were evaluated for 20 different random state restarts to ensure robustness. Each restart produced 2 clusters that were scored by the standard metrics, to determine how closely they matched the true sample classifications.</p></sec></sec><sec id="S13" sec-type="results"><title>Results</title><sec id="S14"><label>E</label><title>Simulated NMR dataset</title><p id="P24">LAVASET yields a non-inferior prediction accuracy compared with traditional RFs when predicting the two simulated groups in the NMR dataset (accuracy: 0.859±0.027 LAVASET, 0.823±0.021 RF, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5</xref>). As shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 6</xref>, the RF method (panels B and D) fails to identify all individual features relating to the same peak(s) and instead only identifies a subset of these. In contrast, LAVASET not only does identify correctly the simulated metabolites, but also assigns appropriate importance values to all or most of the correlated points that encompass a peak. Specifically, for ethanol’s CH<sub>2</sub> peak shown in <xref ref-type="fig" rid="F2">Figure 2A</xref> there is 92% precision and 100% recall in capturing the peak points. For the CH<sub>3</sub> peak precision is 77% and recall 95%. In contrast for the RF model, we see much lower recalls (CH<sub>2</sub> peak: 0.36, CH<sub>3</sub> peak: 0.18) but 100% precision given that the model simply does not capture almost any points on or around the peak. For uracil, the second multi-peak metabolite used to create the simulated dataset, we see that LAVASET has 100% recall in both doublets and 92% and 80% precision respectively. RF underperforms in capturing all the points of these metabolites as well with recalls at 32% and 47%.</p></sec><sec id="S15"><label>F</label><title>MIBS cohort</title><p id="P25">After a grid search evaluation to identify the optimal parameters for the number of neighbours and trees, the highest scoring accuracy was achieved by 1,000 trees and 18 neighbours (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 7</xref>). Average accuracy values overall dropped after considering more than 20 neighbours and there were no significant differences when taking more than 1,000 trees for the specific dataset. <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8A</xref> presents the optimal number of neighbours for the 1,000 trees with a clear peak being displayed on the graph, while the drop after 20 neighbours is also evident there. LAVASET yields similar results when classifying IBS vs HC in the MIBS cohort (only for this cohort, losing 1% in mean accuracy in comparison to RF). Across 20 runs the average accuracy score for LAVASET is 0.68±0.02, for RF-1K 0.69±0.01, and for RF-41K 0.68±0.02. <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8B</xref> boxplots represent these accuracy values, along with precision, recall, and F1 score values. The value ranges shown for the RF-41K model suggest that when running 41,000 trees there is potential for over-fitting, given that the values across each of the 20 runs are almost identical. For the 4 different performance metrics the error bars of LAVASET vs RF-1K and LAVASET vs RF-41K overlap, indicating LAVASET is non-inferior to either RF model (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 8B</xref>).</p><p id="P26">The most pronounced differences between LAVASET and RF are evident when looking at the feature importance. The ability of LAVASET to capture the entirety of peaks attributed to a metabolite surpasses the RF feature evaluation which merely captures less than half of the points that encompass the peak. <xref ref-type="fig" rid="F2">Figure 2</xref> shows the relevant peaks of previously identified metabolites valine (A, B, C) and 2-methylproline (D, E, F). These two metabolites have been previously associated with separating IBS from HC patients, by showing high feature importance in classification models using Support Vector Classifiers (<xref ref-type="bibr" rid="R6">6</xref>). Panels A and D show clearly how LAVASET is able to assign importance values to all points of the valine and 2-methylproline peaks, respectively. The dashed gray lines indicate the previously identified point ranges for the specific metabolites peaks, which present a ground truth for the peak, but can sometimes be affected by small shifts or missing points on the sides of a given peak. In the case of 2-methylproline we notice that LAVASET is also capturing and assigning relatively high importance on points to the left side of the previously-known peak. These points, however, when visualised on the spectrum appear to be part of the rest of the peak and LAVASET only assigns an importance to the points up to where the next peak is starting, without including that next non-related peak. This can also be seen on the right-side of valine in <xref ref-type="fig" rid="F2">Figure 2A</xref>, however in this case importance is relatively lower, presenting a gradient trend as we are reaching the end of the visualised peak.</p><p id="P27">RF, on the other hand, assigns feature importance to sporadic points of the peak, with no real continuance as to the values of importance (valine, <xref ref-type="fig" rid="F2">Figure 2B</xref>, red point indicating high importance next to light blue point indicating more than half of an importance value). Even in the case of the 41K trees we see that the points selected remain almost the same as in the 1K trees, suggesting that an RF cannot reach LAVASET’s ability in capturing whole peaks, even if the number of trees increases 41-fold. Overall, LAVASET’s output is quite stable and not affected significantly by small changes in the number of neighbours. Peak coverage is equivalent when looking at 10-16 neighbours, with the main differences occurring in the value of importance to each point (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 9</xref>).</p></sec><sec id="S16"><label>G</label><title>MTBLS1 cohort</title><p id="P28">LAVASET was further tested on the MTBLS1 T2DM cohort, as described in <xref ref-type="sec" rid="S4">Methods</xref>, to ensure that performance stability and feature importance interpretation remain effective in other cohorts. Consistent with previous examples, LAVASET presents non-inferior results to RF (LAVASET accuracy: 0.82±0.01, RF accuracy: 0.77±0.02, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 10</xref>) and manages to capture the metabolites pre-identified by (<xref ref-type="bibr" rid="R7">7</xref>). Metabolite peak capturing by LAVASET is again superior to RF, by encompassing if not all, most points and attributing feature importance values more equally. Results are expanded in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 11</xref>.</p></sec><sec id="S17"><label>H</label><title>ECG data</title><p id="P29">We assessed LAVASET’s capabilities on more complex data types where it is common practice to use transformations of the raw reading data to infer further information. An example of such data types is transforming Electrocardiogram (ECG) readings to Vectorcardiogram (VCG) inputs. For this task, we used the ECG data from the Physikalisch-Technische Bundesanstalt (PTB) dataset (<xref ref-type="bibr" rid="R8">8</xref>), as described in <xref ref-type="sec" rid="S4">Methods</xref>. LAVASET-1K performed similarly to the RF-1K in this dataset, with accuracy values showing identical results across 20 runs (0.81±0.01, <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 12</xref>) and other metrics having only small differences (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 12</xref>). <xref ref-type="fig" rid="F3">Figure 3</xref> shows the results when taking 20 nearest neighbours (across the time axis as described in <xref ref-type="sec" rid="S4">Methods</xref>) and 1,000 trees. LAVASET is able to capture peaks and anomalies across the ECGs that indicate differences between the HC and MI cases. Given the nature of the dataset and the idiosyncrasies of different MI cases based on the location of the infraction, setting a binary classification by incorporating all types of MI as one target will only capture important features related to all these different MI phenotypes.</p></sec><sec id="S18"><label>I</label><title>CMR Imaging</title><p id="P30">Both the LAVASET and RF models demonstrate equivalent values across all metrics. LAVASET shows an accuracy of 0.878, precision of 0.837, recall of 0.904 and F1-score of 0.869. RF shows an accuracy of 0.875, precision of 0.916, recall of 0.851, and F1-score of 0.882. Given these high values, we are confident about the use of both models for feature extraction, as means to DR. <xref ref-type="fig" rid="F4">Figure 4</xref> shows the results on 100 neighbours and 150,000 trees. LAVASET is able to assign importance values to a larger surface area of the left ventricle, while also demonstrating the most important areas. RF, on the other hand, picks patches of the left ventricle as most important, disregarding other anatomical parts that could be important. To test how informative these selected features are, we used them as input in further DR and clustering via UMAP and k-means. <xref ref-type="table" rid="T1">Table 1</xref> shows the clustering performance across 20 iterations of UMAP and k-means, indicating that the features selected by LAVASET perform constantly better than those selected by RF. We also highlight that in this specific dataset, feature extraction improves the models considerably, given the significantly lower performance observed when taking all features as input.</p></sec></sec><sec id="S19" sec-type="discussion"><title>Discussion</title><p id="P31">We have presented a novel ensemble learning method that aims to enhance variable importance and ensure that correlated features are evaluated appropriately and not independently. LAVASET produces non-inferior performance results to traditional RFs in all but one of the examples tested above, and in both simulated and real datasets. While not sacrificing performance in most examples, it is able to assign feature importances in a superior way, by not missing features that are as ‘important’ by means of correlation, and ensuring that importance values are correctly distributed. In contrast, we have shown that RF allocates feature importance to isolated points, whether that is peak points in a spectrum or points on a 3D mesh, without any clear consistency of the magnitude of importance. Even when substantially increasing the number of trees in RF, the selection of points and assignment of importance are nearly identical to those selected with the least number of trees, and still do not capture the points selected by LAVASET.</p><p id="P32">The motivation behind developing LAVASET stems from the idea of enhancing traditional RFs, in a manner similar to how Group Lasso enhances the Lasso algorithm. In the main premise of Group Lasso, we also assume that there are groups of features that are expected to have similar effects or are related to each other. By penalising the sum of the absolute values (L1 norm) of the coefficients within each group, Group Lasso encourages the model to select entire groups of features together or exclude them altogether (<xref ref-type="bibr" rid="R17">17</xref>). Like in Group Lasso, LAVASET is particularly useful when dealing with high-dimensional data where groups of features exhibit similar importance or are structured in some meaningful way. This was evident by the variety of datasets we tested, where the number of features ranged from 18,000 to 46,000. In this high-dimensional context, LAVASET exhibits stability in its output and remains relatively unaffected by minor variations in the number of neighbours comprising the groups. However, Group Lasso requires non-overlapped groups of features, whereas in LAVASET this can be varied. In fact, in LAVASET, different FOIs can have different numbers of neighbours to increase flexibility. Likewise, LAVASET emulates the kernel filter in convolutional neural networks (CNNs) in that it combines multiple features into a single output (for splitting in LAVASET), however, it does so without condensing the output and attributing the feature importance across the initial features. Other work has investigated the relations between individual features in terms of the similarity of performance at different splits. This methodology is able to discern correlations between individual features, however the mutual forest impact is constrained to evaluating pairs of features only (<xref ref-type="bibr" rid="R18">18</xref>). LAVASET can, in theory, be combined with this to perform an a posteriori analysis of evaluating correlations between groups of latent variables. LAVASET’s main limitation derives from the requirement of defining the aforementioned ‘groups’ (akin to the kernel size in CNNs). This parameter is user-specified and assumes an understanding of the relationship between the variables. In the examples we presented here, this relationship is defined and assigned by distance, whether that is 1D distance across a spectrum, 1D across time-series data, or 3D spatial distance. The influence of the neighbourhood definition is evident, especially in the 3D CMR example. <xref ref-type="fig" rid="F4">Figure 4</xref> clearly shows that the string-like pattern of importances calculated by LAVASET is predominantly driven by the assignment of neighbours for each FOI (<xref ref-type="fig" rid="F4">Figure 4F</xref>). This inherent limitation, however, is what enables LAVASET’s flexibility in creating the groups of neighbours. Distance is only one of the metrics that can be employed. A few other potential examples include genomic distance (combine SNPs via linkage disequilibrium), mass spectrometry isotope patterns (proteomics, metabolomics), or hierarchical relationships (e.g. taxonomy of microbiota). This attribute renders LAVASET more versatile than other similar methods, while giving the user the ability to tailor the algorithm to their specific needs. Hence, it is applicable to a wide variety of datasets and biological questions.</p><p id="P33">This flexibility is also translated to LAVASET’s code implementation. The algorithm also exploits parallelisation in order to speed up computations. The body of the code is written in Python 3.10, while utilizing established C++ scripts for efficiency. Given the nature of the code and algorithm, LAVASET can be run in batches, if needed, or can be easily altered to incorporate additional metrics to distance.</p><p id="P34">To enhance and expand LAVASET’s capabilities, we are working on incorporating the gradient boosting algorithm as one of our built-in additional components. This will extend LAVASET’s core methodology to other ensemble methods and benefit from iterative learning and the specific advantages of boosting trees.</p></sec><sec id="S20" sec-type="conclusions"><title>Conclusion</title><p id="P35">We have presented LAVASET, a novel ensemble method capable to improve feature interpretability by selecting relevant groups of features instead of individual features. Its novel functionality is most useful in datasets with correlations between features. In cases where this is missing from the data, then traditional RFs are more appropriate. LAVASET offers interoperability to the user both by the structure of its code and via the neighbours parameter. It can be applied to almost all omics data types to identify all relevant known or unknown important features and effectively perform feature extraction for DR.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS190071-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d29aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S21"><title>Acknowledgements</title><p>Conceptualisation: JMP and MK. Methodology: JMP, MK and KX. Investigation: MK, JMP and KX. Visualisation: MK and JMP. Writing - original draft: MK and JMP. Writing - editing: MK, JMP, TE, KX, JSW and DPO’R. Supervision: JMP, TE and JSW. We thank Zlatan Mujagic (<xref ref-type="bibr" rid="R6">6</xref>) for providing the raw MIBS cohort NMR data, the DPO’R group for supplying the CMR images, and Sanjay Prasad for the RBH CMR data.</p><sec id="S22"><title>Funding</title><p>This research has been conducted using the UK Biobank Resource under Application Numbers 47602 (Ware - Understanding the genetic architecture of inherited cardiovascular conditions (ICCs) and cardio-renal disease), 40616 (O’Regan - Machine learning discovery of genotype-phenotype associations in cardiovascular science) and 18545 (Matthews - Biobank Brain and Cardiac Mutual Risk Indexing (BBC MRI) study). MK is supported by a Wellcome Trust PhD Studentship in Basic Science (220119/Z/20/Z). JMP was supported by Health Data Research (HDR) UK and the Medical Research Council via a Rutherford Fund Fellowship (MR/S004033/1). JW is supported by Medical Research Council (UK), British Heart Foundation [RE/18/4/34215], and the NIHR Imperial College Biomedical Research Centre. DPO’R is supported by the Medical Research Council (MC_UP_1605/13); National Institute for Health Research (NIHR) Imperial College Biomedical Research Centre; and the British Heart Foundation (RG/19/6/34387, RE/18/4/34215). The views expressed in this work are those of the authors and not necessarily those of the funders. TE gratefully acknowledges support from UKRI BBSRC grants BT/T007974/1 and BB/W002345/1, and EC grants 100173062 and 101079370. For the purpose of Open Access, the authors have applied a Creative Commons attribution (CC BY) licence to any author accepted manuscript version arising.</p></sec></ack><ref-list><title>Bibliography</title><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>Leo</given-names></name></person-group><article-title>Random forests</article-title><source>Mach Learn</source><year>2001</year><volume>45</volume><day>10</day><fpage>5</fpage><lpage>32</lpage><comment>ISSN 08856125</comment><pub-id pub-id-type="doi">10.1023/a:1010933404324</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname><given-names>Jean-Michel</given-names></name><name><surname>Jézéquel</surname><given-names>Pascal</given-names></name><name><surname>Gillois</surname><given-names>Pierre</given-names></name><name><surname>Silva</surname><given-names>Luisa</given-names></name><name><surname>Ben Azzouz</surname><given-names>Faouda</given-names></name><name><surname>Lambert-Lacroix</surname><given-names>So-phie</given-names></name><name><surname>Juin</surname><given-names>Philippe</given-names></name><name><surname>Campone</surname><given-names>Mario</given-names></name><name><surname>Gaultier</surname><given-names>Aurélie</given-names></name><name><surname>Moreau-Gaudry</surname><given-names>Alexandre</given-names></name><name><surname>Antonioli</surname><given-names>Daniel</given-names></name></person-group><article-title>Random forest of perfect trees: concept, performance, applications and perspectives</article-title><source>Bioinformatics</source><year>2021</year><volume>37</volume><day>8</day><fpage>2165</fpage><lpage>2174</lpage><comment>ISSN 1367-4803</comment><pub-id pub-id-type="pmcid">PMC8352507</pub-id><pub-id pub-id-type="pmid">33523112</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btab074</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicodemus</surname><given-names>Kristin K</given-names></name><name><surname>Malley</surname><given-names>James D</given-names></name><name><surname>Strobl</surname><given-names>Carolin</given-names></name><name><surname>Ziegler</surname><given-names>Andreas</given-names></name></person-group><article-title>The behaviour of random forest permutation-based variable importance measures under predictor correlation</article-title><source>BMC Bioinformatics</source><year>2010</year><volume>11</volume><day>2</day><fpage>1</fpage><lpage>13</lpage><comment>ISSN 14712105</comment><pub-id pub-id-type="pmcid">PMC2848005</pub-id><pub-id pub-id-type="pmid">20187966</pub-id><pub-id pub-id-type="doi">10.1186/1471-2105-11-110</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kursa</surname><given-names>Miron B</given-names></name><name><surname>Rudnicki</surname><given-names>Witold R</given-names></name></person-group><article-title>Feature selection with the boruta package</article-title><source>Journal of Statistical Software</source><year>2010</year><volume>36</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.18637/jss.v036.i11</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guyon</surname><given-names>Isabelle</given-names></name><name><surname>Weston</surname><given-names>Jason</given-names></name><name><surname>Barnhill</surname><given-names>Stephen</given-names></name><name><surname>Vapnik</surname><given-names>Vladimir</given-names></name></person-group><article-title>Gene selection for cancer classification using support vector machines</article-title><source>Machine Learning</source><year>2002</year><volume>46</volume><issue>1/3</issue><fpage>389</fpage><lpage>422</lpage><pub-id pub-id-type="doi">10.1023/a:1012487302797</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mujagic</surname><given-names>Zlatan</given-names></name><name><surname>Kasapi</surname><given-names>Melpomeni</given-names></name><name><surname>Jonkers</surname><given-names>Daisy MAE</given-names></name><name><surname>Garcia-Perez</surname><given-names>Isabel</given-names></name><name><surname>Vork</surname><given-names>Lisa</given-names></name><name><surname>Weerts</surname><given-names>Zsa Zsa RM</given-names></name><name><surname>Serrano-Contreras</surname><given-names>Jose Ivan</given-names></name><name><surname>Zhernakova</surname><given-names>Alexandra</given-names></name><name><surname>Kurilshikov</surname><given-names>Alexander</given-names></name><name><surname>Scotcher</surname><given-names>Jamie</given-names></name><name><surname>Holmes</surname><given-names>Elaine</given-names></name><etal/></person-group><article-title>Integrated fecal microbiome–metabolome signatures reflect stress and serotonin metabolism in irritable bowel syndrome</article-title><source>Gut Microbes</source><year>2022</year><volume>14</volume><issue>1</issue><elocation-id>2063016</elocation-id><pub-id pub-id-type="pmcid">PMC9037519</pub-id><pub-id pub-id-type="pmid">35446234</pub-id><pub-id pub-id-type="doi">10.1080/19490976.2022.2063016</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salek</surname><given-names>RM</given-names></name><name><surname>Maguire</surname><given-names>ML</given-names></name><name><surname>Bentley</surname><given-names>E</given-names></name><name><surname>Rubtsov</surname><given-names>DV</given-names></name><name><surname>Hough</surname><given-names>T</given-names></name><name><surname>Cheeseman</surname><given-names>M</given-names></name><name><surname>Nunez</surname><given-names>D</given-names></name><name><surname>Sweatman</surname><given-names>BC</given-names></name><name><surname>Haselden</surname><given-names>JN</given-names></name><name><surname>Cox</surname><given-names>RD</given-names></name><name><surname>Connor</surname><given-names>SC</given-names></name><etal/></person-group><article-title>A metabolomic comparison of urinary changes in type 2 diabetes in mouse, rat, and human</article-title><source>Physiological Genomics</source><year>2007</year><volume>29</volume><issue>2</issue><fpage>99</fpage><lpage>108</lpage><pub-id pub-id-type="pmid">17190852</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bousseljot</surname><given-names>R</given-names></name><name><surname>Kreiseler</surname><given-names>D</given-names></name><name><surname>Schnabel</surname><given-names>A</given-names></name></person-group><article-title>Nutzung der EKG-signaldatenbank CARDIO-DAT der PTB über das internet</article-title><source>Biomedizinische Technik/Biomedical Engineering</source><year>1995</year><month>July</month><volume>40</volume><issue>s1</issue><fpage>317</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1515/bmte.1995.40.s1.317</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberger</surname><given-names>Ary L</given-names></name><name><surname>Amaral</surname><given-names>Luis AN</given-names></name><name><surname>Glass</surname><given-names>Leon</given-names></name><name><surname>Hausdorff</surname><given-names>Jeffrey M</given-names></name><name><surname>Ivanov</surname><given-names>Plamen Ch</given-names></name><name><surname>Mark</surname><given-names>Roger G</given-names></name><name><surname>Mietus</surname><given-names>Joseph E</given-names></name><name><surname>Moody</surname><given-names>George B</given-names></name><name><surname>Peng</surname><given-names>Chung-Kang</given-names></name><name><surname>Eugene Stanley</surname><given-names>H</given-names></name></person-group><article-title>PhysioBank, PhysioToolkit, and PhysioNet</article-title><source>Circulation</source><year>2000</year><month>June</month><volume>101</volume><issue>23</issue><pub-id pub-id-type="pmid">10851218</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kors</surname><given-names>JA</given-names></name><name><surname>van Herpen</surname><given-names>G</given-names></name><name><surname>Sittig</surname><given-names>AC</given-names></name><name><surname>van Bemmel</surname><given-names>JH</given-names></name></person-group><article-title>Reconstruction of the frank vectorcardiogram from standard electrocardiographic leads: diagnostic comparison of different methods</article-title><source>European Heart Journal</source><year>1990</year><month>December</month><volume>11</volume><issue>12</issue><fpage>1083</fpage><lpage>1092</lpage><pub-id pub-id-type="pmid">2292255</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>Wenjia</given-names></name><name><surname>Sinclair</surname><given-names>Matthew</given-names></name><name><surname>Tarroni</surname><given-names>Giacomo</given-names></name><name><surname>Oktay</surname><given-names>Ozan</given-names></name><name><surname>Rajchl</surname><given-names>Martin</given-names></name><name><surname>Vaillant</surname><given-names>Ghislain</given-names></name><name><surname>Lee</surname><given-names>Aaron M</given-names></name><name><surname>Aung</surname><given-names>Nay</given-names></name><name><surname>Lukaschuk</surname><given-names>Elena</given-names></name><name><surname>Sanghvi</surname><given-names>Mihir M</given-names></name><name><surname>Zemrak</surname><given-names>Filip</given-names></name><etal/></person-group><article-title>Automated cardiovascular magnetic resonance image analysis with fully convolutional networks</article-title><source>Journal of Cardiovascular Magnetic Resonance</source><year>2018</year><month>September</month><volume>20</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC6138894</pub-id><pub-id pub-id-type="pmid">30217194</pub-id><pub-id pub-id-type="doi">10.1186/s12968-018-0471-x</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duan</surname><given-names>Jinming</given-names></name><name><surname>Bello</surname><given-names>Ghalib</given-names></name><name><surname>Schlemper</surname><given-names>Jo</given-names></name><name><surname>Bai</surname><given-names>Wenjia</given-names></name><name><surname>Dawes</surname><given-names>Timothy JW</given-names></name><name><surname>Biffi</surname><given-names>Carlo</given-names></name><name><surname>de Marvao</surname><given-names>Antonio</given-names></name><name><surname>Doumoud</surname><given-names>Georgia</given-names></name><name><surname>O’Regan</surname><given-names>Declan P</given-names></name><name><surname>Rueckert</surname><given-names>Daniel</given-names></name></person-group><article-title>Automatic 3d bi-ventricular segmentation of cardiac images by a shape-refined multi-task deep learning approach</article-title><source>IEEE Transactions on Medical Imaging</source><year>2019</year><volume>38</volume><issue>9</issue><fpage>2151</fpage><lpage>2164</lpage><pub-id pub-id-type="pmcid">PMC6728160</pub-id><pub-id pub-id-type="pmid">30676949</pub-id><pub-id pub-id-type="doi">10.1109/TMI.2019.2894322</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sudlow</surname><given-names>Cathie</given-names></name><name><surname>Gallacher</surname><given-names>John</given-names></name><name><surname>Allen</surname><given-names>Naomi</given-names></name><name><surname>Beral</surname><given-names>Valerie</given-names></name><name><surname>Burton</surname><given-names>Paul</given-names></name><name><surname>Danesh</surname><given-names>John</given-names></name><name><surname>Downey</surname><given-names>Paul</given-names></name><name><surname>Elliott</surname><given-names>Paul</given-names></name><name><surname>Green</surname><given-names>Jane</given-names></name><name><surname>Landray</surname><given-names>Martin</given-names></name><etal/></person-group><article-title>Uk biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age</article-title><source>PLoS Med</source><year>2015</year><volume>12</volume><issue>3</issue><elocation-id>e1001779</elocation-id><pub-id pub-id-type="pmcid">PMC4380465</pub-id><pub-id pub-id-type="pmid">25826379</pub-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1001779</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curran</surname><given-names>Lara</given-names></name><name><surname>de Marvao</surname><given-names>Antonio</given-names></name><name><surname>Inglese</surname><given-names>Paolo</given-names></name><name><surname>McGurk</surname><given-names>Kathryn A</given-names></name><name><surname>Schiratti</surname><given-names>Pierre-Raphaël</given-names></name><name><surname>Clement</surname><given-names>Adam</given-names></name><name><surname>Zheng</surname><given-names>Sean L</given-names></name><name><surname>Li</surname><given-names>Surui</given-names></name><name><surname>Pua</surname><given-names>Chee Jian</given-names></name><name><surname>Shah</surname><given-names>Mit</given-names></name><name><surname>Jafari</surname><given-names>Mina</given-names></name><etal/></person-group><article-title>A genotype-phenotype taxonomy of hypertrophic cardiomyopathy</article-title><source>medRxiv</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10729901</pub-id><pub-id pub-id-type="pmid">38014537</pub-id><pub-id pub-id-type="doi">10.1161/CIRCGEN.123.004200</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name><name><surname>Friedman</surname><given-names>Jerome H</given-names></name><name><surname>Olshen</surname><given-names>Richard A</given-names></name><name><surname>Stone</surname><given-names>CJ</given-names></name></person-group><chapter-title>Classification and regression trees</chapter-title><source>Classification and Regression Trees</source><publisher-name>Chapman &amp; Hall</publisher-name><publisher-loc>New York</publisher-loc><year>1984</year><fpage>358</fpage></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>Leland</given-names></name><name><surname>Healy</surname><given-names>John</given-names></name><name><surname>Saul</surname><given-names>Nathaniel</given-names></name><name><surname>Großberger</surname><given-names>Lukas</given-names></name></person-group><article-title>Umap: Uniform manifold approximation and projection</article-title><source>Journal of Open Source Software</source><year>2018</year><volume>3</volume><issue>29</issue><fpage>861</fpage><pub-id pub-id-type="doi">10.21105/joss.00861</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>Ming</given-names></name><name><surname>Lin</surname><given-names>Yi</given-names></name></person-group><article-title>Model selection and estimation in regression with grouped variables</article-title><source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source><year>2005</year><month>December</month><volume>68</volume><issue>1</issue><fpage>49</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00532.x</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voges</surname><given-names>Lucas F</given-names></name><name><surname>Jarren</surname><given-names>Lukas C</given-names></name><name><surname>Seifert</surname><given-names>Stephan</given-names></name></person-group><article-title>Exploitation of surrogate variables in random forests for unbiased analysis of mutual impact and importance of features</article-title><source>Bioinformatics</source><year>2023</year><month>July</month><volume>39</volume><issue>8</issue><pub-id pub-id-type="pmcid">PMC10403431</pub-id><pub-id pub-id-type="pmid">37522865</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btad471</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><p>LAVASET high-level pipeline indicating the different and novel approach in the node splitting step and feature importance calculation. Model input is customisable and LAVASET can perform on different types of omics data, from 1D to 3D.</p></caption><graphic xlink:href="EMS190071-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>Comparison of LAVASET-1K to RF-1K and RF-41K feature importance assignments on pre-identified metabolites. Panels A, B, C shows feature importances (as defined in <xref ref-type="disp-formula" rid="FD2">Equation 2</xref> in <xref ref-type="sec" rid="S4">Methods</xref>) for valine, and panels D, E, F for 2-methylproline. Colourbar on the right indicates the feature importance value range (red = higher, blue = lower). Dashed gray lines indicate the previously identified points for the specific metabolites peaks. X-axis indicates the chemical shift (in parts per million (ppm), <italic>δ</italic>). Y-axis shows the average signal intensity.</p></caption><graphic xlink:href="EMS190071-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>ECG feature importance (Kors regression back-transformed from the VCG feature importance) normalised across the 8 independent ECG leads. Subplot titles indicate the respective leads. X-axes show the time in milliseconds (ms) and y-axes the voltage magnitude in millivolts (mV). The dotted black line indicates a healthy sample and the solid lines represent the 10 MI types (acute, anterior, anteriolateral, anteroseptolateral, anteroseptal, inferior, inferolateral, inferoposterolateral, lateral, posterior) in this dataset and are coloured by relative feature importance.</p></caption><graphic xlink:href="EMS190071-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>3D representations of the left ventricle CMR data points (x, y, z coordinates). Points represent an averaged template of HCM and HC ventricles. The inner and outer structures formed show the endocardium and epicardium, respectively. Colourbar shows the feature importance gradient, indicating that in panel A (LAVASET) the assignment of higher importance is encompassing the entirety of the ventricle structure. In panel B, assignments are given in a patch-like manner for RF. Panels C and D show the points in the 80% quantile from a top view to facilitate the distinction between the inner and outer walls of the ventricle. Panels E and F show 6 distinct neighbourhoods of FOIs. In panel D convex hulls are drawn for each neighbourhood to represent the pattern of points per neighbourhood. Black points indicate the neighbours and the coloured connecting lines emphasise the different neighbourhoods, and the string-like pattern of neighbour points. Panel C shows the feature importance for the respective 6 neighbourhoods.</p></caption><graphic xlink:href="EMS190071-f004"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><p>HCM CMR dataset, feature DR performance. Scores for accuracy, precision, recall, and F1-score across 20 iterations of UMAP and k-means (k=2) on the selected feature sets by importance in LAVASET, RF and without selection. Values shown are the mean±standard deviation.</p></caption><table frame="box" rules="all"><thead><tr style="border-bottom: double"><th valign="top" align="center">Features:</th><th valign="top" align="center">LAVASET</th><th valign="top" align="center">RF</th><th valign="top" align="center">All</th></tr></thead><tbody><tr><td valign="top" align="center">Accuracy</td><td valign="top" align="center"><bold>0.890</bold>±0.01</td><td valign="top" align="center">0.870±0.002</td><td valign="top" align="center">0.797±0.004</td></tr><tr><td valign="top" align="center">Precision</td><td valign="top" align="center">0.905±0.005</td><td valign="top" align="center"><bold>0.913</bold>±0.004</td><td valign="top" align="center">0.817±0.006</td></tr><tr><td valign="top" align="center">Recall</td><td valign="top" align="center"><bold>0.859</bold>±0.009</td><td valign="top" align="center">0.816±0.003</td><td valign="top" align="center">0.763±0.011</td></tr><tr><td valign="top" align="center">F1-score</td><td valign="top" align="center"><bold>0.881</bold>±0.004</td><td valign="top" align="center">0.862±0.002</td><td valign="top" align="center">0.789±0.005</td></tr></tbody></table></table-wrap></floats-group></article>