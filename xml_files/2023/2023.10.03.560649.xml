<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189088</article-id><article-id pub-id-type="doi">10.1101/2023.10.03.560649</article-id><article-id pub-id-type="archive">PPR735284</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">3</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Convergent neural signatures of speech prediction error are a biological marker for spoken word recognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Sohoglu</surname><given-names>Ediz</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Beckers</surname><given-names>Loes</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Davis</surname><given-names>Matthew H.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><aff id="A1"><label>1</label>School of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/00ayhx656</institution-id><institution>University of Sussex</institution></institution-wrap>, <city>Brighton</city>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/055bpw879</institution-id><institution>MRC Cognition and Brain Sciences Unit</institution></institution-wrap>, <city>Cambridge</city>, <country country="GB">UK</country></aff><aff id="A3"><label>3</label>Department of Otorhinolaryngology, Donders Institute for Brain, Cognition and Behaviour, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05wg1m734</institution-id><institution>Radboud University Medical Center</institution></institution-wrap>, <city>Nijmegen</city>, <country country="NL">Netherlands</country></aff><aff id="A4"><label>4</label>Cochlear Ltd., Mechelen, Belgium</aff></contrib-group><author-notes><corresp id="CR1"><bold>Address correspondence to</bold> Ediz Sohoglu (<email>E.Sohoglu@sussex.ac.uk</email>) or Matthew Davis (<email>matt.davis@mrc-cbu.cam.ac.uk</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">We used MEG and fMRI to determine how predictions are combined with speech input in superior temporal cortex. We compared neural responses to bisyllabic spoken words in which the first syllable strongly or weakly predicts the form of the second syllable (e.g. “bingo” versus “tango”). We further compared neural responses to the same second syllables when heard in an unfamiliar pseudoword and therefore in a situation in which predictions mismatch with sensory input (e.g. “snigo” and “meago”). Across multiple imaging modalities and analysis approaches (including information-based pattern analysis methods), we show that neural representations of second syllables are suppressed by strong predictions when predictions match sensory input. However, neural representations of the same second syllables show the opposite effect (i.e. enhanced representations following strongly than weakly-predicting syllables) when predictions mismatch with sensory input. Computational simulations show that this interaction between prediction strength and (mis)match is consistent with prediction error but not alternative (sharpened signal) computations. Neural signatures of prediction error are observed early (beginning 200 ms after the onset of the second syllable), localise to early auditory regions (in fMRI, bilateral Heschl’s gyrus and STG) and are expressed as changes in low-frequency (theta and alpha) power. Our study therefore provides convergent neural evidence that speech perception is supported by the computation of prediction errors in auditory brain regions. These prediction error computations play a central role in the identification of familiar spoken words and perception of unfamiliar pseudowords.</p></abstract><kwd-group><kwd>Predictive coding</kwd><kwd>Speech perception</kwd><kwd>MEG</kwd><kwd>fMRI</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">The information reaching our senses is incomplete and ambiguous such that perception requires active computations for successful interpretation. Nowhere is this more evident than with speech perception. To paraphrase Heraclitus, “No listener ever hears the same speech twice, for it’s not the same sounds and they’re not the same listener”. No two productions of the same word are ever acoustically identical; each will be differently realised due to the ubiquitous variability of natural speech produced by different talkers, and in different contexts. No listener is ever the same as they will have different expectations or prior knowledge for the speech they will hear at different times. Consider for example, hearing a word starting with the syllable “tan”. It can be unclear whether this is the start of the word “tangle”, “tango” or another word that starts with the same initial sounds. This ambiguity can be present even if speech is clearly heard in a quiet background. Yet when listening during everyday conversation, our subjective impression is that spoken word recognition is immediate, clear and accurate. How do listeners achieve such reliable perception despite transient sensory ambiguity?</p><p id="P3">In answering this question, the notion of Bayesian inference is commonly invoked. That is, listeners combine sensory evidence with prior knowledge about which speech sounds they will hear [<xref ref-type="bibr" rid="R1">1</xref>]. This proposal is appealing because it can explain how listeners process speech both rapidly and accurately [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref>]. However, despite widely held agreement that listeners use prior knowledge during speech processing, it remains unclear how this is implemented neurally. One possibility is that neural representations of speech are enhanced by prior knowledge [<xref ref-type="bibr" rid="R4">4</xref>]. This has been termed the sharpened signal account [<xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref>] because in this proposal, neural responses to expected stimuli come to resemble a sharpened (i.e. less ambiguous or less noisy) version of the sensory input. However, it is also the case that expected speech sounds carry relatively little information. Therefore, an alternative and potentially more efficient strategy would be to compute prediction errors i.e. the difference between heard and predicted sounds [<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R11">11</xref>]. In this prediction error account, neural representations are suppressed by prior knowledge and unexpected (and therefore novel and unexplained) sensory information is prioritised for processing.</p><p id="P4">Sharpened signal and prediction error accounts have been difficult to distinguish experimentally. This is because in both accounts, prior knowledge has a similar effect on the overall strength of neural responses [<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref>]. In the case of the prediction error account, brain responses tuned to the heard stimulus features are suppressed when those stimulus features are expected, resulting in an overall reduction in brain activity. With the sharpened signal account, although brain responses tuned to the heard stimulus features are enhanced, this is accompanied by the suppression of responses tuned away from the stimulus input features i.e. competing interpretations. This suppression of brain responses tuned away from the input features also leads to overall weaker activity. Therefore, previous findings that neural responses correlate with moment-by-moment phoneme and word predictability during story comprehension [<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R16">16</xref>] do not provide unambiguous support for prediction error over sharpened signal accounts. Some accounts have proposed that sharpened signal and prediction error computations operate in parallel [<xref ref-type="bibr" rid="R8">8</xref>], and localise to different cortical layers [<xref ref-type="bibr" rid="R17">17</xref>]. Nonetheless, we focus on contrasting sharpened signal and prediction error mechanisms since single mechanism accounts have been proposed in speech perception [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R19">19</xref>] and remain to be distinguished [<xref ref-type="bibr" rid="R20">20</xref>].</p><p id="P5">Multivariate pattern analysis methods that permit measurement of the information conveyed by neural responses can distinguish between sharpened signal and prediction error computations. Indeed, previous work using fMRI [<xref ref-type="bibr" rid="R18">18</xref>] and MEG [<xref ref-type="bibr" rid="R21">21</xref>] has demonstrated a distinctive neural marker of prediction errors which is that neural representations of sensory features of speech show an interaction between signal quality and prior knowledge. This interaction arises because sensory signals that match strong prior expectations are explained away more effectively as signal quality increases and hence neural representations are suppressed even as perceptual outcomes improve. Whereas for sensory signals that follow less informative prior knowledge, increased signal quality leads to a corresponding increase in sensory information that remains unexplained. By contrast, under a sharpened signal account, computational simulations show that neural representations (like perceptual outcomes) are similarly enhanced by increased signal quality and prior knowledge [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>]. Therefore, by experimentally manipulating both signal quality and prior knowledge, and observing the consequences on pattern-based neural measures, it is possible to distinguish between accounts.</p><p id="P6">However, in the aforementioned studies [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>], listeners heard highly distorted (and unnatural sounding) spoken words while prior knowledge was manipulated by presenting matching or nonmatching written words before speech was heard. This is far removed from real-world conditions in which speech is clearly heard (non-distorted) and predictions are not derived from external written cues but from linguistic knowledge e.g. word probabilities vary based on usage frequency and preceding sentence context. Indeed, some investigators have argued that previously documented effects of prior knowledge or prediction on language processing are the consequence of “prediction encouraging” paradigms and fail to generalise to settings more representative of real-world perception [<xref ref-type="bibr" rid="R22">22</xref>].</p><p id="P7">In the current study, we return to this issue by presenting listeners with clearly spoken words. We experimentally manipulated the strength and accuracy of listeners’ prior knowledge with the goal of adjudicating between sharpened signal and prediction error accounts. Here our manipulation of prior knowledge is based on information intrinsic to the speech signal and arises from listeners’ long-term linguistic knowledge of the sounds of familiar spoken words. We compare neural responses to bisyllabic spoken words (e.g. “bingo”, “tango”) in which the first syllable strongly (in “bingo”) or weakly (in “tango”) predicts the form of the second syllable (<xref ref-type="fig" rid="F1">Figure 1A</xref>). In addition, we compare neural responses to the same second syllables when heard in a pseudoword context (e.g. “snigo”, “meago”). Pseudowords, by definition, are unfamiliar to participants and therefore the second syllable of these items mismatches with listeners’ predictions. This creates a listening situation in which strong predictions entirely mismatch with sensory input. This cannot be observed in other experimental situations that forego manipulation of speech content (e.g. story comprehension) and creates a further opportunity for adjudicating between sharpened signal and prediction error computations.</p><p id="P8">We first performed computational simulations (using model architectures and representations schematised in <xref ref-type="fig" rid="F1">Figure 1B, C</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>) to show that, as intended, our experimental manipulations of prediction strength and match/mismatch result in dissociable effects for sharpened signal and prediction error computations (<xref ref-type="fig" rid="F1">Figure 1E</xref>). While prediction strength and prediction congruency act to enhance sharpened signal representations, prediction error computations show an interaction between prediction strength and congruency. This interaction is driven by a reduction of prediction errors when strong predictions match the sensory input compared with weak predictions. When predictions mismatch with sensory input, however, the opposite occurs with prediction errors slightly increasing with prediction strength. We then measured neural responses using 204-channel MEG and 3T fMRI in separate groups of listeners while they performed an incidental (pause detection) listening task to maintain attention (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). Across multiple imaging modalities (MEG, fMRI), analysis approaches (univariate signal magnitude and multivariate pattern), and signal domains (time and time-frequency), we provide convergent neural evidence that speech perception is supported by the computation of prediction errors in auditory brain regions.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Stimulus properties and behavioural responses</title><p id="P9">To manipulate listeners’ predictions, we chose a set of 64 bisyllabic words in which the first syllable (Syl1) either strongly or weakly predicts the same second syllable (Syl2; see <xref ref-type="fig" rid="F1">Figure 1A</xref> which depicts some example items). Here we operationalise prediction strength as the probability of hearing Syl2, conditioned on Syl1 (p(Syl2|Syl1); see <xref ref-type="sec" rid="S14">Methods</xref> section for details and refs. [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R23">23</xref>]). For example, after hearing the first syllable /b I N/, there is only one potential word candidate (<italic>bingo</italic>). Therefore, based on their experience of spoken English, listeners will predict that the second syllable /g @U/ is most likely after hearing /b I N/ (p(Syl2|Syl1) = 1). In contrast, after hearing the first syllable “tan”, there are more potential word candidates (e.g. <italic>tango, tangle, tanker, tankard, tang</italic>) and of these words, <italic>tango</italic> is less frequently used than the competitors <italic>tangle</italic> and <italic>tang</italic>. As a result, listeners can only make a weak prediction for /g @U/ after hearing /t { N/ (p(Syl2|Syl1) = .052). Note that p(Syl2|Syl1) can also be expressed as syllable surprisal [see <xref ref-type="bibr" rid="R23">23</xref>], which is equivalent to the negative log of p(Syl2|Syl1).</p><p id="P10">We validated our experimental manipulation of prediction strength by asking a separate group of listeners to perform a free report gating task [see <xref ref-type="bibr" rid="R24">ref 24</xref> and <xref ref-type="supplementary-material" rid="SD1">SI Methods and Results</xref>]. This confirmed that listeners were more likely to predict the second syllables of our Strong items than for Weak items. Further validation of our experimental manipulation is provided when assessing listeners’ memory for the pseudowords in a cued recall task (see <xref ref-type="supplementary-material" rid="SD1">SI Results</xref>).</p><p id="P11">In addition to manipulating the strength of listeners’ predictions, we manipulated whether Syl2 matched or mismatched with listeners’ predictions. This was achieved by cross-splicing Syl1 and Syl2 between words to create a set of corresponding pseudowords sharing the same set of syllables as the word items. For real word items, Syl2 always matches predictions, at least to some degree. For pseudowords, p(Syl2|Syl1) approaches zero. This is by definition, since these Syl1 and Syl2 combinations will never previously have been heard by participants as English spoken words prior to the experiment. Therefore, for these items, Syl2 entirely mismatches with listeners’ predictions.</p><p id="P12">It is important to note that even though p(Syl2|Syl1) approaches zero for all Mismatch items, we can still expect a difference in prediction strength between Strong+Mismatch and Weak+Mismatch items since Strong and Weak items differ not only in terms of p(Syl2|Syl1), but also in entropy over all possible second syllables i.e. the overall uncertainty of predictions (see <xref ref-type="supplementary-material" rid="SD1">SI Results and Figure S4A</xref>). Indeed in natural speech, conditional probabilities for upcoming speech sounds are negatively correlated with entropy [<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R23">23</xref>]. Going back to the example above, after hearing the first syllable /b I N/, entropy is relatively low because only one word (<italic>bingo</italic>) can be predicted. In this case, the unexpected second syllable (/g @ r*/) follows a state of precise predictions (low uncertainty/entropy). For <italic>tango</italic> on the other hand, entropy is relatively high after hearing the first syllable because predictions are made for multiple second syllables each arising from different word candidates. Here the unexpected second syllable follows a state of imprecise predictions (high uncertainty/entropy). Thus, while we constructed our stimuli based on p(Syl2|Syl1), our manipulation of prediction strength reflects differences in both p(Syl2|Syl1) and syllable entropy. Both aspects of predictability may contribute to perceptual and neural responses in the Match (real word) condition. In the Mismatch condition however, effects of prediction strength can only reflect differences in entropy (as explained above).</p><p id="P13">To assess neural responses to these stimuli, we recorded MEG (N=19) and fMRI (N=21) data in two separate groups of listeners (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). To maintain listeners’ attention, participants were asked to detect brief (200 ms) pauses inserted between Syl1 and Syl2 in occasional target items. As shown in <xref ref-type="supplementary-material" rid="SD1">Figure S1B and S1C</xref>, participants could perform this task quickly and accurately (d-prime higher than 3 and with response times on target-present trials less than 1100 ms).</p></sec><sec id="S4"><title>Computational simulations</title><p id="P14">We performed simulations of sharpened signal and prediction error computations supporting spoken word and pseudoword recognition (model architectures and representations schematised in <xref ref-type="fig" rid="F1">Figure 1B, C</xref> with further illustration of model computations provided in <xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>). The underlying mechanisms were derived from established Bayesian models of spoken word recognition [e.g. 2] guided by observations of functionally equivalent but neurally distinct implementations of these. We focus on the timepoint at which the words and pseudowords in our stimuli diverge (the divergence point; highlighted as bold speech segments in <xref ref-type="fig" rid="F1">Figure 1A</xref>). This is the time at which our experimental manipulation of prediction congruency can affect brain responses: in phonemes, this is at the onset of the second speech segment during Syl2 (98 ms after Syl2 onset on average, with a standard deviation of 46 ms).</p><p id="P15">Both sharpened signal and prediction error computations are derived from conditional (posterior) word probabilities, as estimated using Bayes theorem (for details, see <xref ref-type="sec" rid="S14">Methods</xref>). As with previous work [e.g. 2,11,12], predictions for the next speech segment are estimated from the relative frequencies of the words matching the preceding speech signal, which is appropriate for spoken words heard in isolation (see <xref ref-type="sec" rid="S14">Methods</xref> for details). Our estimates of the sensory evidence are derived from the acoustic similarity between individual speech segments and expresses the degree to which the sensory input matches internal representations of spoken words [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R25">25</xref>]. Because our stimuli consisted of clear speech recordings, we simulated conditions in which the sensory evidence was high (i.e. there was minimal sensory uncertainty; see <xref ref-type="sec" rid="S14">Methods</xref>) but dissociable sharpened signal and prediction error computations are observed when simulating a range of sensory uncertainty levels (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>).</p><p id="P16">To relate these simulations to observed neural responses, we apply signal magnitude (univariate) and pattern (multivariate) analysis procedures used in our MEG and fMRI analyses (described in a subsequent section) to simulated representations of speech segments at the divergence point (results shown in <xref ref-type="fig" rid="F1">Figure 1D, E</xref>). This is performed for each of the four experimental conditions (the 2-by-2 crossing of prediction strength and congruency).</p><p id="P17">The signal magnitude analysis (<xref ref-type="fig" rid="F1">Figure 1D</xref>) focusses on the overall strength (rather than pattern) of neural responses. For this magnitude analysis of prediction error representations, we summed model signals (absolute prediction error) over all possible segment representations (48 in total). However, this approach is ill-suited for the sharpened signal simulation since here the model representations correspond to posterior probability distributions and hence all simulated signals are normalized to sum to one (and will hence never differ between conditions). Therefore, to relate sharpened signals to univariate neural responses, we summarised model responses as the normalized sum of log-transformed probabilities over the 48 segment representations (see <xref ref-type="sec" rid="S14">Methods</xref>). This choice of linking function between the model and neural responses was motivated by the proposal that neural responses encode log rather than linear probabilities [<xref ref-type="bibr" rid="R26">26</xref>]. We also considered an alternative linking function based on entropy, which was motivated by the observation that increased uncertainty in neural responses can be accompanied by an increase in the mean and therefore overall strength of neural responses [<xref ref-type="bibr" rid="R9">9</xref>]. The two linking functions produced equivalent results (this is unsurprising given that entropy is defined as a weighted sum of log probabilities). In the following section we report only the results of using the first linking function (the normalised sum of log-transformed sharpened signals).</p><p id="P18">This univariate signal magnitude analysis shows a broadly similar pattern for sharpened signal and prediction error computations. For familiar words (Match items), both sharpened signal and prediction error simulations show a reduced univariate response to strongly versus weakly predicted second syllables (<xref ref-type="fig" rid="F1">Figure 1D</xref>). In addition, univariate responses are overall larger for pseudowords (Mismatch items) versus words (Match items). Within the Mismatch condition, sharpened signal and prediction error simulations diverge somewhat with the former showing a reduced response to strongly versus weakly predicted second syllables and the latter showing a larger response. For both models, prediction strength and congruency have interactive effects on univariate responses as the effect of prediction strength is largest for familiar words (Match items) and reduced for pseudowords (Mismatch items).</p><p id="P19">The multivariate pattern analysis (<xref ref-type="fig" rid="F1">Figure 1E</xref>), on the other hand, shows clearer differences between sharpened signal and prediction error computations. This multivariate analysis quantifies how strongly phonetic content in the speech input is represented in simulated patterns at the segment level. This is achieved by quantifying the dissimilarity between model representations (pattern distances) for all pairs of items within each condition.</p><p id="P20">The results of this multivariate pattern analysis show that while sharpened signal representations are enhanced for Match items, prediction strength and congruency show a cross-over interaction which only occurs for prediction error representations. Specifically, for signals that match prior predictions (during word processing), prediction errors decrease in magnitude and pattern distance for stronger predictions. However, the opposite effect occurs when predictions mismatch with predictions during pseudoword processing; that is prediction error is increased, and more informative for syllables that mismatch with stronger predictions. In addition, prediction errors are overall larger for Mismatch compared with Match items, reflecting the absence of accurate predictions following the divergence point of the pseudoword (Mismatch) stimuli.</p><p id="P21">These simulations, particularly when analyzed using multivariate pattern analysis, confirm that our experimental manipulations can distinguish between sharpened signal and prediction error computations. We can therefore evaluate the extent to which observed brain responses are more consistent with sharpened signal or prediction error computations.</p></sec><sec id="S5"><title>MEG signal magnitude</title><p id="P22">As shown in <xref ref-type="fig" rid="F2">Figure 2A</xref>, we assessed MEG responses timelocked to the onset of Syl2, with respect to a baseline period prior to Syl1 (i.e. speech onset; see <xref ref-type="sec" rid="S14">Methods</xref>). Note that as a result, the MEG responses at negative latencies shown in <xref ref-type="fig" rid="F2">Figure 2A</xref> are non-zero (coinciding with sensory input from the Syl1 portion of our spoken stimuli).</p><p id="P23">Before assessing the neural timecourse of between-condition differences, we selected 20 (planar gradiometer) sensors with the strongest responses (signal power averaged over conditions, separately for each hemisphere and participant). Sensor selections (i.e. those sensors with the strongest responses) were distributed over temporal and frontal sites bilaterally (shown inset within <xref ref-type="fig" rid="F2">Figure 2A</xref>), consistent with neural generators in superior temporal cortex. We then summarised MEG responses in these “speech responsive” sensors in a univariate fashion by taking the RMS amplitude across sensor selections for each timepoint. Reported effects are all FWE (family-wise error) rate corrected for multiple comparisons across timepoints using cluster permutation methods and a clusterwise threshold of p &lt; 0.05.</p><p id="P24">Following Syl2 onset, prediction strength (Strong vs Weak) and congruency (Match vs Mismatch) had interactive influences on MEG responses from around 200 to 600 ms (results for left hemisphere sensors are shown in <xref ref-type="fig" rid="F2">Figure 2A</xref>; similar results are seen for right hemisphere sensors, depicted in <xref ref-type="supplementary-material" rid="SD1">Figure S2A</xref>). Stronger predictions resulted in weaker MEG responses but only when speech matched with predictions (significant timepoints for the interaction between prediction strength and congruency are indicated as horizontal cyan bars; the simple effect of Strong vs Weak for Matching items is shown as the grey shaded area between MEG traces). MEG responses also showed a main effect of prediction congruency with overall larger responses for Mismatch vs Match items (significant timepoints shown as red horizontal line). These two patterns – interaction between prediction strength and congruency, together with a main effect of Mismatch &gt; Match –are broadly consistent with both sharpened signal and prediction error computations (compare with <xref ref-type="fig" rid="F1">Figure 1D</xref>). Note that although MEG responses do not increase with prediction strength for mismatching items in this particular analysis (a pattern present only in the prediction error simulations; see <xref ref-type="fig" rid="F1">Figure 1D</xref>), they do so for other analyses that will be reported below.</p><p id="P25">Prior to Syl2 onset, a different pattern of results was observed. Here MEG responses increased with prediction strength (indicated as horizontal purple bars in <xref ref-type="fig" rid="F2">Figure 2A</xref>). While the timing of this effect (before Syl2 onset) suggests a neural signal reflecting predictions rather than prediction errors, we cannot rule out that this pre-Syl2 effect reflects acoustic differences since the Syl1 portion of Strong items is longer in duration than for Weak items (see <xref ref-type="supplementary-material" rid="SD1">SI Results and Figure S4A</xref>). This duration difference could potentially lead to larger MEG signal when timelocking to Syl2 onset as neural responses would be shifted later in time for Strong versus Weak items. Indeed, when timelocking to Syl1 onset to control for this difference in timing, the increased MEG response for Strong versus Weak items is no longer apparent (<xref ref-type="supplementary-material" rid="SD1">Figure S2C</xref>). Note that these or other acoustic confounds can only affect pre-Syl2 neural responses since our cross-splicing procedure ensures that post-Syl2, there are no acoustic differences between our four conditions (see <xref ref-type="fig" rid="F1">Figure 1A</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S4A</xref>). Since representations of Syl2 are critical for distinguishing sharpened signal from prediction error computations (<xref ref-type="fig" rid="F1">Figure 1E</xref>), we subsequently focus on post-Syl2 effects.</p><p id="P26">To test whether Syl2 responses are modulated by prediction strength for single items, we correlated p(Syl2|Syl1) with MEG responses across items separately for Match and Mismatch items. We show group-averaged Spearman correlations between prediction strength and MEG responses in left hemisphere sensors in <xref ref-type="fig" rid="F2">Figure 2B</xref> (right hemisphere correlations shown in <xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>). Consistent with the previous analysis averaged over items (<xref ref-type="fig" rid="F2">Figure 2A</xref>), following Syl2, there was an interaction between prediction strength and congruency from 168 to 300 ms and from 356 to 460 ms: Stronger predictions that matched with speech were associated with weaker MEG responses and this correlation differed from that observed when predictions mismatched (significant timepoints indicated as horizontal cyan bars). Individually, the correlations within the Matching condition were significantly negative (indicated as the grey shaded area). Within the Mismatching condition, correlations were weakly positive and significant in the right hemisphere at a later latency (<xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>, 500-600 ms). This pattern of results is more consistent with prediction error than with sharpened signal computations as a positive effect of prediction strength for Mismatch items is only apparent in prediction error simulations (see <xref ref-type="fig" rid="F1">Figure 1D</xref>). Thus, this analysis demonstrates that the item-averaged effects depicted previously in <xref ref-type="fig" rid="F2">Figure 2A</xref> extend to single items. That is, the strength of lexical level predictions modulates neural responses to heard speech with the magnitude and direction of this effect depending on whether subsequent segments match or mismatch with predictions. The significant interaction shown in both factorial and graded analyses is broadly consistent with both sharpened signal and prediction error accounts although the positive effect of prediction strength for Mismatch items (observed in graded analysis) favours the prediction error account.</p><p id="P27">To probe time-frequency correlates of prediction strength and congruency, we also assessed changes in spectral power in the 2-48 Hz range. This time-frequency analysis also showed a significant interaction between prediction strength and congruency in the theta band, with a peak frequency at 3 Hz and a peak latency 120 ms after Syl2 onset (left hemisphere results shown in <xref ref-type="fig" rid="F2">Figure 2D</xref>; a similar pattern was observed in right hemisphere sensors). Similar to the time-domain analyses above, stronger predictions resulted in weaker MEG power but only when speech matched with predictions (see summary in <xref ref-type="fig" rid="F2">Figure 2C</xref>). We also analysed high-frequency gamma activity (52-90 Hz) but did not observe reliable condition-wise differences.</p></sec><sec id="S6"><title>fMRI signal magnitude</title><p id="P28">In a separate group of listeners, we recorded BOLD functional MRI responses to the same stimuli, using a sparse fMRI sequence so that speech was presented in the silent periods between scans (<xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). The procedure (task and number of stimulus repetitions) was otherwise identical to that used in the MEG experiment. This allowed us to localize the neural generators of the MEG effects reported above. Reported effects are all FWE rate corrected using a clusterwise threshold of p &lt; 0.05.</p><p id="P29">As shown in <xref ref-type="fig" rid="F3">Figure 3A</xref>, which presents the univariate fMRI results, the main effect of Mismatch &gt; Match (that was observed following Syl2 onset with MEG) localized to middle/anterior STG bilaterally (see <xref ref-type="table" rid="T1">Table 1</xref> for MNI space coordinates). The main effect of Strong&gt;Weak (observed with MEG prior to Syl2 onset) localized to Heschl’s gyrus (HG), extending into STG. Despite observing an interaction between prediction and congruency on MEG responses following Syl2 onset, there was no equivalent effect in this fMRI analysis. We suggest this is because of the reduced temporal resolution of fMRI, which makes it more difficult to resolve individual responses to Syl2 and Syl1.</p><p id="P30">A potentially more sensitive analysis is to correlate prediction strength i.e. p(Syl2|Syl1) with the magnitude of the BOLD signal across individual items (equivalent to the MEG analysis shown in <xref ref-type="fig" rid="F2">Figure 2B</xref>). Employing this approach, fMRI responses in bilateral HG and middle STG again increased with prediction strength (positive correlation) for both Match and Mismatch items (shown in <xref ref-type="fig" rid="F3">Figure 3B</xref> with MNI space coordinates listed in <xref ref-type="table" rid="T1">Table 1</xref>), demonstrating that the effect depicted previously for items that differ in averaged prediction strength in <xref ref-type="fig" rid="F3">Figure 3A</xref> is also observed for single items which encompass a range of prediction strengths. Because this effect of prediction strength is positive and equally apparent for Match and Mismatch items, we suggest it corresponds to the pre-Syl2 effect observed in MEG i.e. reflecting processing prior to the critical Syl2 event. We therefore will not discuss this effect further because our focus is post-Syl2 processing (as explained above), which can distinguish different computational accounts.</p><p id="P31">Importantly, we also observed an interaction between p(Syl2|Syl1) and prediction congruency in right anterior STG (shown in <xref ref-type="fig" rid="F3">Figure 3B</xref>), revealing a potential neural generator of the interaction effects previously observed with MEG after Syl2 onset (<xref ref-type="fig" rid="F2">Figure 2B</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>). Although this fMRI effect appears driven by a positive correlation in the Mismatch condition (rather than a negative correlation for Match items), the correlation is numerically negative for Match items, which is consistent with earlier MEG analysis (<xref ref-type="fig" rid="F2">Figure 2B</xref> and <xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>). We also note that like the present fMRI analysis, right-hemisphere MEG sensors show a positive correlation in the Mismatch condition at later latencies (<xref ref-type="supplementary-material" rid="SD1">Figure S2B</xref>). As noted earlier for the univariate MEG analysis, this positive effect of prediction strength for Mismatch items is only apparent in prediction error simulations (<xref ref-type="fig" rid="F1">Figure 1D</xref>).</p></sec><sec id="S7"><title>MEG pattern analysis</title><p id="P32">The univariate signal magnitude analyses above shows that the magnitude of neural responses in superior temporal regions is modulated by syllable prediction strength in different directions depending on whether heard speech matches or mismatches with predictions. While the direction of these neural effects is in line with the magnitude of prediction error in our computational simulations, as mentioned above, our computational simulations indicate that multivariate pattern analysis more clearly distinguishes sharpened signal and prediction error accounts. Simulations of prediction error computations show that representational patterns show a specific cross-over interaction between prediction strength and congruency (<xref ref-type="fig" rid="F1">Figure 1E</xref>) which is clearly absent for sharpened signal computations.</p><p id="P33">To examine how predictability modulates neural representational patterns in our MEG data, we computed the Euclidean distance between sensor patterns (over the entire 204 array of planar gradiometers) evoked by pairs of items. Taking this approach, we first asked whether the MEG signal contained phonetic information about the syllables by correlating neural pattern distances with the phonetic dissimilarity between the syllables of item pairs (based on a representational dissimilarity matrix derived from the Levenshtein distance between phonetic transcriptions for different syllables, as illustrated in <xref ref-type="fig" rid="F4">Figure 4A</xref> for Syl2 and <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref> for Syl1). As shown in <xref ref-type="fig" rid="F4">Figure 4B</xref>, MEG patterns correlated with Syl2 phonetic dissimilarities only after Syl2 onset (reliable effects were observed in two clusters between 168 and 928 ms post-DP, with the peak effect observed at 332 ms; indicated by horizontal red bars). In contrast the peak correlation with Syl1 phonetic dissimilarities was largely confined to periods in which Syl1 was heard, with a cluster from -200 to +188 ms, and peak effects occurring at -112 ms (indicated by horizontal beige bars). This confirms that MEG response patterns represent the phonetic content of the syllables as listeners are hearing them, in line with other, similar demonstrations [<xref ref-type="bibr" rid="R29">29</xref>]. Qualitatively similar albeit weaker results are obtained when using cross-validated Mahalanobis distances [<xref ref-type="bibr" rid="R30">30</xref>].</p><p id="P34">We next assessed how neural pattern distances were modulated by syllable predictability by averaging pattern distances over item pairs within each condition and comparing averaged distances between conditions [for a similar approach applied to visual processing, see <xref ref-type="bibr" rid="R31">ref 31</xref>]. As shown in <xref ref-type="fig" rid="F4">Figure 4C</xref>, following Syl2 onset, prediction strength and congruency interacted to influence MEG pattern distances: stronger predictions resulted in larger MEG pattern distances but only when speech mismatched with predictions (simple effect of Strong - Weak [Mismatch] is shown as shaded grey area in <xref ref-type="fig" rid="F4">Figure 4C</xref>, and the first interaction cluster peak at 236 ms, is plotted as an inset). This interaction revealed for MEG patterns after Syl2 onset is more consistent with prediction error than with sharpened signal computations (compare with <xref ref-type="fig" rid="F1">Figure 1E</xref>).</p><p id="P35">Complementing the analysis above, we also examined the correlation between neural pattern distances and Syl2 phonetic dissimilarities, separately for each condition (i.e. the same dissimilarities indicated in <xref ref-type="fig" rid="F4">Figure 4A</xref> but within condition). This provides a second measure of neural representations, one that is potentially more specific to Syl2 phonetic content as opposed to other properties that could differ between items e.g. acoustic, lexical, semantic etc. However, we did not find any timepoints showing an interaction between prediction strength and congruency nor any main effects (<xref ref-type="supplementary-material" rid="SD1">Figure S2D</xref>). Given that our simulations provide an apriori hypothesis that the effect of Strong vs Weak predictions should differ for Match and Mismatch conditions (see <xref ref-type="fig" rid="F1">Figure 1E</xref>), we also tested for simple effects of Strong versus Weak prediction within each congruency condition. This revealed greater correlation between MEG pattern and Syl2 phonetic distances for Strong versus Weak items in the Mismatch condition only. This occurred in two clusters, from 44 to 116 ms and from 148 to 220 ms. These results remain broadly consistent with the earlier analysis shown in <xref ref-type="fig" rid="F4">Figure 4C</xref>. While here we did not observe an interaction between prediction strength and congruency, as before we did observe a positive effect of prediction strength within the Mismatch condition. This result favours the prediction error account (see <xref ref-type="fig" rid="F1">Figure 1E</xref>).</p><p id="P36">Correlating pattern distances with a graded measure of syllable prediction strength (i.e. p(Syl2|Sy1)) revealed further, convergent evidence for prediction error computations (<xref ref-type="fig" rid="F4">Figure 4D</xref>). Stronger predictions for upcoming speech sounds during Syl2 were associated with opposite effects on MEG pattern distances from 360 to 468 ms, depending on prediction congruency (significant timepoints indicated as horizontal cyan bars). During this time period, correlations between prediction strength and pattern distances were positive when predictions mismatched with heard speech (i.e. neural representations were more informative for speech that mismatched with stronger predictions), and negative when predictions matched heard speech (i.e. less informative representations for speech that matched stronger predictions). These findings are as expected for representations that signal prediction error (<xref ref-type="fig" rid="F1">Figure 1E</xref> right). Individually, correlations within the Matching condition were not significantly different from zero. But within the Mismatching condition, correlations were significantly positive from 320 to 544 ms (shown as shaded grey area in <xref ref-type="fig" rid="F4">Figure 4D</xref>).</p><p id="P37">We also applied our pattern analysis procedure to MEG sensor-space patterns after the data was transformed into a time-frequency representation of signal power (<xref ref-type="fig" rid="F4">Figure 4E</xref>). Here there was again an interaction between prediction strength and congruency with a peak at 560 ms in the theta band (peak at 4 Hz, extending to higher alpha frequencies). This interaction was driven by smaller MEG pattern distances (i.e. less informative neural representations) for Matching syllables that follow Strong rather than Weak predictions (<xref ref-type="supplementary-material" rid="SD1">Figure S2E</xref>). Again, this effect is in line with neural responses that signal prediction errors in heard speech. We also applied the same pattern analysis approach to high-frequency gamma activity (52-90 Hz) but did not observe condition-wise differences.</p></sec><sec id="S8"><title>fMRI pattern analysis</title><p id="P38">To localize the neural generators of the MEG pattern analysis effects reported above, we applied a similar multivariate pattern analysis approach to spatial searchlights in auditory brain areas in our fMRI data. As with our MEG pattern analysis, we first asked whether fMRI responses contained phonetic information about Syl2 by correlating neural pattern distances with the phonetic dissimilarity between the second syllables of item pairs. As shown in <xref ref-type="fig" rid="F5">Figure 5A</xref>, multivoxel patterns correlated with Syl2 phonetic dissimilarities in HG bilaterally (see <xref ref-type="table" rid="T1">Table 1</xref> for MNI space locations).</p><p id="P39">We next proceeded to examine between-condition differences in neural representations. Rather than directly analysing pattern distances (as with the equivalent MEG analysis shown in <xref ref-type="fig" rid="F4">Figure 4C</xref>), we opted to analyse a more specific measure of the correlation between neural patterns and Syl2 phonetic dissimilarities. This approach is more optimally suited to measure fMRI responses that are specific to Syl2 as opposed to representing both syllables. This is because while the temporally precise MEG signal can easily resolve neural responses to Syl1 and Syl2, this is not the case for fMRI which integrates sensory information over both syllables. However, by analysing the strength of correlation between fMRI pattern distances and Syl2 phonetic dissimilarities, we can more specifically examine the neural representations of Syl2. Within the left and right HG clusters previously shown in <xref ref-type="fig" rid="F5">Figure 5A</xref>, prediction strength and congruency showed interactive effects on the correlation between multivoxel pattern distances and Syl2 phonetic dissimilarities (shown in <xref ref-type="fig" rid="F5">Figure 5B</xref>; F(1,20) = 10.00, p = .005). This effect did not reliably differ in magnitude between the two hemispheres (the three-way interaction between prediction strength, congruency and hemisphere was non-significant: F(1,20) = 0.170, p = .684). Though the interaction effect failed to reach significance in the left hemisphere (F(1,20) = 3.304, p = .084), the interaction effect was significant in the right hemisphere (F(1,20) = 7.831, p = .011). While tests of simple effects within each hemisphere were not reliable, the numerical trends are entirely consistent with previous analyses and in line with computational predictions: increasing prediction strength resulted in a numerically weaker correlation for a matching Syl2 (F(1,20) = 2.85, p = .107) while the opposite numerical difference (i.e. a stronger correlation for stronger predictions) was observed for mismatching Syl2 items (F(1,20) = 2.70, p = .116).</p><p id="P40">We also examined whether fMRI responses contained phonetic information about Syl1 by correlating neural pattern distances with the phonetic dissimilarity between the first syllables of item pairs. However, we did not find significant correlations (all clusters p &gt; .6 uncorrected).</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P41">Across multiple imaging modalities (MEG, fMRI), analysis approaches (univariate signal magnitude and multivariate pattern), and signal domains (phase-locked, evoked activity and induced, time-frequency MEG responses), we show that neural responses to second syllables in spoken words are modulated by the strength and content of prior knowledge (i.e. predictions for expected speech sounds). We further show that these neural influences of prior knowledge begin around 200 ms after the onset of the second syllable, localise to early auditory regions (in fMRI, bilateral Heschl’s gyrus and STG) and are also expressed as changes in low-frequency (theta and alpha) power.</p><sec id="S10"><title>Neural patterns distinguish prediction error from sharpened signal computations</title><p id="P42">In the current study, the critical analyses used multivariate pattern analysis procedures that previous work has shown can unambiguously distinguish between sharpened signal and prediction error accounts [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>]. By examining the pattern and not just the overall strength of neural signals, responses tuned to the stimulus input features as well as those tuned away from the heard stimulus can be jointly modelled. In this analysis framework, if neural response patterns reflect prediction errors they should contain less information about strongly versus weakly predicted speech sounds (see computational simulations in <xref ref-type="fig" rid="F1">Figure 1E</xref>), and this is indeed what we observed for MEG and fMRI responses to familiar words.</p><p id="P43">While neural representations of second syllables are suppressed for stronger predictions that match heard speech, neural representations of exactly the same syllables show the opposite modulation for second syllables of pseudowords. That is, when predictions mismatch with sensory input, neural representations are enhanced after more strongly-than more weakly-predicting syllables. Our computational simulations and multivariate analysis confirm that the interaction between prediction strength and match/mismatch is entirely consistent with the operation of prediction error computations [<xref ref-type="bibr" rid="R8">8</xref>–<xref ref-type="bibr" rid="R11">11</xref>] but cannot be explained by sharpened signal accounts in which representations of speech are enhanced by prior knowledge [<xref ref-type="bibr" rid="R5">5</xref>–<xref ref-type="bibr" rid="R7">7</xref>]. Our study therefore provides convergent neural evidence – from different imaging modalities and multiple analysis approaches – that speech perception is supported by the computation of prediction error representations in auditory brain regions.</p><p id="P44">It should be noted that in prediction error accounts, computation of prediction errors is not the end goal of the system [as discussed in ref 21]. Rather, they are used to update – and thereby enhance or sharpen – future predictions. In some formulations of predictive coding [<xref ref-type="bibr" rid="R8">8</xref>,<xref ref-type="bibr" rid="R17">17</xref>], these sharpened predictions are represented in separate neural populations. However, we saw little evidence of sharpened representations in our study; perhaps because these sharpened predictions reside in deeper cortical layers [<xref ref-type="bibr" rid="R17">17</xref>] to which MEG [<xref ref-type="bibr" rid="R32">32</xref>] and BOLD [<xref ref-type="bibr" rid="R33">33</xref>] signals are less sensitive. It has also been proposed that prediction errors and predictions are communicated via distinct oscillatory channels [<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref>], although our time-frequency analysis did not provide support for this (discussed below). In future, methodological advances e.g. in layer-specific imaging [<xref ref-type="bibr" rid="R36">36</xref>], or more invasive methods [<xref ref-type="bibr" rid="R37">37</xref>] may help to resolve this issue. Insights may also come from studies that simultaneously probe multiple levels of processing (e.g. to observe prediction updating in higher-level semantic stages of processing).</p><p id="P45">Why are prediction error representations enhanced by prediction strength when predictions mismatch with speech input i.e. for pseudowords? Because the combinations of syllables in these items will never previously have been heard by listeners as English spoken words, the expected probabilities for the second syllables should always approach zero [in information theory terms, surprisal should be maximal; see <xref ref-type="bibr" rid="R3">ref 3</xref>]. Therefore, if neural representations were only sensitive to predictions for the heard syllables (and not alternative syllables), we would not expect any neural differences between Strong+Mismatch and Weak+Mismatch items. However, our manipulation of prediction strength reflects not only differences in predictions for the heard second syllables, but also in the uncertainty of predictions over all possible syllables: In information theory, this uncertainty is formulated as entropy [<xref ref-type="bibr" rid="R23">23</xref>]. Indeed in natural speech, predictions for specific speech sounds are negatively correlated with entropy [<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R23">23</xref>].</p><p id="P46">We suggest our results reflect both aspects of prediction i.e. for the heard syllables as well as the uncertainty of predictions over all possible syllables. In the Strong+Mismatch condition, a focal (low entropy) prediction is misaligned with sensory input. As a result, there is increased information in neural prediction errors because the sensory signal remains “unexplained”. By contrast, weak and more distributed (lower entropy) predictions will tend to align more with speech input representations. This illustrated by the pseudoword <italic>tanger</italic>. After hearing the first syllable /t { N/, predictions are spread weakly over multiple word candidates including the word <italic>tanker</italic> in which the penultimate sound is shared with <italic>tanger</italic>. This overlap results in reduced prediction errors in the Weak+Mismatch condition.</p><p id="P47">Our findings are consistent with neurophysiological findings showing that in addition to the informativeness of heard speech sounds, brain responses are also sensitive to the uncertainty of predictions made in parallel for alternative sounds [<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R38">38</xref>]. Going beyond this previous work however, our current findings suggest that neural sensitivity to prediction uncertainty reflects the specific neural computation of prediction error. Prediction uncertainty is a parsimonious explanation of the prediction strength effect on pseudoword representations. However, it is also possible that the strong violations of predictions that occur for pseudoword items may engage additional or qualitatively different mechanisms to those that support processing of familiar words. Bayesian inference for unfamiliar words is ill-defined because a pseudoword by definition has never previously been heard and therefore has a prior probability of zero [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R3">3</xref>]. In this situation, prediction error is maximal [<xref ref-type="bibr" rid="R3">3</xref>] and may function as a signal to indicate that the listener’s existing internal model is no longer applicable [<xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R39">39</xref>], triggering the encoding of a new word that may later enter into the lexicon [see below for further discussion and refs 3,40] or perceptual learning if the speaker systematically produces certain speech sounds with an unusual pronunciation [<xref ref-type="bibr" rid="R41">41</xref>]. By this view, opposite neural effects of prediction strength during perception of matching and mismatching second syllables arise because distinct neural processes are engaged during perception of words and pseudowords.</p></sec><sec id="S11"><title>Prediction error computations during natural listening</title><p id="P48">Our findings replicate and extend the findings of previous studies that manipulated prior knowledge by presenting matching or nonmatching written words before highly degraded spoken words [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R42">42</xref>]. In this previous work, neural representations showed interactive influences of prior knowledge and signal quality, consistent with a prediction error account. We go beyond this previous work by demonstrating prediction error computations in a more natural listening situation. The present experiments are more naturalistic in two respects: Firstly, we presented clear speech rather than the highly degraded (and hence less natural sounding) noise-vocoded words used previously. Secondly, we used an intrinsic manipulation of prior knowledge based on listeners’ long-term linguistic knowledge of familiar spoken words rather than an extrinsic manipulation of predictions based on written text. This demonstration of prediction errors during more natural listening situations is important because it directly addresses the concern that previous results reflect a “prediction encouraging” paradigm [<xref ref-type="bibr" rid="R22">22</xref>] that would not generalise to other listening situations. Instead, our findings are consistent with the idea that lexical identification operates through prediction error computations and that these are integral to perceptual processing of speech irrespective of the listening situation [<xref ref-type="bibr" rid="R3">3</xref>,<xref ref-type="bibr" rid="R43">43</xref>–<xref ref-type="bibr" rid="R46">46</xref>].</p><p id="P49">One might argue that our findings fall short of demonstrating prediction error computation in a fully naturalistic listening situation as our stimuli comprised individual spoken words as opposed to connected speech. In other work investigating speech processing during perception of connected speech (e.g. stories), neurophysiological responses (measured by MEG, EEG or electrocorticography) are shown to correlate with various measures of time-varying predictability, including phoneme surprisal [<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R27">27</xref>], phoneme entropy [<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R27">27</xref>] and lexical entropy [<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R27">27</xref>]. While these findings clearly demonstrate that speech processing is strongly modulated by prior knowledge, consistent with the notion that speech perception is fundamentally a process of Bayesian inference [<xref ref-type="bibr" rid="R2">2</xref>], they do not indicate whether the underlying neural computations involve sharpened signals or prediction errors. Consider for example, the common observation of reduced responses to predictable speech sounds. This response reduction is compatible with either a prediction error account (through the suppression of responses tuned to input features that are predicted) or a sharpened signal mechanism (through the suppression of responses tuned away from the input features; for a more detailed explanation, see Introduction and refs. [<xref ref-type="bibr" rid="R5">5</xref>,<xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R20">20</xref>]). Indeed, this pattern is evident in our simulations as univariate responses are reduced for strongly versus weakly predicted second syllables of words for both sharpened signal and prediction error simulations.</p><p id="P50">More recently, studies employing connected speech paradigms have shown that neural representations of speech segments and acoustic features are themselves modulated by predictability [<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R47">47</xref>]. These studies use an approach analogous to the present pattern analysis since they specifically link neural effects of surprisal and entropy to neural representations of specific segments or acoustic features. However, previous findings appear inconsistent with respect to sharpened signal and prediction error accounts since some studies show that neural representations are enhanced when predictions are strong [<xref ref-type="bibr" rid="R47">47</xref>] while others show the opposite [<xref ref-type="bibr" rid="R16">16</xref>] or even effects in both directions at different latencies [<xref ref-type="bibr" rid="R29">29</xref>].</p><p id="P51">These inconsistent results from story listening paradigms therefore suggest that alternative experimental manipulations of listeners’ predictions are needed to distinguish between accounts. Our pseudoword condition creates a second situation in which prediction error and sharpened signal computations can be teased apart. Pseudowords are previously unfamiliar to participants, and hence create a unique listening situation in which predictions entirely mismatch with sensory input. Under a sharpened signal account, the pattern of neural representations of speech sounds forming a pseudoword (as compared with a word) should be suppressed, reflecting the low prior probability of hearing these speech sounds. In contrast, under a prediction error account, the exact opposite should occur i.e. enhanced representations for pseudowords, particularly following strongly-predicting first syllables. Neural responses to pseudowords are consistent with this latter pattern and hence are likely to reflect prediction error computations. In our signal magnitude analyses, we observed larger MEG and fMRI responses for pseudowords versus words. For pattern analyses, although we did not observe a reliable main effect of prediction congruency, neural representations of speech for pseudoword items were enhanced following strongly-versus weakly-predicting first syllables. Overall, these pseudoword results again favour a prediction error account.</p><p id="P52">We have previously argued that larger prediction error responses for pseudowords compared to real words provide a potential signal for novelty detection and pseudoword learning [3,interpreting results from 11,48]. Our demonstration of more informative prediction errors for syllables that mismatch with more strongly-predicting first syllables suggests that such pseudowords might be more readily learned. We note that memory studies using visual stimuli have similarly argued for enhanced learning of face-scene associations that generate larger prediction errors at encoding [<xref ref-type="bibr" rid="R49">49</xref>]. Consistent with this proposal, are the present behavioural results from the cued memory recall test conducted after the fMRI scans. This showed that listeners’ recall for pseudowords is more accurate for items that strongly mismatched with predictions in the scanner.</p></sec><sec id="S12"><title>Spatiotemporal and spectral profile of prediction error computations</title><p id="P53">Our results also establish the timing and location of prediction error computations in response to speech. Previous studies using EEG [<xref ref-type="bibr" rid="R13">13</xref>], MEG [<xref ref-type="bibr" rid="R12">12</xref>–<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R29">29</xref>], fMRI [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R53">53</xref>] and electrocorticography [<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R54">54</xref>] disagree about when and where prediction errors are computed in the cortical hierarchy. Each of these techniques has their strengths but none combines spatiotemporal resolution with wide coverage of the cortex. Perhaps because of this, it remains unclear whether predictive neural computations emerge only once sensory information has been transformed into non-acoustic (e.g. phonetic) representations [<xref ref-type="bibr" rid="R12">12</xref>] or whether acoustic representations themselves can support predictive computations [<xref ref-type="bibr" rid="R21">21</xref>]. In the present study, we obtained MEG and fMRI responses to the same stimuli, while participants performed the same incidental (pause detection) task to maintain attention. This feature of our study enabled us to map out prediction error computations precisely in both time and space.</p><p id="P54">In our MEG results, effects of prediction strength and congruency occurred around 200 ms following the onset of the critical second syllable. Note however that our word and pseudoword stimuli are identical until approximately 100 ms after the onset of the second syllable (corresponding to the onset of the subsequent segment). Therefore, relative to when the critical acoustic information became available to listeners, the interaction between prediction strength and congruency occurred with a latency of approximately 100 ms. This latency is compatible with a low-level acoustic locus of prediction error computation but perhaps not early enough to definitively rule out a higher-level (non-acoustic) locus, if word identification is supported by rapid (&lt; 50 ms) cortical mechanisms [<xref ref-type="bibr" rid="R55">55</xref>].</p><p id="P55">However, our fMRI results enable more definitive conclusions, since searchlight pattern results show prediction effects in primary auditory cortex (Heschl’s Gyrus). This localisation to Heschl’s Gyrus is striking as it suggests that predictive computations for speech can operate even at the lowest levels of the cortical processing hierarchy, which leaves open the possibility that top-down predictive influences may even extend subcortically [<xref ref-type="bibr" rid="R56">56</xref>]. Future work manipulating prediction at different linguistic levels (e.g. sublexical, lexical and supralexical) would be valuable to establish the generality of this finding, and to determine whether these effects are uniquely or differentially localised to specific cortical regions.</p><p id="P56">In both signal magnitude and pattern analysis of MEG responses, the interaction between prediction strength and congruency occurred in low-frequency neural responses, with a peak in the theta range (3-4 Hz). An influential proposal claims that bottom-up prediction errors are conveyed by high-frequency (gamma) activity while top-down predictions are conveyed by low-frequency (alpha/beta) signals [<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R57">57</xref>]. This is consistent with the idea that neural representations integrate sensory information over longer timescales at higher cortical levels (where top-down predictions are proposed to originate; see [<xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R51">51</xref>,<xref ref-type="bibr" rid="R58">58</xref>–<xref ref-type="bibr" rid="R60">60</xref>]). If the current findings reflect prediction errors as we have argued, then we might have expected modulation in the gamma range, which we did not observe. Instead, our findings are consistent with other work suggesting that prediction errors for speech are conveyed in low-frequency neural signals [<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R61">61</xref>]. This does not rule out an additional role for gamma responses. It may be for example that theta and gamma responses are coupled [<xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R63">63</xref>] and that gamma effects are not observed in the present study due to the generally lower signal-to-noise ratio of MEG responses at higher frequencies [<xref ref-type="bibr" rid="R64">64</xref>].</p></sec><sec id="S13"><title>Integrating computational models and brain responses to reveal the neural computations supporting speech processing</title><p id="P57">The current study follows previous work that also combined computational models and neuroimaging to test sharpened signal and prediction error accounts [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>]. As has been commented on elsewhere [<xref ref-type="bibr" rid="R65">65</xref>], computational models provide the most reliable way to establish the predictions of a particular theory. This observation also applies to the goal of distinguishing sharpened signal and prediction error theories. For example, opposite outcomes are observed in simulated prediction error representations depending on the level of sensory degradation [see refs <xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref> and also computational simulations in <xref ref-type="supplementary-material" rid="SD1">Figure S3</xref> and <xref ref-type="sec" rid="S14">Methods</xref> section]. While this result is clearly apparent in simulations, it was not previously obvious from verbal descriptions of prediction error theories. In the current study, we also see opposite outcomes in simulated prediction errors depending on whether predictions match or mismatch sensory input during word and pseudoword processing, respectively. Through computational simulations, we could establish that this is a unique signature of prediction error representations during perception of words and pseudowords and furthermore show that this signature is observed in neural responses.</p><p id="P58">Importantly, our simulations also provide an explanation for why prediction strength and match/mismatch combine interactively to influence prediction error representations (see discussion above on why prediction error representations are enhanced by prediction strength when predictions mismatch speech input). However, future work is needed to develop a more complete model of prediction error processing [for examples of other ongoing efforts, see refs 66–68]. For example, in the present model implementation, predictions for upcoming speech sounds are subtracted from the input to compute prediction errors but these segmental predictions are never used to update higher-level lexical predictions. Rather, lexical-level predictions are computed afresh as each new segment is presented (by combining word priors and likelihoods using Bayesian inference). A more complete model would update predictions via prediction errors. It would also operate on real (i.e. acoustic) speech input or connected speech without explicitly requiring word onset locations. We note with interest the success of recent computational models which operate on acoustic speech inputs and are used to predict neural data [<xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R70">70</xref>]. We hope that the present results can constrain future iterations of these models.</p></sec></sec><sec id="S14" sec-type="methods"><title>Methods</title><sec id="S15" sec-type="subjects"><title>Participants</title><p id="P59">40 participants (19 in the MEG experiment, 21 in the fMRI experiment) were tested after being informed of the study’s procedure, which was approved by the Cambridge Psychology Research Ethics Committee. All were right-handed, native speakers of English, aged between 18 and 40 years and had no self-reported history of hearing impairment or neurological disease. The mean age in the MEG experiment was 25 years (SD = 4.63; 12 female, 7 male) and 23 years in the fMRI experiment (SD = 3.14; 13 female, 8 male).</p></sec><sec id="S16"><title>Stimuli</title><p id="P60">Spoken stimuli were 16-bit, 44.1 kHz recordings of a male speaker of southern British English and their duration ranged from 388 to 967 ms (mean = 621, SD = 107). A set of 64 bisyllabic words (Match items) were first selected from the CELEX database [<xref ref-type="bibr" rid="R71">71</xref>] based on the probability of the second syllable (Syl2) conditioned on the first syllable (Syl1): <disp-formula id="FD1"><label>Eq. 1</label><mml:math id="M1"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn><mml:mo>∣</mml:mo><mml:mi>S</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.2em"/><mml:mi>b</mml:mi><mml:mi>o</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mspace width="0.2em"/><mml:mi>s</mml:mi><mml:mi>y</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P61">For this we used syllabified phonetic transcriptions in CELEX such that the first syllable /t { N/, for example, in words such as <italic>tango</italic> and <italic>tangle</italic> were considered to match the speech signal since all these words share the same first syllable /t { N/. However, words such as “tank” were not considered as matching since this word consists of the single syllable /t { N k/. Words were selected as sets of eight items (one example item set is shown in <xref ref-type="fig" rid="F1">Figure 1A</xref>). Within a set of eight items, each word had a unique Syl1 that strongly or weakly predicted one of two Syl2 syllables. By cross-splicing the first and second syllables of these words, we were able to create a set of pseudowords (Mismatch items) that contained the same Syl2 syllables as the real words.</p><p id="P62">To ensure that Syl2 syllables shared between items were acoustically identical, all four recorded syllables were combined using audio morphing using STRAIGHT software [<xref ref-type="bibr" rid="R72">72</xref>] implemented in Matlab (The Mathworks Inc). This software decomposes speech signals into source information and spectral information, and permits high quality speech resynthesis based on modified versions of these representations. This enables flexible averaging and interpolation of parameter values that can generate acoustically intermediate speech tokens [<xref ref-type="bibr" rid="R48">48</xref>, for application see <xref ref-type="bibr" rid="R73">73</xref>]. Exploiting this capability of STRAIGHT, all words and pseudowords sharing the same Syl2 were constructed to be acoustically identical following Syl1 (by averaging STRAIGHT parameters across the four instances of the same Syl2 syllable within a set and splicing the resulting syllable onto Syl1). To ensure that all syllables sounded natural in these contexts, we constrained word selection such that all Syl2 syllables within a set started with the same speech segment. In this way, we could cross-splice syllables without conflicting co-articulatory information at the onset of Syl2. As a result, the point at which words and pseudowords diverged (the divergence point) occurred not at the onset of Syl2 but one speech segment later (post divergence point speech segments highlighted as bold in <xref ref-type="fig" rid="F1">Figure 1A</xref>). Additional stimuli were derived by replacing the Syl1, Syl2, or both syllables, with spectrally and intensity (RMS) matched noise (data for these stimuli are not reported here). In total there were eight sets of items and therefore 240 stimuli (32 items in each of the following conditions: Strong+Match, Weak+Match, Strong+Mismatch, Weak+Mismatch; 32 each of Strong+Noise, Weak+Noise; 32 of Noise+Syl2; 16 of Noise+Noise). For analysis of stimulus properties, see <xref ref-type="supplementary-material" rid="SD1">supplementary Results</xref>.</p><p id="P63">Before the experiment, participants completed a brief practice session lasting approximately three minutes that contained examples of all the stimulus types included (spoken words, pseudowords with and without noise) but using different items to the main experiment. Stimulus delivery was controlled with Psychtoolbox [<xref ref-type="bibr" rid="R74">74</xref>] scripts in Matlab.</p></sec><sec id="S17" sec-type="methods"><title>Procedure</title><p id="P64">Stimulus presentation and task procedure was nearly identical for MEG and fMRI listeners. On each trial, listeners heard one of the spoken items separated by a stimulus onset asynchrony (SOA) of 2.5 sec (see <xref ref-type="supplementary-material" rid="SD1">Figure S1A</xref>). For the MEG experiment, this SOA was jittered by adding a random time interval of ±100 ms. For the fMRI experiment, a constant SOA was used, and stimuli were presented at the offset of the scans (sparse imaging sequence). Participants were asked to look at a fixation cross in the centre of the screen and respond as quickly and accurately as possible whenever they heard a brief pause in the speech stimuli. Pauses consisted of an additional 200 ms silence inserted between Syl1 and Syl2. Pilot testing revealed that pause detection was more difficult when one or both syllables were replaced by noise.</p><p id="P65">Trials were randomly ordered during each of five presentation blocks of 288 trials. Each block consisted of a single presentation of all stimuli plus ‘Pause’ stimuli. Across all five blocks, the order of ‘Pause’ and ‘No Pause’ items was completely randomised; in total there was one ‘Pause’ item for every six ‘No Pause’ items, with this ratio constant across stimulus type (Strong+Match, Weak+Match etc.). Participants received feedback on performance (% correct) at the end of each block.</p></sec><sec id="S18"><title>Computational simulations</title><p id="P66">Both sharpened signal and prediction error computations have the goal of implementing Bayes perceptual inference for spoken words [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R9">9</xref>]: <disp-formula id="FD2"><label>Eq. 2</label><mml:math id="M2"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>E</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mspace width="0.2em"/><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>d</mml:mi><mml:msub><mml:mtext> </mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>∣</mml:mo><mml:mi>W</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:math></disp-formula></p><p id="P67">This equation computes the conditional (posterior) probability of each word, given their prior probabilities (frequency of occurrence) and the available sensory evidence. We modelled uncertainty in the sensory evidence based on the acoustic similarity between individual speech segments. Acoustic representations were obtained from a large set of 1024 words spoken by the same speaker as for the present stimuli. These stimuli comprised recordings from previous studies [<xref ref-type="bibr" rid="R75">75</xref>–<xref ref-type="bibr" rid="R77">77</xref>] as well as the 32 Strong Match items and 32 Weak Match items from the current study. From these stimuli, we computed acoustic energy for individual speech segments (phonemes): 48 unique segments in total, averaged over 3659 repeated tokens (median token count = 52). Acoustic energy was expressed as a function of time, frequency and spectrotemporal modulations [<xref ref-type="bibr" rid="R78">78</xref>], as computed by the nsltools toolbox in Matlab [<ext-link ext-link-type="uri" xlink:href="http://nsl.isr.umd.edu/downloads.html">http://nsl.isr.umd.edu/downloads.html</ext-link>; for details see <xref ref-type="bibr" rid="R21">21</xref>].</p><p id="P68">Segment onset times were obtained using a forced-alignment algorithm included as part of BAS software [79; <ext-link ext-link-type="uri" xlink:href="https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSGeneral">https://clarin.phonetik.uni-muenchen.de/BASWebServices/interface/WebMAUSGeneral</ext-link>]. Acoustic representations were vectorized and averaged over multiple productions of the same segment. We then obtained a between-segment similarity matrix using negative Euclidean distances and normalized each row of this matrix using the softmax function (combined with a temperature parameter <italic>T</italic>) such that each row could be interpreted as segment probabilities: <disp-formula id="FD3"><label>Eq. 3</label><mml:math id="M3"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:msub><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msubsup><mml:mstyle displaystyle="true"><mml:mi>Σ</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mtext>exp</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>d</italic> denotes the Euclidean distance. From these segment probabilities, we could compute the sensory evidence for a word by applying the product rule over word segments [see <xref ref-type="bibr" rid="R2">2</xref>]. The sensory evidence for an unfolding word will be strong if its constituent segments match internal representations that are acoustically dissimilar to other speech sounds in the language. It will also be strong if there are few word competitors sharing the same speech segments. Given that our stimulus set consisted of clear speech recordings, a reasonable assumption is that individual segment probabilities are close to one (indicating high certainty). Nonetheless, even clear speech segments will be perceived with momentary ambiguities as sub-phonemic information unfolds over the duration of a segment. In the absence of human sub-phonemic confusion data [<xref ref-type="bibr" rid="R80">80</xref>] for the British accent of the present stimuli, we scaled acoustic similarities to simulate a range of overall sensory uncertainty levels by varying the Softmax temperature parameter. The same qualitative patterns were present over a range of sensory uncertainty levels (<xref ref-type="supplementary-material" rid="SD1">Figure S3</xref>). The one exception to this is apparent for simulations with the highest degree of sensory uncertainty for which prediction error representations in the Match condition are enhanced relative to simulations with reduced sensory uncertainty (compare the height of the first bar in each of the bottom row of plots in <xref ref-type="supplementary-material" rid="SD1">Figure S3B</xref>). The effect of this is that – under high uncertainty – results show a reverse pattern with enhanced neural representations for Strong versus Weak items. This is consistent with previous modelling and experimental data [<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R21">21</xref>] and is explained by an increase in the informativeness of prediction errors that occurs when strong predictions match noisy or ambiguous sensory input: Even though speech segments are accurately predicted, their <italic>acoustic form</italic> is not as expected (deviating from clear speech).</p><p id="P69">Note that our stimuli were selected based on syllable conditional probabilities p(Syl2|Syl1), which used the syllabified phonetic transcriptions in CELEX. For consistency, we therefore incorporated the syllable boundaries from CELEX into our model simulations. We achieved this by adding a separate segment to the model’s inventory of segments, representing the boundary between syllables. Thus, an item like “bingo” was represented by six segments in total (five speech sounds plus the boundary between the two component syllables). In <xref ref-type="fig" rid="F1">Figure 1B and 1C</xref>, we omit the syllable boundary segment for display purposes.</p><p id="P70">To simulate speech processing as each speech segment in our stimuli was heard, we used Bayes theorem to compute the posterior probability distribution over all words in the lexicon. Then to compute predictions for each upcoming speech segment, we summed posterior probabilities over words sharing the same segment. These segment predictions were then multiplied by individual segment probabilities (derived from acoustic similarity as explained above), to simulate the sharpened signal computation (normalizing to sum to one). For simulating the prediction error computation, segment predictions were instead subtracted from segment probabilities. For analysis of the simulated representations, pattern distances were computed as the Euclidean distance between model signals for pairs of items at the divergence point. For the signal magnitude analysis, we analysed model representations specifically for the speech segment that was heard at the divergence point.</p></sec><sec id="S19"><title>MEG acquisition and analysis</title><p id="P71">Magnetic fields were recorded with a VectorView system (Elekta Neuromag, Helsinki, Finland) containing two orthogonal planar gradiometers at each of 102 positions within a hemispheric array. Data were also acquired by magnetometer and EEG sensors. However, only data from the planar gradiometers were analysed as these sensors have maximum sensitivity to cortical sources directly under them and are therefore less sensitive to noise artifacts [<xref ref-type="bibr" rid="R81">81</xref>]. To monitor eye and heart activity, EOG and ECG signals were recorded with bipolar electrodes.</p><p id="P72">MEG data were processed using the temporal extension of Signal Source Separation [<xref ref-type="bibr" rid="R82">82</xref>] in Maxfilter software to suppress noise sources, compensate for motion, and reconstruct any bad sensors. Also using MaxFilter, the data were transformed to a common coordinate frame for each participant separately (to compensate for changes in head position between blocks). Subsequent processing was done in SPM12 (Wellcome Trust Centre for Neuroimaging, London, UK), FieldTrip (Donders Institute for Brain, Cognition and Behaviour, Radboud University Nijmegen, the Netherlands) and The Decoding Toolbox [<xref ref-type="bibr" rid="R83">83</xref>] implemented in Matlab.</p><p id="P73">MEG data were downsampled to 250 Hz and ICA used to remove eye and heart artefacts (based on an automatic procedure correlating the component time-series with EOG and ECG signals). When epoching, we retained only “no pause” trials. MEG recordings were epoched from -1700 ms to 2000 ms relative to the onset of Syl2 and epochs with large amplitude signals were discarded (time- and channel-averaged power larger than 2 standard deviations from the condition-specific mean for signal magnitude analysis; power larger than 3 standard deviations from the pooled mean for pattern analysis). For signal magnitude (evoked response) analyses only, the data were baseline-corrected from -750 to -500 ms relative to Syl2 onset; this corresponds to the silence period before speech onset (since the duration of Syl1 ranged from 130 to 479 ms; see <xref ref-type="supplementary-material" rid="SD1">Figure S4C</xref> for distribution of Syl1 duration).</p><p id="P74">For time-frequency analysis, we computed the power between 2 and 48 Hz (in 1 Hz steps) by convolving the data from each trial with Morlet wavelets of 7 cycles. To analyse high frequency activity (52-90 Hz), we used multi-taper estimation (±10 Hz smoothing, 200 ms time-windows with a step size of 20 ms). We analysed differences in power attributable to our experimental manipulations without baseline normalization. For time-domain analyses, the data were additionally low-pass filtered below 30 Hz prior to trial averaging. Note that in the case of signal magnitude analyses, MEG responses reflect the average of 160 trials before outlier removal (5 repetitions × 32 items per stimulus type) while for pattern analyses, MEG responses reflect the average of 5 repetitions (since here we analyze item-specific responses). Subsequent analysis was based on the period immediately before and after Syl2 onset (-200 to 1000 ms).</p><p id="P75">For signal magnitude (evoked response) analysis, the MEG data across the sensor array were summarized as the root mean square (RMS) over the 20 sensors with the largest evoked response (separately in each participant and hemisphere). Essentially the same results are obtained when selecting 10 or 40 sensors. The same sensor selections used for the signal magnitude (evoked response) analysis were also used when analysing time-frequency power. For pattern analyses, neural pattern distances were computed by taking the Euclidean distance between sensor patterns (over the entire 204 sensor-array) evoked by each pair of items in the relevant stimulus set. To assess whether MEG responses reflected the phonetic information about the syllables, neural pattern distances were correlated with the phonetic dissimilarities between the syllables of item pairs using Fisher-transformed Spearman’s correlations. To compute the phonetic dissimilarities we used the Levenshtein distance between the phonetic transcriptions for each syllable, normalized by the maximum number of segments within the two syllables being compared; shown in <xref ref-type="fig" rid="F4">Figure 4A</xref> for Syl2 and <xref ref-type="supplementary-material" rid="SD1">Figure S5</xref> for Syl1 [see <xref ref-type="bibr" rid="R77">77</xref> for a similar method applied to scoring phonetic responses]. To maximize statistical power for this particular analysis, correlations were computed both across- and within-condition (see <xref ref-type="fig" rid="F4">Figure 4A</xref>). As shown in <xref ref-type="fig" rid="F4">Figure 4A</xref>, we also excluded dissimilarity of item pairs such as “cargar” and “cougo” since similar but opposite prediction errors are evoked during Syl2 of these items. For example, for “cargar”, listeners predict /g @U/ but hear /g @ r*/. For “cougo”, listeners predict /g @ r*/ but hear /g @U/. The pattern of prediction error in these two cases is identical, albeit opposite in polarity. Given uncertainty on how the polarity of prediction errors is encoded in neural responses [<xref ref-type="bibr" rid="R84">84</xref>], we exclude these pseudoword item pairs from pattern analysis of our neural data and model simulations. We also exclude the corresponding word item pairs (e.g. “cougar” and “cargo”; shown as NaNs in <xref ref-type="fig" rid="F4">Figure 4A</xref>).</p><p id="P76">When correlating syllable conditional probabilities with MEG responses (either magnitude-based or pattern-based measures), we also used Fisher-transformed Spearman’s correlations. For pattern analysis, in which the dependent measures reflect pattern distances for pairs of items, we averaged syllable conditional probabilities for the items in each pair of words (after log transformation).</p><p id="P77">For both signal magnitude (RMS signals, time-frequency power or Spearman correlations) and pattern analyses (pattern distances or Spearman correlations), group-level one-sample t-tests were performed for each timepoint (and frequency bin when relevant) while controlling the family-wise error (FWE) rate using a non-parametric cluster-based permutation procedure based on 5000 iterations [<xref ref-type="bibr" rid="R85">85</xref>].</p><p id="P78">These tests were performed on contrasts of the data, testing for the main effects and interactions in our experimental design, as well as any simple effects. Reported effects were obtained by using a cluster defining height threshold of p &lt; .05 with a cluster t-sum threshold of p &lt; .05 (FWE corrected), unless otherwise stated.</p></sec><sec id="S20"><title>fMRI acquisition and analysis</title><p id="P79">Imaging data were collected on a Siemens 3 Tesla Prisma MRI scanner (<ext-link ext-link-type="uri" xlink:href="http://www.siemens.com">http://www.siemens.com</ext-link>). A total of 291 echo planar imaging (EPI) volumes were acquired in each of 5 scanning runs, using a 32-channel head coil and a multiband sparse imaging sequence (TR = 2.50 sec; TA = 1.135 sec; TE = 30 ms; 48 slices covering the whole brain; flip angle = 78 deg; in-plane resolution = 3 × 3 mm; matrix size = 64 × 64; echo spacing = 0.5 ms; inter-slice gap = 25%). After the third run, field maps were acquired (short TE = 10 ms; long TE = 12.46 ms). The experimental session commenced with the acquisition of a high-resolution T1-weighted structural MRI scan (TR = 2250 ms; TE = 2.99 ms; flip angle = 9°; 1 mm isotropic resolution; matrix size: 256 × 240 × 192 mm; GRAPPA acceleration factor PE = 2; Reference lines PE = 24).</p><p id="P80">fMRI pre-processing was performed in SPM12. After discarding the first four volumes to allow for magnetic saturation effects, the remaining images were realigned and unwarped to the first volume to correct for movement of participants during scanning. Also at the unwarping stage, the acquired field maps were used to correct for geometric distortions in the EPI due to magnetic field variations. Realigned images were co-registered to the mean functional image and then subjected to statistical analysis. For signal magnitude analysis, prior to further processing, images were also normalized to the Montreal Neurological Institute (MNI) template image using the parameters from the segmentation of the structural image (resampled resolution: 2 × 2 × 2 mm) and smoothed with a Gaussian kernel of 6 mm full-width at half-maximum. For pattern analysis, normalization and smoothing were performed only after computing pattern distances.</p><p id="P81">Statistical analysis was based on the general linear model (GLM) of each participant’s fMRI time series, using a 1/128 second highpass filter and AR1 correction for auto-correlation. The design matrix comprised the auditory stimulus events (onset of the spoken words), each modeled as a stick (delta) function and convolved with the canonical haemodynamic response function specified in SPM software. For signal magnitude analysis, separate columns were specified for each of the stimulus types in addition to a column for button presses (replicated for each of the five experimental blocks). For pattern analysis, the design matrix was identical but separate columns were specified for each individual item.</p><p id="P82">Pattern analysis was performed using searchlight analysis [<xref ref-type="bibr" rid="R86">86</xref>] as implemented in the Decoding toolbox, using spheres with a radius of 8 mm and constrained to voxels within the whole-brain mask generated by SPM during model estimation. Pattern distances were computed using the cross-validated Mahalanobis distance [<xref ref-type="bibr" rid="R30">30</xref>]. This ‘Crossnobis’ distance is equivalent to the (cross-validated) Euclidean distance, normalized by the noise covariance between voxels (estimated from the GLM residuals). Cross-validation ensures that the expected pattern distance is zero if two voxel patterns are not statistically different from each other, making it a readily interpretable distance measure. Note that because of this cross-validation, the Crossnobis distance can sometimes be negative if it’s true value is close to zero in the presence of noise. The Crossnobis distance has been demonstrated to be a more reliable and accurate metric for characterizing multivoxel patterns than the correlation or Euclidean distance [<xref ref-type="bibr" rid="R30">30</xref>]. To assess whether fMRI responses reflected the phonetic information about Syl2, neural pattern distances were Spearman correlated with the phonetic dissimilarities between Syl2 portions of item pairs using the same Levenshtein distance measure as the MEG analysis (shown in <xref ref-type="fig" rid="F4">Figure 4A</xref>).</p><p id="P83">For both signal magnitude (contrast images from first-level models or Spearman correlations) and pattern analyses (Spearman correlations), group-level one-sample t-tests were performed for each voxel within a mask covering superior and middle temporal regions (shown in <xref ref-type="fig" rid="F3">Figure 3A</xref>). Inference was conducted parametrically, controlling the family-wise error (FWE) rate using random field theory. Reported effects were obtained by using a cluster defining height threshold of p &lt;.001 with a cluster extent threshold of p &lt; .05 (FWE corrected), unless otherwise stated.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS189088-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d22aAcEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgements</title><p>This research was supported by intermural funding from the Medical Research Council to the Cognition and Brain Sciences Unit (MC_UU_0005/5 and MC_UU_00030/6 to M.H.D.). We thank Sander Van Bree and Clare Cook for assistance with MEG data collection and Steve Eldridge and Karen Kabakulu for assistance with radiography.</p></ack><sec id="S21" sec-type="data-availability"><title>Data availability statement</title><p id="P84">Stimuli, data and analysis scripts will be made available on OSF upon publication of this manuscript.</p></sec><sec id="S22" sec-type="data-availability"><title>Code availability statement</title><p id="P85">Matlab code for the computational simulations will be made available on OSF upon publication of this manuscript.</p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><article-title>Prediction, Bayesian inference and feedback in speech recognition</article-title><source>Language, Cognition and Neuroscience</source><year>2015</year><volume>3798</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC4685608</pub-id><pub-id pub-id-type="pmid">26740960</pub-id><pub-id pub-id-type="doi">10.1080/23273798.2015.1081703</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name></person-group><article-title>Shortlist B: a Bayesian model of continuous speech recognition</article-title><source>Psychological review</source><year>2008</year><volume>115</volume><fpage>357</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">18426294</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Sohoglu</surname><given-names>E</given-names></name></person-group><chapter-title>Three functions of prediction error for Bayesian inference in speech perception</chapter-title><source>Cog Neuro</source><publisher-name>MIT Press</publisher-name><year>2020</year><fpage>177</fpage><lpage>192</lpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title>The TRACE model of speech perception</article-title><source>Cognitive Psychology</source><year>1986</year><volume>18</volume><fpage>1</fpage><lpage>86</lpage><pub-id pub-id-type="pmid">3753912</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><article-title>How Do Expectations Shape Perception?</article-title><source>Trends in Cognitive Sciences</source><year>2018</year><volume>22</volume><fpage>764</fpage><lpage>779</lpage><pub-id pub-id-type="pmid">30122170</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>SO</given-names></name><name><surname>Schrater</surname><given-names>P</given-names></name><name><surname>Kersten</surname><given-names>D</given-names></name></person-group><article-title>Perceptual grouping and the interactions between visual cortical areas</article-title><source>Neural networks : the official journal of the International Neural Network Society</source><year>2004</year><volume>17</volume><fpage>695</fpage><lpage>705</lpage><pub-id pub-id-type="pmid">15288893</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitken</surname><given-names>F</given-names></name><name><surname>Turner</surname><given-names>G</given-names></name><name><surname>Kok</surname><given-names>P</given-names></name></person-group><article-title>Prior Expectations of Motion Direction Modulate Early Sensory Processing</article-title><source>The Journal of Neuroscience</source><year>2020</year><volume>40</volume><fpage>6389</fpage><lpage>6397</lpage><pub-id pub-id-type="pmcid">PMC7424874</pub-id><pub-id pub-id-type="pmid">32641404</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0537-20.2020</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature neuroscience</source><year>1999</year><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>With or without you: predictive coding and Bayesian inference in the brain</article-title><source>Current Opinion in Neurobiology</source><year>2017</year><volume>46</volume><fpage>219</fpage><lpage>227</lpage><pub-id pub-id-type="pmcid">PMC5836998</pub-id><pub-id pub-id-type="pmid">28942084</pub-id><pub-id pub-id-type="doi">10.1016/j.conb.2017.08.010</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>A theory of cortical responses</article-title><source>Philosophical transactions of the Royal Society of London Series B, Biological sciences</source><year>2005</year><volume>360</volume><fpage>815</fpage><lpage>36</lpage><pub-id pub-id-type="pmcid">PMC1569488</pub-id><pub-id pub-id-type="pmid">15937014</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2005.1622</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagnepain</surname><given-names>P</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Temporal Predictive Codes for Spoken Words in Auditory Cortex</article-title><source>Current biology : CB</source><year>2012</year><volume>22</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="pmcid">PMC3405519</pub-id><pub-id pub-id-type="pmid">22425155</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2012.02.015</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Rapid Transformation from Auditory to Linguistic Representations of Continuous Speech</article-title><source>Current biology : CB</source><year>2018</year><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC6339854</pub-id><pub-id pub-id-type="pmid">30503620</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Armeni</surname><given-names>K</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>A hierarchy of linguistic predictions during natural language comprehension</article-title><source>Proc Natl Acad Sci USA</source><year>2022</year><volume>119</volume><elocation-id>e2201968119</elocation-id><pub-id pub-id-type="pmcid">PMC9371745</pub-id><pub-id pub-id-type="pmid">35921434</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2201968119</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><article-title>Two Distinct Neural Timescales</article-title><source>Neuron</source><year>2020</year><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC6981026</pub-id><pub-id pub-id-type="pmid">31806493</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.019</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keshishian</surname><given-names>M</given-names></name><name><surname>Akkol</surname><given-names>S</given-names></name><name><surname>Herrero</surname><given-names>J</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><article-title>Joint, distributed and hierarchically organized encoding of linguistic features in the human auditory cortex</article-title><source>Nat Hum Behav</source><year>2023</year><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC10417567</pub-id><pub-id pub-id-type="pmid">36864134</pub-id><pub-id pub-id-type="doi">10.1038/s41562-023-01520-0</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tezcan</surname><given-names>F</given-names></name><name><surname>Weissbart</surname><given-names>H</given-names></name><name><surname>Martin</surname><given-names>AE</given-names></name></person-group><article-title>A tradeoff between acoustic and linguistic feature encoding in spoken language comprehension</article-title><person-group person-group-type="editor"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><source>eLife</source><year>2023</year><volume>12</volume><elocation-id>e82386</elocation-id><pub-id pub-id-type="pmcid">PMC10328533</pub-id><pub-id pub-id-type="pmid">37417736</pub-id><pub-id pub-id-type="doi">10.7554/eLife.82386</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AMM</given-names></name><name><surname>Usrey</surname><given-names>WMM</given-names></name><name><surname>Adams</surname><given-names>RAA</given-names></name><name><surname>Mangun</surname><given-names>GRR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJJ</given-names></name></person-group><article-title>Canonical Microcircuits for Predictive Coding</article-title><source>Neuron</source><year>2012</year><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="pmcid">PMC3777738</pub-id><pub-id pub-id-type="pmid">23177956</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname><given-names>H</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Prediction Errors but Not Sharpened Signals Simulate Multivoxel fMRI Patterns during Speech Perception</article-title><source>PLoS biology</source><year>2016</year><volume>14</volume><elocation-id>e1002577</elocation-id><pub-id pub-id-type="pmcid">PMC5112801</pub-id><pub-id pub-id-type="pmid">27846209</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002577</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Schmitt</surname><given-names>L-M</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name></person-group><article-title>Reconstructing the predictive architecture of the mind and brain</article-title><source>Trends in Cognitive Sciences</source><year>2022</year><volume>26</volume><fpage>1018</fpage><lpage>1019</lpage><pub-id pub-id-type="pmid">36150970</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luthra</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>MYC</given-names></name><name><surname>You</surname><given-names>H</given-names></name><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Magnuson</surname><given-names>JS</given-names></name></person-group><article-title>Does signal reduction imply predictive coding in models of spoken word recognition?</article-title><source>Psychonomic bulletin &amp; review</source><year>2021</year><pub-id pub-id-type="pmcid">PMC8367925</pub-id><pub-id pub-id-type="pmid">33852158</pub-id><pub-id pub-id-type="doi">10.3758/s13423-021-01924-x</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Rapid computations of spectrotemporal prediction error support perception of degraded speech</article-title><source>eLife</source><year>2020</year><volume>9</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC7641582</pub-id><pub-id pub-id-type="pmid">33147138</pub-id><pub-id pub-id-type="doi">10.7554/eLife.58077</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huettig</surname><given-names>F</given-names></name><name><surname>Mani</surname><given-names>N</given-names></name></person-group><article-title>Is prediction necessary to understand language?</article-title><source>Probably not Language, Cognition and Neuroscience</source><year>2016</year><volume>31</volume><fpage>19</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1080/23273798.2015.1072223</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Davis</surname><given-names>M</given-names></name></person-group><chapter-title>Extracting language content from speech sounds: An information theoretic approach</chapter-title><source>Springer Handbook of Auditory Research</source><publisher-name>Springer</publisher-name><year>2022</year><fpage>113</fpage><lpage>139</lpage></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosjean</surname><given-names>F</given-names></name></person-group><article-title>Spoken word recognition processes and the gating paradigm</article-title><source>Perception &amp; Psychophysics</source><year>1980</year><volume>28</volume><fpage>267</fpage><lpage>283</lpage><pub-id pub-id-type="pmid">7465310</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luce</surname><given-names>PA</given-names></name><name><surname>Pisoni</surname><given-names>DB</given-names></name></person-group><article-title>Recognizing spoken words: the neighborhood activation model</article-title><source>Ear and hearing</source><year>1998</year><volume>19</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="pmcid">PMC3467695</pub-id><pub-id pub-id-type="pmid">9504270</pub-id><pub-id pub-id-type="doi">10.1097/00003446-199802000-00001</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Beck</surname><given-names>JM</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>Probabilistic brains: knowns and unknowns</article-title><source>Nature Neuroscience</source><year>2013</year><volume>16</volume><fpage>1170</fpage><lpage>1178</lpage><pub-id pub-id-type="pmcid">PMC4487650</pub-id><pub-id pub-id-type="pmid">23955561</pub-id><pub-id pub-id-type="doi">10.1038/nn.3495</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Bhattasali</surname><given-names>S</given-names></name><name><surname>Cruz Heredia</surname><given-names>AA</given-names></name><name><surname>Resnik</surname><given-names>P</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Lau</surname><given-names>E</given-names></name></person-group><article-title>Parallel processing in speech perception with local and global representations of linguistic context</article-title><source>eLife</source><year>2022</year><volume>11</volume><elocation-id>e72056</elocation-id><pub-id pub-id-type="pmcid">PMC8830882</pub-id><pub-id pub-id-type="pmid">35060904</pub-id><pub-id pub-id-type="doi">10.7554/eLife.72056</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loftus</surname><given-names>GR</given-names></name><name><surname>Masson</surname><given-names>MEJ</given-names></name></person-group><article-title>Using confidence intervals in within-subject designs</article-title><source>Psychonomic Bulletin &amp; Review</source><year>1994</year><volume>1</volume><fpage>476</fpage><lpage>490</lpage><pub-id pub-id-type="pmid">24203555</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Neural dynamics of phoneme sequences reveal position-invariant code for content and order</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>6606</elocation-id><pub-id pub-id-type="pmcid">PMC9633780</pub-id><pub-id pub-id-type="pmid">36329058</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-34326-1</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><year>2016</year><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Bankson</surname><given-names>BB</given-names></name><name><surname>Harel</surname><given-names>A</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>The representational dynamics of task and object processing in humans</article-title><source>eLife</source><year>2018</year><volume>7</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmcid">PMC5811210</pub-id><pub-id pub-id-type="pmid">29384473</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32816</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname><given-names>M</given-names></name><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Ilmoniemi</surname><given-names>RJ</given-names></name><name><surname>Knuutila</surname><given-names>J</given-names></name><name><surname>Lounasmaa</surname><given-names>OV</given-names></name></person-group><article-title>Magnetoencephalography—theory, instrumentation, and applications to noninvasive studies of the working human brain</article-title><source>Reviews of Modern Physics</source><year>1993</year><volume>65</volume><fpage>413</fpage><lpage>497</lpage><pub-id pub-id-type="doi">10.1103/RevModPhys.65.413</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lankinen</surname><given-names>K</given-names></name><name><surname>Ahlfors</surname><given-names>SP</given-names></name><name><surname>Mamashli</surname><given-names>F</given-names></name><name><surname>Blazejewska</surname><given-names>AI</given-names></name><name><surname>Raij</surname><given-names>T</given-names></name><name><surname>Turpin</surname><given-names>T</given-names></name><etal/></person-group><article-title>Cortical depth profiles of auditory and visual 7 T functional MRI responses in human superior temporal areas</article-title><source>Human Brain Mapping</source><year>2023</year><volume>44</volume><fpage>362</fpage><lpage>372</lpage><pub-id pub-id-type="pmcid">PMC9842898</pub-id><pub-id pub-id-type="pmid">35980015</pub-id><pub-id pub-id-type="doi">10.1002/hbm.26046</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Giraud</surname><given-names>A-L</given-names></name></person-group><article-title>Transitions in neural oscillations reflect prediction errors generated in audiovisual speech</article-title><source>Nature neuroscience</source><year>2011</year><volume>14</volume><fpage>797</fpage><lpage>801</lpage><pub-id pub-id-type="pmid">21552273</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Waite</surname><given-names>AS</given-names></name><name><surname>Kopell</surname><given-names>N</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>Layer and rhythm specificity for predictive routing</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2020</year><volume>117</volume><fpage>31459</fpage><lpage>31469</lpage><pub-id pub-id-type="pmcid">PMC7733827</pub-id><pub-id pub-id-type="pmid">33229572</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2014868117</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kok</surname><given-names>P</given-names></name><name><surname>Bains</surname><given-names>LJ</given-names></name><name><surname>van Mourik</surname><given-names>T</given-names></name><name><surname>Norris</surname><given-names>DG</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name></person-group><article-title>Selective Activation of the Deep Layers of the Human Primary Visual Cortex by Top-Down Feedback</article-title><source>Current Biology</source><year>2016</year><volume>26</volume><fpage>371</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">26832438</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Sellers</surname><given-names>KK</given-names></name><name><surname>Chung</surname><given-names>JE</given-names></name><name><surname>Xu</surname><given-names>D</given-names></name><name><surname>Mischler</surname><given-names>G</given-names></name><etal/></person-group><article-title>Large-scale single-neuron speech sound encoding across the depth of human cortex</article-title><source>Nature</source><year>2023</year><date-in-citation>cited 1 Feb 2024</date-in-citation><pub-id pub-id-type="pmcid">PMC10866713</pub-id><pub-id pub-id-type="pmid">38093008</pub-id><pub-id pub-id-type="doi">10.1038/s41586-023-06839-2</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Tyler</surname><given-names>LK</given-names></name><name><surname>Randall</surname><given-names>B</given-names></name><name><surname>Stamatakis</surname><given-names>EA</given-names></name><name><surname>Marslen-Wilson</surname><given-names>WD</given-names></name></person-group><article-title>Optimally efficient neural systems for processing spoken language</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><fpage>908</fpage><lpage>918</lpage><pub-id pub-id-type="pmcid">PMC3948491</pub-id><pub-id pub-id-type="pmid">23250955</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs366</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>AJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Uncertainty, neuromodulation, and attention</article-title><source>Neuron</source><year>2005</year><volume>46</volume><fpage>681</fpage><lpage>92</lpage><pub-id pub-id-type="pmid">15944135</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davis</surname><given-names>MH</given-names></name><name><surname>Gaskell</surname><given-names>MG</given-names></name></person-group><article-title>A complementary systems account of word learning: neural and behavioural evidence</article-title><source>Philosophical transactions of the Royal Society of London Series, B, Biological sciences</source><year>2009</year><volume>364</volume><fpage>3773</fpage><lpage>800</lpage><pub-id pub-id-type="pmcid">PMC2846311</pub-id><pub-id pub-id-type="pmid">19933145</pub-id><pub-id pub-id-type="doi">10.1098/rstb.2009.0111</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanco-Elorrieta</surname><given-names>E</given-names></name><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Pylkkänen</surname><given-names>L</given-names></name></person-group><article-title>Adaptation to mis-pronounced speech: evidence for a prefrontal-cortex repair mechanism</article-title><source>Sci Rep</source><year>2021</year><volume>11</volume><fpage>97</fpage><pub-id pub-id-type="pmcid">PMC7794353</pub-id><pub-id pub-id-type="pmid">33420193</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-79640-0</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blank</surname><given-names>H</given-names></name><name><surname>Spangenberg</surname><given-names>M</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Neural Prediction Errors Distinguish Perception and Misperception of Speech</article-title><source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source><year>2018</year><volume>38</volume><fpage>6076</fpage><lpage>6089</lpage><pub-id pub-id-type="pmcid">PMC6596154</pub-id><pub-id pub-id-type="pmid">29891730</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3258-17.2018</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>A</given-names></name></person-group><article-title>Whatever next? Predictive brains, situated agents, and the future of cognitive science</article-title><source>Behavioral and Brain Sciences</source><year>2013</year><volume>36</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">23663408</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barascud</surname><given-names>N</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><article-title>Brain responses in humans reveal ideal observer-like sensitivity to complex acoustic patterns</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2016</year><volume>113</volume><fpage>E616</fpage><lpage>25</lpage><pub-id pub-id-type="pmcid">PMC4747708</pub-id><pub-id pub-id-type="pmid">26787854</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1508523113</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>Does predictive coding have a future?</article-title><source>Nature Neuroscience</source><year>2018</year><volume>21</volume><fpage>1019</fpage><lpage>1021</lpage><pub-id pub-id-type="pmid">30038278</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuperberg</surname><given-names>GR</given-names></name><name><surname>Jaeger</surname><given-names>TF</given-names></name></person-group><article-title>What do we mean by prediction in language comprehension?</article-title><source>Language Cognition &amp; Neuroscience</source><year>2016</year><volume>31</volume><fpage>32</fpage><lpage>59</lpage><pub-id pub-id-type="pmcid">PMC4850025</pub-id><pub-id pub-id-type="pmid">27135040</pub-id><pub-id pub-id-type="doi">10.1080/23273798.2015.1102299</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Broderick</surname><given-names>MP</given-names></name><name><surname>Anderson</surname><given-names>AJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Semantic Context Enhances the Early Auditory Encoding of Natural Speech</article-title><source>The Journal of Neuroscience</source><year>2019</year><volume>39</volume><fpage>7564</fpage><lpage>7575</lpage><pub-id pub-id-type="pmcid">PMC6750931</pub-id><pub-id pub-id-type="pmid">31371424</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0584-19.2019</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>YC</given-names></name><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Gilbert</surname><given-names>RA</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Predictive Neural Computations Support Spoken Word Recognition: Evidence from MEG and Competitor Priming</article-title><source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source><year>2021</year><volume>41</volume><fpage>6919</fpage><lpage>6932</lpage><pub-id pub-id-type="pmcid">PMC8360690</pub-id><pub-id pub-id-type="pmid">34210777</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1685-20.2021</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>A</given-names></name><name><surname>Cooper</surname><given-names>E</given-names></name><name><surname>Kaula</surname><given-names>A</given-names></name><name><surname>Anderson</surname><given-names>MC</given-names></name><name><surname>Henson</surname><given-names>R</given-names></name></person-group><article-title>Does prediction error drive one-shot declarative learning?</article-title><source>Journal of Memory and Language</source><year>2017</year><volume>94</volume><fpage>149</fpage><lpage>165</lpage><pub-id pub-id-type="pmcid">PMC5381756</pub-id><pub-id pub-id-type="pmid">28579691</pub-id><pub-id pub-id-type="doi">10.1016/j.jml.2016.11.001</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitt</surname><given-names>L-M</given-names></name><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Tune</surname><given-names>S</given-names></name><name><surname>Rysop</surname><given-names>AU</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Predicting speech from a cortical hierarchy of event-based time scales</article-title><source>Science Advances</source><year>2021</year><volume>7</volume><elocation-id>2020.12.19.423616</elocation-id><pub-id pub-id-type="pmcid">PMC8641937</pub-id><pub-id pub-id-type="pmid">34860554</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abi6070</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caucheteux</surname><given-names>C</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Evidence of a predictive coding hierarchy in the human brain listening to speech</article-title><source>Nat Hum Behav</source><year>2023</year><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC10038805</pub-id><pub-id pub-id-type="pmid">36864133</pub-id><pub-id pub-id-type="doi">10.1038/s41562-022-01516-2</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name><name><surname>Tuckute</surname><given-names>G</given-names></name><name><surname>Kauf</surname><given-names>C</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><etal/></person-group><article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title><source>Proc Natl Acad Sci USA</source><year>2021</year><volume>118</volume><elocation-id>e2105646118</elocation-id><pub-id pub-id-type="pmcid">PMC8694052</pub-id><pub-id pub-id-type="pmid">34737231</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>SL</given-names></name><name><surname>Willems</surname><given-names>RM</given-names></name></person-group><article-title>Word predictability and semantic similarity show distinct patterns of brain activity during language comprehension</article-title><source>Language, Cognition and Neuroscience</source><year>2017</year><volume>32</volume><fpage>1192</fpage><lpage>1203</lpage><pub-id pub-id-type="doi">10.1080/23273798.2017.1323109</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Zada</surname><given-names>Z</given-names></name><name><surname>Buchnik</surname><given-names>E</given-names></name><name><surname>Schain</surname><given-names>M</given-names></name><name><surname>Price</surname><given-names>A</given-names></name><name><surname>Aubrey</surname><given-names>B</given-names></name><etal/></person-group><article-title>Shared computational principles for language processing in humans and deep language models</article-title><source>Nat Neurosci</source><year>2022</year><volume>25</volume><fpage>369</fpage><lpage>380</lpage><pub-id pub-id-type="pmcid">PMC8904253</pub-id><pub-id pub-id-type="pmid">35260860</pub-id><pub-id pub-id-type="doi">10.1038/s41593-022-01026-4</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacGregor</surname><given-names>LJ</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name><name><surname>van Casteren</surname><given-names>M</given-names></name><name><surname>Shtyrov</surname><given-names>Y</given-names></name></person-group><article-title>Ultra-rapid access to words in the brain</article-title><source>Nature communications</source><year>2012</year><volume>3</volume><fpage>711</fpage><pub-id pub-id-type="pmcid">PMC3543931</pub-id><pub-id pub-id-type="pmid">22426232</pub-id><pub-id pub-id-type="doi">10.1038/ncomms1715</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabas</surname><given-names>A</given-names></name><name><surname>Mihai</surname><given-names>G</given-names></name><name><surname>Kiebel</surname><given-names>S</given-names></name><name><surname>Trampel</surname><given-names>R</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name></person-group><article-title>Abstract rules drive adaptation in the subcortical sensory pathway</article-title><person-group person-group-type="editor"><name><surname>Shinn-Cunningham</surname><given-names>BG</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Malmierca</surname><given-names>MS</given-names></name></person-group><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e64501</elocation-id><pub-id pub-id-type="pmcid">PMC7785290</pub-id><pub-id pub-id-type="pmid">33289479</pub-id><pub-id pub-id-type="doi">10.7554/eLife.64501</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Gander</surname><given-names>PE</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Kovach</surname><given-names>CK</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Kawasaki</surname><given-names>H</given-names></name><etal/></person-group><article-title>Neural signatures of perceptual inference</article-title><source>eLife</source><year>2016</year><volume>5</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC4841773</pub-id><pub-id pub-id-type="pmid">26949254</pub-id><pub-id pub-id-type="doi">10.7554/eLife.11476</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerner</surname><given-names>Y</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><article-title>Topographic mapping of a hierarchy of temporal receptive windows using a narrated story</article-title><source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source><year>2011</year><volume>31</volume><fpage>2906</fpage><lpage>15</lpage><pub-id pub-id-type="pmcid">PMC3089381</pub-id><pub-id pub-id-type="pmid">21414912</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3684-10.2011</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Carlson</surname><given-names>CE</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><etal/></person-group><article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title><source>Neuron</source><year>2012</year><volume>76</volume><fpage>423</fpage><lpage>34</lpage><pub-id pub-id-type="pmcid">PMC3517908</pub-id><pub-id pub-id-type="pmid">23083743</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.011</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>Long</surname><given-names>LK</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Irobunda</surname><given-names>I</given-names></name><name><surname>Merricks</surname><given-names>EM</given-names></name><etal/></person-group><article-title>Multiscale temporal integration organizes hierarchical computation in human auditory cortex</article-title><source>Nature Human Behaviour</source><year>2022</year><pub-id pub-id-type="pmcid">PMC8957490</pub-id><pub-id pub-id-type="pmid">35145280</pub-id><pub-id pub-id-type="doi">10.1038/s41562-021-01261-y</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Crosse</surname><given-names>MJ</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Cortical Measures of Phoneme-Level Speech Encoding Correlate with the Perceived Clarity of Natural Speech</article-title><source>eNeuro</source><year>2018</year><volume>5</volume><comment>ENEURO.0084-18.2018</comment><pub-id pub-id-type="pmcid">PMC5900464</pub-id><pub-id pub-id-type="pmid">29662947</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0084-18.2018</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>A-L</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><year>2012</year><date-in-citation>cited 19 Mar 2012</date-in-citation><pub-id pub-id-type="pmcid">PMC4461038</pub-id><pub-id pub-id-type="pmid">22426255</pub-id><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Canolty</surname><given-names>RT</given-names></name><name><surname>Edwards</surname><given-names>E</given-names></name><name><surname>Dalal</surname><given-names>SS</given-names></name><name><surname>Soltani</surname><given-names>M</given-names></name><name><surname>Nagarajan</surname><given-names>SS</given-names></name><name><surname>Berger</surname><given-names>MS</given-names></name><etal/></person-group><article-title>High gamma power is phase-locked to theta oscillations in human neocortex</article-title><source>Science (New York, NY)</source><year>2006</year><volume>313</volume><fpage>1626</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC2628289</pub-id><pub-id pub-id-type="pmid">16973878</pub-id><pub-id pub-id-type="doi">10.1126/science.1128115</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sedley</surname><given-names>W</given-names></name><name><surname>Teki</surname><given-names>S</given-names></name><name><surname>Kumar</surname><given-names>S</given-names></name><name><surname>Overath</surname><given-names>T</given-names></name><name><surname>Barnes</surname><given-names>GR</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name></person-group><article-title>Gamma band pitch responses in human auditory cortex measured with magnetoencephalography</article-title><source>Neuroimage</source><year>2012</year><volume>59</volume><fpage>1904</fpage><lpage>1911</lpage><pub-id pub-id-type="pmcid">PMC3236996</pub-id><pub-id pub-id-type="pmid">21925281</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2011.08.098</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Norris</surname><given-names>D</given-names></name></person-group><chapter-title>How do Computational Models Help us Develop Better Theories?</chapter-title><source>Twenty-first century psycholinguistics: Four cornerstones</source><publisher-name>Lawrence Erlbaum Associates Publishers</publisher-name><publisher-loc>Mahwah NJ, US</publisher-loc><year>2005</year><fpage>331</fpage><lpage>346</lpage></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nour Eddine</surname><given-names>S</given-names></name><name><surname>Brothers</surname><given-names>T</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Spratling</surname><given-names>M</given-names></name><name><surname>Kuperberg</surname><given-names>GR</given-names></name></person-group><article-title>A predictive coding model of the N400</article-title><source>Cognition</source><year>2024</year><volume>246</volume><elocation-id>105755</elocation-id><pub-id pub-id-type="pmcid">PMC10984641</pub-id><pub-id pub-id-type="pmid">38428168</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2024.105755</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hovsepyan</surname><given-names>S</given-names></name><name><surname>Olasagasti</surname><given-names>I</given-names></name><name><surname>Giraud</surname><given-names>A</given-names></name></person-group><article-title>Combining predictive coding and neural oscillations enables online syllable recognition in natural speech</article-title><source>Nature Communications</source><year>2020</year><volume>11</volume><elocation-id>3117</elocation-id><pub-id pub-id-type="pmcid">PMC7305192</pub-id><pub-id pub-id-type="pmid">32561726</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-16956-5</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yildiz</surname><given-names>IB</given-names></name><name><surname>von Kriegstein</surname><given-names>K</given-names></name><name><surname>Kiebel</surname><given-names>SJ</given-names></name></person-group><article-title>From Birdsong to Human Speech Recognition: Bayesian Inference on a Hierarchy of Nonlinear Dynamical Systems</article-title><source>PLoS Computational Biology</source><year>2013</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC3772045</pub-id><pub-id pub-id-type="pmid">24068902</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003219</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnuson</surname><given-names>JS</given-names></name><name><surname>You</surname><given-names>H</given-names></name><name><surname>Luthra</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Nam</surname><given-names>H</given-names></name><name><surname>Escabí</surname><given-names>M</given-names></name><etal/></person-group><article-title>EARSHOT: A Minimal Neural Network Model of Incremental Human Speech Recognition</article-title><source>Cognitive science</source><year>2020</year><volume>44</volume><elocation-id>e12823</elocation-id><pub-id pub-id-type="pmid">32274861</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-haignere</surname><given-names>SV</given-names></name><name><surname>Mcdermott</surname><given-names>JH</given-names></name><name><surname>Kell</surname><given-names>AJE</given-names></name><etal/></person-group><article-title>A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy</article-title><source>Neuron</source><year>2018</year><volume>98</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Baayen</surname><given-names>RH</given-names></name><name><surname>Piepenbrock</surname><given-names>R</given-names></name><name><surname>Van Rijn</surname><given-names>H</given-names></name></person-group><source>The CELEX lexical database (CD-ROM)</source><publisher-name>Philadelphia Linguistics Data Consortium University of Pennsylvania</publisher-name><year>1993</year></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawahara</surname><given-names>H</given-names></name></person-group><article-title>STRAIGHT, exploitation of the other aspect of VOCODER: Perceptually isomorphic decomposition of speech sounds</article-title><source>Acoust Sci &amp; Tech</source><year>2006</year><volume>27</volume><fpage>349</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1250/ast.27.349</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>JC</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Inferior Frontal Cortex Contributions to the Recognition of Spoken Words and Their Constituent Speech Sounds</article-title><source>Journal of cognitive neuroscience</source><year>2017</year><volume>25</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="pmcid">PMC6635126</pub-id><pub-id pub-id-type="pmid">28129061</pub-id><pub-id pub-id-type="doi">10.1162/jocn_a_01096</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Predictive Top-Down Integration of Prior Knowledge during Speech Perception</article-title><source>The Journal of neuroscience : the official journal of the Society for Neuroscience</source><year>2012</year><volume>32</volume><fpage>8443</fpage><lpage>8453</lpage><pub-id pub-id-type="pmcid">PMC6620994</pub-id><pub-id pub-id-type="pmid">22723684</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5069-11.2012</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Carlyon</surname><given-names>RP</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Top-down influences of written text on perceived clarity of degraded speech</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2014</year><volume>40</volume><fpage>186</fpage><lpage>199</lpage><pub-id pub-id-type="pmcid">PMC3906796</pub-id><pub-id pub-id-type="pmid">23750966</pub-id><pub-id pub-id-type="doi">10.1037/a0033206</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sohoglu</surname><given-names>E</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Perceptual learning of degraded speech by minimizing prediction error</article-title><source>Proceedings of the National Academy of Sciences</source><year>2016</year><volume>113</volume><fpage>E1747</fpage><lpage>E1756</lpage><pub-id pub-id-type="pmcid">PMC4812728</pub-id><pub-id pub-id-type="pmid">26957596</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1523266113</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><year>2005</year><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kisler</surname><given-names>T</given-names></name><name><surname>Reichel</surname><given-names>U</given-names></name><name><surname>Schiel</surname><given-names>F</given-names></name></person-group><article-title>Multilingual processing of speech via web services</article-title><source>Computer Speech &amp; Language</source><year>2017</year><volume>45</volume><fpage>326</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/j.csl.2017.01.005</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warner</surname><given-names>N</given-names></name><name><surname>McQueen</surname><given-names>JM</given-names></name><name><surname>Cutler</surname><given-names>A</given-names></name></person-group><article-title>Tracking perception of the sounds of English</article-title><source>The Journal of the Acoustical Society of America</source><year>2014</year><volume>135</volume><fpage>2995</fpage><lpage>3006</lpage><pub-id pub-id-type="pmid">24815279</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hämäläinen</surname><given-names>MS</given-names></name></person-group><article-title>Functional localization based on measurements with a whole-head magnetometer system</article-title><source>Brain topography</source><year>1995</year><volume>7</volume><fpage>283</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">7577326</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taulu</surname><given-names>S</given-names></name><name><surname>Simola</surname><given-names>J</given-names></name><name><surname>Kajola</surname><given-names>M</given-names></name></person-group><article-title>Applications of the signal space separation method</article-title><source>IEEE Transactions on Signal Processing</source><year>2005</year><volume>53</volume><fpage>3359</fpage><lpage>3372</lpage><pub-id pub-id-type="doi">10.1109/TSP.2005.853302</pub-id></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hebart</surname><given-names>MN</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><article-title>The Decoding Toolbox (TDT): a versatile software package for multivariate analyses of functional imaging data</article-title><source>Frontiers in Neuroinformatics</source><year>2015</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC4285115</pub-id><pub-id pub-id-type="pmid">25610393</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00088</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><article-title>Predictive Processing: A Canonical Cortical Computation</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>424</fpage><lpage>435</lpage><pub-id pub-id-type="pmcid">PMC6400266</pub-id><pub-id pub-id-type="pmid">30359606</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG- and MEG-data</article-title><source>Journal of neuroscience methods</source><year>2007</year><volume>164</volume><fpage>177</fpage><lpage>90</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Information-based functional brain mapping</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2006</year><volume>103</volume><fpage>3863</fpage><lpage>3868</lpage><pub-id pub-id-type="pmcid">PMC1383651</pub-id><pub-id pub-id-type="pmid">16537458</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0600244103</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>(A) Our stimuli consisted of multiple sets of bi-syllabic spoken words (“bingo”, “bongo”, etc. transcribed phonetically as /b I N – g @U/, /b Q N – g @U/) and pseudowords (“bingger”, “bongger”, etc. transcribed /b I N – g @ r*/, /b Q N – g @ r*/). Purple numbers indicate the strength of predictions for the second syllable of each item, operationalized as p(Syl2|Syl1): the probability of hearing the second syllable, conditioned on the first syllable (the negative log of this probability is equivalent to syllable surprisal; see [<xref ref-type="bibr" rid="R23">23</xref>]). Brown numbers indicate the entropy of predictions over all possible syllables (equivalent to the weighted average of syllable surprisal over syllables [<xref ref-type="bibr" rid="R23">23</xref>]). Thickness of arrows depicts the overall higher p(Syl2|Syl1) and lower entropy for Strong vs Weak Prediction items. By cross-splicing the syllables in our word items, we created a set of pseudowords in which predictions mismatched with Syl2. The earliest speech segment when the words diverged from the pseudowords (the divergence point) is indicated in bold and always occurred at the second segment of Syl2. (B) Model representations for the example word “bingo”, visualized using word and segment clouds with larger text indicating higher probabilities (for a more complete illustration of these computations, see <xref ref-type="supplementary-material" rid="SD1">Figure S6</xref>). Both sharpened signal and prediction error computations are derived from conditional (posterior) word probabilities, as estimated using Bayes theorem by combining prior word probabilities (frequency of occurrence) and the sensory evidence. Here the sensory evidence is derived from the acoustic similarity between individual speech segments and expresses the degree to which the sensory input matches internal representations of spoken words. Word level representations show posterior probabilities for each word as each segment in the input word is heard prior to the divergence point (only the ten words with the highest probabilities at any one timepoint are shown for illustration purposes). Word posteriors provide predictions for which segment will be heard next (segment predictions shown as green colored phonetic transcriptions). Segment predictions are then combined with the sensory evidence for individual segments (‘Input’; shown as brown colored phonetic transcriptions) through sharpened signal (multiplication) or prediction error (subtraction) computations with outcomes depicted on the right as purple and orange segment clouds, respectively (shown only for the divergence point). Note that the segment probabilities in these simulations are not directly related to the stimulus probabilities in panel A as the simulations are based on segment representations and assume some uncertainty in the sensory input (using acoustic similarity). In contrast the stimulus probabilities in <xref ref-type="fig" rid="F1">Figure 1A</xref>, which were used to select the stimuli, are based on syllable representations and assume ideal observer behavior (i.e. no sensory uncertainty). (C) Example model representations for all four experimental conditions at the divergence point for Strong + Match (“bingo”), Weak + Match (“tango”), Strong + Mismatch (“snigo”), Weak + Mismatch (“meago”) items. Due to differential predictions (top-line, green), identical speech input (/@U/, second line, brown), elicits different neural representations for sharpened signal (third line, purple) and prediction error (fourth line, orange) representations. (D) Results from sharpened signals and prediction error simulations at the divergence point, used to predict the magnitude of neural responses (univariate signal magnitude analysis). Sharpened signals were summarised as the sum of log activity over segment units while prediction errors were summarised as the sum of absolute activity. Dots indicate the model signals for each individual item (32 items in each condition) with bars and error bars indicating the mean and standard error across the full item set). (E) Results from simulations used to predict neural pattern distances, shown in the same way as panel D except the individual datapoints and bars depict the dissimilarity between model representations for all pairs of items (496 item pairs, i.e. 32x31/2) within each condition (multivariate pattern analysis).</p></caption><graphic xlink:href="EMS189088-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>MEG univariate signal magnitude results.</title><p>(A) Traces indicate the RMS of the MEG signal across “speech-responsive” sensors in the left hemisphere, timelocked to the onset of the second syllable (Syl2). Sensor selections are indicated in the topography as the percentage of participants for which a sensor was selected. Thick horizontal bars at the bottom of the graph indicate timepoints when statistical contrasts were significant at a clusterwise threshold of p &lt; .05 FWE corrected (thin horizontal bars show significance at an uncorrected p &lt; .05 threshold). Grey shaded area indicates timepoints when the effect of Strong vs Weak in the Match condition was significant at p &lt; .05 (FWE cluster corrected). M = Match. MM = Mismatch. (B) Group-averaged correlations in left hemisphere sensors between p(Syl2|Syl1) and the RMS of the MEG signal. Gray shaded area indicates timepoints when the correlations within Match or Mismatch conditions were significant at p &lt; .05 (FWE cluster corrected). Error bars (transparent area around traces) indicate within-subject errors [<xref ref-type="bibr" rid="R28">28</xref>]. (C) Bar graph indicates group means and within-subject standard errors [<xref ref-type="bibr" rid="R28">28</xref>] for the four experimental conditions after averaging MEG power over the temporal and frequency extent of the interaction cluster reported in panel D. (D) Image depicts changes in MEG power (between condition differences) across time and frequency in left hemisphere sensors for the interaction (left panel) and simple effect of Weak &gt; Strong in Match trials (right panel). Nontransparent colors indicate significance at p &lt; .05 (FWE cluster corrected) while transparent colors show p &lt; .05 (uncorrected). Interaction effect was computed as (Strong-Weak [Mismatch)]) – (Strong-Weak[Match]).</p></caption><graphic xlink:href="EMS189088-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>fMRI univariate signal magnitude results.</title><p>(A) Repeated measures ANOVA showed main effects of prediction strength (left panel) and congruency (right panel). Statistical maps (p &lt; .05 FWE cluster corrected) are shown overlaid onto a template brain in MNI space. White outlines indicate the search volume which included superior and middle temporal regions bilaterally. (B) Statistical maps (p &lt; .05 FWE cluster corrected) for correlations between p(Syl2|Syl1) and BOLD magnitude. In HG extending into middle STG (left panel), correlations are significantly positive and do not differ between Match and Mismatch items (equivalent to a main effect of prediction strength). In anterior STG (right panel), correlations are significantly larger for Mismatch than for Match items. Bar graphs show group-averaged correlations (Fisher-transformed r-values) between conditional probability and BOLD magnitude averaged across voxels in the clusters indicated (arrows). Error bars show within-subject standard errors suitable for statistical comparison between conditions.</p></caption><graphic xlink:href="EMS189088-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>MEG multivariate pattern analysis results.</title><p>(A) Phonetic dissimilarity matrix expressing the normalized Levenshtein distance between phonetic transcriptions of Syl2 for different item pairs. Right panel inset highlights predicted dissimilarity used in representational analyses. (B) Group-averaged correlation between MEG pattern distances and phonetic dissimilarities for Syl2 (in red) and Syl1 (in beige). Horizontal bars indicate timepoints when correlations were significantly different from zero (thick lines show corrected significance at p &lt; .05 clusterwise; thin lines show uncorrected significance at p &lt; .05). Error bars (transparent area around traces) indicate standard error of the mean. (C) Group-averaged MEG pattern distances with larger values indicating more dissimilar MEG response patterns between items. Thick horizontal bars at the bottom of the graph indicate timepoints when statistical contrasts were significant at a clusterwise threshold of p &lt; .05 FWE corrected (thin horizontal bars show significance at an uncorrected p &lt; .05 threshold). Grey shaded area indicates timepoints when there was a significant difference between Strong and Weak items in the Mismatch condition (p &lt; .05 FWE clusterwise corrected). The inset bar plot shows group means for each condition (and within-subject standard errors as before) after averaging over the temporal extent of the cluster for the interaction contrast. (D) Group-averaged correlations between p(Syl2|Syl1) and MEG pattern distances. Grey shaded area indicates timepoints when there was a significant positive correlation for Mismatch items (p &lt; .05 FWE clusterwise corrected). Error bars (transparent area around traces) indicate within-subject errors [<xref ref-type="bibr" rid="R28">28</xref>]. (E) Group-averaged MEG pattern distances after transforming the MEG signal into time-frequency power. Image depicts the interaction contrast: (Strong-Weak [Mismatch)]) – (Strong-Weak[Match]). Nontransparent colors indicate significance at p &lt; .05 (FWE cluster corrected) while transparent colors indicate p &lt; .05 (uncorrected). Bar graph indicates group means (and within-subject standard errors) after averaging over the temporal and frequency extent of the interaction cluster (black broken outline on the time-frequency image).</p></caption><graphic xlink:href="EMS189088-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>fMRI multivariate pattern analysis results.</title><p>(A) Statistical maps showing voxels in which there was a significant correlation between fMRI pattern distances and phonetic dissimilarities for Syl2 (p &lt; .05 FWE corrected clusterwise). White outline shows search volumes as in <xref ref-type="fig" rid="F3">Figure 3</xref>. (B) Group-averaged correlations (and within-subject standard errors) after averaging over voxels in the left and right hemisphere clusters shown in (A).</p></caption><graphic xlink:href="EMS189088-f005"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>MNI coordinates for fMRI analyses. Also shown are the spatial extent (number of voxels) and peak statistic for each cluster.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top" align="left" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">Location</th><th valign="top" align="right" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">Extent</th><th valign="top" align="right" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">Z-stat</th><th valign="top" align="left" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">x</th><th valign="top" align="left" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">y</th><th valign="top" align="left" style="border-top: 1px solid #000000;border-bottom: 2.25px solid #000000">z</th></tr></thead><tbody><tr><td valign="top" align="left" colspan="6" style="border: none"><bold>Univariate Strong &gt; Weak</bold></td></tr><tr><td valign="top" align="left">Right Heschl’s Gyrus</td><td valign="top" align="right">551</td><td valign="top" align="right">5.74</td><td valign="top" align="right">54</td><td valign="top" align="right">-16</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">    Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">4.84</td><td valign="top" align="right">64</td><td valign="top" align="right">-14</td><td valign="top" align="right">0</td></tr><tr><td valign="top" align="left">    Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">4.05</td><td valign="top" align="right">66</td><td valign="top" align="right">-26</td><td valign="top" align="right">4</td></tr><tr><td valign="top" align="left">Left mid-STG</td><td valign="top" align="right">551</td><td valign="top" align="right">5.41</td><td valign="top" align="right">-58</td><td valign="top" align="right">-22</td><td valign="top" align="right">4</td></tr><tr><td valign="top" align="left">    Left Heschl’s Gyrus</td><td valign="top" align="right"/><td valign="top" align="right">4.55</td><td valign="top" align="right">-44</td><td valign="top" align="right">-24</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">    Left mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">4.55</td><td valign="top" align="right">-46</td><td valign="top" align="right">-32</td><td valign="top" align="right">4</td></tr><tr><td valign="top" align="left" colspan="6"><bold>Univariate Mismatch &gt; Match</bold></td></tr><tr><td valign="top" align="left">Left mid-STG</td><td valign="top" align="right">569</td><td valign="top" align="right">6.00</td><td valign="top" align="right">-64</td><td valign="top" align="right">-24</td><td valign="top" align="right">6</td></tr><tr><td valign="top" align="left">    Left mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">5.81</td><td valign="top" align="right">-60</td><td valign="top" align="right">-16</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">    Left anterior-STG</td><td valign="top" align="right"/><td valign="top" align="right">5.62</td><td valign="top" align="right">-58</td><td valign="top" align="right">-4</td><td valign="top" align="right">-4</td></tr><tr><td valign="top" align="left">Right anterior-STG</td><td valign="top" align="right">205</td><td valign="top" align="right">5.25</td><td valign="top" align="right">58</td><td valign="top" align="right">-4</td><td valign="top" align="right">-4</td></tr><tr><td valign="top" align="left">    Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">4.51</td><td valign="top" align="right">64</td><td valign="top" align="right">-16</td><td valign="top" align="right">0</td></tr><tr><td valign="top" align="left">    Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.54</td><td valign="top" align="right">56</td><td valign="top" align="right">-14</td><td valign="top" align="right">-2</td></tr><tr><td valign="top" align="left" colspan="6"><bold>Univariate Strong+Noise &gt; Weak+Noise</bold></td></tr><tr><td valign="top" align="left">Left anterior-STG</td><td valign="top" align="right">245</td><td valign="top" align="right">4.42</td><td valign="top" align="right">-54</td><td valign="top" align="right">-12</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">    Left Heschl’s Gyrus</td><td valign="top" align="right"/><td valign="top" align="right">4.21</td><td valign="top" align="right">-44</td><td valign="top" align="right">-22</td><td valign="top" align="right">-2</td></tr><tr><td valign="top" align="left">    Left mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.31</td><td valign="top" align="right">-60</td><td valign="top" align="right">-24</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">Right posterior-STG</td><td valign="top" align="right">56</td><td valign="top" align="right">4.25</td><td valign="top" align="right">64</td><td valign="top" align="right">-44</td><td valign="top" align="right">10</td></tr><tr><td valign="top" align="left">    Right posterior-MTG</td><td valign="top" align="right"/><td valign="top" align="right">3.58</td><td valign="top" align="right">60</td><td valign="top" align="right">-54</td><td valign="top" align="right">6</td></tr><tr><td valign="top" align="left">Right mid-STG</td><td valign="top" align="right">115</td><td valign="top" align="right">4.00</td><td valign="top" align="right">54</td><td valign="top" align="right">-20</td><td valign="top" align="right">0</td></tr><tr><td valign="top" align="left">    Right Heschl’s Gyrus</td><td valign="top" align="right"/><td valign="top" align="right">3.30</td><td valign="top" align="right">48</td><td valign="top" align="right">-14</td><td valign="top" align="right">0</td></tr><tr><td valign="top" align="left">    Right anterior-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.10</td><td valign="top" align="right">58</td><td valign="top" align="right">-12</td><td valign="top" align="right">-4</td></tr><tr><td valign="top" align="left" colspan="6"><bold>Univariate correlations with p(Syl2|Syl1) Mismatch+Match</bold></td></tr><tr><td valign="top" align="left">Left Heschl’s Gyrus</td><td valign="top" align="right">467</td><td valign="top" align="right">5.01</td><td valign="top" align="right">-52</td><td valign="top" align="right">-22</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left">     Left Heschl’s Gyrus</td><td valign="top" align="right"/><td valign="top" align="right">4.85</td><td valign="top" align="right">-42</td><td valign="top" align="right">-28</td><td valign="top" align="right">4</td></tr><tr><td valign="top" align="left">     Left mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">4.62</td><td valign="top" align="right">-62</td><td valign="top" align="right">-18</td><td valign="top" align="right">6</td></tr><tr><td valign="top" align="left">Right mid-STG</td><td valign="top" align="right">105</td><td valign="top" align="right">4.04</td><td valign="top" align="right">66</td><td valign="top" align="right">-24</td><td valign="top" align="right">6</td></tr><tr><td valign="top" align="left">     Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.96</td><td valign="top" align="right">64</td><td valign="top" align="right">-32</td><td valign="top" align="right">6</td></tr><tr><td valign="top" align="left">     Right mid-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.56</td><td valign="top" align="right">54</td><td valign="top" align="right">-26</td><td valign="top" align="right">0</td></tr><tr><td valign="top" align="left" colspan="6"><bold>Univariate correlations with p(Syl2|Syl1) Mismatch&gt;Match</bold></td></tr><tr><td valign="top" align="left">Right anterior-STG</td><td valign="top" align="right">71</td><td valign="top" align="right">4.58</td><td valign="top" align="right">58</td><td valign="top" align="right">-2</td><td valign="top" align="right">-4</td></tr><tr><td valign="top" align="left">    Right anterior-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.81</td><td valign="top" align="right">50</td><td valign="top" align="right">-4</td><td valign="top" align="right">-2</td></tr><tr><td valign="top" align="left">    Right anterior-STG</td><td valign="top" align="right"/><td valign="top" align="right">3.59</td><td valign="top" align="right">58</td><td valign="top" align="right">-10</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left" colspan="6"><bold>Multivariate Syl2 phonetic distance</bold></td></tr><tr><td valign="top" align="left">Right Heschl’s Gyrus</td><td valign="top" align="right">148</td><td valign="top" align="right">4.49</td><td valign="top" align="right">56</td><td valign="top" align="right">-6</td><td valign="top" align="right">2</td></tr><tr><td valign="top" align="left" style="border-bottom: 1px solid #ffffff">Left Heschl’s Gyrus</td><td valign="top" align="right" style="border-bottom: 1px solid #ffffff">146</td><td valign="top" align="right" style="border-bottom: 1px solid #ffffff">4.01</td><td valign="top" align="right" style="border-bottom: 1px solid #ffffff">-44</td><td valign="top" align="right" style="border-bottom: 1px solid #ffffff">-20</td><td valign="top" align="right" style="border-bottom: 1px solid #ffffff">0</td></tr></tbody></table></table-wrap></floats-group></article>