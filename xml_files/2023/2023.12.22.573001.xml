<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS193121</article-id><article-id pub-id-type="doi">10.1101/2023.12.22.573001</article-id><article-id pub-id-type="archive">PPR778628</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Dissecting the Complexities of Learning With Infinite Hidden Markov Models</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bruijns</surname><given-names>Sebastian A.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><collab>International Brain Laboratory</collab></contrib><contrib contrib-type="author"><name><surname>Bougrova</surname><given-names>Kcénia</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Laranjeira</surname><given-names>Inês C.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Lau</surname><given-names>Petrina Y. P.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Meijer</surname><given-names>Guido T.</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Miska</surname><given-names>Nathaniel J.</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Noel</surname><given-names>Jean-Paul</given-names></name><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Pan-Vazquez</surname><given-names>Alejandro</given-names></name><xref ref-type="aff" rid="A9">9</xref></contrib><contrib contrib-type="author"><name><surname>Roth</surname><given-names>Noam</given-names></name><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Socha</surname><given-names>Karolina Z.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Urai</surname><given-names>Anne E.</given-names></name><xref ref-type="aff" rid="A8">8</xref></contrib><contrib contrib-type="author"><name><surname>Dayan</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Max Planck Institute for Biological Cybernetics, Tübingen, Germany</aff><aff id="A2"><label>2</label>University of Tübingen, Germany</aff><aff id="A3"><label>3</label>Champalimaud Foundation, Portugal</aff><aff id="A4"><label>4</label>University College London, United Kingdom</aff><aff id="A5"><label>5</label>Sainsbury Wellcome Centre, University College London, United Kingdom</aff><aff id="A6"><label>6</label>University of Washington, USA</aff><aff id="A7"><label>7</label>New York University, USA</aff><aff id="A8"><label>8</label>Leiden University, The Netherlands</aff><aff id="A9"><label>9</label>Princeton University, USA</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author <email>sebastian.bruijns@tuebingen.mpg.de</email>;</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>31</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>23</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Learning to exploit the contingencies of a complex experiment is not an easy task for animals. Individuals learn in an idiosyncratic manner, revising their approaches multiple times as they are shaped, or shape themselves, and potentially end up with different strategies. Their long-run learning curves are therefore a tantalizing target for the sort of individualized quantitative characterizations that sophisticated modelling can provide. However, any such model requires a flexible and extensible structure which can capture radically new behaviours as well as slow changes in existing ones. To this end, we suggest a dynamic input-output infinite hidden semi-Markov model, whose latent states are associated with specific components of behaviour. This model includes an infinite number of potential states and so has the capacity to describe substantially new behaviours by unearthing extra states; while dynamics in the model allow it to capture more modest adaptations to existing behaviours. We individually fit the model to data collected from more than 100 mice as they learned a contrast detection task over tens of sessions and around fifteen thousand trials each. Despite large individual differences, we found that most animals progressed through three major stages of learning, the transitions between which were marked by distinct additions to task understanding. We furthermore showed that marked changes in behaviour are much more likely to occur at the very beginning of sessions, i.e. after a period of rest, and that response biases in earlier stages are not predictive of biases later on in this task.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Engaging with a new environment or experiment raises a multitude of questions: which sensory signals are pertinent to the task at hand, and which are just noise? What actions are relevant to performance? How should observations inform actions? How can reward be maximized? Particularly if the experimenter changes the task suddenly in order to shape behaviour, but also in stable environments, animals solve these problems through a mixture of apparent leaps in performance and slow accumulation of improvements (<xref ref-type="bibr" rid="R6">Breland &amp; Breland, 1951</xref>; <xref ref-type="bibr" rid="R11">Epstein, Kirshnit, Lanza, &amp; Rubin, 1984</xref>; <xref ref-type="bibr" rid="R14">Gallistel, Fairhurst, &amp; Balsam, 2004</xref>; <xref ref-type="bibr" rid="R24">Köhler, 1948</xref>; <xref ref-type="bibr" rid="R26">Krueger &amp; Dayan, 2009</xref>; <xref ref-type="bibr" rid="R29">Luft &amp; Buitrago, 2005</xref>; <xref ref-type="bibr" rid="R31">Maier, 1931</xref>; <xref ref-type="bibr" rid="R33">Moore &amp; Kuchibhotla, 2022</xref>; <xref ref-type="bibr" rid="R39">Rescorla, 1972</xref>). This process of learning is marked by substantial variability across individuals, who progress at different speeds and over distinct intermediate stages (<xref ref-type="bibr" rid="R36">Piaget, 1952</xref>). Even if the ultimate behaviour is indistinguishable, this initial variability can make comparisons across groups during learning challenging (<xref ref-type="bibr" rid="R44">The International Brain Laboratory et al., 2021</xref>). More generally, the particularities of the learning path may never be fully overcome, so that their trace is detectable in performance even after learning has finished and behaviour has stabilised (<xref ref-type="bibr" rid="R9">Dayan, Roiser, &amp; Viding, 2020</xref>).</p><p id="P3">Despite the richness and importance of these dynamics, much of the work on the modelling of learning has ignored this sort of acquisition, generally considering the adjustments that occur to adapt to ongoing changes in modest facets of tasks, such as reversing reward schedules. By this point, most of the problem has been solved, or subjects who failed to learn have been discarded. One of the main reasons for this is that each animal provides only one sample of a learning curve, whereas for fully acquired behaviour, every trial can typically be viewed as another sample from the learned behaviour. This means that learning curve data are generally sparse, further aggravating the problem of large variability. Here, we make use of the large-scale approach to data collection embodied by the International Brain Laboratory (<xref ref-type="bibr" rid="R44">The International Brain Laboratory et al., 2021</xref>), and base our analysis on the multi-session learning curves of more than 100 mice coming to solve a perceptual decision-making task.</p><p id="P4">Nevertheless, there has been some work on quantifying acquisition. One set of results concerns the point in time at which an animal can be said to have ”learned” a task, often defined as reliably above chance performance (<xref ref-type="bibr" rid="R41">Smith et al., 2004</xref>). This kind of change-point detection is a difficult challenge, as behaviour is generally probabilistic and can be erratic, prompting the development of a number of approaches (see e.g. <xref ref-type="bibr" rid="R21">Jang et al. (2015)</xref>; <xref ref-type="bibr" rid="R35">Papachristos and Gallistel (2006)</xref>). These methods are however more concerned with finding changes in general, in particular making a binary distinction between uninformed and learned behaviour, rather then describing used strategies in detail, or finding possible intermediate stages. Recent work addressing strategy inference more specifically, which does consider learning, includes the approach of <xref ref-type="bibr" rid="R30">Maggi et al. (2022)</xref>. They combined a set of simple, pre-selected strategies with an inference mechanism for performing strategy inference on a trial-by-trial basis. By exponentially decaying evidence over time, they were able to track the arrival and departure of various strategies. Their framework does however require formalising possible strategies ahead of time, and cannot incorporate probabilistic strategies. There has also work on forms of progressive meta-learning in humans (<xref ref-type="bibr" rid="R20">Jain et al., 2023</xref>).</p><p id="P5">To accommodate the complexities of learning curves, a descriptive modelling framework must fulfil a number of desiderata. First, at any point along the curve, it should capture the current repertoire of behaviours, characterizing performance. Second, it needs to track this repertoire as behaviour evolves, introducing new components (which we identify as behavioural ’states’) when change is abrupt (e.g. <xref ref-type="bibr" rid="R10">Durstewitz, Vittoz, Floresco, and Seamans (2010)</xref>; <xref ref-type="bibr" rid="R14">Gallistel et al. (2004)</xref>), detecting the re-use of a past state if it re-emerges, and allowing for slow, gradual shifts in a component, for instance, with the steady development of skilled performance (e.g. <xref ref-type="bibr" rid="R42">Song, Baah, Cai, and Niv (2022)</xref>). A third requirement is that the collection of components should be potentially unbounded, since it is not generally possible to pin down ahead of time how many distinct behaviours any individual animal might exhibit. Finally, it should be possible to switch between the states within a current repertoire, as rapidly as from one trial to the next.</p><p id="P6">We satisfy all three requirements by building a model that combines and extends two recent approaches. One is from <xref ref-type="bibr" rid="R2">Ashwood et al. (2022)</xref> (see also <xref ref-type="bibr" rid="R7">Calhoun, Pillow, and Murthy (2019)</xref>), who described decision-making performance after learning with a hidden Markov model (HMM). In this, each hidden or latent state of the model captures a single component of behaviour in the form of a map from task-relevant variables to distributions over choices, for instance via logistic regression. In the case of perceptual decision-making, this generalizes a psychometric function to include factors such as perseveration, by considering previous choices as an input feature. The overall description of behaviour is in terms of a mixture of different policies (such as engaged or disengaged motivational states) that can switch rapidly. However, the HMM approach of <xref ref-type="bibr" rid="R2">Ashwood et al. (2022)</xref> assumes stationarity of behaviour across time, and is constrained to a fixed level of complexity, by specifying the number of states, and thereby the number of parameters, <italic>a priori</italic>. This makes it ill-suited to characterising the dynamic and idiosyncratic progression through training. To address these issues, we adopted the HMM framework to capture abrupt changes, except that (i) along with the motivational factors, the latent states can describe what the animal knows about the task at any point; that (ii) we used a semi-Markov model so that latent states can persist for non-exponentially distributed numbers of trials; and that (iii) the states come from a Bayesian non-parametric structure, allowing for a degree of behavioural complexity that is only constrained by an inbuilt Occam’s razor, and enabling the introduction of new states for suddenly appearing new behaviours (<xref ref-type="bibr" rid="R3">Beal, Ghahramani, &amp; Rasmussen, 2001</xref>; <xref ref-type="bibr" rid="R17">Gershman &amp; Blei, 2012</xref>; <xref ref-type="bibr" rid="R19">Heald, Lengyel, &amp; Wolpert, 2021</xref>; <xref ref-type="bibr" rid="R22">Johnson &amp; Willsky, 2013</xref>; <xref ref-type="bibr" rid="R43">Teh, Jordan, Beal, &amp; Blei, 2006</xref>).</p><p id="P7">The second approach is that of <xref ref-type="bibr" rid="R40">Roy, Bak, Akrami, Brody, and Pillow (2021)</xref>, which effectively considers just a single state, but allows the weights of the logistic regression to be dynamic, tracking changes in behaviour through appropriate changes in the weights. We used this so that the characteristics of our hidden states can evolve slowly, capturing the other prevalent form of acquisition of skilled performance.</p><p id="P8">Using our combined model, we show that learning progressed over a small number of distinct stages which are present in almost all animals. These stages apparently correspond to the sequential acquisition of elements of the task – in our case, particularly associated with taking into account different aspects of the sensory environment inherent to the task. Although this pattern was shared across all the mice, the duration and diversity of the stages differed greatly between individuals. We also found other prevalent patterns, including regression from newer states affording good performance to older ones affording worse performance over the course of the learning curve.</p><p id="P9">We first describe the IBL task and our way of characterising the behaviour mice exhibit; then discuss the details of the model by studying a representative fit to one animal in detail; and conclude by summarising the fits of our model to 119 subjects, highlighting similarities and differences across the population.</p></sec><sec id="S2" sec-type="results"><label>2</label><title>Results</title><p id="P10">We analysed the choices of 119 mice learning a perceptual decision-making task, each of them going through on average 24.2 (total: &gt;2800) sessions and on average <italic>∼</italic>14700 trials (total: &gt;1.7 million) (<xref ref-type="bibr" rid="R44">The International Brain Laboratory et al., 2021</xref>). In this task, head-fixed mice were shown a sinusoidal grating of a controlled contrast, with equal probability on either the right or left side of a screen. They then had to center it (within 60 s) by turning a steering wheel in the appropriate direction. Successful trials led to water reward; unsuccessful trials to a noise burst and a 1 s timeout. Trials were self-paced, with mice signalling their readiness by keeping the wheel still for a period.</p><p id="P11">Mice learned the task according to a rigorous shaping protocol, which introduced more difficult stimuli gradually and actively removed action biases. Accordingly, shaping started with gratings of the highest contrasts: 100% and 50%. At this initial stage, there was no perceptual difficulty, but the animals had to learn the basic contingencies and requirements of the task. Once they had reached sufficient performance on these contrasts (≥ 80% correct for each contrast type on the last 50 trials), 25% strength contrasts were introduced. After performance was good on this extended set as well (same criterion for each contrast), the remaining contrasts were introduced in a staggered manner: 12.5%, 6.125%, and 0%, while the 50% contrast was dropped from the task. For the 0% contrast, one side was rewarded randomly (with probabilities 50% each). A debiasing protocol increased the probability of repeating the stimulus that was just shown when the mouse made a mistake on an easy (100% or 50%) contrast. This served to motivate the animals away from perseverative or biased strategies, and could lead to reward rates lower than 50% (as would otherwise be expected from pure chance).</p><p id="P12">In order to characterise the course of learning across every trial, we developed a flexible model which segments the behaviour of an animal into discrete states that last for variable numbers of trials within a session and can recur in multiple sessions. As this is a descriptive model, we equate a behaviour with its corresponding state, and generally will not distinguish between the two in the text. We first describe how a single state generates choice probabilities on a trial for which it was responsible (<xref ref-type="fig" rid="F1">Fig. 1</xref> within circles); and then how we treat multiple states (<xref ref-type="fig" rid="F1">Fig. 1</xref> arrows).</p><p id="P13">As in previous work (<xref ref-type="bibr" rid="R2">Ashwood et al., 2022</xref>; <xref ref-type="bibr" rid="R40">Roy et al., 2021</xref>), we formalise the response probabilities for the binary choices of mice through logistic regression (omitting the rare trials in which the animal timed out by not responding within 60 s). Trial <italic>t</italic> of session <italic>n</italic> is described by features <bold><italic>f</italic></bold><sub><italic>n,t</italic></sub> comprising: (i) the stimulus, i.e. the contrast on the left and right of the screen; separated, to allow for different sensitivities to leftwards and rightwards stimuli, which is important because mice were frequently differently sensitive to the screen sides in this task (ii) task history, in the shape of an exponentially decaying average of the last 5 actions; since mice appear not to have used reward information to implement win-stay lose-shift strategies, but rather employed a perseverative bias to repeat previous actions (as was also observed in <xref ref-type="bibr" rid="R32">Miller, Botvinick, and Brody (2021)</xref>, <xref ref-type="bibr" rid="R5">Beron, Neufeld, Linderman, and Sabatini (2022)</xref>, and, in the same task at a later stage, by <xref ref-type="bibr" rid="R12">Findling et al. (2023)</xref>) (iii) a bias term to allow for side preferences regardless of other features. If we label the state that is active on this trial as <italic>x</italic><sub><italic>n,t</italic></sub>, the response <italic>y</italic><sub><italic>n,t</italic></sub> ∈ {L, R} (for Left and Right) is modelled by the distribution <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>R</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sig</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where the weights of the states <bold><italic>w</italic></bold><sub><italic>x,n</italic></sub>, ∀<italic>x</italic> are also indexed by session <italic>n</italic> as they can drift across sessions. Here sig(<italic>·</italic>) is the standard logistic sigmoid function.</p><p id="P14">The model generalises a standard hidden Markov model (HMM) in three ways that make it especially suited to describe the phases of learning, see also <xref ref-type="fig" rid="F1">Fig. 1</xref>: (i) it is non-parametric about the number of states, i.e. the number of states appropriate for describing the behaviour of each individual is separately determined, thus accommodating inter-individual differences. This characteristic also allows the model to capture sudden changes in behaviour, as it is able to introduce a new state when behaviour changes systematically (we call this the ’fast process’). (ii) States are dynamic over sessions <italic>n</italic> (see equation 6.2), allowing the nature of behaviour implied by a state to change gradually across session boundaries (<xref ref-type="bibr" rid="R40">Roy et al., 2021</xref>) (we call this the ’slow process’). (iii) Whereas for HMMs, the numbers of trials for which a single state remains active always follows an exponential distribution, we adopt a semi-Markovian approach, allowing for more general distributions (which duly provided a better fit to the data). The prior over these duration distributions encourages temporally extended states, in order to extract persistent behavioural modes, rather than single trial deviations which are more likely noise. Taking all these additions together we end up with a dynamic infinite input-output hidden semi-Markov model (abbreviated as diHMM).</p><p id="P15">The transition matrix over a flexible number of states and the evolution of the psychometric weights are defined by priors, and the Bernoulli observation model provides a likelihood for each trial, allowing for approximate Bayesian inference (further details in Methods). We performed this via a Markov chain Monte-Carlo algorithm, namely Gibbs sampling. For a single animal, the entire response and feature data across all training sessions were fitted together (individuals were fitted entirely separately). Integrating across a number of Gibbs samples from multiple Markov-chains led to a set of behavioural states defined by their session-varying weights <bold><italic>w</italic></bold><sub><italic>x,n</italic></sub> and duration distributions, as well as a hard assignment of every training trial onto one of these states (we discuss a way to estimate how strongly a trial is connected to its state in the Methods). While all other relevant random variables are specified hierarchically or ruled by vague priors, the variance allowing slow changes within states is set, as inference over this variable proved problematic. We revisit this parameter in the discussion.</p></sec><sec id="S3"><label>3</label><title>Single animal fit</title><p id="P16">We visually summarise the model fit to an individual at the resolution of entire sessions as shown in <xref ref-type="fig" rid="F2">Fig. 2</xref>. This particular animal exemplified many of the interesting properties that can be found across the wider population of trained mice. The inferred model contains eight states, but these states were generally active for only a small number of sessions, before being replaced by others. Thus, in a typical session, the mouse used only a small number of states (usually, the majority of trials in a session is explained by a single state). Later states generally represented more adept behaviour, though not exclusively. The mouse started out with state 1 that exhibited a flat psychometric function (PMF; far right of the plot), indicating that the animal did not take into account the relevant feature, the side of the sensory input that was presented on the screen. This particular state only explained the very first session, before being replaced by state 2, which also has a flat PMF, only notably shifted. This shift in bias was strong enough to warrant a new state (rather than using the slow process to change the existing state), but there is no evidence that the animal advanced in its understanding of the underlying task.</p><p id="P17">State 2 lasted for four sessions, meaning behaviour stayed relatively consistent during this time, before being predominantly replaced by state 3, with yet another flat PMF, which showed extremely biased behaviour (leading to an even lower reward rate, due to the bias correction). However, in the same session as the appearance of this state, we also finally saw the introduction of state 4 with a non-flat PMF. This new state showed very good performance on one side (the leftwards side in this case), and more or less random performance on the other side. It seems that the mouse was only taking sensory information from one side of the screen into account when making its choice, and performed randomly if that side was uninformative. It is important to keep in mind that such random behaviour for the other side was more rewarding than always giving the same answer. This is partly since this random answer will sometimes have been correct, by chance, and partly because the bias correction foiled perseverative behaviour by repeating mistaken trials. Thus, random behaviour would lead to more trials on the side at which the animal was proficient (though it would indeed have been better to choose the opposite side deterministically if the stimulus did not appear on the attended side). Along with state 4 we also saw the introduction of state 5, describing the behaviour at the ends of sessions 7, 8, 9, and 11 (and later also the ends of session 14 and 15). Puzzlingly, this state had a good PMF on both sides and a higher reward rate than state 4, but even though this better state was available, the animal seemed incapable or unwilling to use it for the majority of a session.</p><p id="P18">The ”one-sided” state 4 remained active for some sessions, over the course of which the chance-performance side of the PMF gradually improved through the slow process, as can be seen in the evolving PMF (with darker colours showing later sessions). The last major step in learning however appeared abruptly again, as the animal showed a state (6) in which its performance was good on both sides (notably, this new behaviour was yet again different from state 5, though they both had the quality of being good on both sides). Together with state 6 we also saw the introduction of state 7, which captured a strong but transient decline in the quality of behaviour during sessions. Lastly, state 8 represented another notable change in behaviour, as the performance on 100% contrasts approached perfection sufficiently abruptly to warrant a new state, which allowed the mouse to conclude this part of training soon thereafter. Note that this model lacks a formal lapse or trembling ’hand’ process, but it captures errors on easy contrasts by setting the psychometric weights of a state such that the PMF asymptotes at that error rate for the given range of contrasts.</p><p id="P19">Our model also affords a fine-grained look at the use of behavioural states within a session. Although the diHMM provides a full posterior over the states for each trial, this is not directly useful, due to technicalities of the sampling procedure. We therefore processed the chains of samples to extract a measure of how much a trial belonged to a state (details in Methods). We show an excerpt of this for session 12 of the animal we discussed so far in <xref ref-type="fig" rid="F3">Fig. 3</xref>. This shows two rather clear transitions between states. The reasons for the animal to have made such a transition are probably multi-faceted, and may have been both internal, such as insights into the task leading to changed behaviour and more rewards, high or low motivation to obtain reward (<xref ref-type="bibr" rid="R4">Berditchevskaia, Cazé, &amp; Schultz, 2016</xref>), as well as external, such as for example a number of low contrast, perchance unrewarded trials serving to demotivate the animal. We do not model these reasons, and instead only describe observed changes.</p><p id="P20">The within-session fit showcases two interesting points about the model. First, it is able to find temporary, but strong deviations in behaviour. State 7 only explained a couple of dozen trials in two sessions, but while the animal was in this state, it acted in an extremely biased way (comparable, but flipped relative to its earlier state 4, with the notable difference that state 7 was much briefer in duration when it appeared, and we know that this state could not have originated from a lack of task understanding, but was presumably some form of inattention). This change in behaviour can be spotted directly in the response patterns of the animal, and different Gibbs samples of the model’s posterior consistently used a separate state to characterize the trials concerned.</p><p id="P21">The second point is more subtle, and less directly observable in the mouse’s responses: the model used different states to explain behaviour before and after the biased trials explained by state 7, even though behaviour looked about equally good in both time periods. Going into the details of the fit, we can see that the model assigned different error rates on easy contrasts to the two states, and this can be found in the choices: the proportions of choices for the two different sides are significantly different for the two states (ANOVA for responses during trials of state 5 or 6 in session 12, with factors signed contrast and state, the latter having two levels, state 5 and state 6: the factor state is significant with p=0.0011), which can also be gleaned from response dots marked by arrows in those periods in <xref ref-type="fig" rid="F3">Fig. 3</xref>).</p></sec><sec id="S4"><label>4</label><title>Fits across the population</title><p id="P22">The three-fold progression we observed throughout learning in <xref ref-type="fig" rid="F2">Fig. 2</xref>, from flat PMFs, to ”one-sided” behaviour, to generally good performance, is typical for the population of mice we fitted. To define this more objectively, we clustered the states into these three types, based on their reward rate, i.e. for every state we took its PMF (ignoring perseveration) and computed the expected number of correct responses on easy trials (100% or 50% contrast, since some PMFs were only well defined for these contrasts, additionally, including more difficult contrasts can lead to lower reward rates for more broadly defined PMFs even though they are better on easy contrasts). The boundary between type 1 and 2 is at 60% reward rate, and between type 2 and 3 at 78% reward rate (details in Methods). We show an overview and some examples of the different types in <xref ref-type="fig" rid="F4">Fig. 4</xref>. Note that this only exhibits the first PMF of each state, i.e. the behaviour it described at its inception (as states could change their type via the slow process), thereby representing response characteristics after a notable discontinuity in behaviour.</p><p id="P23">In addition to the state types, we define the <italic>stage</italic> at which an animal is, as the highest type it has so far used for the majority of any previous session. For instance, if up to session <italic>s−</italic>1, an animal only used type 1 states, or type 2 states for fewer than 50% of trials, then it would be in stage 1 for those sessions. If on session <italic>s</italic>, it then used type 2 states for more than 50% of trials, it would switch to stage 2 on that session. Since the state types delineate structurally different aspects of task understanding, the stages allow us to determine for how many sessions the animals stayed at a certain level of understanding. Stage 1 involves a sequence of one or more states with flat PMFs which could show a whole range of biases, but share the property that the contrast location seemed not to be taken into account to determine choices. Stage 2 almost always involves states that showed good performance for stimuli on either the right or the left side, but close to uniform guessing for the other side. Only rarely were intermediate PMFs nearly equally good on both sides, <xref ref-type="fig" rid="F4">Fig. 4</xref>b and c account for 85% of intermediate PMFs, and d for the remaining 15% (those rare cases were still closer to other type 2 states than type 3, which we verified by looking at their appearance time during training). After some time in such partially sensitive states, the animals started, in stage 3, apparently paying attention to both sides. The more difficult contrasts were usually quickly introduced once such states were reached, as they tended to be relatively symmetric, and therefore the type requirement for this stage (a close to 80% reward rate or higher), also led to a fulfillment of the criterion for contrast introductions. Generally, it took some further refinement of initial type 3 states, through the reduction of errors on easy trials on either side, to master this stage of training and progress to the next level of shaping in the IBL task.</p><p id="P24">The progression through state types was not monotonic, since multiple states of potentially different types could be simultaneously present in a session, and, e.g. a state of type 2 could dominate in a session after other states of type 3, as in <xref ref-type="fig" rid="F2">Fig. 2</xref> on session 11. Nevertheless, the stage classification is monotonic, as it reports the higher level of performance which the animal is known to have reached.</p><p id="P25">The three stages segment the learning process. Firstly, we can analyse the proportion of training time the animals spent in the different stages, by showing these proportions on a simplex (<xref ref-type="fig" rid="F5">Fig. 5</xref>). The classification into the types proves its behavioural relevance, as the large majority of animals needed some time in each of the stages (i.e. there are only few mice on the edges of the simplex). As we can see, most animals spent the longest time in stage 3 – that is to go from moderately competent performance (based on the prevalence of states that implied at least a reasonable understanding of the task), to behaviour that sufficed to pass the rather stringent training criteria. This was unexpected, as no fundamental change in understanding seems necessary, unlike the changes from stage 1 to 2 (where the animal has presumably to learn to pay attention to the Gabor patch in the first place); and from stage 2 to 3 (where it must learn to pay attention to both sides of the screen). However, reaching the required accuracy seemed difficult, even once the principles of the task were understood. Of course, the increase in reward rate was not great in this period, so there might have been less pressure to improve. Some of the very longest trajectories (the largest circles) were associated with especially large numbers of sessions in stage 3, but overall the average fractional occupation was remarkably consistent across training lengths (for a median split on the total training time, the mean relative occupancy for stage 1, 2, and 3 respectively are: shorter half (0.24, 0.17, 0.59), longer half (0.21, 0.15, 0.64)). Type 2 is the stage which consistently lasted for the fewest sessions, implying that the mice did manage to pay attention to both sides not too long after starting to pay attention to one side.</p><p id="P26">Connected to this is the question of how the two kinds of change affect behaviour. For the slow process (gradual changes within a state), we analysed this by comparing the weights of the PMFs at the first session of a state, to those for its last session. For the fast process (new state introductions), we compared the weights of a newly introduced state to the closest previous state, as determined by the Wasserstein metric on their resulting PMFs (ignoring the perseverative weight). To highlight the change most clearly, we focussed on states which bring the animal into a new stage. These weight evolutions, split by the different types, can be seen in <xref ref-type="fig" rid="F6">Fig. 6</xref>. As the main driver of performance, contrast sensitivities reliably increased both over the lifetime of a state and when a new state got introduced. Surprisingly however, we observed that both the bias and perseverative weights were remarkably stable within a state, even though these weights were not beneficial to performance. The bias tended to change more substantially upon the introduction of a new state, marking a decided difference between what the two types of change accomplished. When comparing the distributions of weight changes of the bias weights (see <xref ref-type="supplementary-material" rid="SD1">Fig. A16 and Fig. A17</xref>), the changes through the fast process tended to be significantly larger (Mann-Whitney-U-test on absolute weight changes, 2 biases, 2 fast change points, 3 slow change processes, for 9 out of these 12 comparisons, the fast changes had a significantly larger change at a 0.05 significance level). We can also see that the perseveration weight played a small, but consistent role throughout learning (though its relative influence waned as the sensitivities kept growing).</p><p id="P27">The introduction of new states signifies notable changes in behaviour, so by studying the patterns of their occurrences, we gain insight into when behaviour was volatile or when substantial progress was made. The two histograms in <xref ref-type="fig" rid="F7">Fig. 7</xref> show when new states first appeared across normalised training and session time, for the entire population of mice. Of course, all animals needed to introduce at least one new state in the first session, to explain any behaviour, therefore we excluded this first state so as not to skew the histograms. In subsequent sessions, gradually fewer states tended to be introduced, indicating that behaviour saw fewer drastic changes as training progressed. We observed earlier that animals spent most of their time in stage 3, i.e. perfecting their behaviour, and we can now conclude that this mostly came about through gradual improvements of behaviour, rather than sudden marked changes. The pattern of introductions within individual sessions is even more striking: the majority of states were introduced at the very start of a session. This resonates with previous findings about changepoints in behaviour occurring at session boundaries (<xref ref-type="bibr" rid="R35">Papachristos &amp; Gallistel, 2006</xref>). Apart from this strong trend, there seems to be a slight tendency for new states to get introduced towards the end of sessions, which might have partly been inattentive or demotivated states which the animals often fell into at the end of a session, and which the model sometimes picked up on when they were consistent and long enough (the end of a session is triggered if it is longer than 90 minutes, or due to slow response times).</p><sec id="S5"><label>4.1</label><title>Inter-individual differences and variability</title><p id="P28">So far, we have highlighted general patterns during learning, but perhaps even more salient than these similarities was the wide-ranging variability across animals. Inter-individual differences, especially during learning, are a known phenomenon (<xref ref-type="bibr" rid="R44">The International Brain Laboratory et al., 2021</xref>), though rather less commonly studied (though, for instance, see <xref ref-type="bibr" rid="R1">Akiti et al. (2022)</xref>; <xref ref-type="bibr" rid="R23">Kastner et al. (2022)</xref>). Such differences are already visible in many of the plots above. Biases in type 1 states spanned the entire range of possible response patterns. Similarly, type 2 PMFs appear to have been randomly biased to one side or another, or, in rare cases, were symmetric and largely unbiased, but did not yet show good performance. We were particularly surprised to notice that we could find no regularity between type 1 and type 2 biases. Of the 57 mice in which type 2 onset occured suddenly, see <xref ref-type="fig" rid="F6">Fig. 6</xref>, 32 had expressed the same direction of bias as the new type 2 state in any previous type 1 state, whereas 25 had not (two-sided Binomial test for whether the proportion of previously expressed biases differs from 0.5 gives p=0.427). Here, we define leftwards, rightwards, and no bias as whether the average rightwards response probability of a state’s PMF is below 0.45, above 0.55, or between those two values respectively. This means that one cannot predict future biases of the animal from its stage one biases.</p><p id="P29">The number of sessions mice required to learn also varied widely, spanning an entire order of magnitude. Similarly, the number of sessions spent in the different stages was also highly variable. To gain insight into the factors underlying the learning steps between the stages, we analysed the correlations between the number of sessions spent in them. The simplex plot does not strongly indicate any patterns, we quantify this by analysing their correlations to one another: duration of stage 1 to stage 2: Pearson’s r=0.31, p=0.0007; stage 1 to stage 3: Pearson’s r=-0.004, p=0.96; stage 2 to stage 3: Pearson’s r=0.27, p=0.0029. Notably, the main chunks of training time, stage 1 and 3, show no correlation whatsoever. A speedy understanding of the basic contingency of the task therefore did not necessarily go along with the ability (or will) to perfect this behaviour quickly, suggesting that they required different competences. The strongest correlation exists between stage 1 and 2, which makes sense in so far as they were both concerned with discovering how to make use of the stimulus information.</p><p id="P30">Another phenomenon complicating broad conclusions about the learning trajectory is the prevalence of sessions in which the animal did not use its current best state (in terms of reward rate) for the majority of trials. The existence of such sessions prevents us from viewing learning as a process of discovering new kinds of behaviour, and then using them based on their performance (as indicated by the amount of reward collected), which would make behaviour monotonically improving. To analyse these regressions, we considered the expected reward rate of the PMF of a state, keeping track of which states have been used so far and updating their performance as they changed slowly across time. We define a session to have regressed if the state which explained the majority of trials was more than 2.5% worse (corresponding to 5% of the total range, since reward rates were roughly limited to the range of 0.5 to 1) in terms of reward rate than the best state of previous sessions (for this, as for the state type classification, we computed an idealised reward rate between 0.5 and 1, not taking into account debiasing and non-strong contrasts). Such regressions occured multiple times in the example animal shown in <xref ref-type="fig" rid="F2">Fig. 2</xref>: state 5, which was considerably better than state 4, was introduced towards the end of session 7; nevertheless state 4 was responsible for the majority of sessions 8 and 9, with state 5 again confined to the end of the sessions. Similarly, state 4 was used in session 11, even though both states 5 and 6 were better; and session 12 was dominated by state 5 which by then was worse than state 6.</p><p id="P31">Looking at such regressions across the population, we can see that they occured in more than 85% of the fitted animals and had a notable effect on the overall training time, see <xref ref-type="fig" rid="F8">Fig. 8</xref>. We were unable to determine any causes for these regressions, in particular the time between the previous session and the current one had no significant correlation to the amount of reward lost through the regression (Pearson’s r=-0.006, p=0.78), and the set of times between the previous and a regressed session was not significantly different from the set of times between a previous and a non-regressed session (Mann-Whitney-U-test p=0.74).</p><p id="P32">15% of mice never had a bad session in the sense of a worse reward rate, and these were generally quite fast learners. However, we also see that slow learners did not necessarily have many regressed sessions, as two mice with close to 60 sessions only had 4 and 5 regressed sessions, meaning their performance was mostly monotonically increasing.</p></sec></sec><sec id="S6" sec-type="discussion"><label>5</label><title>Discussion</title><p id="P33">We have presented a highly flexible model which is designed to describe the stages of learning from the very first day an animal interacts with a task until it becomes an expert. Using it on the shaping sessions of the IBL decision-making task, we distinguished fast, abrupt transitions in behaviour, and slower, gradual ones. Learning on this task decomposed into three distinct stages, through which almost all animals went: Initial, undifferentiated, and often biased behaviour, partial, one-sided understanding of task contingencies, and lastly full understanding of the task. While these broad stroke characteristics were consistent across mice, the details of behaviour in these stages differed largely across the population. Similarly, the way they progressed through these stages differed widely in duration and composition of sudden and gradual steps. In addition to improvements through learning however, animals also often regressed in their performance for large parts of sessions, adding another layer of intricacy to the study of the learning trajectory.</p><p id="P34">We found only a small correlation between the time it took individual mice to progress through some of the behavioural stages, suggesting that they had to draw upon largely different skills to learn the requirements of the task. Similarly, animals expressed varying biases across the stages of learning, without notable tendencies to repeat previous biases. This leads us to speculate that there are different sources of biases: initially, in stage 1, when the mouse paid no attention to the stimulus, it might have been a motor bias, in stage 2 the bias could have been an expression of which side the animal happened to notice first as being informative, and in stage 3 the bias might have stemmed from differences in sensory acuity. In the IBL training scheme, after the sessions we analysed, the mice underwent a further phase (’biased block training’, in which left or right stimuli dominated in blocks of 20-100 trials). Consistent with our other results, the length of this phase also turns out not to correlate with the total pre-bias duration, nor to any of the stage durations (see <xref ref-type="sec" rid="S12">section 6.5</xref> for details). This shows yet again that learning was dominated by a large number of factors, and is seemingly difficult to predict.</p><p id="P35">Our modelling approach has a number of limitations. First, the setting of the slow change variance parameter which determined how much the behaviour of a state could change from one session to the next plays a critical role in steering the trade-off between introducing a new state versus adapting an existing one. We optimised this parameter in terms of cross-validation performance for the entire population, under the limitation that it be small enough that states were independently meaningful. However, the magnitude of slow changes may well depend on the individual, or might even vary across training time, and thus a more differentiated treatment might be appropriate. Furthermore, slow changes may also occur within a session (<xref ref-type="bibr" rid="R40">Roy et al., 2021</xref>), which could be incorporated into the model by adding additional time points at which weights can change. Another desirable extension would be to allow the duration distributions also to change over sessions. As training progresses, an animal might e.g. be able to use a high performant state for longer, which could be reflected via a changing duration distribution.</p><p id="P36">The model may also be extended by adding more observations for the states to explain, as the binary choice behaviour may limit the power to distinguish different behavioural modes. One obvious possibility are the reaction times of the animal’s choices; in principle, this would only require adding a suitable distribution to produce times for each state (e.g., from a drift diffusion decision-making process; <xref ref-type="bibr" rid="R18">Gold and Shadlen (2002)</xref>; <xref ref-type="bibr" rid="R38">Ratcliff and McKoon (2008)</xref>). It would likely be necessary to make the distributions dynamic (by making some of the parameters of the drift-diffusion model dynamic), as the reaction times will surely improve with training, and may do so in quite rapid ways. Beyond this, one could add details of mouse posture to the states, such as pupil dilation and bespoke aspects of body posture, though this would require well-tailored distributions to describe (<xref ref-type="bibr" rid="R47">Wiltschko et al., 2015</xref>).</p><p id="P37">Besides learning, our approach to capturing behavioural evolution should be well suited to model other progressive changes, such as those occurring during ageing (<xref ref-type="bibr" rid="R34">Nyberg, Lövdén, Riklund, Lindenberger, &amp; Bäckman, 2012</xref>). Fast and slow transitions between stages have also been studied for the acquisition of movement sequences (<xref ref-type="bibr" rid="R29">Luft &amp; Buitrago, 2005</xref>). In this context, changes which occur between sessions are labelled slow, i.e. improvement through consolidation during breaks. Improvements during a session are considered fast changes. In our work we divide fast and slow changes via the magnitude of the change in psychometric weights, but only allow fast changes during a session (the slow process can only change weights at session breaks), and capture phenomena like an initial warm-up period through the usage of multiple states. Our results contrast with those in motor skill learning, in that if we examine the new states which bring the animals forward substantially, the vast majority occur for the first time at the start of a session, i.e., through what this literature would label as slow learning.</p><p id="P38">Of particular note are the rather long training trajectories. One might <italic>a priori</italic> assume that these are instantiated via the usage of a larger number of states, but this is not the case. Instead, we often see that few states are taken a long way via small steps, from uninformed to proficient (see <xref ref-type="supplementary-material" rid="SD1">Fig. A15</xref>). It will be important to assess the underlying nature of these states and their progression, by tracking neural data through the course of learning. Note that recovery analyses show that the model can cope effectively with such trajectories, without, for instance, inferring wantonly many states (see <xref ref-type="fig" rid="F14">Fig. 14</xref>).</p><p id="P39">Apart from general insight into learning dynamics, our characterisation could help with the fine-tuning of shaping protocols to facilitate task acquisition. For instance. we observed that the animals initially need to learn to connect the stimulus side information with their behaviour, therefore, if quickly reaching expert behaviour is the only goal, it would seem ideal to make this association as salient as possible, for instance by making it dynamic (’wiggling’) in early trials. Conversely, having the more difficult contrasts present from the start might make the task much more difficult to learn, as the connection between actions and feedback would seem more probabilistic to the animals. We also saw that the last stage of training takes the longest time, in which the animal brings its performances to a consistent enough level to pass this stage of training. We found that the biggest contributor to this, i.e. the aspect of behaviour which held them back the longest (for the IBL-specific requirements), was insufficient consistency on easy contrasts. To accelerate this process it might be necessary to manipulate the reward rate, as the animals get a reasonably high reward rate without being too consistent. It might therefore be worth trying to only reward the animal after two correct responses in a row by the time it gets to stage 3.</p><p id="P40">Previous work using an HMM-based approach discovered demotivated states in behaviour (<xref ref-type="bibr" rid="R2">Ashwood et al., 2022</xref>), in a component of the IBL task that happened after the learning trials that we modelled. While we do occasionally find these (as discussed in the first example mouse), they were more rare than might have been expected from that work. For us, a majority of sessions were explained by just a single state. The dominant source of behavioural variability in our data came from learning and other large jumps in psychometric space, therefore the model used its capacity to capture these, rather than more subtle variations of proficient behaviour which can be found later. Especially towards the end of sessions, one can see a dip in performance of the animals purely by looking at the reward rate, on the order of tens of trials, which the model sometimes acknowledged with a separate state (seen in <xref ref-type="fig" rid="F2">Fig. 2</xref> on a number of sessions). However, occasionally we just see a decrease in the prevalence of all states (meaning that these trials did not consistently enough get assigned the same state in the underlying Gibbs samples, see Methods). We interpret this as behaviour having been too variable during this period, and not reliable for long enough (even across multiple sessions) to have justified its own particular state in the model.</p><p id="P41">Our framework can be flexibly adapted to other cases of long-run learning. For instance, it is possible to tune the model to capture minute changes within sessions as opposed to broad stroke states across sessions, as here, by adjusting the propensity to infer new states for small changes in behaviour. Equally, the modular resampling procedure of the model allows it to be adapted to different kinds of observations, e.g. multinomial or Gaussian, by simply swapping out the inference mechanism of this one component of the model (although only some distributions are convenient for the gradual dynamics). We therefore hope that the tool we developed here will enable a wide range of researchers to study the development of behaviour in a systematic and revealing manner.</p></sec><sec id="S7" sec-type="methods"><label>6</label><title>Methods</title><p id="P42">We structure these methods as follows: We begin with a detailed description of the infinite hidden semi-Markov model, i.e. the specification of the priors and how they relate to the other random variables in the hierarchy. In <xref ref-type="sec" rid="S2">section 2</xref> we describe inference for the logistic regression observation distributions, with a focus on the details of resampling, as these are not described in detail elsewhere. These two components together give us the full dynamic infinite input-output hidden semi-Markov model (diHMM). We cover how to make sense of the generated collection of samples in <xref ref-type="sec" rid="S10">section 6.3</xref>, dealing with the hurdles of label switching and multi-modality, ultimately obtaining well-defined states from the samples. This concludes the details of the model itself. <xref ref-type="sec" rid="S13">Section 6.4</xref> elaborates how we assign states and their PMFs to the three types. Extending slightly beyond the scope of this work, <xref ref-type="sec" rid="S12">section 6.5</xref> relates properties of the initial learning considered here with the next training step, the biased block training. We conclude the methods by showing successful recoveries of various generative models in <xref ref-type="sec" rid="S13">section 6.6</xref>.</p><sec id="S8"><label>6.1</label><title>Infinite hidden semi-Markov model</title><p id="P43">We start by describing the diHMM, focussing on Bayesian inference over its random variables. Following <xref ref-type="bibr" rid="R22">Johnson and Willsky (2013)</xref>, we use Gibbs sampling, a Markov chain Monte-Carlo algorithm (MCMC), to realise an iterative resampling scheme over the model components, including the PMFs of the hidden states and the assignments of the individual trials onto those states. For this purpose, all distributions are paired up with conjugate priors in this section, to enable simple resampling steps. The posterior distribution is ultimately represented by a collection of samples, with every component being assigned an explicit value in each sample.</p><table-wrap id="T1" position="anchor" orientation="portrait"><table frame="box" rules="all"><thead><tr><th align="center" valign="middle" colspan="3">Overview over quantities and counters</th></tr><tr><th align="left" valign="middle">Quantity</th><th align="left" valign="middle">Meaning</th><th align="left" valign="middle">Counter</th></tr></thead><tbody><tr><td align="right" valign="middle">N</td><td align="left" valign="middle">number of sessions</td><td align="left" valign="middle"><italic>n</italic></td></tr><tr><td align="right" valign="middle"><italic>T<sub>n</sub></italic></td><td align="left" valign="middle">number of trials within a session <italic>n</italic></td><td align="left" valign="middle"><italic>t, m</italic></td></tr><tr><td align="right" valign="middle">J</td><td align="left" valign="middle">number of MCMC samples</td><td align="left" valign="middle"><italic>j</italic></td></tr><tr><td align="right" valign="middle">L</td><td align="left" valign="middle">number of states in the model</td><td align="left" valign="middle"><italic>i</italic></td></tr><tr><td align="right" valign="middle">S</td><td align="left" valign="middle">number of states employed within a session</td><td align="left" valign="middle"><italic>s</italic></td></tr></tbody></table></table-wrap><p id="P44">We first describe all the relevant random variables (using the iterator notation from the table above).</p><p id="P45">The technical backbone of an infinite HMM is an hierarchical Dirichlet process. At the top of the hierarchy of this process is the prototypical transition vector <disp-formula id="FD2"><label>(2)</label><mml:math id="M2"><mml:mrow><mml:mi>β</mml:mi><mml:mo>~</mml:mo><mml:mtext>GEM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where GEM (named after Griffiths, Engen, and McCloskey) is a Dirichlet process without a base distribution, a pure stick-breaking process which samples a probability vector over infinitely many elements (which will be states in our case). The concentration parameter <italic>γ</italic> probabilistically determines the size of the individual sticks, and therefore how many states are practically relevant, with a higher <italic>γ</italic> allowing for more states. We put a vague Gamma prior on <italic>γ</italic>, making it, and thereby the propensity to introduce new states, part of the inference as well, with <italic>γ ∼</italic> Gamma(0.001, 0.001).</p><p id="P46">At the next level we sample the transition vectors, a classical HMM component, <italic>π</italic><sub><italic>i</italic></sub> of the individual states <italic>i</italic>. These are tied together via <bold><italic>β</italic></bold>, which is used as the base distribution for a second Dirichlet process <disp-formula id="FD3"><label>(3)</label><mml:math id="M3"><mml:mrow><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>i</mml:mi></mml:mstyle></mml:msub><mml:mo>~</mml:mo><mml:mtext>DP</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> <disp-formula id="FD4"><label>(4)</label><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>~</mml:mo><mml:mtext>GEM</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> <italic>α</italic> is another concentration parameter and determines how closely the <italic>π</italic><sub><italic>i</italic></sub> are related to <bold><italic>β</italic></bold>. Sampling the individual state transition vectors from this common source formalises an overall kind of state popularity. The higher <italic>α</italic>, the more like <bold><italic>β</italic></bold> is <bold><italic>π</italic></bold><sub><bold><italic>i</italic></bold></sub>, <italic>∀i</italic>, and so the more the bias in the frequency of state <italic>i</italic>′ in the particular sample <bold><italic>β</italic></bold> will be reflected in the transitions from <italic>i</italic> to <italic>i</italic>′, and so the more popular <italic>i</italic>′ will be overall. We put another vague Gamma prior on it, <italic>α ∼</italic> Gamma(0.1, 0.1). The initial state distribution <bold><italic>π</italic></bold><sub><bold>0</bold></sub> is drawn entirely separately, with a concentration of 3 as a trade-off between allowing new states but not encouraging the invention of new states at the start of sessions.</p><p id="P47">For our inference scheme, we make use of the weak-limit approximation which puts an upper limit <italic>L</italic> = 15 on the number of states rather than employing the full infinite process. This simplifies the resampling scheme, while still behaving similarly to an infinite HMM if <italic>L</italic> is sufficiently large. Across the entire population, there is only one mouse with 15, one with 14, and three with 12 states; all other mice use fewer states. Furthermore, the minimum fraction of trials captured in states (as described further below) is 99.35% (mean is 99.996%), justifying the choice of <italic>L</italic> = 15. In particular, we still perform inference over the realized state complexity. In the weak-limit framework, <xref ref-type="disp-formula" rid="FD2">Eq. 2</xref>, <xref ref-type="disp-formula" rid="FD3">3</xref>, and <xref ref-type="disp-formula" rid="FD4">4</xref> turn into L-dimensional Dirichlet distributions <disp-formula id="FD5"><label>(5)</label><mml:math id="M5"><mml:mi>β</mml:mi><mml:mo>~</mml:mo><mml:mtext>Dir</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mi>γ</mml:mi><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD6"><label>(6)</label><mml:math id="M6"><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mstyle mathvariant="bold"><mml:mi>i</mml:mi></mml:mstyle></mml:msub><mml:mo>~</mml:mo><mml:mtext>Dir</mml:mtext><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>β</mml:mi></mml:mstyle><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD7"><label>(7)</label><mml:math id="M7"><mml:msub><mml:mi>π</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>~</mml:mo><mml:mtext>Dir</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>/</mml:mo><mml:mi>L</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p id="P48">The transition structure within a session is given by <disp-formula id="FD8"><label>(8)</label><mml:math id="M8"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD9"><label>(9)</label><mml:math id="M9"><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula> where <italic>z</italic><sub><italic>n,s</italic></sub> ∈ {1 … L} is an indicator for the <italic>s</italic>th state within a session <italic>n</italic> (which does not align with the trial number), and <italic>π</italic><sub>0</sub> is the initial state distribution.</p><p id="P49">Given the transition vectors, the workings of the hidden semi-Markov model are fairly standard, except that the duration distributions are specified explicitly rather than being drawn from an exponential distribution (as in a regular HMM). We therefore also prohibit self-transitions, which makes a data augmentation scheme for resampling necessary, as described in <xref ref-type="bibr" rid="R22">Johnson and Willsky (2013)</xref>. Nevertheless, as in a standard HMM, durations are statistically independent of the target state of transitions. Durations are drawn from a negative-binomial distribution, with state-specific random variables, coming from their own priors <disp-formula id="FD10"><label>(10)</label><mml:math id="M10"><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mn>13</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mn>701</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD11"><label>(11)</label><mml:math id="M11"><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mtext>Beta</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD12"><label>(12)</label><mml:math id="M12"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula> (Note the difference between state names <italic>i</italic>, which hold for the entire model, and the session specific state counters <italic>s</italic>, which can be used to find the current state name via the indicator <italic>z</italic><sub><italic>n,s</italic></sub>). We chose a uniform prior over a large range of numbers for the possible values of <italic>r</italic>, to enable long durations, but excluded small values for <italic>r</italic> (in particular <italic>r</italic> = 1 would give the geometric distribution). Small values of <italic>r</italic> encourage transitions after a very small number of trials, which would capture the statistics of the presentation of left and right stimuli by the experimenter rather than the longer-lasting states that we sought.</p><p id="P50">States stay active and generate observations for as long as the drawn duration indicates <disp-formula id="FD13"><label>(13)</label><mml:math id="M13"><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:math></disp-formula> <disp-formula id="FD14"><label>(14)</label><mml:math id="M14"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula> <disp-formula id="FD15"><label>(15)</label><mml:math id="M15"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>R</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>sig</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>f</mml:mi></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula> where we defined <italic>t</italic><sub><italic>n</italic></sub>(<italic>s</italic>) to return the trial on which the <italic>s</italic>th state of a session <italic>n</italic> starts, which allows for the definition of <italic>x</italic><sub><italic>n,t</italic></sub>, the state on any given trial <italic>t</italic>. We denote the logistic sigmoid function as sig. This takes the dot product between the state weights <bold><italic>w</italic></bold><sub><italic>s,n</italic></sub> (which we discuss in the next section) and the input features of the current trial <bold><italic>f</italic></bold><sub><italic>n,t</italic></sub> and produces the probability over the observation <italic>y</italic><sub><italic>n,t</italic></sub>. The binary response variable <italic>y</italic> has 0 representing a leftwards, and 1 a rightwards, choice.</p><p id="P51">We summarise this collection of variables as <disp-formula id="FD16"><mml:math id="M16"><mml:mrow><mml:mi>Θ</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>π</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>N</italic> is the total number of sessions. The result of inference is a set of samples <inline-formula><mml:math id="M17"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi>Θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> Each sample is a full instantiation of the listed random variables, which we can treat as a representation of the posterior. Gibbs sampling works by iterating through all variables, and re-sampling them from their distribution, given all other variables of the model. After updating all variables, the result is one new sample within the MCMC-chain. The details of how to resample the individual components can be found in (<xref ref-type="bibr" rid="R22">Johnson &amp; Willsky, 2013</xref>).</p></sec><sec id="S9"><label>6.2</label><title>Dynamic logistic regression prior and sampling</title><p id="P52">Gibbs sampling resamples each random variable conditioned on all others. Thus, inference over the observation distributions of the states is separate from almost all the rest of the model, only using the information of which trial is currently assigned to which state. We drop the explicit state dependence <italic>i</italic> in <italic>w</italic><sub><italic>i,t</italic></sub> for this section, but it is important to keep in mind that this sampling scheme is applied to every state individually, with each state <italic>s</italic> being influenced only by trials for which <italic>x</italic><sub><italic>n,t</italic></sub> = <italic>s</italic>. We implement slow changes in the characteristics of the states by putting a Gaussian random walk prior on the weights <bold><italic>w</italic></bold><sub><italic>n</italic></sub>, allowing for modest change across session boundaries, parameterised by the variance <italic>σ</italic>. We choose a diffuse initial distribution for the weights, and use cross-validation to select the inter-session variance <italic>σ</italic> = 0.03 (we performed cross-validation on a constrained range of small values, in order to limit the state adaptation process to small changes): <disp-formula id="FD17"><label>(16)</label><mml:math id="M18"><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>w</mml:mi></mml:mstyle><mml:mn>1</mml:mn></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>⋅</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula> <disp-formula id="FD18"><label>(17)</label><mml:math id="M19"><mml:msub><mml:mstyle mathvariant="bold-italic"><mml:mi>w</mml:mi></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>w</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula></p><p id="P53">If a state has no trial assigned to it in a particular session, its weights are held fixed during the next transition, preventing states from morphing radically during a prolonged absence.</p><p id="P54">Inference for the logistic regression weights is performed using Pólya-Gamma data augmentation, which allows for efficient inference in settings with binomial likelihoods (<xref ref-type="bibr" rid="R27">Linderman, Johnson, &amp; Adams, 2015</xref>; <xref ref-type="bibr" rid="R37">Polson, Scott, &amp; Windle, 2013</xref>), since it is not possible to choose a conjugate prior. We review the relevant computations here, for a full treatment we refer to (<xref ref-type="bibr" rid="R48">Windle, Carvalho, Scott, &amp; Sun, 2013</xref>). In the first step of the resampling scheme we sample pseudo-observations. This uses a Pólya-Gamma distribution PG, by first sampling <italic>ω</italic><sub><italic>n</italic></sub> <italic>∼</italic> PG(<italic>b</italic><sub><italic>n</italic></sub>, <bold><italic>ψ</italic></bold><sub><italic>n</italic></sub>), where <bold><italic>ψ</italic></bold><sub><italic>n</italic></sub> = <bold><italic>f</italic></bold> <sub><italic>n</italic></sub> · <bold><italic>w</italic></bold><sub><italic>n</italic></sub> is the dot product of features and weights, and <italic>b</italic><sub><italic>n</italic></sub> is the total number of times this exact instantiation of features was observed in session <italic>n</italic>. However, the same state is associated with more than just one specific instantiation of features (i.e., including contrasts of different strengths and side, and different response histories). To handle this, we treat a single session as multiple different time points, but prevent weight changes between time points that belong to the same session. In this way the observations from different features within the same session are effectively aggregated. To complete the pseudo-observation generation, we need <italic>κ</italic><sub><italic>n</italic></sub> = <italic>a</italic><sub><italic>n</italic></sub> <italic>− b</italic><sub><italic>n</italic></sub><italic>/</italic>2, where <italic>a</italic><sub><italic>n</italic></sub> is the number of rightwards answers observed for the current <bold><italic>ψ</italic></bold><sub><italic>n</italic></sub> under consideration. Now <italic>z</italic><sub><italic>n</italic></sub> = <italic>κ</italic><sub><italic>n</italic></sub><italic>/ω</italic><sub><italic>n</italic></sub> can be treated as if they were drawn from <inline-formula><mml:math id="M20"><mml:mrow><mml:mi mathvariant="script">N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mstyle mathvariant="bold"><mml:mi>ψ</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P55">This data-augmentation serves the purpose of having the <bold><italic>w</italic></bold><sub><italic>n</italic></sub> emit observations with Gaussian noise (after combination with the features <bold><italic>x</italic></bold><sub><italic>n</italic></sub> into <bold><italic>ψ</italic></bold><sub><italic>n</italic></sub>). Since the prior on <bold><italic>w</italic></bold> is a Gaussian random walk, this places inference in the well studied realm of Kalman filtering. To resample the <bold><italic>w</italic></bold><sub><italic>n</italic></sub> we use the forward filter backward sample algorithm (FFBS, <xref ref-type="bibr" rid="R8">Carter and Kohn (1994)</xref>; <xref ref-type="bibr" rid="R13">Frühwirth-Schnatter (1994)</xref>), which filters forwards through all the observations using a Kalman filter, then samples the sequence of <bold><italic>w</italic></bold><sub><italic>n</italic></sub> backwards through time. A single resampling step therefore consists of first drawing the Pólya-Gamma variables to create pseudo-observations, then using them to sample the <bold><italic>w</italic></bold><sub><italic>n</italic></sub> using the FFBS algorithm.</p><p id="P56">We consider four features for the logistic regression: the contrast on the left side, the contrast on the right side, an exponentially weighted history of five previous choices, and a bias. Separating the features for left and right contrast allows the sensitivities to the two sides to be different, an obvious property of mouse responses. Since the notional contrast values do not match the psychophysical difficulty of the contrasts (100% and 50% are both virtually equally easy to perceive, not a factor of 2 apart), we apply a transformation to have a better alignment. For this we follow <xref ref-type="bibr" rid="R40">Roy et al. (2021)</xref> and use a tanh-transformation, mapping the actual contrast <italic>c</italic> onto the input <inline-formula><mml:math id="M21"><mml:mover accent="true"><mml:mi>c</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:math></inline-formula> for our logistic regression through <inline-formula><mml:math id="M22"><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:mi>tanh</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> where we follow the recommendation and set <italic>p</italic> = 5, which scales the steepness of the transformation. This maps the contrasts (1, 0.5, 0.25, 0.125, 0.0625, 0) onto (1, 0.987, 0.848, 0.555, 0.302, 0).</p><p id="P57">The regressor for previous answers, enabling perseveration as a strategy, proved to be beneficial for cross-validated performance. It is associated with the famous law of exercise (<xref ref-type="bibr" rid="R16">Gershman, 2020</xref>; <xref ref-type="bibr" rid="R45">Thorndike, 1911</xref>), and has also been found to be exhibited by the mice in the asypmtotic regime that arises after the sessions that we are presently analysing (<xref ref-type="bibr" rid="R12">Findling et al., 2023</xref>). By contrast, there was no statistical support for a regressor sensitive to the interaction between past choice and past reward, as would be reflected, for instance, in win-stay, lose-shift behaviour. We implement the perseveration regressor as an exponentially weighted sum over the last few trials. We found that weighting the last 5 trials with an exponentially decaying filter with smoothing factor 0.3 worked best (though slightly different parameter settings have almost equal cross validation performance). Thus, we compute this feature on session <italic>n</italic> and trial <italic>m</italic> as such: <disp-formula id="FD19"><label>(18)</label><mml:math id="M23"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>Z</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:munderover><mml:mrow><mml:mi>exp</mml:mi></mml:mrow></mml:mstyle><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn><mml:mo>⋅</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M24"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>5</mml:mn></mml:msubsup><mml:mrow><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.3</mml:mn><mml:mo>⋅</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula> is a normalisation constant, such that the entire exponential filter adds to 1. The transformation 2 * <italic>y</italic> − 1 serves to encode responses as -1 and 1, for the purpose of having the perseverative feature sway the current response appropriately. Therefore, this feature reaches its maximal value of 1 if all 5 previous responses were rightwards and -1 if they were all leftward, putting it on the same scale as the other features. For the first five trials of a session, not all these previous trials are available, so missing ones are encoded as 0 (and we leave the normalisation constant <italic>Z</italic> unchanged). Time-out trials, where the animal did not respond before 60 s have passed, while skipped for the logistic regression of responses, are taken into account for the previous answer regressor, also encoded as 0.</p></sec><sec id="S10"><label>6.3</label><title>Aggregation and interpretation of chains</title><p id="P58">We generally generated 60,000 samples from each of 16 chains (with different starting points), discarding the first 4000 as burn-in. We assessed convergence of the chains using the classical measure <inline-formula><mml:math id="M25"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="R15">Gelman &amp; Rubin, 1992</xref>). This compares intra- and inter-chain variability of bespoke, state-independent features of the chains. To detect differences in the variances of the chains, and other problems which <inline-formula><mml:math id="M26"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> is known to miss, we also used folded-<inline-formula><mml:math id="M27"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> and rank-normalised-<inline-formula><mml:math id="M28"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="R46">Vehtari, Gelman, Simpson, Carpenter, &amp; Bürkner, 2021</xref>). We reduced the memory cost by thinning the chain, using only every 50th sample (we did this purely for memory reasons, not because it is necessary for MCMC algorithms (<xref ref-type="bibr" rid="R28">Link &amp; Eaton, 2012</xref>)). For a first pass, we sought to discard chains which differed substantially from other chains in the explored region in parameter space, either because they never reached the relevant parts of it, or because they spent disproportionate amounts of time in some modes over others. This is a known problem for MCMC algorithms in multi-modal environments, and can be mitigated by taking non-mixed chains and combining them via stacking (<xref ref-type="bibr" rid="R49">Yao, Vehtari, &amp; Gelman, 2022</xref>). However, since our goal here is not prediction, we still want to focus on finding and visualising the most important modes of the posterior, which we did by combining the (possibly not perfectly mixed) chains, and considering the regions of probability space in which they collectively spent the most time. Given the slow transitions between different modes, we also did not split our individual MCMC chains when computing <inline-formula><mml:math id="M29"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> as the two halves of the chains were often too different.</p><p id="P59">As scalars underlying <inline-formula><mml:math id="M30"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> we used the concentration parameters: <italic>α</italic> and <italic>γ</italic>, as they are independent of states, and, as general properties of the fit: the number of trials assigned to the state with the most trials, and the second-most trials, as well as the overall numbers of states with more than 20%, and more than 10% of trials assigned to them (we chose multiple cutoffs to gain information about the fit at different levels of resolution). By greedily discarding the chains which increase <inline-formula><mml:math id="M31"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> the most, we reduced the number of chains under consideration from 16 to at least 8. For this we considered all features and all variants of <inline-formula><mml:math id="M32"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> (normal, folded, rank-normalised) at once, so we were minimising the maximum over all these <inline-formula><mml:math id="M33"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>s</mml:mtext><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> We only further processed the chains when <inline-formula><mml:math id="M34"><mml:mrow><mml:mover><mml:mi>R</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>&lt;</mml:mo><mml:mn>1.05</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> which is more conservative than some recommendations, but, in light of the strong multi-modality, more lenient than the newest ones (<xref ref-type="bibr" rid="R46">Vehtari et al., 2021</xref>).</p><p id="P60">However, it is still not trivial to extract information from the remaining chains given the multi-modality. There are two main sources of multi-modality: (i) genuine uncertainty in the usage of states or the exact setup of the random variables of the states, and (ii) mode equivalence with permuted labels (e.g., state <italic>i</italic> = 1 in the first chain might explain roughly the same set of trials as state <italic>i</italic> = 2 in the second). Although the second source makes evaluating the results more complicated, it is in fact just the sampling scheme working correctly, as there is nothing special about the particular state labels – solutions with permuted state labels are functionally equivalent. For the same reason, even within a single chain, a relatively consistent set of trials might be explained by one label for some part of the chain, but by a different label in another. Indeed, we frequently observed this kind of label switching, where one state completely took over the trials of another within a few sampling steps. In the limit of infinitely many samples, we can expect any trial to have a uniform distribution over the state label assigned to it; the only important question is which other trials were usually accounted for by the same state as the given trial within suitably similar samples.</p><p id="P61">To formalise the necessary abstraction from direct state assignments, we computed co-occupancy matrices <italic>C</italic><sup><italic>j</italic></sup> for each sample <italic>j. C</italic><sup><italic>j</italic></sup> is a matrix of size <italic>T×T</italic>, with <italic>T</italic> being the total number of trials across all sessions of a mouse, whose <italic>t, m</italic><sup>th</sup> entry reports whether trials <italic>t</italic> and <italic>m</italic> (for convenience, dropping the additional session label) used the same state in sample <italic>j</italic> <disp-formula id="FD20"><label>(19)</label><mml:math id="M35"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P62">We used these co-occupancy matrices as a basis for two different processing steps: (i) at a coarser resolution across trials, we applied dimensionality reduction to find posterior modes; (ii) at full resolution, we averaged <italic>C</italic><sup><italic>j</italic></sup> across similar samples <italic>j</italic> to derive a matrix that describes the mutual affiliation of trials, allowing us to overcome the labelling issues. Both steps are reminiscent of representational similarity analysis (<xref ref-type="bibr" rid="R25">Kriegeskorte, Mur, &amp; Bandettini, 2008</xref>), in that, instead of comparing two samples directly, we compare state co-occurrence within the samples.</p><p id="P63">In principle, to explore the posterior, we could have flattened each <italic>C</italic><sup><italic>j</italic></sup> into an <italic>T</italic><sup>2</sup> vector and apply principal components analysis (PCA). However, there were too many trials per mouse (of the order of 13000) to do this at full resolution, so we binned the trials into 170 bins, ignoring session boundaries, and then used the Wasserstein distance to measure state co-occurence between the bins. That is, we define modified matrices <italic>C</italic><sup><italic>′j</italic></sup> as <disp-formula id="FD21"><label>(20)</label><mml:math id="M36"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mn>1</mml:mn></mml:mstyle><mml:mo>−</mml:mo><mml:mo>|</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup><mml:mo>|</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M37"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula> is the proportion of trials in bin <italic>t</italic> which is assigned to state <italic>i</italic> in sample <italic>j. C</italic><sup><italic>′j</italic></sup> reduces to <italic>C</italic><sup><italic>j</italic></sup> for bins comprising a single trial. We then plotted individual samples in the first three dimensions of the PCA-space arising from flattened versions of <italic>C</italic><sup><italic>′j</italic></sup>, as shown in <xref ref-type="fig" rid="F10">Fig. 10</xref>.</p><p id="P64">In doing this, we found that the posterior for a number of animals wanders itinerantly between different modes, reflecting true uncertainty. These modes are distinct solutions and should not be blended. To isolate them, we performed Gaussian density estimation in the 3D PCA space to identify the ones that were most prevalent, as the regions of highest estimated density. We used this clustering to select samples <inline-formula><mml:math id="M38"><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="script">J</mml:mi><mml:mi>η</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> that were sufficiently similar as to comprise an individual mode <inline-formula><mml:math id="M39"><mml:mrow><mml:msup><mml:mi mathvariant="script">J</mml:mi><mml:mi>η</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> For now, we did this by hand; however, the process could be made more formal by fitting a mixture of Gaussians to the posterior, and then selecting samples around the means of the Gaussians with sufficiently large mixture weights.</p><p id="P65">Next, we sought to understand how trials within that mode were co-assigned to states. To do this, we averaged the co-occurrences <inline-formula><mml:math id="M40"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>η</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mi mathvariant="script">J</mml:mi><mml:mi>η</mml:mi></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:msub><mml:mi>∑</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi mathvariant="script">J</mml:mi><mml:mi>η</mml:mi></mml:msup></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mi>j</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and treated <inline-formula><mml:math id="M41"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>η</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mi>η</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula> as a distance matrix, where trials were close if they share a state in most samples in the mode. We then performed hierarchical clustering on <inline-formula><mml:math id="M42"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>η</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> using as a cluster distance <inline-formula><mml:math id="M43"><mml:mrow><mml:mi>d</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>max</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>C</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>η</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mi>v</mml:mi><mml:mo>;</mml:mo><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mi>ν</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> which took as distance between clusters the maximum distance between any two trials in the clusters <italic>u</italic> and <italic>v</italic>. The result of the hierarchical clustering was a tree on the individual trials, and we could extract a clustering by cutting the tree at a certain level. Cutting at, e.g., 0.6 means that we only have clusters in which every trial was explained by the same state in at least 40% (1<italic>−</italic> 0.6) of the samples. For most of our plots we cut at 0.95, which empirically returned good results. Even though this meant that trials needed to use the same state in only 5% of samples to be in one cluster, most trials were assigned to the same state much more often, see <xref ref-type="fig" rid="F11">Fig. 11</xref>. <xref ref-type="fig" rid="F11">Fig. 11</xref> also shows a number of alternative clustering from different thresholds, demonstrating that there is little change across a wide range of thresholds: The 95% threshold leads to 8 states with 100% trial coverage, an 80% cutoff leads to 9 states and 99.92% coverage, a 50% cutoff gives 12 states with 98.77% coverage, and lastly a threshold at 20% gives 15 states and 95.27%. We can thus see that low criteria led to trials becoming unassigned and some states splitting apart, which is why we chose a rather high cutoff. A further verfication that the procedure and its threshold gave a faithful representation of the collection of samples comes from comparing the overall solution against individual solutions from single samples. Empirically, these did indeed align.</p><p id="P66">The states we show are therefore defined at heart by sets of trials. To compute PMFs, we first considered a single MCMC-sample, and noted which states it assigns to the trials within this set on a session-by-session basis (though each individual trial only had one state assigned in a single sample, for the whole set of trials it usually will not just have been a single state, due to random fluctuations, but mostly a single state). We turned the psychometric weights of these states into PMFs, over which we then averaged (in a weighted manner, considering how often any state occured in the set of trials). For a single sample, this resulted in an average PMF of that state for each session. This then got averaged across samples within a cluster (evenly over all selected samples of a mode), to obtain the ultimate result.</p><p id="P67">To determine how closely a single trial is connected to its assigned state, we averaged the proportions of samples in which it was in the same state as all the other trials assigned to this state. That is, for a given trial <italic>t</italic>, we took a row of the consistency matrix <inline-formula><mml:math id="M44"><mml:mrow><mml:msubsup><mml:mi>C</mml:mi><mml:mi>t</mml:mi><mml:mi>η</mml:mi></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> and considered only the entries corresponding to other trials within the state under consideration. We then averaged over those entries, yielding the average proportion of co-assignment. We think of this as a proxy to the posterior over which state a trial is assigned to, as shown in <xref ref-type="fig" rid="F3">Fig. 3</xref> (with the caveat that we only determined how strongly a trial is connected to its state, not how much it is connected to other states).</p></sec><sec id="S11"><label>6.4</label><title>Psychometric type classification</title><p id="P68">We observed by eye that the psychometric functions (PMFs) that the model found for the behavioural states had a tendency to fall into one of three characteristic classes: flat (type 1), half-tuned (type 2), and fully-tuned (type 3). However, the boundaries between the classes were blurry, so we sought an objective distinction, recognizing its inevitable arbitrariness.</p><p id="P69">The measure we used in the main paper is the mean reward rate implied by the PMF on easy trials (100% or 50%), ignoring the effects of perseveration (and the debiasing protocol). We chose the reward rate, since this tends to grow as the animals proceed from ignorance to competence. We chose to assess only the easy trials since early PMFs were not defined on the lower contrasts (since these stimuli were not presented). <xref ref-type="fig" rid="F12">Fig. 12</xref> shows the distribution of such reward rates across all states. It is apparent that there is a rather clear grouping of PMFs with reward rates below 0.6, defining type 1. The boundary between types 2 and 3 is less evident, implying that edge cases will be hard to assign. The threshold reward rate of 0.78 served reasonably, as evidenced in <xref ref-type="fig" rid="F4">Fig. 4</xref>.</p></sec><sec id="S12"><label>6.5</label><title>Bias training analysis</title><p id="P70">As elaborated, the learning steps that were exhibited by the animals seem rather independent, without strong patterns across the stages. This becomes even clearer when considering the next period of learning, the biased training: The basic task stayed the same, but instead of contrasts appearing equiprobably left or right, there were now unsignalled alternations between blocks, lasting 20-100 trials following a truncated exponential distribution, during which a contrast was 80% likely to appear on one side versus 20% on the other. This was of course particularly helpful for 0% contrasts, on which an animal could now reach a much higher reward rate than chance, given a suitable block inference mechanism (a detailed analysis of their actual algorithm was performed in <xref ref-type="bibr" rid="R12">Findling et al. (2023)</xref>). To finish this part of training, mice had to show that their behaviour was sufficiently modulated by the current block. Quite surprisingly, the number of sessions it took them to achieve this shows no correlations to any aspect of pre-bias training (Correlation to type 1 duration: Pearson’s r=-0.08, p=0.36; to type 2 duration: Pearson’s r=-0.11, p=0.22; to type 3 duration: Pearson’s r=-0.1, p=0.29; to total pre-bias training: Pearson’s r=-0.13, p=0.15). This suggests that learning about the biased blocks tapped into yet another type of skill, unaddressed by the requirements of the pre-bias protocol.</p></sec><sec id="S13"><label>6.6</label><title>Model recovery</title><p id="P71">We tested the model and our inference procedures by fitting to data for which the ground truth was available. For this we instantiated all the random variables of the model to specific values and generated responses from it. This was performed for multiple different variable settings, to assess the accuracy of the fitting procedure in all relevant regimes, and using input data (i.e. contrast sequences) from actual training trajectories. The data generated this way were processed in exactly the same way as those of the IBL mice.</p><p id="P72">We paid particular attention to assessing the strength of the inductive biases of the inference procedure - particularly in terms of the number of states it inferred (given that this could be potentially unbounded) and the degree of change between sessions (since slow and fast state changes could interact). We tested multiple settings in which all the data were actually generated from a single state, to test whether the model would incorrectly split behaviour into multiple states. In one setting, the psychometric weights of the state stayed constant throughout all sessions, in the other the weights gradually evolved from poor performance to proficiency (at constant steps of a magnitude that corresponds to a variance of 0.0311, the variance of the fitting procedure was still 0.03). Both fits recovered their ground truth successfully, explaining virtually all trials with a single state, as can be seen for the example of the changing state in <xref ref-type="fig" rid="F13">Fig. 13</xref>. We also tried a variation of the latter situation, in which the psychometric weights changed in (proportionally smaller) steps on every single trial, rather than all at once at a session boundary (as the model assumes). This, too, was recovered by the model with only one state (which we consider the best possible solution, given that the generative process was outside the model class).</p><p id="P73">We also successfully recovered settings from 2 to 9 states, with and without session-to-session variation on the weights, with strongly varying trial proportions between the different states (see <xref ref-type="supplementary-material" rid="SD1">Fig. A18</xref>), and of varying overall training lengths (particularly to test whether long training trajectories lead the model to impose fewer states, making more use of the slow process), as seen in <xref ref-type="fig" rid="F14">Fig. 14</xref>. The model was also tested on a setting with completely implausible PMFs, but with the added difficulty of having a larger number of states active within each session, <xref ref-type="supplementary-material" rid="SD1">Fig. A19</xref>. This, too, was captured accurately. These successful recoveries suggest that the model is able to uncover states that truly correlate with distinctly different modes of behaviour in animals.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Appendix</label><media xlink:href="EMS193121-supplement-Appendix.pdf" mimetype="application" mime-subtype="pdf" id="d148aAdGbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S14"><title>Acknowledgments</title><p>We thank Jonathan Pillow, Anne Churchland, Alexandre Pouget, Charline Tessereau, Zoe Ashwood, Nicholas Roy, Iain Murray, Scott Lindermann and the Behavioural Analysis and Theory working groups of the International Brain Lab for discussions. This work was funded by the Simons Foundation (SAB &amp; PD under ID: 552343, and the IBL in general), the Max Planck Society (SAB, PD), the Wellcome trust (SAB, IBL, PD, ID: 216324), and the Alexander von Humboldt Foundation (PD).</p></ack><sec id="S15" sec-type="data-availability"><title>Data access</title><p id="P74">Please follow these instructions to download the data used in this article. Use for example the following code snippet to download the data using Python.</p><p id="P75"><preformat preformat-type="computer code">
from one.api import ONE
import re
# use password as indicated on the website
one = ONE(base_url=’<ext-link ext-link-type="uri" xlink:href="https://openalyx.internationalbrainlab.org">https://openalyx.internationalbrainlab.org</ext-link>’, password=’*****’)
regexp = re.compile(r’Subjects/\w*/((\w|-)+)/_ibl’)
datasets = one.alyx.rest(’datasets’, ’list’, tag=’2023_Q4_Bruijns_et_al’)
# extract subject names
subjects = [regexp.search(ds[’file_records’][0][’relative_path’]).group(1) for ds in datasets]
# reduce to list of unique names
subjects = list(set(subjects))
for subject in subjects:
trials = one.load_aggregate(’subjects’, subject, ’_ibl_subjectTrials.table’)
training = one.load_aggregate(’subjects’, subject, ’_ibl_subjectTraining.table’)
# save data
</preformat></p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P76"><bold>Authors’ Contributions.</bold> SAB and PD initiated the project. SAB and PD developed the modelling framework, which was implemented by SAB. KB, ICL, PYPL, GTM, NJM, JN, AP, NR, KZS, AEU collected the data. SAB and PD performed the data analysis. The paper was jointly written by SAB and PD</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akiti</surname><given-names>K</given-names></name><name><surname>Tsutsui-Kimura</surname><given-names>I</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Markowitz</surname><given-names>JE</given-names></name><name><surname>Anyoha</surname><given-names>R</given-names></name><etal/><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><article-title>Striatal dopamine explains novelty-induced behavioral dynamics and individual variability in threat prediction</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>22</issue><fpage>3789</fpage><lpage>3804</lpage><pub-id pub-id-type="pmcid">PMC9671833</pub-id><pub-id pub-id-type="pmid">36130595</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.08.022</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Stone</surname><given-names>IR</given-names></name><collab>The International Brain Laboratory</collab><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><etal/><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Mice alternate between discrete strategies during perceptual decision-making</article-title><source>Nature Neuroscience</source><year>2022</year><month>February</month><volume>25</volume><issue>2</issue><fpage>201</fpage><lpage>212</lpage><comment>Retrieved 2023-05-17 from <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-021-01007-z">https://www.nature.com/articles/s41593-021-01007-z</ext-link></comment><pub-id pub-id-type="pmcid">PMC8890994</pub-id><pub-id pub-id-type="pmid">35132235</pub-id><pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Beal</surname><given-names>M</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Rasmussen</surname><given-names>C</given-names></name></person-group><chapter-title>The infinite hidden markov model</chapter-title><person-group person-group-type="editor"><name><surname>Dietterich</surname><given-names>T</given-names></name><name><surname>Becker</surname><given-names>S</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><source>Advances in neural information processing systems</source><publisher-name>MIT Press</publisher-name><year>2001</year><volume>14</volume><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper_files/paper/2001/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf">https://proceedings.neurips.cc/paper_files/paper/2001/file/e3408432c1a48a52fb6c74d926b38886-Paper.pdf</ext-link></comment></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berditchevskaia</surname><given-names>A</given-names></name><name><surname>Cazé</surname><given-names>RD</given-names></name><name><surname>Schultz</surname><given-names>SR</given-names></name></person-group><article-title>Performance in a go/nogo perceptual task reflects a balance between impulsive and instrumental components of behaviour</article-title><source>Scientific Reports</source><year>2016</year><month>Jun</month><day>07</day><volume>6</volume><issue>1</issue><elocation-id>27389</elocation-id><comment>Retrieved from</comment><pub-id pub-id-type="pmcid">PMC4895381</pub-id><pub-id pub-id-type="pmid">27272438</pub-id><pub-id pub-id-type="doi">10.1038/srep27389</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beron</surname><given-names>CC</given-names></name><name><surname>Neufeld</surname><given-names>SQ</given-names></name><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Sabatini</surname><given-names>BL</given-names></name></person-group><article-title>Mice exhibit stochastic and efficient action switching during probabilistic decision making</article-title><source>Proceedings of the National Academy of Sciences</source><year>2022</year><volume>119</volume><issue>15</issue><elocation-id>e2113961119</elocation-id><comment>Retrieved from</comment><pub-id pub-id-type="pmcid">PMC9169659</pub-id><pub-id pub-id-type="pmid">35385355</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2113961119</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breland</surname><given-names>K</given-names></name><name><surname>Breland</surname><given-names>M</given-names></name></person-group><article-title>A field of applied animal psychology</article-title><source>American Psychologist</source><year>1951</year><volume>6</volume><issue>6</issue><fpage>202</fpage><lpage>204</lpage><comment>Retrieved 2023-06-01 from</comment><pub-id pub-id-type="pmid">14847139</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calhoun</surname><given-names>AJ</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><article-title>Unsupervised identification of the internal states that shape natural behavior</article-title><source>Nat Neurosci</source><year>2019</year><month>November</month><volume>22</volume><issue>12</issue><fpage>2040</fpage><lpage>2049</lpage><pub-id pub-id-type="pmcid">PMC7819718</pub-id><pub-id pub-id-type="pmid">31768056</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0533-x</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carter</surname><given-names>CK</given-names></name><name><surname>Kohn</surname><given-names>R</given-names></name></person-group><article-title>On gibbs sampling for state space models</article-title><source>Biometrika</source><year>1994</year><volume>81</volume><issue>3</issue><fpage>541</fpage><lpage>553</lpage><comment>Retrieved 2023-06-26 from <ext-link ext-link-type="uri" xlink:href="http://www.jstor.org/stable/2337125">http://www.jstor.org/stable/2337125</ext-link></comment></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Roiser</surname><given-names>JP</given-names></name><name><surname>Viding</surname><given-names>E</given-names></name></person-group><chapter-title>The first steps on long marches: The costs of active observation</chapter-title><person-group person-group-type="editor"><name><surname>Savulescu</surname><given-names>J</given-names></name><name><surname>Roache</surname><given-names>R</given-names></name><name><surname>Davies</surname><given-names>W</given-names></name><name><surname>Loebel</surname><given-names>JP</given-names></name></person-group><source>Psychiatry Reborn: Biopsychosocial psychiatry in modern medicine</source><publisher-name>Oxford University Press</publisher-name><year>2020</year><month>October</month><fpage>213</fpage><lpage>228</lpage><comment>Retrieved 2023-06-01 from <ext-link ext-link-type="uri" xlink:href="https://academic.oup.com/book/29854/chapter/252998507">https://academic.oup.com/book/29854/chapter/252998507</ext-link></comment></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durstewitz</surname><given-names>D</given-names></name><name><surname>Vittoz</surname><given-names>NM</given-names></name><name><surname>Floresco</surname><given-names>SB</given-names></name><name><surname>Seamans</surname><given-names>JK</given-names></name></person-group><article-title>Abrupt transitions between prefrontal neural ensemble states accompany behavioral transitions during rule learning</article-title><source>Neuron</source><year>2010</year><month>May</month><volume>66</volume><issue>3</issue><fpage>438</fpage><lpage>448</lpage><pub-id pub-id-type="pmid">20471356</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kirshnit</surname><given-names>CE</given-names></name><name><surname>Lanza</surname><given-names>RP</given-names></name><name><surname>Rubin</surname><given-names>LC</given-names></name></person-group><article-title>‘Insight’ in the pigeon: antecedents and determinants of an intelligent performance</article-title><source>Nature</source><year>1984</year><month>March</month><volume>308</volume><issue>5954</issue><fpage>61</fpage><lpage>62</lpage><comment>Retrieved 2023-06-25 from <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/308061a0">https://www.nature.com/articles/308061a0</ext-link></comment><pub-id pub-id-type="pmid">6700713</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Findling</surname><given-names>C</given-names></name><name><surname>Hubert</surname><given-names>F</given-names></name><name><surname>Laboratory</surname><given-names>IB</given-names></name><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Benson</surname><given-names>B</given-names></name><name><surname>Benson</surname><given-names>J</given-names></name><etal/><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Brain-wide representations of prior information in mouse decision-making</article-title><source>bioRxiv</source><year>2023</year><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2023/07/05/2023.07.04.547684.1">https://www.biorxiv.org/content/early/2023/07/05/2023.07.04.547684.1</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2023/07/05/2023.07.04.547684.1.full.pdf">https://www.biorxiv.org/content/early/2023/07/05/2023.07.04.547684.1.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/2023.07.04.547684</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frühwirth-Schnatter</surname><given-names>S</given-names></name></person-group><article-title>Data augmentation and dynamic linear models</article-title><source>Journal of Time Series Analysis</source><year>1994</year><volume>15</volume><issue>2</issue><fpage>183</fpage><lpage>202</lpage><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.1111/j.1467-9892.1994.tb00184.x</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name><name><surname>Fairhurst</surname><given-names>S</given-names></name><name><surname>Balsam</surname><given-names>P</given-names></name></person-group><article-title>The learning curve: Implications of a quantitative analysis</article-title><source>Proceedings of the National Academy of Sciences</source><year>2004</year><month>September</month><volume>101</volume><issue>36</issue><fpage>13124</fpage><lpage>13131</lpage><comment>Retrieved 2023-05-19 from</comment><pub-id pub-id-type="pmcid">PMC516535</pub-id><pub-id pub-id-type="pmid">15331782</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0404965101</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><article-title>Inference from Iterative Simulation Using Multiple Sequences</article-title><source>Statistical Science</source><year>1992</year><volume>7</volume><issue>4</issue><fpage>457</fpage><lpage>472</lpage><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Origin of perseveration in the trade-off between reward and complexity</article-title><source>Cognition</source><volume>204</volume><elocation-id>104394</elocation-id><year>2020</year><month>November</month><comment>Retrieved 2023-06-26 from <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010027720302134">https://linkinghub.elsevier.com/retrieve/pii/S0010027720302134</ext-link></comment><pub-id pub-id-type="pmid">32679270</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name></person-group><article-title>A tutorial on Bayesian nonparametric models</article-title><source>Journal of Mathematical Psychology</source><year>2012</year><month>February</month><volume>56</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><comment>Retrieved 2023-06-01 from <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S002224961100071X">https://linkinghub.elsevier.com/retrieve/pii/S002224961100071X</ext-link></comment><pub-id pub-id-type="doi">10.1016/j.jmp.2011.08.004</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><article-title>Banburismus and the brain: Decoding the relationship between sensory stimuli, decisions, and reward</article-title><source>Neuron</source><year>2002</year><volume>36</volume><issue>2</issue><fpage>299</fpage><lpage>308</lpage><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0896627302009716">https://www.sciencedirect.com/science/article/pii/S0896627302009716</ext-link></comment><pub-id pub-id-type="pmid">12383783</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heald</surname><given-names>JB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><article-title>Contextual inference underlies the learning of sensorimotor repertoires</article-title><source>Nature</source><year>2021</year><month>December</month><volume>600</volume><issue>7889</issue><fpage>489</fpage><lpage>493</lpage><comment>Retrieved 2023-06-01 from <ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41586-021-04129-3">https://www.nature.com/articles/s41586-021-04129-3</ext-link></comment><pub-id pub-id-type="pmcid">PMC8809113</pub-id><pub-id pub-id-type="pmid">34819674</pub-id><pub-id pub-id-type="doi">10.1038/s41586-021-04129-3</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>YR</given-names></name><name><surname>Callaway</surname><given-names>F</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>He</surname><given-names>R</given-names></name><name><surname>Krueger</surname><given-names>PM</given-names></name><name><surname>Lieder</surname><given-names>F</given-names></name></person-group><article-title>A computational process-tracing method for measuring people’s planning strategies and how they change over time</article-title><source>Behavior Research Methods</source><year>2023</year><volume>55</volume><issue>4</issue><fpage>2037</fpage><lpage>2079</lpage><pub-id pub-id-type="pmcid">PMC10250277</pub-id><pub-id pub-id-type="pmid">35819717</pub-id><pub-id pub-id-type="doi">10.3758/s13428-022-01789-5</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>AI</given-names></name><name><surname>Costa</surname><given-names>VD</given-names></name><name><surname>Rudebeck</surname><given-names>PH</given-names></name><name><surname>Chudasama</surname><given-names>Y</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Averbeck</surname><given-names>BB</given-names></name></person-group><article-title>The role of frontal cortical and Medial-Temporal lobe brain areas in learning a bayesian prior belief on reversals</article-title><source>J Neurosci</source><year>2015</year><month>August</month><volume>35</volume><issue>33</issue><fpage>11751</fpage><lpage>11760</lpage><pub-id pub-id-type="pmcid">PMC4540808</pub-id><pub-id pub-id-type="pmid">26290251</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1594-15.2015</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Willsky</surname><given-names>AS</given-names></name></person-group><article-title>Bayesian nonparametric hidden semi-markov models</article-title><source>J Mach Learn Res</source><year>2013</year><month>feb</month><volume>14</volume><issue>1</issue><fpage>673</fpage><lpage>701</lpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastner</surname><given-names>DB</given-names></name><name><surname>Miller</surname><given-names>EA</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Liu</surname><given-names>DF</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Spatial preferences account for inter-animal variability during the continual learning of a dynamic cognitive task</article-title><source>Cell Reports</source><year>2022</year><volume>39</volume><issue>3</issue><fpage>110708</fpage><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2211124722004697">https://www.sciencedirect.com/science/article/pii/S2211124722004697</ext-link></comment><pub-id pub-id-type="pmcid">PMC9096879</pub-id><pub-id pub-id-type="pmid">35443181</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2022.110708</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Köhler</surname><given-names>W</given-names></name></person-group><source>The mentality of apes</source><volume>1917</volume><year>1948</year><fpage>497</fpage><lpage>505</lpage><publisher-loc>East Norwalk, CT, US</publisher-loc><publisher-name>Appleton-Century-Crofts</publisher-name><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.1037/11304-054</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><year>2008</year><comment>Retrieved from</comment><pub-id pub-id-type="pmcid">PMC2605405</pub-id><pub-id pub-id-type="pmid">19104670</pub-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krueger</surname><given-names>KA</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Flexible shaping: How learning in small steps helps</article-title><source>Cognition</source><year>2009</year><month>March</month><volume>110</volume><issue>3</issue><fpage>380</fpage><lpage>394</lpage><comment>Retrieved 2023-06-01 from <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0010027708002850">https://linkinghub.elsevier.com/retrieve/pii/S0010027708002850</ext-link></comment><pub-id pub-id-type="pmid">19121518</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name></person-group><source>Dependent multinomial models made easy: Stick breaking with the pólya-gamma augmentation</source><conf-name>Proceedings of the 28th international conference on neural information processing systems</conf-name><conf-loc>Cambridge, MA, USA</conf-loc><conf-sponsor>MIT Press</conf-sponsor><year>2015</year><volume>2</volume><fpage>3456</fpage><lpage>3464</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Link</surname><given-names>WA</given-names></name><name><surname>Eaton</surname><given-names>MJ</given-names></name></person-group><article-title>On thinning of chains in MCMC: <italic>Thinning of MCMC chains</italic></article-title><source>Methods in Ecology and Evolution</source><year>2012</year><month>February</month><volume>3</volume><issue>1</issue><fpage>112</fpage><lpage>115</lpage><comment>Retrieved 2023-06-19 from</comment><pub-id pub-id-type="doi">10.1111/j.2041-210X.2011.00131.x</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luft</surname><given-names>AR</given-names></name><name><surname>Buitrago</surname><given-names>MM</given-names></name></person-group><article-title>Stages of motor skill learning</article-title><source>Mol Neurobiol</source><year>2005</year><month>December</month><volume>32</volume><issue>3</issue><fpage>205</fpage><lpage>216</lpage><pub-id pub-id-type="pmid">16385137</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maggi</surname><given-names>S</given-names></name><name><surname>Hock</surname><given-names>RM</given-names></name><name><surname>O’Neill</surname><given-names>M</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Moran</surname><given-names>PM</given-names></name><name><surname>Bast</surname><given-names>T</given-names></name><etal/><name><surname>Humphries</surname><given-names>MD</given-names></name></person-group><article-title>Tracking subject’s strategies in behavioural choice experiments at trial resolution</article-title><source>bioRxiv</source><year>2022</year><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/09/04/2022.08.30.505807">https://www.biorxiv.org/content/early/2022/09/04/2022.08.30.505807</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2022/09/04/2022.08.30.505807.full.pdf">https://www.biorxiv.org/content/early/2022/09/04/2022.08.30.505807.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/2022.08.30.505807</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>NR</given-names></name></person-group><article-title>Reasoning and learning</article-title><source>Psychological review</source><year>1931</year><volume>38</volume><issue>4</issue><fpage>332</fpage></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><article-title>From predictive models to cognitive models: Separable behavioral processes underlying reward learning in the rat</article-title><source>bioRxiv</source><year>2021</year><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2021/02/19/461129">https://www.biorxiv.org/content/early/2021/02/19/461129</ext-link><ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/early/2021/02/19/461129.full.pdf">https://www.biorxiv.org/content/early/2021/02/19/461129.full.pdf</ext-link></comment><pub-id pub-id-type="doi">10.1101/461129</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Kuchibhotla</surname><given-names>KV</given-names></name></person-group><article-title>Slow or sudden: Re-interpreting the learning curve for modern systems neuroscience</article-title><source>IBRO Neuroscience Reports</source><year>2022</year><volume>13</volume><fpage>9</fpage><lpage>14</lpage><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S2667242122000367">https://www.sciencedirect.com/science/article/pii/S2667242122000367</ext-link></comment><pub-id pub-id-type="pmcid">PMC9163689</pub-id><pub-id pub-id-type="pmid">35669385</pub-id><pub-id pub-id-type="doi">10.1016/j.ibneur.2022.05.006</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nyberg</surname><given-names>L</given-names></name><name><surname>Lövdén</surname><given-names>M</given-names></name><name><surname>Riklund</surname><given-names>K</given-names></name><name><surname>Lindenberger</surname><given-names>U</given-names></name><name><surname>Bäckman</surname><given-names>L</given-names></name></person-group><article-title>Memory aging and brain maintenance</article-title><source>Trends in Cognitive Sciences</source><year>2012</year><volume>16</volume><issue>5</issue><fpage>292</fpage><lpage>305</lpage><comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S1364661312000836">https://www.sciencedirect.com/science/article/pii/S1364661312000836</ext-link></comment><pub-id pub-id-type="pmid">22542563</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Papachristos</surname><given-names>EB</given-names></name><name><surname>Gallistel</surname><given-names>CR</given-names></name></person-group><article-title>AUTOSHAPED HEAD POKING IN THE MOUSE: A QUANTITATIVE ANALYSIS OF THE LEARNING CURVE</article-title><source>Journal of the Experimental Analysis of Behavior</source><year>2006</year><month>May</month><volume>85</volume><issue>3</issue><fpage>293</fpage><lpage>308</lpage><comment>Retrieved 2023-05-19 from</comment><pub-id pub-id-type="pmcid">PMC1459847</pub-id><pub-id pub-id-type="pmid">16776053</pub-id><pub-id pub-id-type="doi">10.1901/jeab.2006.71-05</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Piaget</surname><given-names>J</given-names></name></person-group><source>The origins of intelligence in children</source><year>1952</year><publisher-loc>New York, NY, US</publisher-loc><publisher-name>W W Norton &amp; Co</publisher-name><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.1037/11494-000</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polson</surname><given-names>NG</given-names></name><name><surname>Scott</surname><given-names>JG</given-names></name><name><surname>Windle</surname><given-names>J</given-names></name></person-group><article-title>Bayesian inference for logistic models using polya-gamma latent variables</article-title><year>2013</year></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><article-title>The diffusion decision model: Theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><year>2008</year><volume>20</volume><issue>4</issue><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="pmcid">PMC2474742</pub-id><pub-id pub-id-type="pmid">18085991</pub-id><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rescorla</surname><given-names>RA</given-names></name></person-group><article-title>A theory of pavlovian conditioning: Variations in the effectiveness of reinforcement and non-reinforcement</article-title><source>Classical conditioning, Current research and theory</source><year>1972</year><volume>2</volume><fpage>64</fpage><lpage>69</lpage></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Bak</surname><given-names>JH</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Extracting the dynamics of behavior in sensory decision-making experiments</article-title><source>Neuron</source><year>2021</year><month>February</month><volume>109</volume><issue>4</issue><fpage>597</fpage><lpage>610</lpage><elocation-id>e6</elocation-id><comment>Retrieved 2023-05-17 from <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627320309636">https://linkinghub.elsevier.com/retrieve/pii/S0896627320309636</ext-link></comment><pub-id pub-id-type="pmcid">PMC7897255</pub-id><pub-id pub-id-type="pmid">33412101</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.004</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AC</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Wirth</surname><given-names>S</given-names></name><name><surname>Yanike</surname><given-names>M</given-names></name><name><surname>Hu</surname><given-names>D</given-names></name><name><surname>Kubota</surname><given-names>Y</given-names></name><etal/><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><article-title>Dynamic analysis of learning in behavioral experiments</article-title><source>J Neurosci</source><year>2004</year><month>January</month><volume>24</volume><issue>2</issue><fpage>447</fpage><lpage>461</lpage><pub-id pub-id-type="pmcid">PMC6729979</pub-id><pub-id pub-id-type="pmid">14724243</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2908-03.2004</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Baah</surname><given-names>PA</given-names></name><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><article-title>Humans combine value learning and hypothesis testing strategically in multi-dimensional probabilistic reward learning</article-title><source>PLOS Computational Biology</source><year>2022</year><month>November</month><volume>18</volume><issue>11</issue><elocation-id>e1010699</elocation-id><comment>Retrieved 2023-06-19 from</comment><pub-id pub-id-type="pmcid">PMC9683628</pub-id><pub-id pub-id-type="pmid">36417419</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010699</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teh</surname><given-names>YW</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Beal</surname><given-names>MJ</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name></person-group><article-title>Hierarchical Dirichlet Processes</article-title><source>Journal of the American Statistical Association</source><year>2006</year><month>December</month><volume>101</volume><issue>476</issue><fpage>1566</fpage><lpage>1581</lpage><comment>Retrieved 2023-06-20 from</comment><pub-id pub-id-type="doi">10.1198/016214506000000302</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>The International Brain Laboratory</collab><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><etal/><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><year>2021</year><month>May</month><volume>10</volume><elocation-id>e63711</elocation-id><comment>Retrieved 2023-06-01 from <ext-link ext-link-type="uri" xlink:href="https://elifesciences.org/articles/63711">https://elifesciences.org/articles/63711</ext-link></comment><pub-id pub-id-type="pmcid">PMC8137147</pub-id><pub-id pub-id-type="pmid">34011433</pub-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Thorndike</surname><given-names>EL</given-names></name></person-group><source>Animal intelligence: Experimental studies</source><year>1911</year><publisher-loc>Lewiston, NY, US</publisher-loc><publisher-name>Macmillan Press</publisher-name><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.5962/bhl.title.55072</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Bürkner</surname><given-names>P-C</given-names></name></person-group><article-title>Rank-normalization, folding, and localization: An improved r for assessing convergence of MCMC (with discussion)</article-title><source>Bayesian Analysis</source><year>2021</year><month>jun</month><volume>16</volume><issue>2</issue><comment>Retrieved from</comment><pub-id pub-id-type="doi">10.1214/20-ba1221</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiltschko</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>M</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Peterson</surname><given-names>R</given-names></name><name><surname>Katon</surname><given-names>J</given-names></name><name><surname>Pashkovski</surname><given-names>S</given-names></name><etal/><name><surname>Datta</surname><given-names>S</given-names></name></person-group><article-title>Mapping Sub-Second Structure in Mouse Behavior</article-title><source>Neuron</source><year>2015</year><month>December</month><volume>88</volume><issue>6</issue><fpage>1121</fpage><lpage>1135</lpage><comment>Retrieved 2023-05-17 from <ext-link ext-link-type="uri" xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0896627315010375">https://linkinghub.elsevier.com/retrieve/pii/S0896627315010375</ext-link></comment><pub-id pub-id-type="pmcid">PMC4708087</pub-id><pub-id pub-id-type="pmid">26687221</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.031</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Windle</surname><given-names>J</given-names></name><name><surname>Carvalho</surname><given-names>CM</given-names></name><name><surname>Scott</surname><given-names>JG</given-names></name><name><surname>Sun</surname><given-names>L</given-names></name></person-group><article-title>Efficient data augmentation in dynamic models for binary and count data</article-title><year>2013</year></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Y</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><article-title>Stacking for non-mixing bayesian computations: The curse and blessing of multimodal posteriors</article-title><source>J Mach Learn Res</source><year>2022</year><month>jan</month><volume>23</volume><issue>1</issue></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><p>Visual representation of the main components of the model. Each state, represented by a circle, has an observation distribution associated with it, shown inside its circle. This distribution is implemented via logistic regression, to consider the contrast of the current trial and a weighted history of previous choices (the latter is not shown here). The weights underlying these regressions can change from session to session, resulting in shifts of the psychometric functions (PMFs) they represent; we depict this evolution here with varying shades of grey. States are connected to other existing states via transition probabilities <italic>p</italic>. In addition to that, states also have the option to transition into a new state, to describe a type of behaviour which is not well captured by any of the existing states. Lastly, staying in the same state for more than one trial is not modelled via a self-transition probability, but instead each state has its own duration distribution, which determines for how many trials it remains active.</p></caption><graphic xlink:href="EMS193121-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><p>Dynamic infinite hidden Markov model (diHMM) fit to mouse KS014. The topmost row shows the overall performance during the session, as percent correct, and the current stage of learning as the background colour (we elaborate on learning stages later in the text). Vertical lines with shaded circles at the top indicate the sessions during which new contrasts were introduced. The remaining rows show the prevalent behavioural states (label to the right) ordered by appearance, indicating which percentage of trials they explained during each session. To the far right of every state we show its psychometric functions (PMFs) across time, ignoring the contribution from the history of previous choices. The saturation of the colours of the states indicates successive appearance, and match the PMF plots.</p></caption><graphic xlink:href="EMS193121-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><p>Excerpt of state assignments in session #12 from the mouse of <xref ref-type="fig" rid="F2">Fig. 2</xref>. The left y-axis serves as a scale for how connected a trial is to the other trials of that state (see Methods for details). The right y-axis shows the contrast. The dot colour indicates the animal’s response. One can see how the drastic and sudden change in the response patterns, leftwards (blue) answers for rightwards (positive) contrasts, from trial <italic>∼</italic>330 to <italic>∼</italic>380 is detected by the model with a state transition. The PMFs of state 5 and 6 looked similar, but did in fact represent significantly higher rates of errors on the right and left side respectively, we highlighted these mistakes with arrows.</p></caption><graphic xlink:href="EMS193121-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><p>Summary of the PMFs associated with the different types, for this we use the first PMF of every state in every animal. Each subplot shows a specific type: (a) type 1 in green; (b-d) type 2 in blue, further split by whether the PMF is left-biased (b), right-biased (c), or symmetric (d); and (e) type 3 in red. The thick lines indicate the overall mean over PMFs of the type, the shaded regions show the range in which 95% of the PMFs fell (computed separately for each contrast level), and the thin lines show samples of individual PMFs of these groups. Text in the bottom right indicates how many PMFs of this type were present across the entire population.</p></caption><graphic xlink:href="EMS193121-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><p>Proportions of sessions it took each mouse to reach the next major step in training, as defined by the 3 stages, scattered as circles on a simplex (the larger the proportion of sessions within a specific stage, the closer the dot for that animal is to that corner of the simplex). Simplex corners are identified by example PMFs from the type of that stage. Marker area indicates the total number of sessions (min=5, max=80). The magenta star marks the average proportion, the size of the star indicates the mean number of sessions (which was 24.2). The histogram shows the overall distribution over the number of training sessions.</p></caption><graphic xlink:href="EMS193121-f005"/></fig><fig id="F6" position="float"><label>Fig. 6</label><caption><p>Evolution of the weights of states on average, through slow and sudden changes. Subplots titled by a type represent the weight changes from the first appearance of a state of this type to its last, so only state-internal slow changes (we only considered states which appear on at least 5 sessions, but not more than 15, as both extremes would skew these averages, since states might have changed their type in this time). Subplots with a title indicating a transition from one type to another show how much each weight of the new state differed from the weights of the closest previously existing state, and is based exclusively on the states which first brought the mouse into a new stage (i.e. for ”Type 1 → 2”, we only took into account the first type 2 state exhibited by the mouse, and only when that state is type 2 from its inception). Coloured stars on the leftmost and rightmost plot indicate the average value of the weights of only the very first states of each mouse and the weights of the dominant state on the last session, respectively. We split the bias weights into left-biased and right-biased, as they would otherwise cancel out. Whereas contrast sensitivities increased both through fast and slow changes, it is noticeable that biases stayed almost constant throughout the lifetime of a state on average, but changed more noticeably through sudden transitions.</p></caption><graphic xlink:href="EMS193121-f006"/></fig><fig id="F7" position="float"><label>Fig. 7</label><caption><p>Histograms of all state introductions, excluding the first state of every animal, across all of training (left) and within sessions (right). We colour by state type and normalise the entire length of training of an animal, and all individual sessions, onto the range between 0 and 1 for comparison purposes. The inset on the right plot shows the bar of the first time bin uninterrupted.</p></caption><graphic xlink:href="EMS193121-f007"/></fig><fig id="F8" position="float"><label>Fig. 8</label><caption><p>Scatterplot of the total number of sessions of an animal against the number of sessions in which a state with a worse PMF than the currently best available one explained the majority of trials. We add an offset to points coinciding at the same spot for visibility. While there is considerable variability for any given number of regressed sessions, there is a clear positive correlation (Pearson’s r=0.78, p&lt; 1 <italic>×</italic> 10<sup><italic>−</italic>24</sup>).</p></caption><graphic xlink:href="EMS193121-f008"/></fig><fig id="F9" position="float"><label>Fig. 9</label><caption><title>Visualisation of the different variables across training.</title></caption><graphic xlink:href="EMS193121-f009"/></fig><fig id="F10" position="float"><label>Fig. 10</label><caption><p>Individual MCMC-samples can be scattered in 2D principal component (PC) space, to find regions of high probability. To make those regions more salient, we colour individual samples according to a Gaussian density estimation (the density estimation occurs in 3D PC space). We can see, that there are multiple modes of varying importance, with one mode being particularly dominant.</p></caption><graphic xlink:href="EMS193121-f010"/></fig><fig id="F11" position="float"><label>Fig. 11</label><caption><p>Consistency matrix <italic>C</italic><sup><italic>η</italic></sup> of the animal seen in <xref ref-type="fig" rid="F2">Fig. 2</xref>, with different state assignments, based on cutting the hierarchical clustering tree at different levels, as noted above the colouring assignment. The ticks on the left mark session boundaries, with the numbers indicating the total number of trials so far. State colour on the bars to the right was determined based on the ranking of how many trials are assigned to the state, therefore a colour change need not be a major change in state assignments. As can be seen, state assignments are robust in a large range of cutoff values, with large states staying particularly consistent. Most of the change comes from the splitting of smaller states, and some trials losing a state assignment altogether (a state needed at least 40 trials to be coloured; if the state of a trial had less than that, we colour it white).</p></caption><graphic xlink:href="EMS193121-f011"/></fig><fig id="F12" position="float"><label>Fig. 12</label><caption><p>Histogram of the mean reward rates on easy trials of the PMFs of all states, at the moment they first appeared. Vertical lines indicate the boundaries we used to classify states into the three types. The boundaries we drew do align with points of low density in the histogram.</p></caption><graphic xlink:href="EMS193121-f012"/></fig><fig id="F13" position="float"><label>Fig. 13</label><caption><p>Model recovery where behaviour was generated using only one state, with a changing PMF. The red and black outlines indicate ground truth, which was almost perfectly recovered. For the changing PMF, we only show the first and last ground truth (the initial PMFs are not incorrectly recovered for low contrasts, but are just not learnable due to the limited contrast set during this period)</p></caption><graphic xlink:href="EMS193121-f013"/></fig><fig id="F14" position="float"><label>Fig. 14</label><caption><p>Model recovery where behaviour was generated using 9 different states, on a large number of sessions. The red and black outlines indicate ground truth, which is recovered close to perfectly, in particular correctly recovering the number of states. On session 9, state 2 is incorrectly split between state 1 and 4, the only major flaw.</p></caption><graphic xlink:href="EMS193121-f014"/></fig></floats-group></article>