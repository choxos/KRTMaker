<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS188721</article-id><article-id pub-id-type="doi">10.1101/2023.09.27.559716</article-id><article-id pub-id-type="archive">PPR731683</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Vision plays a calibrating role in discriminating threat-related vocal emotions</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Falagiarda</surname><given-names>Federica</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Occelli</surname><given-names>Valeria</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Collignon</surname><given-names>Olivier</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Institute for Research in Psychology (IPSY), Institute of Neuroscience (IoNS), Université catholique de Louvain (UcL), Belgium</aff><aff id="A2"><label>2</label>Department of Psychology, Edge Hill University, United Kingdom</aff><aff id="A3"><label>3</label>Center of Mind/Brain Sciences, University of Trento, Italy</aff><aff id="A4"><label>4</label>HES-SO Valais-Walis, The Sense Innovation and Research Center, Lausanne and Sion, Switzerland</aff><author-notes><corresp id="CR1">Corresponding authors: Prof. Olivier Collignon, Institute for Research in Psychology (IPSY), Institute of Neuroscience (IoNS), University of Louvain, Phone: +32 10474475, <email>olivier.collignon@uclouvain.be</email>., Dr. Valeria Occelli, Department of Psychology, Edge Hill University, Phone: +44 (0) 1695 592676, <email>occelliv@edgehill.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>29</day><month>09</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>09</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">The ability to reliably discriminate vocal expressions of emotion is crucial to engage in successful social interactions. This process is arguably more crucial for blind individuals, since they cannot extract social information from faces and bodies, and therefore chiefly rely on voices to infer the emotional state of their interlocutors. Blind have demonstrated superior abilities in several aspects of auditory perception, but research on their ability to discriminate vocal features is still scarce and has provided unclear results. Here, we used a gating psychophysical paradigm to test whether early blind people would differ from individually matched sighted controls at the recognition of emotional expressions. Surprisingly, blind people showed lower performance than controls in discriminating specific vocal emotions. We presented segments of non-linguistic emotional vocalizations of increasing duration (100 to 400ms), portraying five basic emotions (fear, happy, sad, disgust, angry), and we asked our participants for an explicit emotion categorization task. We then calculated sensitivity indices and confusion patterns of their performance. We observed better performance of the sighted group in the discrimination of angry and fearful expression, with no between-group differences for other emotions. This result supports the view that vision plays a calibrating role for specific threat-related emotions specifically.</p></abstract><kwd-group><kwd>blindness</kwd><kwd>emotion</kwd><kwd>threat</kwd><kwd>voice</kwd><kwd>auditory</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Achieving successful interactions with our conspecifics is a crucial skill to possess for humans and other social animals (<xref ref-type="bibr" rid="R21">Darwin, 1872</xref>). Such success is notably mediated by the efficient interpretation of a variety of sensory signals. In particular, facial, bodily and vocal signals convey a wealth of information about our interlocutors, with such information often being redundant across the senses. For example, identity, affect, age, gender, trustworthiness or speech can all be inferred through faces as well as through voices (<xref ref-type="bibr" rid="R44">Lange et al., 2022</xref>; <xref ref-type="bibr" rid="R61">Perrodin et al., 2015</xref>; <xref ref-type="bibr" rid="R75">Young et al. 2020</xref>). Ample evidence exists that neurotypical individuals rely more heavily on one sense compared to the others when performing specific inference on social information, with vision being the sense that is considered the most reliable for a large variety of perceptual tasks like the discrimination of age, emotion and identity (<xref ref-type="bibr" rid="R2">Amilon et al., 2007</xref>; <xref ref-type="bibr" rid="R16">Collignon et al., 2008</xref>; <xref ref-type="bibr" rid="R31">Hanley et al., 1998</xref>).</p><p id="P3">Despite its inherent multisensory nature, the perception of emotion expression is indeed known to be visually dominant. Research has shown that when differentiating between different basic emotions, visual cues override auditory cues across a wide range of emotions. Moreover, during incongruent audio-visual stimulation, observers tend to rely more on the emotions displayed through facial expressions rather than the voice when making decisions (<xref ref-type="bibr" rid="R16">Collignon et al., 2008</xref>). Additionally, when both facial and vocal expressions are presented simultaneously, participants’ errors are predominantly influenced by visual errors rather than auditory errors (<xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>). These findings highlight the increased reliance on vision, even in situations where multiple senses are expected to provide optimal information. This stronger reliance on vision rather than audition is thought to relate to the enhanced salience of emotional features delivered through faces rather than voices (<xref ref-type="bibr" rid="R16">Collignon et al., 2008</xref>; <xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>).</p><p id="P4">When vision has not been available throughout someone’s life, as it is the case for congenitally blind individuals, the reliance on vocal signals for most socially relevant information is arguably higher. Research on the perceptual consequence of early visual deprivation on the remaining senses has delivered multifaceted outcomes. Some research has shown that, because of the sensory deprivation, blind people perform better than sighted at a variety of non-visual tasks, such as sound pitch discrimination (<xref ref-type="bibr" rid="R29">Gougoux et al., 2004</xref>), sound localization (<xref ref-type="bibr" rid="R5">Battal et al., 2020</xref>) and fast speech discrimination (<xref ref-type="bibr" rid="R57">Moos &amp; Trouvain, 2007</xref>). Such better performance in sensory deprived populations, at tasks carried out in their non-deprived senses, are often explained through sensory compensation, the idea that a sensory deprived individual must compensate for the lack of input in one sense by enhancing the computational capabilities of the remaining senses (<xref ref-type="bibr" rid="R6">Bavelier et al., 2006</xref>; <xref ref-type="bibr" rid="R17">Collignon et al., 2009</xref>). In contrast, deprived people have been shown to exhibit impairments in other perceptual tasks. For example, blind individuals show worse performance compared to sighted in auditory bisection (e.g. <xref ref-type="bibr" rid="R28">Gori et al., 2014</xref>). Such impairment is sometimes explained within the frame of sensory calibration, or rather lack thereof (<xref ref-type="bibr" rid="R71">Tonelli et al., 2015</xref>). A sensory calibration hypothesis posits that the dominant modality in a specific cognitive or sensory task is necessary during development to calibrate the other senses for that task. As a consequence, someone who lacked sensory calibration would perform more poorly, even when said task is performed unimodally in the non-dominant modality.</p><p id="P5">Interestingly, the sensory compensation and the sensory calibration hypotheses produce opposite predictions when looking at vocal discrimination of emotion in blind participants. Since emotion discrimination seems to be visually dominant, the lack of visual calibration might impair the optimal development of the categorization of vocal emotion expressions. On the other hand, lack of vision might be compensated by higher efficiency at extracting vocal information for successful social interactions, thus resulting in better performance. Research that has evaluated performance of the blind population on voice perception in general, and vocal expression of emotion in particular, remains limited and have produced conflicting results. For example, early blind people show better voice recognition abilities and learn new voice-name associations faster than sighted individuals (<xref ref-type="bibr" rid="R10">Bull et al., 1983</xref>; <xref ref-type="bibr" rid="R35">Hölig et al., 2014</xref>), while no performance difference was found between blind and sighted when asked to evaluate the body size of a speaker from their voice (<xref ref-type="bibr" rid="R62">Pisanski et al., 2016</xref>).</p><p id="P6">When it comes to emotional signals, developmental studies have shown that blind children and adolescents are worse and slower than the sighted when discriminating prosodies of emotional sentences (<xref ref-type="bibr" rid="R14">Chen et al., 2021</xref>) and have difficulties at recognizing vocal expressions of emotions (<xref ref-type="bibr" rid="R9">Blau, 1964</xref>; <xref ref-type="bibr" rid="R22">Dyck et al., 2004</xref>; <xref ref-type="bibr" rid="R55">Minter et al., 1991</xref>). Furthermore, blind children face challenges in emotion regulation (<xref ref-type="bibr" rid="R12">Chennaz et al. 2022</xref>) and have difficulties expressing emotions and reading other people’s mind (<xref ref-type="bibr" rid="R66">Roch-Levecq, 2006</xref>). In adults, <xref ref-type="bibr" rid="R27">Gamond et al. (2017)</xref> have tested the ability to detect a target emotion in vocalizations through a dichotic listening task, and found no accuracy difference between blind and sighted, while blind people were overall slower to respond. Martin and colleagues (2019) showed that participants with typical vision showed a greater advantage compared to blind participants when the emotional prosody used in a sentence matched its semantic content. In contrast, <xref ref-type="bibr" rid="R39">Klinge et al. (2010)</xref> found that the blind people were instead faster at responding to negative emotions in a discrimination task performed during fMRI scanning. The different results are likely due to differences in specific tasks employed, which might have not been appropriate to unveil group differences, or to the insufficient sensitivity of the experimental designs, which led to performance close to ceiling. Overall, these results depict an unclear picture, thus leaving the issue of better or worse performance of blind individuals in discrimination of voices, unresolved.</p><p id="P7">In the present study, we tested the auditory discrimination of a group of early blind individuals and a group of individually matched sighted individuals while categorizing vocal expression of a wide range of emotions. We considered our previously employed gating paradigm (<xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>), an optimal way to test group differences, since the segmentation of the stimuli in different lengths creates a challenging task that has a higher chance of revealing subtle intergroup effects (<xref ref-type="bibr" rid="R19">Creupelandt et al., 2020</xref>). This paradigm assumes that when a perceiver performs the extraction of emotional information from voices, discrete stored properties of each emotion matching the incoming expression are rapidly and partially activated, similarly to the activation of the “word initial cohort” when hearing incoming speech (Marslen-Wilson, 1987). Our goal was to test whether early blind differs in the way they accumulate informational evidence at different time points during the unfolding of vocal emotional signals. In addition, we investigated whether discrimination follows a similar confusion pattern across our emotional set between the two groups.</p></sec><sec id="S2" sec-type="methods"><title>Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P8">The current study includes 16 early blind (EB) individuals and 16 sighted controls (SC) matched in age (±5 years) and sex (8 male subjects per group, mean age EB group = 34.7 years, mean age SC group = 34.1, no significant difference in groups’ ages: t = 0.19, p = 0.85), selected via opportunity sampling. <xref ref-type="table" rid="T1">Table 1</xref> shows the information regarding the cause and age onset of blindness for the blind group. All participants had self-reported normal audition. The experimental protocol was approved by the institutional review boards of the University of Trento, Italy, and the Center for Mind/Brain Sciences (CIMeC). All participants provided a written informed consent prior to the experiment and received a monetary reimbursement for their participation.</p></sec><sec id="S4"><title>Stimuli and design</title><p id="P9">The employed stimuli (available at <ext-link ext-link-type="uri" xlink:href="http://osf.io/yedsz">http://osf.io/yedsz</ext-link>) consist of a set of non-linguistic vocal emotional expressions (i.e., vocalizations) representing the five basic emotions of anger, disgust, fear, joy and sadness, uttered by four different individuals (2 males). These exact stimuli have been used as auditory stimulation in our previous multisensory study conducted on the neurotypical population (see <xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>; for a more detailed explanation of the stimuli selection process). We used a behavioral <italic>gating</italic> task (<xref ref-type="bibr" rid="R30">Grosjean, 1980</xref>) to present acoustic segments of incremental length (i.e., gate). The shortest stimulation segment is constituted by a silent audio clip lasting 100 ms. Participants were not informed of the presence of stimuli with no useful information for the purpose of the task, therefore they were asked to always provide an answer, even in the case of very short clips. Progressively longer gates were built through an increment of ~33.37 ms to reach a maximum duration of stimulus presentation of 400 ms (i.e., gate 1: 100ms, gate 2: 133ms, gate 3: 167ms, … gate 10: 400ms).</p></sec><sec id="S5" sec-type="methods"><title>Procedure</title><p id="P10">Participants sat in a dimly lit room, in front of a laptop. Participants were additionally blindfolded prior to entering the room. The stimulation was presented through over-ear headphones (Hiearcool L1) at a comfortable, self-adjusted volume level. The task required from the participants was to identify the expressed emotion by pressing one of five buttons, where each button would correspond to one of the five possible emotion expressions (i.e., “anger”, “disgust”, “fear”, “joy”, “sadness”). At the beginning of the session, the experimenter would place the hands of the participants on the buttons required for the task, and play a pre-recorded file delivering the instructions. All participants would undergo a pre-experimental block aimed at ensuring that they had learned the mapping between each button and its assigned response. They also underwent a short practice block to familiarize with the experimental task.</p><p id="P11">During both pre-practice and practice blocks, participants received auditory feedback on the correctness of their answer, while no further feedback was given for the rest of the experiment. Each trial started with an interstimulus interval (ISI) that varied between 500 and 1000ms, followed by an auditory stimulus, selected randomly. After each stimulus, a tone was presented to signal to the participants that the 4s response time window had started; this was necessary to receive answers in all conditions, including those where no sound had been detected. Each emotion expression was portrayed by four different actors, therefore four trials in each emotion and each gate constituted a full repetition of the stimuli. Each participant was presented with two repetitions of the stimuli, for a total of 400 trials, presented as fully randomized in the two blocks of the experimental session. The experiment was run with Matlab 2013 using the Psychophysics toolbox extension (PTB-3, <xref ref-type="bibr" rid="R38">Kleiner et al., 2007</xref>).</p></sec></sec><sec id="S6"><title>Transparency and openness</title><p id="P12">In compliance with APA endorsed Transparency and Openness Promotion Guidelines, we have reported above how we determined our sample size and data exclusions, and described all manipulations and measures we used in the study. We have shared the links to the stimuli, the data, analysis code, and research materials. Supplemental materials are available online and accessible via the journal website. Data were collected in 2017-18 and analyzed using R, version 4.2.1 (R Core Team, 2022); the packages used have been specified within the text. This study’s design and its analysis were not preregistered.</p></sec><sec id="S7"><title>Analyses and results</title><sec id="S8"><title>Sensitivity indices</title><p id="P13">We first discarded the trials in which the participants did not respond due to time out, or accidentally pressed a different button than the ones assigned for response (1.6% of the total). The rest of the data were used towards the computation of sensitivity indices (or <italic>d’,</italic> d-primes). The sensitivity indices constitute an unbiased measure of performance since they not only consider correct responses, but incorrect responses too (Signal Detection Theory, <xref ref-type="bibr" rid="R50">Macmillan &amp; Creelman, 2004</xref>; <xref ref-type="bibr" rid="R69">Tanner &amp; Swets, 1954</xref>). They are calculated as <italic>d’ = z(Hit rate) - z(False alarm rate).</italic> The d’ were calculated for each subject, in each emotion expression at each gate (see <xref ref-type="fig" rid="F1">Figure 1</xref>) and were subsequently analyzed to address our experimental question.</p><p id="P14">As previously mentioned, the shortest stimuli (gate 1) contained no emotional information, therefore the correspondent d’ values are theoretically meaningless and were removed from the following analyses. Outliers among the sensitivity indices were defined as the d’ values that exceeded 2.5 standard deviations (SD) from the mean of their condition, where one condition is defined by one of the five emotional categories in one gate for each group separately (cf. <xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>). Nineteen values, corresponding to 1.3% of the d’ values, were found to be outlying and subsequently discarded. The remaining sensitivity indices were submitted to a generalized linear mixed model (GLMM) via the “lmerTest” package (<xref ref-type="bibr" rid="R43">Kuznetsova et al., 2017</xref>), with the predictors Group, Emotion and Gates as fixed effects, and Subject as random effect, and with the formula: <italic>d-prime ~ group * emotion * gate +(1/subj)</italic> (marginal R<sup>2</sup> = .46; conditional R<sup>2</sup> = .51). In order to evaluate the global effects of the predictors, an analysis of variance was computed on the model through the “emmeans” package (<xref ref-type="bibr" rid="R47">Lenth, 2019</xref>) and using the Kenward-Rogers degrees of freedom approximation method. The ANOVA revealed a significant main effect of Emotion [F(4,1301.3) = 69.69, p &lt; 0.001] and Gate [F(8, 1301.2) = 98.85, p &lt; 0.001], as well as a significant interaction of Group by Emotion [F(4,1301.3) = 9.98, p &lt; 0.001], Group by Gate [F(8,1301.2) = 2.97, p = 0.003], and Emotion by Gate [F(32,1301.1) = 5.13, p &lt; 0.001]. The main effect of Group was non-significant [F(1, 30.0) = 2.1, p = 0.157], and so was the three-way interaction [F(32,1301) = 0.57, p = 0.97].</p><p id="P15">Post-hoc analyses on the main effect of Gate showed, as expected, a general increase in performance at the task as the stimulus duration increased, until reaching a plateau around gate 8, corresponding to a stimulus duration of 333ms. All tested gates were significantly different from each other, except gates 2 and 3; gate 5 and 6. Gate 7 was not significantly different from gate 6, 8 or 9; gate 8, 9 and 10 were not significantly different from each other (all significant p’s &lt; 0.039; all other p’s &gt; 0.24; all p’s adjusted through Tukey’s method for comparing a family of 9 estimates, see <xref ref-type="fig" rid="F2">Figure 2A</xref>). Post-hoc analyses on the main effect of Emotion showed that anger and fear lead to significantly higher performance in the task than all other three emotions: anger vs disgust, anger vs joy, anger vs sadness, fear vs disgust, fear vs joy, fear vs sadness (p’s &lt; 0.001). The d’ values for anger and fear were not significantly different from each other, and the same was shown for disgust, joy and sadness: anger vs fear, disgust vs joy, disgust vs joy, joy vs disgust (p’s &gt; 0.083; all p’s adjusted through Tukey’s method for comparing a family of 5 estimates, see <xref ref-type="fig" rid="F2">Figure 2B</xref>).</p><p id="P16">Post-hoc analyses on the interaction between Group and Emotion aimed at five comparisons of interest, namely the comparison between the two groups within the same emotion category. Those comparisons revealed to be significant for anger and fear, where the sighted participants’ group exhibited overall higher sensitivity, hence higher performance than the blind group (anger: EB vs SC, p = 0.038, fear: EB vs SC, p = 0.015), but not for the other emotions (disgust, joy and sadness: EB vs SC, p’s &gt; 0.99; all p’s adjusted through Tukey’s method for comparing a family of 10 estimates; see <xref ref-type="fig" rid="F3">Figure 3</xref>.).</p><p id="P17">Post-hoc analyses on the interaction between Gate and Group failed to highlight any significant difference in the performance of the two groups across the gates, all p’s &gt; .05 (see <xref ref-type="fig" rid="F1">Figure 1</xref>). Lastly, the interaction between Gate and Emotion pointed to a different increase in performance at the increased stimulus length in the different emotions, which means that the amount of information at each gate differed across vocal expression of emotions. The thorough interpretation of each potentially significant post-hoc pairwise comparison for these two interactions is beyond the scope of this investigation and will not be discussed further.</p></sec><sec id="S9"><title>Confusion matrices</title><p id="P18">To test whether the behavior of sighted and blind individuals was similar when producing incorrect responses, we made use of confusion matrices. Confusion matrices were calculated for each subject with the responses at each investigated time point separately. In confusion matrices, the diagonal represents the correct responses, while all the data laying off the diagonal represents incorrect responses,or confusion patterns (see <xref ref-type="fig" rid="F2">Figure 2</xref> for the color-coded average matrices of the two groups at each gate).</p><p id="P19">Through confusion matrices, we wanted to test whether the two groups behaved similarly when producing incorrect responses; in particular, we wanted to test whether their incorrect responses would be more similar within one group compared to the other group. A significant difference would hint at a partially different strategy between the groups when completing the task. To address this question, we calculated an intragroup and an intergroup measure of correlation at each gate. The intragroup correlation was computed by calculating Pearson’s correlation coefficient between the confusion patterns (on-diagonal data discarded; see <xref ref-type="bibr" rid="R65">Ritchie et al., 2017</xref>) of each subject with the average confusions of the rest of his/her own group (blind or sighted). The intergroup correlation was instead computed by correlating each subject to the mean of the other group. In order to reveal any difference, we tested the average intragroup vs the average intergroup correlation coefficients (Fisher transformed Pearson’s r). No significant difference was found at any gate (t-tests, all p’s<sub>FDR</sub> &gt; 0.39). Failing to find a significant difference between intragroup and intergroup correlations supports to the idea that the incorrect responses are similar in the two groups.</p><p id="P20">Furthermore, we tested the intergroup correlation against zero. The results showed significant coefficients for gate 1, and gate 4 to 10. Gate 1 and 4 showed a weak correlation coefficient (gate 1: r<sub>FisherZ</sub> = 0.15, t = 2.83, p<sub>FDR</sub> = 0.01; gate 4: r<sub>FisherZ</sub> = 0.16, t = 3.43, p<sub>FDR</sub> = 0.002), while gates 5 to 10 showed moderate to strong degrees of correlation (gate 5: r<sub>FisherZ</sub> = 0.38, t = 6.4, p<sub>FDR</sub> &lt; 0.0001; gate 6: r<sub>FisherZ</sub> = 0.4, t = 7.84, p<sub>FDR</sub> &lt; 0.0001; gate 7: r<sub>FisherZ</sub> = 0.48, t = 11.16, p<sub>FDR</sub> &lt; 0.0001; gate 8 r<sub>FisherZ</sub> = 0.58, t = 10.83, p<sub>FDR</sub> &lt; 0.0001; gate 9: r<sub>FisherZ</sub> = 0.54, t = 12.13, p<sub>FDR</sub> &lt; 0.0001; gate 10: r<sub>FisherZ</sub> = 0.61, t = 12.41, p<sub>FDR</sub> &lt; 0.0001). No significant correlation was found for gate 2 and 3 (gate2: r<sub>FisherZ</sub> = 0.09, t = 1.62, p<sub>FDR</sub> = 0.13; gate 3: r<sub>FisherZ</sub> = 0.09, t = 1.34, p<sub>FDR</sub> = 0.19). These results showed an increasing
y higher similarity between the incorrect responses (the confusions) of the two groups for stimuli of 200ms and longer (gate 4 and above), as well as a small degree of similarity of the confusions for the shortest stimuli (gate 1). At gate 1, where no emotional information was actually presented, the correlation is probably given by a similar consistency in the responses that subjects adopted. Looking at the color-coded matrices in <xref ref-type="fig" rid="F3">Figure 3</xref>, it seems that, under such high uncertainty context (no actual information is available to complete the task), there might be a potential bias to report disgust. From gate 4 onwards, when, arguably, enough information is delivered to lead to a more consistent emotion categorization process, the similarity in the confusion patterns raised, showing that blind and sighted confused vocal emotions at a similar extent. Based on a mere visual evaluation of the matrices, we might notice that joy seems to be misclassified as sadness in both groups, more pronouncedly around the middle of the signal (see <xref ref-type="fig" rid="F3">Figure 3</xref>). All correlation values are summarized in <xref ref-type="fig" rid="F4">Figure 4</xref>, and their distributions represented in <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>.</p><p id="P21">We also confirmed our results using a non-parametric, measure of sensitivity (A-prime; <xref ref-type="bibr" rid="R76">Zhang and Mueller, 2005</xref>) as supplemental material.</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P22">In the present study, we compared the ability of sighted and blind individuals at categorizing emotions, as perceived through vocal non-linguistic segments of different lengths. The gating paradigm we used relies on extracting emotional information from vocal expressions in conditions where information about emotions is progressively accumulated over time. The challenging condition imposed on the perceptual system avoids any ceiling effects and therefore allows for sensitive measurement of the abilities to efficiently extract emotionally relevant acoustic information. Our results show that sighted individuals outperformed the blind at discriminating anger and fear, while no difference between the groups was found for expressions of disgust, joy or sadness. Furthermore, we observed that, independently of the sensory experience of our participants – blind or sighted – the threat-related auditory expressions of anger and fear were recognized better compared to the other investigated categories (<xref ref-type="fig" rid="F2">Figure 2B</xref>).</p><p id="P23">In past research, threat-related emotions, with a particular attention given to fear, have proved to stand out when compared to other basic emotional categories (<xref ref-type="bibr" rid="R20">Damasio &amp; Carvalho, 2013</xref>). For example, facial (<xref ref-type="bibr" rid="R54">Milders et al., 2006</xref>) or vocal (<xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>) expressions of fear are detected or discriminated better than other basic emotions. Also, fearful faces more easily and preferentially break through conscious perception, whether evaluated through binocular rivalry (<xref ref-type="bibr" rid="R3">Amting et al., 2010</xref>), or continuous flash suppressions (<xref ref-type="bibr" rid="R74">Yang et al., 2007</xref>). Four months old infants as well as adults exhibit an avoidant looking behavior when presented with angry and fearful faces, but not with happy, sad or neutral faces, highlighting how the recognition of threat is present very early in life, and that threat is an attribute allegedly independent from valence (since sadness, like anger and fear, is also negative in valence, but did not trigger avoidance; <xref ref-type="bibr" rid="R36">Hunnius et al., 2011</xref>). These conclusions can be extended to the domain of threat-related vocal signals, as newborns of just a few days of age show higher and differential mismatch responses for fearful and angry vocalizations compared to happy and neutral ones (<xref ref-type="bibr" rid="R15">Cheng et al. 2012</xref>). These results altogether highlight the importance of these threat-related emotions, that arguably resides in their link to fast responses towards survival, a “fight-or-flight” type of responses.</p><p id="P24">Our results suggest a potential calibrating role of visual experience for the optimal development of the perception of threat-related vocal expressions. Why is the discrimination of fear and anger vocal expressions specifically susceptible to calibration by visual experience? As described above, fear and anger expressions show the highest discrimination when compared to other emotions, meaning that they need less presentation time for a correct discrimination to be reached. Interestingly, this is also true for facial expressions of emotions (<xref ref-type="bibr" rid="R24">Falagiarda &amp; Collignon, 2019</xref>). This might relate to the importance of quickly and efficiently recognize these two emotions for survival in humans and other animals, with evolution creating pressure for their visual expression being very salient (<xref ref-type="bibr" rid="R20">Damasio &amp; Carvalho, 2013</xref>). One might therefore speculate that multisensory perceptual learning and calibration processes are particularly important for those emotion. Indeed, perceptual learning studies have shown that audiovisual training reflects in a more refined discriminating of unimodal auditory stimuli (<xref ref-type="bibr" rid="R48">Lidestam et al., 2014</xref>; <xref ref-type="bibr" rid="R56">Moradi et al., 2019</xref>). It might therefore be that the optimal development of the discrimination of anger and fear through voices more specifically depend on their coupling with facial expression through development (<xref ref-type="bibr" rid="R75">Young et al., 2020</xref>), something that is absent in early blind people. Throughout their lives, sighted people are massively exposed to sensory stimulation being simultaneously delivered via visual and auditory channels (<xref ref-type="bibr" rid="R37">Klasen et al., 2012</xref>). Being able to pool together facial and vocal cues and combining them with contextual information is of pivotal importance in emotion interpretation, due to the highly changeable and ambiguous nature of interpersonal interactions (<xref ref-type="bibr" rid="R70">Theurel et al., 2016</xref>; <xref ref-type="bibr" rid="R75">Young et al., 2020</xref>). Unlike the sighted, early blind people cannot rely on extensive training at integrating these different sources of information, resulting in a detrimental impact on their capability to interpret emotional states from vocal inputs (<xref ref-type="bibr" rid="R55">Minter et al., 1991</xref>; <xref ref-type="bibr" rid="R72">Valente et al., 2018</xref>). A further avenue to gather evidence on the effect of visual calibration could be the investigation of late onset blindness: late blind individuals have lost the sense of sight later in life, implying that vision was present at the critical period needed for calibration. A sensory calibration hypothesis would therefore not predict an impairment at a vocal discrimination task in late blind compared to early blind subjects (cf. <xref ref-type="bibr" rid="R14">Chen et al., 2021</xref>).</p><p id="P25">Another, not mutually exclusive, speculation to explain the specific impairment in the discrimination of anger and fear expressions in blind people may relate to a lower exposure to threat-related situations in blind people. Modern societies are almost never designed for the comfortable fruition of people with disabilities and impairment. We could speculate that blind individuals grew up in a more “protected” environment than their non-visually impaired peers. They have therefore been potentially less exposed to threatening situations, and the people around them might have been less inclined to express anger or fear in their presence. If this were true, they might have overall lower expertise with rageous and frightened vocal utterances, resulting in lower discriminative abilities. A further account worth considering here relates to the concept of embodiment of emotion, according to which perceiving and thinking about emotion automatically activate motoric re-experience of emotion in one’s self (<xref ref-type="bibr" rid="R32">Hari &amp; Kujala, 2009</xref>; <xref ref-type="bibr" rid="R41">Kragel &amp; LaBar, 2016</xref>; <xref ref-type="bibr" rid="R58">Niedenthal, 2007</xref>). For instance, the presentation of facial expressions of emotion triggers the motor recreation of the perceived facial expression in the observer’s own face, known as facial mimicry (<xref ref-type="bibr" rid="R59">Niedenthal et al., 2010</xref>). The embodied processing of facial expression contributes to the recognition and understanding of emotions, thus playing a role as social regulation (Beffarra et al., 2012; Hess &amp; Fisher, 2013). Interestingly, embodied responses have been observed for auditory stimulation (<xref ref-type="bibr" rid="R25">Fino et al., 2016</xref>; <xref ref-type="bibr" rid="R40">Korb et al., 2015</xref>), including nonverbal emotion vocalizations (<xref ref-type="bibr" rid="R33">Hawk et al., 2012</xref>). One might argue whether similar effects could be observed in early blind, a population who have not been exposed to facial expressions of emotion. Could facial mimicry be observed in people lacking any modelling experience in how to express emotions via facial cues? Studies on this topic suggest that, although visual learning does not seem to be a prerequisite to exhibit patterns of facial expressions, some variations in the control and intensity of emotions have been noticed in the blind population (see <xref ref-type="bibr" rid="R72">Valente et al., 2018</xref> for a review). In particular, blind individuals conform less to the display rules dictating what emotions and levels of intensity is socially acceptable to show in different social contexts (<xref ref-type="bibr" rid="R26">Galati et al., 2003</xref>; <xref ref-type="bibr" rid="R42">Kunz et al., 2012</xref>). Thus, the possibility holds that blind individuals, not having been exposed to visual expressions of emotion during their life, might be impaired in their capability to perform motor simulations of such expressions, mimic such expressions of emotion (although see <xref ref-type="bibr" rid="R4">Arias et al., 2021</xref>), which, in turn, can reflect in poorer vocal emotion discrimination in this population.</p><p id="P26">One might argue whether the different emotion socialization process blind and sighted individuals have experienced during their development might have resulted in the use of distinct strategies by the two groups while performing our task. To answer this question, we performed a closer inspection of their incorrect responses, or confusions, to highlight potential emerging patterns. We represented their errors as confusion matrices and computed intergroup correlations for each of the ten stimulus durations of the design. This analysis revealed increasingly higher degrees of correlation for stimuli starting at 200ms and above (gates 4-10; see <xref ref-type="fig" rid="F4">Figure 4</xref>). This indicates that the two groups confuse the same emotions when responding incorrectly. The idea that similar discriminative strategies are employed by the two groups is supported also by the comparison of the intergroup correlations with the intragroup correlations, which did not show any differences at all gates, further supporting the idea that the incorrect performances of the two groups are highly aligned, and no group-specific sub-patterns of responses seem to exist. Based on these results, we speculate that blind and the sighted people engage similar perceptual processes to classify emotions. If the responses in uncertain circumstances, that later lead to incorrect discriminatory decisions, are driven by similar factors in the blind and sighted groups, we can argue that these factors should not include “visual” strategies (e.g. visual imagery), since the blind would not be able to implement them. We would therefore suggest, firstly, that a certain amount of information needs to unfold in time before perceptual elements get extracted in a consistent manner across individuals (in the present design, 200ms of stimulation), and secondly, that these perceptual elements used towards a discriminatory decision are acoustic features of the vocal emotional expressions. Whether the results we observed are specific to the emotional domain, or they rather reflect the way blind people base their decisions on accumulating auditory information in general, is beyond the scope of the present paper and could be matter of future investigation. Along the same line, in accordance with recent evidence showing that 24 distinct emotional states can be extracted from non-linguistic vocalizations (<xref ref-type="bibr" rid="R18">Cowen et al., 2019</xref>), it would be interesting to test more fine-grained emotional states in addition to the primary emotions tested here.</p><p id="P27">In conclusion, we investigated the abilities of early blind and sighted individuals at a vocal emotion discrimination task based on stimuli of incremental length. An overall higher sensitivity for expressions of anger and fear was found when compared to other emotions. Additionally, sighted participants performed better than blind at discriminating those same two threat-related emotional categories, while no difference was found for expressions of disgust, joy or sadness. These results show firstly how angry and fearful expressions set themselves aside from other basic emotions, extending results in vision (<xref ref-type="bibr" rid="R51">Marsh et al., 2005a</xref>, <xref ref-type="bibr" rid="R52">2005b</xref>; <xref ref-type="bibr" rid="R63">Pichon et al., 2009</xref>; <xref ref-type="bibr" rid="R67">Sacco &amp; Hugenberg, 2009</xref>) in the less investigated auditory domain (but also see <xref ref-type="bibr" rid="R23">Erlich et al., 2013</xref>; <xref ref-type="bibr" rid="R68">Scott et al., 1997</xref>). Secondly, these results suggest that facial inputs are required for the fine tuning of discrimination of threat-related expressions. Lastly, we observed similar pattern of confusions across emotions in the two groups once enough signal has unfolded over time, suggesting the adoption of similar and allegedly non-visually based strategies while performing the task. Even if speculative, we suggested mechanistic explanations as to why the discrimination of vocalization of fear and anger suffers from a lack of visual experience.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplemental Material</label><media xlink:href="EMS188721-supplement-Supplemental_Material.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d36aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S11"><title>Acknowledgements</title><p>This work was supported by a European Research Council starting grant (MADVIS grant #337573) attributed to Olivier Collignon and by a European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement (#701250) awarded to Valeria Occelli. Federica Falagiarda is a research fellow and Olivier Collignon a research associate at the National Fund for Scientific Research of Belgium (FRS-FNRS).</p></ack><sec id="S12" sec-type="data-availability"><title>Openly available data</title><p id="P28">The data supporting the findings reported in this paper as well as the scripts used in the analyses are openly available from the Open Science Framework repository at the link: <ext-link ext-link-type="uri" xlink:href="https://osf.io/4zksp/">https://osf.io/4zksp/</ext-link>.</p></sec><fn-group><fn id="FN1"><p id="P29"><bold>CRediT author statement</bold></p><p id="P30">Federica Falagiarda: Conceptualization, Data curation, Methodology, Software, Formal Analysis, Investigation, Writing- Original Draft, Visualization</p><p id="P31">Olivier Collignon: Conceptualization, Methodology, Writing- Review and Editing, Supervision, Project Administration, Funding Acquisition</p><p id="P32">Valeria Occelli: Investigation, Data curation, Writing - Review &amp; Editing, Funding acquisition</p></fn></fn-group><ref-list><title>Bibliography</title><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><article-title>The Ventriloquist Effect Results from Near-Optimal Bimodal Integration</article-title><source>Current Biology</source><year>2004</year><volume>14</volume><issue>3</issue><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">14761661</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amilon</surname><given-names>K</given-names></name><name><surname>van de Weijer</surname><given-names>J</given-names></name><name><surname>Schötz</surname><given-names>S</given-names></name></person-group><source>The impact of visual and auditory cues in age estimation</source><publisher-name>Berlin Springer</publisher-name><year>2007</year><volume>4441</volume></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amting</surname><given-names>JM</given-names></name><name><surname>Greening</surname><given-names>SG</given-names></name><name><surname>Mitchell</surname><given-names>DGV</given-names></name></person-group><article-title>Multiple Mechanisms of Consciousness: The Neural Correlates of Emotional Awareness</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><issue>30</issue><fpage>10039</fpage><lpage>10047</lpage><pub-id pub-id-type="pmcid">PMC6633384</pub-id><pub-id pub-id-type="pmid">20668188</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6434-09.2010</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arias</surname><given-names>P</given-names></name><name><surname>Bellmann</surname><given-names>C</given-names></name><name><surname>Aucouturier</surname><given-names>JJ</given-names></name></person-group><article-title>Facial mimicry in the congenitally blind</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>19</issue><fpage>R1112</fpage><lpage>R1114</lpage><pub-id pub-id-type="pmid">34637708</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battal</surname><given-names>C</given-names></name><name><surname>Occelli</surname><given-names>V</given-names></name><name><surname>Bertonati</surname><given-names>G</given-names></name><name><surname>Falagiarda</surname><given-names>F</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name></person-group><article-title>General Enhancement of Spatial Hearing in Congenitally Blind People</article-title><source>Psychological Science</source><year>2020</year><volume>31</volume><issue>9</issue><fpage>1129</fpage><lpage>1139</lpage><pub-id pub-id-type="pmid">32846109</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bavelier</surname><given-names>D</given-names></name><name><surname>Dye</surname><given-names>MW</given-names></name><name><surname>Hauser</surname><given-names>PC</given-names></name></person-group><article-title>Do deaf individuals see better?</article-title><source>Trends in Cognitive Sciences</source><year>2006</year><volume>10</volume><issue>11</issue><fpage>512</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC2885708</pub-id><pub-id pub-id-type="pmid">17015029</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2006.09.006</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beffara</surname><given-names>B</given-names></name><name><surname>Ouellet</surname><given-names>M</given-names></name><name><surname>Vermeulen</surname><given-names>N</given-names></name><name><surname>Basu</surname><given-names>A</given-names></name><name><surname>Morisseau</surname><given-names>T</given-names></name><name><surname>Mermillod</surname><given-names>M</given-names></name></person-group><article-title>Enhanced embodied response following ambiguous emotional processing</article-title><source>Cognitive Processing</source><year>2012</year><volume>13</volume><fpage>S103</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">22802035</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>LE</given-names></name><name><surname>Auer</surname><given-names>ETJ</given-names></name><name><surname>Eberhardt</surname><given-names>SP</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name></person-group><article-title>Auditory Perceptual Learning for Speech Perception Can be Enhanced by Audiovisual Training</article-title><source>Frontiers in Neuroscience</source><year>2013</year><volume>7</volume><fpage>34</fpage><pub-id pub-id-type="pmcid">PMC3600826</pub-id><pub-id pub-id-type="pmid">23515520</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00034</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Blau</surname><given-names>S</given-names></name></person-group><chapter-title>An ear for an eye: Sensory compensation and judgements of affect by the blind</chapter-title><person-group person-group-type="editor"><name><surname>Davitz</surname><given-names>JR</given-names></name></person-group><source>The communication of emotional meaning</source><publisher-name>McGraw-Hill</publisher-name><publisher-loc>New York</publisher-loc><year>1964</year><fpage>113</fpage><lpage>127</lpage></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bull</surname><given-names>R</given-names></name><name><surname>Rathborn</surname><given-names>H</given-names></name><name><surname>Clifford</surname><given-names>BR</given-names></name></person-group><article-title>The Voice-Recognition Accuracy of Blind Listeners</article-title><source>Perception</source><year>1983</year><volume>12</volume><issue>2</issue><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="pmid">6657428</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burr</surname><given-names>D</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name><name><surname>Morrone</surname><given-names>MC</given-names></name></person-group><article-title>Auditory dominance over vision in the perception of interval duration</article-title><source>Experimental Brain Research</source><year>2009</year><volume>198</volume><issue>1</issue><fpage>49</fpage><lpage>57</lpage><pub-id pub-id-type="pmid">19597804</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chennaz</surname><given-names>L</given-names></name><name><surname>Valente</surname><given-names>D</given-names></name><name><surname>Baltenneck</surname><given-names>N</given-names></name><name><surname>Baudouin</surname><given-names>JY</given-names></name><name><surname>Gentaz</surname><given-names>E</given-names></name></person-group><article-title>Emotion regulation in blind and visually impaired children aged 3 to 12 years assessed by a parental questionnaire</article-title><source>Acta psychologica</source><year>2022</year><volume>225</volume><elocation-id>103553</elocation-id><pub-id pub-id-type="pmid">35279432</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Charbonneau</surname><given-names>G</given-names></name><name><surname>Veronneau</surname><given-names>M</given-names></name><name><surname>Boudrias-Fournier</surname><given-names>C</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name></person-group><article-title>The ventriloquist in periphery: Impact of eccentricity-related reliability on audio-visual localization</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>12</issue><fpage>20</fpage><pub-id pub-id-type="pmid">24167163</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Lu</surname><given-names>M</given-names></name><name><surname>Yao</surname><given-names>X</given-names></name></person-group><article-title>The recognition of emotional prosody in students with blindness: Effects of early visual experience and age development</article-title><source>British Journal of Developmental Psychology</source><year>2021</year><elocation-id>bjdp.12394</elocation-id><pub-id pub-id-type="pmid">34467548</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>Y</given-names></name><name><surname>Lee</surname><given-names>S-Y</given-names></name><name><surname>Chen</surname><given-names>H-Y</given-names></name><name><surname>Wang</surname><given-names>P-Y</given-names></name><name><surname>Decety</surname><given-names>J</given-names></name></person-group><article-title>Voice and Emotion Processing in the Human Neonatal Brain</article-title><source>Journal of Cognitive Neuroscience</source><year>2012</year><volume>24</volume><issue>6</issue><fpage>1411</fpage><lpage>1419</lpage><pub-id pub-id-type="pmid">22360593</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Girard</surname><given-names>S</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name></person-group><article-title>Audio-visual integration of emotion expression</article-title><source>Brain Research</source><year>2008</year><volume>1242</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="pmid">18495094</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Voss</surname><given-names>P</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name></person-group><article-title>Cross-modal plasticity for the spatial processing of sounds in visually deprived subjects</article-title><source>Experimental Brain Research</source><year>2009</year><volume>192</volume><issue>3</issue><fpage>343</fpage><lpage>358</lpage><pub-id pub-id-type="pmid">18762928</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowen</surname><given-names>AS</given-names></name><name><surname>Elfenbein</surname><given-names>HA</given-names></name><name><surname>Laukka</surname><given-names>P</given-names></name><name><surname>Keltner</surname><given-names>D</given-names></name></person-group><article-title>Mapping 24 emotions conveyed by brief human vocalization</article-title><source>American Psychologist</source><year>2019</year><volume>74</volume><issue>6</issue><fpage>698</fpage><pub-id pub-id-type="pmcid">PMC6586540</pub-id><pub-id pub-id-type="pmid">30570267</pub-id><pub-id pub-id-type="doi">10.1037/amp0000399</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creupelandt</surname><given-names>C</given-names></name><name><surname>D’Hondt</surname><given-names>F</given-names></name><name><surname>de Timary</surname><given-names>P</given-names></name><name><surname>Falagiarda</surname><given-names>F</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Maurage</surname><given-names>P</given-names></name></person-group><article-title>Selective visual and crossmodal impairment in the discrimination of anger and fear expressions in severe alcohol use disorder</article-title><source>Drug and Alcohol Dependence</source><year>2020</year><volume>213</volume><elocation-id>108079</elocation-id><pub-id pub-id-type="pmid">32554170</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Damasio</surname><given-names>A</given-names></name><name><surname>Carvalho</surname><given-names>GB</given-names></name></person-group><article-title>The nature of feelings: Evolutionary and neurobiological origins</article-title><source>Nature Reviews. Neuroscience</source><year>2013</year><volume>14</volume><issue>2</issue><fpage>143</fpage><lpage>152</lpage><pub-id pub-id-type="pmid">23329161</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>CR</given-names></name></person-group><source>The expression of the emotions in man and animals</source><publisher-name>John Murray</publisher-name><publisher-loc>London</publisher-loc><year>1872</year></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyck</surname><given-names>MJ</given-names></name><name><surname>Farrugia</surname><given-names>C</given-names></name><name><surname>Shochet</surname><given-names>IM</given-names></name><name><surname>Holmes-Brown</surname><given-names>M</given-names></name></person-group><article-title>Emotion recognition/understanding ability in hearing or vision-impaired children: Do sounds, sights, or words make the difference?</article-title><source>Journal of Child Psychology and Psychiatry</source><year>2004</year><volume>45</volume><issue>4</issue><fpage>789</fpage><lpage>800</lpage><pub-id pub-id-type="pmid">15056310</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname><given-names>N</given-names></name><name><surname>Lipp</surname><given-names>OV</given-names></name><name><surname>Slaughter</surname><given-names>V</given-names></name></person-group><article-title>Of hissing snakes and angry voices: human infants are differentially responsive to evolutionary fear-relevant sounds</article-title><source>Developmental Science</source><year>2013</year><volume>16</volume><issue>6</issue><fpage>894</fpage><lpage>904</lpage><pub-id pub-id-type="pmid">24118715</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falagiarda</surname><given-names>F</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name></person-group><article-title>Time-resolved discrimination of audio-visual emotion expressions</article-title><source>Cortex</source><year>2019</year><volume>119</volume><fpage>184</fpage><lpage>194</lpage><pub-id pub-id-type="pmid">31151087</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fino</surname><given-names>E</given-names></name><name><surname>Menegatti</surname><given-names>M</given-names></name><name><surname>Avenanti</surname><given-names>A</given-names></name><name><surname>Rubini</surname><given-names>M</given-names></name></person-group><article-title>Enjoying vs. smiling: Facial muscular activation in response to emotional language</article-title><source>Biological Psychology</source><year>2016</year><volume>118</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="pmid">27164178</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Galati</surname><given-names>D</given-names></name><name><surname>Sini</surname><given-names>B</given-names></name><name><surname>Schmidt</surname><given-names>S</given-names></name><name><surname>Tinti</surname><given-names>C</given-names></name></person-group><article-title>Spontaneous facial expressions in congenitally blind and sighted aged 8–11</article-title><source>Journal of Visual Impairment &amp; Blindness</source><year>2003</year><volume>97</volume><fpage>418</fpage><lpage>428</lpage></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gamond</surname><given-names>L</given-names></name><name><surname>Vecchi</surname><given-names>T</given-names></name><name><surname>Ferrari</surname><given-names>C</given-names></name><name><surname>Merabet</surname><given-names>LB</given-names></name><name><surname>Cattaneo</surname><given-names>Z</given-names></name></person-group><article-title>Emotion processing in early blind and sighted individuals</article-title><source>Neuropsychology</source><year>2017</year><volume>31</volume><issue>5</issue><fpage>516</fpage><lpage>524</lpage><pub-id pub-id-type="pmcid">PMC5757241</pub-id><pub-id pub-id-type="pmid">28287776</pub-id><pub-id pub-id-type="doi">10.1037/neu0000360</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gori</surname><given-names>M</given-names></name><name><surname>Sandini</surname><given-names>G</given-names></name><name><surname>Martinoli</surname><given-names>C</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><article-title>Impairment of auditory spatial localization in congenitally blind human subjects</article-title><source>Brain</source><year>2014</year><volume>137</volume><issue>1</issue><fpage>288</fpage><lpage>293</lpage><pub-id pub-id-type="pmcid">PMC3891446</pub-id><pub-id pub-id-type="pmid">24271326</pub-id><pub-id pub-id-type="doi">10.1093/brain/awt311</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gougoux</surname><given-names>F</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name><name><surname>Voss</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name></person-group><article-title>Pitch discrimination in the early blind</article-title><source>Nature</source><year>2004</year><volume>430</volume><issue>6997</issue><fpage>309</fpage><pub-id pub-id-type="pmid">15254527</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grosjean</surname><given-names>F</given-names></name></person-group><article-title>Spoken word recognition processes and the gating paradigm</article-title><source>Perception &amp; Psychophysics</source><year>1980</year><volume>28</volume><issue>4</issue><fpage>267</fpage><lpage>283</lpage><pub-id pub-id-type="pmid">7465310</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanley</surname><given-names>JR</given-names></name><name><surname>Smith</surname><given-names>ST</given-names></name><name><surname>Had</surname><given-names>J</given-names></name></person-group><article-title>I Recognise You but I Can’t Place You: An Investigation of Familiar-only Experiences during Tests of Voice and Face Recognition</article-title><source>The Quarterly Journal Of Experimental Psychology</source><year>1998</year><volume>51A</volume><issue>5</issue><fpage>179</fpage><lpage>195</lpage></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hari</surname><given-names>R</given-names></name><name><surname>Kujala</surname><given-names>MV</given-names></name></person-group><article-title>Brain basis of human social interaction: from concepts to brain imaging</article-title><source>Physiological Reviews</source><year>2009</year><volume>89</volume><fpage>453</fpage><lpage>79</lpage><pub-id pub-id-type="pmid">19342612</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawk</surname><given-names>ST</given-names></name><name><surname>Fischer</surname><given-names>AH</given-names></name><name><surname>Van Kleef</surname><given-names>GA</given-names></name></person-group><article-title>Face the noise: embodied responses to nonverbal vocalizations of discrete emotions</article-title><source>Journal of Personality and Social Psychology</source><year>2012</year><volume>102</volume><fpage>796</fpage><lpage>814</lpage><pub-id pub-id-type="pmid">22059840</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hess</surname><given-names>U</given-names></name><name><surname>Fischer</surname><given-names>A</given-names></name></person-group><article-title>Emotional mimicry as social regulation</article-title><source>Personality and social psychology review: an official journal of the Society for Personality and Social Psychology</source><year>2013</year><volume>17</volume><fpage>1421</fpage><lpage>57</lpage><pub-id pub-id-type="pmid">23348982</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hölig</surname><given-names>C</given-names></name><name><surname>Föcker</surname><given-names>J</given-names></name><name><surname>Best</surname><given-names>A</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><article-title>Brain systems mediating voice identity processing in blind humans: Voice Identity Processing in Blind Humans</article-title><source>Human Brain Mapping</source><year>2014</year><volume>35</volume><issue>9</issue><fpage>4607</fpage><lpage>4619</lpage><pub-id pub-id-type="pmcid">PMC6869241</pub-id><pub-id pub-id-type="pmid">24639401</pub-id><pub-id pub-id-type="doi">10.1002/hbm.22498</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunnius</surname><given-names>S</given-names></name><name><surname>de Wit</surname><given-names>TCJ</given-names></name><name><surname>Vrins</surname><given-names>S</given-names></name><name><surname>von Hofsten</surname><given-names>C</given-names></name></person-group><article-title>Facing threat: Infants’ and adults’ visual scanning of faces with neutral, happy, sad, angry, and fearful emotional expressions</article-title><source>Cognition &amp; Emotion</source><year>2011</year><volume>25</volume><issue>2</issue><fpage>193</fpage><lpage>205</lpage><pub-id pub-id-type="pmid">21432667</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klasen</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>Y-H</given-names></name><name><surname>Mathiak</surname><given-names>K</given-names></name></person-group><article-title>Multisensory emotions: Perception, combination and underlying neural processes</article-title><source>Reviews in the Neurosciences</source><year>2012</year><volume>23</volume><issue>4</issue><fpage>381</fpage><lpage>392</lpage><pub-id pub-id-type="pmid">23089604</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><source>What’s new in Psychtoolbox-3? Perception</source><year>2007</year><volume>36</volume></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinge</surname><given-names>C</given-names></name><name><surname>Röder</surname><given-names>B</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name></person-group><article-title>Increased amygdala activation to emotional auditory stimuli in the blind</article-title><source>Brain</source><year>2010</year><volume>133</volume><issue>6</issue><fpage>1729</fpage><lpage>1736</lpage><pub-id pub-id-type="pmid">20453040</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Korb</surname><given-names>S</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Grandjean</surname><given-names>D</given-names></name></person-group><article-title>Reappraising the voices of wrath</article-title><source>Social Cognitive and Affective Neuroscience</source><year>2015</year><volume>10</volume><fpage>1644</fpage><lpage>1660</lpage><pub-id pub-id-type="pmcid">PMC4666101</pub-id><pub-id pub-id-type="pmid">25964502</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsv051</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kragel</surname><given-names>PA</given-names></name><name><surname>LaBar</surname><given-names>KS</given-names></name></person-group><article-title>Somatosensory Representations Link the Perception of Emotional Expressions and Sensory Experience</article-title><source>eNeuro</source><year>2016</year><volume>3</volume><issue>2</issue><elocation-id>ENEURO.0090-15.2016</elocation-id><pub-id pub-id-type="pmcid">PMC4894916</pub-id><pub-id pub-id-type="pmid">27280154</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0090-15.2016</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kunz</surname><given-names>M</given-names></name><name><surname>Faltermeier</surname><given-names>N</given-names></name><name><surname>Lautenbacher</surname><given-names>S</given-names></name></person-group><article-title>Impact of visual learning on facial expressions of physical distress: A study on voluntary and evoked expressions of pain in congenitally blind and sighted individuals</article-title><source>Biological Psychology</source><year>2012</year><volume>89</volume><issue>2</issue><fpage>467</fpage><lpage>476</lpage><pub-id pub-id-type="pmid">22227356</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuznetsova</surname><given-names>A</given-names></name><name><surname>Brockhoff</surname><given-names>PB</given-names></name><name><surname>Christensen</surname><given-names>RHB</given-names></name></person-group><article-title>LmerTest Package: Tests in Linear Mixed Effects Models</article-title><source>Journal of Statistical Software</source><year>2017</year><volume>82</volume><issue>13</issue><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.18637/jss.v082.i13</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lange</surname><given-names>J</given-names></name><name><surname>Heerdink</surname><given-names>MW</given-names></name><name><surname>van Kleef</surname><given-names>GA</given-names></name></person-group><article-title>Reading emotions, reading people: Emotion perception and inferences drawn from perceived emotions</article-title><source>Current Opinions in Psychology</source><year>2022</year><volume>43</volume><fpage>85</fpage><lpage>90</lpage><pub-id pub-id-type="pmid">34303128</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalonde</surname><given-names>K</given-names></name><name><surname>Werner</surname><given-names>LA</given-names></name></person-group><article-title>Development of the Mechanisms Underlying Audiovisual Speech Perception Benefit</article-title><source>Brain Sciences</source><year>2021</year><volume>11</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC7824772</pub-id><pub-id pub-id-type="pmid">33466253</pub-id><pub-id pub-id-type="doi">10.3390/brainsci11010049</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavan</surname><given-names>N</given-names></name><name><surname>Mileva</surname><given-names>M</given-names></name><name><surname>Burton</surname><given-names>AM</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name><name><surname>McGettigan</surname><given-names>C</given-names></name></person-group><article-title>Trait evaluations of faces and voices: Comparing within- and between-person variability</article-title><source>Journal of Experimental Psychology: General</source><year>2021</year><volume>150</volume><issue>9</issue><fpage>1854</fpage><lpage>1869</lpage><pub-id pub-id-type="pmcid">PMC7612101</pub-id><pub-id pub-id-type="pmid">33734774</pub-id><pub-id pub-id-type="doi">10.1037/xge0001019</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Lenth</surname><given-names>R</given-names></name></person-group><source>Emmeans: Estimated Marginal Means, Aka Least-squares Means. R package version 1.3.4</source><year>2019</year><comment>Retrieved from: <ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=emmeans">https://CRAN.R-project.org/package=emmeans</ext-link></comment></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lidestam</surname><given-names>B</given-names></name><name><surname>Moradi</surname><given-names>S</given-names></name><name><surname>Pettersson</surname><given-names>R</given-names></name><name><surname>Ricklefs</surname><given-names>T</given-names></name></person-group><article-title>Audiovisual training is better than auditory-only training for auditory-only speech-in-noise identification</article-title><source>The Journal of the Acoustical Society of America</source><year>2014</year><volume>136</volume><issue>2</issue><fpage>EL142</fpage><lpage>147</lpage><pub-id pub-id-type="pmid">25096138</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lukas</surname><given-names>S</given-names></name><name><surname>Philipp</surname><given-names>AM</given-names></name><name><surname>Koch</surname><given-names>I</given-names></name></person-group><article-title>Crossmodal attention switching: Auditory dominance in temporal discrimination tasks</article-title><source>Acta Psychologica</source><year>2014</year><volume>153</volume><fpage>139</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">25463554</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macmillan</surname><given-names>NA</given-names></name><name><surname>Creelman</surname><given-names>CD</given-names></name></person-group><source>Signal detection theory: A user’s guide</source><publisher-name>Psycholgical Press</publisher-name><year>2004</year></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marsh</surname><given-names>AA</given-names></name><name><surname>Adams</surname><given-names>RB</given-names><suffix>Jr</suffix></name><name><surname>Kleck</surname><given-names>RE</given-names></name></person-group><article-title>Why do fear and anger look the way they do? Form and social function in facial expressions</article-title><source>Personality and Social Psychology Bulletin</source><year>2005a</year><volume>31</volume><fpage>73</fpage><lpage>86</lpage><pub-id pub-id-type="pmid">15574663</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marsh</surname><given-names>AA</given-names></name><name><surname>Ambady</surname><given-names>N</given-names></name><name><surname>Kleck</surname><given-names>RE</given-names></name></person-group><article-title>The effects of fear and anger facial expressions on approach- and avoidance-related behaviors</article-title><source>Emotion</source><year>2005b</year><volume>5</volume><issue>1</issue><fpage>119</fpage><lpage>24</lpage><pub-id pub-id-type="pmid">15755225</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martins</surname><given-names>AT</given-names></name><name><surname>Faísca</surname><given-names>L</given-names></name><name><surname>Vieira</surname><given-names>H</given-names></name><name><surname>Gonçalves</surname><given-names>G</given-names></name></person-group><article-title>Emotional recognition and empathy both in deaf and blind adults</article-title><source>The Journal of Deaf Studies and Deaf Education</source><year>2019</year><volume>24</volume><issue>2</issue><fpage>119</fpage><lpage>127</lpage><pub-id pub-id-type="pmid">30668877</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milders</surname><given-names>M</given-names></name><name><surname>Sahraie</surname><given-names>A</given-names></name><name><surname>Logan</surname><given-names>S</given-names></name><name><surname>Donnellon</surname><given-names>N</given-names></name></person-group><article-title>Awareness of faces is modulated by their emotional meaning</article-title><source>Emotion</source><year>2006</year><volume>6</volume><issue>1</issue><fpage>10</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">16637746</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minter</surname><given-names>ME</given-names></name><name><surname>Hobson</surname><given-names>RP</given-names></name><name><surname>Pring</surname><given-names>L</given-names></name></person-group><article-title>Recognition of vocally expressed emotion by congenitally blind children</article-title><source>Journal of Visual Impairment &amp; Blindness</source><year>1991</year><volume>85</volume><issue>10</issue><fpage>411</fpage><lpage>415</lpage></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moradi</surname><given-names>S</given-names></name><name><surname>Lidestam</surname><given-names>B</given-names></name><name><surname>Ning Ng</surname><given-names>EH</given-names></name><name><surname>Danielsson</surname><given-names>H</given-names></name><name><surname>Rönnberg</surname><given-names>J</given-names></name></person-group><article-title>Perceptual Doping: An Audiovisual Facilitation Effect on Auditory Speech Processing, From Phonetic Feature Extraction to Sentence Identification in Noise</article-title><source>Ear and Hearing</source><year>2019</year><volume>40</volume><issue>2</issue><fpage>312</fpage><lpage>327</lpage><pub-id pub-id-type="pmcid">PMC6400397</pub-id><pub-id pub-id-type="pmid">29870521</pub-id><pub-id pub-id-type="doi">10.1097/AUD.0000000000000616</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moos</surname><given-names>A</given-names></name><name><surname>Trouvain</surname><given-names>J</given-names></name></person-group><source>Comprehension of ultra-fast speech-blind vs.” normally hearing” persons</source><conf-name>Proceedings of the 16th International Congress of Phonetic Sciences</conf-name><year>2007</year></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niedenthal</surname><given-names>PM</given-names></name></person-group><article-title>Embodying emotion</article-title><source>Science</source><year>2007</year><volume>316</volume><fpage>1002</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">17510358</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niedenthal</surname><given-names>PM</given-names></name><name><surname>Mermillod</surname><given-names>M</given-names></name><name><surname>Maringer</surname><given-names>M</given-names></name><name><surname>Hess</surname><given-names>U</given-names></name></person-group><article-title>The Simulation of Smiles (SIMS) model: Embodied simulation and the meaning of facial expression</article-title><source>The Behavioral and Brain Sciences</source><year>2010</year><volume>33</volume><issue>6</issue><fpage>417</fpage><lpage>433</lpage><comment>discussion 433-80</comment><pub-id pub-id-type="pmid">21211115</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ortega</surname><given-names>L</given-names></name><name><surname>Guzman-Martinez</surname><given-names>E</given-names></name><name><surname>Grabowecky</surname><given-names>M</given-names></name><name><surname>Suzuki</surname><given-names>S</given-names></name></person-group><article-title>Audition dominates vision in duration perception irrespective of salience, attention, and temporal discriminability</article-title><source>Attention, Perception, &amp; Psychophysics</source><year>2014</year><volume>76</volume><issue>5</issue><fpage>1485</fpage><lpage>1502</lpage><pub-id pub-id-type="pmcid">PMC4096074</pub-id><pub-id pub-id-type="pmid">24806403</pub-id><pub-id pub-id-type="doi">10.3758/s13414-014-0663-x</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Abel</surname><given-names>TJ</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name></person-group><article-title>Who is That? Brain Networks and Mechanisms for Identifying Individuals</article-title><source>Trends in Cognitive Sciences</source><year>2015</year><volume>19</volume><issue>12</issue><fpage>783</fpage><lpage>796</lpage><pub-id pub-id-type="pmcid">PMC4673906</pub-id><pub-id pub-id-type="pmid">26454482</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2015.09.002</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pisanski</surname><given-names>K</given-names></name><name><surname>Oleszkiewicz</surname><given-names>A</given-names></name><name><surname>Sorokowska</surname><given-names>A</given-names></name></person-group><article-title>Can blind persons accurately assess body size from the voice?</article-title><source>Biology Letters</source><year>2016</year><volume>12</volume><issue>4</issue><elocation-id>20160063</elocation-id><pub-id pub-id-type="pmcid">PMC4881350</pub-id><pub-id pub-id-type="pmid">27095264</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2016.0063</pub-id></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichon</surname><given-names>S</given-names></name><name><surname>de Gelder</surname><given-names>B</given-names></name><name><surname>Grèzes</surname><given-names>J</given-names></name></person-group><article-title>Two different faces of threat. Comparing the neural systems for recognizing fear and anger in dynamic body expressions</article-title><source>Neuroimage</source><year>2009</year><volume>47</volume><issue>4</issue><fpage>1873</fpage><lpage>83</lpage><pub-id pub-id-type="pmid">19371787</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name><name><surname>Penel</surname><given-names>A</given-names></name></person-group><article-title>Auditory dominance in temporal processing: New evidence from synchronization with simultaneous visual and auditory sequences</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2002</year><volume>28</volume><issue>5</issue><fpage>1085</fpage><lpage>1099</lpage><pub-id pub-id-type="pmid">12421057</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ritchie</surname><given-names>JB</given-names></name><name><surname>Bracci</surname><given-names>S</given-names></name><name><surname>Op de Beeck</surname><given-names>H</given-names></name></person-group><article-title>Avoiding illusory effects in representational similarity analysis: What (not) to do with the diagonal</article-title><source>Neuroimage</source><year>2017</year><volume>148</volume><fpage>197</fpage><lpage>200</lpage><pub-id pub-id-type="pmid">28069538</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roch-Levecq</surname><given-names>AC</given-names></name></person-group><article-title>Production of basic emotions by children with congenital blindness: Evidence for the embodiment of theory of mind</article-title><source>British Journal of Developmental Psychology</source><year>2006</year><volume>24</volume><issue>3</issue><fpage>507</fpage><lpage>528</lpage></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacco</surname><given-names>DF</given-names></name><name><surname>Hugenberg</surname><given-names>K</given-names></name></person-group><article-title>The look of fear and anger: facial maturity modulates recognition of fearful and angry expressions</article-title><source>Emotion</source><year>2009</year><volume>9</volume><issue>1</issue><fpage>39</fpage><lpage>49</lpage><pub-id pub-id-type="pmid">19186915</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scott</surname><given-names>SK</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name><name><surname>Calder</surname><given-names>AJ</given-names></name><name><surname>Hellawell</surname><given-names>DJ</given-names></name><name><surname>Aggleton</surname><given-names>JP</given-names></name><name><surname>Johnson</surname><given-names>M</given-names></name></person-group><article-title>Impaired auditory recognition of fear and anger following bilateral amygdala lesions</article-title><source>Nature</source><year>1997</year><volume>385</volume><issue>6613</issue><fpage>254</fpage><lpage>7</lpage><pub-id pub-id-type="pmid">9000073</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanner</surname><given-names>WP</given-names></name><name><surname>Swets</surname><given-names>JA</given-names></name></person-group><article-title>A decision-making theory of visual detection</article-title><source>Psychological Review</source><year>1954</year><volume>61</volume><issue>6</issue><fpage>401</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">13215690</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theurel</surname><given-names>A</given-names></name><name><surname>Witt</surname><given-names>A</given-names></name><name><surname>Malsert</surname><given-names>J</given-names></name><name><surname>Lejeune</surname><given-names>F</given-names></name><name><surname>Fiorentini</surname><given-names>C</given-names></name><name><surname>Koviljka</surname><given-names>B</given-names></name><name><surname>Gentaz</surname><given-names>E</given-names></name></person-group><article-title>The integration of visual context information in facial emotion recognition in 5-to 15-years-olds</article-title><source>Journal of Experimental Child Psychology</source><year>2016</year><volume>150</volume><fpage>252</fpage><lpage>271</lpage><pub-id pub-id-type="pmid">27367301</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tonelli</surname><given-names>A</given-names></name><name><surname>Brayda</surname><given-names>L</given-names></name><name><surname>Gori</surname><given-names>M</given-names></name></person-group><article-title>Task-dependent calibration of auditory spatial perception through environmental visual observation</article-title><source>Frontiers in Systems Neuroscience</source><year>2015</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC4451354</pub-id><pub-id pub-id-type="pmid">26082692</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00084</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valente</surname><given-names>D</given-names></name><name><surname>Theurel</surname><given-names>A</given-names></name><name><surname>Gentaz</surname><given-names>E</given-names></name></person-group><article-title>The role of visual experience in the production of emotional facial expressions by blind people: a review</article-title><source>Psychonomic Bulletin &amp; Review</source><year>2018</year><volume>25</volume><issue>2</issue><fpage>483</fpage><lpage>497</lpage><pub-id pub-id-type="pmcid">PMC5902524</pub-id><pub-id pub-id-type="pmid">28646269</pub-id><pub-id pub-id-type="doi">10.3758/s13423-017-1338-0</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watson</surname><given-names>CS</given-names></name><name><surname>Qiu</surname><given-names>WW</given-names></name><name><surname>Chamberlain</surname><given-names>MM</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Auditory and visual speech perception: Confirmation of a modalityindependent source of individual differences in speech recognition</article-title><source>The Journal of the Acoustical Society of America</source><year>1996</year><volume>100</volume><issue>2</issue><fpage>1153</fpage><lpage>1162</lpage><pub-id pub-id-type="pmid">8759968</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Zald</surname><given-names>DH</given-names></name><name><surname>Blake</surname><given-names>R</given-names></name></person-group><article-title>Fearful expressions gain preferential access to awareness during continuous flash suppression</article-title><source>Emotion</source><year>2007</year><volume>7</volume><issue>4</issue><fpage>882</fpage><lpage>886</lpage><pub-id pub-id-type="pmcid">PMC4038625</pub-id><pub-id pub-id-type="pmid">18039058</pub-id><pub-id pub-id-type="doi">10.1037/1528-3542.7.4.882</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Young</surname><given-names>AW</given-names></name><name><surname>Frühholz</surname><given-names>S</given-names></name><name><surname>Schweinberger</surname><given-names>SR</given-names></name></person-group><article-title>Face and Voice Perception: Understanding Commonalities and Differences</article-title><source>Trends in Cognitive Sciences</source><year>2020</year><volume>24</volume><issue>5</issue><fpage>398</fpage><lpage>410</lpage><pub-id pub-id-type="pmid">32298625</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Mueller</surname><given-names>ST</given-names></name></person-group><article-title>A note on ROC analysis and non-parametric estimate of sensitivity</article-title><source>Psychometrika</source><year>2005</year><volume>70</volume><issue>1</issue><fpage>203</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1007/s11336-003-1119-8</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Sensitivity indices are here represented by gates and separately by group for each investigated emotion expression, as well as across emotions.</title></caption><graphic xlink:href="EMS188721-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Representation of the main effect of gate [A] and the main effect of emotion [B].</title><p>In [A] sensitivity increases as the stimulus duration, i.e. the amount of information available, increases, reaching a plateau at gate 8 (333ms). In [B] each circle represents one participant; Anger and Fear lead to significantly higher performance than the other three emotion expressions. Error bars are the SDs.</p></caption><graphic xlink:href="EMS188721-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Representation of the interaction between emotion and group.</title><p>In the expressions of anger and fear, sighted individuals exhibit significantly higher performance compared to blind subjects. Each circle represents one participant. Error bars are the SDs.</p></caption><graphic xlink:href="EMS188721-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Average confusion matrices by group and by gate.</title><p>The emotions contained in the stimuli are represented by rows, while the emotions of the subjects’ responses are represented by columns (A = anger, D = disgust, F = fear, J = joy, S = sadness). The diagonal of each matrix represents the correct responses, off-diagonal data represents the actual confusion patterns. Between the two matrices at each gate is the average inter-group correlation coefficient (here reported untransformed), while next to each matrix is the value of the intra-group correlation of each group. Significance is assessed on Fisher’s transformed coefficient values.</p></caption><graphic xlink:href="EMS188721-f004"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><title>Etiology and blindness onset age of the blind group.</title><p>Age at the time of testing and any residual light perception are also reported.</p></caption><table frame="box" rules="all"><thead><tr><th align="left" valign="top">Subj ID</th><th align="left" valign="top">Age</th><th align="left" valign="top">Sex</th><th align="left" valign="top">Blindness onset</th><th align="left" valign="top">Etiology</th><th align="left" valign="top">Residual vision</th></tr></thead><tbody><tr><td align="left" valign="top">EB1</td><td align="left" valign="top">31</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Retinitis pigmentosa</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB2</td><td align="left" valign="top">45</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Optic nerve hypoplasia</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB3</td><td align="left" valign="top">27</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Microphthalmia</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB4</td><td align="left" valign="top">38</td><td align="left" valign="top">M</td><td align="left" valign="top">7 months</td><td align="left" valign="top">Retinal burn in incubator</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB5</td><td align="left" valign="top">31</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Retinal detachment</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB6</td><td align="left" valign="top">31</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Retinopathy</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB7</td><td align="left" valign="top">54</td><td align="left" valign="top">F</td><td align="left" valign="top">Trouble onset at 1-2 years (non-evolutionary)</td><td align="left" valign="top">Retinoblastoma</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB8</td><td align="left" valign="top">31</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Premature retinopathy</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB9</td><td align="left" valign="top">29</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Premature retinopathy</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB10</td><td align="left" valign="top">37</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">congenital bilateral Microphthalmia</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB11</td><td align="left" valign="top">33</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Hypoglossal optic nerve impairment</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB12</td><td align="left" valign="top">30</td><td align="left" valign="top">F</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Retinitis pigmentosa</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB13</td><td align="left" valign="top">48</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Bilateral congenital glaucoma</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB14</td><td align="left" valign="top">29</td><td align="left" valign="top">F</td><td align="left" valign="top">8 months</td><td align="left" valign="top">Retinitis pigmentosa</td><td align="left" valign="top">Dark/light</td></tr><tr><td align="left" valign="top">EB15</td><td align="left" valign="top">27</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Toxoplasmosis during pregnancy</td><td align="left" valign="top">No</td></tr><tr><td align="left" valign="top">EB16</td><td align="left" valign="top">34</td><td align="left" valign="top">M</td><td align="left" valign="top">Birth</td><td align="left" valign="top">Bilateral congenital anophthalmia</td><td align="left" valign="top">No</td></tr></tbody></table></table-wrap></floats-group></article>