<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192583</article-id><article-id pub-id-type="doi">10.1101/2023.12.04.569908</article-id><article-id pub-id-type="archive">PPR769366</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Coherent categorical information triggers integration-related alpha dynamics</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chen</surname><given-names>Lixiang</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Cichy</surname><given-names>Radoslaw Martin</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Kaiser</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><aff id="A1"><label>1</label>Department of Education and Psychology, Freie Universität Berlin, Berlin, Germany</aff><aff id="A2"><label>2</label>Mathematical Institute, Department of Mathematics and Computer Science, Physics, Geography, Justus-Liebig-Universität Gießen, Gießen, Germany</aff><aff id="A3"><label>3</label>Center for Mind, Brain and Behavior (CMBB), Philipps-Universität Marburg and Justus-Liebig-Universität Gießen, Marburg, Germany</aff></contrib-group><author-notes><corresp id="CR1"><label>*</label>Correspondence: <email>lixiang.chen@fu-berlin.de</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>07</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>05</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">To create coherent visual experiences, the brain spatially integrates the complex and dynamic information it receives from the environment. We previously demonstrated that feedback-related alpha activity carries stimulus-specific information when two spatially and temporally coherent naturalistic inputs can be integrated into a unified percept. In this study, we sought to determine whether such integration-related alpha dynamics are triggered by categorical coherence in visual inputs. In an EEG experiment, we manipulated the degree of coherence by presenting pairs of videos from the same or different categories through two apertures in the left and right visual hemifields. Critically, video pairs could be video-level coherent (i.e., stem from the same video), coherent in their basic-level category, coherent in their superordinate category, or incoherent (i.e., stem from videos from two entirely different categories). We conducted multivariate classification analyses on rhythmic EEG responses to decode between the video stimuli in each condition. As the key result, we significantly decoded the video-level coherent and basic-level coherent stimuli, but not the superordinate coherent and incoherent stimuli, from cortical alpha rhythms. This suggests that alpha dynamics play a critical role in integrating information across space, and that cortical integration processes are flexible enough to accommodate information from different exemplars of the same basic-level category.</p></abstract><kwd-group><kwd>natural scenes</kwd><kwd>cortical feedback</kwd><kwd>spatiotemporal coherence</kwd><kwd>alpha rhythms</kwd><kwd>multivariate pattern analysis</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">During everyday life, our visual system continuously receives intricate and dynamic information from our surroundings. To derive meaningful interpretations from these stimuli, the brain integrates dynamic sensory inputs across the visual field, culminating in a seamlessly unified, behaviorally adaptive percept of the world (<xref ref-type="bibr" rid="R5">Block, 2007</xref>; <xref ref-type="bibr" rid="R13">Cohen et al., 2016</xref>).</p><p id="P3">Classic theories of vision posit that visual integration is solved along the feedforward cascade (<xref ref-type="bibr" rid="R30">Riesenhuber and Poggio, 1999</xref>; <xref ref-type="bibr" rid="R15">DiCarlo and Cox, 2007</xref>). However, our recent study (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>) challenges this perspective by revealing a prominent role of top-down feedback when stimuli are spatiotemporally coherent and afford integration. Such feedback is evident from stimulus-specific representations in neural alpha dynamics, which can be spatially localized to early visual cortex. This result suggests that integration-related feedback traverses the hierarchy in alpha rhythms from high-level visual cortex all the way to retinotopic early visual cortex. Our findings align well with theories that posit a multiplexing of information, where feedback is specifically routed via low-frequency alpha or beta rhythms (<xref ref-type="bibr" rid="R2">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="R35">van Kerkoerle et al., 2014</xref>; <xref ref-type="bibr" rid="R17">Fries, 2015</xref>; <xref ref-type="bibr" rid="R23">Michalareas et al., 2016</xref>).</p><p id="P4">However, our previous study used stimuli that were either coherent at the level of the individual video (i.e., two parts of the same video played in the left and right hemifields) or highly incoherent (i.e., two entirely different videos in the two hemifields). We thus could not address what level of spatiotemporal coherence in the stimuli is needed to trigger integration-related alpha dynamics.</p><p id="P5">In this study, we address this question in an EEG experiment. We manipulated the degree of spatiotemporal coherence by presenting videos from the same or different categories through two apertures left and right of the central fixation. Our findings showed that stimuli coherent at the level of individual videos are coded in cortical alpha dynamics. Critically, similar representations in alpha rhythms were also observed when different videos from the same basic-level category were presented, but not when the videos were from the same superordinate category or from different superordinate categories. This suggests that neural integration exhibits some flexibility, so that broadly consistent videos from the same category can trigger alpha dynamics linked to integration.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P6">Twenty-five healthy participants (14 females, mean age: 24.1 ± 3.9 years), with normal or corrected-to-normal vision, participated in the experiment. A minimum sample size of 24 was determined using G*Power (<xref ref-type="bibr" rid="R16">Faul et al., 2007</xref>), with an effect size of 0.25 (comparison of decoding performance between the coherent and incoherent conditions in the alpha frequency band in the EEG study) as derived from our previous study (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>), a significance level of 0.05, and a power of 0.8. All participants provided written informed consent before taking part in the experiment and they received either course credit or monetary reimbursement for their participation. The experiment was approved by the ethical committee of the Department of Education and Psychology at Freie Universität Berlin and was conducted following the Declaration of Helsinki.</p></sec><sec id="S4"><title>Stimuli and design</title><p id="P7">We selected sixteen 3-second videos (30 Hz) depicting various everyday events for the experiment. The videos were from four categories (4 exemplars for each category): birds flying, camels walking, cars running, and train moving (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). We presented videos through two apertures left and right of the central fixation (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>). The apertures had a diameter of 6° visual angle, and the closest distance between the aperture and the central fixation point was 2.64° visual angle. The central fixation dot was displayed at a visual angle of 0.44°.</p><p id="P8">We designed four different conditions by showing parts from the same video or different videos (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). In the video-level coherent condition, we displayed two parts of the same video through the apertures. In the basic-level coherent condition, the two parts were from two different videos belonging to the same category (e.g., bird video 1 &amp; bird video 2). In the superordinate coherent condition, the two parts were from two different videos belonging to the same superordinate category (e.g., bird video 1 &amp; camel video 1). In the incoherent condition, the two parts were from videos belonging to different superordinate categories (e.g., bird video 1 &amp; car video 1). In the basic-level coherent condition, the videos of each category were presented in fixed pairs (e.g., bird video 1 &amp; bird video 2, bird video 3 &amp; bird video 4). We similarly paired the videos for the superordinate coherent (e.g., bird video 1 &amp; camel video 1, bird video 2 &amp; camel video 2) and incoherent conditions (e.g., bird video 1 &amp; car video 1, bird video 2 &amp; car video 2). Therefore, there were a total of 64 unique video stimuli (16 stimuli for each of the 4 conditions).</p><p id="P9">Participants were comfortably seated at a distance of 60 cm from a monitor with a resolution of 1680 × 1050 pixels and a refresh rate of 60 Hz. The presentation of stimuli and recording of participants’ behavioral responses were controlled using MATLAB and the Psychophysics Toolbox (<xref ref-type="bibr" rid="R6">Brainard, 1997</xref>; <xref ref-type="bibr" rid="R26">Pelli, 1997</xref>). Each trial began with a 0.5 s fixation dot. Subsequently, a unique video stimulus was shown for 3 s, during which the color of the fixation changed periodically (every 200 ms) and turned either green or yellow at a single random point in the sequence (but not the first or last point). After the video, participants were presented with a response screen, prompting them to indicate whether a green or yellow fixation dot had appeared in the sequence. The next trial would not start until the participant’s response was received. Participants were instructed to keep central fixation during the video presentation to ensure that the two videos presented stimulated different visual fields. An example trial for the basic-level coherent condition is shown in <xref ref-type="fig" rid="F1">Fig. 1C</xref>. In the experiment, participants performed the color discrimination task on fixation with very high accuracy (video-level coherent: 95.8 ± 2.9%, basic-level coherent: 96.2 ± 3.0%, superordinate coherent: 96.3 ± 3.1%, incoherent: 96.0 ± 2.9%), indicating reliable fixation control. In the experiment, each of the 64 unique stimuli was shown 12 times. A total of 768 trials were presented in random order.</p></sec><sec id="S5"><title>EEG recording and preprocessing</title><p id="P10">EEG data were acquired at a sampling rate of 1000 Hz using an EASYCAP 64-electrode system with a Brainvision actiCHamp amplifier. Electrodes were arranged according to the 10-10 system. All electrodes were referenced online to the FCz.</p><p id="P11">We preprocessed the data using Fieldtrip (<xref ref-type="bibr" rid="R24">Oostenveld et al., 2011</xref>). We first filtered the data at 1–100 Hz and epoched the data from -0.5 to 3.5 s relative to the onset of the stimulus. Then, we performed baseline correction by subtracting the mean signal in the prestimulus window (-0.5 to 0 s), after which we downsampled the data to 200 Hz. Next, we conducted visual inspection to exclude noisy trials and channels, and then interpolated the removed channels (2.6 ± 1.2 channels) using their neighboring channels. Finally, we used independent component analysis (ICA) to identify and remove artifacts associated with blinks and eye movements.</p></sec><sec id="S6"><title>EEG spectral analysis</title><p id="P12">We performed spectral analysis on the preprocessed EEG data using FieldTrip, in the same way as in our previous study (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>). For each trial, we estimated power spectra separately for each channel within the alpha (8–12 Hz), beta (13–30 Hz), and gamma (31–70 Hz) frequency bands. The analysis was done for the whole period of stimulus presentation (0–3 s). For the low frequency 8–30 Hz, we applied a single taper with a Hanning window, with a step size of 1 Hz for the alpha band and 2 Hz for the beta band. For the gamma band, we used the discrete prolate spheroidal sequences (DPSS) multitaper method with ±8 Hz smoothing (in steps of 2 Hz).</p></sec><sec id="S7"><title>Multivariate decoding analysis</title><p id="P13">We performed multivariate decoding analysis to investigate the frequency-specific representations of video stimuli using CoSMoMVPA (<xref ref-type="bibr" rid="R25">Oosterhof et al., 2016</xref>) and LIBSVM (<xref ref-type="bibr" rid="R7">Chang and Lin, 2011</xref>). Given that integration-related alpha dynamics originate from retinotopic visual cortex (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>), we selected 17 parietal and occipital (PO) channels (Oz, O1, O2, POz, PO3, PO4, PO7, PO8, Pz, P1, P2, P3, P4, P5, P6, P7, P8) over visual cortex (<xref ref-type="bibr" rid="R22">Kaiser et al., 2019</xref>) for our analysis. From these channels, we extracted the patterns of spectral power across these channels to classify the four video pairings within each condition (video-level coherent, basic-level coherent, superordinate coherent, incoherent), separately for the alpha, beta, and gamma frequency bands. We conducted the classification using the linear support vector machine (SVM) with leave-one-trial-out cross-validation. One trial was assigned to the test set, while the remaining N-1 trials were used to train the classifier. We conducted the classification repeatedly until every trial was left out once, and averaged the resulting accuracies across trials. In each classification, we balanced the number of trials across categories, resulting in a maximum of 188 trials for the training set (47 for each category). To reduce the dimensionality of the data, we applied principal component analysis (PCA) to the data before classification (<xref ref-type="bibr" rid="R8">Chen et al., 2022</xref>). We performed PCA on the training data, and then projected the resulting PCA solution onto the testing data. We selected a subset of components that explained 99% of the variance of the training data. As a result, we obtained decoding accuracy for each frequency band and each condition, indicating the degree to which the video stimuli were accurately represented in different frequency bands. We performed a one-sample <italic>t</italic>-test to compare the decoding performance against chance level (25%) for testing whether the stimuli could be represented in each frequency band (FDR-correction, <italic>p</italic> &lt; 0.05). Furthermore, to investigate whether the frequency-specific representations were modulated by the degree of stimulus coherence, we conducted a two-way ANOVA (4 conditions × 3 frequencies) and post-hoc paired <italic>t</italic>-tests to compare the decoding performance between conditions separately for each frequency band (FDR-correction, <italic>p</italic> &lt; 0.05).</p><p id="P14">To investigate where the effects are localized and whether the effects are maximum over visual cortex, we performed searchlight decoding analysis. For each channel, we defined a searchlight including itself and its 10 nearest neighboring channels and then used the spectral power patterns across these channels to decode between the four video pairings within each condition, separately for each frequency band (alpha, beta, and gamma). Identically to the decoding analysis using PO channels, we used leave-one-trial-out cross-validation and applied principal component analysis (PCA) for the classification. The whole classification process was iterated over all channels. As a result, we obtained decoding accuracy in each channel separately for each frequency band and each condition. To localize the significant decoding for each condition, we used a one-sample permutation test (10,000 iterations), comparing the decoding accuracy against the chance level (25%) in each channel and then performing cluster-based multiple comparison corrections (<italic>p</italic> &lt; 0.05).</p><p id="P15">To investigate the representation of stimuli in time-locked broadband responses, we performed time-resolved decoding analysis. We classified between the four video pairings within each condition using broadband responses across PO channels at each time point from -0.1 to 1 s relative to stimulus onset (decoding already approached chances level well before 1 s). The decoding parameters were identical to the frequency-resolved decoding analysis using PO channels. The resulting decoding timeseries were smoothed with a moving average of 5 time points. Separately for each time point, we used a one-sample <italic>t</italic>-test to compare decoding against chance and paired <italic>t</italic>-tests to compare the difference between conditions. Multiple comparison corrections were conducted using FDR (<italic>p</italic> &lt; 0.05), and only clusters of at least 5 consecutive significant time points were considered (<xref ref-type="bibr" rid="R8">Chen et al., 2022</xref>).</p><p id="P16">Following our previous study (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>), we primarily investigated integration-related effects in spectral EEG power. However, in principle, such effects may also be represented in the phase of neural rhythms (e.g., resulting from the different temporal dynamics of the videos). We thus decoded the four video pairings within each condition using patterns of spectral phase across PO channels separately for alpha, beta, and gamma bands. Here, we performed the decoding analysis and statistical comparisons using the same approaches as in the frequency-resolved decoding analysis on spectral power.</p></sec><sec id="S8"><title>Eye tracking recording and processing</title><p id="P17">Eye movements were recorded monocularly (right eye) at 1000 Hz with an Eyelink 1000 Tower Mount (SR Research Ltd., Mississauga, Ontario, Canada) using the Psychophysics and Eyelink Toolbox extensions (<xref ref-type="bibr" rid="R14">Cornelissen et al., 2002</xref>). At the beginning of the experiment, we used a standard 9-point calibration to calibrate eye position.</p><p id="P18">We preprocessed eye-tracking data using Fieldtrip. Specifically, we segmented the data into epochs from -0.5 to 3.5 s relative to stimulus onset and downsampled the data to a sampling rate of 200 Hz. The preprocessed data were transformed from their original screen coordinate units (pixels) to visual angle units (degrees). We next excluded the trials that were removed in the EEG analysis. To check the fixation stability, we calculated the mean and standard deviation (SD) of the horizontal and vertical eye movements during video presentation (0–3 s) in each trial and then averaged the mean and SD values across trials separately for each condition. We found no significant differences in both horizontal [comparisons of mean: <italic>F</italic>(3, 72) = 0.74, <italic>p</italic> = 0.53; comparisons of SD: <italic>F</italic>(3, 72) = 0.56, <italic>p</italic> = 0.64] and vertical eye movements [comparisons of mean: <italic>F</italic>(3, 72) = 0.71, <italic>p</italic> = 0.55; comparisons of SD: <italic>F</italic>(3, 72) = 0.97, <italic>p</italic> = 0.41] between the four conditions.</p></sec></sec><sec id="S9" sec-type="results"><title>Results</title><p id="P19">To study the frequency-specific representations of video stimuli, we decoded between video stimuli within each condition (video-level coherent, basic-level coherent, superordinate coherent, incoherent) using patterns of spectral power across channels separately for each frequency band (alpha, beta, gamma). In this analysis, we found significant above-chance decoding only in the alpha band and for the video-level coherent and basic-level coherent stimuli (<xref ref-type="fig" rid="F2">Fig. 2A</xref>). Using a 4-condition × 3-frequency two-way ANOVA, we identified a significant interaction effect between condition and frequency [<italic>F</italic>(6, 144) = 3.75, <italic>p</italic> = 0.002]. Subsequently, we conducted post-hoc <italic>t</italic>-tests to examine differences between conditions in each frequency band.</p><p id="P20">In the alpha band, we observed a decrease in decoding accuracy as the spatial coherence of stimuli reduced, indicating that integration-related alpha activity is modulated by the coherence of the stimuli. Specifically, the video-level coherent stimuli were decoded better than the superordinate coherent stimuli [<italic>t</italic>(24) = 3.90, <italic>p</italic> &lt; 0.001; <xref ref-type="fig" rid="F2">Fig. 2A</xref>] as well as better than the incoherent stimuli [<italic>t</italic>(24) = 4.37, <italic>p</italic> &lt; 0.001; <xref ref-type="fig" rid="F2">Fig. 2A</xref>]. Similarly, basic-level coherent stimuli were also more decodable than both the superordinate coherent stimuli [<italic>t</italic>(24) = 3.317, <italic>p</italic> = 0.004; <xref ref-type="fig" rid="F2">Fig. 2A</xref>] and incoherent stimuli [<italic>t</italic>(24) = 3.083, <italic>p</italic> = 0.005; <xref ref-type="fig" rid="F2">Fig. 2A</xref>]. We found no significant difference between the video-level coherent and the basic-level coherent conditions [<italic>t</italic>(24) = 1.290, <italic>p</italic> = 0.314]. Importantly, the difference in alpha decoding across conditions was not related to an absence of stimulus representation in the more incoherent conditions in the first place: When decoding from time-locked broadband responses, we found significant decoding for all conditions within the first 500 ms of processing that leveled off towards chance level during the first second (<xref ref-type="fig" rid="F2">Fig. 2B</xref>). However, there was no significant between-condition difference in decoding from the time-locked responses (<xref ref-type="fig" rid="F2">Fig. 2B</xref>), consistent with our previous results (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>). In addition, we found no significant effects in the beta and gamma frequency bands (all <italic>p</italic> &gt; 0.05).</p><p id="P21">Given that these analyses were only conducted on rhythmic patterns in the PO channels (see Materials and Methods), we aimed to confirm that these effects indeed originate over visual cortex in a channel-space searchlight analysis (<xref ref-type="bibr" rid="R21">Kaiser et al., 2016</xref>). In this analysis, we observed significant decoding only in the alpha band, primarily in the PO channels, and only for the video-level coherent and basic-level coherent stimuli (<xref ref-type="fig" rid="F2">Fig. 2C</xref>). We found no effects for the other two, more incoherent conditions. Together, these results suggest that alpha activity plays a key role in the integration of visual information across space. They further highlight that integration-related alpha dynamics are not only triggered when stimuli are video-level coherent (i.e., when the same video was shown through the apertures), but that integration processes are flexible enough to accommodate information that comes from videos belonging to the same basic-level category.</p><p id="P22">To investigate whether the integration-related effects were also represented in the phase of neural rhythms, we used the spectral phase to decode between stimuli. While the basic-level coherent stimuli were decodable in the alpha band and incoherent stimuli were decodable in the beta band (<xref ref-type="fig" rid="F2">Fig. 2D</xref>), we did not find reliable differences between conditions (all <italic>p</italic> &gt; 0.05, FDR-corrected; <xref ref-type="fig" rid="F2">Fig. 2D</xref>). This suggests that integration-related stimulus information is coded in the power of cortical alpha dynamics.</p></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P23">In this study, we investigated the involvement of alpha dynamics in the integration of visual information. We specifically asked whether integration-related alpha dynamics are also observed when videos are broadly consistent in category. Utilizing multivariate decoding analysis on spectrally resolved EEG data, we show that both video-level coherent and basic-level coherent stimuli were decodable from alpha-band EEG activity. In contrast, we found no alpha-band decoding for the superordinate coherent and incoherent stimuli. Our results suggest that categorical coherence of natural videos modulates the involvement of alpha-frequency activity: Alpha-related integration is triggered not only when visual stimuli are entirely coherent but also when these stimuli share common attributes resulting from their basic-level category membership.</p><p id="P24">Our results support our previous finding (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>) that alpha dynamics play a key role in the integration of visual information across space. Together with a significant correspondence between alpha activity and V1 response in our previous study, we interpret the coding of stimulus-specific information in alpha as integration-related feedback. This interpretation is in line with a series of studies demonstrating that alpha rhythms carry cortical feedback from higher-order brain regions (<xref ref-type="bibr" rid="R3">Bastos et al., 2015</xref>, <xref ref-type="bibr" rid="R1">2020</xref>; <xref ref-type="bibr" rid="R23">Michalareas et al., 2016</xref>), and also encode stimulus-specific information (<xref ref-type="bibr" rid="R37">Xie et al., 2020</xref>; <xref ref-type="bibr" rid="R20">Kaiser, 2022</xref>; <xref ref-type="bibr" rid="R34">Stecher and Kaiser, 2023</xref>). This perspective highlights the dynamic and active role that alpha rhythms may play in cognitive processes, in contrast to the passive or inhibitory roles often ascribed to them (<xref ref-type="bibr" rid="R27">Pfurtscheller et al., 1996</xref>; <xref ref-type="bibr" rid="R32">Romei et al., 2008</xref>; <xref ref-type="bibr" rid="R19">Jensen and Mazaheri, 2010</xref>; <xref ref-type="bibr" rid="R18">Haegens et al., 2011</xref>; <xref ref-type="bibr" rid="R10">Clayton et al., 2015</xref>).</p><p id="P25">Our results indicate that integration-related alpha dynamics can be triggered not only by the presentation of video snippets from the same video, but also by the presentation of different videos from the same basic-level category. This suggests a spectral signature of the category-level nature of feedback information used for visual integration. The basic level is defined as the level that has the highest degree of cue validity (<xref ref-type="bibr" rid="R33">Rosch, 1978</xref>). Basic level categories maximize the number of attributes shared by members of the category, and minimize the number of attributes shared with other categories. This sweet spot might be the one also used by the brain when implementing integration. However, our study cannot entirely clarify whether the integration-related alpha activity is indeed triggered by a more abstract coherence in basic-level category, presumably coded in high-level visual cortex (<xref ref-type="bibr" rid="R36">Walther et al., 2009</xref>; <xref ref-type="bibr" rid="R29">Proklova et al., 2016</xref>) or by the spatiotemporal coherence of visual features associated with a category (<xref ref-type="bibr" rid="R11">Coggan et al., 2019</xref>, <xref ref-type="bibr" rid="R12">2022</xref>; <xref ref-type="bibr" rid="R31">Robert et al., 2023</xref>). More research is needed to clarify this question.</p><p id="P26">In our previous study, we found that gamma rhythms, previously associated with feedforward processing in visual cortex (<xref ref-type="bibr" rid="R2">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="R35">van Kerkoerle et al., 2014</xref>; <xref ref-type="bibr" rid="R17">Fries, 2015</xref>; <xref ref-type="bibr" rid="R23">Michalareas et al., 2016</xref>), carried more information about incoherent than about coherent inputs, suggesting that feedforward processing is to some degree dominated by integration-related feedback (<xref ref-type="bibr" rid="R9">Chen et al., 2023</xref>). By contrast, we were not able to decode between the videos from gamma rhythms across all four conditions in the current study. Several factors may explain this difference. First, a different group of participants were scanned with a different EEG system for the current study. As gamma activity can be weak and unreliable in EEG recordings, it may not be systematically observed in each experiment (<xref ref-type="bibr" rid="R28">Pitts et al., 2014</xref>). Second, we used different stimuli than in our previous report. In the previous study, we tried to maximize incoherence by picking very different videos (featuring different colors, movements, etc.). Here we designed the experiment without focusing on maximizing such differences. However, such more drastic incoherence may be needed to induce reliable gamma activity: Given the extended presentation duration of the video stimuli (3 seconds) and the absence of rapid or unexpected visual events, reliable predictions may explain away feedforward inputs carried by gamma rhythms when the videos are similar enough on some dimensions. Further studies are needed to clarify the role of gamma dynamics in coding feedforward information propagation in similar paradigms.</p><p id="P27">Taken together, our findings emphasize the key role of alpha dynamics in the construction of coherent and unified visual percepts during naturalistic vision. They further suggest that integration-related alpha dynamics does not operate in an all-or-none fashion, but that the coarse coherence between inputs stemming from the same basic-level category can effectively trigger neural correlates of integration.</p></sec></body><back><ack id="S14"><title>Acknowledgments</title><p>L.C. is supported by a PhD stipend from the China Scholarship Council (CSC). R.M.C is supported by the Deutsche Forschungsgemeinschaft (DFG; CI241/1-1, CI241/3-1, CI241/7-1) and by a European Research Council (ERC) starting grant (ERC-2018-STG 803370). D.K. is supported by the Deutsche Forschungsgemeinschaft (DFG; SFB/TRR135, project number 222641018; KA4683/5-1, project number 518483074), “The Adaptive Mind”, funded by the Excellence Program of the Hessian Ministry of Higher Education, Science, Research and Art, and an ERC Starting Grant (PEP, ERC-2022-STG 101076057). Views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. The authors thank the HPC Service of ZEDAT, Freie Universität Berlin (<xref ref-type="bibr" rid="R4">Bennett et al., 2020</xref>), for computing time.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Lundqvist</surname><given-names>M</given-names></name><name><surname>Waite</surname><given-names>AS</given-names></name><name><surname>Kopell</surname><given-names>N</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>Layer and rhythm specificity for predictive routing</article-title><source>Proc Natl Acad Sci</source><year>2020</year><volume>117</volume><fpage>31459</fpage><lpage>31469</lpage><pub-id pub-id-type="pmcid">PMC7733827</pub-id><pub-id pub-id-type="pmid">33229572</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2014868117</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Usrey</surname><given-names>WM</given-names></name><name><surname>Adams</surname><given-names>RA</given-names></name><name><surname>Mangun</surname><given-names>GR</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><year>2012</year><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="pmcid">PMC3777738</pub-id><pub-id pub-id-type="pmid">23177956</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname><given-names>AM</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>Bosman</surname><given-names>CA</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Dowdall</surname><given-names>JR</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Visual Areas Exert Feedforward and Feedback Influences through Distinct Frequency Channels</article-title><source>Neuron</source><year>2015</year><volume>85</volume><fpage>390</fpage><lpage>401</lpage><pub-id pub-id-type="pmid">25556836</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bennett</surname><given-names>L</given-names></name><name><surname>Melchers</surname><given-names>B</given-names></name><name><surname>Proppe</surname><given-names>B</given-names></name></person-group><source>Curta: A General-purpose High-Performance Computer at ZEDAT</source><publisher-loc>Berlin</publisher-loc><publisher-name>Freie Universität</publisher-name><year>2020</year><pub-id pub-id-type="doi">10.17169/refubium-26754</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Block</surname><given-names>N</given-names></name></person-group><article-title>Consciousness, accessibility, and the mesh between psychology and neuroscience</article-title><source>Behav Brain Sci</source><year>2007</year><volume>30</volume><fpage>481</fpage><lpage>499</lpage><pub-id pub-id-type="pmid">18366828</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><article-title>The Psychophysics Toolbox</article-title><source>Spat Vis</source><year>1997</year><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>C-C</given-names></name><name><surname>Lin</surname><given-names>C-J</given-names></name></person-group><article-title>LIBSVM: a library for support vector machines</article-title><source>ACM Trans Intell Syst Technol</source><year>2011</year><volume>2</volume><fpage>1</fpage><lpage>27</lpage></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Semantic scene-object consistency modulates N300/400 EEG components, but does not automatically facilitate object representations</article-title><source>Cereb Cortex</source><year>2022</year><volume>32</volume><fpage>3553</fpage><lpage>3567</lpage><pub-id pub-id-type="pmid">34891169</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Alpha-frequency feedback to early visual cortex orchestrates coherent naturalistic vision</article-title><source>Sci Adv</source><year>2023</year><volume>9</volume><elocation-id>eadi2321</elocation-id><pub-id pub-id-type="pmcid">PMC10637741</pub-id><pub-id pub-id-type="pmid">37948520</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.adi2321</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayton</surname><given-names>MS</given-names></name><name><surname>Yeung</surname><given-names>N</given-names></name><name><surname>Cohen Kadosh</surname><given-names>R</given-names></name></person-group><article-title>The roles of cortical oscillations in sustained attention</article-title><source>Trends Cogn Sci</source><year>2015</year><volume>19</volume><fpage>188</fpage><lpage>195</lpage><pub-id pub-id-type="pmid">25765608</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Baker</surname><given-names>DH</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name></person-group><article-title>Selectivity for mid-level properties of faces and places in the fusiform face area and parahippocampal place area</article-title><source>Eur J Neurosci</source><year>2019</year><volume>49</volume><fpage>1587</fpage><lpage>1596</lpage><pub-id pub-id-type="pmid">30589482</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coggan</surname><given-names>DD</given-names></name><name><surname>Watson</surname><given-names>DM</given-names></name><name><surname>Wang</surname><given-names>A</given-names></name><name><surname>Brownbridge</surname><given-names>R</given-names></name><name><surname>Ellis</surname><given-names>C</given-names></name><name><surname>Jones</surname><given-names>K</given-names></name><name><surname>Kilroy</surname><given-names>C</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name></person-group><article-title>The representation of shape and texture in category-selective regions of ventral-temporal cortex</article-title><source>Eur J Neurosci</source><year>2022</year><volume>56</volume><fpage>4107</fpage><lpage>4120</lpage><pub-id pub-id-type="pmcid">PMC9545892</pub-id><pub-id pub-id-type="pmid">35703007</pub-id><pub-id pub-id-type="doi">10.1111/ejn.15737</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Dennett</surname><given-names>DC</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><article-title>What is the Bandwidth of Perceptual Experience?</article-title><source>Trends Cogn Sci</source><year>2016</year><volume>20</volume><fpage>324</fpage><lpage>335</lpage><pub-id pub-id-type="pmcid">PMC4898652</pub-id><pub-id pub-id-type="pmid">27105668</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2016.03.006</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornelissen</surname><given-names>FW</given-names></name><name><surname>Peters</surname><given-names>EM</given-names></name><name><surname>Palmer</surname><given-names>J</given-names></name></person-group><article-title>The Eyelink Toolbox: Eye tracking with MATLAB and the Psychophysics Toolbox</article-title><source>Behav Res Methods Instrum Comput</source><year>2002</year><volume>34</volume><fpage>613</fpage><lpage>617</lpage><pub-id pub-id-type="pmid">12564564</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><article-title>Untangling invariant object recognition</article-title><source>Trends Cogn Sci</source><year>2007</year><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faul</surname><given-names>F</given-names></name><name><surname>Erdfelder</surname><given-names>E</given-names></name><name><surname>Lang</surname><given-names>A-G</given-names></name><name><surname>Buchner</surname><given-names>A</given-names></name></person-group><article-title>G*Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences</article-title><source>Behav Res Methods</source><year>2007</year><volume>39</volume><fpage>175</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">17695343</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Rhythms for Cognition: Communication through Coherence</article-title><source>Neuron</source><year>2015</year><volume>88</volume><fpage>220</fpage><lpage>235</lpage><pub-id pub-id-type="pmcid">PMC4605134</pub-id><pub-id pub-id-type="pmid">26447583</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.034</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haegens</surname><given-names>S</given-names></name><name><surname>Nácher</surname><given-names>V</given-names></name><name><surname>Luna</surname><given-names>R</given-names></name><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Jensen</surname><given-names>O</given-names></name></person-group><article-title>α-Oscillations in the monkey sensorimotor network influence discrimination performance by rhythmical inhibition of neuronal spiking</article-title><source>Proc Natl Acad Sci</source><year>2011</year><volume>108</volume><fpage>19377</fpage><lpage>19382</lpage><pub-id pub-id-type="pmcid">PMC3228466</pub-id><pub-id pub-id-type="pmid">22084106</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1117190108</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>O</given-names></name><name><surname>Mazaheri</surname><given-names>A</given-names></name></person-group><article-title>Shaping functional architecture by oscillatory alpha activity: gating by inhibition</article-title><source>Front Hum Neurosci</source><year>2010</year><volume>4</volume><elocation-id>186</elocation-id><pub-id pub-id-type="pmcid">PMC2990626</pub-id><pub-id pub-id-type="pmid">21119777</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2010.00186</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Spectral brain signatures of aesthetic natural perception in the α and β frequency bands</article-title><source>J Neurophysiol</source><year>2022</year><volume>128</volume><fpage>1501</fpage><lpage>1505</lpage><pub-id pub-id-type="pmid">36259673</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The Neural Dynamics of Attentional Selection in Natural Scenes</article-title><source>J Neurosci</source><year>2016</year><volume>36</volume><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="pmcid">PMC6601932</pub-id><pub-id pub-id-type="pmid">27733605</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Turini</surname><given-names>J</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>A neural mechanism for contextualizing fragmented inputs during naturalistic vision</article-title><source>Elife</source><year>2019</year><volume>8</volume><elocation-id>e48182</elocation-id><pub-id pub-id-type="pmcid">PMC6802952</pub-id><pub-id pub-id-type="pmid">31596234</pub-id><pub-id pub-id-type="doi">10.7554/eLife.48182</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Vezoli</surname><given-names>J</given-names></name><name><surname>van Pelt</surname><given-names>S</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><article-title>Alpha-Beta and Gamma Rhythms Subserve Feedback and Feedforward Influences among Human Visual Cortical Areas</article-title><source>Neuron</source><year>2016</year><volume>89</volume><fpage>384</fpage><lpage>397</lpage><pub-id pub-id-type="pmcid">PMC4871751</pub-id><pub-id pub-id-type="pmid">26777277</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.018</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name></person-group><article-title>FieldTrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><article-title>CoSMoMVPA: multi-modal multivariate pattern analysis of neuroimaging data in Matlab/GNU Octave</article-title><source>Front Neuroinformatics</source><year>2016</year><volume>10</volume><elocation-id>27</elocation-id><pub-id pub-id-type="pmcid">PMC4956688</pub-id><pub-id pub-id-type="pmid">27499741</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00027</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spat Vis</source><year>1997</year><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfurtscheller</surname><given-names>G</given-names></name><name><surname>Stancák</surname><given-names>A</given-names></name><name><surname>Neuper</surname><given-names>Ch</given-names></name></person-group><article-title>Event-related synchronization (ERS) in the alpha band — an electrophysiological correlate of cortical idling: A review</article-title><source>Int J Psychophysiol</source><year>1996</year><volume>24</volume><fpage>39</fpage><lpage>46</lpage><pub-id pub-id-type="pmid">8978434</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pitts</surname><given-names>MA</given-names></name><name><surname>Padwal</surname><given-names>J</given-names></name><name><surname>Fennelly</surname><given-names>D</given-names></name><name><surname>Martínez</surname><given-names>A</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Gamma band activity and the P3 reflect post-perceptual processes, not visual awareness</article-title><source>NeuroImage</source><year>2014</year><volume>101</volume><fpage>337</fpage><lpage>350</lpage><pub-id pub-id-type="pmcid">PMC4169212</pub-id><pub-id pub-id-type="pmid">25063731</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.07.024</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Disentangling Representations of Object Shape and Object Category in Human Visual Cortex: The Animate–Inanimate Distinction</article-title><source>J Cogn Neurosci</source><year>2016</year><volume>28</volume><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nat Neurosci</source><year>1999</year><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robert</surname><given-names>S</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Vaziri-Pashkam</surname><given-names>M</given-names></name></person-group><article-title>Disentangling Object Category Representations Driven by Dynamic and Static Visual Input</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><fpage>621</fpage><lpage>634</lpage><pub-id pub-id-type="pmcid">PMC9888510</pub-id><pub-id pub-id-type="pmid">36639892</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0371-22.2022</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romei</surname><given-names>V</given-names></name><name><surname>Brodbeck</surname><given-names>V</given-names></name><name><surname>Michel</surname><given-names>C</given-names></name><name><surname>Amedi</surname><given-names>A</given-names></name><name><surname>Pascual-Leone</surname><given-names>A</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name></person-group><article-title>Spontaneous fluctuations in posterior α-band EEG activity reflect variability in excitability of human visual areas</article-title><source>Cereb Cortex</source><year>2008</year><volume>18</volume><fpage>2010</fpage><lpage>2018</lpage><pub-id pub-id-type="pmcid">PMC2517102</pub-id><pub-id pub-id-type="pmid">18093905</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhm229</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rosch</surname><given-names>E</given-names></name></person-group><chapter-title>Principles of Categorization</chapter-title><person-group person-group-type="editor"><name><surname>Rosch</surname><given-names>E</given-names></name><name><surname>Lloyd</surname><given-names>Barbara</given-names></name></person-group><source>Cognition and categorization, Cognition and Categorization</source><publisher-name>Lawrence Erlbaum Associates</publisher-name><year>1978</year><fpage>27</fpage><lpage>48</lpage></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stecher</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name></person-group><article-title>Imaginary scenes are represented in cortical alpha activity</article-title><year>2023</year><elocation-id>2023.10.23.563249</elocation-id><date-in-citation>Accessed November 22, 2023</date-in-citation><comment>Available at: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.10.23.563249v1">https://www.biorxiv.org/content/10.1101/2023.10.23.563249v1</ext-link></comment></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kerkoerle</surname><given-names>T</given-names></name><name><surname>Self</surname><given-names>MW</given-names></name><name><surname>Dagnino</surname><given-names>B</given-names></name><name><surname>Gariel-Mathis</surname><given-names>M-A</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>van der Togt</surname><given-names>C</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex</article-title><source>Proc Natl Acad Sci</source><year>2014</year><volume>111</volume><fpage>14332</fpage><lpage>14341</lpage><pub-id pub-id-type="pmcid">PMC4210002</pub-id><pub-id pub-id-type="pmid">25205811</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1402773111</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>DB</given-names></name><name><surname>Caddigan</surname><given-names>E</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name></person-group><article-title>Natural Scene Categories Revealed in Distributed Patterns of Activity in the Human Brain</article-title><source>J Neurosci</source><year>2009</year><volume>29</volume><fpage>10573</fpage><lpage>10581</lpage><pub-id pub-id-type="pmcid">PMC2774133</pub-id><pub-id pub-id-type="pmid">19710310</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0559-09.2009</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>S</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name></person-group><article-title>Visual imagery and perception share neural representations in the alpha frequency band</article-title><source>Curr Biol</source><year>2020</year><volume>30</volume><fpage>2621</fpage><lpage>2627</lpage><elocation-id>e5</elocation-id><pub-id pub-id-type="pmcid">PMC7416104</pub-id><pub-id pub-id-type="pmid">32750335</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2020.07.023</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Stimuli and experimental design.</title><p><bold>A)</bold> Snapshots from the video stimulus set. <bold>B)</bold> In the experiment, videos were presented through two apertures left and right of the central fixation, manipulated in four conditions: video-level coherent, coherent in the basic-level category, coherent in the superordinate category, and incoherent. <bold>C)</bold> During video presentation, the color of the central dot changed periodically (every 200 ms) and participants were asked to report whether there was a green or yellow fixation dot included in the sequence.</p></caption><graphic xlink:href="EMS192583-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>EEG decoding analysis.</title><p><bold>A)</bold> EEG frequency-resolved decoding analysis on spectral power. In each condition, we classified the four video pairings within each condition (video-level coherent, basic-level coherent, superordinate coherent, incoherent) using patterns of spectral power across 17 parietal and occipital (PO) channels, separately for each frequency band (alpha, beta, gamma). We found significant decoding only in the alpha band and only for the video-level coherent and basic-level coherent stimuli (indicated by asterisks color-coded as result dots). Additionally, the stimuli in the video-level coherent and basic-level coherent conditions were decoded better than the stimuli in the superordinate coherent and incoherent conditions (indicated by black asterisks over lines connecting compared data points). These results suggest that integration-related alpha dynamics are not only observed when videos are video-level coherent, but also when similar videos are from the same basic-level category. Error bars represent standard errors. *: <italic>p</italic> &lt; 0.05 (FDR-corrected). <bold>B</bold>) EEG time-resolved decoding analysis. We decoded between the four video pairings within each condition using time-resolved broadband responses across 17 PO channels at each time point from -0.1 to 1 s relative to the onset of the stimulus. We found significant decoding for all conditions within the first 500 ms of processing but no significant differences between conditions. Line markers denote significant above-chance decoding color-coded as result curves (<italic>p</italic> &lt; 0.05, FDR-corrected). <bold>C)</bold> EEG searchlight decoding analysis. For each channel, we defined a searchlight including itself and its 10 nearest neighboring channels, and then used the patterns of alpha power across these 11 channels to decode between the four video pairings within each condition. We found significant decoding only for the video-level coherent and basic-level coherent stimuli primarily in PO channels (circles reflect significant channel locations). <bold>D)</bold> EEG frequency-resolved decoding analysis on spectral phase. We classified the four video pairings within each condition using patterns of spectral phase across 17 PO channels, separately for alpha, beta, and gamma frequency bands. We found no significant differences between conditions. Error bars represent standard errors. *: <italic>p</italic> &lt; 0.05 (FDR-corrected).</p></caption><graphic xlink:href="EMS192583-f002"/></fig></floats-group></article>