<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190989</article-id><article-id pub-id-type="doi">10.1101/2023.11.12.566754</article-id><article-id pub-id-type="archive">PPR758660</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Multi-timescale reinforcement learning in the brain</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Masset</surname><given-names>Paul</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">*</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Tano</surname><given-names>Pablo</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="fn" rid="FN1">*</xref></contrib><contrib contrib-type="author"><name><surname>Kim</surname><given-names>HyungGoo R.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Malik</surname><given-names>Athar N.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Pouget</surname><given-names>Alexandre</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib><contrib contrib-type="author"><name><surname>Uchida</surname><given-names>Naoshige</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">✉</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Molecular and Cellular Biology, Harvard University, USA</aff><aff id="A2"><label>2</label>Center for Brain Science, Harvard University, USA</aff><aff id="A3"><label>3</label>Department of Basic Neuroscience, University of Geneva, Switzerland</aff><aff id="A4"><label>4</label>Department of Biomedical Engineering, Sungkyunkwan University, Suwon 16419, Republic of Korea</aff><aff id="A5"><label>5</label>Center for Neuroscience Imaging Research, Institute for Basic Science (IBS), Suwon 16419, Republic of Korea</aff><aff id="A6"><label>6</label>Department of Neurosurgery, Warren Alpert Medical School of Brown University, USA</aff><aff id="A7"><label>7</label>Norman Prince Neurosciences Institute, Rhode Island Hospital, USA</aff><author-notes><corresp id="CR1">
<label>✉</label>
<bold>Materials &amp; Correspondence</bold> Correspondence should be addressed to Paul Masset (<email>paul_masset@fas.harvard.edu</email>), Alexandre Pouget (<email>alexandre.pouget@unige.ch</email>) or Naoshige Uchida (<email>uchida@mcb.harvard.edu</email>).</corresp><fn fn-type="equal" id="FN1"><label>*</label><p id="P1">These authors contributed equally to this work.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>15</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>14</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">To thrive in complex environments, animals and artificial agents must learn to act adaptively to maximize fitness and rewards. Such adaptive behavior can be learned through reinforcement learning<sup><xref ref-type="bibr" rid="R1">1</xref></sup>, a class of algorithms that has been successful at training artificial agents<sup><xref ref-type="bibr" rid="R2">2</xref>–<xref ref-type="bibr" rid="R6">6</xref></sup> and at characterizing the firing of dopamine neurons in the midbrain<sup><xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. In classical reinforcement learning, agents discount future rewards exponentially according to a single time scale, controlled by the discount factor. Here, we explore the presence of multiple timescales in biological reinforcement learning. We first show that reinforcement agents learning at a multitude of timescales possess distinct computational benefits. Next, we report that dopamine neurons in mice performing two behavioral tasks encode reward prediction error with a diversity of discount time constants. Our model explains the heterogeneity of temporal discounting in both cue-evoked transient responses and slower timescale fluctuations known as dopamine ramps. Crucially, the measured discount factor of individual neurons is correlated across the two tasks suggesting that it is a cell-specific property. Together, our results provide a new paradigm to understand functional heterogeneity in dopamine neurons, a mechanistic basis for the empirical observation that humans and animals use non-exponential discounts in many situations <sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R14">14</xref></sup>, and open new avenues for the design of more efficient reinforcement learning algorithms.</p></abstract></article-meta></front><body><sec id="S1"><title>Main</title><p id="P3">The ability to anticipate forthcoming events is crucial in choosing the right course of action. Predictive models have been a primary contender for the function of the cortex <sup><xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R16">16</xref></sup> and are at the core of recent proposals to design intelligent artificial systems <sup><xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R18">18</xref></sup>. Many of these proposals rely on temporal difference (TD) reinforcement learning (RL) in which the TD learning rule is used to learn predictive information <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>. By updating current estimates based on future expected estimates – TD methods have been remarkably successful in solving tasks that require predicting future rewards and planning actions to obtain them <sup><xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R23">23</xref></sup>. In parallel, the TD learning rule has been used to explain the activity patterns of dopamine neurons in the midbrain, one of the classic examples where a normative computation has been successfully assigned to a genetically defined neuron type <sup><xref ref-type="bibr" rid="R7">7</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. However, there is mounting evidence suggesting that the representations encoded in dopamine neurons are far richer and more complex than a simple scalar reward prediction error <sup><xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R32">32</xref></sup>, prompting reconsideration of the computational framework.</p><p id="P4">The standard formulation of TD learning assumes a fixed discount factor (that is, a single learning timescale) which, after convergence, results in exponential discounting: the value of a future reward is reduced by a fixed fraction per unit time (or time step). Although this formulation is important for simplicity and self-consistency of the learning rule, it is well known that humans and other animals do not exhibit exponential discounting when faced with inter-temporal choices. Instead, they tend to show hyperbolic discounting: there is a fast drop in value followed by a slower rate for further delays<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>. Far from being irrational, non-exponential discounting can be optimal depending on the uncertainty in the environment as has been documented in the behavioral economics and foraging literature <sup><xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R34">34</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Humans and animals can modulate their discounting function to adapt to the temporal statistics of the environment and maladaptive behavior can be a signature of mental state or disease <sup><xref ref-type="bibr" rid="R36">36</xref>–<xref ref-type="bibr" rid="R39">39</xref></sup>.</p><p id="P5">The TD rule can potentially be extended to learn more complex predictive representations than the mean discounted future reward of the traditional value function, both in artificial <sup><xref ref-type="bibr" rid="R40">40</xref>–<xref ref-type="bibr" rid="R44">44</xref></sup> and biological neural systems <sup><xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R46">46</xref></sup>. A growing body of evidence points to the rich nature of temporal representations in biological systems <sup><xref ref-type="bibr" rid="R47">47</xref>–<xref ref-type="bibr" rid="R49">49</xref></sup> and particularly in the basal ganglia <sup><xref ref-type="bibr" rid="R50">50</xref>–<xref ref-type="bibr" rid="R53">53</xref></sup>. Understanding how these rich temporal representations are learned remains a key question in neuroscience and psychology. An important component across most temporal-learning proposals is the presence of multiple timescales <sup><xref ref-type="bibr" rid="R46">46</xref>,<xref ref-type="bibr" rid="R54">54</xref>–<xref ref-type="bibr" rid="R59">59</xref></sup> which enables capturing temporal dependencies across a diverse range of durations: shorter timescales typically handle rapid changes and immediate dependencies, while longer timescales capture slow-changing features or long-term dependencies <sup><xref ref-type="bibr" rid="R57">57</xref></sup>. Furthermore, work in AI suggests that the performance of deep RL algorithms can be improved by incorporating learning at multiple timescales <sup><xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R61">61</xref></sup>. We therefore ask whether reinforcement learning in the brain exhibits such multi-timescale properties.</p><p id="P6">We first investigate the computational implications of multi-timescale RL. We then show that dopamine neurons encode predictions at diverse timescales, providing a potential neural substrate for multi-timescale reinforcement learning in the brain.</p><sec id="S2"><title>Computational advantages of multi-timescale learning</title><p id="P7">We first examine the computational advantages of RL agents employing multiple timescales over those utilizing a single timescale. We start with a simple example environment where a cue predicts a future reward at a specific time (<xref ref-type="fig" rid="F1">Fig. 1</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>). In standard RL algorithms, the agent learns to predict future rewards, compressed into a single scalar value, i.e. the sum of discounted future rewards expected from the current state <sup><xref ref-type="bibr" rid="R1">1</xref>,<xref ref-type="bibr" rid="R19">19</xref></sup>: <inline-formula><mml:math id="M1"><mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, where <italic>V</italic>(<italic>s</italic>) is the value of the state <italic>s, r</italic><sub><italic>t</italic></sub> is reward at time <italic>t</italic>, and <italic>γ</italic> is the discount factor (0 &lt; <italic>γ</italic> &lt; 1, see <xref ref-type="sec" rid="S11">Methods</xref>). <italic>E</italic> denotes the expectation over stochasticity in the environment and actions. Let <italic>V</italic><sub><italic>i</italic></sub> be the value learned using a discount <italic>γ</italic><sub><italic>i</italic></sub>. Moving the discount factor <italic>γ</italic> out of the expectation, this equation can be rewritten (truncating at <italic>t</italic> = <italic>T</italic>) as</p><p id="P8">
<disp-formula id="FD1"><label>(1)</label><mml:math id="M2"><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mn>1</mml:mn><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mtext> </mml:mtext><mml:mo>…</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
Where we assume that timesteps transitions are discrete and of size Δ<italic>t</italic> (see <xref ref-type="sec" rid="S11">Methods</xref>). Thus, single-timescale learning projects all the timestep-specific expected rewards (<italic>E</italic>(<italic>r</italic>|<italic>t</italic>)) onto a single scalar (<italic>V</italic><sub><italic>i</italic></sub>) through exponential discounting (<xref ref-type="fig" rid="F1">Fig. 1a</xref>) and therefore entangles reward timing and reward size. When learning with multiple timescales, instead of collapsing all future rewards onto a single scalar, there is vector of value predictions, each computing value with its own discount factor <italic>γ</italic><sub><italic>i</italic></sub>
<sup>45</sup>:
<disp-formula id="FD2"><label>(2)</label><mml:math id="M3"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mspace width="2em"/><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>E</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mrow><mml:mo>|</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
The last equality shows that the array of values learned with multiple discounts (Value space in <xref ref-type="fig" rid="F1">Fig. 1b</xref>) corresponds to the Z-transform (i.e., the discrete Laplace transform) of the array that indicates the expected reward at all future timesteps (Temporal space in <xref ref-type="fig" rid="F1">Fig. 1b</xref>). Since the Z-transform is invertible, the agent employing TD learning with multiple timescales can decode the expected temporal evolution of rewards from the representation of values that it learned, by applying a fixed, regularized decoder <italic>L</italic><sup>−1</sup> to the learned values <sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup> (<xref ref-type="fig" rid="F1">Fig. 1b</xref>, fourth panel illustrates a situation with one reward per trajectory but this approach also work for multiple reward, see <xref ref-type="sec" rid="S11">Methods</xref> and ref [<sup><xref ref-type="bibr" rid="R45">45</xref></sup>]). Intuitively, when learning with multiple timescales, the relative amplitude of the learned cue values as a function of discount factor (Value space in <xref ref-type="fig" rid="F1">Fig. 1b</xref>) depends only on reward timing, and thus the agent can decode reward timing independently of reward magnitude.</p><p id="P9">To illustrate the computational advantages of Laplace-transform multi-timescale agents, we consider several simple example tasks. The agent navigates through a linear track (a sequence of 15 states), where it encounters a reward of a certain magnitude (<italic>R</italic>) at a specific time point (<italic>t</italic><sub><italic>R</italic></sub>, see <xref ref-type="fig" rid="F2">Fig. 2a</xref>). The value of <italic>R</italic> and <italic>t</italic><sub><italic>R</italic></sub> changes across episodes and remains constant within episodes. Each episode is initiated by a cue presented at the initial state (<italic>s</italic>). Within each episode, the agent first learns the expected future rewards (i.e. the value, <italic>V</italic><sub><italic>γ</italic></sub>(<italic>s</italic>)) predicted by the cue using a simple RL algorithm (<italic>N</italic> backups of tabular TD learning) employing one or multiple discount factors. Using the learned values associated with the cue, the agent then performs various tasks, using a deep neural network (DNN) trained across episodes with a policy gradient [PG] method; <xref ref-type="fig" rid="F2">Fig. 2b</xref> and see <xref ref-type="sec" rid="S11">Methods</xref> for details). Therefore, in our model, multi-timescale values are not used directly to produce behavior. Instead, they act as an enriched state representation from which task-specific behavior can be subsequently decoded (similarly to actor-critic and representation learning architectures like distributional RL <sup><xref ref-type="bibr" rid="R41">41</xref></sup>). Our goal is to evaluate the advantages of the multi-timescale value representation over the single-timescale one.</p><sec id="S3"><title>Task 1: disentangling reward timing and reward magnitude</title><p id="P10">We first asked whether an agent can correctly discern the magnitude (<italic>R</italic>) and the timing (<italic>t</italic><sub><italic>R</italic></sub>) of reward separately (<xref ref-type="fig" rid="F2">Fig. 2c</xref>). We vary <italic>R</italic> and <italic>t</italic><sub><italic>R</italic></sub> across episodes. In each episode, the agent learns the values of states using 1, 2 or 3 discount factors. We then train the DNN across episodes to decode the timing of the reward (<italic>t</italic><sub><italic>R</italic></sub>) with the vector of values associated with the cue {<italic>V</italic><sub><italic>γ</italic></sub>(<italic>s</italic>)} as its input. With a single timescale, perfect performance is unattainable: a high value at the cue could signify a small reward in the near future or a large reward in the distant future. In contrast, the pattern of values across discount factors (third panel in <xref ref-type="fig" rid="F1">Fig. 1b</xref>) is invariant to reward magnitude. As a result, multi-timescale agents can disentangle the timing (<italic>t</italic><sub><italic>R</italic></sub>) and the magnitude (<italic>R</italic>) of reward (<xref ref-type="fig" rid="F2">Fig. 2c</xref>, right, <xref ref-type="fig" rid="F6">Extended Data Fig. 1a-c</xref>). Generally, the precision at which the timing and magnitude can be recovered depends on the number of discount factors being used (<xref ref-type="fig" rid="F6">Extended Data Fig. 1a-c,j-l</xref>).</p></sec><sec id="S4"><title>Task 2: learning values with non-exponential temporal discounts</title><p id="P11">While several tasks can be optimally solved by knowing the exponentially discounted state-values (i.e., where the value of a reward at time <italic>t</italic> decreases as <italic>γ</italic><sup><italic>t</italic></sup>), the optimal temporal discount in a specific task depends on its temporal contingencies like its hazard rate, the cost of time and the uncertainty over time <sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup>. Indeed, human and animal judgements are generally more consistent with a hyperbolic discount (i.e., decreasing as 1/(1+<italic>γt</italic>)) than an exponential one <sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>. However, the bootstrapping process of traditional TD value learning naturally converges to exponentially discounted values, so to perform optimally across tasks with arbitrary temporal contingencies, TD-learning agents need to adapt their exponentially discounted values to arbitrary, possibly non-exponential discounts. Crucially, multi-timescale systems encode the expected reward magnitudes at all future times (<italic>E</italic>[<italic>r</italic>|<italic>t</italic> = 0], <italic>E</italic>[<italic>r</italic>|<italic>t</italic> = Δ<italic>t</italic>], <italic>E</italic>[<italic>r</italic>|<italic>t</italic> = 2Δ<italic>t</italic>], …) in the inverse temporal Laplace space (i.e., after transforming the multi-timescale value estimates with <italic>L</italic><sup>−1</sup>, see <xref ref-type="fig" rid="F1">Fig. 1b</xref>). Consequently, they could weight the time-specific expected rewards with any chosen discount weights (e.g.<italic>w</italic><sub>0</sub><italic>E</italic>[<italic>r</italic>|<italic>t</italic> = 0] + <italic>w</italic><sub>1</sub><italic>E</italic>[<italic>r</italic>|<italic>t</italic> = Δ<italic>t</italic>] + ⋯) to retrieve the specific discount necessitated by the task. We demonstrate this in a task where the agent goal is to report the value of the initial state (<italic>s</italic>) using a hyperbolic discount (i.e. the value of a reward <italic>R</italic> at time <italic>t</italic><sub><italic>R</italic></sub> is <italic>R</italic> / (1+0.9<italic>t</italic><sub><italic>R</italic></sub>)). With a single timescale, the learned exponentially discounted value cannot be accurately adapted into a hyperbolic one, but multi-timescale systems can reliably report the hyperbolic value of the cue given a diversity of exponential ones (<xref ref-type="fig" rid="F2">Fig. 2d</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig. 1d-f</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>).</p></sec><sec id="S5"><title>Task 3: inferring temporal information before convergence</title><p id="P12">In the above example (<xref ref-type="fig" rid="F2">Fig. 2c</xref>), we showed that multi-timescale agents can disentangle the timing and the magnitude of rewards, which are typically intertwined in agents that rely on a single discount factor. This occurs because the shape of value function across discount factors encodes the proximity to rewards (<xref ref-type="fig" rid="F1">Fig. 1b</xref>, third panel). We further hypothesized that, multi-timescale agents can leverage this advantage of extracting timing information even before value learning has fully converged. Consider an agent that has encountered a reward only a limited number of times (<italic>N</italic>). For single-timescale systems, a high value of the cue could be due to a short delay (<italic>t</italic><sub><italic>R</italic></sub>) or simply because the value estimate has undergone more positive updates from an initial value of 0. In contrast, the shape of values encoded <italic>across</italic> discount factors is invariant to the number of reward encounters (<italic>N</italic>), to the extent that all value estimates depart from similar baselines and share similar learning parameters. As a result, multi-timescale agents can decode the time of reward (<italic>t</italic><sub><italic>R</italic></sub>) even in situations where learning is incomplete (<xref ref-type="fig" rid="F2">Fig. 2e</xref>, <xref ref-type="fig" rid="F6">Extended Data Figs. 1g-i</xref> and <xref ref-type="fig" rid="F7">2</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>).</p></sec><sec id="S6"><title>Task 4: state-dependent discount factor</title><p id="P13">Moreover, multi-timescale systems can preferentially adjust between myopic and farsighted perspectives based on the present circumstances. Consider a slightly more intricate maze with two branching points (<xref ref-type="fig" rid="F2">Fig. 2f</xref>). In this maze, each state is associated with a random reward drawn uniformly between –0.5 and 0.5, except for two states (<italic>s</italic> and <italic>s’</italic>, orange circles) which result in a deterministic reward of 1. The optimal strategy in this scenario is to move upwards at both states <italic>s</italic> and s’, we define performance as the fraction of optimal choices across episodes. When learning from a limited number of experiences, the smaller stochastic rewards can overpower the larger deterministic rewards, making it challenging to achieve optimal performance. At state <italic>s</italic>, only far-sighted agents can discern the significance of the large deterministic rewards, thereby causing myopic agents to perform near chance at <italic>s</italic>. At state <italic>s</italic>’, the situation is reversed. Far-sighted agents not only integrate the close-by large reward but also all the stochastic rewards farther in the future. Myopic agents, in contrast, assign greater weight to the reward of 1 compared to the future stochastic rewards, thus enabling optimal performance at <italic>s’</italic>. Therefore, only agents that could dynamically adapt between being far-sighted at <italic>s</italic> and myopic at <italic>s’</italic> can attain optimal performance when learning from limited experiences. Indeed, the multi-timescale of <xref ref-type="fig" rid="F2">Fig. 2b</xref> achieves in this task a maximum performance of 83±1% with a single discount and a performance of 94±1% with two discounts. The superior performance is due to its demonstrated ability to discern the temporal distance to the relevant events in the environment (here, the large deterministic rewards), and subsequently focus on the myopic or far-sighted values depending on the estimated distance. We also observe the benefits of the myopic learning bias in more realistic navigation scenarios (<xref ref-type="fig" rid="F6">Extended Data Figs. 1m-o</xref> and <xref ref-type="fig" rid="F8">3</xref>) as well as in more complex Deep RL settings where additional timescales act as auxiliary tasks (<xref ref-type="fig" rid="F2">Fig. 2g</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>).</p><p id="P14">To summarize, in multi-timescale value systems the vectorized learning signal robustly contains temporal information independently of the information about reward magnitude. This property empowers agents to selectively focus on either myopic or far-sighted estimates depending on the current situation.</p></sec></sec><sec id="S7"><title>The diversity of discount factors across dopamine neurons conveys distributional information about the timing of future rewards</title><p id="P15">In the previous section, we demonstrated the computational advantages of learning with multiple discount factors for an RL agent. Building upon these findings, we next investigated whether the brain employs such multi-timescale RL. Toward this goal, we examined the activity of dopamine neurons, which are believed to encode the TD error term in RL algorithms.</p><p id="P16">To characterize the discounting properties of individual dopaminergic neurons, mice were trained in a cued delay task <sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R63">63</xref></sup> in which on a given trial, one out of four distinct odor cues indicated its associated timing of a water reward (<xref ref-type="fig" rid="F3">Fig. 3a</xref>). These odor cues were preceded by a trial start cue (green computer screen) by 1.25s. The trial start cue reduced the timing uncertainty of the odor cue and ensured that the responses of dopaminergic neurons to the odor cues were mostly driven by a valuation signal rather than a saliency signal <sup><xref ref-type="bibr" rid="R64">64</xref>,<xref ref-type="bibr" rid="R65">65</xref></sup>. Mice showed anticipatory licking prior to reward delivery. Importantly, the onset of the anticipatory licking was delayed for trials with cues predicting longer reward delays, indicating that the mice learned the delay contingencies (<xref ref-type="fig" rid="F3">Fig. 3b</xref>). We recorded optogenetically identified single dopamine neurons in the ventral tegmental area (VTA) (<italic>n</italic> = 78, see <xref ref-type="sec" rid="S11">Methods</xref>). We focused our analysis on neurons (<italic>n</italic> = 50) who passed the selection criteria (including mean cue response firing rate above 2 spikes/s, positive goodness of fit on test data, see <xref ref-type="sec" rid="S11">Methods</xref>). As expected from RL theory and the prediction error framework, the average responses to the reward cue decreased as the predicted reward timing increased <sup><xref ref-type="bibr" rid="R50">50</xref>,<xref ref-type="bibr" rid="R63">63</xref></sup>(<xref ref-type="fig" rid="F3">Fig. 3c</xref>, <xref ref-type="fig" rid="F9">Extended Data Fig. 4a-b</xref>). However, cue responses of individual neurons showed a great diversity of discounting across the reward delays ranging from neurons responding strongly only to the cue indicating the shortest delay to neurons with a gradual decay of their response with cued reward delay (<xref ref-type="fig" rid="F3">Fig. 3d-e</xref>).</p><p id="P17">To characterize the discount properties of individual neurons, we fit them individually using both an exponential discount model and a hyperbolic discount model. The exponential model provided a better fit to the neurons’ responses than the hyperbolic model (<italic>P</italic> = 2.2 x 10<sup>-5</sup>, two-tailed <italic>t</italic>-test; <xref ref-type="fig" rid="F3">Fig. 3f</xref> and <xref ref-type="fig" rid="F9">Extended Data Fig. 4c-e</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>) contrary to a previous observation in non-human primates <sup><xref ref-type="bibr" rid="R63">63</xref></sup>. Organism level hyperbolic-like discounting can, therefore, arise from the diversity of exponential discounting in single neurons, as discussed above with artificial agents (<xref ref-type="fig" rid="F2">Fig. 2d</xref>, see also refs [<sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R60">60</xref></sup>]). This view is consistent with the wide distribution of inferred discount factors obtained across the population (0.56 ± 0.21 s<sup>-1</sup>, mean ± s.d., <xref ref-type="fig" rid="F3">Fig. 3g</xref>). Fits to simulated data suggest that our estimate of inferred parameters is robust and primarily constrained by the number of trials (<xref ref-type="fig" rid="F9">Extended Data Fig. 4f-h</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>).</p><p id="P18">As we have shown above, artificial agents equipped with diverse discount factors exhibit various advantages. One key aspect contributing to these advantages is their unique ability to independently extract reward timing information, which is lacking in single timescale agents. We next asked whether dopamine neurons provide a population code in which the structured heterogeneity across the population enables decoding of reward timing or the expected reward across time, <italic>E</italic>(<italic>r</italic>|<italic>t</italic>). Mathematically, this transformation can be achieved by the inverse Laplace transform (or its discrete equivalent the Z-transform, <xref ref-type="fig" rid="F3">Fig. 3j</xref>) <sup><xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R57">57</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup>. In our data set, the dopaminergic cue responses for each reward delay exhibited unique shapes as a function of discount factors, suggesting that reward timing information is embedded in the dopaminergic population responses (<xref ref-type="fig" rid="F3">Fig. 3h</xref>, compare with <xref ref-type="fig" rid="F1">Fig. 1b</xref>, third panel). The temporal horizon across the population, which underlies these cue responses, can be visualized through the discount matrix which indicates for each neuron the relative value of a future reward depending on the inferred discount factor (<xref ref-type="fig" rid="F3">Fig. 3i</xref>).</p><p id="P19">If the dopaminergic population code is consistent with the Laplace code explored above (<xref ref-type="fig" rid="F1">Fig. 1</xref>-<xref ref-type="fig" rid="F2">2</xref>), reward timing should be recoverable from the dopamine neurons’ cue responses with a regularized discrete inverse Laplace transform of the neural activity (which does not require training a decoder). In our task, we can use the TD-error driven cue responses (instead of the value in <xref ref-type="disp-formula" rid="FD2">equation 2</xref>) as they are driven by the discounted future value (<inline-formula><mml:math id="M4"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mo> </mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:math></inline-formula>, see <xref ref-type="sec" rid="S11">Methods</xref>). This implies that the right-hand side of <xref ref-type="disp-formula" rid="FD2">equation 2</xref> can be approximated by the population dopamine responses. We used a pseudo-inverse of the discount matrix (computed using half of all trials) based on regularized singular value decomposition to approximate the inverse Laplace transform (<xref ref-type="fig" rid="F3">Fig. 3j</xref>, <xref ref-type="fig" rid="F10">Extended Data Fig. 5a-d</xref>, see <xref ref-type="sec" rid="S11">Methods</xref> and ref[<sup><xref ref-type="bibr" rid="R45">45</xref></sup>]) and applied it to dopamine neuron cue responses (computed on the held out half of the trials). Remarkably, the decoder was able to predict reward timing, closely matching the true reward delay (<xref ref-type="fig" rid="F3">Fig. 3k</xref>, top row). This prediction was lost if we shuffled the neuron identities indicating that it is not a generic property of the discount matrix (<xref ref-type="fig" rid="F3">Fig. 3k</xref>, bottom row). We quantified this decoding by computing a distance metric (using 1-Wasserstein distance) between the true and predicted reward delay across conditions (<italic>P</italic> = 1.2 x 10<sup>-4</sup> for 0.6 s reward delay, <italic>P</italic> &lt; 1.0 x 10<sup>-20</sup> for the other delays, one-tailed Wilcoxon signed rank test; <xref ref-type="fig" rid="F10">Extended Data Fig. 5e</xref>, see <xref ref-type="sec" rid="S11">Methods</xref>). Predictions from the model were more accurate than an alternative model with a single discount factor (<italic>P</italic><sub><italic>t</italic> = 0.6s</sub> = 1, <italic>P</italic><sub><italic>t</italic> = 1.5s</sub> &lt; 1.0 x 10<sup>-31</sup>, <italic>P</italic><sub><italic>t</italic> = 3.75s</sub> = 0.0135, <italic>P</italic><sub><italic>t</italic> = 9.375s</sub> &lt; 1.0 x 10<sup>-14</sup>, one-tailed Wilcoxon signed rank test; <xref ref-type="fig" rid="F10">Extended Data Fig. 5f-g</xref> and see <xref ref-type="sec" rid="S11">Methods</xref>). Consistent with the above observation that cue responses were fit better with exponential over hyperbolic discounting models, the accuracy of reward timing decoding was typically higher when using the discount matrix from the exponential model than the one from the hyperbolic model (<italic>P</italic><sub><italic>t</italic> = 0.6s</sub> = 1, <italic>P</italic><sub><italic>t</italic> = 1.5s</sub> &lt; 1.0 x 10<sup>-31</sup>, <italic>P</italic><sub><italic>t</italic> = 3.75s</sub> &lt; 1.0 x 10<sup>-33</sup>, <italic>P</italic><sub><italic>t</italic> = 9.375s</sub> &lt; 1.0 x 10<sup>-3</sup>, one-tailed Wilcoxon signed rank test; <xref ref-type="fig" rid="F11">Extended Data Fig. 6a-e</xref>). Furthermore, the decoding performance was comparable to simulated data with matched trial numbers, indicating that the remaining uncertainty in decoded reward timing is primarily driven by limited sample size in the data (e.g., the number of neurons and the number of trials per condition, <xref ref-type="fig" rid="F11">Extended Data Fig. 6f-g</xref> and see <xref ref-type="sec" rid="S11">Methods</xref>).</p><p id="P20">Together these results establish that dopamine neurons compute prediction errors with a heterogeneity of discount factors and show that the structure in this heterogeneity can be exploited by downstream circuits to decode reward timing.</p></sec><sec id="S8"><title>Heterogeneity of discount factors explains diverse ramping activity across dopamine neurons</title><p id="P21">In the task above (<xref ref-type="fig" rid="F3">Fig. 3</xref>), prediction errors in dopamine neurons were measured through discrete transitions in the value functions at the time of cue. In more naturalistic environments, value might change more smoothly, for example when an animal approaches a goal <sup><xref ref-type="bibr" rid="R66">66</xref></sup>. In these tasks, ramps in dopaminergic signaling have been initially interpreted as quantifying value functions <sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R66">66</xref></sup> but were recently shown to conform to the predictions of the TD learning model. Specifically, these ramps can be understood as moment-by-moment changes in values or as TD error along an increasingly convex value function in which the derivative is also increasing <sup><xref ref-type="bibr" rid="R67">67</xref>–<xref ref-type="bibr" rid="R69">69</xref></sup>. Here we show that some of this heterogeneity can be understood as evidence for multi-timescale RL across dopamine neurons.</p><p id="P22">We analyzed the activity of optogenetically identified dopamine neurons (<italic>n</italic> = 90, see <xref ref-type="sec" rid="S11">Methods</xref> and ref [<sup><xref ref-type="bibr" rid="R68">68</xref></sup>]) while mice traversed along a linear track in virtual reality (VR). Although mice were free to locomote, their movements did not affect the dynamics of the scene (see <xref ref-type="sec" rid="S11">Methods</xref> and ref [<sup><xref ref-type="bibr" rid="R68">68</xref></sup>] for details). At trial onset, a linear track appeared, the scene moved at continuous speed and reward was delivered around 7.35 seconds after motion onset (<xref ref-type="fig" rid="F4">Fig. 4a</xref>). The slope of ramping across neurons was on average positive (<xref ref-type="fig" rid="F4">Fig. 4b-c</xref>) but single neurons exhibited a diversity of ramping activity (<xref ref-type="fig" rid="F4">Fig. 4c-e</xref>) ranging from monotonic upward and downward ramps to non-monotonic ramps.</p><p id="P23">We hypothesized that this seemingly puzzling heterogeneity can be understood as a signature of multi timescale reinforcement learning. Considering that the value function is set by the limits on the precision of internal timing mechanisms and the reduction in uncertainty due to visual feedback <sup><xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R70">70</xref></sup>, we first assume that heterogeneous dopamine neurons contribute to learning a common model of the environment and therefore share a common value function (see <xref ref-type="sec" rid="S11">Methods</xref>). Depending on the shape of this value function, governed by the statistics of the environment being learned, the TD error from neurons with different discount factors will exhibit different type of activity ramps. At a given time, the sign of the TD error will depend on the relative scale of the upcoming increase in value and the reduction of this future value due to discounting. Given an increase in value <italic>1/γ</italic><sub><italic>o</italic></sub> (with <italic>γ</italic><sub><italic>o</italic></sub> &lt; 1<italic>)</italic> a neuron with a discount factor smaller, equal or larger than <italic>γ</italic><sub><italic>o</italic></sub> <italic>w</italic>ill experience a negative, zero or positive TD error respectively (see <xref ref-type="fig" rid="F12">Extended Data Fig. 7a</xref> and <xref ref-type="sec" rid="S11">Methods</xref>). For an exponential value function (<xref ref-type="fig" rid="F4">Fig. 4g</xref>, left panel), where the value increases by a fixed factor <inline-formula><mml:math id="M5"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula> at every timestep, a neuron with discount factor <italic>γ</italic><sub><italic>o</italic></sub> will have no TD error during the entire visual scene (red line, <xref ref-type="fig" rid="F4">Fig. 4f,g</xref>). A neuron with a higher (or lower) discount factor than <italic>γ</italic><sub><italic>o</italic></sub> will experience an upward (or downward) monotonic ramp in its activity (darker and lighter red line in <xref ref-type="fig" rid="F4">Fig. 4f-g</xref> respectively). However, if the value function is non-exponential (for example cubic as a function of distance to reward, <xref ref-type="fig" rid="F4">Fig. 4h</xref>, left panel), there will not be a neuron whose discount factor is able to match the increases in value function at all timesteps. Neurons with high or low discount factors will still ramp upwards or downwards (darker and lighter red line in <xref ref-type="fig" rid="F4">Fig. 4h-i</xref> respectively), but neurons with intermediate discount factors will exhibit non-monotonic ramping (red line, <xref ref-type="fig" rid="F4">Fig. 4h-i</xref>) as observed in the neural data.</p><p id="P24">To fit this model to the dopaminergic neurons, we used a bootstrapped constrained optimization procedure on a continuous formulation of the TD error <sup><xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R71">71</xref></sup> (see <xref ref-type="sec" rid="S11">Methods</xref>) by fitting a non-parametric common value function and neuron-specific gains, baselines and discount factors. Although the gain and baseline activity scale the range of activity, only the interaction between the value function and the discount factor affects the shape of the TD error across time (see <xref ref-type="sec" rid="S11">Methods</xref>). The heterogeneity of ramping activity across single neurons is explained (<xref ref-type="fig" rid="F4">Fig. 4l-m</xref>) by a common convex value function (<xref ref-type="fig" rid="F4">Fig. 4j</xref>) and a diversity of discount factors across single neurons (<xref ref-type="fig" rid="F4">Fig. 4k</xref>). We did not observe a significant correlation between inferred parameters and the medio-lateral position of the implanted electrodes (<xref ref-type="fig" rid="F12">Extended Data Fig. 7b-d</xref>). So far, we proposed a descriptive model with a common value function across neurons suggesting that single neurons predictions errors are pooled to create a single value function and world model. Recent models for distributed prediction errors across dopamine neurons have instead used parallel loops where individual neurons contribute to estimating sperate value functions <sup>25,45,72–75</sup>. Instead of a common value function, the dopamine neurons can be part of independent loops and share a common expectation of reward timing. We obtained similar results in this common reward expectation model (see <xref ref-type="sec" rid="S11">Methods</xref> and <xref ref-type="fig" rid="F13">Extended Data Fig. 8</xref>).</p><p id="P25">Together these results show that diversity in slow changes in activity across single neuron (known as dopamine ramps) in environments with gradual changes in value can be explained by a diversity of discount factors and is a signature of multi-timescale reinforcement learning.</p></sec><sec id="S9"><title>Inferred discount factors for single neurons are correlated across the two behavioral tasks</title><p id="P26">Distributional RL and other distributed RL formulations provide agents with greater flexibility as they allow agents to adapt risk sensitivity and discounting to the statistics of the environment <sup><xref ref-type="bibr" rid="R41">41</xref>,<xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R73">73</xref></sup>. However, they leave open the question of the biological implementation of this adaptivity. Specifically, the tuning of single dopamine neurons, controlled by the sensitivity to reward size or the discount factor, could be either a circuit property and therefore task and context specific or it could be a cell-specific property, with the contribution of different neurons recruited according to task demands. However, measurements of tuning diversity at the single neuron level are usually done in a single behavioral task <sup><xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R76">76</xref></sup>, leaving open the question of this implementation across contexts.</p><p id="P27">Here, we characterized discount factors across two behavioral tasks and a subset (<italic>n</italic> = 43) of the single neurons analyzed above (<xref ref-type="fig" rid="F3">Figures 3</xref> and <xref ref-type="fig" rid="F4">4</xref>) were recorded on the same day in both behavioral tasks. Using this data set, we found that the discount factors inferred independently across the two behavioral tasks are correlated (<xref ref-type="fig" rid="F5">Fig. 5a-b</xref>). Furthermore, in the cued delay task, we were able to decode subjective reward timing from population cue responses using the discount matrix built from the discount factors inferred in the virtual reality task (<italic>P</italic><sub><italic>t</italic> = 0.6s</sub> = 1, <italic>P</italic><sub><italic>t</italic> = 1.5s</sub> &lt; 1.1 x 10<sup>-20</sup>, <italic>P</italic><sub><italic>t</italic> = 3.75s</sub> &lt; 3.8 x 10<sup>-20</sup>, <italic>P</italic><sub><italic>t</italic> = 9.375s</sub> &lt; 2.9 x 10<sup>-5</sup>, compared to shuffled data, <xref ref-type="fig" rid="F14">Extended Data Fig. 9</xref> and see <xref ref-type="sec" rid="S11">Methods</xref>). These results suggest that the discount factor (or its ranking) is a cell-specific property and strongly constrains the biological implementation of multi-timescale reinforcement learning in the brain.</p></sec></sec><sec id="S10" sec-type="discussion"><title>Discussion</title><p id="P28">In this work, we have analyzed the unique computational benefits of multi-timescale reinforcement learning agents and shown that we can explain multiple aspects of the activity of dopaminergic neurons through that lens.</p><p id="P29">The understanding of dopaminergic neurons as computing a reward prediction error from TD reinforcement learning algorithms has transformed our understanding of their function. However, recent experimental work expanding the anatomical locations of recordings and the task designs has shown heterogeneity in dopamine responses that is not readily explained within the canonical TD framework <sup><xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R66">66</xref>,<xref ref-type="bibr" rid="R77">77</xref>,<xref ref-type="bibr" rid="R78">78</xref></sup>. However, a number of these seemingly anomalous findings can be reconciled and integrated within extensions of the RL framework, further reinforcing the power and versatility of the TD theory in capturing the intricacies of brain learning mechanisms <sup><xref ref-type="bibr" rid="R24">24</xref>,<xref ref-type="bibr" rid="R25">25</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R45">45</xref>,<xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R72">72</xref>,<xref ref-type="bibr" rid="R74">74</xref>,<xref ref-type="bibr" rid="R75">75</xref>,<xref ref-type="bibr" rid="R79">79</xref></sup>. In this work, we reveal an additional source of dopaminergic heterogeneity: they encode prediction errors across multiple timescales. Together, these results indicate that at least some of the heterogeneity observed in dopamine responses reflects variations in key parameters within the RL framework. Thus, these results indicate that the dopamine system employs “parameterized vector prediction errors”, including a discrete Laplace transform of the future temporal evolution of the reward function, allowing for the learning and representation of richer information than what can be achieved with scalar prediction errors in the traditional RL framework.</p><p id="P30">The constraint on the anatomical implementation of multi-timescale RL suggested by the alignment of discount factors between the two tasks could also inform algorithm design. Adapting the discount factor has been used to improve performance in several algorithms, with proposed methods ranging from meta-learning an optimal discount factor <sup><xref ref-type="bibr" rid="R80">80</xref></sup>, learning state dependent discount factors <sup><xref ref-type="bibr" rid="R81">81</xref>,<xref ref-type="bibr" rid="R82">82</xref></sup>, or combining parallel exponentially discounting agents <sup><xref ref-type="bibr" rid="R55">55</xref>,<xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R61">61</xref></sup>. Our results provide evidence supporting the third model but the recruitment mechanisms of the neurons to adapt the global discounting function with task or context and the link between anatomical location and discounting<sup><xref ref-type="bibr" rid="R53">53</xref></sup> remain open questions. Similarly, the contribution of this vectorized error signal on the downstream temporal representations<sup><xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R51">51</xref></sup> remains to be explored.</p><p id="P31">Understanding how this recruitment occurs will be a key step towards a mechanistic understanding of the contribution of this timescale diversity to calibration and miscalibration in intertemporal choices. There has been a conundrum that RL theories use exponential discounting while humans and animals often exhibit hyperbolic discounting. A previous study, that examined discounting in dopamine neurons, argued that single dopamine neurons exhibit hyperbolic discounting <sup><xref ref-type="bibr" rid="R63">63</xref></sup>. However, they used uncued reward responses for zero reward delay, likely biasing the estimate toward hyperbolic (as responses to unpredicted rewards are typically large and potentially contaminated by salience signals). In contrast, our data are consistent with exponential discounting at the level of single neurons, suggesting that RL machinery defined by each dopamine neuron conforms to the rules of a simple RL algorithm. Hyperbolic-like discounting can occur when these diverse exponential discounting are combined at the organism level <sup><xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R36">36</xref>,<xref ref-type="bibr" rid="R55">55</xref></sup>. More generally, the relative contribution of multiple timescales to the global computation governs the discount function at the organism level and should be calibrated to the uncertainty in the hazard rate of the environment <sup><xref ref-type="bibr" rid="R14">14</xref></sup>.</p><p id="P32">Appropriately recruiting the heterogeneity of discount factors is therefore important to adapt to the temporal uncertainty of the environment. This view draws parallels with the distributional RL hypothesis that naturally fits with current work on anhedonia as a miscalibration of optimism and pessimism can lead to biases in the learned value <sup><xref ref-type="bibr" rid="R25">25</xref></sup>. Miscalibration of the discounting spectrum can lead to excessive patience or impulsivity. A bias in this distribution due to genetical, developmental or transcriptional factors could bias the learning at the level of the organism towards short- or long-term goals. Behaviorally such bias would manifest itself as an apparent impulsivity or lack of motivation, leading to a potential mechanistic interpretation of these maladaptive behaviors. Similarly, this view could guide the design of algorithms that recruit and leverage these adaptive temporal predictions.</p><p id="P33">Our study establishes a new paradigm to understand the functional role of prediction error computation in dopaminergic neurons and opens new avenues to develop mechanistic explanations for deficits in intertemporal choice in disease and inspire the design of new algorithms.</p></sec><sec id="S11" sec-type="methods"><title>Methods</title><sec id="S12"><title>Animal care and surgical procedures</title><p id="P34">The mouse behavioral and electrophysiological data presented here was collected as part of a previous study where all experimental procedures are described in details <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. As described in this study, all procedures were performed in accordance with the National Institutes of Health Guide for the Care and Use of Laboratory Animals and approved by the Harvard Animal Care and Use Committee.</p><p id="P35">We used a total of 13 adult C57/BL6J DAT-Cre male mice. Mice were backcrossed for over 5 generations with C57/BL6J mice, Animals were singly housed after surgery on a reverse 12 hr dark/12 hr light cycle (dark from 7am to 7pm). Single dopaminergic neurons were optogenetically identified using custom built micro drives with 8 tetrodes and an optical fiber as described in our previous study <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. Significance was assessed using the stimulus associated spike latency test (SALT) <sup><xref ref-type="bibr" rid="R83">83</xref></sup>.</p><p id="P36">All mice (<italic>n</italic> = 13) were used in the virtual reality task and 8 of those were also used in the cued delay task. The targeted medio-lateral (ML) location varied from 320μm to 1048μm for neurons recorded in the virtuality task and for neurons recorded in the cued delay task. Neurons recorded at ML position &gt; 900μm were excluded from the analysis as they were considered to be in the substantia nigra pars compacta (SNc).</p></sec><sec id="S13"><title>Reinforcement learning at multiple timescales</title><p id="P37">In standard reinforcement learning, the value of a state <italic>s</italic> under a given policy <italic>π</italic> is defined as the expected sum of discounted future rewards:
<disp-formula id="FD3"><label>(3)</label><mml:math id="M6"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
The discount factor <italic>γ</italic> (whose value is between 0 and 1) is a fixed factor at each time step devaluating future rewards. This exponentially functional form for the temporal discount is not arbitrary. This temporal discount is naturally produced by the TD learning rule, a bootstrapping mechanism that updates the value estimates using the experienced transition from <italic>s</italic> to <italic>s</italic><sup>′</sup> with reward <italic>r</italic> :
<disp-formula id="FD4"><label>(4)</label><mml:math id="M7"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>←</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>V</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>α</italic> is the learning rate. This update process converges to the values defined above under very general conditions <sup><xref ref-type="bibr" rid="R19">19</xref></sup> and has been experimentally proven to be an extremely robust and efficient learning rule for Deep RL systems <sup><xref ref-type="bibr" rid="R22">22</xref>,<xref ref-type="bibr" rid="R84">84</xref></sup>.</p><p id="P38">After convergence, the value <italic>V</italic>(<italic>s</italic>) can be rewritten by taking the sum and the discount factor outside of the expectation:
<disp-formula id="FD5"><label>(5)</label><mml:math id="M8"><mml:msub><mml:mi>V</mml:mi><mml:mi>γ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
Where we have added a <italic>γ</italic> subscript to the value to indicate that the value is computed for that particular discount, and we have omitted the dependence of the expectation on <italic>π</italic> for simplicity. This last expression reveals a very useful property: <italic>V</italic><sub><italic>γ</italic></sub>(<italic>s</italic>), as a function of the discount <italic>γ</italic> ∈ (0,1), is the unilateral Z-transform of <italic>E</italic>[<italic>r</italic><sub><italic>t</italic></sub>|<italic>s</italic>] as a function of future time <italic>t</italic> ∈ (0, ∞), with real-valued parameter <italic>γ</italic><sup>−1</sup> (i.e., the discrete-time equivalent of the Laplace transform<sup><xref ref-type="bibr" rid="R85">85</xref></sup>). Since the Z-transform is invertible, in the limit of computing values with an infinite amount of <italic>γ</italic>’s, the agent can recover the expected rewards at all future times <inline-formula><mml:math id="M9"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup></mml:math></inline-formula> from the set of learned values {<italic>V</italic> <sub><italic>γ</italic></sub> (<italic>s</italic>)}<sub><italic>γ</italic>∈(0,1</sub>) :
<disp-formula id="FD6"><label>(6)</label><mml:math id="M10"><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>γ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup></mml:math></disp-formula>
Thus, if the agent performs TD learning with an infinite amount of discounts, the converging points of the TD backups would encode not only the expected sum of discounted rewards, as in traditional RL, but also the <italic>expected reward at all future timesteps</italic>, though the latter lies in a different space, analogous to the frequency and temporal spaces of the Fourier transform.</p></sec><sec id="S14"><title>Decoding Tasks</title><p id="P39">The three tasks in <xref ref-type="fig" rid="F2">Fig. 2c-e</xref> were designed with a similar structure. In the three tasks, the policy gradient (PG) network is composed of 2 Fully Connected layers of 32 units each, separated by ReLU nonlinearities. The PG network receives in its input the values learned by TD-learning and reports in its output the corresponding estimate for each task. Values were learned using tabular TD-learning as indicated in the previous section. In <xref ref-type="fig" rid="F2">Fig. 2c-e</xref> and <xref ref-type="fig" rid="F6">Extended Data Fig. 1</xref>, the PG network was trained across 1,000 episodes. The precise structure of each episode depends on the task (see details below). In general, in each episode the agent learns values from scratch using TD-learning for a specific experimental condition (i.e. a Markov decision process, or MDP), and the PG network maximizes its reporting performance across episodes. Thus, for each episode <italic>i</italic>, the policy (<italic>π</italic><sub><italic>θ</italic></sub>) is a map from the learned multi-timescale values <inline-formula><mml:math id="M11"><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> to actions (<italic>a</italic><sub><italic>i</italic></sub>). The parameters (<italic>θ</italic>) of the PG network are optimized to maximize reporting accuracy across episodes (the specific measure to report depends on the experimental condition). The parameters were learned by optimizing the traditional policy gradient loss, using an Adam optimizer with a learning rate of 0.001 to maximize the task-specific expected return <italic>J</italic>(<italic>π</italic><sub><italic>θ</italic></sub>) of the policy <italic>π</italic><sub><italic>θ</italic></sub>:
<disp-formula id="FD7"><label>(7)</label><mml:math id="M12"><mml:msub><mml:mo>∇</mml:mo><mml:mi>θ</mml:mi></mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mo>~</mml:mo><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mi>log</mml:mi><mml:msub><mml:mi>π</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>B</italic> is a batch of <italic>N=100</italic> episodes and C<sub><italic>i</italic></sub> is a reinforcement learning binary signal indicating whether the report (<italic>a</italic><sub><italic>i</italic></sub>, the output of the network) was correct or incorrect for episode <italic>i</italic>, given the learned multi-timescale values <inline-formula><mml:math id="M13"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>. To tackle the exploration-exploitation problem we extend the policy using <italic>ϵ</italic>-greedy, with <italic>ϵ</italic> = 0.3 (performance is reported with <italic>ϵ</italic> = 0).</p><p id="P40">In Task 1 (<xref ref-type="fig" rid="F2">Fig. 2c</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig. 1a-c</xref>), in each episode a discrete reward time <italic>t</italic><sub><italic>R</italic></sub> is sampled between 1 and 15 and a discrete reward magnitude R sampled between 1 and 15. This defines a Markov Decision Process (MDP) shown in <xref ref-type="fig" rid="F6">Extended Data Fig. 1a</xref>. For this MDP, TD-learning was used to learn the value of the first state of the MDP <italic>s</italic>, which we will refer to as the “cue”. In all tasks, the value of the cue was learned using one, two or three discount factors (γ) from the set {0.6,0.9,0.99}, depending on the experimental condition. The results indicated as ‘Three γ’ corresponds to the discount factors [0.6,0.9,0.99]. Since there is noise in the simulation (see below), the results indicated as ‘One γ’ corresponds to the top performer over three identical discount factors ([0.6,0.6,0.6], [0.9,0.9,0.9], [0.99,0.99,0.99]) and analogously for the results indicated as ‘Two γ’. After performing TD-learning, the values are fed as input into the PG network whose output is the guessed reward time (the network has 15 discrete actions, corresponding to reporting reward times from 1 to 15). Performance was evaluated as the fraction of correct responses across test episodes (1 for estimating the correct reward time, 0 otherwise). We show the performance of the PG network as it is trained in <xref ref-type="fig" rid="F6">Extended Data Fig. 1c</xref>. In <xref ref-type="fig" rid="F6">Extended Data Fig. 1j-l</xref> we show a similar experiment but using two reward times and reward magnitudes in the MDP.</p><p id="P41">In Task 2 (<xref ref-type="fig" rid="F2">Fig. 2d</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig. 1d-f</xref>), the structure of each episode was as in Task 1 but with a discrete reward time <italic>t</italic><sub><italic>R</italic></sub> sampled between 1 and 8 and a discrete reward magnitude <italic>R</italic> sampled between 1 and 4. The learned values were input into a PG network with 32 possible discrete outputs, representing the 32 possible hyperbolic values obtained in all the possible experiment (4 possible reward magnitudes × 8 possible reward times):
<disp-formula id="FD8"><label>(8)</label><mml:math id="M14"><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mn>0.9</mml:mn><mml:msub><mml:mi>t</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:math></disp-formula>
Performance was evaluated as the fraction of correct responses across episodes.</p><p id="P42">In Task 3 (<xref ref-type="fig" rid="F2">Fig. 2e</xref>, <xref ref-type="fig" rid="F6">Extended Data Fig. 1g-i</xref>) we use the MDP shown in <xref ref-type="fig" rid="F6">Extended Data Fig. 1g</xref> while keeping <italic>R</italic> fixed at 1 but varying <italic>t</italic><sub><italic>R</italic></sub> and the number of times (<italic>N</italic>) that the full MDP has been experienced by the agents. Since TD-backups are performed online after every transition, <italic>N</italic> is proportional to the total number of TD-backups. The (possibly incomplete) learned values at <italic>s</italic> from these <italic>N</italic> experiences were fed into the PG network (<xref ref-type="fig" rid="F6">Extended Data Fig. 1h</xref>) which was trained across episodes to optimize the reporting performance of <italic>t</italic><sub><italic>R</italic></sub>.</p><p id="P43">We also evaluate learning in incomplete-information situations using the MDP shown in <xref ref-type="fig" rid="F6">Extended Data Fig. 1m-o</xref>. In each episode, the length of the two branches is uniformly sampled from 5 to 15 (if they are the same, they are re-sampled until being different). Thus, in each episode, there is a shorter branch and a longer branch. Each branch is experienced a random number of times (<italic>N</italic>) sampled from a uniform distribution with the range of 1 and 99 [denoted by Uniform(1,99)]. Thus, the number of TD backups performed for the two branches could be highly asymmetric. The learned values (with one or multiple discounts) were fed as input into the PG network with a binary output indicating which path was the shortest one, performance was evaluated as the fraction of correct responses (<xref ref-type="fig" rid="F6">Extended Data Fig. 1o</xref>). Single-timescale agents can incorrectly believe that one branch is shorter than the other one if it has been experienced more often, but multi-timescale agents can determine the distance to the reward independently of the asymmetric experience.</p><p id="P44">In all tasks, the TD-learning process was corrupted by noise. In each episode, the learning rate was sampled from a normal distribution with mean of 0.1 and variance of 0.001 [denoted by <italic>𝒩</italic>(0.1,0.001)] and the number of TD backups was sampled from Uniform(59,99) (except in the tasks with incomplete learning, e.g. <xref ref-type="fig" rid="F2">Fig. 2e</xref>). This variability was included to make sure that the decoder learns robust decoding strategies instead of just memorizing the exact values of each experimental condition. For example, as we argued in the main text, with one discount, the value of a temporally close small reward is similar to the value of a temporally far high reward, so reward time cannot be disentangled from reward magnitude. However, although these two values are similar, they are not identical, so a decoder with enough precision could learn to memorize them in order to report reward time. Introducing a small amount of random noise in the learning process assures robustness in the evaluation of the reporting performance.</p></sec><sec id="S15"><title>Recovering temporal information before TD learning converges</title><p id="P45">In <xref ref-type="fig" rid="F7">Extended Data Fig. 2</xref> we illustrate intuitively why the temporal information is available before TD learning converges for multi-timescale agents (experiment in <xref ref-type="fig" rid="F1">Fig. 1e</xref>). Consider the two experiments in <xref ref-type="fig" rid="F7">Extended Data Fig. 2a</xref>, one with a short wait between the cue and reward (pink) and one with a longer wait (cyan). For a single timescale agent (<xref ref-type="fig" rid="F7">Extended Data Fig. 2b</xref>), the value of the cue depends not only on the experiment length but also on the number of times that each experiment has been experienced (<italic>N</italic>, the number of TD-backups). Thus, for a given set of learning parameters (learning rate, discount factor, timestep length and reward magnitude), the single-timescale agent can incorrectly believe that the cyan cue indicates the shorter trajectory, if it has been experienced more often (left part of the plot). However, as we show theoretically in this section, since temporal information is encoded <italic>across</italic> discount factors for a multi-timescale agent, multi-timescale agents can determine reward timing independently of <italic>N</italic>. In <xref ref-type="fig" rid="F7">Extended Data Fig. 2c</xref>, the patterns of three dots highlighted with rectangles are indicative of the reward time and are only affected by the learning parameters by a multiplicative factor. Indeed, when we plot the multi-timescale values as a function of the number of times that the experiments are experienced (<italic>N</italic>, <xref ref-type="fig" rid="F7">Extended Data Fig. 2d-e</xref>), we see that the pattern across discounts is maintained, enabling a downstream system to robustly decode reward timing.</p><p id="P46">The following is a theoretical proof of this advantage. Consider a multi-timescale agent performing TD learning on the trajectory <italic>s</italic> → ⋯ → <italic>s</italic><sub><italic>T</italic></sub> in which there is no variability in outcome timing (i.e., non-zero outcomes always happen at the same states, but their magnitude can be stochastic) and all rewards are positive. Under these assumptions, the agent is able to decode reward timing if it has access to <inline-formula><mml:math id="M15"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula>, the future times at which outcomes <italic>r</italic><sub><italic>τ</italic></sub> are non-zero given the current state, where <inline-formula><mml:math id="M16"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is a Kronecker delta function that is equal to 1 if <italic>r</italic><sub><italic>τ</italic></sub> is zero and equal to 0 otherwise. <italic>At any time during TD learning</italic>, the value estimate for <italic>s</italic> computed with TD learning can be written with the following general expression (note the absence of the expectation):
<disp-formula id="FD9"><label>(9)</label><mml:math id="M17"><mml:msub><mml:mi>V</mml:mi><mml:mi>γ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>τ</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:msub><mml:mi>f</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
where <italic>f</italic><sub><italic>τ</italic></sub> (<italic>α, N, R</italic><sub>0:<italic>τ</italic></sub>) is a non-zero scalar that depends on <italic>τ</italic>, on the learning rate <italic>α</italic>, on the number of times the trajectory has been experienced <italic>N</italic> and on the history of outcome magnitudes experienced in the past <italic>R</italic><sub>0:<italic>τ</italic></sub>. This decoupling shares similarity with the successor representation <sup><xref ref-type="bibr" rid="R62">62</xref>,<xref ref-type="bibr" rid="R86">86</xref>,<xref ref-type="bibr" rid="R87">87</xref></sup>. Crucially, <italic>f</italic><sub><italic>τ</italic></sub>(<italic>α, N, R</italic><sub>0:<italic>τ</italic></sub>) does not depend on <italic>γ</italic>, so, at all times during learning, it holds that:
<disp-formula id="FD10"><label>(10)</label><mml:math id="M18"><mml:msup><mml:mi>Z</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>γ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math></disp-formula>
Since <italic>f</italic><sub><italic>τ</italic></sub>(<italic>α, N, R</italic><sub>0:<italic>τ</italic></sub>) is non-zero for all <italic>τ</italic>’s and <inline-formula><mml:math id="M19"><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>τ</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:math></inline-formula> is only non-zero at <italic>τ</italic>’s in which a reward happens, the non-zero values of the right-hand side expression indicates the future reward timings. In other words, applying the inverse transform <italic>at any time during learning</italic> to the multi-timescale estimate {<italic>V</italic> <sub><italic>γ</italic></sub> (<italic>s</italic>) <sub><italic>γ</italic>∈(0,1)</sub>} gives an expression whose non-zero values are the future outcome timings. In summary, in the absence of timing stochasticity the multi-timescale agent can recover future outcome timing before TD converges, a capability that is not present in single-timescale agents.</p></sec><sec id="S16"><title>Myopic learning bias: branching task</title><p id="P47">In <xref ref-type="fig" rid="F2">Fig. 2f</xref>, we present a simple MDP to highlight the myopic learning bias during training. In each episode, the agent learns from 3 trajectories: one that moves up at <italic>s</italic>, another that moves down at <italic>s</italic> but up at state <italic>s’</italic>, and one that moves up at <italic>s</italic> and <italic>s’</italic>. Since rewards are stochastic, the information that the agent gets on each episode is incomplete. To evaluate how well the agent acts given limited information, we average performance over the following procedure: (1) sample rewards along the three trajectories mentioned before, (2) learn the Q-values (until convergence) for <italic>s</italic> and <italic>s’</italic> using the rewards from the sampled trajectories and (3) choose the actions that maximize the Q-values. Performance is then measured as the proportions of right decisions across 10,000 iterations of this procedure. In <xref ref-type="fig" rid="F2">Fig. 2f</xref> we evaluate performance as the fraction of episodes in which the Q-value of the branch with the deterministic reward is higher than the Q-value of the branch without the deterministic rewards.</p><p id="P48">To evaluate the multi-timescale agent of <xref ref-type="fig" rid="F2">Fig. 2b</xref> on this task, we followed a similar procedure. In each episode, we randomize the identity of the top and bottom branches after the bifurcation, which defines an episode-specific MDP. For each episode-specific MDP, the agent performs Q-learning until near convergence using the 3 trajectories mentioned in the previous paragraph. The Q-values at the current state (<italic>s</italic> or <italic>s’</italic>) are fed into the policy learning architecture of <xref ref-type="fig" rid="F2">Fig. 2b</xref>, which outputs the decision to move up or down in the episode-specific MDP. The policy-learning network is trained across episodes to produce actions that maximize overall task performance. For the single-discount agent, we report the maximum performance over the agents with discounts [0.6,0.6] and [0.99,0.99], which achieve a performance of 77±2% and 83±1% respectively. For the multi-discount agent, we use the discounts [0.6,0.99], which achieves a performance of 94±1%. The error bars correspond to the s.e.m. across 500 episodes in a validation set.</p></sec><sec id="S17"><title>Myopic learning bias: navigation task</title><p id="P49">Previous theoretical work showed that a myopic discount in RL can serve as a regularizer when approximating the value function from a limited number of trajectories <sup><xref ref-type="bibr" rid="R88">88</xref></sup>. In <xref ref-type="fig" rid="F8">Extended Data Fig. 3</xref> we highlight the fact that the benefit of the myopic discount is contingent upon the distance between the current state and significant environmental events. Consider the simple navigation scenario depicted in <xref ref-type="fig" rid="F8">Extended Data Fig. 3a</xref>. The agent’s motion is random and isotropic, garnering a minor random reward from a normal distribution with mean 0 and s.d. 0.01 in each step and three more substantial rewards upon reaching the areas denoted by fire (<italic>r</italic> = –4) and water (<italic>r</italic> = 2) symbols. We evaluate how well the agent can determine the true value function (under a discount factor <italic>γ</italic> = 0.99<italic>)</italic> under the aforementioned stochastic policy. Crucially, the agent must perform this task after experiencing only a limited number of trajectories. The grey arrows show an example trajectory, with the actual and estimated values for these trajectories shown in <xref ref-type="fig" rid="F8">Extended Data Fig. 3b</xref>.</p><p id="P50">We evaluate accuracy using the Kendall rank correlation coefficient between the true value function in the entire maze and the value estimates. The Kendall coefficient measures the fraction of concordant pairs between the two value functions (across all pairs of states in the maze). For every pair of states, it computes whether the two value functions agree on which element of the pair is the larger one. Note that this measure of accuracy is behaviorally more relevant than alternative accuracy measures that compare the absolute magnitude of values across states. In other words, for an agent navigating the maze, it is more important to be accurate on the relative values of alternative goal states than on their absolute values. Consider the trajectory shown in <xref ref-type="fig" rid="F8">Extended Data Fig. 3b</xref>. For this trajectory, the myopic estimate (using a discount factor <italic>γ</italic> = 0.6, green) clearly provides a better estimate of the true value function (grey) than using the true discount factor <italic>γ</italic> = 0.99 (brown). We can quantify that the myopic estimate is a better approximation of the true value function by evaluating the agreement between pairs of states along the estimated and true curves (i.e. by computing the Kendall coefficient).</p><p id="P51">In <xref ref-type="fig" rid="F8">Extended Data Fig. 3c-d</xref> the agent learns from N randomly sampled trajectories starting either in the lower half (blue) or upper half (red) of the maze. The values for the states in the <italic>N</italic> sampled trajectories are learned until convergence using the rewards and transitions in the sampled trajectories. After convergence, we compute the Kendall rank correlation between the estimates and the true value function, and report performance as the average correlation across 10,000 sets of <italic>N</italic> sampled trajectories. <xref ref-type="fig" rid="F8">Extended Data Fig. 3c</xref> shows that when learning from two randomly sampled trajectories, the estimates of the value function using a myopic discount factor are more accurate than far-sighted discounts when crucial events are in the near future (i.e the trajectories start in the lower half of the maze, blue curve in <xref ref-type="fig" rid="F8">Extended Data Fig. 3c</xref>). This result agrees with the intuition built in <xref ref-type="fig" rid="F8">Extended Data Fig. 3c</xref> when learning from a single trajectory. However, if the agent is distant from important events (i.e. trajectories starting in the upper half of the maze, red curve), the myopic estimates approach the noise level, while estimates with larger discount factors are more accurate. As expected, with the accumulation of more data from the environment, that is, more trajectories, the far-sighted estimate progressively aligns with the true value compute with <italic>γ</italic> = 0.99 in the entire maze (<xref ref-type="fig" rid="F8">Extended Data Fig. 3d</xref>)</p></sec><sec id="S18"><title>Myopic learning bias: networks with discount factors as auxiliary tasks</title><p id="P52">An alternative way to leverage multi-timescale learning benefits, in contrast to the architecture presented in <xref ref-type="fig" rid="F2">Fig. 2b</xref>, is to employ them as auxiliary tasks (<xref ref-type="fig" rid="F2">Fig. 2g</xref>, top). These networks only act according to the value of a single behavioral timescale, but concurrently learn about multiple other timescales as auxiliary tasks to enhance the representation in the hidden layers, which allows them to obtain superior performance in complex RL environments <sup><xref ref-type="bibr" rid="R60">60</xref>,<xref ref-type="bibr" rid="R89">89</xref></sup>. This approach is similar to Distributional RL networks that learn the quantiles of the value distribution but act according to the expectation of that distribution <sup><xref ref-type="bibr" rid="R41">41</xref></sup>. Notably, we show that the auxiliary learning timescales display the myopic learning bias highlighted so far. In the Lunar-Lander task (<xref ref-type="fig" rid="F2">Fig. 2g</xref>, bottom) where the agent must land a spacecraft, Q-values computed using a myopic discount provide a more accurate representation of the future when the agent is close to the landing site (blue), whereas the opposite holds when the agent is far from the landing site (red).</p><p id="P53">In the Lunar Lander environment in <xref ref-type="fig" rid="F2">Fig. 2g</xref>, the state space consists of eight elements, including the position and velocity of the lander, its angular position and angular velocity, as well as an additional input related to the contact with the ground. The action space is composed of four actions: doing nothing and activating one of three different engines. The agent is a Deep-Q-network <sup><xref ref-type="bibr" rid="R3">3</xref></sup> (DQN) with two hidden layers of 512 units each, separated by ReLU activation functions. In addition to the Q-values that control the agent, the network has Q-values for 25 additional discounts factors equally spaced between 0.6 and 0.99. Thus, if there are |a| actions in the environment, for each discount the network has |a| additional output units. All sets of |a| units (one for each discount) use the Huber (i.e. Smooth L1, <italic>β=1</italic>) Q-learning loss function with its corresponding discount. All the auxiliary Q-learning losses update the action that was actually chosen in the environment by the behavioral units, and thus all of them learn the consequences of the behavioral policy, but using different discount factors. The total loss function uset to train the network averages the Q-learning losses of all the discount factors. To train the DQN, we use a learning buffer of 20,000 samples, a learning rate of 10<sup>-3</sup> and a batch size of 32. As in traditional DQNs, we use a target network to compute the TD target, which is updated every 1,000 samples with the weights from the policy network to stabilize the learning process. For exploration, the agent uses a linearly decreasing ε-greedy policy that goes from <italic>ε = 1</italic>.<italic>0</italic> at the first sample to a minimum value of <italic>ε = 0</italic>.<italic>01</italic> after 40,000 samples.</p><p id="P54">Our goal is to compute the degree to which Q-values computed with alternative discounts can capture the true Q-value of the behavioral policy. The multi-timescale DQN uses a behavioral discount <italic>γ</italic><sub><italic>beh</italic></sub> = 0.99, and its policy is produced by choosing actions that maximize the Q-values with that discount factor. As in the navigation scenario presented in the previous section, our hypothesis is that, when important events lie in the proximal future (here, close to the landing site), the Q-values learned using myopic discounts capture the true behavioral Q-value more accurately, while far-sighted discounts are more accurate when important events lie in the distant future (far from the landing site).</p><p id="P55">Under the policy of the DQN (<italic>π</italic><sub><italic>DQN</italic></sub>), the true value of state <italic>s</italic> is:
<disp-formula id="FD11"><label>(11)</label><mml:math id="M20"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mtext>beh </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:msub><mml:mi>π</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>Q</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mtext> </mml:mtext></mml:mrow><mml:mi>t</mml:mi></mml:msubsup></mml:mrow></mml:mstyle><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></disp-formula>
If the DQN has perfectly learned the Q-value of state <italic>s</italic>, then the estimate <italic>Q</italic><sub><italic>γ</italic></sub>(<italic>s, a</italic><sub><italic>beh</italic></sub>) of the DQN should be equal to <inline-formula><mml:math id="M21"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, where <italic>a</italic> <sub><italic>beh</italic></sub> is the action produced by the DQN at <italic>s</italic>. We evaluate accuracy as the degree to which the estimated <italic>Q</italic><sub><italic>γ</italic></sub>(<italic>s, a</italic><sub><italic>beh</italic></sub>) captures the true <inline-formula><mml:math id="M22"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula>, and compare accuracy across the auxiliary discount factors.</p><p id="P56">After training the network for 50,000 samples (and achieving close-to-optimal performance), we compute <inline-formula><mml:math id="M23"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> empirically across states by recording the actual discounted sum of rewards obtained by the agent when departing from state <italic>s</italic>. We calculate <inline-formula><mml:math id="M24"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> empirically for 25,000 states. Then, we compare, across states, the empirically calculated <inline-formula><mml:math id="M25"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></inline-formula> with the Q-values produced by the DQN at those states.</p><p id="P57">To measure Accuracy, we use the Kendall rank correlation as in the previous section. The Kendall correlation measures the fraction of concordant pairs between samples from <inline-formula><mml:math id="M26"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> and from the estimated <italic>Q</italic><sub><italic>γ</italic></sub>, across pairs of states. As in the navigation scenario presented in the previous section, for an agent deciding which state to navigate to, it is more important to be accurate on the relative values between pairs of states than on the absolute value of individual states. Therefore, the Kendall correlation is behaviorally more relevant than other accuracy metrics that compare the absolute magnitude of <inline-formula><mml:math id="M27"><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup></mml:math></inline-formula> and <italic>Q</italic> <sub><italic>γ</italic></sub> (e.g. <inline-formula><mml:math id="M28"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi/></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mi>γ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mtext>beh </mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula>).</p><p id="P58">Given that the environment and the training process are stochastic, we report the accuracy by averaging over 10 randomly initialized networks.</p></sec><sec id="S19"><title>Cued delay task</title><p id="P59">All the data in the experiments with mice were collected in the previous study <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. The experimental details including the surgical procedures, behavioral setup, and the behavioral tasks have been described there <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. We will here focus on the task description as our analysis includes task conditions that were not analyzed in the previous study.</p><p id="P60">Mice were head-fixed on a wheel in front of three computer monitors and an odor port. At trial onset, the screens flashed green to indicate the beginning of the trial. After <italic>t</italic> = 1.25s, an odor cue was delivered. This reward delay cue was one of four possible odors, and each cue was associated with a unique reward delay chosen from 0.6, 1.5, 3.75 or 9.375 seconds. The association between odor and reward delay was randomized across mice. The inter-trial interval was adjusted depending on the reward delays such that the trial start cues were spaced by 17-20s. Mice performed 81.4 ± 12.5 trials (mean ± s.d.) per session across the 36 sessions in which neurons were recorded in the task.</p></sec><sec id="S20"><title>Approach-to-target virtual reality (VR) task</title><p id="P61">We refer the reader to the prior study for details on the experimental procedures <sup><xref ref-type="bibr" rid="R68">68</xref></sup>. Mice were also trained in additional conditions, which we do not analyze in the present study, including teleport and speed modulation in the virtual reality scene.</p><p id="P62">Here, we analyzed single neuron recordings in the sessions with no teleport or speed manipulation and in the open-loop condition. Mice were free to locomote, but their motion did not affect the dynamics of the visual scene. After scene motion onset, the visual scene progressed at constant speed until the reward was delivered after 7.35s.</p><p id="P63">Mice performed 58.8 ± 21.7 trials (mean ± s.d.) per session across the 60 sessions in which neurons were recorded in the task. Spiking activity was convolved with a box filter of length 10 ms. When plotting neural activity, we further convolved the responses by a causal exponential filter (<italic>e</italic><sup><italic>-0</italic>.<italic>05dt</italic></sup>). Spiking rate traces across neurons were normalized using a modified z-score. The mean was taken as the average firing activity cross the first 1.5s and the standard deviation across the entire 4.35s.</p></sec><sec id="S21"><title>Fitting neural activity in the cued delay task</title><p id="P64">For the cued delay task, we fit the responses of single neurons to the delay cue (calculated as the firing rate in the time interval 0.1<italic>s</italic> &lt; <italic>t</italic> &lt; 0.4<italic>s</italic> after the cue onset, see shaded area in <xref ref-type="fig" rid="F3">Fig. 3c</xref>) using two discounting models as in ref[<sup><xref ref-type="bibr" rid="R63">63</xref></sup>], the classic exponential model and a hyperbolic model. For the exponential model, we fit the responses to a cue predicting a reward in <italic>τ</italic> seconds by:
<disp-formula id="FD12"><label>(12)</label><mml:math id="M29"><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mi>γ</mml:mi><mml:mi>τ</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula>
The discount factor <italic>γ</italic> can also be expressed as a discount rate λ and vice versa: <italic>λ</italic> = − ln <italic>γ</italic> or <italic>γ</italic> = <italic>e</italic><sup>−<italic>λ</italic></sup>. The discount factors fitted to data are always expressed in units of <italic>seconds</italic>, that is the discount factor is the devaluation one second into the future.</p><p id="P65">For the hyperbolic model we used a standard model for hyperbolic discounting in which the parameter <italic>k</italic> controls discounting:
<disp-formula id="FD13"><label>(13)</label><mml:math id="M30"><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>h</mml:mi><mml:mi>y</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>b</mml:mi><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
We fitted both models by minimizing mean squared error (the <italic>fit</italic> function in MATLAB). For both models we constrained the baseline and gain parameters such that 0 &lt; <italic>b</italic> &lt; 40 and 0 &lt; <italic>α</italic> &lt; 40. For the exponential model, the discount rate was constrained such that 0.0001 &lt; <italic>λ</italic> &lt; 20 and for the hyperbolic model, the discount parameter was constrained such that 0 &lt; <italic>k</italic> &lt; 20. Note that all the parameters are fitted independently for each single neuron.</p><p id="P66">To characterize the robustness and significance of our estimated parameters we used a bootstrap procedure. For each run, we split the trials in half and fit the models independently on each half. We computed for each split the explained variance using the other half of the data (<xref ref-type="fig" rid="F9">Extended Data Fig. 4c-d</xref>) and correlated the inferred parameter values for each neuron across both splits (<xref ref-type="fig" rid="F9">Extended Data Fig. 4f-g</xref>).</p><p id="P67">We restricted our subsequent analysis to neurons that had a positive explained variance on the test set (<italic>n=17</italic> neurons excluded), an average firing rate in the cue period over the 4 delays above 2 spikes/s (<italic>n=11</italic> neurons excluded) and with medio-lateral distance above 900μm (<italic>n=4</italic> neurons excluded). Non-selected neurons are shown in <xref ref-type="fig" rid="F9">Extended Data Fig. 4b</xref>. Poorly fit neurons often were non-canonical dopamine neurons who also did not exhibit a strong reward response.</p></sec><sec id="S22"><title>Decoding expected reward timing from population responses</title><p id="P68">The vectorized prediction error allows us to directly decode the expected timing of reward given the cue responses<sup><xref ref-type="bibr" rid="R45">45</xref></sup>. The value at time t is given by:
<disp-formula id="FD14"><label>(14)</label><mml:math id="M31"><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>⌈</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mi>t</mml:mi><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msup><mml:mi>γ</mml:mi><mml:mi>t</mml:mi></mml:msup></mml:mrow></mml:mstyle><mml:msub><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>⌉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mn>2</mml:mn><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>
In the cued delay task, at the time of the cue indicating reward delay, the response of dopaminergic neurons is driven by the discounted future reward. The reward prediction error <italic>δ</italic><sub><italic>t</italic></sub> = <italic>r</italic><sub><italic>t</italic></sub> + <italic>γ</italic><sup>Δ<italic>t</italic></sup>
<italic>V</italic><sub><italic>t</italic>+1</sub> − <italic>V</italic><sub><italic>t</italic></sub> becomes simply <italic>δ</italic><sub><italic>t</italic></sub> = <italic>γ</italic><sup>Δ<italic>t</italic></sup>
<italic>V</italic><sub><italic>t</italic>+1</sub> + <italic>cst</italic> as there is no reward delivered at the time of the cue <inline-formula><mml:math id="M32"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mtext>cue </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> and the reward expectation before the reward cue delivery is identical across conditions (<inline-formula><mml:math id="M33"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>C</mml:mi></mml:math></inline-formula>; where <italic>C</italic> is a constant). Thus, the TD error at the time of reward delay cue <inline-formula><mml:math id="M34"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></inline-formula> becomes <inline-formula><mml:math id="M35"><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>γ</mml:mi><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi/></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>C</mml:mi></mml:math></inline-formula> and if we assume the constant is 0 or the TD-error is baseline subtracted, at convergence the prediction error is given by:
<disp-formula id="FD15"><label>(15)</label><mml:math id="M36"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mn>2</mml:mn><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
In single timescale RL, the temporal information is collapsed, and it is not possible for the system receiving the learning signal (the striatum in this case) to untangle the signal. However, in a distributed system learning at multiple timescales the reward expectation <italic>E</italic>(<italic>r</italic>|<italic>t</italic>) is encoded with multiple discount factors <italic>γ</italic><sub><italic>i</italic></sub> :
<disp-formula id="FD16"><label>(16)</label><mml:math id="M37"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>…</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>1</mml:mn><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mn>2</mml:mn><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>n</mml:mi><mml:mtext>T</mml:mtext></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mn>2</mml:mn><mml:mtext>Δ</mml:mtext><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>=</mml:mo><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>
The temporal information about reward timing is now distributed across neurons and if the tuning of individual neurons is sufficiently diverse, we can write:
<disp-formula id="FD17"><label>(17)</label><mml:math id="M38"><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mn>2</mml:mn><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></disp-formula>
Where <bold>L</bold>
<sup>−1</sup> is the approximate pseudo-inverse of <bold>L</bold> such that <bold>L</bold>
<sup>−1</sup>
<bold>L</bold> ≈ <italic>I</italic>. In practice, the matrix <bold>L</bold> is not very well conditioned as the rows of the matrix are exponentially decaying functions, so the right side (further in the future) is sparsely populated (see <xref ref-type="fig" rid="F3">Fig. 3i</xref> and <xref ref-type="fig" rid="F10">Extended Data Fig. 5a-c</xref>). We therefore will need to use a regularized pseudoinverse.</p><p id="P69">To invert the discount matrix <bold>L</bold>, we use the regularized Singular value decomposition (SVD) approach similar to the one proposed in ref[<sup><xref ref-type="bibr" rid="R45">45</xref></sup>]. We then normalize the resulting prediction in order to constrain it to be a probability distribution (<italic>p</italic>(<italic>r</italic>|<italic>t</italic>) &gt; 0, <italic>for all t</italic> and <bold>∑</bold>
<sub><italic>t</italic></sub> <italic>p</italic>(<italic>r</italic>|<italic>t</italic>) = 1). More specifically, the regularized SVD approach corresponds to optimizing:
<disp-formula id="FD18"><label>(18)</label><mml:math id="M39"><mml:msup><mml:mrow><mml:mo>‖</mml:mo><mml:mrow><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mo>Δ</mml:mo><mml:mi>d</mml:mi></mml:msub></mml:mrow><mml:mo>‖</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>‖</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:math></disp-formula>
The standard SVD of the discount matrix can be written as:
<disp-formula id="FD19"><label>(19)</label><mml:math id="M40"><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:msub><mml:mi>u</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:msubsup><mml:mi>v</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>U</mml:mi><mml:mi>S</mml:mi><mml:msup><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:math></disp-formula>
<disp-formula id="FD20"><label>(20)</label><mml:math id="M41"><mml:mrow><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>L</mml:mi></mml:munderover><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>s</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msub><mml:mo>Δ</mml:mo><mml:mi>d</mml:mi></mml:msub><mml:msub><mml:mi>v</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>≡</mml:mo><mml:msup><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>L</mml:mi></mml:mstyle><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mo>Δ</mml:mo><mml:mi>d</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>
where Δ<sub><italic>d</italic></sub> = [<italic>δ</italic><sub>1</sub> … <italic>δ</italic><sub><italic>N</italic></sub>]<sup><italic>T</italic></sup>. The smooth regularization introduced by the Tikhonov regularization through the parameter <italic>α</italic> (which we can choose by inspection of the distribution of singular values <italic>σ</italic><sub><italic>s</italic></sub>, see below) is more robust than a strict truncated SVD in which we only take a number of factors and set the remaining ones to zero. An alternative approximation to this inverse problem is Post’s approximation <sup><xref ref-type="bibr" rid="R57">57</xref>,<xref ref-type="bibr" rid="R62">62</xref></sup>. It relies on evaluating higher order derivatives and lacks robustness if the Laplace space is not sampled with enough precision (i.e. not enough neurons tiling the γ space).</p><p id="P70">The procedure in the previous section allows us to estimate the discount factor independently for each neuron. We then choose a discretization step Δ<italic>t</italic> = 100<italic>ms</italic> and a temporal horizon <italic>T</italic> = 12<italic>s</italic> over which to make the prediction. This allows us to construct the discount matrix <bold>L</bold> shown in <xref ref-type="fig" rid="F3">Fig. 3h</xref> for the exponential model and <xref ref-type="fig" rid="F11">Extended Data Fig. 6c</xref> for the hyperbolic model. In order to choose a suitable value for the regularization parameter <italic>α</italic> we perform the regular SVD on the discount matrix <bold>L</bold> and assess the values at which the singular values become negligible. We choose a value of <italic>α</italic> that corresponds to the transition between large singular values and negligible ones (see <xref ref-type="fig" rid="F10">Extended Data Fig. 5b</xref>). Using this approach, we used <italic>α</italic> = 2 in our decoding analysis.</p><p id="P71">For each delay, we construct a pseudo-population response Δ<sub><italic>d</italic></sub> across the recorded neurons. For each bootstrap, we take the mean activity for each cue, subtract the inferred baseline parameter <italic>b</italic>, and normalize the maximum response to 1. To assess the robustness of the predictions, we use the mean responses and baseline from half the trials to construct Δ<sub><italic>d</italic></sub> and use the estimated discount factors from the other half of the trials to estimate <bold>L</bold><sup>−1</sup> and we repeat this approach for each bootstrap (<italic>n</italic><sub><italic>predictions</italic></sub> = 200). In the figures (<xref ref-type="fig" rid="F3">Fig. 3k</xref> and <xref ref-type="fig" rid="F11">Extended Data Fig. 6d,f</xref> and <xref ref-type="fig" rid="F14">9c</xref>), the thin lines correspond to the predictions from individual bootstraps and the thicker line to the average of these predictions. For shuffle control, we randomize the identity of the neurons in the pseudo-population response Δ<sub><italic>d</italic></sub>. This means that in the shuffle control a given neuron is not decoded with its corresponding weights but by a random row of the decoding matrix <bold>L</bold><sup>−1</sup>.</p><p id="P72">In order to ensure that the prediction corresponds to a probability distribution, we normalize the resulting prediction of reward timing. We first set the probability of obtaining a reward to zero for all times in which the prediction was negative, then we normalize the distribution to be a valid probability distribution (such that the probability mass over <italic>t</italic> ∈ [0,12] sums to 1).</p><p id="P73">For the time decoding using a single average discount factor, we use a different approach. The inversion procedure would not work as the discount matrix would be of rank 1. Instead, if we assume a fixed known reward size and a single discount factor, the response of individual neurons would correspond to different estimates of the reward timing. For each bootstrap we can estimate the expected reward timing for each neuron. For a given firing rate FR for the held out data, we can estimate the reward timing using the parameter estimates from the trained data. The baseline <italic>b</italic><sub><italic>i</italic></sub> and gain <italic>α</italic><sub><italic>i</italic></sub> parameters are specific to each neuron while the discount factor <italic>γ</italic> is the average discount factor across all the neurons. The expected reward timing for neuron <italic>i</italic> is given by the following equation:
<disp-formula id="FD21"><label>(21)</label><mml:math id="M42"><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>log</mml:mi><mml:mi>max</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>F</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0.0001</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mi>γ</mml:mi></mml:mrow></mml:mfrac></mml:math></disp-formula>
Together, the neurons provide a distribution of expected reward timing with each neuron predicting a sample of the distribution of expected reward times. The average distribution is obtained by averaging the distributions across all the bootstraps, excluding predicted reward times beyond 12 seconds and normalizing the distribution to be a probability distribution. Similarly to the SVD-based decoding, in <xref ref-type="fig" rid="F10">Extended Data Fig. 5f</xref> the thin lines correspond to the predictions from individual bootstraps and the thicker line to the average of these predictions.</p></sec><sec id="S23"><title>Quantifying reward timing decoding accuracy</title><p id="P74">In order to quantify the reward timing decoding accuracy, we used the 1-Wasserstein distance (or earth mover’s distance) between distributions as our metric. We used the 1-Wasserstein distance as the difference in support between the predicted reward timing distribution (probability mass as most locations) and the single true reward timing (probability mass at a single location) is not conducive to using the KL-divergence.</p><p id="P75">For each bootstrap, we generated <italic>n</italic> = 100,000 samples from the predicted reward timing distributions and computed the 1-Wassertsein distance between the predicted reward timing and the true corresponding reward delay (using the MATLAB function <italic>ws_distance</italic> from <ext-link ext-link-type="uri" xlink:href="https://github.com/nklb/wasserstein-distance">https://github.com/nklb/wasserstein-distance</ext-link>). For each condition (exponential fit, hyperbolic fit, average discount factor, simulation fit and their associated shuffled predictions) we obtained a distribution of 1-Wasserstein distances across the bootstraps (<italic>n</italic> = 200). To assess the significance of the differences in reward timing predictions across conditions, we used the one-tailed Wilcoxon’s signed rank test (using the MATLAB function <italic>signrank</italic>).</p></sec><sec id="S24"><title>Fitting neural activity in the VR task</title><p id="P76">To quantify the heterogeneity of discount factors in the VR task, we fit the neural activity in the last 4.30 seconds (<italic>t</italic> = 3.05 seconds after scene motion onset) of the approach to reward period in which the ramping activity was most pronounced. In order to assess the robustness of the fit, we used a bootstrap procedure in which for each bootstrap (<italic>n</italic><sub><italic>bootstrap</italic></sub> = 100), we partition the trials in two halves and compute the two average PSTHs using <italic>dt</italic> = 0.1 second as our discretization step. We then compute the mean value of the parameters across all bootstraps. We limit our analysis to neurons whose firing rate over the analysis period is larger than 2 spikes/s. We fit the two models (common value function and common reward timing expectation) to this data.</p><p id="P77">In the VR task, the expectations vary smoothly as a function of time and distance and we therefore use the discretized formulation of the TD error for continuous time in our fits <sup><xref ref-type="bibr" rid="R69">69</xref>,<xref ref-type="bibr" rid="R71">71</xref></sup>:
<disp-formula id="FD22"><label>(22)</label><mml:math id="M43"><mml:msub><mml:mi>δ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mi>ln</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
Although this formulation is also discretized as the standard formulation of the TD error, the presence of the derivative <inline-formula><mml:math id="M44"><mml:mfrac><mml:mrow><mml:mo>Δ</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> (which is computed numerically) improves the stability of the fitting procedure. The two models differ in whether value function is estimated directly (and shared across neurons) or indirectly (and distinct across neurons). The discount factor is also in units of <italic>seconds</italic>, allowing for a comparison with the values estimated in the cued delay task.</p></sec><sec id="S25"><title>Common value function model</title><p id="P78">In the common value function model, <italic>V(t)</italic> is common across neurons and is directly fitted by the optimization procedure which minimizes:
<disp-formula id="FD23"><label>(23)</label><mml:math id="M45"><mml:msub><mml:mi>min</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>‖</mml:mo><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mo>Δ</mml:mo><mml:msup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:math></disp-formula>
With,
<disp-formula id="FD24"><label>(24)</label><mml:math id="M46"><mml:mo>Δ</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:math></disp-formula>
We fit the gains, baseline, and discount factors of individual neurons (<italic>α</italic><sub><italic>i</italic></sub>, <italic>b</italic><sub><italic>i</italic></sub> and <italic>γ</italic><sub><italic>i</italic></sub> respectively) and the join value function <italic>V</italic> using a constrained optimization procedure (<italic>fmincon</italic> in MATLAB, <italic>α</italic><sub><italic>i</italic></sub> ∈ [0.05,50], <italic>b</italic><sub><italic>i</italic></sub> ∈ [0.05,12], <italic>γ</italic><sub><italic>i</italic></sub> ∈ [0.05,0.999999], and <italic>V</italic> ∈ [0.05,5]).</p></sec><sec id="S26"><title>Common Reward Expectation model</title><p id="P79">In the common reward expectation model, the reduction in uncertainty in reward timing due to sensory feedback as the mice approach the reward leads to an upwards ramp in the average TD error signal across dopaminergic neurons <sup><xref ref-type="bibr" rid="R68">68</xref>–<xref ref-type="bibr" rid="R70">70</xref></sup>. In a task like the cued delay task shown in <xref ref-type="fig" rid="F3">Fig. 3</xref>, once the cue has been presented, the time estimation until the reward is based on the internal clock of the mice that suffers from scalar timing (i.e., the standard deviation of the noise in the estimation grows linearly with the estimation time <sup><xref ref-type="bibr" rid="R49">49</xref></sup>. In the VR task, there is visual feedback and as the mice approach the reward, the uncertainty is instead reduced (<xref ref-type="fig" rid="F13">Extended Data Fig. 8a</xref>). We also show that this alternative model also provides a similar explanation of ramping diversity as originating from a heterogeneity of discount factors (<xref ref-type="fig" rid="F13">Extended Data Fig. 8</xref>).</p><p id="P80">We use a joint fitting procedure in which we simultaneously fit the discount factors across neurons and the expected timing of reward as a function of position in the virtual track. Similarly to <sup><xref ref-type="bibr" rid="R69">69</xref></sup>, we interpret the ramping in single neurons as originating from the reduction in uncertainty due to the visual feedback as the mice approach the reward. Although each neuron has a distinct discount factor and its own value function, the world model which parametrizes the changes in reward expectation with visual feedback is shared across dopaminergic neurons. This arises as this shared model is the product of the integration of the diverse dopamine signals as well as other neural computations controlling reward expectations <sup><xref ref-type="bibr" rid="R29">29</xref></sup>.</p><p id="P81">Individual neurons therefore act as independent agents estimating value given a shared expectation of reward timing. Each neuron has a distinct discount factor <italic>γ</italic><sub>i</sub> with which it computes value given the expected reward timing. We assume that inference has converged and therefore we have the value <italic>V</italic><sub><italic>i</italic></sub> associated with neuron <italic>i</italic>:
<disp-formula id="FD25"><label>(25)</label><mml:math id="M47"><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munderover><mml:mi>∑</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mrow><mml:msubsup><mml:mi>γ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>τ</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:math></disp-formula>
Here, we assume that <italic>E</italic>(<italic>r</italic>|<italic>τ, t, T</italic>) takes the form a folded normal distribution with parameters <italic>μ</italic> = <italic>T</italic> − <italic>t</italic> and (fitted) standard deviation <italic>σ</italic>. The folded normal distribution reflects the weight of the negative component of a normal distribution back onto positive values <sup><xref ref-type="bibr" rid="R90">90</xref></sup>. The folded normal distribution formulation leads to the following distribution for the expected reward timing for <italic>τ</italic> &gt; 0 :
<disp-formula id="FD26"><label>(26)</label><mml:math id="M48"><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo>∣</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:mi>cosh</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>
In our analysis, the mean, <italic>μ</italic> = <italic>T</italic> − <italic>t</italic>, is given by the current position in the VR track and the only fitted parameter is the standard deviation <italic>σ</italic>. At each time step we fit a different value of the standard deviation. As observed through the fitting procedure, the standard deviation is initially high and reduces as the mice approach the reward location. This is an indication that similarly than proposed in <sup><xref ref-type="bibr" rid="R69">69</xref></sup> the ramping in the dopaminergic neuron’s activity arises from the reduction in uncertainty due to the visual feedback as the mice approach the reward. We use a slightly different formulation than in ref [<sup><xref ref-type="bibr" rid="R69">69</xref></sup>] as we require additional flexibility to fit data and specifically need to go beyond the assumptions of Gaussian state uncertainty. Note also that we assume here that the uncertainty is in the timing of the reward rather than in the state.</p><p id="P82">In order to normalize the contributions of the different neurons, we used a normalized firing rate and therefore only fit the discount factor <italic>γ</italic> and standard deviation <italic>σ</italic> of the reward expectation.
<disp-formula id="FD27"><label>(27)</label><mml:math id="M49"><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>γ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:msub><mml:mo>‖</mml:mo><mml:mi>F</mml:mi><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mo>Δ</mml:mo><mml:msup><mml:mo>‖</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:math></disp-formula>
With,
<disp-formula id="FD28"><label>(28)</label><mml:math id="M50"><mml:mo>Δ</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd><mml:mo>⋯</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math></disp-formula>
We performed the constrained optimization with the <italic>MATLAB</italic> function <italic>fmincon</italic> and constrain the parameters such that <italic>γ</italic> ∈ [0.001, 0.99] and <italic>σ</italic> ∈ [0.1, 12].</p></sec><sec id="S27"><title>Comparing parameters across tasks</title><p id="P83">We used two methods to assess the relationship between the inferred discount factors in the approach-to-reward VR task and the cued delay task. First, we used the mean parameters across bootstraps and computed the Spearman correlation. Next, we computed, for <italic>n</italic> = 10,000 randomly selected (with replacement) pairs of bootstraps, the Spearman correlation between the parameters across the two tasks and plotted the distribution of these correlation.</p><p id="P84">For the decoding of reward timing using parameters inferred in the VR task, we also used a bootstrap approach. We computed the discount matrix and the decoding matrix for each bootstrap estimate of the discount factors in the VR task.</p></sec><sec id="S28"><title>Simulations to assess limits on parameter estimation</title><p id="P85">To assess the contribution of the limits imposed by the number of trials and the stochasticity in firing rates to the accuracy of the reward timing prediction and the similarity of inferred parameters across tasks, we ran a series of simulations with parameters chosen to match those inferred from the data. For the simulation parameters, we use the mean inferred value for the parameters across all the bootstraps for the respective task.</p><p id="P86">For the cued delay task, we generated for each neuron <italic>n</italic> = 80 trials (<italic>n</italic> = 20 per delay), comparable to behavioral sessions in the task, simulated cue responses by taking samples from a Poisson distribution with a rate parameter corresponding to the value predicted by the exponential discount model for the corresponding reward delay. We used the same procedure as for analyzing the recorded data by performing <italic>n</italic> = 100 bootstrap and fitting the simulated data on random partitions of the data.</p><p id="P87">For the VR task, we generated for each neuron <italic>n</italic> = 80 trials, comparable to behavioral sessions in the task, by taking samples from a Poisson distribution with a rate parameter corresponding to the predicted activity given <xref ref-type="disp-formula" rid="FD22">equation 22</xref>. We then performed the fitting procedures similarly than for the experimental data.</p></sec></sec><sec id="S297" sec-type="extended-data"><title>Extended data figures and tables</title><fig id="F6" position="anchor"><label>Extended Data Fig 1</label><caption><title>Decoding simulations for multi-timescale vs. single-timescale agents</title><p id="P88"><bold>(a-c)</bold>. Experiment corresponding to <xref ref-type="fig" rid="F2">Fig. 2c</xref>. (decoding reward timing). <bold>a</bold>, MDP with reward <italic>R</italic> at time <italic>t</italic><sub><italic>R</italic></sub>. <bold>b</bold>, Diagram of the decoding experiment. In each episode, the reward magnitude and time are randomly sampled from discrete uniform distributions, which defines the MDP in <bold>a</bold>. Values are learned until near convergence using TD-learning. Values with different discount factors are learned independently. The learned values for the cue (<italic>s</italic>) are fed into a non-linear decoder which learns, across MDPs, to report the reward time. <bold>c</bold>, Decoding performance as the decoder is trained. Different colors indicate the discount factors used in TD-learning. <bold>(d-f)</bold>. Experiment corresponding to <xref ref-type="fig" rid="F2">Fig. 2d</xref>. (Decoding value with hyperbolic discount). <bold>d</bold>, MDP with reward <italic>R</italic> at time <italic>t</italic><sub><italic>R</italic></sub>. <bold>e</bold>, Diagram of the decoding experiment. In each episode, the reward magnitude and time are randomly sampled from discrete uniform distributions, which defines the MDP in <bold>d</bold>. Values are learned until near convergence using TD-learning. Values with different discount factors are learned independently. The learned values for the cue (<italic>s</italic>) are fed into a non-linear decoder which learns, across MDPs, to report the hyperbolic value of the cue. <bold>f</bold>, Decoding performance as the decoder is trained. Different colors indicate the discount factors used in TD-learning. <bold>(g-i)</bold>. Experiment corresponding to <xref ref-type="fig" rid="F2">Fig. 2e</xref>. (decoding reward timing before convergence). <bold>g</bold>, MDP with reward equal to 1 at time <italic>t</italic><sub><italic>R</italic></sub>. <bold>h</bold>, Diagram of the decoding experiment. In each episode, the reward time and the number of TD iterations (<italic>N</italic>) are sampled from discrete uniform distributions. Values are learned by performing N TD-learning backups on the MDP. Values with different discount factors are learned independently. The learned values for the cue (<italic>s</italic>) are fed into a non-linear decoder which learns, across MDPs, to report the reward time. <bold>i</bold>, Decoding performance as the decoder is trained. Different colors indicate the discount factors used in TD-learning. <bold>(j-l)</bold>. Decoding reward timing in a more complex task. <bold>j</bold>, MDP with two rewards of magnitude <italic>R</italic>1 and <italic>R</italic>2 at times <italic>t</italic><sub><italic>R</italic>1</sub> and <italic>t</italic><sub><italic>R</italic>2</sub>. <bold>k</bold>, Diagram of the decoding experiment. In each episode, both reward magnitudes and times are sampled from discrete uniform distributions. The learned values for the cue (<italic>s</italic>) are fed into a non-linear decoder which learns, across MDPs, to report both reward times. <bold>l</bold>, Decoding performance as the decoder is trained. Different colors indicate the discount factors used in TD-learning. <bold>(m-o)</bold>. Decoding length of branches in an MDP during training. <bold>m</bold>, MDP with two possible trajectories. In this example, the upwards trajectory is longer than the downwards trajectory. <bold>n</bold>, Diagram of the decoding experiment. In each episode, the length of the two branches D and the number of times that TD-backups are performed for each branch are randomly sampled from uniform discrete distributions. Then, TD-backups are performed for the two branches the corresponding number of times. After this, they are fed into a decoder which is trained, across episodes, to report the shorter branch. <bold>o</bold>, Decoding performance as the decoder is trained. Different colors indicate the discount factors used in TD-learning. In panels <bold>c, f, i, k</bold> and <bold>o</bold>, the shaded area corresponds to the standard deviation of the estimate over 2 repeats and smoothed of 100 episodes.</p></caption><graphic xlink:href="EMS190989-f006"/></fig><fig id="F7" position="anchor"><label>Extended Data Fig 2</label><caption><title>Temporal estimates are available before convergence for multi-timescale agents</title><p id="P89"><bold>a</bold>, Two experiments, one with a short wait between the cue and reward (pink), and one with a longer wait (cyan). <bold>b</bold>, The identity of the cue with the higher value for a single-timescale agent (here <italic>γ</italic>=0.9) depends on the number of times that the experiments have been experienced. When the longer trajectory has been experienced significantly more often than the short one, the single-timescale agent can incorrectly believe that it has a larger value. <bold>c</bold>, For a multi-timescale agent, the pattern of values learned across discount factors is only affected by a multiplicative factor that depends on the learning rate, the prior values and the asymmetric learning experience. The pattern therefore contains unique information about outcome time. <bold>d</bold>,<bold>e</bold>, When plotted as a function of the number of times that trajectories are experienced, the pattern of values across discount factors is only affected by a multiplicative factor. In other words, for the pink cue, the larger discount factors are closer together than they are to the smaller discount factor, and the opposite for the cyan cue. This pattern is maintained at every point along the x-axis, and therefore is independent of the asymmetric experience, and it enables a downstream system to decode reward timing.</p></caption><graphic xlink:href="EMS190989-f007"/></fig><fig id="F8" position="anchor"><label>Extended Data Fig 3</label><caption><title>Myopic learning bias</title><p id="P90"><bold>a</bold>, Maze to highlight the myopic learning bias. Rewards are indicated with water and fire. An example trajectory is shown with transparent arrows. The red and blue bars to the right denote the states in the Lower and Upper half. <bold>b</bold>, True (grey) and estimated (green and brown) values for the example trajectory on top and shown in panel a. In the x-axis we highlight the starting timestep with <italic>s</italic>, the timestep when the fire is reached and the timestep when the water is reached. <bold>c</bold>, Accuracy (y-axis) is measured as the Kendall tau coefficient between the estimate with a specific gamma (x-axis) and the true value function <italic>V</italic><sub><italic>γ</italic></sub> = 0.99. Error bars are deviations across 300 sets of sampled trajectories. The red (blue) curve shows average accuracy for the states on the upper (lower) half of the maze, indicated with color lines on panel a. <bold>d</bold>, As the sampled number of trajectories increases, the myopic learning bias disappears.</p></caption><graphic xlink:href="EMS190989-f008"/></fig><fig id="F9" position="anchor"><label>Extended Data Fig 4</label><caption><title>Single neuron responses and robustness of fit in the cued delay task</title><p id="P91"><bold>a</bold>, PSTHs of single selected neurons (<italic>n</italic> = 50) responses to the cues predicting a reward delay of 0.6s, 1.5s, 3.75s, and 9.375s (from top to bottom). Neurons are sorted by the inferred value of the discount factor <italic>γ</italic>. Neural responses are normalized by z-scoring each neuron across its activity to all 4 conditions. <bold>b</bold>, PSTHs of single non-selected neurons (<italic>n</italic> = 23) responses to the cues predicting a reward delay of (from top to bottom). Neurons are sorted by the inferred value of the discount factor <italic>γ</italic>. Neural responses are normalized by z-scoring each neuron across its activity to all 4 conditions. <bold>c</bold>, Variance explained for training vs testing data for the exponential model. For each bootstrap, the variance explained was computed on both the half of the trials used for fitting (train) and the other half of the trials (test). Neurons (<italic>n</italic> = 13) with a negative variance explained on the test data are excluded from the decoding analysis (grey dots). <bold>d</bold>, Same as panel <bold>c</bold> but for the fits for the hyperbolic model. <bold>e</bold>, Goodness of fit on held-out data for each selected neuron for the exponential and hyperbolic models. The data lies above the diagonal line suggesting a better fit from the exponential model as shown in <xref ref-type="fig" rid="F3">Fig. 3f</xref>. Error bars indicate 95% confidence interval using bootstrap. <bold>f</bold>, The values of the inferred parameters in the exponential model are robust across bootstraps. top row, Inferred value of the parameters across two halves of the trials (single bootstrap) for the gain α, baseline b and discount factor <italic>γ</italic> respectively. Bottom row, Distribution across <italic>n</italic> = 100 bootstraps of the Pearson correlations between the inferred parameter values in the two halves of the trials for the gain α (mean = 0.84, <italic>P</italic> &lt; 1 x 10<sup>-20</sup>), baseline b (v, mean = 0.9, <italic>P</italic> &lt; 1.0 x 10<sup>-32</sup>) and discount factor <italic>γ</italic> (vi, mean = 0.93, <italic>P</italic> &lt; 1.0 x 10<sup>-46</sup>). <bold>g</bold>, Same as panel <bold>e</bold> but for the hyperbolic model with distribution of correlations for the gain α (mean=0.86, p&lt;1e<sup>-26</sup>), baseline b (v, mean = 0.88, P &lt; 1.0 x 10<sup>-28</sup>) and shape parameter k (vi, mean = 0.76, <italic>P</italic> &lt; 1.0 x 10<sup>-11</sup>). <bold>h</bold>, Same as panel <bold>e</bold> and <bold>g</bold> but for the exponential model simulated responses with distribution of correlations for the gain α (mean = 0.86, <italic>P</italic> &lt; 1.0 x 10<sup>-10</sup>), baseline b (v, mean = 0.88, <italic>P</italic> &lt; 1.0 x 10<sup>-24</sup>) and discount factor <italic>γ</italic> (vi, mean = 0.76, <italic>P</italic> &lt; 1.0 x 10<sup>-26</sup>). Note that the distributions of inferred parameters are in a similar range than the fits to the data suggesting that trial numbers constrain the accuracy of parameter estimation. Significance is the highest <italic>p</italic>-value for all the bootstraps for a given parameters assessed via <italic>t</italic>-test.</p></caption><graphic xlink:href="EMS190989-f009"/></fig><fig id="F10" position="anchor"><label>Extended Data Fig 5</label><caption><title>Decoding reward timing using the regularized pseudo-inverse of the discount matrix</title><p id="P92"><bold>(a-c)</bold>, Singular value decomposition (SVD) of the discount matrix. <bold>a</bold>, left singular vectors (in the neuron space). <bold>b</bold>, Singular values. The black line at 2 indicates the values of the regularization term α. <bold>c</bold>, right singular vectors (in the time space). <bold>d</bold>, Decoding matrix based on the regularized pseudo-inverse. <bold>e</bold>, Distribution of 1-Wassertein distances between the reward timing and the predicted reward timing from the decoding on the test data exponential fits (shown in <xref ref-type="fig" rid="F3">Fig. 3k</xref>, top row) and on the shuffled data (shown if <xref ref-type="fig" rid="F3">Fig. 3k</xref>, bottom row). The prediction from the test data are better predictions (smaller 1-Wasserstein distance) than shuffled data (<italic>P</italic> = 1.2 x 10<sup>-4</sup> for 0.6 s reward delay, <italic>P</italic> &lt; 1.0 x 10<sup>-20</sup> for the other delays, one-tailed Wilcoxon signed rank test, see <xref ref-type="sec" rid="S11">Methods</xref>). <bold>f</bold>, Decoded subjective expected timing of future reward <italic>E</italic>(<italic>r</italic>|<italic>t</italic>) using a model with a single discount factor (the mean discount factor across the population, see <xref ref-type="sec" rid="S11">Methods</xref>). <bold>g</bold>, Distribution of 1-wassertein distances between the reward timing and the predicted reward timing from the decoding on the test data from exponential fits (shown in <xref ref-type="fig" rid="F3">Fig. 3k</xref>, top row) and on the average exponential model (shown in <bold>f</bold>). Decoding is better for the exponential model from <xref ref-type="fig" rid="F3">Fig. 3</xref> than the average exponential model except for the shortest delay (<italic>P</italic>(<italic>t</italic> = 0.6s) = 1, <italic>P</italic>(<italic>t</italic> = 1.5s) &lt; 1.0 x 10<sup>-31</sup>, <italic>P</italic>(<italic>t</italic> = 3.75) = 0.0135, <italic>P</italic>(<italic>t</italic> = 9.375s) &lt; 1.0 x 10<sup>-14</sup>), one-tailed Wilcoxon signed rank test, see <xref ref-type="sec" rid="S11">Methods</xref>).</p></caption><graphic xlink:href="EMS190989-f010"/></fig><fig id="F11" position="anchor"><label>Extended Data Fig 6</label><caption><title>Decoding reward timing from the first to the hyperbolic model and exponential model simulations</title><p id="P93"><bold>a</bold>, Distribution of the inferred discount parameter k across the neurons. <bold>b</bold>, Correlation between the discount factor inferred in the exponential model of the discount parameter k from the hyperbolic model (<italic>r</italic> = -0.9, <italic>P</italic> &lt; 1.0 x 10<sup>-30</sup>, <italic>t</italic>-test). Note the in the hyperbolic model a larger value of k implies faster discounting hence the negative correlation. <bold>c</bold>, Discount matrix for the hyperbolic model. For each neuron we plot the relative value of future events given its inferred discount parameter. Neurons are sorted by decreasing estimated value of the discount parameter. <bold>d</bold>, Decoded subjective expected timing of future reward <italic>E</italic>(<italic>r</italic>|<italic>t</italic>) using the discount matrix from the hyperbolic model (see <xref ref-type="sec" rid="S11">Methods</xref>). <bold>e</bold>, Distribution of 1-Wassertein distances between the reward timing and the predicted reward timing from the decoding on the test data with the exponential model (shown in <xref ref-type="fig" rid="F3">Fig. 3k</xref>, top row) and on the test data with the hyperbolic model (shown in <bold>d</bold>). Decoding is better for the exponential model from <xref ref-type="fig" rid="F3">Fig. 3</xref> than the hyperbolic model except for the shortest delay (<italic>P</italic>(<italic>t</italic> = 0.6s) = 1, <italic>P</italic>(<italic>t</italic> = 1.5s) &lt; 1.0 x 10<sup>-31</sup>, <italic>P</italic>(<italic>t</italic> = 3.75) &lt; 1.0 x 10<sup>-33</sup>, <italic>P</italic>(<italic>t</italic> = 9.375s) &lt; 1.0 x 10<sup>-3</sup>), one-tailed Wilcoxon signed rank test, see <xref ref-type="sec" rid="S11">Methods</xref>). <bold>f</bold>, Decoded subjective expected timing of future reward <italic>E</italic>(<italic>r</italic>|<italic>t</italic>) using simulated data based on the parameters of the exponential model (see <xref ref-type="sec" rid="S11">Methods</xref>). <bold>g</bold>, Distribution of 1-Wassertein distances between the reward timing and the predicted reward timing from the decoding on the test data from exponential fits (shown in <xref ref-type="fig" rid="F3">Fig. 3k</xref>, top row) and on the simulated data from the parameters of the exponential fits (shown in <bold>f</bold>). Decoding is marginally better for the data predictions (<italic>P</italic>(<italic>t</italic> = 0.6s) = 0.002, <italic>P</italic>(<italic>t</italic> = 1.5s) = 0.999, <italic>P</italic>(<italic>t</italic> = 3.75) &lt;1 x 10<sup>-12</sup>, <italic>P</italic>(<italic>t</italic> = 9.375s) = 0.027), one-tailed Wilcoxon signed rank test, see <xref ref-type="sec" rid="S11">Methods</xref>), suggesting that decoding accuracy is limited by the number of trials.</p></caption><graphic xlink:href="EMS190989-f011"/></fig><fig id="F12" position="anchor"><label>Extended Data Fig 7</label><caption><title>Ramping, discounting and anatomy</title><p id="P94"><bold>a</bold>, Ramping in the prediction error signal is controlled by the relative contribution of value increases and discounting. If the value increase (middle) exactly matches the discounting, there is no prediction error (middle equation, right). If the discounting is smaller than the value increase (large discount factor) then there is a positive TD error (top equation, right). If the discounting is larger (small discount factor) than the value increase then there a negative TD error (bottom equation, right). A single timescale agent with no state uncertainty will learn an exponential value function but if there is state uncertainty (see ref[<sup><xref ref-type="bibr" rid="R69">69</xref></sup>]) or the global value function arises from combining the contribution of single-timescale agents then the value function is likely t be non-exponential. <bold>b</bold>, The discount factor inferred in the VR task is not correlated with the medio-lateral (ML) position of the implant (Pearson’s <italic>r</italic> = 0.015, <italic>P</italic> = 0.89). <bold>c</bold>, The baseline parameter inferred in the VR task is not correlated with the medio-lateral (ML) position of the implant (Pearson’s <italic>r</italic> = -0.011, <italic>P</italic> = 0.92). <bold>d</bold>, The inferred gain in the VR task reduces with increasing medio-lateral (ML) position but the effect does not reach significance (Pearson’s <italic>r</italic> = -0.19, <italic>P</italic> = 0.069).</p></caption><graphic xlink:href="EMS190989-f012"/></fig><fig id="F13" position="anchor"><label>Extended Data Fig 8</label><caption><title>Discounting heterogeneity explains ramping diversity in a common reward expectation model</title><p id="P95"><bold>a</bold>, Uncertainty in reward timing reduces as mice approach the reward zone. Not only does the mean expected reward time reduces but the standard deviation of the estimate also reduces. Distribution in the bottom row from fitted data (see panels <bold>c-i</bold>). <bold>b</bold>, Simulations showing how reduction in uncertainty in reward timing (shared across neurons) and diverse discount factors lead to heterogeneous ramping activity in dopamine neurons. First panel. In this model, the uncertainty in the subjective estimate of reward timing (measured by the standard deviation) reduces as the mice approach the reward. Second panel. Distribution of subjective expected time to reward as a function of the true time to reward. The distribution is sampled from a folded normal distribution. The standard deviation reduces as reward approaches as shown in the first panel. Third panel. Given the subjective expected time to reward, common to all neurons due to a single world mode, we can compute a value function for each neuron given its discount factor. Fourth panel. This leads to a heterogeneity of TD errors across neurons, including monotonic upward and downwards ramps as well as non-monotonic ramps. <bold>c</bold>, The inferred standard deviation of the reward expectation model reduces as a function of time to reward. Line indicates the mean inferred standard deviation and the shading indicates the standard error of the mean over 100 bootstraps. <bold>d</bold>, Subjective expected timing of the reward as a function of true time to reward. As the mice approach the reward not only does the mean expected time to reward reduces but the uncertainty of the reward timing captured by the standard deviation shown in <bold>c</bold> also reduces. This effect leads to increasingly convex value functions that lead to the observed ramps in dopamine neuron activity. <bold>e</bold>, Value function for each individual neuron. <bold>f</bold>, Distribution of inferred discount factors under the common reward expectation model. g, Although the range of discount factor between the fits from the common value (x-axis) and common reward expectation (y-axis) models differs, the inferred discount factors are strongly correlated for single neurons (Spearman’s <italic>ρ</italic> = 0.93, <italic>P</italic> &lt; 1.0 x 10<sup>-20</sup>). <bold>h</bold>, Predicted ramping activity from the model fits under the common reward expectation model. <bold>i</bold>, Diversity of ramping activity across single neurons as mice approach reward (aligned by inferred discount factor in the common reward expectation model).</p></caption><graphic xlink:href="EMS190989-f013"/></fig><fig id="F14" position="anchor"><label>Extended Data Fig 9</label><caption><title>Decoding reward timing in the cud delayed reward task using parameters inferred in the VR task</title><p id="P96"><bold>a</bold>, Discount matrix computed using the parameters inferred in the VR tasks for neurons recorded across both tasks and used in the cross-task decoding. <bold>b</bold>, Dopamine neurons cue responses in the cued delay task. Neurons are aligned as in <bold>a</bold> according to increasing discount factor inferred in the VR task. <bold>c</bold>, Top row: Decoded reward timing using discount factors inferred in the VR task. Bottom row: The ability to decode reward timing is lost when shuffling the identities of the cue responses. <bold>d</bold>, Except for the shortest delay, decoded reward timing is more accurate than shuffle as measured by the 1-Wassertsein distance (<italic>P</italic><sub><italic>t</italic> = 0.6s</sub> = 1, <italic>P</italic><sub><italic>t</italic> = 1.5s</sub> &lt; 1.1 x 10<sup>-20</sup>, <italic>P</italic><sub><italic>t</italic> = 3.75s</sub> &lt; 3.8 x 10<sup>-20</sup>, <italic>P</italic><sub><italic>t</italic> = 9.375s</sub> &lt; 2.9 x 10<sup>-5</sup>).</p></caption><graphic xlink:href="EMS190989-f014"/></fig></sec></body><back><ack id="S30"><title>Acknowledgements</title><p>We thank Dr. Samuel Gershman and Dr. John Mikhael for their contributions to the preceding studies and Dr. Mitsuko Watabe-Uchida for advice on task design. We thank the members of the Uchida and Pouget labs including Adam Lowet and Mark Burrell for discussions and comments. We also thank Wilka Carvalho, Gautam Reddy and Torben Ott for their comments on the manuscript. This work is supported by NIH BRAIN Initiative grants (R01NS226753, U19NS113201) and NIH grant 5R01DC017311 to N.U and by a grant from the Swiss National Science Foundation (315230_197296) to A.P.</p></ack><sec id="S31" sec-type="data-availability"><title>Data availability</title><p>The code used for simulations can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/pablotano8/multi_timescale_RL">https://github.com/pablotano8/multi_timescale_RL</ext-link>. The data code for the electrophysiological experiments and the corresponding analysis code will be uploaded to a public repository upon acceptance.</p></sec><fn-group><fn fn-type="con" id="FN3"><p id="P97"><bold>Author contributions</bold></p><p id="P98">P.M., P.T., A.P. and N.U. conceived the project. P.M., H.R.K, A.N.M and N.U designed the electrophysiology experiments. A.N.M and H.R.K. performed the electrophysiology experiments and curated data. P.T. Performed simulations with artificial agents. P.M. Performed analysis of electrophysiological data. P.M., P.T., A.P. and N.U. wrote the paper with input from H.R.K.</p></fn><fn id="FN4" fn-type="conflict"><p id="P99"><bold>Competing interest statement</bold></p><p id="P100">The authors declare no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning series)</source><publisher-name>A Bradford Book</publisher-name><year>2018</year><volume>552</volume></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tesauro</surname><given-names>G</given-names></name></person-group><article-title>Temporal difference learning and TD-Gammon</article-title><source>Commun ACM</source><year>1995</year><volume>38</volume><fpage>58</fpage><lpage>68</lpage></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><etal/></person-group><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><year>2015</year><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><etal/></person-group><article-title>Mastering the game of Go with deep neural networks and tree search</article-title><source>Nature</source><year>2016</year><volume>529</volume><fpage>484</fpage><lpage>489</lpage><pub-id pub-id-type="pmid">26819042</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecoffet</surname><given-names>A</given-names></name><name><surname>Huizinga</surname><given-names>J</given-names></name><name><surname>Lehman</surname><given-names>J</given-names></name><name><surname>Stanley</surname><given-names>KO</given-names></name><name><surname>Clune</surname><given-names>J</given-names></name></person-group><article-title>First return, then explore</article-title><source>Nature</source><year>2021</year><volume>590</volume><fpage>580</fpage><lpage>586</lpage><pub-id pub-id-type="pmid">33627813</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wurman</surname><given-names>PR</given-names></name><etal/></person-group><article-title>Outracing champion Gran Turismo drivers with deep reinforcement learning</article-title><source>Nature</source><year>2022</year><volume>602</volume><fpage>223</fpage><lpage>228</lpage><pub-id pub-id-type="pmid">35140384</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><year>1997</year><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Neuronal reward and decision signals: from theories to data</article-title><source>Physiol Rev</source><year>2015</year><volume>95</volume><fpage>853</fpage><lpage>951</lpage><pub-id pub-id-type="pmcid">PMC4491543</pub-id><pub-id pub-id-type="pmid">26109341</pub-id><pub-id pub-id-type="doi">10.1152/physrev.00023.2014</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>JY</given-names></name><name><surname>Haesler</surname><given-names>S</given-names></name><name><surname>Vong</surname><given-names>L</given-names></name><name><surname>Lowell</surname><given-names>BB</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Neuron-type-specific signals for reward and punishment in the ventral tegmental area</article-title><source>Nature</source><year>2012</year><volume>482</volume><fpage>85</fpage><lpage>88</lpage><pub-id pub-id-type="pmcid">PMC3271183</pub-id><pub-id pub-id-type="pmid">22258508</pub-id><pub-id pub-id-type="doi">10.1038/nature10754</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Commons</surname><given-names>ML</given-names></name><name><surname>Mazur</surname><given-names>JE</given-names></name><name><surname>Nevin</surname><given-names>JA</given-names></name><name><surname>Rachlin</surname><given-names>H</given-names></name></person-group><source>Effect Of Delay And Of Intervening Events On Reinforcement Value</source><publisher-name>Taylor &amp; Francis Group</publisher-name><year>2013</year><volume>344</volume></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ainslie</surname><given-names>G</given-names></name></person-group><article-title>Specious reward: a behavioral theory of impulsiveness and impulse control</article-title><source>Psychol Bull</source><year>1975</year><volume>82</volume><fpage>463</fpage><lpage>496</lpage><pub-id pub-id-type="pmid">1099599</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frederick</surname><given-names>S</given-names></name><name><surname>Loewenstein</surname><given-names>G</given-names></name><name><surname>O’Donoghue</surname><given-names>T</given-names></name></person-group><article-title>Time Discounting and Time Preference: A Critical Review</article-title><source>J Econ Lit</source><year>2002</year><volume>40</volume><fpage>351</fpage><lpage>401</lpage></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laibson</surname><given-names>D</given-names></name></person-group><article-title>Golden Eggs and Hyperbolic Discounting</article-title><source>Q J Econ</source><year>1997</year><volume>112</volume><fpage>443</fpage><lpage>478</lpage></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sozou</surname><given-names>PD</given-names></name></person-group><article-title>On hyperbolic discounting and uncertain hazard rates</article-title><source>Proceedings of the Royal Society B: Biological Sciences</source><year>1998</year><volume>265</volume><fpage>2015</fpage><lpage>2020</lpage></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RP</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nat Neurosci</source><year>1999</year><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><article-title>Predictive processing: A canonical cortical computation</article-title><source>Neuron</source><year>2018</year><volume>100</volume><fpage>424</fpage><lpage>435</lpage><pub-id pub-id-type="pmcid">PMC6400266</pub-id><pub-id pub-id-type="pmid">30359606</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><source>A Path Towards Autonomous Machine Intelligence</source><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://openreview.net/forum?id=BZ5a1r-kVsf">https://openreview.net/forum?id=BZ5a1r-kVsf</ext-link></comment></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Bowling</surname><given-names>MH</given-names></name><name><surname>Pilarski</surname><given-names>PM</given-names></name></person-group><article-title>The Alberta Plan for AI Research</article-title><source>arXiv</source><year>2022</year><pub-id pub-id-type="doi">10.48550/arxiv.2208.11173</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name></person-group><article-title>Learning to predict by the methods of temporal differences</article-title><source>Mach Learn</source><year>1988</year><volume>3</volume><fpage>9</fpage><lpage>44</lpage></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><etal/></person-group><article-title>Continuous control with deep reinforcement learning</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arxiv.1509.02971</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Narasimhan</surname><given-names>K</given-names></name><name><surname>Kulkarni</surname><given-names>T</given-names></name><name><surname>Barzilay</surname><given-names>R</given-names></name></person-group><article-title>Language Understanding for Text-based Games Using Deep Reinforcement Learning</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arxiv.1506.08941</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><etal/></person-group><article-title>Asynchronous Methods for Deep Reinforcement Learning</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arxiv.1602.01783</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><etal/></person-group><article-title>Reinforcement learning, fast and slow</article-title><source>Trends Cogn Sci (Regul Ed)</source><year>2019</year><volume>23</volume><fpage>408</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">31003893</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>MPH</given-names></name><name><surname>Schoenbaum</surname><given-names>G</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Rethinking dopamine as generalized prediction error</article-title><source>Proc Biol Sci</source><year>2018</year><volume>285</volume><pub-id pub-id-type="pmcid">PMC6253385</pub-id><pub-id pub-id-type="pmid">30464063</pub-id><pub-id pub-id-type="doi">10.1098/rspb.2018.1645</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dabney</surname><given-names>W</given-names></name><etal/></person-group><article-title>A distributional code for value in dopamine-based reinforcement learning</article-title><source>Nature</source><year>2020</year><volume>577</volume><fpage>671</fpage><lpage>675</lpage><pub-id pub-id-type="pmcid">PMC7476215</pub-id><pub-id pub-id-type="pmid">31942076</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1924-6</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>J</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name></person-group><article-title>Striatal circuits for reward learning and decision-making</article-title><source>Nat Rev Neurosci</source><year>2019</year><volume>20</volume><fpage>482</fpage><lpage>494</lpage><pub-id pub-id-type="pmcid">PMC7231228</pub-id><pub-id pub-id-type="pmid">31171839</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0189-2</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Multiple dopamine systems: weal and woe of dopamine</article-title><source>Cold Spring Harb Symp Quant Biol</source><year>2018</year><volume>83</volume><fpage>83</fpage><lpage>95</lpage><pub-id pub-id-type="pmid">30787046</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engelhard</surname><given-names>B</given-names></name><etal/></person-group><article-title>Specialized coding of sensory, motor and cognitive variables in VTA dopamine neurons</article-title><source>Nature</source><year>2019</year><volume>570</volume><fpage>509</fpage><lpage>513</lpage><pub-id pub-id-type="pmcid">PMC7147811</pub-id><pub-id pub-id-type="pmid">31142844</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1261-9</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Believing in dopamine</article-title><source>Nat Rev Neurosci</source><year>2019</year><volume>20</volume><fpage>703</fpage><lpage>714</lpage><pub-id pub-id-type="pmcid">PMC7472313</pub-id><pub-id pub-id-type="pmid">31570826</pub-id><pub-id pub-id-type="doi">10.1038/s41583-019-0220-7</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamid</surname><given-names>AA</given-names></name><etal/></person-group><article-title>Mesolimbic dopamine signals the value of work</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>117</fpage><lpage>126</lpage><pub-id pub-id-type="pmcid">PMC4696912</pub-id><pub-id pub-id-type="pmid">26595651</pub-id><pub-id pub-id-type="doi">10.1038/nn.4173</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohebi</surname><given-names>A</given-names></name><etal/></person-group><article-title>Dissociable dopamine dynamics for learning and motivation</article-title><source>Nature</source><year>2019</year><volume>570</volume><fpage>65</fpage><lpage>70</lpage><pub-id pub-id-type="pmcid">PMC6555489</pub-id><pub-id pub-id-type="pmid">31118513</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1235-y</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berke</surname><given-names>JD</given-names></name></person-group><article-title>What does dopamine mean?</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>787</fpage><lpage>793</lpage><pub-id pub-id-type="pmcid">PMC6358212</pub-id><pub-id pub-id-type="pmid">29760524</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0152-y</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ainslie</surname><given-names>G</given-names></name></person-group><article-title>Specious reward: a behavioral theory of impulsiveness and impulse control</article-title><source>Psychol Bull</source><year>1975</year><volume>82</volume><fpage>463</fpage><lpage>496</lpage></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>P</given-names></name><name><surname>Maskin</surname><given-names>E</given-names></name></person-group><article-title>Uncertainty and hyperbolic discounting</article-title><source>American Economic Review</source><year>2005</year><volume>95</volume><fpage>1290</fpage><lpage>1299</lpage></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>P</given-names></name></person-group><article-title>Discounting climate change</article-title><source>J Risk Uncertain</source><year>2008</year><volume>37</volume><fpage>141</fpage><lpage>169</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Addiction as a computational process gone awry</article-title><source>Science</source><year>2004</year><volume>306</volume><fpage>1944</fpage><lpage>1947</lpage><pub-id pub-id-type="pmid">15591205</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milenkova</surname><given-names>M</given-names></name><etal/></person-group><article-title>Intertemporal choice in Parkinson’s disease</article-title><source>Mov Disord</source><year>2011</year><volume>26</volume><fpage>2004</fpage><lpage>2010</lpage><pub-id pub-id-type="pmid">21567457</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lempert</surname><given-names>KM</given-names></name><name><surname>Phelps</surname><given-names>EA</given-names></name></person-group><article-title>The malleability of intertemporal choice</article-title><source>Trends Cogn Sci (Regul Ed)</source><year>2016</year><volume>20</volume><fpage>64</fpage><lpage>74</lpage><pub-id pub-id-type="pmcid">PMC4698025</pub-id><pub-id pub-id-type="pmid">26483153</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2015.09.005</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lempert</surname><given-names>KM</given-names></name><name><surname>Steinglass</surname><given-names>JE</given-names></name><name><surname>Pinto</surname><given-names>A</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Simpson</surname><given-names>HB</given-names></name></person-group><article-title>Can delay discounting deliver on the promise of RDoC?</article-title><source>Psychol Med</source><year>2019</year><volume>49</volume><fpage>190</fpage><lpage>199</lpage><pub-id pub-id-type="pmid">30070191</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><etal/></person-group><source>Horde: A Scalable Real-Time Architecture for Learning Knowledge from Unsupervised Sensorimotor Interaction</source><conf-name>The 10th International Conference on Autonomous Agents and Multiagent Systems International Foundation for Autonomous Agents and Multiagent Systems</conf-name><year>2011</year><volume>2</volume><fpage>761</fpage><lpage>768</lpage></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Rowland</surname><given-names>M</given-names></name></person-group><source>Distributional reinforcement learning</source><year>2023</year><publisher-name>The MIT Press</publisher-name></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stadie</surname><given-names>BC</given-names></name><name><surname>Levine</surname><given-names>S</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name></person-group><article-title>Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arxiv.1507.00814</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><article-title>Prioritized Experience Replay</article-title><source>arXiv</source><year>2015</year><pub-id pub-id-type="doi">10.48550/arxiv.1511.05952</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaderberg</surname><given-names>M</given-names></name><etal/></person-group><article-title>Reinforcement Learning with Unsupervised Auxiliary Tasks</article-title><source>arXiv</source><year>2016</year><pub-id pub-id-type="doi">10.48550/arxiv.1611.05397</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tano</surname><given-names>P</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>A local temporal difference code for distributional reinforcement learning</article-title><source>NeurIPS</source><year>2020</year><volume>33</volume><fpage>13662</fpage><lpage>13673</lpage></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunec</surname><given-names>IK</given-names></name><name><surname>Momennejad</surname><given-names>I</given-names></name></person-group><article-title>Predictive representations in hippocampal and prefrontal hierarchies</article-title><source>J Neurosci</source><year>2022</year><volume>42</volume><fpage>299</fpage><lpage>312</lpage><pub-id pub-id-type="pmcid">PMC8802932</pub-id><pub-id pub-id-type="pmid">34799416</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1327-21.2021</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mauk</surname><given-names>MD</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><article-title>The neural basis of temporal processing</article-title><source>Annu Rev Neurosci</source><year>2004</year><volume>27</volume><fpage>307</fpage><lpage>340</lpage><pub-id pub-id-type="pmid">15217335</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buhusi</surname><given-names>CV</given-names></name><name><surname>Meck</surname><given-names>WH</given-names></name></person-group><article-title>What makes us tick? Functional and neural mechanisms of interval timing</article-title><source>Nat Rev Neurosci</source><year>2005</year><volume>6</volume><fpage>755</fpage><lpage>765</lpage><pub-id pub-id-type="pmid">16163383</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>A</given-names></name><name><surname>Yousefzadeh</surname><given-names>SA</given-names></name><name><surname>Meck</surname><given-names>WH</given-names></name><name><surname>Moser</surname><given-names>M-B</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><article-title>The neural bases for timing of durations</article-title><source>Nat Rev Neurosci</source><year>2022</year><volume>23</volume><fpage>646</fpage><lpage>665</lpage><pub-id pub-id-type="pmid">36097049</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiorillo</surname><given-names>CD</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>The temporal precision of reward prediction in dopamine neurons</article-title><source>Nat Neurosci</source><year>2008</year><volume>11</volume><fpage>966</fpage><lpage>973</lpage><pub-id pub-id-type="pmid">18660807</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mello</surname><given-names>GBM</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><article-title>A scalable population code for time in the striatum</article-title><source>Curr Biol</source><year>2015</year><volume>25</volume><fpage>1113</fpage><lpage>1122</lpage><pub-id pub-id-type="pmid">25913405</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name></person-group><article-title>Midbrain dopamine neurons control judgment of time</article-title><source>Science</source><year>2016</year><volume>354</volume><fpage>1273</fpage><lpage>1277</lpage><pub-id pub-id-type="pmid">27940870</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enomoto</surname><given-names>K</given-names></name><name><surname>Matsumoto</surname><given-names>N</given-names></name><name><surname>Inokawa</surname><given-names>H</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name><name><surname>Yamada</surname><given-names>H</given-names></name></person-group><article-title>Topographic distinction in long-term value signals between presumed dopamine neurons and presumed striatal projection neurons in behaving monkeys</article-title><source>Sci Rep</source><year>2020</year><volume>10</volume><fpage>8912</fpage><pub-id pub-id-type="pmcid">PMC7265398</pub-id><pub-id pub-id-type="pmid">32488042</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-65914-0</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiebel</surname><given-names>SJ</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><article-title>A hierarchy of time-scales and the brain</article-title><source>PLoS Comput Biol</source><year>2008</year><volume>4</volume><elocation-id>e1000209</elocation-id><pub-id pub-id-type="pmcid">PMC2568860</pub-id><pub-id pub-id-type="pmid">19008936</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000209</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name></person-group><article-title>Temporal-difference reinforcement learning with distributed representations</article-title><source>PLoS ONE</source><year>2009</year><volume>4</volume><elocation-id>e7362</elocation-id><pub-id pub-id-type="pmcid">PMC2760757</pub-id><pub-id pub-id-type="pmid">19841749</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0007362</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name><name><surname>Barto</surname><given-names>AC</given-names></name></person-group><article-title>Hierarchically organized behavior and its neural foundations: a reinforcement learning perspective</article-title><source>Cognition</source><year>2009</year><volume>113</volume><fpage>262</fpage><lpage>280</lpage><pub-id pub-id-type="pmcid">PMC2783353</pub-id><pub-id pub-id-type="pmid">18926527</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2008.08.011</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shankar</surname><given-names>KH</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>A scale-invariant internal representation of time</article-title><source>Neural Comput</source><year>2012</year><volume>24</volume><fpage>134</fpage><lpage>193</lpage><pub-id pub-id-type="pmid">21919782</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>SC</given-names></name><etal/></person-group><article-title>Prediction of immediate and future rewards differentially recruits cortico-basal ganglia loops</article-title><source>Nat Neurosci</source><year>2004</year><volume>7</volume><fpage>887</fpage><lpage>893</lpage><pub-id pub-id-type="pmid">15235607</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W</given-names></name><name><surname>Mohebi</surname><given-names>A</given-names></name><name><surname>Berke</surname><given-names>J</given-names></name></person-group><article-title>Striatal dopamine pulses follow a temporal discounting spectrum</article-title><source>BioRxiv</source><year>2021</year><pub-id pub-id-type="doi">10.1101/2021.10.31.466705</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedus</surname><given-names>W</given-names></name><name><surname>Gelada</surname><given-names>C</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name></person-group><article-title>Hyperbolic Discounting and Learning over Multiple Horizons</article-title><source>arXiv</source><year>2019</year></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sherstan</surname><given-names>C</given-names></name><name><surname>Dohare</surname><given-names>S</given-names></name><name><surname>MacGlashan</surname><given-names>J</given-names></name><name><surname>Günther</surname><given-names>J</given-names></name><name><surname>Pilarski</surname><given-names>PM</given-names></name></person-group><article-title>Gamma-Nets: Generalizing Value Estimation over Timescale</article-title><source>AAAI</source><year>2020</year><volume>34</volume><fpage>5717</fpage><lpage>5725</lpage></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Momennejad</surname><given-names>I</given-names></name><name><surname>Howard</surname><given-names>MW</given-names></name></person-group><article-title>Predicting the future with multi-scale successor representations</article-title><source>BioRxiv</source><year>2018</year><pub-id pub-id-type="doi">10.1101/449470</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kobayashi</surname><given-names>S</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Influence of reward delays on responses of dopamine neurons</article-title><source>J Neurosci</source><year>2008</year><volume>28</volume><fpage>7837</fpage><lpage>7846</lpage><pub-id pub-id-type="pmcid">PMC3844811</pub-id><pub-id pub-id-type="pmid">18667616</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1600-08.2008</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Dopamine reward prediction-error signalling: a two-component response</article-title><source>Nat Rev Neurosci</source><year>2016</year><volume>17</volume><fpage>183</fpage><lpage>195</lpage><pub-id pub-id-type="pmcid">PMC5549862</pub-id><pub-id pub-id-type="pmid">26865020</pub-id><pub-id pub-id-type="doi">10.1038/nrn.2015.26</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>H</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><article-title>Midbrain dopamine neurons signal aversion in a reward-context-dependent manner</article-title><source>eLife</source><year>2016</year><volume>5</volume><pub-id pub-id-type="pmcid">PMC5070948</pub-id><pub-id pub-id-type="pmid">27760002</pub-id><pub-id pub-id-type="doi">10.7554/eLife.17328</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howe</surname><given-names>MW</given-names></name><name><surname>Tierney</surname><given-names>PL</given-names></name><name><surname>Sandberg</surname><given-names>SG</given-names></name><name><surname>Phillips</surname><given-names>PEM</given-names></name><name><surname>Graybiel</surname><given-names>AM</given-names></name></person-group><article-title>Prolonged dopamine signalling in striatum signals proximity and value of distant rewards</article-title><source>Nature</source><year>2013</year><volume>500</volume><fpage>575</fpage><lpage>579</lpage><pub-id pub-id-type="pmcid">PMC3927840</pub-id><pub-id pub-id-type="pmid">23913271</pub-id><pub-id pub-id-type="doi">10.1038/nature12475</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>Dopamine ramps are a consequence of reward prediction errors</article-title><source>Neural Comput</source><year>2014</year><volume>26</volume><fpage>467</fpage><lpage>471</lpage><pub-id pub-id-type="pmid">24320851</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HR</given-names></name><etal/></person-group><article-title>A Unified Framework for Dopamine Signals across Timescales</article-title><source>Cell</source><year>2020</year><volume>183</volume><fpage>1600</fpage><lpage>1616</lpage><elocation-id>e25</elocation-id><pub-id pub-id-type="pmcid">PMC7736562</pub-id><pub-id pub-id-type="pmid">33248024</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2020.11.013</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Kim</surname><given-names>HR</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The role of state uncertainty in the dynamics of dopamine</article-title><source>Curr Biol</source><year>2022</year><volume>32</volume><fpage>1077</fpage><lpage>1087</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="pmcid">PMC8930519</pub-id><pub-id pub-id-type="pmid">35114098</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2022.01.025</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guru</surname><given-names>A</given-names></name><etal/></person-group><article-title>Ramping activity in midbrain dopamine neurons signifies the use of a cognitive map</article-title><source>BioRxiv</source><year>2020</year><pub-id pub-id-type="doi">10.1101/2020.05.21.108886</pub-id></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><article-title>Reinforcement learning in continuous time and space</article-title><source>Neural Comput</source><year>2000</year><volume>12</volume><fpage>219</fpage><lpage>245</lpage><pub-id pub-id-type="pmid">10636940</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>RS</given-names></name><name><surname>Engelhard</surname><given-names>B</given-names></name><name><surname>Witten</surname><given-names>IB</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A vector reward prediction error model explains dopaminergic heterogeneity</article-title><source>BioRxiv</source><year>2022</year><pub-id pub-id-type="doi">10.1101/2022.02.28.482379</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lowet</surname><given-names>AS</given-names></name><name><surname>Zheng</surname><given-names>Q</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Distributional reinforcement learning in the brain</article-title><source>Trends Neurosci</source><year>2020</year><volume>43</volume><fpage>980</fpage><lpage>997</lpage><pub-id pub-id-type="pmcid">PMC8073212</pub-id><pub-id pub-id-type="pmid">33092893</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2020.09.004</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millidge</surname><given-names>BG</given-names></name><name><surname>Song</surname><given-names>Y</given-names></name><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><article-title>Reward-Bases: Dopaminergic Mechanisms for Adaptive Acquisition of Multiple Reward Types</article-title><source>BioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.05.09.540067</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cruz</surname><given-names>BF</given-names></name><etal/></person-group><article-title>Action suppression reveals opponent parallel control via striatal circuits</article-title><source>Nature</source><year>2022</year><volume>607</volume><fpage>521</fpage><lpage>526</lpage><pub-id pub-id-type="pmid">35794480</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eshel</surname><given-names>N</given-names></name><name><surname>Tian</surname><given-names>J</given-names></name><name><surname>Bukwich</surname><given-names>M</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><article-title>Dopamine neurons share common response function for reward prediction error</article-title><source>Nat Neurosci</source><year>2016</year><volume>19</volume><fpage>479</fpage><lpage>486</lpage><pub-id pub-id-type="pmcid">PMC4767554</pub-id><pub-id pub-id-type="pmid">26854803</pub-id><pub-id pub-id-type="doi">10.1038/nn.4239</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Menegas</surname><given-names>W</given-names></name><name><surname>Akiti</surname><given-names>K</given-names></name><name><surname>Amo</surname><given-names>R</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Watabe-Uchida</surname><given-names>M</given-names></name></person-group><article-title>Dopamine neurons projecting to the posterior striatum reinforce avoidance of threatening stimuli</article-title><source>Nat Neurosci</source><year>2018</year><volume>21</volume><fpage>1421</fpage><lpage>1430</lpage><pub-id pub-id-type="pmcid">PMC6160326</pub-id><pub-id pub-id-type="pmid">30177795</pub-id><pub-id pub-id-type="doi">10.1038/s41593-018-0222-1</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collins</surname><given-names>AL</given-names></name><name><surname>Saunders</surname><given-names>BT</given-names></name></person-group><article-title>Heterogeneity in striatal dopamine circuits: Form and function in dynamic reward seeking</article-title><source>J Neurosci Res</source><year>2020</year><volume>98</volume><fpage>1046</fpage><lpage>1069</lpage><pub-id pub-id-type="pmcid">PMC7183907</pub-id><pub-id pub-id-type="pmid">32056298</pub-id><pub-id pub-id-type="doi">10.1002/jnr.24587</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name></person-group><article-title>Asymmetric and adaptive reward coding via normalized reinforcement learning</article-title><source>PLoS Comput Biol</source><year>2022</year><volume>18</volume><elocation-id>e1010350</elocation-id><pub-id pub-id-type="pmcid">PMC9345478</pub-id><pub-id pub-id-type="pmid">35862443</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1010350</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Z</given-names></name><name><surname>van Hasselt</surname><given-names>HP</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><article-title>Meta-Gradient Reinforcement Learning</article-title><source>Advances in Neural Information Processing Systems</source><year>2018</year></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>N</given-names></name><name><surname>Uchibe</surname><given-names>E</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name></person-group><source>Reinforcement learning with state-dependent discount factor in 2013 IEEE Third Joint International Conference on Development and Learning and Epigenetic Robotics (ICDL)</source><publisher-name>IEEE</publisher-name><year>2013</year><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1109/DevLrn.2013.6652533</pub-id></element-citation></ref><ref id="R82"><label>82</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schlegel</surname><given-names>M</given-names></name><etal/></person-group><article-title>General value function networks</article-title><source>jair</source><year>2021</year><volume>70</volume><fpage>497</fpage><lpage>543</lpage></element-citation></ref><ref id="R83"><label>83</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kvitsiani</surname><given-names>D</given-names></name><etal/></person-group><article-title>Distinct behavioural and network correlates of two interneuron types in prefrontal cortex</article-title><source>Nature</source><year>2013</year><volume>498</volume><fpage>363</fpage><lpage>366</lpage><pub-id pub-id-type="pmcid">PMC4349584</pub-id><pub-id pub-id-type="pmid">23708967</pub-id><pub-id pub-id-type="doi">10.1038/nature12176</pub-id></element-citation></ref><ref id="R84"><label>84</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulkumaran</surname><given-names>K</given-names></name><name><surname>Deisenroth</surname><given-names>MP</given-names></name><name><surname>Brundage</surname><given-names>M</given-names></name><name><surname>Bharath</surname><given-names>AA</given-names></name></person-group><article-title>Deep reinforcement learning: A brief survey</article-title><source>IEEE Signal Process Mag</source><year>2017</year><volume>34</volume><fpage>26</fpage><lpage>38</lpage></element-citation></ref><ref id="R85"><label>85</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oppenheim</surname><given-names>A</given-names></name><name><surname>Willsky</surname><given-names>A</given-names></name><name><surname>Hamid</surname><given-names>W</given-names></name></person-group><source>Signals and Systems 1000</source><publisher-name>Pearson</publisher-name><year>1996</year></element-citation></ref><ref id="R86"><label>86</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Improving Generalization for Temporal Difference Learning: The Successor Representation</article-title><source>Neural Comput</source><year>1993</year><volume>5</volume><fpage>613</fpage><lpage>624</lpage></element-citation></ref><ref id="R87"><label>87</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><article-title>The successor representation: its computational logic and neural substrates</article-title><source>J Neurosci</source><year>2018</year><volume>38</volume><fpage>7193</fpage><lpage>7200</lpage><pub-id pub-id-type="pmcid">PMC6096039</pub-id><pub-id pub-id-type="pmid">30006364</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0151-18.2018</pub-id></element-citation></ref><ref id="R88"><label>88</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>R</given-names></name><name><surname>Meir</surname><given-names>R</given-names></name><name><surname>Ciosek</surname><given-names>K</given-names></name></person-group><source>Discount Factor as a Regularizer in Reinforcement Learning</source><publisher-name>PMLR</publisher-name><year>2020</year></element-citation></ref><ref id="R89"><label>89</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Badia</surname><given-names>AP</given-names></name><etal/></person-group><source>Agent57: Outperforming the Atari Human Benchmark</source><publisher-name>PMLR</publisher-name><year>2020</year></element-citation></ref><ref id="R90"><label>90</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leone</surname><given-names>FC</given-names></name><name><surname>Nelson</surname><given-names>LS</given-names></name><name><surname>Nottingham</surname><given-names>RB</given-names></name></person-group><article-title>The folded normal distribution</article-title><source>Technometrics</source><year>1961</year><volume>3</volume><fpage>543</fpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Single timescale and multi-timescale reinforcement learning</title><p id="P101"><bold>a</bold>, In single-timescale value learning, the value of a cue (at t = 0) predicting future rewards (first panel) is evaluated by discounting these rewards with a single exponential discounting function (second panel). The expected reward size and timing are encoded, but confounded, in the value of the cue (third panel). <bold>b</bold>, In multi-timescale value learning, the same reward delays are evaluated with multiple discounting functions (second panel). The relative value of a cue as a function of the discount depends on the reward delay (third panel). A simple linear decoder based on the Laplace transform can thus reconstruct both the expected timing and magnitude of rewards (fourth panel).</p></caption><graphic xlink:href="EMS190989-f001"/></fig><fig id="F2" position="float"><label>Fig 2</label><caption><title>Computational advantages of multi-timescale reinforcement learning</title><p id="P102"><bold>a</bold>, Experiment to compare single-vs. multi-timescale learning. <bold>b</bold>, Architecture to evaluate multi-timescale advantages. In each episode (defined by a specific R, t<sub>R</sub> and N) the value function is learned via tabular updates. The policy gradient network is trained across episodes to maximize the accuracy of the report. <bold>c</bold>, The timing t<sub>R</sub> and reward size R is varied across episodes, the task of the policy gradient (PG) network is to report t<sub>R</sub>. <bold>d</bold>, The timing t<sub>R</sub> and reward size R is varied across episodes, the task is to report the inferred value of s using a hyperbolic discount. <bold>e</bold>, The timing t<sub>R</sub> and number of sampled trajectories N is varied across episodes, the task of the policy gradient (PG) network is to report t<sub>R</sub>. In <bold>c-e</bold>, Performance is reported after 1,000 training episodes. Error bars are the standard deviations (s.d.) across 100 test episodes and 3 trained policy gradient (PG) networks. <bold>f</bold>, Myopic learning bias. Top: Task structure to evaluate the learning bias induced by the discount factor, the three dots collapse 5 transitions between black states. Bottom: Performance at selecting the branch with the large deterministic reward under incomplete learning conditions. At state s (orange), agents with larger discount factors (far-sighted) are more accurate. At state s’ (blue), agents with a small discount factor (myopic) are more accurate. Error bars are half s.d. across 10,000 episodes, maximums are highlighted with stars. <bold>g</bold>, Top: Architecture that learns about multiple timescales as auxiliary tasks. Bottom: Accuracy of the Q-values in the Lunar Lander environment as a function of their discount factor, estimated as the fraction of concordant state pairs between the empirical value function and the discount specific Q-value estimated by the network, when the agent is close to the goal (blue) or far from the goal (orange), see <xref ref-type="sec" rid="S11">Methods</xref> for details. Error bars are s.e.m across 10 trained networks, maximums are highlighted with stars.</p></caption><graphic xlink:href="EMS190989-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Dopamine neurons exhibit a diversity of discount factors that enables decoding of reward delays</title><p id="P103"><bold>a</bold>, Outline of the task structure. <bold>b</bold>, The mice exhibit anticipatory licking prior to reward delivery for all 4 reward delays indicating that they have learned task contingencies (mean across behavior for all recorded neurons, shaded error bar indicates 95% confidence interval using bootstrap). <bold>c</bold>, Average PSTH across the task for the 4 trial types. Inset shows the firing rate in the 0.5s following the cue predicting reward delay. The firing rate in the shaded grey box (0.1s &lt; t &lt; 0.4s) was used as the cue response in subsequent analysis. <bold>d</bold>, Example of fits of the responses to the cue predicting reward delay of two single neurons with high (top panel) and low (bottom panel) discount factors. <bold>e</bold>, Normalized response to the cues predicting reward delays across the population. For each neuron, the response was normalized to the highest response across the 4 possible delays. Inset on right, corresponding inferred discount factor for each neuron. <bold>f</bold>. The exponential model is a better fit to the data than the hyperbolic one as quantified by distance of mean R<sup><xref ref-type="bibr" rid="R2">2</xref></sup> to the unit line. Mean = 0.0147, P = 2.2 x 10<sup>-5</sup>, two-tailed t-test. Shading indicated significance for single neurons across bootstraps (dark blue: P &lt; 0.05). <bold>g</bold>, Distribution of inferred discount factors across neurons. For each neuron, the discount factor was taken as the mean discount factor across bootstraps. <bold>h</bold>. Shape of the relative population response as a function of reward delay. Normalized to the strongest cue response for each neuron. Thick lines, smoothed fit, dotted lines, theory, dots, responses of individual neurons. <bold>i</bold>, Discount matrix. For each neuron we plot the relative value of future events given its inferred discount factor. Neurons are sorted as in panel <bold>d</bold> by increasing inferred value of the discount factor. Vertical bars on top of panel are color coded to indicate timing of the rewards in the task. <bold>j</bold>, Outline of the decoding procedure. We compute the singular value decomposition (SVD) of the discount matrix <bold>L</bold>. Then, we use the SVD to compute a regularized pseudo-inverse <bold>L</bold>
<sup>-1</sup>. Finally, we normalize the resulting prediction into a probability distribution. <bold>k</bold>, The subjective expected timing of future reward E(r|t) can be decoded from the population responses to the cue predicting reward delay. Decoding based on mean cue responses for test data (top row, see <xref ref-type="sec" rid="S11">Methods</xref>). The ability to decode the timing of expected future reward is not due to a general property of the discounting matrix and collapses if we randomize the identity of the cue responses (bottom row, see <xref ref-type="fig" rid="F10">Extended Data Fig. 5e</xref> and <xref ref-type="sec" rid="S11">Methods</xref>).</p></caption><graphic xlink:href="EMS190989-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>The diversity of discount factors across dopamine neurons explains qualitatively different ramping activity</title><p id="P104"><bold>a</bold>, Experimental setup. Left panel, View of the virtual reality corridor at movement initiation. Middle and right, Schematics of the experimental setup. <bold>b</bold>, Average activity of single dopaminergic neurons (n = 90) exhibit an upward ramp in the last few seconds of the track prior to reward delivery. <bold>c</bold>, The slope of the activity ramp (computed between the two black horizontal ticks in panel <bold>b</bold>) is positive on average but varies across neurons (population: mean slope = 0.097, P = 0.0175. Single neurons: positive and P &lt; 0.05: n = 53; negative and P &lt; 0.05: n = 32; P &gt; 0.05: n = 5, two-tailed t-test). <bold>d</bold>, Example single neurons showing diverse ramping activity in the final approach to reward including, monotonic upwards (dark red), non-monotonic (red) and monotonic downwards (light red) ramps. <bold>e</bold>, Individual neurons across the population exhibit a spectrum of diversity in their ramping activity. Neurons are sorted according to inferred discount factor from the common value function model (panel <bold>k</bold>). <bold>f</bold>, Diversity of ramping with an exponential value function. There is no TD error for an agent with the same discount factor as the parameter of the value function (red line). The TD error ramps upwards (downwards) if the discount factor is larger (smaller), dark red and light red lines respectively. <bold>g</bold>, Diversity of ramping as a function of discount factor for an exponential value function. <bold>h</bold>, Diversity of ramping with cubic value function. Agents with large (small) discount factor experience a monotonic positive (negative) ramp in their TD error (dark red and light red lines respectively). Agents with intermediate discount factors experience non-monotonic ramps (red line). <bold>i</bold>, Diversity of ramping as a function of discount factor for an exponential value function. Unlike in the exponential value function case, no agent matches its discount to the value function at all the time steps. <bold>j</bold>, The inferred value function is convex. Thin grey lines represent the inferred value function for each bootstrap. Thick blue line represents mean over bootstraps. <bold>k</bold>, Histogram of inferred discount factors. 0.42 ± 0.23 (mean ± s.d.). <bold>l</bold>, Example model fits for the single neurons shown in panel <bold>d. m</bold>, The model captures the diversity of ramping activity across the population. Neurons are ordered by inferred discount factor as in panel <bold>e</bold>.</p></caption><graphic xlink:href="EMS190989-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Discount factors of single dopaminergic neurons are correlated across behavioral contexts</title><p id="P105"><bold>a</bold>, Correlation between the discount factors inferred in the VR task and the discount factors inferred in the cued delay task (r = 0.45, P = 0.0013). <bold>b</bold>, Distribution of correlations between the discount factors across the two tasks for randomly sampled pairs of bootstrap estimates (0.34 ± 0.104, mean ± s.d., P &lt; 1.0 x 10<sup>-30</sup>, two-tailed t-test).</p></caption><graphic xlink:href="EMS190989-f005"/></fig></floats-group></article>