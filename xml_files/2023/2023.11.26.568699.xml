<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS191794</article-id><article-id pub-id-type="doi">10.1101/2023.11.26.568699</article-id><article-id pub-id-type="archive">PPR765007</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">4</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Humans adapt rationally to approximate estimates of uncertainty</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Pulcu</surname><given-names>Erdem</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Browning</surname><given-names>Michael</given-names></name><xref ref-type="aff" rid="A1">a</xref><xref ref-type="aff" rid="A2">b</xref></contrib></contrib-group><aff id="A1"><label>a</label>Department of Psychiatry, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><aff id="A2"><label>b</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04c8bjx39</institution-id><institution>Oxford Health NHS Trust</institution></institution-wrap>, <city>Oxford</city>, <country country="GB">UK</country></aff><author-notes><corresp id="CR1">
<label>*</label><email>erdem.pulcu@psych.ox.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>28</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>27</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><title>Summary</title><p id="P1">Efficient learning requires estimation of, and adaptation to, different forms of uncertainty. If uncertainty is caused by randomness in outcomes (noise), observed events should have less influence on beliefs, whereas if uncertainty is caused by a change in the process being estimated (volatility) the influence of events should increase. Previous work has demonstrated that humans respond appropriately to changes in volatility, but there is less evidence of a rational response to noise. Here, we test adaptation to variable levels of volatility and noise in human participants, using choice behaviour and pupillometry as a measure of the central arousal system. We find that participants adapt as expected to changes in volatility, but not to changes in noise. Using a Bayesian observer model, we demonstrate that participants are, in fact, adapting to estimated noise, but that their estimates are imprecise, leading them to misattribute it as volatility and thus to respond inappropriately.</p></abstract><kwd-group><kwd>Uncertainty</kwd><kwd>Bayesian Models</kwd><kwd>Pupillometry</kwd><kwd>Learning</kwd></kwd-group></article-meta></front><body><p id="P2">It is much easier to respond appropriately to an event if we know what has caused it. For example, if heavy traffic means that our drive into work takes longer than normal, the best course of action the next time we have to make the journey depends on what caused the traffic to be heavier (<xref ref-type="bibr" rid="R27">Yu &amp; Dayan, 2005</xref>). If it was caused by a one-off or random event, such as a broken-down lorry, then we should continue using the same route as before, whereas if it was caused by some longer-term change, perhaps there are new road works nearby disrupting the traffic, we should consider a different route. Frequently, however, the causes of events are not obvious, we experience the heavy traffic but aren’t sure why it has occurred. In these situations, the best we can do is make an educated guess, based on our experience, about what broad type of causal process has led to recent events. In the case of the drive into work, if the traffic has been heavier for a number of days in a row it is likely that some prolonged shift has occurred, and we should change routes, whereas if the traffic changes noisily from day to day, then we should probably stick with our usual route. In the learning literature, this problem is often framed as a competitive attribution of uncertainty to one of two types; expected uncertainty, which is caused by the variability of noisy associations and unexpected uncertainty, which is caused by longer lasting changes (sometimes called volatility) in an association (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R21">Pulcu &amp; Browning, 2017</xref>; <xref ref-type="bibr" rid="R27">Yu &amp; Dayan, 2005</xref>). The behavioural importance of this attribution process can be seen in the driving example given above; an event caused by noise requires the opposite behavioural response (continuing to use the same route) than the same event caused by volatility (switching routes). Consequently, effective decision making often depends on the accurate attribution of uncertainty, with misattribution having a substantial detrimental effect on choice (<xref ref-type="bibr" rid="R22">Pulcu &amp; Browning, 2019</xref>).</p><p id="P3">The influence of events on subsequent choice can be estimated within a reinforcement learning framework as the learning rate parameter (<xref ref-type="bibr" rid="R24">Sutton &amp; Barto, 2018</xref>), with a higher learning rate indicating a greater influence of the event on behaviour. As described above, the normative response to changes in volatility and noise is to use a higher learning rate when volatility is high and/or noise is low (<xref ref-type="bibr" rid="R22">Pulcu&amp; Browning, 2019</xref>; <xref ref-type="bibr" rid="R27">Yu &amp; Dayan, 2005</xref>). A large number of studies have found the predicted increase in learning rates in response to higher outcome volatility in human learners (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>, <xref ref-type="bibr" rid="R2">2008</xref>; <xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R8">Gagne et al., 2020</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R21">Pulcu &amp; Browning, 2017</xref>). In contrast, the evidence for adaptation of learning in response to changes in outcome noise is less complete. Previous studies have described the expected reduction of learning rates when outcome noise is high, but only when the level of noise is explicitly signalled in a task (<xref ref-type="bibr" rid="R7">Diederen &amp; Schultz, 2015</xref>), or when it is made unambiguous by virtue of being very much smaller than changes in outcome caused by volatility (<xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>, <xref ref-type="bibr" rid="R16">2012</xref>). As illustrated in the driving example above, we are often faced with situations in which there exists significant uncertainty about whether an event has been caused by volatility or noise. To date however, the degree to which human learners are able to discriminate between these types of uncertainty, when they are not explicitly labelled, has not been closely examined.</p><p id="P4">From a neurobiological perspective, activity of central modulatory neurotransmitter systems have been argued to represent distinct sources of uncertainty during learning, with central norepinepheric (NE)/locus coeruleus (LC) activity described as signalling changes in the associations (i.e. volatility) and central cholinergic activity representing noise (<xref ref-type="bibr" rid="R27">Yu &amp; Dayan, 2005</xref>). Electrophysiological measures of LC activity in non-human primates have been shown to correlate with pupil dilation (<xref ref-type="bibr" rid="R10">Joshi et al., 2016</xref>) suggesting it may be possible to estimate activity in this system in humans using pupilometry. Taking this approach, indirect support for this role of the NE system has been provided by studies of human participants that report greater pupillary size in volatile relative to stable contexts (<xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R21">Pulcu &amp; Browning, 2017</xref>). However, the pupil also responds to other learning signals, such as surprise (<xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R18">O’Reilly et al., 2013</xref>; <xref ref-type="bibr" rid="R20">Preuschoff et al., 2011</xref>) and has been reported as being smaller when outcome noise is high (<xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>). Neuroimaging evidence suggests an association between activity in other central neurotransmitter nuclei, including the cholinergic basal forebrain, and pupil dilation (<xref ref-type="bibr" rid="R6">de Gee et al., 2017</xref>). Overall, this suggests that the pupillary signal may reflect a more general belief updating process (<xref ref-type="bibr" rid="R18">O’Reilly et al., 2013</xref>) rather than a specific volatility signal and thus that, like learning rates, pupil size should increase when noise is reduced as well as when volatility is increased.</p><p id="P5">In this paper we test whether human participants modify their learning in situations in which the attribution of uncertainty as volatility or noise is challenging (<xref ref-type="fig" rid="F1">Figure 1a-c</xref>). We report the results of a study in which participants completed a learning task during which the noise and volatility of both win and loss outcomes were independently manipulated. Participant behaviour was characterised using learning rate parameters derived from reinforcement learning models of choice, while interpretation of the results was facilitated by a Bayesian Ideal Observer model that was developed to provide a benchmark comparator to participant behaviour (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R19">Piray &amp; Daw, 2021</xref>; <xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>) and by the collection of pupillometry data as a physiological marker of central neurotransmitter function (<xref ref-type="bibr" rid="R6">de Gee et al., 2017</xref>; <xref ref-type="bibr" rid="R10">Joshi et al., 2016</xref>). It was predicted that human participants would be able to adapt appropriately to the cause of the events they encountered—using a higher learning rate, and displaying increased pupil size, when volatility was high and when noise was low for both win and loss outcomes (<xref ref-type="fig" rid="F1">Figure 1d</xref>).</p><sec id="S1" sec-type="results"><title>Results</title><sec id="S2"><title>Participant Demographics</title><p id="P6">70 participants (see Supplementary Table 1 for demographic information) completed a learning task in which they had to choose one of two stimuli based on the separately estimated magnitudes of win and loss outcomes associated with the stimuli (<xref ref-type="fig" rid="F1">Figure 1</xref>). Participants were able to learn the best option to choose in the task, selecting the most highly rewarded option on an average of 71% of trials (range 65% - 74%).</p></sec><sec id="S3"><title>Experimental Manipulation of Volatility and Noise Influences Participant Choice Behaviour</title><p id="P7">As explained above, high levels of volatility and low levels of noise should increase the degree to which outcomes influence choice behaviour. A crude metric of this effect is provided by examining participant choice as a function of the previous outcome. In the task, a win outcome of &gt;50p or a loss outcome of &lt;50p associated with Shape A prompts participants to select Shape A in the subsequent trial, with the other outcomes (i.e. win &lt;50p and loss &gt;50p) prompting choice of Shape B. The influence of the outcomes on choice can therefore be roughly estimated as the relative proportion of trials in which Shape A was chosen when it was prompted by a previous outcome of a given magnitude, compared to when Shape B was prompted. Analysis of this choice metric (<xref ref-type="fig" rid="F2">Figure 2a-b</xref>) found the expected effect of volatility, with participant choice being more influenced by previous outcomes when volatility was higher (F(1,696)=99.8, <italic>p</italic>&lt;0.001). An effect of noise was observed, but in the opposite direction to expected, with outcomes influencing choice more when noise was increased (F(1,696)=4.79, <italic>p</italic>=0.03). No significant difference between the influence of win and loss outcomes was found (F(1,696)=1, <italic>p</italic>=0.32) and there was no interaction between volatility and noise (F(1,693)=0.61, <italic>p</italic>=0.4). Having found some evidence of an impact of the uncertainty manipulations on a crude measure of subject choice we next sought to characterise this effect using reinforcement learning models fitted to participant choices.</p></sec><sec id="S4"><title>Participants Adjust Normatively to Changes in Volatility but not Noise</title><p id="P8">We aimed to capture the computational process that underlies participant choice behaviour by fitting different reinforcement learning models to choice data separately for each block of the task and each participant. The best fitting RL model included separate learning rates for win and loss outcomes allowing estimation of the degree to which participants adjusted these learning rates in response to the block-wise changes in outcome volatility and noise (see <xref ref-type="supplementary-material" rid="SD1">supplementary Materials and Methods</xref> for model comparison and selection analyses).</p><p id="P9">Consistent with the analysis of choice data reported above, there was a significant main effect of volatility (<xref ref-type="fig" rid="F2">Figure 2c-d</xref>; F(1,696)=22.2, <italic>p</italic>&lt;0.001), with a higher learning rate used when volatility was high. There was no main effect of noise (F(1,696)=0.63, <italic>p</italic>=0.43) on learning rate or outcome valence (F(1,696)=0.15, <italic>p</italic>=0.7). An interaction between volatility and noise (F(1,693)=7.74, <italic>p</italic>=0.006) was also significant. A higher volatility led to a significantly raised learning rate when noise was low (F(1,383)=27.1, <italic>p</italic>&lt;0.01), with a non-significant increase when noise was high (F(1,311)=1.13, <italic>p</italic>=0.29). Higher noise was associated with a non-significant reduction in learning rates when volatility was high (F(1,347)=2.57, <italic>p</italic>=0.11) but to a significant increase in learning rate when volatility was low (F(1,347)=4.7, <italic>p</italic>=0.031).</p><p id="P10">In summary, analysis of both crude choice data and learning rates indicates that participants adapted appropriately to changes in the volatility of learned associations but did not show a consistent response to changes in noise. In the next section we utilise a Bayesian Observer Model (BOM) to investigate potential causes for this relative insensitivity to noise.</p></sec><sec id="S5"><title>Using a Bayesian Observer Model to Characterise Noise Insensitivity</title><p id="P11">Bayesian Observer Models (BOM) can be used as normative benchmarks against which human behaviour may be compared (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R19">Piray &amp; Daw, 2021</xref>; <xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>). BOMs are generally not fit to participant choice, rather these models invert a generative process assumed to underlie observed events and provide an estimate of the belief of an idealised agent exposed to the same outcomes as participants. We developed a BOM (<xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>) based on the generative process underlying the outcome magnitudes of our task (<xref ref-type="fig" rid="F3">Figure 3a</xref>). The BOM explicitly estimates the volatility and noise of the outcomes and uses these estimates to influence its belief about the likely magnitude of upcoming outcomes (see <xref ref-type="sec" rid="S10">methods</xref> for more details). We first tested whether the BOM reproduced the normative learning rate adaptation to changes in volatility and noise described in the introduction, by exposing the model to the same outcomes as participants and using the model’s belief about the likely magnitude of the win and loss outcome on each trial to generate choices. We then estimated the effective learning rate of the model by fitting the same RL model used to analyse participants’ choices to the model’s choices. These learning rates are presented in <xref ref-type="fig" rid="F3">Figure 3f</xref> (<xref ref-type="fig" rid="F3">Figure 3e</xref> reproduces the learning rates of participants, averaged across wins and losses, for comparison). As can be seen the BOM adapts as expected, using a higher learning rate both when volatility increases (F(1,696)=422, <italic>p</italic>&lt;0.001) and when noise decreases (F(1,696)=21.2, <italic>p</italic>&lt;0.001). No effect of outcome valence or interaction between volatility and noise (all <italic>p</italic>&gt;0.09) was observed.</p><p id="P12">Having shown that an optimal learner adjusts its learning rate to changes in volatility and noise as expected, we next sought to understand the relative noise insensitivity of participants. In these analyses we “lesion” the BOM, to reduce its performance in some way, and then assess whether doing so recapitulates the pattern of learning rate adaptation observed for participants (<xref ref-type="fig" rid="F3">Fig 3e</xref>). In other words, we damage the model so it performs less well and then assess whether this damage makes the behaviour of the BOM (shown in <xref ref-type="fig" rid="F3">Fig 3f</xref>) more closely resemble that seen in participants (<xref ref-type="fig" rid="F3">Fig 3e</xref>). First, we tested the impact of completely removing the ability of the BOM to adjust to changes in either volatility (<xref ref-type="fig" rid="F3">Figure 3b</xref>) or noise (<xref ref-type="fig" rid="F3">Figure 3c</xref>) by removing the top nodes of the model (i.e. <italic>kmu</italic> or <italic>vs</italic> respectively). Removing these nodes forces the BOM to estimate the mean volatility or noise across all task blocks rather than estimating local periods where they are higher or lower (see supplementary video). As illustrated in <xref ref-type="fig" rid="F3">Figure 3g-h</xref>, neither of these lesions recapitulates the pattern of learning rates observed in participants, with the volatility lesioned model attributing increased volatility to noise and thus decreasing its learning rate during periods of higher volatility (main effect of volatility; F(1,696)=11.9, p&lt;0.001) and the SD-lesioned model treating any form of uncertainty as volatility and thus increasing its learning rate in response to increased noise (main effect of noise; F(1,696)=227, p&lt;0.001). This suggests that human participants are able to adapt to changes in outcome volatility and noise to some degree but are less sensitive to these changes than the intact BOM.</p><p id="P13">We next assessed whether a relative degrading of the model’s representation of volatility and noise (<xref ref-type="fig" rid="F3">Figure 3d</xref>) altered its behaviour in a manner similar to participants. This was achieved by independently coarsening the model’s representation of volatility and noise, with the degree of coarsening selected to make the model’s choices as similar as possible to those of a given participant. Details of this coarsening process are provided in the methods section, but in simple terms, at one extreme, the intact model’s beliefs about current volatility and noise are represented as probability distributions over many possible values, with the number of values used gradually reduced during coarsening, until the coarsest model treats each form of uncertainty as being either “high” or “low”. As can be seen from <xref ref-type="fig" rid="F3">Figure 3i</xref>, this relative degrading of the model’s representation of uncertainty more closely recapitulated the learning rates observed in participants, with a significant increase in learning rate in response to increased volatility (F(1,696)=59, <italic>p</italic>&lt;0.001) and no effect of noise (F(1,696)=2.3, <italic>p</italic>=0.13). In total the BOM fitted to participant choices had 5 parameters (i.e. volatility and SD acuity for rewards and losses and a single inverse temperature term). This was compared with two reinforcement learning models: the measurement model described previously, which was fitted to individual blocks and therefore had many more parameters (18 in total; learning rates for wins and losses for each block, one inverse temperature term per block), and a simple version of the same model which was fitted across all blocks and had 3 parameters in total (win and loss LR and one inverse temperature parameter). Model comparison between the BOM and the RW models based on BIC scores favoured the BOM (mean (SD) for BOM; 235 (54), for complex RL measurement model; 281 (57), for simple RL model; 246 (58)).</p><p id="P14">In the next sections we characterise how coarsening the BOM changes its behaviour and assess whether it provides an accurate account of participants’ noise insensitivity.</p></sec><sec id="S6"><title>The Degraded BOM Misattributes Noise as Volatility</title><p id="P15">The BOM was degraded by reducing the number of bins it used to represent volatility and/or noise, until its behaviour most closely matched that of participants. This process led to a greater coarsening of the noise than the volatility dimension (<xref ref-type="fig" rid="F4">Figure 4a</xref>; F(1,69)=49, <italic>p</italic>&lt;0.001), with no effect of outcome valence (F(1,69)=0.73, <italic>p</italic>=0.4), suggesting that the degraded model maintained a generally less precise representation of noise than volatility. In order to investigate the impact of this coarsening on the model’s beliefs, we used the degraded BOM’s estimates of volatility and noise to categorise task trials as either high or low volatility/noise (i.e. trials in which the model’s estimates of these variables were higher/lower than the mean) and compared these to the same trial labels generated by the intact BOM. Consistent with the greater degradation of the noise dimension, coarsening the model caused it to miscategorise more trials which the intact BOM had labelled as having high than low noise (<xref ref-type="fig" rid="F4">Figure 4b</xref>; F(1,69)=30.7, <italic>p</italic>&lt;0.01) with no effect of volatility (F(1,69)=1.9, <italic>p</italic>=0.17) or outcome valence (F(1,69)=0.004, <italic>p</italic>=0.95). As illustrated in <xref ref-type="fig" rid="F4">Figure 4c</xref>, when the degraded BOM miscategorised high noise trials, it tended to label them as having high, rather than low, volatility. Overall, these results indicate that coarsening the BOM caused it, relative to the intact BOM, to misattribute high noise trials as high volatility trials.</p></sec><sec id="S7"><title>The Degraded BOM Rescues Optimal Behaviour</title><p id="P16">The process of fitting the degraded BOM to participant behaviour can be understood as searching for a configuration of the model (akin to a grid-based maximum likelihood estimation) in which participant choice conforms to the normative response to volatility and noise coded in the model’s structure. In other words, participants’ learning rates should increase when the degraded BOM’s estimate of volatility is high and, critically, when it estimates that noise is low. We demonstrate this by reanalysing participant behaviour, using the trial labels of the degraded BOM to indicate periods of low/high volatility and noise in place of the task block labels used in the original analysis. In effect, this approach allows us to test whether participants make internally consistent errors during learning, i.e. reducing their learning rates for outcomes that they thought — instead of the actual task structure — were associated with high volatility and/or low noise. As can be seen (<xref ref-type="fig" rid="F4">Figure 4f</xref>), participants significantly increased their learning rate when the degraded BOM estimated volatility to be high (F(1,566)=86, <italic>p</italic>&lt;0.001) and noise to be low (F(1,566)=81, <italic>p</italic>&lt;0.001). In control analyses, this normative response to uncertainty was not seen when the labels from the intact rather than the degraded BOM were used (<xref ref-type="fig" rid="F4">Figure 4e</xref>), or when the BOM’s representation of outcome mean was degraded, rather than its estimates of volatility and noise (supplementary materials).</p></sec><sec id="S8"><title>Assuming Human Participants Use the Degraded BOM’s Estimates of Volatility and Noise also Rescues Normative Pupillary Response</title><p id="P17">If the degraded BOM is a fair representation of how participants are performing the learning task, then we would expect it to be better able to explain physiological markers of uncertainty estimation than the simple task block structure or the intact BOM. Specifically, participants’ pupils should be larger when the degraded BOM thinks that volatility is high and when it thinks noise is low. We first show (<xref ref-type="fig" rid="F5">Figures 5a-c</xref>) that participants’ pupils do not adapt normatively to the task block structure, with no main effect of block volatility (F(1,1723)=0.002, p=0.9) and an increase of pupil size in response to higher noise (F(1,1723)=13.8 p&lt;0.001). In contrast, analysis using the trial labels derived from the degraded model (<xref ref-type="fig" rid="F5">Figure 5d-f</xref>) recovered the expected increase in pupil size in response to both raised volatility (F(1,2067)=105, p&lt;0.001) and reduced noise (F(1,2067)=42.3, p&lt;0.001) suggesting that the model provides a reasonable measure of participants’ estimates of these parameters. Finally, we tested whether the degraded BOM was able to explain more variance in the pupil data than the intact BOM. In order to do this, we first regressed participants’ pupil data against the estimated volatility and noise of the intact BOM, as well as a range of other task related factors (<xref ref-type="fig" rid="F5">Figure 5g</xref>; see <xref ref-type="sec" rid="S10">methods</xref> for more details of analysis). Having removed the variance accounted for by these factors, we then regressed the residuals of this first level analyses against the degraded model’s estimates of volatility and noise. This second level analysis (<xref ref-type="fig" rid="F5">Figure 5h-i</xref>) indicated that the degraded model was able to account for variance associated with outcome noise that was not explained by the full model (F(1,286)=4.1, p=0.04), but did not explain additional variance associated with outcome volatility (F(1,286)=0.1, p=0.75). In summary, assuming that participants used the degraded BOM’s estimates of outcome volatility and noise rescued the normative pattern of physiological adaptation during the task.</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P18">Humans respond in a rational, if approximate, manner to the causal statistics of dynamic environments. We found that participants adapted as expected to changes in outcome volatility, but were relatively insensitive to changes in noise. Using a degraded Bayesian Observer Model (BOM) to characterise participants’ behaviour suggested that they responded appropriately to a relatively coarse estimation of the level of noise, that led to its misattribution as volatility. Analysis of pupillometry data using the degraded model again suggested that participants were responding normatively to changes in estimated noise, but that these estimates diverged from the true noise of experienced outcomes. These results illustrate that human learners are able adapt to the statistical properties of their environment, but during this process they make internally consistent errors, utilising higher learning rates as a result of misattributing environmental noise as volatility which also leads to suboptimal choice.</p><p id="P19">Using a task in which volatility and noise varied independently between blocks, we found that human learners adapted as expected (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R22">Pulcu &amp; Browning, 2019</xref>) to blockwise changes in the volatility of both win and loss outcomes, increasing the learning rate used when volatility was high vs. low. In contrast, the expected reduction of learning rates in response to increased outcome noise was not apparent, with participants employing a significantly higher learning rate in response to increased noise when volatility was low and a numerically lower learning rate when volatility was high. The absence of a normative response to blockwise changes in noise is at odds with previous work which has described a reduction in learning rates during periods of high noise (<xref ref-type="bibr" rid="R7">Diederen &amp; Schultz, 2015</xref>; <xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>, <xref ref-type="bibr" rid="R16">2012</xref>). However, in this previous work the level of noise was either explicitly presented to participants (as a bar on screen representing the standard deviation of the generative process in Diederen &amp; Schultz) or was made unambiguous by being very different from changes caused by volatility (in Nassar et al, noise was generated using an SD of 5 or 10, while the average change due to volatility was 100). By design, in the current task high noise and volatility resulted in a similar range of magnitudes (<xref ref-type="fig" rid="F1">Figure 1b</xref>) forcing participants to use the temporal sequence of outcomes to discriminate between the different forms of uncertainty. Our behavioural results suggest that, in the absence of unambiguous differences between outcomes caused by volatility and those caused by noise, participants’ ability to estimate and/or adapt to changes in noise is reduced. Interestingly, a recent study reported that participants do not adjust their choice or estimated confidence in response to variability in the orientation of arrays of visual gratings (<xref ref-type="bibr" rid="R9">Herce Castañón et al., 2019</xref>), suggesting that an insensitivity to outcome noise may be a general feature of human decision making, rather than a specific component of learning.</p><p id="P20">Noise fundamentally limits the reliability of information (<xref ref-type="bibr" rid="R14">MacKay, 2003</xref>) and ignoring it has a clear detrimental impact on inference (<xref ref-type="fig" rid="F3">Figure 3h</xref>), causing agents to be unnecessarily influenced by chance events (<xref ref-type="bibr" rid="R22">Pulcu &amp; Browning, 2019</xref>). It would therefore be surprising if human learners were completely insensitive to this process, particularly given evidence that they can respond normatively when the level of noise is unambiguous (<xref ref-type="bibr" rid="R7">Diederen &amp; Schultz, 2015</xref>; <xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>, <xref ref-type="bibr" rid="R16">2012</xref>). We developed an ideal Bayesian Observer Model (BOM; <xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="R19">Piray &amp; Daw, 2021</xref>; <xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>) to investigate the degree to which participants were adapting to noise. The intact BOM displayed the expected behavioural response to changes in both volatility and noise (<xref ref-type="fig" rid="F3">Figure 3f</xref>) and, as a result, did not accurately capture the behaviour of participants (<xref ref-type="fig" rid="F3">Figure 3e</xref>). Completely removing the BOM’s ability to adapt to noise (or volatility) did not recapitulate participant choice behaviour (<xref ref-type="fig" rid="F3">Figure 3g-h</xref>), whereas coarsening its representation of volatility and noise, produced a much closer match (<xref ref-type="fig" rid="F3">Figure 3i</xref>). This suggests that participants were relatively, rather than completely insensitive to noise and that they tended to misattribute high noise as volatility (<xref ref-type="fig" rid="F4">Figure 4</xref>). However, an important caveat to this interpretation is that the degree of coarsening was selected using participants’ choices. The better behavioural match of the coarsened BOM to participant learning rates may therefore be simply because this model was fitted to the same choices used to calculate the learning rates, whereas the intact and fully lesioned models were not. We therefore sought to validate the coarsened BOM by assessing its ability to account for participants’ pupillary data, and by comparing it with an alternative fitted BOM which coarsened the representation of the generative mean, rather than the estimated uncertainty (see supplementary materials). Participants’ pupil size did not vary systematically between different block types, whereas they were significantly larger when the degraded BOM estimated volatility to be high and noise to be low (<xref ref-type="fig" rid="F5">Figure 5a-f</xref>). Similarly, the estimated noise of the degraded BOM accounted for additional variance in pupil size, over and above the intact BOM (<xref ref-type="fig" rid="F5">Figure 5g-i</xref>). In contrast, the alternative mean-degraded BOM did not recapitulate participants’ learning rates (<xref ref-type="supplementary-material" rid="SD1">supplementary figure 3</xref>) and was not able to account for changes in participant pupil size (<xref ref-type="supplementary-material" rid="SD1">supplementary figure 4</xref>). The finding that participants’ pupil size covaries in the expected direction with the degraded BOM’s estimated levels of both volatility and noise provides some reassurance that the model is capturing the dynamics of participants’ uncertainty estimates. More generally, the presence of both volatility and noise signals in this data, indicate that, as suggested previously (<xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="R18">O’Reilly et al., 2013</xref>), the pupillometry signal reflects general belief updating rather than specifically volatility.</p><p id="P21">An outstanding question is why participants might be particularly insensitive to changes in outcome noise. It is tempting to try to answer this question by reference to the processes by which the BOM was coarsened (i.e. the insensitivity was caused by a reduction in the precision by which noise was represented in a multi-dimensional probability distribution). However, the BOM described here was developed as an algorithmic description of how the learning task may be solved. As far as we are aware, there is little evidence that it accurately describes the cognitive or neural implementation of uncertainty estimation. Alternative algorithmic approaches to the general problem of uncertainty estimation have been described (<xref ref-type="bibr" rid="R11">Kalman, 1960</xref>; <xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>; <xref ref-type="bibr" rid="R19">Piray &amp; Daw, 2021</xref>; <xref ref-type="bibr" rid="R22">Pulcu &amp; Browning, 2019</xref>), including simpler approaches that avoid computationally expensive representations of multi-dimensional distributions (<xref ref-type="bibr" rid="R11">Kalman, 1960</xref>; <xref ref-type="bibr" rid="R17">Nassar et al., 2010</xref>) and which therefore may be more likely implementational candidates. In other words, the current results indicate that human learners are relatively insensitive to changes in outcome noise, but do not specify the lower level mechanisms that determine this effect.</p><p id="P22">Previous work examining the neural representations of uncertainty have tended to report correlations between brain activity and some task-based estimate of one form of uncertainty at a time (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R25">Walker et al., 2020</xref>, <xref ref-type="bibr" rid="R26">2023</xref>). We are not aware of work that has, for example, systematically varied volatility and noise and reported distinct correlations for each. An interesting possibility as to how different forms of uncertainty may be encoded is suggested by parallels with the neuronal decoding literature. One question addressed by this literature is how the brain decodes changes in the world from the distributed, noisy neural responses to those changes, with a particular focus on the influence of different forms of between-neuron correlation (<xref ref-type="bibr" rid="R1">Averbeck et al., 2006</xref>; <xref ref-type="bibr" rid="R12">Kohn et al., 2016</xref>). Specifically, signal-correlation, the degree to which different neurons represent similar external quantities (required to track volatility) is distinguished from, and often limited by, noise-correlation, the degree to which the activity of different neurons covaries independently of these external quantities. One possibility relevant to the current study, which resembles the underlying logic of the BOM, is that a population of neurons represents the estimated mean of the generative process that produces task outcomes. In this case, volatility would be tracked as the signal-correlation across this population, whereas noise would be analogous to the noise-correlation and, crucially, misestimation of noise as volatility might arise as misestimation of these two forms of correlation. While the current study clearly cannot adjudicate on the neural representation of these processes, our finding of distinct behavioural and physiological responses to the two forms of uncertainty, does suggest that separable neural representations of uncertainty are maintained.</p><p id="P23">A related question is whether other, non-Bayesian model formulations may be able to account for participants’ learning adaptation in response to volatility and noise. Of note, the reinforcement learning model used to measure learning rates in separate blocks does not achieve this goal—as this model is fitted separately to each block rather than adapting between blocks (NB the simple reinforcement learning model that is fitted across all blocks does not capture participant behaviour, see supplementary information). One candidate class of model that has potential here is latent-state models (<xref ref-type="bibr" rid="R5">Cochran &amp; Cisler, 2019</xref>), in which the variance and unexpected changes in the process being learned (which have a degree of similarity with noise and volatility respectively) is estimated and used to alter the model’s rates of updating as well as the estimated number of states being considered. Using the model described by Cochran and Cisler, we were unable to replicate the learning rate adaptation demonstrated by participants in the current study (see supplementary information) although it remains possible that other latent state formulations may be more successful.</p><p id="P24">In conclusion, human learners adapt rationally, to estimates of the volatility and noise of experienced outcomes. However, these estimates are approximate leading to a relative insensitivity to outcome noise.</p></sec><sec id="S10" sec-type="methods"><title>Methods</title><sec id="S11"><title>Experimental model and subject details</title><sec id="S12" sec-type="subjects"><title>Participants</title><p id="P25">70 English-speaking participants aged between 18 and 65 were recruited from the general public using print and online advertisements. A previous study (<xref ref-type="bibr" rid="R21">Pulcu &amp; Browning, 2017</xref>) on behavioural response to changes in volatility reported an effect size of <italic>d</italic>=0.7. As the effect size of a noise manipulation was not clear, we recruited a sample size sufficient to detect an effect size of half this value (<italic>d</italic>=0.35) with 80% power. Participants were excluded from the study if they had any psychological or neurological disorders or were currently on psychotropic medication. No exclusion criteria related to task performance were used.</p></sec></sec><sec id="S13"><title>Method details</title><sec id="S14"><title>General procedure</title><p id="P26">Participants attended a single study visit during which they completed the learning task. The study was approved by the University of Oxford Central Research Ethics Committee (R49753/RE001). All participants provided written informed consent to take part in the study, in accordance with the Declaration of Helsinki.</p></sec><sec id="S15"><title>Behavioural Paradigm</title><p id="P27">The reinforcement learning (RL) task consisted of six blocks, each comprising 60 trials. In each trial, participants were presented with two abstract shapes taken from the Agathodaimon font (i.e. shape A and shape B). Two different shapes were used in each block, with rest sessions between blocks. The shapes were presented randomly on either side of the screen. Participants were explicitly instructed that this randomised location did not influence the outcome magnitudes. Participants attempted to accumulate as much money as possible by learning the likely magnitude of the wins and losses associated with each shape and using this information to guide their choice. On each trial, participants chose one of two shapes, with their choice highlighted by a black frame (see <xref ref-type="fig" rid="F1">Fig 1a</xref>). Following the choice, the win and loss amounts associated with the chosen shape were presented, in randomised order, for a jittered period (2-6 sec, mean: 4 sec) inside two empty bars, above and below the fixation cross. The win amount was shown as a green area in the upper bar, and the loss amount represented as a red area in the lower bar. The total length of each bar represented £1 (i.e. of wins or losses) and thus the amount associated with the chosen shape was the proportion of the bar filled by the green/red areas (e.g. three quarters of the upper bar being green, would mean that the chosen option was associated with a win of 75p). Participants were informed that the unshaded area of each bar was the amount associated with the unchosen option. Thus, on each trial participants knew how much they had won/lost and how much they would have won/lost if they had chosen the other option. This feature simplified the task; rather than having to separately estimate the wins and losses associated with each shape, participants only had to estimate these values for one shape (with the other shape being £1 minus this value). For each trial participants received the difference between the win and loss amounts associated with their choice. A running total amount of money was displayed in the centre of the screen, under the bars and was updated at the beginning of the subsequent trial with the recent winnings. Participants were informed that the task would be split into 6 blocks, that they had to learn which was the best option to choose, and that this option may change over time. They were not informed about the different forms of uncertainty we were investigating or of the underlying structure of the task (that uncertainty varied between blocks).</p><p id="P28">The wins and the losses associated with each shape followed independent outcome schedules (<xref ref-type="fig" rid="F1">Figure 1b</xref>), generated from a Gaussian distribution. In each block, the win and loss outcomes had either high or low volatility and high or low noise. When volatility was low, the mean of the Gaussian distribution remained constant, when volatility was high the mean changed from between 25-40 and 60-75 every 9-15 trials. When noise was low the standard deviation of the Gaussian was set to 5, whereas when noise was high the standard deviation was 35. As can be seen from <xref ref-type="fig" rid="F1">Figure 1b</xref>, these schedules resulted in similar ranges of outcome magnitudes for periods of high noise and high volatility. The first block for every participant had high volatility and low noise for both win and loss outcomes and was used to familiarise participants with the task. Choices from this block were not used in the analyses presented (although including them does not alter the reported pattern of results). The schedules in the remaining five blocks were presented in a randomised order with the constraint that, across both win and loss outcomes, each of the four combinations of volatility and noise level (<xref ref-type="fig" rid="F1">Figure 1B</xref>) were presented either 2 or 3 times. Thus, while each participant completed at least two blocks with each of the four combinations of high/low volatility/noise, the specific pairings of win and loss volatility/noise levels, differed across participants. This approach was used in preference to a fully factorial design in order to keep the total task duration to a manageable level. At the end of the experiment, participants were paid one fifth of their total winnings, plus a £15 baseline rate for turning up to take part.</p><p id="P29">Pupillometry data was collected for 36 of the 70 participants. During collection of pupillary data, the task was presented on a VGA monitor connected to a laptop computer running Presentation software version 18.3 (Neurobehavioural Systems). An identical behavioural version of the task, presented using Psychtoolbox 3.0 on MATLAB (MathWorks Inc.), was used to collect behavioural data from the remaining 34 participants. In the pupillometry version, participants’ heads were stabilised using a head-and-chin rest placed 70 cm from the screen on which the eye tracking system was mounted (Eyelink 1000 Plus; SR Research). The eye tracking device was configured to record the coordinates of both of the eyes and pupil area at a rate of 500 Hz. The task stimuli were drawn on either side of a fixation cross which marked the middle of the screen and were offset by 7° visual angle. The testing session lasted approximately 70 min per participant.</p></sec></sec><sec id="S16"><title>Analysis of Choice Data</title><sec id="S17"><title>Non-model based measure of the influence of outcomes</title><p id="P30">The manipulation of uncertainty in the reinforcement learning task is expected to alter the degree to which participants’ choices are influenced by the outcomes they experience. A simple, if somewhat crude, measure of this influence can be calculated as the proportion of trials in a block in which participants select the choice prompted by the win or loss outcomes on the previous trial. Generally, win outcomes of &gt;50p and loss outcomes of &lt;50p associated with a shape will prompt selection of the same shape on the next trial, whereas other outcomes will prompt selection of the alternative shape. The overall effect of win outcomes on choice can therefore be estimated as: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>50</mml:mn><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>50</mml:mn><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P31">That is, the probability of choosing shape A, given that, on the previous trial, a win of &gt;50p was associated with shape A – the probability of choosing Shape A, given that, on the previous trial a win of &lt;50p was associated with Shape A. Similarly, the effect of loss outcomes is estimated as: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>50</mml:mn><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="0.2em"/><mml:mi>A</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>50</mml:mn><mml:mi>p</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P32">However, choice is also influenced by the magnitude of the outcome; a win of 90p will have a greater effect on subsequent choice than a win of 55p. Blocks with high levels of either volatility or noise have more extreme magnitudes than blocks with low levels of both (<xref ref-type="fig" rid="F1">Figure 1b</xref>) which will bias any comparison of this metric between blocks. In order to limit the effect of this bias, we estimated the simple choice metric only for trials in which the previous outcome lay in the range of magnitudes common to all four blocks, 35-65.</p></sec><sec id="S18"><title>Reinforcement Learning Model</title><p id="P33">While the choice metric described above provides a relatively transparent measure of the influence of task outcomes on choice, it does not account for differences in outcome magnitude making it liable to bias. We therefore fitted a simple reinforcement learning model to measure block-wise learning rates, which provide a more principled estimate of the degree to which choices are influenced by outcomes. The model combines a learning phase in which the magnitude of wins and losses associated with a shape are estimated (note that it is not necessary to learn the magnitudes associated with the other shape, as these are simply 1-those described below) <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>a</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>
<disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo></mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>Q</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mtext>_</mml:mtext><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mo>−</mml:mo></mml:msub><mml:mi>a</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P34">In these equations, <italic>Qwin</italic>_<italic>a</italic><sub>(<italic>t</italic>)</sub> and <italic>Qloss</italic>_<italic>a</italic><sub>(<italic>t</italic>)</sub> are the estimated win and loss magnitudes associated with Shape A on trial <italic>t, win</italic><sub>(<italic>t</italic>)</sub> and <italic>loss</italic><sub>(<italic>t</italic>)</sub> are the observed win and loss outcome magnitudes and <italic>α</italic><sub><italic>win</italic></sub> and <italic>α</italic><sub><italic>loss</italic></sub> are the win and loss learning rates. These values are then combined in a decision phase such that: <disp-formula id="FD5"><mml:math id="M5"><mml:mrow><mml:mspace width="0.2em"/><mml:mi>P</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>_</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>_</mml:mo><mml:mi>a</mml:mi><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>_</mml:mo><mml:mi>a</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P35">Where <italic>Pchoice</italic>_<italic>a</italic><sub>(<italic>t</italic>)</sub> is the probability that Shape A will be chosen on trial <italic>t</italic> and <italic>β</italic> is a single inverse decision temperature. This model was initiated with <italic>Qwin</italic>_<italic>a</italic><sub>(0)</sub> = <italic>Qloss</italic>_<italic>a</italic><sub>(0)</sub> = 0.5 and the three free parameters (<italic>win</italic><sub>(<italic>t</italic>)</sub>, <italic>loss</italic><sub>(<italic>t</italic>)</sub> and <italic>β</italic>) were estimated for each block and each participant by calculating the joint posterior probability given participant choice, marginalising each parameter and deriving the parameters’ expected values (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>). See supplementary materials for model selection data.</p><p id="P36">Some analyses reported in the paper (i.e. where trials are labelled as high/low volatility and high/low noise by the Bayesian Observer Model rather than by task block) cannot be modelled using this block-wise approach (as different types of trial are interleaved throughout the task, rather than blocked). In these analyses a similar, single model was fit across all trials in the task. This model had 8 different learning rates (separate win and loss learning rates, for each combination of high/low volatility and high/low noise labelled trials) and a single inverse temperature parameter. Although this model is somewhat less flexible than the blockwise modelling approach (i.e. it has 8, rather than 10 learning rates, and 1 rather than 5 inverse temperatures), it produces the same pattern of results when applied to participant choices split by task block (all estimated learning rates correlate at <italic>r</italic>&gt;0.8, <xref ref-type="fig" rid="F2">Figure 2c-d</xref> show results from blockwise fitting, <xref ref-type="fig" rid="F3">Figure 3e</xref> from the simpler model). This simpler model was fit using stan, with 5000 burn in and 5000 estimation trials, with posterior convergence visually checked and rhat values of less than 1.1 accepted.</p><p id="P37">Note that neither of these models describe how participants adjust to different levels of volatility and noise, they simply estimate the learning rates used in each block/type of trial, which are expected to vary in response to differences in levels of uncertainty (in contrast, the Bayesian Observer Model described below does estimate uncertainty and adjust to levels of uncertainty).</p></sec><sec id="S19"><title>Bayesian Observer Model</title><p id="P38">A recursive, grid-based Bayesian Observer Model (BOM) was developed, similar to that described by Behrens and colleagues (<xref ref-type="bibr" rid="R3">Behrens et al., 2007</xref>; <xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>). The BOM is based on a generative process (see <xref ref-type="fig" rid="F3">Figure 3</xref>), and described fully in Pulcu et al. (<xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>). Below we summarise the key aspects of the model.</p><p id="P39">The BOM assumes that the observed outcomes at a given time point <italic>t, y</italic><sub><italic>t</italic></sub>, are generated from a Gaussian distribution with an unknown mean, <italic>μ</italic><sub><italic>t</italic></sub>, and standard deviation, <inline-formula><mml:math id="M6"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, with the later producing noise in the observed outcomes (<xref ref-type="fig" rid="F1">Figure 1b-c</xref>). <disp-formula id="FD6"><mml:math id="M7"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P40">As illustrated in <xref ref-type="fig" rid="F1">Figure 1b-c</xref>, the mean of this distribution may change between time points, leading to volatility in the task environment, with this change described by a second level Gaussian distribution, centered on the current mean and with a standard deviation of <inline-formula><mml:math id="M8"><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. The mean of the generative Gaussian distribution in the following trial is drawn from: <disp-formula id="FD7"><mml:math id="M9"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P41">Both the noise (<italic>SD</italic><sub><italic>t</italic></sub>) and volatility (<italic>vmu</italic><sub><italic>t</italic></sub>) parameters can also change between time points with their change governed by Gaussian distributions centered on their current value with standard deviations of <italic>e</italic><sup><italic>vSD</italic></sup> and <italic>e</italic><sup><italic>kmu</italic></sup> respectively. These higher-level parameters allow the model to account for periods in which noise and volatility are high and other periods in which they are low (for example, as caused by the uncertainty changes between task blocks). <disp-formula id="FD8"><mml:math id="M10"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>P</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P42">The BOM estimates the joint posterior probability of the five causal parameters, given the choice outcome it has observed. The joint probability distribution at time point <italic>t</italic> is defined as: <disp-formula id="FD9"><mml:math id="M11"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P43">This joint probability distribution can be thought of as the BOM’s belief about the values of each parameter in the generative model. A Markovian assumption (i.e. that nodes of the model are sufficient to describe the generative process) simplifies this process and illustrates the recursive update performed by the BOM: <disp-formula id="FD10"><mml:math id="M12"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P44">We initialized the joint posterior, before observation of any task outcomes as a uniform distribution. The BOM performs the update, first using Bayes’ rule to incorporate the effect of the most recently observed outcome, and then accounts for the drifting parameters by using the conditional probability of the new value of the drifting parameter, given the initial value and drift rate (See; <xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref> for a detailed account of this updating process): <disp-formula id="FD11"><mml:math id="M13"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mo>∫</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mspace width="0.2em"/><mml:mi>j</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>k</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>…</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>S</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>v</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>m</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P45">The value of each node is derived at every time point by marginalizing over all but the relevant dimension of the joint probability distribution and calculating the expected value of that dimension.</p><p id="P46">During the task, the shapes presented to participants change between each task block, which means that, at the start of each block, participants have to relearn the mean associated with each shape. This was dealt with in the BOM by flattening the mu dimension of the joint probability distribution at the start of each trial (i.e. replacing the values of the mean dimension, with the average of the joint distribution across this dimension). The effect of this is to reset the model’s belief about the actual magnitude associated with the two new shapes, while maintaining its belief about the overall volatility and noise of the outcomes.</p><p id="P47">The BOM was provided with the win and loss outcomes (as values between 0 and 1) for each trial, across all trials in the task (excluding the first practice block, although including this did not alter the pattern of results). It treated the two outcomes as independent (i.e. the win outcome did not influence estimates for the loss outcome and vice versa) and transformed the outcomes to the infinite real line using the logistic transform before estimating the posterior probability (<xref ref-type="bibr" rid="R23">Pulcu et al., 2022</xref>).</p></sec><sec id="S20"><title>Lesioning the Bayesian Observer Model</title><p id="P48">A number of different lesions were applied to the BOM. First, it’s ability to estimate changes in either volatility or noise was removed. This was achieved simply by removing the <italic>kmu</italic> or <italic>vSD</italic> nodes from the BOM (reducing the dimensionality of the joint distribution by one in each case). The effect of this is to force the BOM to estimate the mean volatility and noise (respectively) across the whole task, rather than to modify its estimates of these parameters between trials.</p><p id="P49">The second approach induced a graded, rather than absolute, lesion. This was achieved by reducing the precision with which the BOM represented the volatility-related nodes (<italic>vmu</italic> and <italic>kmu</italic>) and/or the noise related nodes (<italic>SD</italic> and <italic>vSD</italic>). More specifically, the BOM’s estimates of the values of each of the five nodes are encoded on a five dimensional grid, with each dimension on the grid representing the possible range of values of a particular node, from low to high, using a fixed number of points. The probability ascribed by the model to a specific point on this dimension is the relative probability that the value of the node lies within the bin of values that is closer to the point, than to adjacent points. For example, say the value of volatility (<italic>vmu</italic>) ranged from 0 to 10 and was represented by 10 bins. In this case volatility would be represented by a probability mass function over the 10 bins (&lt;0.5, 0.5-1.5, 1.5-2.5, …, &gt; 9.5). Lesioning occurred by independently varying the number of bins used in the volatility-related and/or noise-related dimensions, from a maximum of 20, to a minimum of 2 (i.e. with only 2 bins volatility/noise would be represented as simply “high” or “low”). The degree of lesioning selected for each individual participant was determined as the number of bins for the volatility and noise dimensions that, after passing the model estimates through a softmax action selector with a single inverse temperature parameter (i.e. as described for the RL model), maximized the likelihood that the model would make the same choices as the participant, across all task blocks. This process of lesioning therefore progressively coarsens the BOM’s representation of the two types of uncertainty and selects the degree of coarsening that results in choices as similar as possible to participants (see supplementary materials for an alternative model that coarsens the representation of the mean values).</p></sec></sec><sec id="S21"><title>Pupilometry Data Preprocessing</title><p id="P50">Pupilometry data were collected using the Eyelink II system (SRresearch) from both eyes, sampled at 500Hz. Preprocessing involved the following steps: Eye blinks were identified using the built in filter of the Eyelink system and were removed from the data. A linear interpolation was implemented for all missing data points (including blinks). The resulting trace was subjected to a low pass Butterworth filter (cut-off of 3.75 Hz), z transformed across the session (<xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>), and then averaged across the two eyes. The pupil response to the win and the loss outcomes were extracted separately from each trial, using a time window based on the presentation of the outcomes. This included a 2-s pre-outcome period, and a 6-s period following outcome presentation. Individual trials were excluded from the pupilometry analysis if more than 50% of the data from the outcome period had been interpolated (mean =6.7% of trials) (<xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>). The first 5 trials from each block were not used in the analysis as initial pupil adaption can occur in response to luminance changes in this period (<xref ref-type="bibr" rid="R4">Browning et al., 2015</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>). The preprocessing resulted in two sets of timeseries per participant, one set containing pupil size data for each included trial when the win outcomes were displayed and the other when the loss outcomes were displayed. These pupil area data were binned into one second bins across the outcome period for analysis (NB <xref ref-type="fig" rid="F5">Figure 5a-f</xref>). This analysis was supplemented by an individual regression approach (<xref ref-type="fig" rid="F5">Figure 5g-i</xref>) in which individual participants’ pupil area timeseries was first regressed against estimated trialwise volatility and noise from the intact BOM (<xref ref-type="fig" rid="F5">Figure 5g</xref>), as well as a number of control variables (constant term, amount won/lost on trial (i.e. magnitude of outcome), valence of outcome (win or loss), order in which outcomes were presented (win first/loss first), trial number (1:360), whether shape chosen switched on next trial or not (1:0)). The residuals from this regression were then regressed against estimated trialwise volatility and noise from the degraded BOM (<xref ref-type="fig" rid="F5">Figure 5h,i</xref>). These regression analyses resulted in timeseries of beta-weights that were analysed in the same manner as raw pupil size data.</p></sec><sec id="S22"><title>Quantification and statistical analysis</title><p id="P51">Behavioural data were analysed using linear mixed effect models (<italic>fitlme</italic> function of Matlab (2022a)) with participant ID included as a random factor and volatility, noise and valence added as fixed factors. Two-way interactions between fixed effects were also tested (main effects are reported from models without interaction terms). Addition of random slopes for any of the fixed factors decreased LME model fit statistics and so were not included (<xref ref-type="bibr" rid="R15">Matuschek et al., 2017</xref>). Analysis of timeseries pupillometry data included the additional fixed effect factor of time across the outcome period. Learning rates were transformed to the infinite real line using a logistic transform before analyses (untransformed data are displayed in figures for ease of interpretation). The normality of the distribution of the residuals of the LME analyses were checked both visually and with a one-sample Kolmogorov-Smirnov test. Changes in the classification of trials between the full and degraded BOM (<xref ref-type="fig" rid="F4">Figure 4b</xref>) were analysed using a repeated measures ANOVA with within subject factors of volatility, noise and valence. Raw data are superimposed on all summary figures.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS191794-supplement-Supplementary_Materials.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d17aAcHbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S24"><title>Acknowledgements</title><p>We would like to thank James Gunnell for help in collecting the data. This study was funded by a MRC Clinician Scientist Fellowship awarded to MB (MR/N008103/1). MB was supported by the Oxford Health NIHR Biomedical Research Centre. The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.</p></ack><sec id="S23" sec-type="data-availability"><title>Code and Data Availability</title><p id="P52">Study data and analysis scripts, including code for the various models used are available at: <ext-link ext-link-type="uri" xlink:href="https://osf.io/j7md3/">https://osf.io/j7md3/</ext-link>.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P53"><bold>Author Contributions</bold></p><p id="P54">MB and EP conceived the study, EP collected the data, MB and EP wrote the paper.</p></fn><fn id="FN2" fn-type="conflict"><p id="P55"><bold>Disclosures</bold></p><p id="P56">MB has received travel expenses from Lundbeck for attending conferences and consultancy from Jansen, CHDR and Novartis. EP declares no potential conflict of interest.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Averbeck</surname><given-names>BB</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Neural correlations, population coding and computation</article-title><source>Nature Reviews Neuroscience</source><year>2006</year><volume>7</volume><issue>5</issue><fpage>358</fpage><lpage>366</lpage><pub-id pub-id-type="pmid">16760916</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MF</given-names></name></person-group><article-title>Associative learning of social value</article-title><source>Nature</source><year>2008</year><volume>456</volume><issue>7219</issue><fpage>245</fpage><lpage>249</lpage><pub-id pub-id-type="pmcid">PMC2605577</pub-id><pub-id pub-id-type="pmid">19005555</pub-id><pub-id pub-id-type="doi">10.1038/nature07538</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Walton</surname><given-names>ME</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Learning the value of information in an uncertain world</article-title><source>Nature Neuroscience</source><year>2007</year><volume>10</volume><issue>9</issue><fpage>1214</fpage><lpage>1221</lpage><pub-id pub-id-type="pmid">17676057</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>M</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Jocham</surname><given-names>G</given-names></name><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><article-title>Anxious individuals have difficulty learning the causal statistics of aversive environments</article-title><source>Nature Neuroscience</source><year>2015</year><volume>18</volume><issue>4</issue><fpage>590</fpage><lpage>596</lpage><pub-id pub-id-type="pmcid">PMC4644067</pub-id><pub-id pub-id-type="pmid">25730669</pub-id><pub-id pub-id-type="doi">10.1038/nn.3961</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cochran</surname><given-names>AL</given-names></name><name><surname>Cisler</surname><given-names>JM</given-names></name></person-group><article-title>A flexible and generalizable model of online latent-state learning</article-title><source>PLoS Computational Biology</source><year>2019</year><volume>15</volume><issue>9</issue><elocation-id>e1007331</elocation-id><pub-id pub-id-type="pmcid">PMC6762208</pub-id><pub-id pub-id-type="pmid">31525176</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007331</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Gee</surname><given-names>JW</given-names></name><name><surname>Colizoli</surname><given-names>O</given-names></name><name><surname>Kloosterman</surname><given-names>NA</given-names></name><name><surname>Knapen</surname><given-names>T</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><article-title>Dynamic modulation of decision biases by brainstem arousal systems</article-title><source>eLife</source><year>2017</year><volume>6</volume><elocation-id>e23232</elocation-id><pub-id pub-id-type="pmcid">PMC5409827</pub-id><pub-id pub-id-type="pmid">28383284</pub-id><pub-id pub-id-type="doi">10.7554/eLife.23232</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diederen</surname><given-names>KMJ</given-names></name><name><surname>Schultz</surname><given-names>W</given-names></name></person-group><article-title>Scaling prediction errors to reward variability benefits error-driven learning in humans</article-title><source>Journal of Neurophysiology</source><year>2015</year><volume>114</volume><issue>3</issue><fpage>1628</fpage><lpage>1640</lpage><pub-id pub-id-type="pmcid">PMC4563025</pub-id><pub-id pub-id-type="pmid">26180123</pub-id><pub-id pub-id-type="doi">10.1152/jn.00483.2015</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gagne</surname><given-names>C</given-names></name><name><surname>Zika</surname><given-names>O</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Bishop</surname><given-names>SJ</given-names></name></person-group><article-title>Impaired adaptation of learning to contingency volatility in internalizing psychopathology</article-title><source>eLife</source><year>2020</year><volume>9</volume><elocation-id>e61387</elocation-id><pub-id pub-id-type="pmcid">PMC7755392</pub-id><pub-id pub-id-type="pmid">33350387</pub-id><pub-id pub-id-type="doi">10.7554/eLife.61387</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><article-title>Human noise blindness drives suboptimal cognitive inference</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><elocation-id>1719</elocation-id><pub-id pub-id-type="pmcid">PMC6461696</pub-id><pub-id pub-id-type="pmid">30979880</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-09330-7</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Kalwani</surname><given-names>RM</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Relationships between Pupil Diameter and Neuronal Activity in the Locus Coeruleus, Colliculi, and Cingulate Cortex</article-title><source>Neuron</source><year>2016</year><volume>89</volume><issue>1</issue><fpage>221</fpage><lpage>234</lpage><pub-id pub-id-type="pmcid">PMC4707070</pub-id><pub-id pub-id-type="pmid">26711118</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.028</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><article-title>A New Approach to Linear Filtering and Prediction Problem</article-title><source>Transactions of the ASME</source><year>1960</year><volume>82</volume><issue>D</issue><fpage>35</fpage><lpage>45</lpage></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Coen-Cagli</surname><given-names>R</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Correlations and Neuronal Population Information</article-title><source>Annual Review of Neuroscience</source><year>2016</year><volume>39</volume><fpage>237</fpage><lpage>256</lpage><pub-id pub-id-type="pmcid">PMC5137197</pub-id><pub-id pub-id-type="pmid">27145916</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-070815-013851</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krishnamurthy</surname><given-names>K</given-names></name><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Sarode</surname><given-names>S</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Arousal-related adjustments of perceptual biases optimize perception in dynamic environments</article-title><source>Nature Human Behaviour</source><year>2017</year><volume>1</volume><pub-id pub-id-type="pmcid">PMC5638136</pub-id><pub-id pub-id-type="pmid">29034334</pub-id><pub-id pub-id-type="doi">10.1038/s41562-017-0107</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><source>InformationTheory,Inference,and Learning Algorithms</source><publisher-name>Cambridge University Press</publisher-name><year>2003</year><edition>1st</edition></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matuschek</surname><given-names>H</given-names></name><name><surname>Kliegl</surname><given-names>R</given-names></name><name><surname>Vasishth</surname><given-names>S</given-names></name><name><surname>Baayen</surname><given-names>H</given-names></name><name><surname>Bates</surname><given-names>D</given-names></name></person-group><article-title>Balancing Type I error and power in linear mixed models</article-title><source>Journal of Memory and Language</source><year>2017</year><volume>94</volume><fpage>305</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2017.01.001</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Rumsey</surname><given-names>KM</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Parikh</surname><given-names>K</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nat Neurosci</source><year>2012</year><volume>15</volume><issue>7</issue><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="pmcid">PMC3386464</pub-id><pub-id pub-id-type="pmid">22660479</pub-id><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><article-title>An Approximately Bayesian Delta-Rule Model Explains the Dynamics of Belief Updating in a Changing Environment</article-title><source>Journal of Neuroscience</source><year>2010</year><volume>30</volume><issue>37</issue><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="pmcid">PMC2945906</pub-id><pub-id pub-id-type="pmid">20844132</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>JX</given-names></name><name><surname>Schüffelgen</surname><given-names>U</given-names></name><name><surname>Cuell</surname><given-names>SF</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name></person-group><article-title>Dissociable effects of surprise and model update in parietal and anterior cingulate cortex</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2013</year><volume>110</volume><issue>38</issue><fpage>E3660</fpage><lpage>3669</lpage><pub-id pub-id-type="pmcid">PMC3780876</pub-id><pub-id pub-id-type="pmid">23986499</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1305373110</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piray</surname><given-names>P</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><article-title>A model for learning based on the joint estimation of stochasticity and volatility</article-title><source>Nature Communications</source><year>2021</year><volume>12</volume><issue>1</issue><elocation-id>6587</elocation-id><pub-id pub-id-type="pmcid">PMC8592992</pub-id><pub-id pub-id-type="pmid">34782597</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-26731-9</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preuschoff</surname><given-names>K</given-names></name><name><surname>‘t Hart</surname><given-names>BM</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name></person-group><article-title>Pupil Dilation Signals Surprise: Evidence for Noradrenaline’s Role in Decision Making</article-title><source>Frontiers in Neuroscience</source><year>2011</year><volume>5</volume><fpage>115</fpage><pub-id pub-id-type="pmcid">PMC3183372</pub-id><pub-id pub-id-type="pmid">21994487</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00115</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulcu</surname><given-names>E</given-names></name><name><surname>Browning</surname><given-names>M</given-names></name></person-group><article-title>Affective bias as a rational response to the statistics of rewards and punishments</article-title><source>eLife</source><year>2017</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC5633345</pub-id><pub-id pub-id-type="pmid">28976304</pub-id><pub-id pub-id-type="doi">10.7554/eLife.27879</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulcu</surname><given-names>E</given-names></name><name><surname>Browning</surname><given-names>M</given-names></name></person-group><article-title>The Misestimation of Uncertainty in Affective Disorders</article-title><source>Trends in Cognitive Sciences</source><year>2019</year><pub-id pub-id-type="pmid">31431340</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulcu</surname><given-names>E</given-names></name><name><surname>Saunders</surname><given-names>KEA</given-names></name><name><surname>Harmer</surname><given-names>CJ</given-names></name><name><surname>Harrison</surname><given-names>PJ</given-names></name><name><surname>Goodwin</surname><given-names>GM</given-names></name><name><surname>Geddes</surname><given-names>JR</given-names></name><name><surname>Browning</surname><given-names>M</given-names></name></person-group><article-title>Using a generative model of affect to characterize affective variability and its response to treatment in bipolar disorder</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2022</year><volume>119</volume><issue>28</issue><elocation-id>e2202983119</elocation-id><pub-id pub-id-type="pmcid">PMC9282445</pub-id><pub-id pub-id-type="pmid">35787043</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2202983119</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><source>Reinforcement Learning: An Introdcution</source><edition>Second</edition><publisher-name>MIT Press</publisher-name><year>2018</year></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>A neural basis of probabilistic computation in visual cortex</article-title><source>Nature Neuroscience</source><year>2020</year><volume>23</volume><issue>1</issue><fpage>122</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">31873286</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Pohl</surname><given-names>S</given-names></name><name><surname>Denison</surname><given-names>RN</given-names></name><name><surname>Barack</surname><given-names>DL</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Block</surname><given-names>N</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Meyniel</surname><given-names>F</given-names></name></person-group><article-title>Studying the neural representations of uncertainty</article-title><source>Nature Neuroscience</source><year>2023</year><volume>26</volume><issue>11</issue><fpage>1857</fpage><lpage>1867</lpage><pub-id pub-id-type="pmid">37814025</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>AJ</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><article-title>Uncertainty, neuromodulation, and attention</article-title><source>Neuron</source><year>2005</year><volume>46</volume><issue>4</issue><fpage>681</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">15944135</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>The Magnitude Learning Task</title><p><bold>(a)</bold> Timeline of one trial from the learning task. On each trial participants were presented with two abstract shapes and were asked to choose one of them. The empty bars above and below the fixation cross represented the total available wins and losses for the trial, the full length of each bar was equivalent to £1. Participants chose a shape and then were shown the proportion of each outcome that was associated with their chosen shape as coloured regions of the bars (green for wins and red for losses). The empty portions of the bars indicated the win and loss magnitudes associated with the unchosen option, allowing participants to infer which shape would have been the better option on every trial. The task consisted of six blocks of sixty trials each. The volatility and noise of the two outcomes varied independently between blocks with different shapes used in each block. Panel <bold>b</bold> illustrates outcomes from the four block types. As can be seen blocks with high volatility and low noise (top left), and those with low volatility and high noise (bottom right), present participants with a similar range of magnitudes. Participants therefore have to distinguish whether variability in the outcomes is caused by volatility or noise from the temporal structure of the outcomes rather than the size of changes in magnitude (cf; <xref ref-type="bibr" rid="R7">Diederen &amp; Schultz, 2015</xref>; <xref ref-type="bibr" rid="R13">Krishnamurthy et al., 2017</xref>; <xref ref-type="bibr" rid="R16">Nassar et al., 2012</xref>). Panel <bold>c</bold> shows two example blocks (one block in grey, the other in white) with both win (green) and loss outcomes (red) displayed. Panel <bold>d</bold> shows the expected adaptation of learning rates in response to the manipulation of volatility and noise; for both win and loss outcomes, learning rates should be increased when volatility is high and when noise is low.</p></caption><graphic xlink:href="EMS191794-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The impact of uncertainty manipulations on participant choice.</title><p>Panels <bold>a</bold> and <bold>b</bold> report a summary metric for the effect of win and loss outcomes on subsequent choice. The metric was calculated as the proportion of trials in which an outcome of magnitude 51-65 associated with Shape A was followed by choice of the shape prompted by the outcome (i.e. Shape A for win outcomes, Shape B for loss outcomes) relative to when the outcome magnitude was 49-35 (see methods and materials for more details). We focused on this outcome range as these range of magnitudes were covered by all volatility x noise conditions and it was dictated by the relatively smaller range coverage in the low volatility low noise condition (also see <xref ref-type="fig" rid="F1">Figure 1C</xref> loss outcomes shown in red between trials 60-120). The higher this number, the greater the tendency for a participant to choose the shape prompted by an outcome. As can be seen, the outcome of previous trials had a greater influence on participant choice when volatility was high, with a small effect of noise, in the opposite direction to that predicted. Panels <bold>c</bold> and <bold>d</bold> report the win and loss learning rates estimated from the same data. Again, the expected effect of volatility is observed, this time with no consistent effect of noise. Bars represent the mean (±SEM) of the data, with individual data points superimposed.</p></caption><graphic xlink:href="EMS191794-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>The Behaviour of Bayesian Observer Models.</title><p>Bayesian Observer Models (BOM) invert generative descriptions of a process, indicating how an idealised observer may learn. We developed a BOM based on the generative model of the task we used (<bold>a</bold>). Details of the BOM are provided in the methods, briefly it assumes that observations (<italic>y</italic><sub><italic>i</italic></sub>) are generated from a Gaussian distribution with a mean (<italic>mu</italic><sub><italic>i</italic></sub>) and standard deviation (<italic>SD</italic><sub><italic>i</italic></sub>). Between observations, the mean changes with the rate of change controlled by the volatility parameter (<italic>vmu</italic><sub><italic>i</italic></sub>). The standard deviation and volatility of this model estimate the noise and volatility described for the task. The last parameters control the change in volatility (<italic>kmu</italic>) and standard deviation (<italic>vs</italic>) between observations, allowing the model to account for different periods when these types of uncertainty are high and others when they are low. The BOM adjusts it learning rate in a normative fashion (<bold>f</bold>), increasing it when volatility is higher, or noise is lower. The BOM was lesioned in a number of different ways in an attempt to recapitulate the learning rate adaptation observed in participants (shown in panel <bold>e</bold>). Removing the ability of the BOM to adapt to changes in volatility (<bold>b</bold>) or noise (<bold>c</bold>) did not achieve this goal (<bold>g</bold>,<bold>h</bold>). However, degrading the BOMs representation of uncertainty (<bold>d</bold>) was able to recapitulate the behavioural pattern of participants (<bold>i</bold>). Bars represent the mean (±SEM) of participant learning rates, with raw data points presented as circles behind each bar.</p></caption><graphic xlink:href="EMS191794-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Analysis of the behaviour of the degraded BOM.</title><p>The process of degrading the BOM involved reducing the number of bins used to represent the volatility and noise dimensions independently until the choice of the model matched that of participants. Panel <bold>a</bold> illustrates the number of bins selected by this process for the volatility and noise dimensions (averaged across win and loss outcomes). As can be seen the degraded BOM maintained a less precise representation of noise than volatility. In order to understand the behaviour of the degraded model, the model’s estimated <italic>vmu</italic><sub><italic>i</italic></sub> and <italic>SD</italic><sub><italic>i</italic></sub> were used to label individual trials as high/low volatility and noise (NB greater than or less than the mean value of the estimates). These trial labels were compared with the same labels from the intact model, which were used as an ideal comparator (panels <bold>b</bold> and <bold>c</bold>). Panel <bold>b</bold> illustrates the proportion of trials in which the labels of the two models agreed, arranged by the ground truth labels of the full model and averaged across win and loss outcomes. The dotted line indicates the agreement expected by chance. The degraded model trial labels differed from those of the full model particularly for high noise trials, with no impact of trial volatility. Panel <bold>c</bold> provides more details on how the degraded model misattributes trials. In this figure, the labels assigned by the full model are arranged along the x axis. The colour of each square represents the proportion of trials with a specific full model label that received the indicated label of the degraded model (arranged along the y axis). The diagonal squares illustrate agreement between models as reported in panel <bold>b</bold>. As highlighted by the red outlines, trials which the full model labelled as having high noise were generally mislabelled by the degraded model as having high volatility. Reanalysis of participant choices using the trial labels provided by the full (panel <bold>e</bold>) and degraded (panel <bold>f</bold>) models indicate that participants adapt their learning rates in a normative fashion when the degraded model trial labels are used (panel <bold>f</bold>), but not when the full model labels are used (panel <bold>e</bold>). Panel <bold>d</bold> illustrates the same analysis using the original task block labels for comparison. Bars represent the mean (±SEM) of participant learning rates, with raw data points presented as circles behind each bar. See <xref ref-type="supplementary-material" rid="SD1">supplementary figure 1</xref> for a comparison of the behaviour of the degraded BOM with an alternative fitted model.</p></caption><graphic xlink:href="EMS191794-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Analysis of pupillometry data.</title><p>Z-scored pupil area from 2 seconds before to 6 seconds after win (panel <bold>a</bold>) and loss (panel <bold>b</bold>) outcomes, split by task block. Lines illustrate average size, with shaded area illustrating SEM. Panel <bold>c</bold> Pupil size averaged across whole outcome period and both win and loss outcomes. Pupil size did not systematically vary by task block. Panels <bold>d-f</bold>, as above but using the trial labels derived from the degraded model. Pupil size was significantly larger for trials labelled as having high vs. low volatility and low vs. high noise. Panel <bold>g</bold> displays the mean (SEM) effect of volatility and noise as estimated by the full BOM derived from a regression analysis of pupil data. The residuals from this analysis were then regressed against the estimated volatility and noise from the degraded model. A time course of the regression weights from this analysis is shown in panel <bold>h</bold>, with the mean coefficients across the whole period shown in panel <bold>i</bold>. The degraded model’s estimated noise accounted for a significant amount of variance not captured by the full model (pink line in <bold>h</bold> is below 0, the mean effect across the period is represented by dashed lines and arrows in panel <bold>i</bold>). See <xref ref-type="supplementary-material" rid="SD1">supplementary figure 2</xref> for comparison of the degraded BOM with an alternative fitted model.</p></caption><graphic xlink:href="EMS191794-f005"/></fig></floats-group></article>