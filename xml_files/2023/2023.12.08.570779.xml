<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS192676</article-id><article-id pub-id-type="doi">10.1101/2023.12.08.570779</article-id><article-id pub-id-type="archive">PPR771451</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Category-based attention facilitates memory search</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Shang</surname><given-names>Linlin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Yeh</surname><given-names>Lu-Chun</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Zhao</surname><given-names>Yuanfang</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Wiegand</surname><given-names>Iris</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Peelen</surname><given-names>Marius V.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Donders Institute for Brain, Cognition and Behaviour, Radboud University, 6525 GD Nijmegen, The Netherlands</aff><aff id="A2"><label>2</label>Mathematical Institute, Department of Mathematics and Computer Science, Physics, Geography, Justus-Liebig-University Gießen, 35392 Gießen, Germany</aff><aff id="A3"><label>3</label>Department of Cognitive Science, Johns Hopkins University, Baltimore, MD, USA</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding authors: Radboud University 6525 GD Nijmegen The Netherlands <email>linlin.shang@donders.ru.nl</email>; <email>marius.peelen@donders.ru.nl</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>10</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>09</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">We often need to decide whether the object we look at is also the object we look for. When we look for one specific object, this process can be facilitated by preparatory feature-based attention. However, when we look for multiple objects at the same time (e.g., the products on our shopping list) such a strategy may no longer be possible, as research has shown that we can actively prepare to detect only one object at a time. Therefore, looking for multiple objects may additionally involve search in long-term memory, slowing down decision making. Interestingly, however, previous research has shown that memory search can be very efficient when distractor objects are from a different category than the items in the memory set. Here, using EEG, we show that this efficiency is supported by top-down attention at the category level. In Experiment 1, human participants (both sexes) performed a memory search task on individually presented objects of the same or different category as the objects in the memory set. We observed category-level attentional modulation of distractor processing from ~150 ms after stimulus onset, expressed both as an evoked response modulation and as an increase in decoding accuracy of same-category distractors. In Experiment 2, memory search was performed on two concurrently presented objects. When both objects were distractors, spatial attention (indexed by the N2pc component) was directed to the object that was of the same category as the objects in the memory set. Together, these results demonstrate how attention can facilitate memory search.</p></abstract><kwd-group><kwd>memory search</kwd><kwd>categorical similarity</kwd><kwd>top-down modulation</kwd><kwd>attentional template</kwd><kwd>object perception</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Visual object processing is modulated by top-down goals. For example, the same object evokes a stronger neural response in visual cortex when the object is a target as compared to when it is a distractor (e.g., <xref ref-type="bibr" rid="R7">Chelazzi et al., 1993</xref>; <xref ref-type="bibr" rid="R1">Bansal et al., 2014</xref>). These modulations have typically been studied in the context of attention, with a top-down attentional set (or “template”) modulating visual processing (<xref ref-type="bibr" rid="R11">Desimone &amp; Duncan, 1995</xref>). Such templates can operate at different levels of the visual hierarchy, from simple visual features to high-level object categories (<xref ref-type="bibr" rid="R3">Battistoni et al., 2017</xref>).</p><p id="P3">While the mechanisms behind single-target detection have been extensively studied, much less is known about multiple-target detection (<xref ref-type="bibr" rid="R41">Ort &amp; Olivers 2020</xref>), even though this task is common in daily life. For example, when we are in the supermarket, we must decide whether the product we look at is one of the (possibly many) products on our memorized shopping list. This task is typically referred to as a memory search task (<xref ref-type="bibr" rid="R47">Sternberg, 1966</xref>), as it involves searching memory for the currently fixated item. Indeed, as memory set size (MSS) increases, responses systematically slow down, reflecting the memory search process (<xref ref-type="bibr" rid="R53">Wolfe, 2012</xref>).</p><p id="P4">An important difference between single-target and multiple-target detection is that observers can no longer use an attentional template-based strategy when looking for multiple targets. This is because only one or two attentional templates can be activated at a given time (<xref ref-type="bibr" rid="R20">Houtkamp &amp; Roelfsema, 2006</xref>; <xref ref-type="bibr" rid="R40">Olivers et al., 2011</xref>; <xref ref-type="bibr" rid="R41">Ort &amp; Olivers 2020</xref>; <xref ref-type="bibr" rid="R51">van Moorselaar et al., 2014</xref>; <xref ref-type="bibr" rid="R54">Wolfe 2021</xref>). Accordingly, the attentional template-based modulation of visual object processing, as observed for single-target detection, may be absent for multiple-target detection (i.e., memory search). This is in line with findings from memory research, showing relatively late (~300-500 msec) electroencephalography (EEG) responses over mid-frontal and parietal electrodes reflecting recognition memory (see <xref ref-type="bibr" rid="R45">Rugg &amp; Curran, 2007</xref> for a review), rather than the earlier (150-200 ms) attentional modulation observed in single-target detection tasks (<xref ref-type="bibr" rid="R52">VanRullen &amp; Thorpe, 2001</xref>; <xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>).</p><p id="P5">Interestingly, however, memory search efficiency depends on the categorical relationship between the objects in the memory set and the probe (<xref ref-type="bibr" rid="R8">Cunningham &amp; Wolfe, 2012</xref>, <xref ref-type="bibr" rid="R9">2014</xref>; <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>). If the probe (e.g., a banana) is of the same category as the items in the memory set (e.g., apple, pear, orange), search is inefficient, such that RT increases strongly with increasing MSS. However, when the probe is of a different category (e.g., an animal), search is highly efficient, such that RT increases only weakly with increasing MSS (<xref ref-type="bibr" rid="R8">Cunningham &amp; Wolfe, 2012</xref>, <xref ref-type="bibr" rid="R9">2014</xref>; <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>). These findings could reflect differences in memory search efficiency, such that between-category distractors are rejected efficiently because they are represented distinctly in long-term memory (<xref ref-type="fig" rid="F1">Figure 1A</xref>). Alternatively, however, participants could use the shared category of the memory items to form a category-level attentional template, thereby efficiently rejecting between-category distractors even before commencing search in long-term memory (<xref ref-type="bibr" rid="R9">Cunningham &amp; Wolfe, 2014</xref>; <xref ref-type="fig" rid="F1">Figure 1B</xref>).</p><p id="P6">Here, in two experiments, we used EEG to test whether participants spontaneously form category-level attentional templates in a memory search task. We tested if, and when, visual object processing is modulated by category similarity in a memory search task. In Experiment 1, we found that distractors that were of the same category as the memorized items (within-category distractors) received more attention, and were therefore processed more strongly, than distractors that were of a different category (between-category distractors). In Experiment 2, we used two-object displays to show that this modulation results in the rapid allocation of spatial attention towards within-category distractors. Altogether, our results show that the between-category advantage in memory search is partly explained by category-level attentional modulation.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><sec id="S4"><title>Experiment 1a</title><p id="P7">Forty-one participants (15 females; mean age, 23.81 years; age range, 20-35 years) were recruited from the online platform Prolific to arrive at a final sample size of 40. Twenty participants (8 females; mean age, 22.95 years; age range, 20-28 years) were assigned to the animate-category group, while 21 participants (7 females; mean age, 24.94 years; age range, 20-35 years) were assigned to the inanimate-category group. One participant in this group had to be excluded because of low accuracy (ACC) for set size 16 (&lt;65%). All participants signed an online informed consent form and received 6 euro per hour for their participation in the experiment, which was approved by the Ethics Committee of the Faculty of Social Sciences, Radboud University Nijmegen.</p></sec><sec id="S5"><title>Experiment 1b</title><p id="P8">We employed pwr [R] package to compute the sample size at the significance level of 0.05 with Cohen’s d 0.5 and power 0.8. Thirty-four participants were needed. We included 32 participants (23 females; age range, 18-30 years with <italic>Mean</italic> = 22.031 years and <italic>SD</italic> = 3.036) in Experiment 1b because of the Latin square design (see Experimental Design and Procedure). All participants had normal or corrected-to-normal vision. The participants gave written informed consent and received a gift card of 10 euro per hour for their participation. The study was approved by the Ethics Committee of the Faculty of Social Sciences, Radboud University Nijmegen.</p></sec><sec id="S6"><title>Experiment 2</title><p id="P9">To arrive at 32 participants, as in Experiment 1b, 35 right-handed participants (11 females; age ranges from 18 to 35 years with <italic>Mean</italic> = 22.51 years and <italic>SD</italic> = 3.61) were recruited. Three participants were excluded due to missing more than 20% of trials after incorrect responses exclusion and artifact rejection. All participants had normal or corrected-to-normal vision. The participants gave written informed consent and received a gift card of 10 euro per hour for their participation. The study was approved by the Ethics Committee of the Faculty of Social Sciences, Radboud University Nijmegen.</p></sec></sec><sec id="S7"><title>Stimuli</title><sec id="S8"><title>Experiment 1a</title><p id="P10">Stimuli consisted of full-color photographs of isolated animals (from Google Images) and inanimate objects (from <xref ref-type="bibr" rid="R4">Brady et al., 2008</xref>). Both categories had 30 subcategories (e.g., horse, cat, binoculars, bowl), and each subcategory consisted of 17 exemplar images, for a total of 1020 unique images. Stimulus size was 500 × 500 pixels. The experiment was programmed with PsychoPy v2020.2.3 (<xref ref-type="bibr" rid="R42">Peirce et al., 2019</xref>) and was hosted on Pavlovia.</p></sec><sec id="S9"><title>Experiment 1b</title><p id="P11">The stimuli were the same as in Experiment 1a. The experiment was programmed with PsychoPy v2022.2.4 (<xref ref-type="bibr" rid="R42">Peirce et al., 2019</xref>) and ran on a 24-inch monitor (BenQ XL2420Z) with a refresh rate of 120 Hz and a resolution of 1920×1080. Participants were required to keep a distance of approximately 57 cm from the screen, and the stimuli subtended a visual angle of 4.9°.</p></sec><sec id="S10"><title>Experiment 2</title><p id="P12">The stimuli were a subset of those used in Experiment 1, removing one subcategory of each superordinate category for a total of 986 full-color images of unique isolated objects. Stimuli were presented on a white background with a visual angle of 4°. The experiment was programmed with PsychoPy v2022.2.5 (<xref ref-type="bibr" rid="R42">Peirce et al., 2019</xref>) and was presented on a 24-inch monitor (BenQ XL2420Z) with a refresh rate of 120 Hz and a resolution of 1920×1080.</p></sec></sec><sec id="S11"><title>Experimental Design and Procedure</title><sec id="S12"><title>Experiment 1a</title><p id="P13">The experimental design followed a 2 (animate/inanimate category group; between-subjects) × 2 (within/between category; within-subjects) × 5 (MSS 1/2/4/8/16; within-subjects) mixed factorial design. Category group was manipulated between subjects, such that each participant remembered either animate or inanimate objects in all blocks of the experiment. MSS was blocked, with one block for each MSS, for a total of 5 blocks per participant. Block order was randomized. Each block consisted of the following three phases (<xref ref-type="fig" rid="F2">Figure 2</xref>):<list list-type="simple" id="L1"><list-item><label>(1)</label><p id="P14">In the first phase (memorization), participants memorized the objects belonging to the memory set in that block. Object images were randomly assigned to the memory set, with the constraint that each image in the set came from a different subordinate category (e.g., horse, cat). All images in the memory set were new to the participant. In each trial, a central fixation cross appeared for 800 msec as a prompt, followed by an object presented in the center of the screen on a white background, one at a time, for 3000 msec with an inter-stimulus interval (ISI) of 950 msec. Participants were instructed to memorize the objects without giving a response.</p></list-item><list-item><label>(2)</label><p id="P15">In the second phase (memory test), participants again viewed the objects but now had to indicate, with a button press, whether the object belonged to the memory set (press “z”) or not (press “m”). This task was self-paced. Non-target objects were randomly drawn from the same sub-ordinate categories as the target objects (e.g., a different cat). Half of the objects were targets and half were not, presented in random order without repetition. Participants had to be at least 80% correct on two subsequent tests to be able to proceed to the next phase of the experiment. If they did not meet this criterion, they would repeat Phase 1.</p></list-item><list-item><label>(3)</label><p id="P16">In the third and main phase (memory search) of each block, participants performed a speeded old/new recognition task, deciding for each object whether or not it was part of the memory set. This phase consisted of 60 trials, presented in random order. Twenty percent of the trials (12 trials) showed an image selected from the memory set, while the remaining 48 trials were target-absent trials. Of these target-absent trials, 24 belonged to the animate category and 24 belonged to the inanimate category. Therefore, depending on the category group the participant was assigned to, these could be either within- or between-category distractors (relative to the memory set). In each trial, a central fixation cross appeared for 800 msec as a prompt, followed by an object presented in the center of the screen on a white background for 200 msec with an ISI of 1800 msec. Participants had to indicate whether the object belonged to the memory set within 2000 msec. The target images were randomly drawn from the memory set, such that these could repeat within a block. However, all the non-target images were unique across the whole experiment.</p></list-item></list></p></sec><sec id="S13"><title>Experiment 1b</title><sec id="S14"><title>Memory Search</title><p id="P17">The memory search design generally followed the design of Experiment 1a with minor adjustments. To simplify the experimental procedure, set size 16 was removed. Furthermore, unlike Experiment 1a, the category (animate or inanimate) was manipulated within participants (i.e., participants memorized the targets from both animate and inanimate categories). As in Experiment 1a, each MSS was designed as a block; there were therefore 8 blocks in total in the memory task. A Latin square design was used to order the four MSS blocks, with category order within MSS randomly determined. Unlike Experiment 1a, the fixation cross was always presented except during the stimulus presentation, and the ISI in the memory search phase was jittered between 1800 and 2300 msec. Other procedures were identical to Experiment 1a (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p></sec><sec id="S15"><title>Visual Oddball Task</title><p id="P18">A visual oddball task was included to measure visually evoked response patterns to animate and inanimate objects without a memory search task. These data were used to train an animate/inanimate classifier, ensuring that classifier training was done on independent data. In each run, 50 animate and 50 inanimate objects were shown for 200 msec, one by one, with an ISI of 1800-2300 msec, in random order. The 100 objects were randomly selected from the same stimulus pool as used for the memory search task. Objects that were selected for the oddball task were not selected for the memory search task. In addition to the objects, there were ten two-digit numbers that were randomly interspersed. Participants pressed a button when seeing one of these numbers. In total, participants performed the oddball task three times. These runs were preceded and followed by two memory search blocks.</p></sec><sec id="S16"><title>EEG Acquisition and Pre-Processing</title><p id="P19">Scalp EEG signals were recorded with a customized 64-channel active electrode actiCAP system with 500 Hz sampling rate. AFz served as ground electrode, and TP9 placed on left mastoid as a reference electrode. FT9/FT10 and Fp1/Fp2 were reset to left/right and up/down eye movement recorder. Impedance of all the electrodes was kept below 20 kΩ. The EEG data were pre-processed in Python 3.10 using custom code adapted from MNE toolbox (<xref ref-type="bibr" rid="R17">Gramfort, 2013</xref>). All the data were bandpass filtered (0.1 and 40 Hz) and resampled to 250 Hz. Each trial epoch was segmented from -200 to 800 msec relative to the onset of the object. Only epochs with correct responses were included in further analyses. Then, independent component analysis (ICA) was performed for each subject to remove components of eye movements and blinks. Finally, the ICA-corrected data were re-referenced to the average of all channels, and were baseline corrected by subtracting the mean activity from -200 to 0 msec.</p></sec></sec><sec id="S17"><title>Experiment 2</title><p id="P20">As in Experiment 1b, Experiment 2 manipulated category (animate/inanimate) and MSS (1,2,4,8) within subjects, resulting in 8 blocks per participant. Block order was randomized. Each block again consisted of three phases (<xref ref-type="fig" rid="F3">Figure 3</xref>).</p><p id="P21">Unlike Experiment 1b, the third phase now consisted of a memory search task in which participants were presented with a pair of object images. Participants decided, as quickly as possible, whether the image pair included one of the memorized objects. Each block included a total of 120 trials, presented in random order. In target-present trials (50%), the target was presented together with a within- or a between-category distractor (<xref ref-type="fig" rid="F3">Figure 3</xref>). In target-absent trials (50%), three combinations of two distractors (two within-category distractors, two between-category distractors, one within- and one between-category distractors) were presented with equal probability (<xref ref-type="fig" rid="F3">Figure 3</xref>). Category (animate/inanimate) and target location (left/right visual field) were counterbalanced for each combination. At the beginning of each trial, a black fixation cross appeared for 500 msec, followed by one of the five combinations randomly presented for 200 msec, with a jittered inter-trial interval between 1800 and 2200 msec, during which participants needed to press up arrow key or down arrow key to indicate whether the image pair contained a target or not. Participants were instructed to blink only after making a response.</p><sec id="S18"><title>EEG Acquisition and Pre-Processing</title><p id="P22">The EEG acquisition system and the pre-processing were exactly the same as Experiment 1b. However, trials with correct responses were segmented from -200 to 500 msec relative to the onset of the image pair. Eye movements and other artifacts were removed based on visual inspection. ICA correction was not applied due to the relationship between saccades and visuospatial attention (<xref ref-type="bibr" rid="R25">Kowler et al., 1995</xref>). After that, the clean data were re-referenced to the average of all channels. Baseline correction was from -200 to the stimulus onset. All the pre-processing above was completed in Python 3.11 using the MNE toolbox (<xref ref-type="bibr" rid="R17">Gramfort, 2013</xref>).</p></sec></sec><sec id="S19"><title>Statistical Analyses</title><sec id="S20"><title>Experiment 1a</title><p id="P23">All analyses focused on reaction time (RT) to target-absent trials (80% of trials) in the memory search task, for which each image was presented only once. Only correct responses and RTs above 200 msec were included in further analyses. Furthermore, for each participant, RTs beyond 3 standard deviations (SD) from the condition mean were excluded. Under this criterion, a total of 0.260% data points were excluded from further analyses. RT was analyzed in a three-way mixed-design analysis of variance (ANOVA) with one between- (animate-/inanimate-category group) and two within-subjects factors (within-/between-category condition; MSS 1/2/4/8/16). The Greenhouse-Geisser correction was applied to adjust for lack of sphericity (<xref ref-type="bibr" rid="R21">Jennings &amp; Wood, 1976</xref>), and only corrected degrees of freedom and <italic>p</italic>-values are reported. Because the three-way interaction was not significant, <italic>F</italic><sub>(4, 152)</sub> = 1.228, <italic>p</italic> = 0.277, <italic>ηP<sup>2</sup></italic> = 0.033, we collapsed the data across animate and inanimate groups in all subsequent analyses. Accuracy was &gt;92% in all conditions. The result pattern of accuracy across conditions were in line with the RT results (data not shown).</p></sec><sec id="S21"><title>Experiment 1b</title><p id="P24">The exclusion criteria of behavioral data were identical to those of Experiment 1a. In total, 1.139% of data points were removed. Only correct responses in target-absent trials were included for both behavioral and EEG analysis. Accuracy was &gt;98% in all conditions and the pattern of accuracy across conditions supported the RT results (data not shown).</p></sec></sec><sec id="S22"><title>ERP Analyses</title><p id="P25">The ERP analysis focused on P1, N1, and P2 components over posterior electrodes. We first visually inspected the ERP waveform for each participant. All participants showed ERP waveforms with recognizable P1, N1, and P2 peaks. The time window for each component was defined based on the peak range among the participants (<xref ref-type="bibr" rid="R44">Robinson et al., 2015</xref>). P1 peak was observed between 100 to 160 msec after stimulus onset. N1 and P2 peaks were from 160 to 200 msec and from 200 to 300 msec, respectively. Finally, twelve electrodes, P5/P6, P7/P8, PO3/PO4, PO7/PO8, PO9/PO10, and O1/O2, were selected for further analysis as these electrodes showed similar visually evoked activity.</p><p id="P26">Two-way repeated-measures ANOVA (MSS 1/2/4/8 × within-/between-category conditions) was employed to test for differences in the mean amplitudes of the twelve posterior electrodes across MSS and category conditions, separately for the P1, N1, and P2 components. The Greenhouse-Geisser correction was applied for adjusting for lack of sphericity (<xref ref-type="bibr" rid="R21">Jennings &amp; Wood, 1976</xref>), and only corrected degrees of freedom and <italic>p</italic>-value are reported. Then, cluster-based nonparametric permutation tests (<xref ref-type="bibr" rid="R32">Maris &amp; Oostenveld, 2007</xref>) were employed to further examine the time courses of the main effects and interaction with 1-ms resolution from 100-300 msec; the range that included the three ERP components.</p></sec><sec id="S23"><title>Decoding analyses</title><p id="P27">Based on the pattern across the twelve electrodes, a linear support vector machine (SVM) was employed to conduct cross-task decoding analysis. The visual oddball task data was used to train an animacy decoder, which was used to decode the object categories in the memory search task between 0 and 600 msec, separately for each participant. Temporal resolution was down-sampled to 100 Hz. The area under the receiver-operator-characteristic curve (AUC) was employed to evaluate the performance on classification, which referred to the probability to distinguish positive and negative classes. As classification metric, it is independent from the classifier threshold, and more robust for imbalanced classes than classification accuracy (<xref ref-type="bibr" rid="R50">Treder, 2020</xref>).</p><sec id="S24"><title>Experiment 2</title><p id="P28">The exclusion criteria for removing trials based on behavioral responses were identical to those of Experiment 1. In total, 1.471% of data points were excluded from further analysis.</p><p id="P29">Accuracy was &gt;91% in all conditions and the pattern of accuracy across conditions supported the RT results (data not shown).</p></sec></sec><sec id="S25"><title>ERP Analyses</title><p id="P30">ERP analyses focused on the amplitude of the N2pc component, which was defined by the time window of 200 to 299 msec (<xref ref-type="bibr" rid="R30">Luck &amp; Hillyard, 1994b</xref>; <xref ref-type="bibr" rid="R56">Yeh &amp; Peelen, 2022</xref>) at two electrode sites PO7/8 (<xref ref-type="bibr" rid="R5">Burra &amp; Kerzel, 2013</xref>; <xref ref-type="bibr" rid="R15">Eimer &amp; Kiss, 2008</xref>; <xref ref-type="bibr" rid="R24">Kiss et al., 2008</xref>; <xref ref-type="bibr" rid="R33">Mazza et al., 2007</xref>; <xref ref-type="bibr" rid="R48">Stoletniy et al., 2022</xref>). For target-present trials, differences in the N2pc (contra-ipsilateral responses) were tested in a two-way repeated-measures ANOVA (MSS 1/2/4/8 × T-Dw/T-Db category). Finally, cluster-based nonparametric permutation test (<xref ref-type="bibr" rid="R32">Maris &amp; Oostenveld, 2007</xref>) was adopted to test the time courses of the main effects and interaction with 1-ms resolution from 100-400 msec.</p></sec></sec></sec><sec id="S26" sec-type="results"><title>Results</title><sec id="S27"><title>Experiment 1a</title><p id="P31">Experiment 1a was a behavioral study aimed at replicating previous findings of category effects in memory search (e.g., <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>) but now using a paradigm that would be suitable to use with EEG (Experiment 1b). To avoid differential repetition effects across conditions (<xref ref-type="bibr" rid="R38">Nosofsky, Cao, et al., 2014</xref>), we measured behavioral responses to individually presented distractor objects, with distractor objects making up 80% of trials. Each distractor image was only shown once. We asked: 1) whether search efficiency was modulated by the categorical similarity between the distractors and the objects in the memory set, and 2) whether reaction times for distractors under these conditions would follow the typical log-linear relationship with set size (e.g., <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>; <xref ref-type="bibr" rid="R53">Wolfe, 2012</xref>).</p><sec id="S28"><title>Set Size and Category Effects</title><p id="P32">A two-way repeated-measures ANOVA with RT as dependent variable and memory set size (MSS; 1/2/4/8/16) and category (within-/between-category conditions) as independent variables revealed significant main effects of MSS, <italic>F</italic><sub>(3.41, 133)</sub> = 35.521, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.477, and category, <italic>F</italic><sub>(1, 39)</sub> = 320.039, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.891. Furthermore, the interaction between MSS and category was significant, <italic>F</italic><sub>(4, 156)</sub> = 24.230, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.383. As can be observed in <xref ref-type="fig" rid="F4">Figure 4A</xref>, MSS had a stronger effect (i.e., memory search was less efficient) for within-category than between-category distractors. The simple main effect of set size was significant for both within-, <italic>F</italic><sub>(4, 156)</sub> = 54.442, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.583, and between-category conditions, <italic>F</italic><sub>(4, 156)</sub> = 10.873, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.218.</p></sec><sec id="S29"><title>Linear vs Loglinear models</title><p id="P33">The increase with MSS visibly displayed a non-linear increase, in line with previous work (<xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>; <xref ref-type="bibr" rid="R53">Wolfe, 2012</xref>). To confirm these results statistically, the RTs from set size 1 to 8 were used to predict the performance on set size 16 (<xref ref-type="fig" rid="F4">Figure 4C</xref>), following previous studies (e.g., <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>). For both category conditions, no significant difference was observed between the observed data and the predicted data based on the log2 model, <italic>t</italic><sub>(78)</sub> = 1.406, <italic>p</italic> = 0.164, <italic>d</italic> = 0.314 in within-category condition and <italic>t</italic><sub>(78)</sub> = 0.898, <italic>p</italic> = 0.372, <italic>d</italic> = 0.201 in between-category condition. By contrast, the predicted data based on a linear model was significantly higher than the observed data in both within-category condition, <italic>t</italic><sub>(78)</sub> = 4.997, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 1.117, and between-category condition, <italic>t</italic><sub>(78)</sub> = 2.434, <italic>p</italic> = 0.018, <italic>d</italic> = 0.544.</p><p id="P34">Finally, we fitted the log-linear model to the observed data using all five set sizes. Confirming the category × MSS interaction observed in the ANOVA, the log-linear slope coefficients for the two category conditions differed significantly, <italic>t</italic><sub>(39)</sub> = 9.604, <italic>p</italic> &lt; 0.001, <italic>d</italic> = 1.519, with a steeper slope for within-than between-category distractors.</p></sec><sec id="S30"><title>Summary</title><p id="P35">In this behavioral experiment, we replicated previous findings of a log-linear increase of RT with MSS (e.g., <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>; <xref ref-type="bibr" rid="R53">Wolfe, 2012</xref>). Interestingly, this was observed for distractor objects, which made up 80% of trials. Furthermore, each of these objects was presented only once, excluding the possibility that the set size effect reflected the influence of differential repetition (e.g., items repeating more often in low than high set size conditions; <xref ref-type="bibr" rid="R38">Nosofsky, Cao, et al., 2014</xref>; <xref ref-type="bibr" rid="R39">Nosofsky, Cox, et al., 2014</xref>). Most importantly for the present purpose, we found a strong category effect on search efficiency: memory search was much more efficient for distractors that were categorically dissimilar to the items in the memory set than for distractors that were of the same category as the items in the memory set (<xref ref-type="bibr" rid="R8">Cunningham &amp; Wolfe, 2012</xref>, <xref ref-type="bibr" rid="R9">2014</xref>; <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>).</p></sec></sec><sec id="S31"><title>Experiment 1b</title><p id="P36">Experiment 1b adopted EEG to test when categorical similarity modulates the processing of the distractor objects. We reasoned that if the between-category advantage is driven by the (proactive) use of categorical attentional templates, this would be observed as a modulation of relatively early visual processing (150-250 ms). By contrast, if the between-category advantage is due to a more efficient search in memory (post visual processing), no such early modulation would be observed. Accordingly, we focused our analysis on two visually evoked event-related potential (ERP) components that emerge within the first 200 ms after stimulus onset: P1 and N1. While the P1 is only modulated by spatial attention, the N1 is modulated by feature-based attention (<xref ref-type="bibr" rid="R19">Hopf et al., 2004</xref>; <xref ref-type="bibr" rid="R34">Motter, 1994</xref>; but see <xref ref-type="bibr" rid="R58">Zhang &amp; Luck, 2009</xref>). Similar to feature-based attention, category-based attention was shown to modulate processing from 150-200 ms after stimulus onset (<xref ref-type="bibr" rid="R52">VanRullen &amp; Thorpe, 2001</xref>), with better decoding of attended than unattended categories at this latency (<xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>). Based on these findings, we expected that a category-based attention mechanism during memory search would similarly modulate the N1 component and increase the accuracy of object category decoding at that latency. Finally, the P2 component was also included in our analyses, based on previous studies implicating the P2 in matching perceptual inputs to memory templates (<xref ref-type="bibr" rid="R13">Dunn et al., 1998</xref>; <xref ref-type="bibr" rid="R16">Freunberger et al., 2007</xref>; <xref ref-type="bibr" rid="R27">Lefebvre et al., 2005</xref>; <xref ref-type="bibr" rid="R29">Luck &amp; Hillyard, 1994a</xref>).</p><sec id="S32"><title>Behavioral Results</title><p id="P37"><xref ref-type="fig" rid="F4">Figure 4B</xref> shows the behavioral results of Experiment 1b. These results replicated the findings of Experiment 1a. There were significant main effects of MSS, <italic>F</italic><sub>(3, 93)</sub> = 31.591, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.505, and category, <italic>F</italic><sub>(1, 31)</sub> = 91.205, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.746. As in Experiment 1a, the interaction between MSS and category was significant, <italic>F</italic><sub>(2.34, 72.61)</sub> = 6.798, <italic>p</italic> = 0.003, <italic>ηP<sup>2</sup></italic> = 0.180. Simple effects of MSS were significant in both within-category condition, <italic>F</italic><sub>(3, 93)</sub> = 42.176, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.576, and between-category condition, <italic>F</italic><sub>(3, 93)</sub> = 14.41, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.317. Pairwise comparisons showed significant category effects for all MSSs (<italic>p</italic> &lt; 0.001), except for set size 1 (<italic>p</italic> = 0.175).</p><p id="P38">The linear/log 2 prediction based on set sizes 1 to 4 demonstrated that the log-linear model was a better fit for both within-category and between-category conditions (<xref ref-type="fig" rid="F4">Figure 4D</xref>): Within category: log-linear vs observed: <italic>t</italic><sub>(62)</sub> = -0.121, <italic>p</italic> = 0.904, <italic>d</italic> = 0.030, while linear vs observed: <italic>t</italic><sub>(62)</sub> = 2.116, <italic>p</italic> = 0.019, <italic>d</italic> = 0.529 (1-tailed; greater); between category: log-linear vs observed: <italic>t</italic><sub>(62)</sub> = 0.575, <italic>p</italic> = 0.568, <italic>d</italic> = 0.144 while linear vs observed: <italic>t</italic><sub>(62)</sub> = 1.798, <italic>p</italic> = 0.039, <italic>d</italic> = 0.449 (1-tailed; greater). The slope coefficients between these two conditions (fitting the model on all set sizes) were also significantly different, <italic>t</italic><sub>(31)</sub> = 3.488, <italic>p</italic> = 0.001, <italic>d</italic> = 0.617.</p></sec><sec id="S33"><title>ERP Results</title><p id="P39">Separate ANOVAs were run for the three components of interest (P1, N1, P2). There were no significant effects for the P1, MSS effect: <italic>F</italic><sub>(3, 93)</sub> = 0.629, <italic>p</italic> = 0.598, <italic>ηP<sup>2</sup></italic> = 0.020, category effect: <italic>F</italic><sub>(1, 31)</sub> = 1.593, <italic>p</italic> = 0.216, <italic>ηP<sup>2</sup></italic> = 0.049, and interaction: <italic>F</italic><sub>(3, 93)</sub> = 0.034, <italic>p</italic> = 0.992, <italic>ηP<sup>2</sup></italic> = 0.001 (<xref ref-type="fig" rid="F5">Figure 5B</xref>). Importantly, confirming our hypothesis, the N1 showed a significant main effect of category, <italic>F</italic><sub>(1, 31)</sub> = 9.185, <italic>p</italic> = 0.005, <italic>ηP<sup>2</sup></italic> = 0.229 (<xref ref-type="fig" rid="F5">Figure 5C</xref>). The main effect of MSS, <italic>F</italic><sub>(3, 93)</sub> = 2.272, <italic>p</italic> = 0.085, <italic>ηP<sup>2</sup></italic> = 0.068, and the interaction between category and MSS, <italic>F</italic><sub>(3, 93)</sub> = 0.285, <italic>p</italic> = 0.836, <italic>ηP<sup>2</sup></italic> = 0.009, were not significant. Finally, the P2 showed main effects of MSS, <italic>F</italic><sub>(3, 93)</sub> = 9.233, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.229, and category, <italic>F</italic><sub>(1, 31)</sub> = 18.195, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.370 (<xref ref-type="fig" rid="F5">Figure 5D</xref>). The interaction between category and MSS was not significant, <italic>F</italic><sub>(3, 93)</sub> = 0.650, <italic>p</italic> = 0.585, <italic>ηP<sup>2</sup></italic> = 0.021.</p><p id="P40">The ERP results were confirmed by a cluster permutation test (<xref ref-type="fig" rid="F6">Figure 6</xref>), showing significant category effects from 152 to 260 msec (cluster-based <italic>p</italic> = 0.001) and significant MSS effects from 188 to 292 msec (cluster-based <italic>p</italic> = 0.002). No interaction effects were found in this analysis.</p></sec><sec id="S34"><title>Decoding Results</title><p id="P41">To test whether attention modulated information about object category, we decoded the category of the distractor objects using a classifier trained on data from a separate experiment that did not involve memory search (see <xref ref-type="sec" rid="S2">Materials and Methods</xref>), following the cross-decoding approach of a previous attention study (<xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>). Decoding accuracy reflects the representational strength of the distractor objects, rather than the amplitude of the evoked responses. Results showed that AUC scores in all eight conditions reached significance (cluster-based <italic>p</italic> &lt; 0.05) from around 130-150 to 400-580 msec (<xref ref-type="fig" rid="F7">Figure 7A, 7B</xref>), with the first peak at around 170 msec, in line with previous decoding studies (Carlson et al., 2013; Cichy et al., 2014; <xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>).</p><p id="P42">Next, we averaged decoding accuracy across the time window of each ERP component and tested these using repeated-measures ANOVAs. In line with the ERP results, no significant effects were observed for the P1 time window (<italic>p</italic> &gt; 0.335, for all tests; <xref ref-type="fig" rid="F7">Figure 7C</xref>). Interestingly, the N1 showed a significant main effect of category, <italic>F</italic><sub>(1, 31)</sub> = 4.516, <italic>p</italic> = 0.042, <italic>ηP<sup>2</sup></italic> = 0.127, with better decoding for within-than between-category distractors (<xref ref-type="fig" rid="F7">Figure 7D</xref>). The main effect of MSS, and the interaction between MSS and category, were not significant (<italic>p</italic> &gt; 0.678, for all tests). Finally, no significant effects were observed for the P2 time window (<italic>p</italic> &gt; 0.109, for all tests; <xref ref-type="fig" rid="F7">Figure 7E</xref>).</p></sec><sec id="S35"><title>Summary</title><p id="P43">The behavioral results of Experiment 1b replicated those of Experiment 1a, again showing that memory search was more efficient for between-than within-category distractors. The ERP results showed that category membership modulated the visually evoked N1 component (160-200 ms) as well as the subsequent P2 component (200-300 ms), while set size only modulated the P2 component. The cluster permutation test confirmed these results, showing a relatively early category effect, from 152 to 260 msec, while set size effects emerged from 188 to 292 msec. Finally, the decoding results of the N1 window showed that object category decoding was higher for within-category distractors than between-category distractors. Altogether, these results provide evidence for category-level attentional modulation during a memory search task. Distractor objects matching the category of the memory set received more processing than non-matching distractor objects, demonstrated both by a differential evoked responses (<xref ref-type="bibr" rid="R52">VanRullen &amp; Thorpe, 2001</xref>) and more accurate categorical representation (<xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>) around 160-200 ms after onset.</p></sec></sec><sec id="S36"><title>Experiment 2</title><p id="P44">In Experiment 2, we followed up on the findings of Experiment 1b, testing whether category-matching distractors attract spatial attention. By having participants search for targets in two-object displays (<xref ref-type="fig" rid="F3">Figure 3</xref>), we could measure the allocation of spatial attention using the lateralized N2pc EEG component: previous studies showed that template-matching objects (e.g., targets) during visual search attract spatial attention, eliciting an N2pc (<xref ref-type="bibr" rid="R14">Eimer, 1996</xref>; <xref ref-type="bibr" rid="R31">Luck et al., 2000</xref>). This target-elicited N2pc is reduced when a target appears together with a distractor that partially matches the template (<xref ref-type="bibr" rid="R35">Nako et al., 2016</xref>; <xref ref-type="bibr" rid="R55">Wu et al., 2016</xref>; <xref ref-type="bibr" rid="R57">Yeh et al., 2019</xref>; <xref ref-type="bibr" rid="R56">Yeh &amp; Peelen, 2022</xref>). Therefore, if participants adopted a category-level attentional template in our memory search task, we expected the N2pc to be reduced when a target appeared next to a within-category (“T-Dw”) as compared to a between-category (“T-Db”) distractor. For the same reason, we expected to observe an N2pc in response to a within-category distractor (“Dw”) when shown together with a between-category (“Db”) distractor.</p><sec id="S37"><title>Behavioral Results</title><p id="P45"><xref ref-type="fig" rid="F8">Figure 8A</xref> shows the RT results for target-present trials. A two-way repeated-measures ANOVA (MSS; 1/2/4/8 × category; T-Dw/T-Db) showed a main effect of MSS, <italic>F</italic><sub>(2.03, 62.88)</sub> = 104.097, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.771, a main effect of category, <italic>F</italic><sub>(1, 31)</sub> = 32.212, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.510, and an interaction between MSS and category, <italic>F</italic><sub>(3, 93)</sub> = 4.568, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.128. Simple main effects of MSS were also significant within both the T-Dw condition, <italic>F</italic><sub>(2.23, 69.1)</sub> = 88.3, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.576, and the T-Db condition, <italic>F</italic><sub>(1.93, 59.9)</sub> = 92.5, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.749. Simple main effects of category were observed in MSS 4 and 8, <italic>F</italic><sub>(1, 31)</sub> = 20.4, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.397 and <italic>F</italic><sub>(1, 31)</sub> = 11.5, <italic>p</italic> = 0.002, <italic>ηP<sup>2</sup></italic> = 0.271, but not in MSS 1 and 2, <italic>F</italic><sub>(1, 31)</sub> = 0.65, <italic>p</italic> = 0.426, <italic>ηP<sup>2</sup></italic> = 0.021 and <italic>F</italic><sub>(1, 31)</sub> = 1.55, <italic>p</italic> = 0.222, <italic>ηP<sup>2</sup></italic> = 0.048.</p><p id="P46">For target-absent trials (<xref ref-type="fig" rid="F8">Figure 8B</xref>), a two-way repeated-measures ANOVA (MSS 1/2/4/8 × Dw-Db/Dw-Dw/Db-Db category) showed a main effect of MSS, <italic>F</italic><sub>(3,93)</sub> = 99.905, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.763, a main effect of category, <italic>F</italic><sub>(1.61,50.05)</sub> = 88.455, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.740, and an interaction between set size and category, <italic>F</italic><sub>(3.7,114.82)</sub> = 12.108, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.281. Significant main effects of MSS were observed in all three category conditions (Dw-Db: <italic>F</italic><sub>(3, 93)</sub> = 95.57, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.755; Dw-Dw: <italic>F</italic><sub>(3, 93)</sub> = 75.906, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.710; Db-Db: <italic>F</italic><sub>(3, 93)</sub> = 34.368, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.526).</p></sec><sec id="S38"><title>ERP Results</title><sec id="S39"><title>N2pc induced by targets</title><p id="P47">In a first analysis, we wanted to verify that the targets in our experiment evoked a reliable N2pc. Averaged across conditions, we observed a strong N2pc, with a more negative response contralateral vs ipsilateral to the target from around 200 ms after stimulus onset (<xref ref-type="fig" rid="F9">Figure 9A</xref>). Next, we averaged the amplitude of the evoked response in the N2pc time window (200-299 ms after onset) and tested the N2pc effect for each set size (<xref ref-type="fig" rid="F9">Figure 9B</xref>). This analysis revealed a significant N2pc for each set size (<italic>p</italic> &lt; 0.005, for all tests).</p><p id="P48">Having established a reliable target-related N2pc, we then tested how this effect was modulated by MSS and category through a two-way repeated-measures ANOVA with the N2pc as dependent variable (contra-ipsi, averaged across 200-299 ms) and MSS (1/2/4/8) and category (T-Dw/T-Db) as independent variables. Results are shown in <xref ref-type="fig" rid="F10">Figure 10A</xref>. The main effects of MSS and category were significant, MSS: <italic>F</italic><sub>(3, 93)</sub> = 8.672, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.219, category: <italic>F</italic><sub>(1, 31)</sub> = 49.580, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.615. The interaction between MSS and category was also significant, <italic>F</italic><sub>(3, 93)</sub> = 4.822, <italic>p</italic> = 0.004, <italic>ηP<sup>2</sup></italic> = 0.135. Following up on the interaction, we found that the simple main effect of MSS was significant in the T-Dw condition, <italic>F</italic><sub>(3, 93)</sub> = 11.4, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.268, but not in the T-Db condition, <italic>F</italic><sub>(3, 93)</sub> = 1.87, <italic>p</italic> = 0.14, <italic>ηP<sup>2</sup></italic> = 0.057. Furthermore, the effect of category was significant for MSS 4 and 8, <italic>F</italic><sub>(1, 31)</sub> = 20.1, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.394 and <italic>F</italic><sub>(1, 31)</sub> = 19.1, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.381, but not for MSS 1 and 2, <italic>F</italic><sub>(1, 31)</sub> = 0.64, <italic>p</italic> = 0.43, <italic>ηP<sup>2</sup></italic> = 0.02 and <italic>F</italic><sub>(1, 31)</sub> = 1.81, <italic>p</italic> = 0.188, <italic>ηP<sup>2</sup></italic> = 0.055.</p><p id="P49">These results were confirmed by cluster permutation tests (<xref ref-type="fig" rid="F10">Figures 10B and 10C</xref>), which showed significant MSS effects from 220 to 284 msec and 308 to 392 msec (both cluster-based <italic>p</italic> = 0.001) and significant category effects from 168 to 300 msec (cluster-based <italic>p</italic> = 0.001). The interaction of the two effects was significant from 264 to 328 msec (cluster-based <italic>p</italic> = 0.013).</p><p id="P50">Together, these results confirm our first hypothesis, that the target-induced N2pc is reduced in the presence of a within-category distractor. Mirroring the behavioral results, this reduction was stronger for larger set size.</p></sec><sec id="S40"><title>N2pc induced by distractors</title><p id="P51">Next, we tested our second hypothesis, that of an N2pc induced by a within-category (“Dw”) versus between-category (“Db”) distractors. Dw-Dw and Db-Db trials were not included in the analysis because there was, by definition, no lateralized attentional bias in these two conditions. Results confirmed our hypothesis: we observed a significant difference between contra- and ipsi-lateral responses from around 200 ms after stimulus onset (<xref ref-type="fig" rid="F11">Figure 11A</xref>). Averaging responses across the N2pc time window (200 and 299 msec) revealed a significant N2pc, <italic>F</italic><sub>(1, 31)</sub> = 121.917, <italic>p</italic> &lt; 0.001, <italic>ηP<sup>2</sup></italic> = 0.797. The N2pc did not differ significantly across set size, <italic>F</italic><sub>(3, 93)</sub> = 1.474, <italic>p</italic> = 0.227, <italic>ηP<sup>2</sup></italic> = 0.045 (<xref ref-type="fig" rid="F11">Figure 11B</xref>).</p></sec></sec><sec id="S41"><title>Summary</title><p id="P52">The results of Experiment 2 showed that the target-elicited N2pc was reduced when the target was shown next to a same-category distractor. Furthermore, spatial attention (indexed by the N2pc component) was directed towards distractors that matched the category of the memory set. Altogether, these results provide evidence that participants formed a categorical attentional template, with spatial attention being directed to distractor objects belonging to the category of the memory set.</p></sec></sec></sec><sec id="S42" sec-type="discussion"><title>Discussion</title><p id="P53">In three experiments, we investigated the role of attention in memory search. In an online behavioral experiment (Experiment 1a), participants memorized target objects; then, during a memory search phase, they viewed one object at a time and decided whether the object was part of the memorized set of objects. The memory set always consisted of objects from a single category (animate or inanimate objects), while the distractor objects could be from this or another category. Our analyses focused on responses to these distractor objects (80% of trials) as a function of memory set size (1, 2, 4, 8, 16) and category (same or different category as the memory set). Results showed that memory search was much more efficient for between-category than within-category distractors, replicating earlier work (<xref ref-type="bibr" rid="R8">Cunningham &amp; Wolfe, 2012</xref>, <xref ref-type="bibr" rid="R9">2014</xref>; <xref ref-type="bibr" rid="R12">Drew &amp; Wolfe, 2014</xref>). Using EEG (Experiment 1b), we tested whether this increased efficiency could be explained by attentional modulation at the level of object category. Results confirmed this hypothesis, showing that early visual object processing was modulated by the category membership of the distractor: We found a larger N1 in response to distractors from the target category compared to distractors from a different category. Furthermore, decoding analyses showed that within-category distractors were more strongly represented than between-category distractors at this latency. The results of Experiment 1 are in line with the attentional modulation of visual processing observed in single-target detection tasks (<xref ref-type="bibr" rid="R23">Kaiser et al., 2016</xref>; <xref ref-type="bibr" rid="R52">VanRullen &amp; Thorpe, 2001</xref>). This modulation is much earlier than the typical time window of memory retrieval, which usually starts after ~300 msec (<xref ref-type="bibr" rid="R10">Curran &amp; Hancock, 2007</xref>; <xref ref-type="bibr" rid="R37">Noh et al., 2018</xref>; <xref ref-type="bibr" rid="R46">Rugg et al., 1998</xref>; <xref ref-type="bibr" rid="R45">Rugg &amp; Curran, 2007</xref>). In Experiment 2, we presented two objects simultaneously to test whether spatial attention (indexed by the N2pc component) was guided to the location of template-matching objects (<xref ref-type="bibr" rid="R2">Battistoni et al., 2020</xref>; Eimer, 2014; <xref ref-type="bibr" rid="R19">Hopf et al., 2004</xref>; <xref ref-type="bibr" rid="R54">Wolfe, 2021</xref>). We found that spatial attention was directed towards distractor objects that were of the same category as the items in the memory set. Together, the results provide evidence that participants spontaneously use the shared category of the memory items to form a category-level attentional template. By allocating more attentional resources to the features (N1) and location (N2pc) of the target category, they were able to efficiently reject between-category distractors before commencing search in long-term memory.</p><p id="P54">The behavioral results of all experiments (<xref ref-type="fig" rid="F4">Figure 4</xref>; <xref ref-type="fig" rid="F8">Figure 8</xref>) and the target related N2pc results of Experiment 2 (<xref ref-type="fig" rid="F10">Figure 10A</xref>) revealed that category interacted with memory set size, such that the categorical modulation became stronger with increasing set size. This result can be explained in at least two ways. First, it is possible that participants only started to use an attentional template-based strategy when memory search became effortful (i.e., with high set size). Alternatively, participants may have adopted an attentional template-based strategy for all set sizes, but the specificity of the template varied with the memory set size. For a set size of two, the template may have been specific to the subcategories of the two targets (e.g., a cat and a horse). In that case, distractor objects from other subcategories (e.g., reptile, bird) may not have provided a strong match to the template. Instead, for higher set sizes, a larger number of subcategories made up the memory set. In that case, the features that the subcategories had in common were more likely to generalize to the distractor objects from the same superordinate category (e.g., animals). Future studies could systematically manipulate the similarity of the subcategories within the memory set to distinguish between these accounts.</p><p id="P55">The interaction between set size and category also shows that set size more strongly affected the rejection of within-category than between-category distractors. This suggests that within-category distractors activated a memory search process, while between-category distractors did so only weakly. Of note, the effect of set size was still significant for between-category distractors in all behavioral analyses, suggesting that categorical attention was not fully preventing between-category distractors from entering the memory search process.</p><p id="P56">Interestingly, not all analyses showed an interaction between set size and category. Specifically, the modulation of visual distractor processing in Experiment 1b (<xref ref-type="fig" rid="F5">Figure 5C</xref>; <xref ref-type="fig" rid="F7">7D</xref>) and the distractor evoked N2pc in Experiment 2 (<xref ref-type="fig" rid="F11">Figure 11B</xref>) only showed a main effect of category. It is possible that the absence of an interaction in these analyses reflected a lack of power (e.g., see the weak trend towards an interaction in <xref ref-type="fig" rid="F11">Figure 11B</xref>). Alternatively, the interaction may reflect a true dissociation between these measures. For example, in Experiment 2, if we assume that the within-category distractor only weakly matched the template when set size was low (e.g., because the template was specific to the subcategories in the memory set) these distractors would not have provided strong competition when shown next to a target, resulting in the category x set size interaction observed for the target-related N2pc. However, when the same within-category distractor is shown next to a between-category distractor, it would still provide a relatively better match to the template than the between-category distractor, and thus attract spatial attention even in the low set size conditions.</p><p id="P57">Our findings raise the question of what features the categorical template consists of, and whether categorical templates are specific to the categories used here. Animate and inanimate objects differ in terms of mid-, and high-level visual features (e.g., <xref ref-type="bibr" rid="R22">Jozwik et al., 2022</xref>; <xref ref-type="bibr" rid="R28">Long et al., 2018</xref>; <xref ref-type="bibr" rid="R43">Proklova et al., 2016</xref>; <xref ref-type="bibr" rid="R49">Thorat et al., 2019</xref>) and it has been proposed that the human visual system is particularly sensitive to these category-diagnostic features (<xref ref-type="bibr" rid="R36">New et al., 2007</xref>), as also reflected in the animate-inanimate organization of the ventral visual cortex (<xref ref-type="bibr" rid="R6">Chao et al., 1999</xref>; <xref ref-type="bibr" rid="R18">Grill-Spector &amp; Weiner, 2014</xref>; <xref ref-type="bibr" rid="R26">Kriegeskorte et al., 2008</xref>; <xref ref-type="bibr" rid="R49">Thorat et al., 2019</xref>). This raises the possibility that categorical attention in memory search, as revealed here, is specific to the distinction between animate and inanimate objects. Future studies will need to test whether our results generalize to other categorical distinctions (e.g., fruit vs non-fruit). We anticipate that results are most likely to generalize to categories that, like animals, are highly familiar and that are characterized by diagnostic visual features (<xref ref-type="bibr" rid="R3">Battistoni et al., 2017</xref>).</p><p id="P58">To conclude, our study reveals a crucial role of attention in memory search. When observers look for multiple objects at the same time, they can use the objects’ shared categorical features to direct attention at that level, leading to the efficient rejection of distractor objects belonging to other categories (<xref ref-type="fig" rid="F1">Figure 1B</xref>).</p></sec></body><back><ack id="S43"><title>Funding</title><p>LS was supported by the China Scholarship Council (CSC). The project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (grant agreement No. 725970).</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bansal</surname><given-names>AK</given-names></name><name><surname>Madhavan</surname><given-names>R</given-names></name><name><surname>Agam</surname><given-names>Y</given-names></name><name><surname>Golby</surname><given-names>A</given-names></name><name><surname>Madsen</surname><given-names>JR</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name></person-group><article-title>Neural Dynamics Underlying Target Detection in the Human Brain</article-title><source>Journal of Neuroscience</source><year>2014</year><volume>34</volume><issue>8</issue><fpage>3042</fpage><lpage>3055</lpage><pub-id pub-id-type="pmcid">PMC3931508</pub-id><pub-id pub-id-type="pmid">24553944</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3781-13.2014</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battistoni</surname><given-names>E</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Hickey</surname><given-names>C</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The time course of spatial attention during naturalistic visual search</article-title><source>Cortex</source><year>2020</year><volume>122</volume><fpage>225</fpage><lpage>234</lpage><pub-id pub-id-type="pmid">30563703</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Battistoni</surname><given-names>E</given-names></name><name><surname>Stein</surname><given-names>T</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Preparatory attention in visual cortex: Preparatory attention in visual cortex</article-title><source>Annals of the New York Academy of Sciences</source><year>2017</year><volume>1396</volume><issue>1</issue><fpage>92</fpage><lpage>107</lpage><pub-id pub-id-type="pmid">28253445</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brady</surname><given-names>TF</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name><name><surname>Alvarez</surname><given-names>GA</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><article-title>Visual long-term memory has a massive storage capacity for object details</article-title><source>Proceedings of the National Academy of Sciences of the United States of America</source><year>2008</year><volume>105</volume><issue>38</issue><fpage>14325</fpage><lpage>14329</lpage><pub-id pub-id-type="pmcid">PMC2533687</pub-id><pub-id pub-id-type="pmid">18787113</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0803390105</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burra</surname><given-names>N</given-names></name><name><surname>Kerzel</surname><given-names>D</given-names></name></person-group><article-title>Attentional capture during visual search is attenuated by target predictability: Evidence from the N2pc, Pd, and topographic segmentation: Saliency and target predictability</article-title><source>Psychophysiology</source><year>2013</year><volume>50</volume><issue>5</issue><fpage>422</fpage><lpage>430</lpage><pub-id pub-id-type="pmid">23418888</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>LL</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Martin</surname><given-names>A</given-names></name></person-group><article-title>Attribute-based neural substrates in temporal cortex for perceiving and knowing about objects</article-title><source>Nature neuroscience</source><year>1999</year><volume>2</volume><issue>10</issue><fpage>913</fpage><lpage>919</lpage><pub-id pub-id-type="pmid">10491613</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chelazzi</surname><given-names>L</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name><name><surname>Desimone</surname><given-names>R</given-names></name></person-group><article-title>A neural basis for visual search in inferior temporal cortex</article-title><source>Nature</source><year>1993</year><volume>363</volume><issue>6427</issue><fpage>345</fpage><lpage>347</lpage><pub-id pub-id-type="pmid">8497317</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>CA</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Lions or tigers or bears: Oh my! Hybrid visual and memory search for categorical targets</article-title><source>Visual Cognition</source><year>2012</year><volume>20</volume><issue>9</issue><fpage>1024</fpage><lpage>1027</lpage><pub-id pub-id-type="doi">10.1080/13506285.2012.726455</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>CA</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>The role of object categories in hybrid visual and memory search</article-title><source>Journal of Experimental Psychology: General</source><year>2014</year><volume>143</volume><issue>4</issue><fpage>1585</fpage><lpage>1599</lpage><pub-id pub-id-type="pmcid">PMC4115034</pub-id><pub-id pub-id-type="pmid">24661054</pub-id><pub-id pub-id-type="doi">10.1037/a0036313</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curran</surname><given-names>T</given-names></name><name><surname>Hancock</surname><given-names>J</given-names></name></person-group><article-title>The FN400 indexes familiarity-based recognition of faces</article-title><source>NeuroImage</source><year>2007</year><volume>36</volume><issue>2</issue><fpage>464</fpage><lpage>471</lpage><pub-id pub-id-type="pmcid">PMC1948028</pub-id><pub-id pub-id-type="pmid">17258471</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.12.016</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desimone</surname><given-names>R</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><article-title>Neural Mechanisms of Selective Visual Attention</article-title><source>Annual Review of Neuroscience</source><year>1995</year><volume>18</volume><issue>1</issue><fpage>193</fpage><lpage>222</lpage><pub-id pub-id-type="pmid">7605061</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drew</surname><given-names>T</given-names></name><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Hybrid search in the temporal domain: Evidence for rapid, serial logarithmic search through memory</article-title><source>Attention, Perception, Psychophysics</source><year>2014</year><volume>76</volume><issue>2</issue><fpage>296</fpage><lpage>303</lpage><pub-id pub-id-type="pmcid">PMC4350328</pub-id><pub-id pub-id-type="pmid">24343519</pub-id><pub-id pub-id-type="doi">10.3758/s13414-013-0606-y</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>BR</given-names></name><name><surname>Dunn</surname><given-names>DA</given-names></name><name><surname>Languis</surname><given-names>M</given-names></name><name><surname>Andrews</surname><given-names>D</given-names></name></person-group><article-title>The Relation of ERP Components to Complex Memory Processing</article-title><source>Brain and Cognition</source><year>1998</year><volume>36</volume><issue>3</issue><fpage>355</fpage><lpage>376</lpage><pub-id pub-id-type="pmid">9647684</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>The N2pc component as an indicator of attentional selectivity</article-title><source>Electroencephalography and Clinical Neurophysiology</source><year>1996</year><volume>99</volume><issue>3</issue><fpage>225</fpage><lpage>234</lpage><pub-id pub-id-type="pmid">8862112</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eimer</surname><given-names>M</given-names></name><name><surname>Kiss</surname><given-names>M</given-names></name></person-group><article-title>Involuntary Attentional Capture is Determined by Task Set: Evidence from Event-related Brain Potentials</article-title><source>Journal of Cognitive Neuroscience</source><year>2008</year><volume>20</volume><issue>8</issue><fpage>1423</fpage><lpage>1433</lpage><pub-id pub-id-type="pmcid">PMC2564114</pub-id><pub-id pub-id-type="pmid">18303979</pub-id><pub-id pub-id-type="doi">10.1162/jocn.2008.20099</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freunberger</surname><given-names>R</given-names></name><name><surname>Klimesch</surname><given-names>W</given-names></name><name><surname>Doppelmayr</surname><given-names>M</given-names></name><name><surname>Höller</surname><given-names>Y</given-names></name></person-group><article-title>Visual P2 component is related to theta phase-locking</article-title><source>Neuroscience Letters</source><year>2007</year><volume>426</volume><issue>3</issue><fpage>181</fpage><lpage>186</lpage><pub-id pub-id-type="pmid">17904744</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><article-title>MEG and EEG data analysis with MNE-Python</article-title><source>Frontiers in Neuroscience</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3872725</pub-id><pub-id pub-id-type="pmid">24431986</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2013.00267</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name></person-group><article-title>The functional architecture of the ventral temporal cortex and its role in categorization</article-title><source>Nature Reviews Neuroscience</source><year>2014</year><volume>15</volume><issue>8</issue><fpage>536</fpage><lpage>548</lpage><pub-id pub-id-type="pmcid">PMC4143420</pub-id><pub-id pub-id-type="pmid">24962370</pub-id><pub-id pub-id-type="doi">10.1038/nrn3747</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopf</surname><given-names>J-M</given-names></name><name><surname>Boelmans</surname><given-names>K</given-names></name><name><surname>Schoenfeld</surname><given-names>MA</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Heinze</surname><given-names>H-J</given-names></name></person-group><article-title>Attention to Features Precedes Attention to Locations in Visual Search: Evidence from Electromagnetic Brain Responses in Humans</article-title><source>The Journal of Neuroscience</source><year>2004</year><volume>24</volume><issue>8</issue><fpage>1822</fpage><lpage>1832</lpage><pub-id pub-id-type="pmcid">PMC6730400</pub-id><pub-id pub-id-type="pmid">14985422</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3564-03.2004</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>The effect of items in working memory on the deployment of attention and the eyes during visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2006</year><volume>32</volume><issue>2</issue><fpage>423</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">16634680</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jennings</surname><given-names>J</given-names></name><name><surname>Wood</surname><given-names>CC</given-names></name></person-group><article-title>The epsilon-Adjustment Procedure for Repeated-Measures Analyses of Variance</article-title><source>Psychophysiology</source><year>1976</year><volume>13</volume><issue>3</issue><fpage>277</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">1273235</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jozwik</surname><given-names>KM</given-names></name><name><surname>Najarro</surname><given-names>E</given-names></name><name><surname>Van Den Bosch</surname><given-names>JJ</given-names></name><name><surname>Charest</surname><given-names>I</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><article-title>Disentangling five dimensions of animacy in human brain and behaviour</article-title><source>Communications Biology</source><year>2022</year><volume>5</volume><issue>1</issue><elocation-id>1247</elocation-id><pub-id pub-id-type="pmcid">PMC9663603</pub-id><pub-id pub-id-type="pmid">36376446</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-04194-y</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Oosterhof</surname><given-names>NN</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The Neural Dynamics of Attentional Selection in Natural Scenes</article-title><source>Journal of Neuroscience</source><year>2016</year><volume>36</volume><issue>41</issue><fpage>10522</fpage><lpage>10528</lpage><pub-id pub-id-type="pmcid">PMC6601932</pub-id><pub-id pub-id-type="pmid">27733605</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1385-16.2016</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiss</surname><given-names>M</given-names></name><name><surname>Van Velzen</surname><given-names>J</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>The N2pc component and its links to attention shifts and spatially selective visual processing</article-title><source>Psychophysiology</source><year>2008</year><volume>45</volume><issue>2</issue><fpage>240</fpage><lpage>249</lpage><pub-id pub-id-type="pmcid">PMC2248220</pub-id><pub-id pub-id-type="pmid">17971061</pub-id><pub-id pub-id-type="doi">10.1111/j.1469-8986.2007.00611.x</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowler</surname><given-names>E</given-names></name><name><surname>Anderson</surname><given-names>E</given-names></name><name><surname>Dosher</surname><given-names>B</given-names></name><name><surname>Blaser</surname><given-names>E</given-names></name></person-group><article-title>The role of attention in the programming of saccades</article-title><source>Vision Research</source><year>1995</year><volume>35</volume><issue>13</issue><fpage>1897</fpage><lpage>1916</lpage><pub-id pub-id-type="pmid">7660596</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Ruff</surname><given-names>DA</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Bodurka</surname><given-names>J</given-names></name><name><surname>Esteky</surname><given-names>H</given-names></name><etal/><name><surname>Bandettini</surname><given-names>PA</given-names></name></person-group><article-title>Matching categorical object representations in inferior temporal cortex of man and monkey</article-title><source>Neuron</source><year>2008</year><volume>60</volume><issue>6</issue><fpage>1126</fpage><lpage>1141</lpage><pub-id pub-id-type="pmcid">PMC3143574</pub-id><pub-id pub-id-type="pmid">19109916</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2008.10.043</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lefebvre</surname><given-names>CD</given-names></name><name><surname>Marchand</surname><given-names>Y</given-names></name><name><surname>Eskes</surname><given-names>GA</given-names></name><name><surname>Connolly</surname><given-names>JF</given-names></name></person-group><article-title>Assessment of working memory abilities using an event-related brain potential (ERP)-compatible digit span backward task</article-title><source>Clinical Neurophysiology</source><year>2005</year><volume>116</volume><issue>7</issue><fpage>1665</fpage><lpage>1680</lpage><pub-id pub-id-type="pmid">15908268</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>B</given-names></name><name><surname>Yu</surname><given-names>CP</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><article-title>Mid-level visual features underlie the high-level categorical organization of the ventral stream</article-title><source>Proceedings of the National Academy of Sciences</source><year>2018</year><volume>115</volume><issue>38</issue><fpage>E9015</fpage><lpage>E9024</lpage><pub-id pub-id-type="pmcid">PMC6156638</pub-id><pub-id pub-id-type="pmid">30171168</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1719616115</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Electrophysiological correlates of feature analysis during visual search</article-title><source>Psychophysiology</source><year>1994a</year><volume>31</volume><issue>3</issue><fpage>291</fpage><lpage>308</lpage><pub-id pub-id-type="pmid">8008793</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Hillyard</surname><given-names>SA</given-names></name></person-group><article-title>Spatial filtering during visual search: Evidence from human electrophysiology</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1994b</year><volume>20</volume><issue>5</issue><fpage>1000</fpage><lpage>1014</lpage><pub-id pub-id-type="pmid">7964526</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luck</surname><given-names>SJ</given-names></name><name><surname>Woodman</surname><given-names>GF</given-names></name><name><surname>Vogel</surname><given-names>EK</given-names></name></person-group><article-title>Event-related potential studies of attention</article-title><source>Trends in Cognitive Sciences</source><year>2000</year><volume>4</volume><issue>11</issue><fpage>432</fpage><lpage>440</lpage><pub-id pub-id-type="pmid">11058821</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name></person-group><article-title>Nonparametric statistical testing of EEG-and MEG-data</article-title><source>Journal of Neuroscience Methods</source><year>2007</year><volume>164</volume><issue>1</issue><fpage>177</fpage><lpage>190</lpage><pub-id pub-id-type="pmid">17517438</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazza</surname><given-names>V</given-names></name><name><surname>Turatto</surname><given-names>M</given-names></name><name><surname>Umiltà</surname><given-names>C</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>Attentional selection and identification of visual objects are reflected by distinct electrophysiological responses</article-title><source>Experimental Brain Research</source><year>2007</year><volume>181</volume><issue>3</issue><fpage>531</fpage><lpage>536</lpage><pub-id pub-id-type="pmcid">PMC2258005</pub-id><pub-id pub-id-type="pmid">17602216</pub-id><pub-id pub-id-type="doi">10.1007/s00221-007-1002-4</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motter</surname><given-names>B</given-names></name></person-group><article-title>Neural correlates of attentive selection for color or luminance in extrastriate area V4</article-title><source>The Journal of Neuroscience</source><year>1994</year><volume>14</volume><issue>4</issue><fpage>2178</fpage><lpage>2189</lpage><pub-id pub-id-type="pmcid">PMC6577115</pub-id><pub-id pub-id-type="pmid">8158264</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-04-02178.1994</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nako</surname><given-names>R</given-names></name><name><surname>Grubert</surname><given-names>A</given-names></name><name><surname>Eimer</surname><given-names>M</given-names></name></person-group><article-title>Category-based guidance of spatial attention during visual search for feature conjunctions</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2016</year><volume>42</volume><issue>10</issue><fpage>1571</fpage><lpage>1586</lpage><pub-id pub-id-type="pmid">27213833</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>New</surname><given-names>J</given-names></name><name><surname>Cosmides</surname><given-names>L</given-names></name><name><surname>Tooby</surname><given-names>J</given-names></name></person-group><article-title>Category-specific attention for animals reflects ancestral priorities, not expertise</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><issue>42</issue><fpage>16598</fpage><lpage>16603</lpage><pub-id pub-id-type="pmcid">PMC2034212</pub-id><pub-id pub-id-type="pmid">17909181</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0703913104</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noh</surname><given-names>E</given-names></name><name><surname>Liao</surname><given-names>K</given-names></name><name><surname>Mollison</surname><given-names>MV</given-names></name><name><surname>Curran</surname><given-names>T</given-names></name><name><surname>de Sa</surname><given-names>VR</given-names></name></person-group><article-title>Single-Trial EEG Analysis Predicts Memory Retrieval and Reveals Source-Dependent Differences</article-title><source>Frontiers in Human Neuroscience</source><year>2018</year><volume>12</volume><fpage>258</fpage><pub-id pub-id-type="pmcid">PMC6048228</pub-id><pub-id pub-id-type="pmid">30042664</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00258</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosofsky</surname><given-names>RM</given-names></name><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Cox</surname><given-names>GE</given-names></name><name><surname>Shiffrin</surname><given-names>RM</given-names></name></person-group><article-title>Familiarity and categorization processes in memory search</article-title><source>Cognitive Psychology</source><year>2014</year><volume>75</volume><fpage>97</fpage><lpage>129</lpage><pub-id pub-id-type="pmid">25240209</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosofsky</surname><given-names>RM</given-names></name><name><surname>Cox</surname><given-names>GE</given-names></name><name><surname>Cao</surname><given-names>R</given-names></name><name><surname>Shiffrin</surname><given-names>RM</given-names></name></person-group><article-title>An exemplar-familiarity model predicts short-term and long-term probe recognition across diverse forms of memory search</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><year>2014</year><volume>40</volume><issue>6</issue><fpage>1524</fpage><lpage>1539</lpage><pub-id pub-id-type="pmid">24749963</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olivers</surname><given-names>CNL</given-names></name><name><surname>Peters</surname><given-names>J</given-names></name><name><surname>Houtkamp</surname><given-names>R</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name></person-group><article-title>Different states in visual working memory: When it guides attention and when it does not</article-title><source>Trends in Cognitive Sciences</source><year>2011</year><volume>15</volume><pub-id pub-id-type="pmid">21665518</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ort</surname><given-names>E</given-names></name><name><surname>Olivers</surname><given-names>CN</given-names></name></person-group><article-title>The capacity of multiple-target search</article-title><source>Visual Cognition</source><year>2020</year><volume>28</volume><issue>5-8</issue><fpage>330</fpage><lpage>355</lpage></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname><given-names>J</given-names></name><name><surname>Gray</surname><given-names>JR</given-names></name><name><surname>Simpson</surname><given-names>S</given-names></name><name><surname>MacAskill</surname><given-names>M</given-names></name><name><surname>Höchenberger</surname><given-names>R</given-names></name><name><surname>Sogo</surname><given-names>H</given-names></name><name><surname>Kastman</surname><given-names>E</given-names></name><name><surname>Lindeløv</surname><given-names>JK</given-names></name></person-group><article-title>PsychoPy2: Experiments in behavior made easy</article-title><source>Behavior Research Methods</source><year>2019</year><volume>51</volume><issue>1</issue><fpage>195</fpage><lpage>203</lpage><pub-id pub-id-type="pmcid">PMC6420413</pub-id><pub-id pub-id-type="pmid">30734206</pub-id><pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Kaiser</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Disentangling representations of object shape and object category in human visual cortex: The animate-inanimate distinction</article-title><source>Journal of cognitive neuroscience</source><year>2016</year><volume>28</volume><issue>5</issue><fpage>680</fpage><lpage>692</lpage><pub-id pub-id-type="pmid">26765944</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robinson</surname><given-names>AK</given-names></name><name><surname>Reinhard</surname><given-names>J</given-names></name><name><surname>Mattingley</surname><given-names>JB</given-names></name></person-group><article-title>Olfaction Modulates Early Neural Responses to Matching Visual Objects</article-title><source>Journal of Cognitive Neuroscience</source><year>2015</year><volume>27</volume><issue>4</issue><fpage>832</fpage><lpage>841</lpage><pub-id pub-id-type="pmid">25269111</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Curran</surname><given-names>T</given-names></name></person-group><article-title>Event-related potentials and recognition memory</article-title><source>Trends in Cognitive Sciences</source><year>2007</year><volume>11</volume><issue>6</issue><fpage>251</fpage><lpage>257</lpage><pub-id pub-id-type="pmid">17481940</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rugg</surname><given-names>MD</given-names></name><name><surname>Mark</surname><given-names>RE</given-names></name><name><surname>Walla</surname><given-names>P</given-names></name><name><surname>Schloerscheidt</surname><given-names>AM</given-names></name><name><surname>Birch</surname><given-names>CS</given-names></name><name><surname>Allan</surname><given-names>K</given-names></name></person-group><article-title>Dissociation of the neural correlates of implicit and explicit memory</article-title><source>Nature</source><year>1998</year><volume>392</volume><issue>6676</issue><fpage>595</fpage><lpage>598</lpage><pub-id pub-id-type="pmid">9560154</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sternberg</surname><given-names>S</given-names></name></person-group><article-title>High-Speed Scanning in Human Memory</article-title><source>Science</source><year>1966</year><volume>153</volume><issue>3736</issue><fpage>652</fpage><lpage>654</lpage><pub-id pub-id-type="pmid">5939936</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoletniy</surname><given-names>AS</given-names></name><name><surname>Alekseeva</surname><given-names>DS</given-names></name><name><surname>Babenko</surname><given-names>VV</given-names></name><name><surname>Anokhina</surname><given-names>PV</given-names></name><name><surname>Yavna</surname><given-names>DV</given-names></name></person-group><article-title>The N2pc Component in Studies of Visual Attention</article-title><source>Neuroscience and Behavioral Physiology</source><year>2022</year><volume>52</volume><issue>8</issue><fpage>1299</fpage><lpage>1309</lpage><pub-id pub-id-type="doi">10.1007/s11055-023-01359-y</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thorat</surname><given-names>S</given-names></name><name><surname>Proklova</surname><given-names>D</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The nature of the animacy organization in human ventral temporal cortex</article-title><source>Elife</source><year>2019</year><volume>8</volume><elocation-id>e47142</elocation-id><pub-id pub-id-type="pmcid">PMC6733573</pub-id><pub-id pub-id-type="pmid">31496518</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47142</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Treder</surname><given-names>MS</given-names></name></person-group><article-title>MVPA-Light: A Classification and Regression Toolbox for Multi-Dimensional Data</article-title><source>Frontiers in Neuroscience</source><year>2020</year><volume>14</volume><fpage>289</fpage><pub-id pub-id-type="pmcid">PMC7287158</pub-id><pub-id pub-id-type="pmid">32581662</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2020.00289</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Moorselaar</surname><given-names>D</given-names></name><name><surname>Theeuwes</surname><given-names>J</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name></person-group><article-title>In competition for the attentional template: Can multiple items within visual working memory guide attention?</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2014</year><volume>40</volume><issue>4</issue><fpage>1450</fpage><lpage>1464</lpage><pub-id pub-id-type="pmid">24730738</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>VanRullen</surname><given-names>R</given-names></name><name><surname>Thorpe</surname><given-names>SJ</given-names></name></person-group><article-title>The Time Course of Visual Processing: From Early Perception to Decision-Making</article-title><source>Journal of Cognitive Neuroscience</source><year>2001</year><volume>13</volume><issue>4</issue><fpage>454</fpage><lpage>461</lpage><pub-id pub-id-type="pmid">11388919</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Saved by a Log: How Do Humans Perform Hybrid Visual and Memory Search?</article-title><source>Psychological Science</source><year>2012</year><volume>23</volume><issue>7</issue><fpage>698</fpage><lpage>703</lpage><pub-id pub-id-type="pmcid">PMC3966104</pub-id><pub-id pub-id-type="pmid">22623508</pub-id><pub-id pub-id-type="doi">10.1177/0956797612443968</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolfe</surname><given-names>JM</given-names></name></person-group><article-title>Guided Search 6.0: An updated model of visual search</article-title><source>Psychonomic Bulletin Review</source><year>2021</year><volume>28</volume><issue>4</issue><fpage>1060</fpage><lpage>1092</lpage><pub-id pub-id-type="pmcid">PMC8965574</pub-id><pub-id pub-id-type="pmid">33547630</pub-id><pub-id pub-id-type="doi">10.3758/s13423-020-01859-9</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>R</given-names></name><name><surname>Pruitt</surname><given-names>Z</given-names></name><name><surname>Runkle</surname><given-names>M</given-names></name><name><surname>Scerif</surname><given-names>G</given-names></name><name><surname>Aslin</surname><given-names>RN</given-names></name></person-group><article-title>A neural signature of rapid category-based target selection as a function of intra-item perceptual similarity, despite inter-item dissimilarity</article-title><source>Attention, Perception, Psychophysics</source><year>2016</year><volume>78</volume><issue>3</issue><fpage>749</fpage><lpage>760</lpage><pub-id pub-id-type="pmcid">PMC4811727</pub-id><pub-id pub-id-type="pmid">26732265</pub-id><pub-id pub-id-type="doi">10.3758/s13414-015-1039-6</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>L-C</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>The time course of categorical and perceptual similarity effects in visual search</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2022</year><volume>48</volume><issue>10</issue><fpage>1069</fpage><lpage>1082</lpage><pub-id pub-id-type="pmid">35951407</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname><given-names>L-C</given-names></name><name><surname>Yeh</surname><given-names>Y-Y</given-names></name><name><surname>Kuo</surname><given-names>B-C</given-names></name></person-group><article-title>Spatially Specific Attention Mechanisms Are Sensitive to Competition during Visual Search</article-title><source>Journal of Cognitive Neuroscience</source><year>2019</year><volume>31</volume><issue>8</issue><fpage>1248</fpage><lpage>1259</lpage><pub-id pub-id-type="pmid">31037989</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Luck</surname><given-names>SJ</given-names></name></person-group><article-title>Feature-based attention modulates feedforward visual processing</article-title><source>Nature Neuroscience</source><year>2009</year><volume>12</volume><issue>1</issue><fpage>24</fpage><lpage>25</lpage><pub-id pub-id-type="pmid">19029890</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance statement</title></caption><p>When we are in the supermarket, we repeatedly decide whether a product we look at (e.g., a banana) is on our memorized shopping list (e.g., apples, oranges, kiwis). This requiressearching our memory, which takes time. However, when the product is of an entirely different category (e.g., dairy instead of fruit), the decision can be made quickly. Here, we used EEG to show that this between-category advantage in memory search tasks is supported by top-down attentional modulation of visual processing: The visual response evoked by distractor objects was modulated by category membership, and spatial attention was quickly directed to the location of within-category (vs. between-category) distractors. These results demonstrate a close link between attention and memory.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><p>Schematic illustration of two accounts explaining category effects in memory search. The memory set in this example consists of four animals. (A) On a memory search account, all objects enter the memory search phase after visual object processing is completed. Faster RTs for between-category than within-category distractors are explained by differences in the efficiency of the memory search process. (B) On an attentional modulation account, participants attend to the category of the items in the memory set, modulating visual processing and resulting in the quick rejection of between-category distractors, largely avoiding memory search for these items. The current experiments were aimed at providing evidence for category-level attentional modulation of visual processing.</p></caption><graphic xlink:href="EMS192676-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>Illustration of the three phases in each block of Experiments 1a and b. The example presents an animate block with two targets (i.e., MSS 2).</p></caption><graphic xlink:href="EMS192676-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>Illustration of the procedure of Experiment 2. The example presents an animate block with two targets (i.e., MSS 2). For target-present trials, a target shown with a within-category distractor condition was abbreviated as T-Dw, a target with a between-category distractor as T-Db; for target-absent trials, trials showing one distractor of the target (i.e., within) category together with one distractor of the other (i.e., between) category was abbreviated as Dw-Db, trials where both distractors were within category as Dw-Dw. Finally, trials where both distractors were between category were abbreviated as Db-Db. Our main analyses focused on T-Dw, T-Db, and Dw-Db conditions, as we hypothesized lateralized responses in these conditions.</p></caption><graphic xlink:href="EMS192676-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>Mean RT (in sec) and fitted models in Experiment 1a and b. (A) and (B) show the mean RTs in Experiment 1a and b, respectively. (C) and (D) show the fitted models in Experiment 1a and b, respectively. The light grey dots and dark grey crosses refer to the observed data in within-category and between-category conditions. The dashed lines represent linear models, while the solid lines represent log-linear models. For all the figures, the error bars represent the standard error of the mean; the light blue represents within-category conditions while the dark blue represents between-category conditions.</p></caption><graphic xlink:href="EMS192676-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><p>EEG results of Experiment 1b. (A) The mean amplitude based on 12 electrodes (P5/P6, P7/P8, PO3/PO4, PO7/PO8, PO9/PO10, and O1/O2) averaged across all conditions, illustrating the three ERP components of interest. (B), (C), and (D) compare the mean amplitude of the 12 electrodes in the two category and four MSS conditions.</p></caption><graphic xlink:href="EMS192676-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Cluster-based analyses of Experiment 1b. (A) Main effect of category. The light blue solid line represents within-category condition while the dark blue dashed line represents between-category condition. (B)Main effect of MSS. The blue color from dark to light refers to set size 1, 2, 4 and 8, respectively. The grey bars indicate the time windows of significant main effects based on a cluster permutation test.</p></caption><graphic xlink:href="EMS192676-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><p>Cross-task decoding in Experiment 1b. (A) AUC as a function of category condition. The light blue solid line represents within-category condition while the dark blue dashed line represents between-category condition. (B) AUC as a function of MSS. The blue color from dark to light refers to set size 1, 2, 4 and 8, respectively. (C), (D), and (E) compare the mean AUC in the two categories and four MSS conditions, averaged across the time points of each component (P1, N1, P2). The main effect of category was significant for the N1 time window (panel D).</p></caption><graphic xlink:href="EMS192676-f007"/></fig><fig id="F8" position="float"><label>Figure 8</label><caption><p>Mean RT (in sec) in Experiment 2. (A) RTs in target-present trials. The red bars represent the within-category condition (T-Dw; target shown next to a within-category distractor) while the blue bars represent the between-category condition (T-Db; target shown next to a between-category distractor). (B) RTs in target-absent trials. The grey, yellow, and green bars represent Dw-Db, Dw-Dw, and Db-Db trials.</p></caption><graphic xlink:href="EMS192676-f008"/></fig><fig id="F9" position="float"><label>Figure 9</label><caption><p>Main effect of visual field location, showing a target related N2pc. (A) Main effect of contra/ipsi-lateral visual field. (B) Comparison of the contra/ipsi-lateral amplitude averaged across the N2pc time window (200-299 msec).</p></caption><graphic xlink:href="EMS192676-f009"/></fig><fig id="F10" position="float"><label>Figure 10</label><caption><p>ERP results of Experiment 2. (A) Comparison of the mean N2pc amplitude in the two categories and four MSS conditions. (B) Main effect of MSS in a cluster analysis. (C) Main effect of category in a cluster analysis. (D) Interaction of MSS and category in a cluster analysis. Different colors from dark to light refer to MSS 1 to 8. T-Dw = a target shown next to a within-category distractor, T-Db = a target shown next to a between-category distractor.</p></caption><graphic xlink:href="EMS192676-f010"/></fig><fig id="F11" position="float"><label>Figure 11</label><caption><p>Main effect of visual field location for the Dw-Db condition (target-absent trials). In this analysis, contralateral is relative to the location of the within-category distractor (Dw). (A) Main effect of contra/ipsi-lateral visual field based on a cluster permutation test (cluster-based <italic>p</italic> = 0.001). (B) Comparison of the contra/ipsi-lateral amplitude averaged across the N2pc time window (200-299 msec).</p></caption><graphic xlink:href="EMS192676-f011"/></fig></floats-group></article>