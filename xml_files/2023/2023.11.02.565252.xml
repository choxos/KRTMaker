<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190609</article-id><article-id pub-id-type="doi">10.1101/2023.11.02.565252</article-id><article-id pub-id-type="archive">PPR753221</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>ReptiLearn: A Smart Home Cage for Behavioral Experiments in Reptiles</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Eisenberg</surname><given-names>Tal</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Shein-Idelson</surname><given-names>Mark</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><aff id="A1"><label>1</label>School of Neurobiology, Biochemistry, and Biophysics, Tel Aviv University, Israel</aff><aff id="A2"><label>2</label>Sagol School of Neuroscience, Tel Aviv University, Israel</aff></contrib-group><author-notes><corresp id="CR1">
<label>*</label>Corresponding Author - <email>sheinmark@tauex.tau.ac.il</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>05</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>03</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Understanding behavior and its evolutionary underpinnings is crucial for unraveling the complexities of brain function. Traditional approaches strive to reduce behavioral complexity by designing short-term, highly constrained behavioral tasks with dichotomous choices in which animals respond to defined external perturbation. In contrast, natural behaviors evolve over multiple time scales and under minimally constrained conditions in which actions are selected through bi-directional interactions with the environment and without human intervention. Recent technological advancements have opened up new possibilities for more natural experimental designs by replacing stringent experimental control with accurate multidimensional behavioral analysis. However, these approaches have been tailored to fit only a small number of species. This specificity limits the experimental opportunities offered by species diversity. Further, it hampers comparative analyses which are essential for extracting overarching behavioral principles and for examining behavior from an evolutionary perspective. To address this limitation, we developed ReptiLearn - a versatile, low-cost, Python-based solution, optimized for conducting automated long-term experiments in the home cage of reptiles, without human intervention. In addition, this system offers unique features such as precise temperature measurement and control, live prey reward dispensers, engagement with touch screens, and remote control through a user-friendly web interface. Finally, ReptiLearn incorporates low-latency closed-loop feedback allowing bi-directional interactions between animals and their environments. Thus, ReptiLearn provides a comprehensive solution for researchers studying behavior in ectotherms and beyond, bridging the gap between constrained laboratory settings and natural behavior in non-conventional model systems. We demonstrate the capabilities of ReptiLearn by automatically training the lizard <italic>Pogona vitticeps</italic> on a complex spatial learning task requiring association learning, displaced reward learning and reversal learning.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Nervous systems evolved to facilitate behaviors enhancing survivability and reproduction (<xref ref-type="bibr" rid="R1">1</xref>). Understanding these behaviors and their evolutionary origins is crucial for unraveling the complexities of nervous systems and the computational processes they support (<xref ref-type="bibr" rid="R2">2</xref>). This endeavor is carried out within several frameworks. Behavioral ecology aims to capture the full range of natural behaviors in the wild. Psychology, in contrast, is more focused on developing highly constrained behavioral experiments (<xref ref-type="bibr" rid="R3">3</xref>) using animals as models for elucidating specific aspects of human behavior (<xref ref-type="bibr" rid="R4">4</xref>). Neuroethology combines approaches from both disciplines to study natural behaviors across a wide diversity of species from which general behavioral principles and their neurophysiological underpinnings can be extracted (<xref ref-type="bibr" rid="R5">5</xref>). Correspondingly, neuroethologists constructed experimental setups striving to approach conditions in the wild (<xref ref-type="bibr" rid="R6">6</xref>) in which animals engage in a bi-directional interaction with their environments (<xref ref-type="bibr" rid="R7">7</xref>) across multiple time scales (<xref ref-type="bibr" rid="R8">8</xref>). Psychologists, on the other hand, mostly utilized reductionist experimental designs within laboratories (<xref ref-type="bibr" rid="R9">9</xref>), allowing them to simplify behavioral complexities and concentrate on particular behavioral aspects (<xref ref-type="bibr" rid="R10">10</xref>). This approach offers quantifiability, more repeated trials, reduced variability within and between animals, and increased statistical power (<xref ref-type="bibr" rid="R3">3</xref>). However, these benefits come at the expense of capturing only a limited subset of an animal’s behavioral repertoire that is confined to a specific time scale. Further, this approach may introduce biases due to human handling and the use of unnatural tasks that lack the bi-directional interaction between animals and their environment as observed under natural conditions.</p><p id="P3">Recent technological advances offer a new opportunity to study natural behaviors in the lab (<xref ref-type="bibr" rid="R2">2</xref>). Miniaturization of cameras, automation of natural stimuli and reward delivery, and powerful computational tools now enable researchers to create autonomous environments that more closely resemble natural conditions (<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R16">16</xref>) or home cage conditions (<xref ref-type="bibr" rid="R17">17</xref>). New signal processing methods facilitate automatic annotation and analysis of the vast amounts of behavioral data collected in each experiment, resulting in a broad range of extracted behavioral features and improved statistical power (<xref ref-type="bibr" rid="R18">18</xref>). These rich data sets offer a window into the variability inherent in animal behavior (<xref ref-type="bibr" rid="R5">5</xref>). Importantly, many of these solutions are provided as open-source low-cost hardware and software packages, making them accessible to a wide range of research labs (<xref ref-type="bibr" rid="R19">19</xref>).</p><p id="P4">While the behavioral approaches above chart a promising path forward, they do not offer a comprehensive solution for many experimental scenarios. Primarily, they have been tailored to a limited number of species. Specifically, since the 1980’s, a handful of genetically tractable model systems began to increasingly dominate scientific studies (<xref ref-type="bibr" rid="R20">20</xref>–<xref ref-type="bibr" rid="R22">22</xref>). Correspondingly, automated home-cage monitoring and behavioral setups have been developed primarily for mice (<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R17">17</xref>,<xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R24">24</xref>), fruit flies (<xref ref-type="bibr" rid="R25">25</xref>) and zebrafish (<xref ref-type="bibr" rid="R26">26</xref>), and offer species-specific behavioral paradigms (<xref ref-type="bibr" rid="R27">27</xref>). This specificity extends to the devices used for interacting and rewarding the animals. For example, experimental systems usually lack temperature control and their automatic food dispensers are optimized for delivering dry food pellets rather than live insects. These properties limit applicability to non-conventional animal models such as species from the reptilian and amphibian classes.</p><p id="P5">These classes are large and diverse with many species offering unique perspectives on various biological and evolutionary research questions (<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R28">28</xref>–<xref ref-type="bibr" rid="R31">31</xref>). In recent years, research on these classes has gained momentum (<xref ref-type="bibr" rid="R32">32</xref>–<xref ref-type="bibr" rid="R34">34</xref>), aided by new genetic methodologies for probing (<xref ref-type="bibr" rid="R35">35</xref>) and manipulating (<xref ref-type="bibr" rid="R36">36</xref>) gene expression (<xref ref-type="bibr" rid="R37">37</xref>,<xref ref-type="bibr" rid="R38">38</xref>). In contrast to these advancements, implementation of new methodologies for studying behavior is limited. Correspondingly, the cognitive capacities of reptiles and amphibians remain poorly understood and research linking behavior with neurophysiology is scarce (<xref ref-type="bibr" rid="R39">39</xref>). This deficiency can be attributed in part to the challenges posed by studying ectothermic vertebrates.</p><p id="P6">Ectothermic behavior is highly dependent on environmental temperatures and heat sources (<xref ref-type="bibr" rid="R40">40</xref>). Thus, reducing unexplained behavioral variability necessitates continuous monitoring and control of thermal conditions during experiments. Furthermore, ectothermy favors survival strategies with low energy consumption which manifests in increased immobility (<xref ref-type="bibr" rid="R41">41</xref>). Such behaviors yield sparser behavioral data and require continuous long-term experiments with a sufficient sample size for robust statistical analysis. In contrast, most experimental approaches are not suited for continuous experiments over weeks and lack paradigms for bi-directional interactions with animals during such long periods. Another challenge for behavioral research in ectothermic vertebrates is reinforcement using food rewards. These animals can often go without eating and drinking for extended periods compared to mammals (<xref ref-type="bibr" rid="R42">42</xref>,<xref ref-type="bibr" rid="R43">43</xref>), potentially reducing their motivation and engagement in behavioral tasks. Moreover, most automated systems employed in mammalian research make use of liquid rewards (<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref>), which are unsuitable for species requiring live prey as a reward. Conversely, reinforcers such as heat can be utilized but are not included in automated behavioral systems. Thus, human intervention is required during experiments, making it more difficult to increase sample sizes and reduce variability.</p><p id="P7">To address these issues, we developed ReptiLearn - a new comprehensive solution for behavioral experiments in reptiles (<xref ref-type="fig" rid="F1">Figure 1a</xref>). This platform includes unique hardware components such as a fine-grained temperature control and measurement apparatus, automated live feed reward dispensers, a touch screen for providing visual stimulation and logging touch choices, and modules for interacting with Arduino components (<xref ref-type="fig" rid="F1">Figure 1b</xref>). ReptiLearn is ideally suited for continuous experiments over extended time scales with the arena serving as the animal’s home cage for days to weeks. Arena components are controlled by a dedicated software suite allowing flexible design of fully automated experiments and extraction of behavioral features (<xref ref-type="fig" rid="F1">Figure 1b</xref>). These experiments can be controlled remotely by a web-based user interface for increased accessibility (<xref ref-type="fig" rid="F1">Figure 1d</xref>). Finally, to facilitate automated bi-directional interactions of animals with their environment, ReptiLearn incorporates low-latency components that allow closing a loop between behavioral dynamics and arena hardware.</p></sec><sec id="S2" sec-type="results"><title>Results</title><p id="P8">We first set out to test the general performance of ReptiLearn and then demonstrate its capabilities by training <italic>Pogona vitticeps</italic> lizards on a spatial learning task. Locomotion and posture changes are basic behaviors providing valuable information on behavioral states and strategies (<xref ref-type="bibr" rid="R44">44</xref>). Further, in nature, these behaviors are strongly linked to changes in the animal environment. For example, movement can trigger a predator attack or a prey escape response (<xref ref-type="bibr" rid="R45">45</xref>). To incorporate such interactions, we implemented real-time feedback between animal movements and arena apparatus (e.g., delivery of food or visual stimuli) (<xref ref-type="fig" rid="F1">Figure 1b, c</xref>). To track movement in video streams, we integrated into ReptiLearn two video processing pipelines. In the first, our aim was to optimize processing speed in real-time experiments. To do so, we fine-tuned and trained YOLOv4 - a light-weight convolutional neural network - to detect the head bounding box (<xref ref-type="fig" rid="F2">Figure 2a</xref>; Bochkovskiy et al., 2020) which provides information about the lizard’s position and head movements (<xref ref-type="fig" rid="F2">Figure 2a</xref>). To test the position estimation processing speed, we measured, for each frame, the time elapsed until an output bounding box was generated. The mean latency was 7.92ms (SD=0.36ms, <xref ref-type="fig" rid="F2">Figure 2b</xref>) when using a consumer-grade GPU (NVIDIA RTX 3080 TI), which is low enough for allowing real-time tracking of every frame in streams of up to 125Hz. To estimate the total latency in closed-loop experiments (including image acquisition, arena controller and feedback circuit), we measured the time between turning on a LED in the arena and its detection in the video stream. This delay added 43ms (SD=9.34ms) to processing time (<xref ref-type="fig" rid="F2">Figure 2c</xref>), resulting in a total of 50.92ms (SD=9.35ms) for location based closed-loop feedback.</p><p id="P9">YOLOv4 offers a good tradeoff between accuracy (<xref ref-type="fig" rid="F2">Figure 2d, e</xref>) and computation time (<xref ref-type="fig" rid="F2">Figure 2b</xref>). Using a test set of 400 randomly selected images (from 4 animals) and a confidence threshold of 80%, the model achieved a recall of 100% (no false positives), and a precision (true positive / total positive) of 78% (<xref ref-type="fig" rid="F2">Figure 2d</xref>). Out of the true positive detections, the average intersection-over-union metric (IoU) between the predicted and manually annotated bounding boxes was 73% (<xref ref-type="fig" rid="F2">Figure 2e</xref>) indicating a good accuracy. While the above approach provides a fast but minimal solution for real-time feedback, it could be easily replaced with other models if computational time constraints are loosened (<xref ref-type="bibr" rid="R18">18</xref>,<xref ref-type="bibr" rid="R47">47</xref>). To acquire richer position and posture information, we used a second approach in which we fed the head bounding box calculated by YOLO to an object segmentation model (Segment Anything Model; Kirillov et al., 2023). This model provides a mask of the entire animal body. The processing time of this model is much longer, making it less practical for low-latency real-time applications. Its advantage, however, is that it does not require additional training or manual annotation and can provide rich data about the animal’s behavior (as later shown).</p><p id="P10">Long-term automated experiments require providing the necessary conditions for survival. Additionally, the controlled delivery of these conditions can serve as reinforcers, playing a crucial role in behavioral experiments. Reptiles and amphibians require ecologically relevant reinforcers such as live food and heat (<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R50">50</xref>). To integrate automatic delivery of live food, we modified inexpensive aquarium fish feeders (<xref ref-type="sec" rid="S4">Methods</xref>). By adding a simple control circuit, we achieved a short mean latency of 3.21s (SD=0.54s) for live food delivery. This was measured by calculating the time difference from sending a command to the worm dispenser until the dispensed worm was detected hitting the arena floor in the video stream (<xref ref-type="fig" rid="F2">Figure 2f</xref>). Feeders were filled with worms (e.g., <italic>Tenebrio molitor</italic> larva) each kept in a small compartment with food. These feeders can be stacked vertically to increase the number of rewards (up to 30 worms in two stacked feeders) and require no manual intervention except loading every few days.</p><p id="P11">The ability to spatiotemporally control arena temperatures is important for experiments with ectotherms (<xref ref-type="bibr" rid="R49">49</xref>). To achieve this, we integrated a grid of infra-red heat lamps (<xref ref-type="fig" rid="F1">Figure 1b</xref>) that were activated by a relay module and controlled using an Arduino board (<xref ref-type="sec" rid="S4">Methods</xref>). The heat lamp coverage provides fine-grained control over the arena’s thermal gradient (<xref ref-type="fig" rid="F3">Figure 3a</xref>) thus allowing to test temperature preference and thermal regulation under flexible spatial configurations. In addition, heat lamps, in contrast to ceramic heating elements, can be quickly turned on and off to provide instantaneous reward without the need for manual refilling, as in the case of live insect rewards (<xref ref-type="bibr" rid="R51">51</xref>). We estimated the heating dynamics using a digital temperature sensor (<xref ref-type="fig" rid="F3">Figure 3b</xref>). A detectable increase of 1°C on the arena floor is observed within 18.11 seconds (SD=1.7s) of turning the heat source on. While temperature change sensitivity in <italic>Pogona vitticeps</italic> is unknown, reptiles are equipped with molecular machinery for sensing temperature changes smaller than 1°C (<xref ref-type="bibr" rid="R52">52</xref>), presumably allowing much faster heat reward detection.</p><p id="P12">Integrating temperature control with body temperature measurements can open up many possibilities for studying temperature dependent behavioral features such as thermal regulation (<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R53">53</xref>). Doing so requires continuous measurement of body temperature during long experiments. While measuring core body temperature requires internal probes, skin temperatures are easily measurable using a thermal camera (<xref ref-type="bibr" rid="R54">54</xref>) and were used to accurately estimate core temperatures in <italic>Pogona barbata</italic> (<xref ref-type="bibr" rid="R55">55</xref>) and additional lizards of similar size (<xref ref-type="bibr" rid="R56">56</xref>,<xref ref-type="bibr" rid="R57">57</xref>). To measure skin temperature using an infra-red camera, we first generated a segmentation mask of the lizard’s body position using SAM on the regular video camera stream (as described above; <xref ref-type="fig" rid="F3">Figure 3c</xref>). We then transformed arena coordinates from the visual camera to the thermal camera (<xref ref-type="sec" rid="S4">Methods</xref>) and estimated body temperature by calculating the median pixel temperature across the mask. We validated the accuracy of the transformed body mask by showing a consistent temperature drop between the lizard’s body to its surrounding over thermal video frames (<xref ref-type="sec" rid="S4">Methods</xref>; Figure S1).</p><p id="P13">Combining measurements of animal temperature together with its dynamics can shed light on thermo-regulation strategies. <xref ref-type="fig" rid="F3">Figure 3d</xref> shows temperature dynamics (<xref ref-type="fig" rid="F3">Figure 3d</xref>, red curve) during a day of measurement together with travel speed (<xref ref-type="fig" rid="F3">Figure 3d</xref>, blue curve), calculated by integrating position changes over time. Periods of basking were detected (<xref ref-type="fig" rid="F3">Figure 3d</xref>, gray shade) when lizards entered a circle of ~40cm diameter under the heat lamp. This analysis shows that the lizard spent a large fraction of its time stationary. During most of this time, the lizard was under the heat lamp and increased its temperature. Occasionally, it was stationary in cold spots and decreased its temperature (<xref ref-type="fig" rid="F3">Figure 3d</xref>, white areas). Between these stationary periods, the lizard exhibited short bouts of activity (<xref ref-type="fig" rid="F3">figure 3d</xref>, blue traces). This dynamic allowed the lizard to maintain an average temperature of 36.0°C (SD=2.3°C) which is considerably different from the ambient temperature (28.2, SD=0.5°C), in general accordance with previous studies (<xref ref-type="bibr" rid="R57">57</xref>,<xref ref-type="bibr" rid="R53">53</xref>). Interestingly, while in some instances basking was observed following food reward (<xref ref-type="fig" rid="F3">Figure 3d</xref>, green lines), this was not the general case hinting that the decision to bask after feeding is not always prioritized.</p><p id="P14">ReptiLearn also includes hardware and software for interacting with reptiles visually and via touch (<xref ref-type="bibr" rid="R58">58</xref>). To that aim we integrated a touch screen into the experimental setup (<xref ref-type="fig" rid="F1">Figure 1b</xref>). Our software includes a web application that can be used to display and animate custom stimuli on any number of screens. In addition, animal screen touches are registered and relayed back to the system for real-time feedback to the displayed stimuli, or other triggerable arena components. Together, ReptiLearn integrates multiple methodologies for interacting with animals and for delivering rewards with a low latency feedback, which are instrumental for effective learning and conditioning paradigms (<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R59">59</xref>–<xref ref-type="bibr" rid="R61">61</xref>).</p><p id="P15">To demonstrate the capabilities of the system we trained lizards on a spatial task requiring association learning, displaced reward learning and reversal learning. Lizards were placed in a ReptiLearn arena, which served as their home-cage for the duration of the experiment (lasting two to three weeks). The arena contained a shelter, a water dish, and a heated basking area. The experiment consisted of three blocks. In the first block, lizards were required to enter the feeder dish area in order to receive live food reward from the automatic feeder in the rewarded location (<xref ref-type="fig" rid="F4">Figure 4a</xref>). Specifically, upon entering the reward location and staying there for 2s (wait period), the LED in the feeder location started to blink (<xref ref-type="fig" rid="F4">Figure 4a</xref>) and 5 seconds later, a command to dispense the reward was sent. This block served to habituate the animal to the arena and to associate entering the dish location with the light blink and the food reward. To separate reward sessions, we implemented a cooldown period of 20 seconds (starting from LED blink start) during which no rewards can be obtained. In addition, to prevent frequent rewards without new action, animals had to exit the reward area (gray area outside the reward circle in <xref ref-type="fig" rid="F4">figure 4a</xref>) before entering it again to receive another reward.</p><p id="P16">In the second block, we repeated the same reward protocol but changed the reinforced area to the top right corner (<xref ref-type="fig" rid="F4">Figure 4b</xref>) without changing the location of the reward. This block served to examine whether lizards can associate their action (entering a specific location), with an outcome (reward) in another location (displaced reward). In the third block we examined if lizards could reverse their learning by changing the reward location to the upper left corner (<xref ref-type="fig" rid="F4">Figure 4b</xref>). The transition between blocks was automatic after 60 rewards were received, or after six days, whichever occurred first.</p><p id="P17">Lizards successfully entered the rewarded areas in each block and received rewards, all without any guidance or interaction with humans throughout the entire experiment. The reward rate was not homogenous over time (<xref ref-type="fig" rid="F4">Figure 4c</xref>). Reward rate was low at the beginning of the day and increased during the afternoon. However, these dynamics were lizard specific with different lizards showing increased reward rates during different times (Figure S2). To assess whether animals learned the task, or entered the rewarded areas by chance, we posited that significant change in the entry rate to a reinforced area implies an association between the area and the reward. We therefore tracked the entry rate to the reinforced area and to all other areas (uniformly distributed on a grid with centers marked by rectangles in <xref ref-type="fig" rid="F4">Figure 4d</xref>) and calculated the differences in entry rate (ΔER) between consecutive blocks (<xref ref-type="fig" rid="F4">Figure 4d, e</xref>). The first transition in reward location (block 1→2) was accompanied by an increased ΔER to the reinforced area (<xref ref-type="fig" rid="F4">Figure 4d</xref>, orange circle). An increase in ΔER was also observed for the reward location (feeder area). This increase is expected since the lizard continues to receive the mealworms in the feeder location. To quantify the changes in the lizard’s behavior, we plotted ΔER for each area as a function of the distance from the reinforced area and conducted a regression analysis (<xref ref-type="fig" rid="F4">Figure 4g</xref>). In this analysis, we exclude the area around the feeder since the animals had to enter this area to receive rewards (<xref ref-type="fig" rid="F4">Figure 4g</xref>-green dots). We expected an increase in ΔER to areas closer to the reinforced area. Such an increase was evident (<xref ref-type="fig" rid="F4">Figure 4g</xref>, p-value &lt; 0.001). Repeating this experiment in additional animals showed a similar and significant decrease in ΔER with distance (<xref ref-type="fig" rid="F4">Figure 4i</xref>). These results indicate that <italic>Pogona</italic> can learn to associate one location in the arena with an outcome in a different location. We next examined if the lizards could perform a reversal learning task. After the lizards learned the first location, we shifted the reward location. Following the switch, we observed an increased ΔER in the new reinforced location (<xref ref-type="fig" rid="F4">Figure 4e</xref>, blue circle). Furthermore, we observed a decrease in the previous reward location (<xref ref-type="fig" rid="F4">Figure 4e</xref>, orange circle). This result corresponded with a significant (p-value&lt;0.001) spatial decay of ΔER with distance from the new reinforced location (<xref ref-type="fig" rid="F4">Figure 4h</xref>). Correspondingly, examining all movement trajectories on the last day of training (time was segmented from position data into stationary and non-stationary using Kleinberg’s burst detection algorithm; Kleinberg, 2002; see <xref ref-type="sec" rid="S4">Methods</xref>) revealed that lizards were engaged in stereotypical movement paths between the feeder and the rewarded location (<xref ref-type="fig" rid="F4">Figure 4f</xref>). Successful learning in the reversal task was significant across animals (<xref ref-type="fig" rid="F4">Figure 4i</xref>) with one animal failing to learn the reversal (see Supplementary Table 1 for U values and statistical significance for all animals and blocks). However, this animal showed a strong decrease in movement in the 3<sup>rd</sup> block which may explain its insignificant learning. Thus, using ReptiLearn we were able to successfully train lizards on a complex spatial task without human intervention.</p></sec><sec id="S3" sec-type="discussion"><title>Discussion</title><p id="P18">In this manuscript we introduce ReptiLearn - a versatile, low-cost, open-source experimental arena for behavioral experiments in reptiles (<xref ref-type="fig" rid="F1">Figure 1a</xref>). As far as we are aware, this is the first comprehensive automated solution for behavioral investigations in reptiles. It effectively addresses numerous challenges inherent to behavioral studies in reptiles, amphibians and beyond. Specifically, ReptiLearn facilitates precise control and monitoring of arena and animal temperature. It incorporates a specialized feeder for delivery of live prey over long periods. In addition, the system is fully automated and offers a wide range of classes for controlling hardware components, which can be remotely controlled and monitored through a user-friendly web-interface. Finally, ReptiLearn operates in real-time and permits the flexible coupling of arena components to design diverse experiments (<xref ref-type="fig" rid="F1">Figure 1c</xref>). The ReptiLearn code, and a user-friendly installation procedure complemented by tutorials, is accessible at <ext-link ext-link-type="uri" xlink:href="https://github.com/EvolutionaryNeuralCodingLab/reptiLearn">https://github.com/EvolutionaryNeuralCodingLab/reptiLearn</ext-link>.</p><p id="P19">While primarily designed with reptiles in mind, ReptiLearn offers innovative solutions that can be applied to experiments involving other animal models. The low-cost feeder, along with its control circuit and software, can prove valuable for conditioning species reliant on live prey, such as shrews (<xref ref-type="bibr" rid="R63">63</xref>) and insectivorous birds (<xref ref-type="bibr" rid="R64">64</xref>). The web-based user interface (<xref ref-type="fig" rid="F1">Figure 1d</xref>) is ideal for continuous, long-term experiments as it simplifies remote monitoring of multiple camera feeds and can be accessed easily via mobile devices like smartphones and tablets. Additionally, the highly parallelized image processing pipeline can efficiently scale with the number of CPU cores and GPUs used, allowing for tracking animals across multiple cameras, forming the basis for 3D tracking solutions. This can be particularly useful for combining image analysis data from different devices, for example, for estimating real-time skin temperature and employing it as an input for closed-loop feedback (<xref ref-type="fig" rid="F3">Figure 3</xref>). Combining SAM with object detection models, such as YOLO, for generating animal body segmentation masks presents a promising new approach that can save many hours of manual annotation and be used for analyzing animal behavior dynamics.</p><p id="P20">The fine-grained control and monitoring of both the arena’s and animals’ temperatures removes barriers when studying thermoregulation in ectotherms (<xref ref-type="bibr" rid="R54">54</xref>) and endotherms (<xref ref-type="bibr" rid="R65">65</xref>,<xref ref-type="bibr" rid="R66">66</xref>). By utilizing advanced tools for identifying animals in video streams and automatically registering them to images captured by thermal cameras, we were able to track animal temperature continuously. This approach offers distinct advantages over traditional methods. Such solutions require surgical procedures for implanting temperature probes (<xref ref-type="bibr" rid="R56">56</xref>,<xref ref-type="bibr" rid="R67">67</xref>) and telemeters (<xref ref-type="bibr" rid="R53">53</xref>), the telemetry location information has a lower spatial resolution relative to video, and there is no access to posture information. Further, our approach enables placing thermoregulation within a wider behavioral and neurophysiological context (<xref ref-type="bibr" rid="R2">2</xref>): First, skin temperature information can be measured for any task the animal performs and can be combined with shuttling boxes when accurate linear gradients are required (<xref ref-type="bibr" rid="R68">68</xref>). Second, tracking and manipulating food rewards allows incorporating metabolic considerations into experiments. Third, the arena is compatible with neurophysiological measurements allowing to link thermoregulation with brain activity (<xref ref-type="bibr" rid="R69">69</xref>). Finally, the ability to dynamically alter thermal conditions using real-time feedback opens new avenues of thermal regulation research and significantly enhances the system’s flexibility.</p><p id="P21">ReptiLearn is well suited for investigating short-range spatial cognition (<xref ref-type="bibr" rid="R70">70</xref>). We demonstrated its efficacy in a complex spatial task involving <italic>Pogona vitticeps</italic> and encompassing association learning, displaced reward learning and reversal learning. In addition, we demonstrate that reptiles can use free-exploration without any guidance or human feedback to learn this task and flexibly associate specific positions with a reward in another location. Our results align with prior studies describing reptile spatial learning abilities (<xref ref-type="bibr" rid="R71">71</xref>–<xref ref-type="bibr" rid="R75">75</xref>). Notably, our innovative approach departs from previous spatial essays that necessitated performing an extensive number of trials, each lasting up to tens of minutes (<xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R74">74</xref>), until performance criteria were met (<xref ref-type="bibr" rid="R76">76</xref>). These experiments were usually conducted manually, demanding substantial efforts from experimenters, such as baiting food rewards in each trial (<xref ref-type="bibr" rid="R73">73</xref>) or repositioning of animals (<xref ref-type="bibr" rid="R74">74</xref>). In contrast, our paradigm allows unrestricted animal movement throughout the entire experiment eliminating possible biases introduced by human handling. Further, our automated long-term recording approach is ideally suited for ectotherms with low metabolic rates and behaviors that likely extend over long timescales. Another added benefit of this approach is collecting large statistical datasets. By harnessing continuous position tracking (<xref ref-type="fig" rid="F4">Figure 4f</xref>), temperature manipulation and monitoring (<xref ref-type="fig" rid="F3">Figure 3</xref>), visual stimulation (<xref ref-type="fig" rid="F1">Figure 1b</xref>), and real-time feedback (<xref ref-type="fig" rid="F1">Figure 1c</xref>, <xref ref-type="fig" rid="F2">2</xref>), ReptiLearn allows expanding the range of questions studied in reptile spatial cognition.</p><p id="P22">Despite pioneering work in the field (<xref ref-type="bibr" rid="R59">59</xref>,<xref ref-type="bibr" rid="R74">74</xref>,<xref ref-type="bibr" rid="R77">77</xref>,<xref ref-type="bibr" rid="R78">78</xref>), our understanding of the cognitive abilities of reptiles, and cold blooded animals in general, remains limited. Such research can shed light on the evolution of cognition and provide a comparative perspective critical for generalized understanding (<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R30">30</xref>–<xref ref-type="bibr" rid="R34">34</xref>). However, these efforts have been constrained by the lack of modern behavioral and neurophysiological tools suited for cold-blooded species. ReptiLearn substantially enhances the toolbox available to behavioral researchers studying cold-blooded animals and serves as a proof of concept for an automated, minimally-intrusive approach to exploring reptile cognition.</p></sec><sec id="S4" sec-type="methods"><title>Methods</title><sec id="S5"><title>Animals</title><p id="P23">Four <italic>Pogona vitticeps</italic> lizards participated in the experiment. Two adults - a male (animal 1, 186g), and a female (animal 2, 231g), and two juveniles - male (animal 3, 121g), female (animal 4, 117g). Lizards were purchased from local dealers and housed in an animal house at Tel Aviv University’s zoological gardens. Lizards were kept in a 12–12 h light (07:00–19:00) and dark cycle and a room temperature of 24 °C. All experiments were approved by Tel Aviv University ethical committee (Approval number: 04-21-055).</p></sec><sec id="S6"><title>ReptiLearn software</title><sec id="S7"><title>General design</title><p id="P24">The ReptiLearn software, written in Python, provides a toolkit for automating closed-loop behavioral tasks, collecting behavioral data, and extracting basic behavioral features. It includes a customizable real-time image processing pipeline that can be used to process and record synchronized video data from multiple cameras or other image sources. An arena controller program provides a generic way to integrate custom hardware components into the system without writing code by communicating with any number of Arduino microcontroller boards (Figure S3). Controlling and monitoring the system can be done remotely through a web-based user interface (Web UI; <xref ref-type="fig" rid="F1">Figure 1d</xref>). Users can implement new automated experiments by writing Python scripts and linking them to experiment sessions. These scripts can automate any part of the system based on real-time information gathered from arena sources. Non-programmers can customize existing scripts by modifying session parameters through the Web UI. A scheme of blocks and trials makes it possible to design complex experimental sequences. The software can run on a wide range of operating systems thanks to Python’s cross-platform support. It was tested on Ubuntu 20.04, Ubuntu 22.04, and recent versions of Microsoft Windows and macOS.</p><p id="P25">To overcome limitations in Python’s concurrency model, the software makes extensive use of separate OS processes (Figure S3). These processes are synchronized using a central state store, which holds the current state of all system components in one place. The Web UI and other external applications can receive updates whenever the state data changes by making a WebSocket connection to the system HTTP server. The server also provides an API to control the system remotely, and the MQTT protocol is supported for communicating with external devices and software (described below).</p><p id="P26">The software is licensed under the open-source GPL-3.0 license. Source code, detailed installation instructions, and guides for running and adapting the system are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/EvolutionaryNeuralCodingLab/reptiLearn">https://github.com/EvolutionaryNeuralCodingLab/reptiLearn</ext-link>.</p></sec><sec id="S8"><title>Web-based user interface</title><p id="P27">The Web UI is implemented as a separate JavaScript application using the React framework (code is available at /ui in the GitHub repository). After an initial build process (described in the <ext-link ext-link-type="uri" xlink:href="https://github.com/EvolutionaryNeuralCodingLab/reptiLearn/blob/master/docs/getting_started.md">“Getting Started”</ext-link> guide), the system HTTP server can be used to access it from any device that includes a modern web browser (Figure S4). It provides live video streaming from multiple sources and communicates with all other system components through a WebSocket connection and an HTTP API. The various features of the interface are described below in relevant sections.</p></sec><sec id="S9"><title>Video system</title><p id="P28">Image data from multiple sources can be acquired using the system for real-time processing and offline analysis (Figure S5). ImageSource classes collect raw data from cameras or other imaging devices and make it available for further processing. Support for FLIR cameras and potentially other GenICam cameras is provided by the FLIRImageSource class using the Spinnaker SDK Python bindings. Allied Vision cameras are also supported through the AlliedVisionImageSource class, which utilizes the Vimba SDK. Additionally, video files can be used as sources for simulation and debugging purposes using the VideoImageSource class. This class also supports capturing images from standard webcams. Additional sources can be supported by writing new ImageSource classes and storing them in Python modules (inside the /system/image_sources folder).</p><p id="P29">ImageObserver classes provide an interface for further processing image data acquired by ImageSource classes. Each ImageObserver is attached to an ImageSource and is notified whenever a new image is acquired. Similarly to image sources, observer classes found inside the /system/image_observers folder are automatically recognized. The repository includes a YOLOv4ImageObserver class that performs object detection and can generate bounding boxes for objects or animals in the arena (the YOLO model weights are not included; see <ext-link ext-link-type="uri" xlink:href="https://github.com/EvolutionaryNeuralCodingLab/reptiLearn/blob/master/docs/getting_started.md">“Getting Started”</ext-link> guide). Additional simple observer classes are included as examples for implementing new processing algorithms. Multiple ImageObserver classes can be attached to the same ImageSource, and an additional VideoWriter observer is attached by default to each source for recording and encoding video. Both sources and observers can be added and configured using the Web UI “Video Settings” window (Figure S4a). When adding new sources and observers, all available classes found in their respective folders are listed, and users can modify classes and reload them without needing to restart the software.</p><p id="P30">To improve performance and utilize multiple CPU cores or GPUs, each ImageSource and ImageObserver instance runs in a separate OS process and communicates with other components through shared memory buffers to avoid expensive data copying (Figure S5). There is no limit to the number of sources and observers that can be used concurrently. Using this architecture, data from each image source can be simultaneously used for encoding video, streaming video over HTTP, and real-time processing by multiple algorithms with minimal latency. Observer output can be accessed from the experiment script using a provided API.</p><p id="P31">Video is encoded by the FFmpeg library using the ImageIO python library. Encoding profiles can be defined in the system configuration file and selected in the “Video Settings” window. For encoding using NVIDIA GPUs, we used the NVENC encoder, however, any encoder can be configured by setting FFmpeg parameters accordingly. Specific FFmpeg builds can be used by pointing the ImageIO library to a particular FFmpeg executable (see <ext-link ext-link-type="uri" xlink:href="https://github.com/EvolutionaryNeuralCodingLab/reptiLearn/blob/master/docs/getting_started.md">“Getting Started”</ext-link> guide). Video recording from multiple sources can be manually initiated using the Web UI or automatically from experiment scripts. In addition to video files, each recording includes a CSV file containing a timestamp for each frame and a JSON file containing metadata about the recording. The ImageSource supplies timing data for each frame and, in supporting sources, represents the camera exposure start time. Any data derived from the image data will also include the original image timestamp.</p><p id="P32">Camera synchronization is accomplished by connecting the output pin of an Arduino board to the GPIO input of supporting cameras (Figure S7c). A trigger interface is provided by the arena controller for this purpose (see below) and does not require programming the Arduino manually. The video system automatically identifies an existing trigger and provides manual control through the Web UI. Additionally, the trigger is automatically paused for one second before starting a recording to simplify synchronization with external sources, such as electrophysiological data.</p></sec><sec id="S10"><title>Arena controller</title><p id="P33">Communicating with the various arena electronic components (described below) is done using the arena controller program (in the /arena folder of the repository). The program is integrated into the ReptiLearn system; however, it can also be used standalone or even on a different computer. It maintains two-way communication between the rest of the system and any number of Arduino boards by relaying commands received over an MQTT connection to serial protocol over USB, as well as forwarding data received from Arduino boards over designated MQTT topics using a simple JSON-based protocol. A single Arduino program includes all the code necessary to operate a wide range of devices, avoiding the need to program the boards manually. The “Arena Settings” window in the Web UI provides an interface to configure the controller, identify connected boards, and upload the unified Arduino program to each board. Each device connected to an Arduino board is controlled by an interface class. Several interface classes are implemented, and more can be added by implementing them using C++. For example, the LineInterface is used to control a single digital output (e.g., for switching light sources), the FeederInterface controls the automatic feeder (described below), and the TriggerInterface is used for sending TTL pulses at a selected frequency to synchronize camera acquisition. Each interface provides specific configuration parameters, has a current state value (e.g., a measurement or whether it’s turned on or off), and can respond to multiple commands (see docs/arena_interfaces.md for more details).</p><p id="P34">Once configured, the ReptiLearn system integrates with the controller in several ways. The arena module (at system/arena.py) is responsible for executing the controller program on startup and provides functions for communicating with it, which can be used to control interfaces from experiment scripts. It also maintains a list of all current interface values in the state store, which is updated after each interface command is sent, and by polling the interfaces at a fixed interval (once a minute, by default). Additionally, it can be configured to store interface values (such as temperature sensor measurements) in CSV files or a database using a data logger (see <xref ref-type="sec" rid="S12">data collection</xref> below). The Web UI provides an Arena menu (Figure S4b) for manual interaction with the controller, where individual interfaces are listed with their current state. It can also be used to control the execution of the controller program and to send commands to interfaces, for example, to trigger a reward feeder or toggle a digital output manually. Once the controller is configured, the same configuration file is automatically used to generate this menu.</p></sec><sec id="S11"><title>Touchscreen interface</title><p id="P35">A web application is included to display stimuli on screens and receive touch input into the system (at /canvas). The application uses the Konva.js 2d canvas JavaScript library and exposes large parts of its API over a bi-directional MQTT connection. The canvas module (/system/canvas.py) provides all the necessary functionality to communicate with the web application through the Canvas class. This architecture supports multiple screens by communicating with multiple web app instances, possibly running on different computers. Canvas classes can be used in experiment scripts to display various objects, such as shapes, images, or videos, and manipulate and animate properties of these objects (e.g., their position or color). Scripts can be notified of various events, such as screen touches of specific objects, video and animation progress, and other events supported by Konva.js. Example experiment scripts that use this module are available in the repository (see /system/experiments/canvas_shapes.py, for example).</p></sec><sec id="S12"><title>Data collection</title><p id="P36">The software provides several methods for collecting and storing data for offline analysis. When starting a session, a session folder is created inside the session root folder (as defined in the configuration file), in which all video, image, and data files are stored. Data loggers, implemented by the DataLogger class (see /system/data_log.py), can be used to store data in CSV files as well as in TimescaleDB database tables, which can also be used to provide real-time visualization of data using 3rd-party applications. Each data logger runs in a separate OS process to ensure that data collection does not interfere with other parts of the system.</p><p id="P37">Several specialized data loggers are provided. An event logger (see /system/event_log.py) automatically keeps track of session events and can be further configured to log changes of specific state store values or incoming MQTT messages with specific topics. Additionally, experiment scripts can use the event logger to log any event relevant to the experimental paradigm. ImageObserver loggers (ObserverLogger class) can be configured to collect the output of a specific observer (for example, to record animal position for each video frame). Lastly, one can define custom loggers within scripts using a generic API to record data from any source, such as screen touches or the position of objects on the screen (for example, see /system/experiments/canvas_video.py). An offline analysis module (at /system/analysis.py) provides classes and functions for analyzing the data stored in these folders, simplifying tasks such as finding video frames matching a specific session event or reading time-series data created by data loggers.</p></sec><sec id="S13"><title>Failure recovery</title><p id="P38">Ensuring the ability to quickly recover in the face of unavoidable interruptions, such as power or system failures, is crucial for the success of long-term experiments. Consequently, data loggers were designed to instantly save all gathered data to the session folder. The session state is periodically recorded in JSON format, triggered by significant events like the initiation and conclusion of trials and blocks. Furthermore, sessions can be paused and resumed at a later time without introducing any disruptions or complexities to the analysis process.</p></sec><sec id="S14"><title>Creating and running automated experiments</title><p id="P39">As described above, each system feature can be automated by implementing and using experiment scripts. The experiment module (at /system/experiment.py) includes an Experiment class that provides hooks for triggering code at various events: when the session is loaded (setup method), when a trial or block begins or ends (run_trial, end_trial, run_block, and end_block methods), and when the session is closed (release method). Sessions can be created and configured to run a specific script using the “New Session” dialog in the Web UI (Figure S4c). Any Experiment class found inside the experiments folder (at /system/experiments) is automatically listed and can be used in a session. Example scripts and scripts that were used in this study can be found in this directory of the GitHub repository.</p><p id="P40">Scripts can define default parameters, which can then be modified using the session section of the UI once a session is opened (Figure S4d). The session section also allows defining experiment blocks, each having different parameter values. Additionally, several builtin parameters are provided for controlling the number and duration of trials in each block, as well as block and inter-trial interval durations ($num_trials, $trial_duration, $block_duration, and $inter_trial_interval, respectively). Experiment classes can also define custom actions that can be triggered using the Web UI.</p><p id="P41">The system simplifies the process of developing and testing experiment scripts. Log messages generated by the code become instantly accessible within the Web UI, and updating the code following modifications doesn’t necessitate a system restart. This results in a swift and efficient code-test-debug feedback cycle. Since ReptiLearn is developed purely in Python, scripts can access all of the system code and interface with additional Python packages as needed.</p></sec><sec id="S15"><title>Task scheduling</title><p id="P42">Controlling the timing of code execution is an integral part of running automated experiments. Consequently, the system provides several ways to schedule tasks. Scripts can utilize the schedule module (at /system/schedule.py) to set timers and trigger functions to run at specific times each day or at regular intervals. The asyncio python library is also supported and can be used for similar purposes. Additionally, task functions can be defined inside the tasks folder (at /system/tasks) to be scheduled manually using the Schedule menu in the Web UI.</p></sec></sec><sec id="S16"><title>ReptiLearn hardware and arena</title><sec id="S17"><title>Arena cage</title><p id="P43">The arena was shaped like a box without its top face. It consisted of a frame built with square aluminum profiles and walls and floor made from 3 mm thick aluminum composite panels. The floor dimensions were 70 cm by 100 cm, and its height was 45 cm. An additional 100 cm long profile was placed one meter above the centerline of the arena floor using additional profiles to support the top-view and thermal cameras, as well as the grid of heat lamps (described below).</p></sec><sec id="S18"><title>Arena computer</title><p id="P44">The arena hardware was connected to a desktop computer placed next to the arena, on which the ReptiLearn software was running (Intel Core i7-11700K CPU, 32GB DDR4 memory, NVIDIA GEFORCE RTX 3080 Ti GPU, 500GB SAMSUNG 980 M.2 NVME SSD, and a 2TB 7200 RPM HDD). An Ubuntu 22.04 Linux operating system was used.</p></sec><sec id="S19"><title>Video and thermal cameras</title><p id="P45">Five cameras were used to acquire video and thermal data in the arena. They were attached to the arena using short adjustable arms (Noga Engineering &amp; Technology LC6100). Three FLIR Firefly S USB3 monochrome cameras (FFY-U3-16S2M-S) with 6 mm lens (Boowon BW60BLF) were attached to profiles at the top edges of the arena walls. A top-down view of the arena floor was captured using a FLIR Blackfly S USB3 color camera (BFS-U3-16S2C-CS) with a 2.8-10 mm varifocal lens (Computar A4Z2812CS-MPIR). The camera was attached to the middle of the center-top profile (described above) and was used for real-time position tracking. To prevent IR light emitted by heat lamps from flooding images, we covered the Firefly camera lens with IR cutoff filters which were attached to the lens using custom 3D-printed holders. The Blackfly color version was used, even though only monochrome images were acquired since it came pre-equipped with an IR cutoff filter. Video was mainly recorded at a frame rate of 50hz.</p><p id="P46">We used a thermal IR camera (FLIR A70) positioned next to the Blackfly camera above the arena to measure temperature dynamics during daily activity periods. The camera captures images in a resolution of 640 x 480 pixels at 16 bits per pixel (bpp), representing temperatures in up to 10 mK resolution. The camera thermal resolution is advertised as 45mK or less; however, we linearly scaled each image to 8 bpp and encoded it into video files to store the thermal data. We chose a temperature range between 20°C and 45°C and scaled accordingly, which resulted in approximately 0.1°C resolution. The camera supports up to 30 frames per second, however, images were taken at a frequency of about 3Hz due to technical issues, which FLIR developers eventually solved (but only after we finished conducting the experiments).</p></sec><sec id="S20"><title>Touch screen</title><p id="P47">A touch screen (ELO Touch Solutions AccuTouch 1790L 17” LCD open frame) was attached to one of the shorter arena walls using screws. The touch screen was connected to the arena computer using HDMI and USB cables. A cardboard was fitted around the screen to prevent animals from using it to leave the arena.</p></sec><sec id="S21"><title>Arduino microcontroller boards</title><p id="P48">Three Arduino boards (Arduino Nano Every) were used to control the arena lighting, food dispensers, temperature sensors, and heat lamps and send TTL signals to synchronize the video cameras (<xref ref-type="fig" rid="F1">Figure 1b</xref>, Figure S7). The boards were connected to the arena computer using USB cables and placed inside a box attached to the external side of an arena wall.</p></sec><sec id="S22"><title>Light</title><p id="P49">The arena was lit using an LED strip (12V, 6500K white LEDs, approximately 3.4 meters long) that was attached using adhesive to profiles at the top edge of the four arena walls. This provided relatively uniform lighting across the arena, minimizing shadows. The strip was controlled using a relay module (based on an Omron G5LE-14-DC5 5VDC SPDT relay). The module’s EN, VDD, and GND control ports were connected to one of the Arduino boards, and it was used to control the DC output of the LED strip 12V, 5A power supply unit (Figure S7a).</p></sec><sec id="S23"><title>Live prey dispenser</title><p id="P50">We used a widely available aquarium food dispenser (EVNICE EV200GW) for rewarding animals with live prey. It was attached to one of the top edges of the arena using the included clamp and released rewards into a small dish placed on the arena floor below it. To reduce the device latency (about 15 seconds), we modified its operation by directly connecting a ULN2003 stepper motor driver board to the feeder’s 28BYJ-48 stepper motor (Figure S7a). An Arduino board controls the motor driver, and Arduino code, integrated into the arena controller, implements an alternative motor control sequence, significantly reducing its latency (<xref ref-type="fig" rid="F2">Figure 2f</xref>). To increase reward capacity, we stacked two feeders on top of each other by aligning them so that a higher feeder released its reward through the release hole of the feeder below it. The experiment module tracked the number of available rewards in each feeder and determined which device should discharge a reward accordingly. After resupplying the feeders, researchers can notify the software remotely using the Web UI.</p></sec><sec id="S24"><title>Heat grid</title><p id="P51">Twelve infra-red halogen heat lamp bulbs (24 volts, 50 watts) were attached above the arena, arranged in an equally spaced grid of three rows by four columns. We attached eight flat slotted steel bars to the center-top profile (see above) to hold the lamps. For each lamp, we attached 20 cm steel M6 spacers to the steel bars and attached a ceramic G6.35 socket to the other end of each spacer using appropriate holders and screws. The spacers lower the lamps towards the arena floor, increasing floor temperature and reducing the heating area of each lamp. The lamps were connected to a dedicated Arduino board through a 16-channel relay module board (Figure S7b). The relay board was powered by a 12A, 24V DC power supply unit (PSU), which we used to simultaneously operate up to two lamps. We selected this high maximum current capacity to prevent potential problems caused by the high inrush current of halogen filaments. Another relay module (based on an Omron G5LE-14-DC5 5VDC SPDT relay) controlled the PSU output as an additional safety measure (Figure S7a). The grid enabled targeted heating of specific arena regions by more than 10°C, covering nearly the entire arena floor (<xref ref-type="fig" rid="F3">Figure 3a</xref>).</p></sec><sec id="S25"><title>Temperature sensors</title><p id="P52">We used two plastic-covered digital DS18B20 temperature sensors to measure temperature conditions in the arena. One sensor was attached to the back wall of the arena and measured ambient temperatures. The second sensor was placed on the bottom of the opposite wall under the basking area heat lamp. The sensors were connected to one of the Arduino boards using a 1-Wire connection (Figure S7a).</p></sec></sec><sec id="S26"><title>Spatial learning task</title><p id="P53">Animals were moved from the animal house to the arena near the end of the light period and remained there for the duration of the experiment (two to three weeks). Before entering the arena, each animal was placed inside an open box filled with shallow water under direct sunlight to prevent excessive dirt from reducing position-tracking accuracy and expose them briefly to UV light from the sun (an additional UV light source should be placed above the arena for longer sessions). Subsequently, animals were placed in the dark arena, simulating night time, and the experiment protocol began the following day (at 7 a.m.). During all experiments, lizards received a vegetable meal every seven days. They were given live food manually in case they did not reach a minimum of 10 worms every three days.</p><p id="P54">The experiment was fully automated by the experiment python module (see system/experiments/loclearn2.py). The LED strip and a single heat lamp were automatically turned on from 7 a.m. to 7 p.m. daily. During the light period, the module recorded video from all five cameras and controlled the YOLOv4 ImageObserver to track the animal’s head position. The ImageObserver output was stored in CSV files for later analysis and was also used by the experiment module in real-time to decide when to reward the animal according to its position (see <xref ref-type="sec" rid="S2">results</xref> section). The module was also responsible for tracking the number of available rewards in each stacked feeder and used the state store to display this information in the Web UI. It provided an action button to reset the number of available rewards following manual resupply. The event logger tracked the time when rewards were released, their resupply times, and the moments when the animal entered or exited the reinforced area. Switching between the three reinforced areas was done manually using the Web UI by moving to the next block in the session UI section. Switching was always done at the beginning of a day so that the same area was used throughout each day.</p></sec><sec id="S27"><title>Analysis</title><sec id="S28"><title>Closed-loop latency test</title><p id="P55">We estimated the minimal latency of delivering a closed-loop stimulus using the system by placing an LED on the arena floor, measuring the intensity of a single pixel that was contained within the LED light spot in each top-view camera frame, and sending a command to the arena controller to turn the LED off once its intensity reached a certain threshold. We measured the latency by counting the number of frames (at 60Hz frame rate) in which the LED was turned on. Thus, the latency measurement is quantized to 17ms bins and represents an upper bound of the real latency.</p></sec><sec id="S29"><title>Animal head position tracking</title><p id="P56">To determine whether the animal is inside the reinforced area, we used the YOLOv4 convolutional neural network (<xref ref-type="bibr" rid="R46">46</xref>) to detect the animal’s head bounding box in each top-view camera frame. We fine-tuned YOLOv4 to detect lizards’ heads by training on a custom dataset consisting of approximately 2000 grayscale images of <italic>Pogona vitticeps</italic> lizards from various angles, cameras, and different backgrounds, as well as around 1400 grayscale images that did not contain a lizard, which were added to reduce the number of false-positive detections. Approximately 800 of these negative examples were images of birds and insects from the Imagenet dataset (<xref ref-type="bibr" rid="R79">79</xref>), and the rest were images taken in several empty arenas. The resulting model was able to detect the lizard head in a wide range of images with various individual lizards, camera angles, and arenas (<xref ref-type="fig" rid="F2">Figure 2d</xref>). See <xref ref-type="fig" rid="F2">Figure 2a</xref> for an example of a bounding box detected by the model.</p><p id="P57">During experiments, we resized each top-view camera acquired frame to 416 x 416 pixels and then processed it with the fine-tuned model. We used a confidence threshold of 80%, ignoring detections with lower scores. When the model returned multiple bounding boxes, we used the box closest to the previous detected one and ignored the others. We used the bounding box’s center point to estimate the animal’s head position in the arena. We evaluated the model’s performance by comparing manually annotated head bounding boxes and model-derived bounding boxes. We used a validation set of 400 images sampled uniformly from 800 hours of experimental videos of four lizards. To estimate bounding box accuracy, we divided the intersection area of the two boxes by the area of their union (Intersection over Union, IoU).</p></sec><sec id="S30"><title>Travel distance measurements</title><p id="P58">We measured the travel distance of the animal by summing the distances between head position coordinates in each pair of consecutive video frames. We noticed that random fluctuations in the output of YOLOv4 increased travel distance considerably, especially since lizards spend long periods of time in the same location. To remove such contributions, we used Gaussian filtering on the position coordinates (window size = 51; SD = 17). To transform the measurements from pixels to centimeters, we multiplied the values by a constant factor, which was calculated by computing the distance between several pixel pairs with known physical distances and averaging the resulting multiplication factors.</p></sec><sec id="S31"><title>Basking periods estimation</title><p id="P59">Animals’ basking periods were determined based on position data. Animals were considered basking when their head bounding-box centroid resided within a radius of 20 cm around the heat lamp.</p></sec><sec id="S32"><title>Thermal measurements</title><p id="P60">We used a thermal camera to estimate ambient and animal skin temperatures during experiment sessions (see details above). We recovered temperature data for each pixel by decoding video files and linearly scaling the decoded 8bit images to Celsius values. We then calculated the ambient arena temperature for each frame by averaging over all image pixels. This measurement exhibited the same trend as the ambient temperature sensor on the arena wall but was 1.67°C higher on average (SD = 0.16) due to the inclusion of the basking area in the calculation. We estimated animals’ body temperature as follows: We found the temporally closest regular camera frame for each thermal video frame and extracted the animal head bounding box centroid using YOLOv4. We used the centroid coordinates as an input prompt to the SAM segmentation model (<xref ref-type="bibr" rid="R48">48</xref>) and produced a segmentation mask containing all animal pixels. We then linearly transformed coordinates to shift from the visual camera to the thermal camera arena coordinates. This transformation was based on a set of 112 manually labeled reference point pairs to align the centroid and mask with the coordinate space of the thermal image. We used SAM again to generate a second segmentation mask of the animal in the thermal image using the transformed centroid as input. The transformed mask was then passed through two iterations of erosion and was finally used to determine the estimated body temperatures by computing the median intensity across all mask pixels.</p><p id="P61">To analyze temperature dynamics over entire days, we extracted thermal video frames in 3-second intervals and executed the procedure described above for each frame. To make sure body temperature measurements are accurate, masks with areas outside a specific range were discarded (800 - 4000 pixels for thermal image masks and 11000 - 30000 pixels for regular camera image masks). Additionally, due to the low acquisition frequency of thermal images, pairs of regular and thermal images were occasionally unsynchronized, especially during fast movement bouts. To solve this issue, we calculated the IoU between each pair of masks and removed frames in which the IoU was below 0.3 (2% of the frames; M = 0.5; SD = 0.11). Finally, we filtered the sequence of estimated body temperatures extracted from valid frames using a moving average (window size = 51).</p><p id="P62">We validated the generated thermal body masks by examining the temperature gradients across mask edge points. Since lizards were warmer than ambient temperature, these gradients are expected to sharply decrease at lizard edges. We used a uniform sample of body masks comprising approximately 10% of the total masks generated during the measurement day. Near the heat lamp, the ambient temperatures are high, and it’s harder to evaluate gradients. We therefore removed frames in which lizards partially overlapped with the basking area (24% of the original sample). We proceeded by calculating the temperature gradient along line segments originating from the animal mask center of mass, and extending outwards such that each edge point was at the center of its respective segment (Figure S1a). Line segments were discarded when their inner part (from animal center to edge) was not fully contained in the mask or when their outer part (from the edge until the line endpoint) overlapped with mask points. For each segment, we calculated the intensity of each pixel according to the thermal image. We aligned the segments according to their middle points and averaged to produce a mean gradient for each frame. Finally, we calculated the distribution of intensities as a function of distance along the line (Figure S1b). We aligned the average mask segments in the same way, normalized their intensities to z-score units, and calculated the median gradient across the masks (Figure S1b, red line).</p></sec><sec id="S33"><title>Segmentation into periods of movement and quiescence</title><p id="P63">We segmented each experiment day into periods in which the animal moved in the arena or stayed in place based on the position-tracking data generated by the YOLOv4 model. First, we used time-based linear interpolation to replace missing position values. We applied a Gaussian filter (windows size = 51) to reduce the jitter of the model output, resulting in a matrix of <italic>T</italic>x2, where <italic>T</italic> is the number of video frames recorded daily. Then, we created a time-delay embedding from this data (windows size = 50, gap size = 2; Takens, 1981) and calculated the differences between consecutive columns for each row, resulting in a matrix of <italic>T</italic>x100 (each row containing 50 interleaved 2-dimensional velocity vectors). We conducted a principal-component analysis on this matrix and kept the first six principal components (explaining more than 90% of the data variance in an animal), which resulted in a <italic>T</italic>x6 matrix. We then converted this matrix into a binary vector of size <italic>T</italic> by calculating the L2-norm of each row and applying a threshold. A value of 1 indicated a movement sample, and 0 indicated a stationary sample. This vector was then used as an input to Kleinberg’s burst detection algorithm (<xref ref-type="bibr" rid="R62">62</xref>) to segment into periods of stationarity and movement.</p></sec><sec id="S34"><title>Entry rate statistics</title><p id="P64">To measure whether animals significantly increased their entry rate to the reinforced area compared to the previous block (ΔER), we defined 96 areas (including both 2nd and 3rd reinforced areas) in an evenly-spaced grid across the arena floor, each with the same size and shape as the reinforced areas (Figure S6). We used the animal’s position data to estimate the entry rate for each area in each block, resulting in ΔER values for each area and block transition. To calculate the entry rate for each area, we used offline simulations, assuming each time that a different area was the reinforced one and incorporating all rules for releasing a reward used in the actual task (as specified in the results section). For correlation analysis, areas were classified as either neighboring the feeder area, the 1st reinforced area, the 2nd reinforced area, or not neighboring any reinforced areas (green, orange, blue, and gray areas in Figure S6, respectively). This was determined based on whether areas had overlapping sections with the reinforced areas, however, the classification was slightly altered to ensure areas were only considered neighbors of a single reinforced area, and maintaining an equal number of neighbors for the 2nd and 3rd reinforced areas. The simulation output was the number of rewards per block. We divided this value by the number of light hours in each block to derive reward rates.</p></sec></sec></sec></body><back><ack id="S35"><title>Acknowledgements</title><p>This project has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement No. 949838) and from the Israel Science Foundation (ISF, grant No. 1133/20). The authors are most grateful to R. Eyal for guidance during the initial phase of the project; A. Shvartsman for technical and administrative assistance; the animal caretaker crew for lizard care; the Shein-Idelson laboratory for their suggestions during this work; F. Baier for comments on the manuscript.</p></ack><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghysen</surname><given-names>A</given-names></name></person-group><article-title>The origin and evolution of the nervous system</article-title><source>Int J Dev Biol</source><year>2003</year><month>Dec</month><day>1</day><volume>47</volume><issue>7–8</issue><fpage>555</fpage><lpage>62</lpage><pub-id pub-id-type="pmid">14756331</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Gire</surname><given-names>D</given-names></name><name><surname>Hoke</surname><given-names>K</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Kelley</surname><given-names>D</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><etal/></person-group><article-title>Natural behavior is the language of the brain</article-title><source>Curr Biol</source><year>2022</year><month>May</month><day>23</day><volume>32</volume><issue>10</issue><fpage>R482</fpage><lpage>93</lpage><pub-id pub-id-type="pmcid">PMC10082559</pub-id><pub-id pub-id-type="pmid">35609550</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2022.03.031</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Myers</surname><given-names>A</given-names></name><name><surname>Hansen</surname><given-names>CH</given-names></name></person-group><source>Experimental psychology</source><year>2011</year><date-in-citation>cited 2023 Sep 29</date-in-citation><publisher-name>Cengage Learning</publisher-name><comment>[Internet] Available from<ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?hl=en&amp;lr=&amp;id=He8IAAAAQBAJ&amp;oi=fnd&amp;pg=PR7&amp;dq=experimental%5C+psychology&amp;ots=iLB_u5v3Gv&amp;sig=s8l57TQKS_AxiSjH8Za42NP1-e0">https://books.google.com/books?hl=en&amp;lr=&amp;id=He8IAAAAQBAJ&amp;oi=fnd&amp;pg=PR7&amp;dq=experimental%5C+psychology&amp;ots=iLB_u5v3Gv&amp;sig=s8l57TQKS_AxiSjH8Za42NP1-e0</ext-link></comment></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garner</surname><given-names>JP</given-names></name><name><surname>Gaskill</surname><given-names>BN</given-names></name><name><surname>Weber</surname><given-names>EM</given-names></name><name><surname>Ahloy-Dallaire</surname><given-names>J</given-names></name><name><surname>Pritchett-Corning</surname><given-names>KR</given-names></name></person-group><article-title>Introducing Therioepistemology: the study of how knowledge is gained from animal research</article-title><source>Lab Anim</source><year>2017</year><month>Apr</month><volume>46</volume><issue>4</issue><fpage>103</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">28328885</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tinbergen</surname><given-names>N</given-names></name></person-group><article-title>On aims and methods of Ethology</article-title><source>Z Für Tierpsychol</source><year>1963</year><volume>20</volume><issue>4</issue><fpage>410</fpage><lpage>33</lpage></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Perona</surname><given-names>P</given-names></name></person-group><article-title>Toward a Science of Computational Ethology</article-title><source>Neuron</source><year>2014</year><month>Oct</month><day>1</day><volume>84</volume><issue>1</issue><fpage>18</fpage><lpage>31</lpage><pub-id pub-id-type="pmid">25277452</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nourizonoz</surname><given-names>A</given-names></name><name><surname>Zimmermann</surname><given-names>R</given-names></name><name><surname>Ho</surname><given-names>CLA</given-names></name><name><surname>Pellat</surname><given-names>S</given-names></name><name><surname>Ormen</surname><given-names>Y</given-names></name><name><surname>Prévost-Solié</surname><given-names>C</given-names></name><etal/></person-group><article-title>EthoLoop: automated closed-loop neuroethology in naturalistic environments</article-title><source>Nat Methods</source><year>2020</year><month>Oct</month><volume>17</volume><issue>10</issue><fpage>1052</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">32994566</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Robson</surname><given-names>DN</given-names></name><name><surname>Li</surname><given-names>JM</given-names></name></person-group><article-title>A dynamical systems view of neuroethology: Uncovering stateful computation in natural behaviors</article-title><source>Curr Opin Neurobiol</source><year>2022</year><month>Apr</month><day>1</day><volume>73</volume><elocation-id>102517</elocation-id><pub-id pub-id-type="pmid">35217311</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maselli</surname><given-names>A</given-names></name><name><surname>Gordon</surname><given-names>J</given-names></name><name><surname>Eluchans</surname><given-names>M</given-names></name><name><surname>Lancia</surname><given-names>GL</given-names></name><name><surname>Thiery</surname><given-names>T</given-names></name><name><surname>Moretti</surname><given-names>R</given-names></name><etal/></person-group><article-title>Beyond simple laboratory studies: Developing sophisticated models to study rich behavior</article-title><source>Phys Life Rev</source><year>2023</year><month>Sep</month><day>1</day><volume>46</volume><fpage>220</fpage><lpage>44</lpage><pub-id pub-id-type="pmid">37499620</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skinner</surname><given-names>BF</given-names></name></person-group><article-title>‘Superstition’in the pigeon</article-title><source>J Exp Psychol</source><year>1948</year><volume>38</volume><issue>2</issue><fpage>168</fpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bermudez Contreras</surname><given-names>E</given-names></name><name><surname>Sutherland</surname><given-names>RJ</given-names></name><name><surname>Mohajerani</surname><given-names>MH</given-names></name><name><surname>Whishaw</surname><given-names>IQ</given-names></name></person-group><article-title>Challenges of a small world analysis for the continuous monitoring of behavior in mice</article-title><source>Neurosci Biobehav Rev</source><year>2022</year><month>May</month><day>1</day><volume>136</volume><elocation-id>104621</elocation-id><pub-id pub-id-type="pmid">35307475</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinze</surname><given-names>H</given-names></name><name><surname>Nejc</surname><given-names>K</given-names></name><name><surname>Hiroki</surname><given-names>S</given-names></name><name><surname>Takashi</surname><given-names>S</given-names></name><name><surname>Saido</surname><given-names>TC</given-names></name><name><surname>Bart</surname><given-names>DS</given-names></name><etal/></person-group><article-title>A novel fully-automated system for lifelong continuous phenotyping of mouse cognition and behaviour</article-title><source>bioRxiv</source><year>2022</year><date-in-citation>cited 2022 Jun 25</date-in-citation><elocation-id>2022.06.18.496688</elocation-id><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.06.18.496688v1">https://www.biorxiv.org/content/10.1101/2022.06.18.496688v1</ext-link></comment></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiryk</surname><given-names>A</given-names></name><name><surname>Janusz</surname><given-names>A</given-names></name><name><surname>Zglinicki</surname><given-names>B</given-names></name><name><surname>Turkes</surname><given-names>E</given-names></name><name><surname>Knapska</surname><given-names>E</given-names></name><name><surname>Konopka</surname><given-names>W</given-names></name><etal/></person-group><article-title>IntelliCage as a tool for measuring mouse behavior – 20 years perspective</article-title><source>Behav Brain Res</source><year>2020</year><month>Jun</month><day>18</day><volume>388</volume><elocation-id>112620</elocation-id><pub-id pub-id-type="pmid">32302617</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Remmelink</surname><given-names>E</given-names></name><name><surname>Loos</surname><given-names>M</given-names></name><name><surname>Koopmans</surname><given-names>B</given-names></name><name><surname>Aarts</surname><given-names>E</given-names></name><name><surname>van der Sluis</surname><given-names>S</given-names></name><name><surname>Smit</surname><given-names>AB</given-names></name><etal/></person-group><article-title>A 1-night operant learning task without food-restriction differentiates among mouse strains in an automated home-cage environment</article-title><source>Behav Brain Res</source><year>2015</year><month>Apr</month><day>15</day><volume>283</volume><fpage>53</fpage><lpage>60</lpage><pub-id pub-id-type="pmid">25601577</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Bermudez-Contreras</surname><given-names>E</given-names></name><name><surname>Nazari</surname><given-names>M</given-names></name><name><surname>Sutherland</surname><given-names>RJ</given-names></name><name><surname>Mohajerani</surname><given-names>MH</given-names></name></person-group><article-title>Low-cost solution for rodent home-cage behaviour monitoring</article-title><source>PLOS ONE</source><year>2019</year><month>Aug</month><day>2</day><volume>14</volume><issue>8</issue><elocation-id>e0220751</elocation-id><pub-id pub-id-type="pmcid">PMC6677321</pub-id><pub-id pub-id-type="pmid">31374097</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0220751</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voikar</surname><given-names>V</given-names></name><name><surname>Gaburro</surname><given-names>S</given-names></name></person-group><article-title>Three Pillars of Automated Home-Cage Phenotyping of Mice: Novel Findings, Refinement, and Reproducibility Based on Literature and Experience</article-title><source>Front Behav Neurosci</source><year>2020</year><month>Oct</month><day>30</day><volume>14</volume><date-in-citation>cited 2021 Apr 6</date-in-citation><comment>[Internet]</comment><pub-id pub-id-type="pmcid">PMC7662686</pub-id><pub-id pub-id-type="pmid">33192366</pub-id><pub-id pub-id-type="doi">10.3389/fnbeh.2020.575434</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mingrone</surname><given-names>A</given-names></name><name><surname>Kaffman</surname><given-names>A</given-names></name><name><surname>Kaffman</surname><given-names>A</given-names></name></person-group><article-title>The Promise of Automated Home-Cage Monitoring in Improving Translational Utility of Psychiatric Research in Rodents</article-title><source>Front Neurosci</source><year>2020</year><date-in-citation>cited 2022 Nov 5</date-in-citation><volume>14</volume><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.frontiersin.org/articles/10.3389/fnins.2020.618593">https://www.frontiersin.org/articles/10.3389/fnins.2020.618593</ext-link></comment><pub-id pub-id-type="pmcid">PMC7773806</pub-id><pub-id pub-id-type="pmid">33390898</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2020.618593</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><etal/></person-group><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nat Neurosci</source><year>2018</year><month>Sep</month><volume>21</volume><issue>9</issue><fpage>1281</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>White</surname><given-names>SR</given-names></name><name><surname>Amarante</surname><given-names>LM</given-names></name><name><surname>Kravitz</surname><given-names>AV</given-names></name><name><surname>Laubach</surname><given-names>M</given-names></name></person-group><article-title>The future is open: open-source tools for behavioral neuroscience research</article-title><source>eneuro</source><year>2019</year><date-in-citation>cited 2023 Sep 30</date-in-citation><volume>6</volume><issue>4</issue><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.eneuro.org/content/6/4/ENEURO.0223-19.2019.short">https://www.eneuro.org/content/6/4/ENEURO.0223-19.2019.short</ext-link></comment><pub-id pub-id-type="pmcid">PMC6712209</pub-id><pub-id pub-id-type="pmid">31358510</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0223-19.2019</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costantini</surname><given-names>F</given-names></name><name><surname>Lacy</surname><given-names>E</given-names></name></person-group><article-title>Introduction of a rabbit β-globin gene into the mouse germ line</article-title><source>Nature</source><year>1981</year><volume>294</volume><issue>5836</issue><fpage>92</fpage><lpage>4</lpage><pub-id pub-id-type="pmid">6945481</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Naumann</surname><given-names>RK</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><chapter-title>Function and evolution of the reptilian cerebral cortex</chapter-title><source>Evolutionary neuroscience</source><year>2020</year><date-in-citation>cited 2023 Sep 30</date-in-citation><fpage>213</fpage><lpage>45</lpage><publisher-name>Elsevier</publisher-name><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/B978012820584600009X">https://www.sciencedirect.com/science/article/pii/B978012820584600009X</ext-link></comment></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tröder</surname><given-names>SE</given-names></name><name><surname>Zevnik</surname><given-names>B</given-names></name></person-group><article-title>History of genome editing: From meganucleases to CRISPR</article-title><source>Lab Anim</source><year>2022</year><month>Feb</month><volume>56</volume><issue>1</issue><fpage>60</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">33622064</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Francis</surname><given-names>NA</given-names></name><name><surname>Bohlke</surname><given-names>K</given-names></name><name><surname>Kanold</surname><given-names>PO</given-names></name></person-group><article-title>Automated Behavioral Experiments in Mice Reveal Periodic Cycles of Task Engagement within Circadian Rhythms</article-title><source>eneuro</source><year>2019</year><month>Sep</month><day>5</day><elocation-id>ENEURO.0121-19.2019</elocation-id><pub-id pub-id-type="pmcid">PMC6775758</pub-id><pub-id pub-id-type="pmid">31488550</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0121-19.2019</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maroteaux</surname><given-names>G</given-names></name><name><surname>Loos</surname><given-names>M</given-names></name><name><surname>Van Der Sluis</surname><given-names>S</given-names></name><name><surname>Koopmans</surname><given-names>B</given-names></name><name><surname>Aarts</surname><given-names>E</given-names></name><name><surname>Van Gassen</surname><given-names>K</given-names></name><etal/></person-group><article-title>High-throughput phenotyping of avoidance learning in mice discriminates different genotypes and identifies a novel gene: High-throughput phenotyping</article-title><source>Genes Brain Behav</source><year>2012</year><month>Oct</month><volume>11</volume><issue>7</issue><fpage>772</fpage><lpage>84</lpage><pub-id pub-id-type="pmcid">PMC3508728</pub-id><pub-id pub-id-type="pmid">22846151</pub-id><pub-id pub-id-type="doi">10.1111/j.1601-183X.2012.00820.x</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cano-Ferrer</surname><given-names>X</given-names></name><name><surname>Roberts</surname><given-names>RJV</given-names></name><name><surname>French</surname><given-names>AS</given-names></name><name><surname>de Folter</surname><given-names>J</given-names></name><name><surname>Gong</surname><given-names>H</given-names></name><name><surname>Nightingale</surname><given-names>L</given-names></name><etal/></person-group><source>OptoPi: An open source flexible platform for the analysis of small animal behaviour</source><year>2022</year><date-in-citation>cited 2022 Jul 16</date-in-citation><elocation-id>2022.07.12.499786</elocation-id><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.07.12.499786v2">https://www.biorxiv.org/content/10.1101/2022.07.12.499786v2</ext-link></comment><pub-id pub-id-type="pmcid">PMC10545942</pub-id><pub-id pub-id-type="pmid">37795340</pub-id><pub-id pub-id-type="doi">10.1016/j.ohx.2023.e00443</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Štih</surname><given-names>V</given-names></name><name><surname>Petrucco</surname><given-names>L</given-names></name><name><surname>Kist</surname><given-names>AM</given-names></name><name><surname>Portugues</surname><given-names>R</given-names></name></person-group><article-title>Stytra: An open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments</article-title><source>PLOS Comput Biol</source><year>2019</year><month>Apr</month><day>8</day><volume>15</volume><issue>4</issue><elocation-id>e1006699</elocation-id><pub-id pub-id-type="pmcid">PMC6472806</pub-id><pub-id pub-id-type="pmid">30958870</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006699</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><collab>Laboratory The International Brain</collab><person-group person-group-type="author"><name><surname>Aguillon-Rodriguez</surname><given-names>V</given-names></name><name><surname>Angelaki</surname><given-names>D</given-names></name><name><surname>Bayer</surname><given-names>H</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><etal/></person-group><article-title>Standardized and reproducible measurement of decision-making in mice</article-title><source>eLife</source><year>2021</year><month>May</month><day>20</day><volume>10</volume><elocation-id>e63711</elocation-id><pub-id pub-id-type="pmcid">PMC8137147</pub-id><pub-id pub-id-type="pmid">34011433</pub-id><pub-id pub-id-type="doi">10.7554/eLife.63711</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><article-title>Resynthesizing behavior through phylogenetic refinement</article-title><source>Atten Percept Psychophys</source><year>2019</year><month>Oct</month><volume>81</volume><issue>7</issue><fpage>2265</fpage><lpage>87</lpage><pub-id pub-id-type="pmcid">PMC6848052</pub-id><pub-id pub-id-type="pmid">31161495</pub-id><pub-id pub-id-type="doi">10.3758/s13414-019-01760-1</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Meester</surname><given-names>G</given-names></name><name><surname>Baeckens</surname><given-names>S</given-names></name></person-group><article-title>Reinstating reptiles: from clueless creatures to esteemed models of cognitive biology</article-title><source>Behaviour</source><year>2021</year><volume>158</volume><issue>12–13</issue><fpage>1057</fpage><lpage>76</lpage></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>CT</given-names></name><name><surname>Hale</surname><given-names>ME</given-names></name><name><surname>Okano</surname><given-names>H</given-names></name><name><surname>Okabe</surname><given-names>S</given-names></name><name><surname>Mitra</surname><given-names>P</given-names></name></person-group><article-title>Comparative principles for next-generation neuroscience</article-title><source>Front Behav Neurosci</source><year>2019</year><volume>13</volume><fpage>12</fpage><pub-id pub-id-type="pmcid">PMC6373779</pub-id><pub-id pub-id-type="pmid">30787871</pub-id><pub-id pub-id-type="doi">10.3389/fnbeh.2019.00012</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>RJV</given-names></name><name><surname>Pop</surname><given-names>S</given-names></name><name><surname>Prieto-Godino</surname><given-names>LL</given-names></name></person-group><article-title>Evolution of central neural circuits: state of the art and perspectives</article-title><source>Nat Rev Neurosci</source><year>2022</year><month>Oct</month><day>26</day><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="pmid">36289403</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brenowitz</surname><given-names>EA</given-names></name><name><surname>Zakon</surname><given-names>HH</given-names></name></person-group><article-title>Emerging from the bottleneck: benefits of the comparative approach to modern neuroscience</article-title><source>Trends Neurosci</source><year>2015</year><volume>38</volume><issue>5</issue><fpage>273</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC4417368</pub-id><pub-id pub-id-type="pmid">25800324</pub-id><pub-id pub-id-type="doi">10.1016/j.tins.2015.02.008</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><article-title>On the value of model diversity in neuroscience</article-title><source>Nat Rev Neurosci</source><year>2020</year><volume>21</volume><issue>8</issue><fpage>395</fpage><lpage>6</lpage><pub-id pub-id-type="pmid">32514109</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yartsev</surname><given-names>MM</given-names></name></person-group><article-title>The emperor’s new wardrobe: Rebalancing diversity of animal models in neuroscience research</article-title><source>Science</source><year>2017</year><month>Oct</month><day>27</day><volume>358</volume><issue>6362</issue><fpage>466</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">29074765</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagy</surname><given-names>ZT</given-names></name><name><surname>Sonet</surname><given-names>G</given-names></name><name><surname>Glaw</surname><given-names>F</given-names></name><name><surname>Vences</surname><given-names>M</given-names></name></person-group><article-title>First large-scale DNA barcoding assessment of reptiles in the biodiversity hotspot of Madagascar, based on newly designed COI primers</article-title><source>Plos One</source><year>2012</year><volume>7</volume><issue>3</issue><elocation-id>e34506</elocation-id><pub-id pub-id-type="pmcid">PMC3316696</pub-id><pub-id pub-id-type="pmid">22479636</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0034506</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donyavi</surname><given-names>T</given-names></name><name><surname>Bandehpour</surname><given-names>M</given-names></name><name><surname>Kazemi</surname><given-names>B</given-names></name></person-group><article-title>Preparation of transgenic Iranian lizard Leishmania coding HIL-12</article-title><source>Iran J Microbiol</source><year>2017</year><volume>9</volume><issue>5</issue><fpage>305</fpage><pub-id pub-id-type="pmcid">PMC5748450</pub-id><pub-id pub-id-type="pmid">29296276</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasys</surname><given-names>AM</given-names></name><name><surname>Park</surname><given-names>S</given-names></name><name><surname>Ball</surname><given-names>RE</given-names></name><name><surname>Alcala</surname><given-names>AJ</given-names></name><name><surname>Lauderdale</surname><given-names>JD</given-names></name><name><surname>Menke</surname><given-names>DB</given-names></name></person-group><article-title>CRISPR-Cas9 Gene Editing in Lizards through Microinjection of Unfertilized Oocytes</article-title><source>Cell Rep</source><year>2019</year><month>Aug</month><volume>28</volume><issue>9</issue><fpage>2288</fpage><lpage>2292</lpage><elocation-id>e3</elocation-id><pub-id pub-id-type="pmcid">PMC6727204</pub-id><pub-id pub-id-type="pmid">31461646</pub-id><pub-id pub-id-type="doi">10.1016/j.celrep.2019.07.089</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthews</surname><given-names>BJ</given-names></name><name><surname>Vosshall</surname><given-names>LB</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Dickinson</surname><given-names>MH</given-names></name><name><surname>Vosshall</surname><given-names>LB</given-names></name><name><surname>Dow</surname><given-names>JAT</given-names></name></person-group><article-title>How to turn an organism into a model organism in 10 ‘easy’ steps</article-title><source>J Exp Biol</source><year>2020</year><month>Feb</month><day>1</day><volume>223</volume><issue>Suppl_1</issue><elocation-id>jeb218198</elocation-id><pub-id pub-id-type="pmcid">PMC7790198</pub-id><pub-id pub-id-type="pmid">32034051</pub-id><pub-id pub-id-type="doi">10.1242/jeb.218198</pub-id></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>A</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name></person-group><article-title>Cold-blooded cognition: reptilian cognitive abilities</article-title><source>Oxf Handb Comp Evol Psychol</source><year>2012</year><fpage>129</fpage><lpage>43</lpage></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tattersall</surname><given-names>GJ</given-names></name><name><surname>Sinclair</surname><given-names>BJ</given-names></name><name><surname>Withers</surname><given-names>PC</given-names></name><name><surname>Fields</surname><given-names>PA</given-names></name><name><surname>Seebacher</surname><given-names>F</given-names></name><name><surname>Cooper</surname><given-names>CE</given-names></name><etal/></person-group><chapter-title>Coping with Thermal Challenges: Physiological Adaptations to Environmental Temperatures</chapter-title><source>Comprehensive Physiology</source><year>2012</year><date-in-citation>cited 2023 Oct 4</date-in-citation><fpage>2151</fpage><lpage>202</lpage><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://onlinelibrary.wiley.com/doi/abs/10.1002/cphy.c110055">https://onlinelibrary.wiley.com/doi/abs/10.1002/cphy.c110055</ext-link></comment><pub-id pub-id-type="pmid">23723035</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pough</surname><given-names>FH</given-names></name></person-group><article-title>The Advantages of Ectothermy for Tetrapods</article-title><source>Am Nat</source><year>1980</year><month>Jan</month><volume>115</volume><issue>1</issue><fpage>92</fpage><lpage>112</lpage></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>de Andrade</surname><given-names>DV</given-names></name></person-group><chapter-title>Temperature effects on the metabolism of amphibians and reptiles: Caveats and recommendations</chapter-title><source>Amphibian and Reptile Adaptations to the Environment</source><year>2017</year><date-in-citation>cited 2023 Sep 30</date-in-citation><fpage>129</fpage><lpage>54</lpage><publisher-name>CRC Press</publisher-name><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.researchgate.net/profile/Denis-Andrade-2/publication/303884846_Temperature_effects_on_the_metabolism_of_amphibians_and_reptiles_Caveats_and_recommendations/links/5a57e86e45851529a2ee5685/Temperature-effects-on-the-metabolism-of-amphibians-and-reptiles-Caveats-and-recommendations.pdf">https://www.researchgate.net/profile/Denis-Andrade-2/publication/303884846_Temperature_effects_on_the_metabolism_of_amphibians_and_reptiles_Caveats_and_recommendations/links/5a57e86e45851529a2ee5685/Temperature-effects-on-the-metabolism-of-amphibians-and-reptiles-Caveats-and-recommendations.pdf</ext-link></comment></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vitt</surname><given-names>LJ</given-names></name><name><surname>Caldwell</surname><given-names>JP</given-names></name></person-group><source>Herpetology: an introductory biology of amphibians and reptiles</source><year>2013</year><date-in-citation>cited 2023 Sep 30</date-in-citation><publisher-name>Academic press</publisher-name><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://books.google.com/books?hl=en&amp;lr=&amp;id=Gay9N_ry79kC&amp;oi=fnd&amp;pg=PP1&amp;dq=Herpetology+An+Introductory+Biology+of+Amphibians+and+Reptiles&amp;ots=UN7WxKoz0y&amp;sig=Rc1JheU0-iQzym6bNAwG0gjuQwY">https://books.google.com/books?hl=en&amp;lr=&amp;id=Gay9N_ry79kC&amp;oi=fnd&amp;pg=PP1&amp;dq=Herpetology+An+Introductory+Biology+of+Amphibians+and+Reptiles&amp;ots=UN7WxKoz0y&amp;sig=Rc1JheU0-iQzym6bNAwG0gjuQwY</ext-link></comment></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>TD</given-names></name><name><surname>Shaevitz</surname><given-names>JW</given-names></name><name><surname>Murthy</surname><given-names>M</given-names></name></person-group><article-title>Quantifying behavior to understand the brain</article-title><source>Nat Neurosci</source><year>2020</year><month>Dec</month><volume>23</volume><issue>12</issue><fpage>1537</fpage><lpage>49</lpage><pub-id pub-id-type="pmcid">PMC7780298</pub-id><pub-id pub-id-type="pmid">33169033</pub-id><pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mearns</surname><given-names>DS</given-names></name><name><surname>Donovan</surname><given-names>JC</given-names></name><name><surname>Fernandes</surname><given-names>AM</given-names></name><name><surname>Semmelhack</surname><given-names>JL</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name></person-group><article-title>Deconstructing Hunting Behavior Reveals a Tightly Coupled Stimulus-Response Loop</article-title><source>Curr Biol</source><year>2020</year><month>Jan</month><volume>30</volume><issue>1</issue><fpage>54</fpage><lpage>69</lpage><elocation-id>e9</elocation-id><pub-id pub-id-type="pmid">31866365</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bochkovskiy</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>CY</given-names></name><name><surname>Liao</surname><given-names>HYM</given-names></name></person-group><article-title>YOLOv4: Optimal Speed and Accuracy of Object Detection</article-title><source>ArXiv200410934 Cs Eess</source><year>2020</year><month>Apr</month><day>22</day><date-in-citation>cited 2020 Oct 21</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2004.10934">http://arxiv.org/abs/2004.10934</ext-link></comment></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aziz</surname><given-names>L</given-names></name><name><surname>Haji Salam</surname><given-names>MdSB</given-names></name><name><surname>Sheikh</surname><given-names>UU</given-names></name><name><surname>Ayub</surname><given-names>S</given-names></name></person-group><article-title>Exploring Deep Learning-Based Architecture, Strategies, Applications and Current Trends in Generic Object Detection: A Comprehensive Review</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>170461</fpage><lpage>95</lpage></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirillov</surname><given-names>A</given-names></name><name><surname>Mintun</surname><given-names>E</given-names></name><name><surname>Ravi</surname><given-names>N</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Rolland</surname><given-names>C</given-names></name><name><surname>Gustafson</surname><given-names>L</given-names></name><etal/></person-group><source>Segment Anything</source><year>2023</year><date-in-citation>cited 2023 Jun 3</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2304.02643">http://arxiv.org/abs/2304.02643</ext-link></comment></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brattstrom</surname><given-names>BH</given-names></name></person-group><article-title>Learning studies in lizards</article-title><source>Behav Neurol Lizards</source><year>1978</year><volume>1978</volume><fpage>173</fpage><lpage>81</lpage></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>CJC</given-names></name><name><surname>Jiang</surname><given-names>Z</given-names></name><name><surname>Hatton</surname><given-names>AJ</given-names></name><name><surname>Tribe</surname><given-names>A</given-names></name><name><surname>Bouar</surname><given-names>ML</given-names></name><name><surname>Guerlin</surname><given-names>M</given-names></name><etal/></person-group><article-title>Environmental enrichment for captive Eastern blue-tongue lizards (Tiliqua scincoides)</article-title><source>Anim Welf</source><year>2011</year><month>Aug</month><volume>20</volume><issue>3</issue><fpage>377</fpage><lpage>84</lpage></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>FD</given-names></name></person-group><article-title>Thermal reinforcement and thermoregulatory behaviour in the lizard Dipsosaurus dorsalis: An operant technique</article-title><source>Anim Behav</source><year>1969</year><month>Aug</month><day>1</day><volume>17</volume><fpage>446</fpage><lpage>51</lpage><pub-id pub-id-type="pmid">5370960</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>YZ</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Lai</surname><given-names>R</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Du</surname><given-names>WG</given-names></name></person-group><article-title>Molecular sensors for temperature detection during behavioral thermoregulation in turtle embryos</article-title><source>Curr Biol</source><year>2021</year><month>Jul</month><day>26</day><volume>31</volume><issue>14</issue><fpage>2995</fpage><lpage>3003</lpage><elocation-id>e4</elocation-id><pub-id pub-id-type="pmid">34015251</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>V</given-names></name><name><surname>Tattersall</surname><given-names>GJ</given-names></name></person-group><article-title>The Effect of Thermal Quality on the Thermoregulatory Behavior of the Bearded Dragon Pogona vitticeps: Influences of Methodological Assessment</article-title><source>Physiol Biochem Zool</source><year>2009</year><month>May</month><volume>82</volume><issue>3</issue><fpage>203</fpage><lpage>17</lpage><pub-id pub-id-type="pmid">19323642</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tattersall</surname><given-names>GJ</given-names></name><name><surname>Cadena</surname><given-names>V</given-names></name></person-group><article-title>Insights into animal temperature adaptations revealed through thermal imaging</article-title><source>Imaging Sci J</source><year>2010</year><month>Oct</month><day>1</day><volume>58</volume><issue>5</issue><fpage>261</fpage><lpage>8</lpage></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartholomew</surname><given-names>GA</given-names></name><name><surname>Tucker</surname><given-names>VA</given-names></name></person-group><article-title>Control of Changes in Body Temperature, Metabolism, and Circulation by the Agamid Lizard, Amphibolurus barbatus</article-title><source>Physiol Zool</source><year>1963</year><volume>36</volume><issue>3</issue><fpage>199</fpage><lpage>218</lpage></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barroso</surname><given-names>FM</given-names></name><name><surname>Riaño</surname><given-names>G</given-names></name><name><surname>Sannolo</surname><given-names>M</given-names></name><name><surname>Carretero</surname><given-names>MA</given-names></name><name><surname>Rato</surname><given-names>C</given-names></name></person-group><article-title>Evidence from Tarentola mauritanica (Gekkota: Phyllodactylidae) helps validate thermography as a tool to infer internal body temperatures of lizards</article-title><source>J Therm Biol</source><year>2020</year><month>Oct</month><day>1</day><volume>93</volume><elocation-id>102700</elocation-id><pub-id pub-id-type="pmid">33077121</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrick</surname><given-names>D</given-names></name></person-group><article-title>Body surface temperature and length in relation to the thermal biology of lizards</article-title><source>Biosci Horiz Int J Stud Res</source><year>2008</year><month>Jun</month><day>1</day><volume>1</volume><issue>2</issue><fpage>136</fpage><lpage>42</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller-Paul</surname><given-names>J</given-names></name><name><surname>Wilkinson</surname><given-names>A</given-names></name><name><surname>Aust</surname><given-names>U</given-names></name><name><surname>Steurer</surname><given-names>M</given-names></name><name><surname>Hall</surname><given-names>G</given-names></name><name><surname>Huber</surname><given-names>L</given-names></name></person-group><article-title>Touchscreen performance and knowledge transfer in the red-footed tortoise (Chelonoidis carbonaria</article-title><source>Behav Processes</source><year>2014</year><month>Jul</month><day>1</day><volume>106</volume><fpage>187</fpage><lpage>92</lpage><pub-id pub-id-type="pmid">24946312</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burghardt</surname><given-names>G</given-names></name></person-group><article-title>Learning processes in reptiles</article-title><source>Biol Reptil</source><year>1977</year><month>Jan</month><day>1</day><volume>7</volume><fpage>555</fpage><lpage>681</lpage></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kangas</surname><given-names>BD</given-names></name><name><surname>Bergman</surname><given-names>J</given-names></name></person-group><article-title>Touchscreen Technology in the Study of Cognition-Related Behavior</article-title><source>Behav Pharmacol</source><year>2017</year><month>Dec</month><volume>28</volume><issue>8</issue><fpage>623</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC5687822</pub-id><pub-id pub-id-type="pmid">29064843</pub-id><pub-id pub-id-type="doi">10.1097/FBP.0000000000000356</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname><given-names>JR</given-names></name><name><surname>Hofbauer</surname><given-names>M</given-names></name><name><surname>Bastien</surname><given-names>R</given-names></name><name><surname>Griessner</surname><given-names>J</given-names></name><name><surname>Higgins</surname><given-names>P</given-names></name><name><surname>Farooqui</surname><given-names>S</given-names></name><etal/></person-group><article-title>Virtual reality for freely moving animals</article-title><source>Nat Methods</source><year>2017</year><month>Oct</month><volume>14</volume><issue>10</issue><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="pmcid">PMC6485657</pub-id><pub-id pub-id-type="pmid">28825703</pub-id><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kleinberg</surname><given-names>J</given-names></name></person-group><source>Bursty and hierarchical structure in streams</source><conf-name>Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</conf-name><year>2002</year><date-in-citation>cited 2023 Sep 28</date-in-citation><fpage>91</fpage><lpage>101</lpage><conf-sponsor>ACM</conf-sponsor><conf-loc>Edmonton Alberta Canada</conf-loc><comment>[Internet]</comment><pub-id pub-id-type="doi">10.1145/775047.775061</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geyer</surname><given-names>B</given-names></name><name><surname>Erickson</surname><given-names>NA</given-names></name><name><surname>Müller</surname><given-names>K</given-names></name><name><surname>Grübel</surname><given-names>S</given-names></name><name><surname>Hueber</surname><given-names>B</given-names></name><name><surname>Hetz</surname><given-names>SK</given-names></name><etal/></person-group><article-title>Establishing and Maintaining an Etruscan Shrew Colony</article-title><source>J Am Assoc Lab Anim Sci</source><year>2022</year><month>Jan</month><day>1</day><volume>61</volume><issue>1</issue><fpage>52</fpage><lpage>60</lpage><pub-id pub-id-type="pmcid">PMC8786385</pub-id><pub-id pub-id-type="pmid">34772472</pub-id><pub-id pub-id-type="doi">10.30802/AALAS-JAALAS-21-000068</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boström</surname><given-names>JE</given-names></name><name><surname>Dimitrova</surname><given-names>M</given-names></name><name><surname>Canton</surname><given-names>C</given-names></name><name><surname>Håstad</surname><given-names>O</given-names></name><name><surname>Qvarnström</surname><given-names>A</given-names></name><name><surname>Ödeen</surname><given-names>A</given-names></name></person-group><article-title>Ultra-Rapid Vision in Birds</article-title><source>PLOS ONE</source><year>2016</year><month>Mar</month><day>18</day><volume>11</volume><issue>3</issue><elocation-id>e0151099</elocation-id><pub-id pub-id-type="pmcid">PMC4798572</pub-id><pub-id pub-id-type="pmid">26990087</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0151099</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>CJ</given-names></name><name><surname>Fogelson</surname><given-names>L</given-names></name></person-group><article-title>Comparative effects of hypoxia on behavioral thermoregulation in rats, hamsters, and mice</article-title><source>Am J Physiol-Regul Integr Comp Physiol</source><year>1991</year><month>Jan</month><volume>260</volume><issue>1</issue><fpage>R120</fpage><lpage>5</lpage><pub-id pub-id-type="pmid">1992812</pub-id></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terrien</surname><given-names>J</given-names></name></person-group><article-title>Behavioral thermoregulation in mammals: a review</article-title><source>Front Biosci</source><year>2011</year><volume>16</volume><issue>1</issue><elocation-id>1428</elocation-id><pub-id pub-id-type="pmid">21196240</pub-id></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>KR</given-names></name><name><surname>Cadena</surname><given-names>V</given-names></name><name><surname>Endler</surname><given-names>JA</given-names></name><name><surname>Kearney</surname><given-names>MR</given-names></name><name><surname>Porter</surname><given-names>WP</given-names></name><name><surname>Stuart-Fox</surname><given-names>D</given-names></name></person-group><article-title>Color Change for Thermoregulation versus Camouflage in Free-Ranging Lizards</article-title><source>Am Nat</source><year>2016</year><month>Dec</month><volume>188</volume><issue>6</issue><fpage>668</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">27860512</pub-id></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lara-Reséndiz</surname><given-names>RA</given-names></name><name><surname>Gadsden</surname><given-names>H</given-names></name><name><surname>Rosen</surname><given-names>PC</given-names></name><name><surname>Sinervo</surname><given-names>B</given-names></name><name><surname>Méndez-De la Cruz</surname><given-names>FR</given-names></name></person-group><article-title>Thermoregulation of two sympatric species of horned lizards in the Chihuahuan Desert and their local extinction risk</article-title><source>J Therm Biol</source><year>2015</year><month>Feb</month><day>1</day><volume>48</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmid">25660624</pub-id></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albeck</surname><given-names>N</given-names></name><name><surname>Udi</surname><given-names>DI</given-names></name><name><surname>Eyal</surname><given-names>R</given-names></name><name><surname>Shvartsman</surname><given-names>A</given-names></name><name><surname>Shein-Idelson</surname><given-names>M</given-names></name></person-group><article-title>Temperature-robust rapid eye movement and slow wave sleep in the lizard Laudakia vulgaris</article-title><source>Commun Biol</source><year>2022</year><month>Nov</month><day>29</day><volume>5</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC9709036</pub-id><pub-id pub-id-type="pmid">36446903</pub-id><pub-id pub-id-type="doi">10.1038/s42003-022-04261-4</pub-id></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>G</given-names></name></person-group><chapter-title>Spatial cognition in reptiles</chapter-title><person-group person-group-type="editor"><name><surname>Baker</surname><given-names>KJ</given-names></name></person-group><source>Reptiles</source><year>2011</year><fpage>81</fpage><lpage>100</lpage><publisher-loc>New York</publisher-loc><publisher-name>Nova Science Publishers</publisher-name></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname><given-names>LB</given-names></name><name><surname>Crews</surname><given-names>D</given-names></name><name><surname>Wilczynski</surname><given-names>W</given-names></name></person-group><article-title>Spatial and reversal learning in congeneric lizards with different foraging strategies</article-title><source>Anim Behav</source><year>1999</year><month>Feb</month><day>1</day><volume>57</volume><issue>2</issue><fpage>393</fpage><lpage>407</lpage><pub-id pub-id-type="pmid">10049480</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Font</surname><given-names>E</given-names></name></person-group><article-title>Rapid learning of a spatial memory task in a lacertid lizard (Podarcis liolepis)</article-title><source>Behav Processes</source><year>2019</year><month>Dec</month><volume>169</volume><elocation-id>103963</elocation-id><pub-id pub-id-type="pmid">31545992</pub-id></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leal</surname><given-names>M</given-names></name><name><surname>Powell</surname><given-names>BJ</given-names></name></person-group><article-title>Behavioural flexibility and problem-solving in a tropical lizard</article-title><source>Biol Lett</source><year>2012</year><month>Feb</month><day>23</day><volume>8</volume><issue>1</issue><fpage>28</fpage><lpage>30</lpage><pub-id pub-id-type="pmcid">PMC3259950</pub-id><pub-id pub-id-type="pmid">21752816</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2011.0480</pub-id></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>López</surname><given-names>JC</given-names></name><name><surname>Rodríguez</surname><given-names>F</given-names></name><name><surname>Gómez</surname><given-names>Y</given-names></name><name><surname>Vargas</surname><given-names>JP</given-names></name><name><surname>Broglio</surname><given-names>C</given-names></name><name><surname>Salas</surname><given-names>C</given-names></name></person-group><article-title>Place and cue learning in turtles</article-title><source>Anim Learn Behav</source><year>2000</year><month>Dec</month><day>1</day><volume>28</volume><issue>4</issue><fpage>360</fpage><lpage>72</lpage></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noble</surname><given-names>DWA</given-names></name><name><surname>Carazo</surname><given-names>P</given-names></name><name><surname>Whiting</surname><given-names>MJ</given-names></name></person-group><article-title>Learning outdoors: male lizards show flexible spatial learning under semi-natural conditions</article-title><source>Biol Lett</source><year>2012</year><month>Oct</month><day>17</day><volume>8</volume><issue>6</issue><fpage>946</fpage><lpage>8</lpage><pub-id pub-id-type="pmcid">PMC3497152</pub-id><pub-id pub-id-type="pmid">23075525</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2012.0813</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Day</surname><given-names>LB</given-names></name><name><surname>Ismail</surname><given-names>N</given-names></name><name><surname>Wilczynski</surname><given-names>W</given-names></name></person-group><article-title>Use of Position and Feature Cues in Discrimination Learning by the Whiptail Lizard (Cnemidophorus inornatus</article-title><source>J Comp Psychol</source><year>2003</year><volume>117</volume><fpage>440</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">14717646</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>L</given-names></name><name><surname>Wilkinson</surname><given-names>A</given-names></name></person-group><chapter-title>Evolution of cognition: A comparative approach</chapter-title><person-group person-group-type="editor"><name><surname>Barth</surname><given-names>FG</given-names></name><name><surname>Giampieri-Deutsch</surname><given-names>P</given-names></name><name><surname>Klein</surname><given-names>HD</given-names></name></person-group><source>Sensory Perception</source><year>2012</year><fpage>135</fpage><lpage>52</lpage><publisher-name>Springer Vienna</publisher-name><publisher-loc>Vienna</publisher-loc><date-in-citation>cited 2023 Sep 27</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/978-3-211-99751-2_8">http://link.springer.com/10.1007/978-3-211-99751-2_8</ext-link></comment></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Whiting</surname><given-names>MJ</given-names></name><name><surname>Noble</surname><given-names>DWA</given-names></name></person-group><chapter-title>Lizards – Measuring Cognition: Practical Challenges and the Influence of Ecology and Social Behaviour</chapter-title><person-group person-group-type="editor"><name><surname>Amici</surname><given-names>F</given-names></name><name><surname>Bueno-Guerra</surname><given-names>N</given-names></name></person-group><source>Field and Laboratory Methods in Animal Cognition: A Comparative Guide</source><year>2018</year><fpage>266</fpage><lpage>85</lpage><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><date-in-citation>[cited 2023 Oct 4]</date-in-citation><comment>[Internet] Available from: <ext-link ext-link-type="uri" xlink:href="https://www.cambridge.org/core/books/field-and-laboratory-methods-in-animal-cognition/lizards-measuring-cognition-practical-challenges-and-the-influence-of-ecology-and-social-behaviour/81B7A01FFCE5948CD7B810E9999CF277">https://www.cambridge.org/core/books/field-and-laboratory-methods-in-animal-cognition/lizards-measuring-cognition-practical-challenges-and-the-influence-of-ecology-and-social-behaviour/81B7A01FFCE5948CD7B810E9999CF277</ext-link></comment></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><source>Imagenet: A large-scale hierarchical image database</source><conf-name>2009 IEEE conference on computer vision and pattern recognition</conf-name><year>2009</year><fpage>248</fpage><lpage>55</lpage><conf-sponsor>Ieee</conf-sponsor><date-in-citation>cited 2023 Sep 25</date-in-citation><comment>[Internet] Available from:<ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/5206848/">https://ieeexplore.ieee.org/abstract/document/5206848/</ext-link></comment></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Takens</surname><given-names>F</given-names></name></person-group><chapter-title>Detecting strange attractors in turbulence</chapter-title><person-group person-group-type="editor"><name><surname>Rand</surname><given-names>D</given-names></name><name><surname>Young</surname><given-names>LS</given-names></name></person-group><source>Dynamical Systems and Turbulence, Warwick 1980</source><year>1981</year><volume>898</volume><fpage>366</fpage><lpage>81</lpage><date-in-citation>cited 2022 Dec 24</date-in-citation><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin Heidelberg</publisher-loc><comment>[Internet] (Lecture Notes in Mathematics) Available from: <ext-link ext-link-type="uri" xlink:href="http://link.springer.com/10.1007/BFb0091924">http://link.springer.com/10.1007/BFb0091924</ext-link></comment></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Features and design of the ReptiLearn behavioral system.</title><p>(<bold>a)</bold> A schematic of the features supported by ReptiLearn. ReptiLearn is written in Python, provides an API for automating experimental tasks, runs real-time processing, controls arena hardware components including live food dispensers and a heat reward system, collects time-series data, features a web-based user interface for remote monitoring and control. <bold>(b)</bold> Diagram of hardware components in ReptiLearn. The arena includes synchronized visual and thermal cameras, temperature sensors, live prey feeders, a grid of 12 heat lamps covering the arena, illumination LEDs and a touchscreen. The hardware is controlled using Arduino boards and designed with generic interfaces for diverse research needs. ReptiLearn can run with different subsets of the above components. <bold>(c)</bold> A schematic illustrating the real-time closed loop processing in ReptiLearn. ReptiLearn allows implementing closed-loop behavioral tasks linking any of the following features in real-time: Animal and ambient temperature, animal position and posture, animal screen touches, live prey or heat reward, and visual stimulation on the screen. <bold>(d)</bold> Screenshot of the web-based user interface. The interface allows monitoring the cameras (top left) and the status of hardware (top menu) as well as controlling the arena (top menu) and experiments remotely (experimental design panel on the right). Experiment events and system information appear in the log (bottom).</p></caption><graphic xlink:href="EMS190609-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>ReptiLearn exhibits high performance with low latencies.</title><p><bold>(a)</bold> Example of the fine-tuned YOLOv4 model output. The model was trained to detect the bounding box of the lizard’s head (red). The box centroid (green) was used to estimate the animal’s position. <bold>(b)</bold> YOLOv4 object detection latency distribution. Latencies were measured between the time of receiving the image by the model and the output time of position calculation. <bold>(c)</bold> Distribution of latencies in closed-loop experiments. Latencies were measured from the time of sending a command to turn on a LED to the time of detecting LED intensity change in the video stream (this delay comprises the arena controller, video acquisition and LED detection analysis). <bold>(d)</bold> A confusion matrix showing the results of the fine-tuned YOLOv4 model over the validation set. Model was tuned to have a zero false positive rate. <bold>(e)</bold> YOLOv4 intersection-over-union (IoU) distribution over a validation set consisting of 400 images sampled uniformly from video data of 4 animals and indicating good overlap with animal head. <bold>(f)</bold> Food dispenser latency distribution. Latencies were measured from the time of sending a command to the worm dispenser to the detection of the dispensed worm in the video stream after landing on the arena floor. Rewards are received within an average time of 3.21s (SD=0.54s).</p></caption><graphic xlink:href="EMS190609-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>ReptiLearn allows extracting complex relations between skin temperature and spatial dynamics.</title><p><bold>(a)</bold> Heat gradients of a grid of 12 heat lamps across the arena floor. The outer, middle, and inner areas of each lamp represent temperature increases of 4°C, 6°C, and 8°C above baseline, respectively. <bold>(b)</bold> Temperature dynamics of a single heat lamp measured with a digital temperature sensor over trials (gray) and on average (black) relative to initial temperature (0). The lamp was turned off after 30 minutes (red line). <bold>(c)</bold> Skin temperatures were calculated by first segmenting the animal in the regular camera image (top, green) using the SAM algorithm, based on the output centroid of the YOLOv4 model (orange dot). Next, a linear function <italic>f</italic> is used to transform the mask and centroid to the thermal image pixel space (right top) and the skin temperature is determined by calculating the median over the mask. (right bottom) A comparison with a SAM mask calculated directly from the thermal camera (red) is used to assess the validity of the transformed mask. <bold>(d)</bold> Movement dynamics (travel speed, blue) and corresponding skin temperature changes (red, with ambient temperature in orange) as well as reward times (green) and basking periods (gray) measured over a single day.</p></caption><graphic xlink:href="EMS190609-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Lizards learn a spatial task in a closed-loop automated paradigm.</title><p><bold>(a)</bold> Schematics of a spatial learning task block. The lizard was conditioned to visit a predefined area in the arena (yellow circle). Following the lizard’s detection in the rewarded location, a LED automatically blinked for 5 seconds, after which a live prey was delivered. The lizard had to stay inside the rewarded area for at least two seconds to receive a reward. After each reward, the animal had to exit the cooldown area (gray circle), and 20 seconds had to pass before the lizard could return to trigger another reward. <bold>(b)</bold> The experiment consisted of three blocks (as in (a)). In the first, the feeder area (green) was reinforced with food reward. In the second and third blocks, the top-right corner (orange) and top-left corner (blue) were reinforced, respectively. <bold>(c)</bold> Raster plot of reward times for animal 1 over the full 12-day experiment. Background colors correspond to the reinforced areas. <bold>(d)</bold> The change (between the 1st and 2nd blocks) in the lizard’s entry rate (ΔER) to different areas plotted on the physical space of the arena. Each square is a mean over entries to a circular area (same size as the reinforced area) surrounding the square. Notice the strongest increase for the 2nd reinforced area but also a significant increase in the reward location. <bold>(e)</bold> Same as (e) but for the changes between the 2nd and 3rd blocks of the same animal. Notice the strong increase for the 3rd area (blue) and strong decrease for the 2nd area. <bold>(f)</bold> Position trajectories over a single day (animal 1, block 3, day 3), segmented into stationary (gray dots marking mean location during stationary periods) and movement periods (colored lines). <bold>(g)</bold> ΔER (as in (e)) for each square as a function of its distance from the 2nd reinforced area. Linear regression analysis shows significant (p&lt;0.001) correlation of C=-0.72. Areas overlapping with the feeder area (green points) were excluded from the regression. Blue and orange dots correspond to blue and orange areas in (b). <bold>(h)</bold> Same as (g) but for the increase between the 2nd and 3rd blocks as a function of its distance from the 3rd reinforced area (p&lt;0.001; C=-0.77). (<bold>i</bold>) Distributions of correlation coefficients for ΔER as a function of distance (as in (g) and (h)) across areas for different animals and block (orange and blue dots correspond to blocks 1→2 and blocks 2→3, respectively). Areas in the histogram are color coded as in (g). Orange and blue arrows indicate the center of the reinforced areas of the 2nd and 3rd blocks, respectively. Notice strong shifts in ΔER correlation distribution following the change in rewarded area for all animals, except 3rd block in Animal 2.</p></caption><graphic xlink:href="EMS190609-f004"/></fig></floats-group></article>