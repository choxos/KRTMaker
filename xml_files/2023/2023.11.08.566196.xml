<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190953</article-id><article-id pub-id-type="doi">10.1101/2023.11.08.566196</article-id><article-id pub-id-type="archive">PPR758254</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Stacking Models of Brain Dynamics Improves Prediction of Subject Traits in fMRI</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Griffin</surname><given-names>B</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Ahrends</surname><given-names>C</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Woolrich</surname><given-names>M</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Smith</surname><given-names>S</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib></contrib-group><aff id="A1"><label>1</label>Oxford Centre for Functional MRI of the Brain (FMRIB), Wellcome Centre for Integrative Neuroimaging, Nuffield Department of Clinical Neurosciences, University of Oxford, Oxford, OX3 9DU, United Kingdom</aff><aff id="A2"><label>2</label>Center of Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University, Aarhus, Denmark</aff><aff id="A3"><label>3</label>Oxford Centre for Human Brain Activity (OHBA), Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford, Oxford, United Kingdom</aff><pub-date pub-type="nihms-submitted"><day>14</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>13</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Beyond structural and time-averaged functional connectivity brain measures, the way brain activity dynamically unfolds can add important information when investigating individual cognitive traits. One approach to leveraging this information is to extract features from models of brain network dynamics to predict individual traits. However, there are two potential sources of variation in the models’ estimation which will in turn affect the predictions: first, in certain cases, the estimation variability due to different initialisations or choice of inference method; and second, the variability induced by the choice of the model hyperparameters that determine the complexity of the model. Rather than merely being statistical noise, this variability may be useful in providing complementary information that can be leveraged to improve prediction accuracy. We propose stacking, a prediction-driven approach for model selection, to leverage this variability. Specifically, we combine predictions from multiple models of brain dynamics to generate predictions that are accurate and robust across multiple cognitive traits. We demonstrate the approach using the Hidden Markov Model, a probabilistic generative model of brain network dynamics. We show that stacking can significantly improve the prediction of subject-specific phenotypes, which is crucial for the clinical translation of findings.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">A major objective in neuroscience is to develop predictive models that aim to discover associations between brain data and subject traits (e.g., clinical or behavioural measures). Accurate and robust predictions are essential if models are to be used for real-world applications, particularly in clinical settings (<xref ref-type="bibr" rid="R17">Dinsdale et al., 2022</xref>). In recent years, for instance, there has been a growing interest in using structural MRI to explore the relationship between ageing, cognitive decline, and certain brain disorders (<xref ref-type="bibr" rid="R25">Kaufmann et al., 2019</xref>; <xref ref-type="bibr" rid="R45">Shahab et al., 2019</xref>), as well as the illness course of psychoses patients (<xref ref-type="bibr" rid="R35">Mourao-Miranda et al., 2012</xref>). However, while structural connectivity can be a powerful predictor, it cannot capture the diverse patterns of neural activity and communication from which cognition emerges. For instance, by analysing resting-state functional MRI (rfMRI), several neurological disorders (e.g., epilepsy, Alzheimer’s) have been associated with disruptions in static (time-averaged) functional connectivity (FC) (<xref ref-type="bibr" rid="R26">Kavitha et al., 2019</xref>; <xref ref-type="bibr" rid="R38">Pievani et al., 2011</xref>). Furthermore, investigating the dynamics of the brain using fMRI has revealed associations between dynamic (time-varying) FC and complex traits, such as intelligence, beyond anatomy or static FC (<xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>).</p><p id="P3">A common approach to examining brain dynamics is the sliding time window approach (<xref ref-type="bibr" rid="R41">Sakoğlu et al., 2010</xref>). Features extracted via the application of this method to fMRI data have been used to predict intelligence (<xref ref-type="bibr" rid="R44">Sen &amp; Parhi, 2021</xref>), and the disease progression of Alzheimer’s patients (<xref ref-type="bibr" rid="R1">Abrol et al., 2019</xref>). Although there are several variants of this approach (<xref ref-type="bibr" rid="R6">Allen et al., 2014</xref>; <xref ref-type="bibr" rid="R27">Leonardi &amp; Van De Ville, 2015</xref>; <xref ref-type="bibr" rid="R29">Liégeois et al., 2016</xref>), the estimation of FC is inherently noisy and can be influenced by factors such as length and placement of the time windows, affecting the model’s ability to detect temporal variations of interest (<xref ref-type="bibr" rid="R34">Mokhtari et al., 2019</xref>; <xref ref-type="bibr" rid="R39">Preti et al., 2017</xref>).</p><p id="P4">An alternative approach to examining brain dynamics is to use generative models of brain network dynamics (that are trained on the entire data set) and then extract features that can predict individual subject traits (e.g., fluid intelligence). One example of this is the use of multivariate autoregressive models, which can capture dynamic FC at a temporal resolution of a few seconds, and have been used to predict task-based phenotypes better than static FC (<xref ref-type="bibr" rid="R28">Liégeois et al., 2019</xref>). In our study, we employed the Hidden Markov Model (HMM), a probabilistic model of temporal dynamics of amplitude and FC that characterises the data using a discrete number of amplitude and FC states and transitions between them (<xref ref-type="bibr" rid="R58">Vidaurre et al. 2017</xref>). By doing so, we aim to leverage a wider and more diverse range of information.</p><p id="P5">As showed previously, the HMM can be combined with predictive machine learning methods to predict subject traits from fMRI recordings (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>; <xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>). To generate predictions, here we used the Fisher kernel method, a mathematically principled way of combining generative models like the HMM with predictive methods through the use of a kernel function (<xref ref-type="bibr" rid="R43">Schölkopf &amp; Smola, 2001</xref>; <xref ref-type="bibr" rid="R46">Shawe-Taylor et al., 2004</xref>). This approach enabled us to preserve the structure of the generative model while leveraging the predictive power of kernel methods (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>; <xref ref-type="bibr" rid="R23">Jaakkola et al., 2000</xref>; <xref ref-type="bibr" rid="R24">Jaakkola &amp; Haussler, 1998</xref>).</p><p id="P6">However, the practical implementation of subject trait predictions based on functional data relies on the accuracy and robustness of the predictions. Regardless of the chosen generative model, there can be two potential sources of variation in these predictions. First, there can be run-to-run variability (e.g., due to different initialisations of the estimation process for a given dataset) for approaches relying on non-convex optimisation procedures (<xref ref-type="bibr" rid="R7">Alonso &amp; Vidaurre, 2023</xref>); the HMM inference is an example of this, which can impact subsequent predictions made using features derived from the fitted HMM model. Second, the choice of generative model hyperparameters, such as the model order (e.g., number of HMM states) or the strength of regularisation, affects the result of the estimation, so modifying these hyperparameters can add an additional factor of variability. Variability is generally viewed as a disadvantage because it poses a problem for interpretability. When it comes to predicting subject traits using the HMM, moreover, it can be difficult to determine the optimal HMM run or hyperparameter choice, potentially leading to inconsistent prediction accuracies.</p><p id="P7">In our study, however, we aimed to identify variability that offers distinct and complementary information (across different analyses of the same data) rather than being driven by estimation noise. Then, we employed a stacked generalisation framework (<xref ref-type="bibr" rid="R16">Breiman, 1996</xref>; <xref ref-type="bibr" rid="R60">Wolpert, 1992</xref>) that leverages this variability to improve the accuracy and robustness in predicting subject traits. Previous studies have used stacking to integrate multimodal neuroimaging data for age prediction (<xref ref-type="bibr" rid="R19">Engemann et al., 2020</xref>) and mild cognitive impairment classification (<xref ref-type="bibr" rid="R52">Vaghari et al., 2022</xref>). In this study, we used stacking to substitute model selection with model integration—in other words, our approach bypassed the complex decision of selecting the best HMM hyperparameter choices, albeit at the expense of increased computation. Specifically, our method aimed to integrate complementary descriptions of brain dynamics, such as capturing different temporal scales, as they are captured by different configurations of the HMM (i.e., across a wide range of hyperparameters).</p><p id="P8">Focusing on resting-state fMRI (rfMRI) due to its wide availability, we demonstrate the efficacy of our stacking framework on data from two large-scale neuroimaging datasets: UK Biobank (UKB) (<xref ref-type="bibr" rid="R49">Sudlow et al., 2015</xref>) and the Human Connectome Project (HCP) (<xref ref-type="bibr" rid="R53">Van Essen et al., 2013</xref>). For both datasets, we explored the relationships between rfMRI data and a range of cognitive traits. By doing so, we found that stacking predictions generated from different configurations of the HMM generally outperformed simpler approaches in terms of robustness and accuracy.</p></sec><sec id="S2" sec-type="materials | methods"><label>2</label><title>Materials &amp; Methods</title><p id="P9">To predict a set of cognitive traits from models of rfMRI FC dynamics, our approach involved multiple steps. In brief, we first independently ran the HMM on the rfMRI data multiple times, generating a prediction from each HMM (referred to as base-level predictions). Subsequently, we used these base-level predictions as input features for a meta-model to generate a stacked prediction.</p><p id="P10">The process used to produce a base-level prediction is outlined in <xref ref-type="fig" rid="F1">Figure 1</xref>. For each base-level prediction, an HMM was run on a temporally concatenated fMRI timeseries of all subjects to obtain a group-level HMM (<xref ref-type="bibr" rid="R58">Vidaurre et al., 2017</xref>, <xref ref-type="bibr" rid="R57">2021</xref>) (<xref ref-type="fig" rid="F1">Figure 1a</xref>). We then used the Fisher kernel method, a mathematically principled approach to predicting target variables from an HMM (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>; <xref ref-type="bibr" rid="R24">Jaakkola &amp; Haussler, 1998</xref>) (<xref ref-type="fig" rid="F1">Figure 1b</xref>). We compared each subject’s timeseries to the group-level HMM in the HMM’s parameter space, assuming that similar subjects would induce similar parameter gradients in this space. Using these comparisons, we created a subjects-by-subjects similarity matrix—the Fisher kernel matrix. Finally, subject-specific trait predictions were generated using these similarity matrices via a kernel ridge regression (KRR) model (<xref ref-type="fig" rid="F1">Figure 1c</xref>).</p><p id="P11">Specifically, we generated 100 base-level predictions in this way, independently estimating an HMM for each base-level prediction from the same fMRI timeseries; 50 times with fixed hyperparameters (to investigate the run-to-run variability of the HMM), and 50 times with varying hyperparameters (to investigate variability induced by varying the hyperparameters). We then generated two stacked predictions by separately combining the two sets of 50 predictions. We now present all the steps that integrate this procedure in more detail and introduce the two datasets to which we applied the framework, UKB and HCP.</p><sec id="S3"><label>2.1</label><title>HCP neuroimaging and non-imaging data</title><p id="P12">We used publicly available rfMRI data from 1,001 subjects from the Human Connectome Project (HCP) dataset (<xref ref-type="bibr" rid="R53">Van Essen et al., 2013</xref>). The full acquisition details and preprocessing pipeline for the HCP dataset are described in <xref ref-type="bibr" rid="R54">Van Essen et al. (2012)</xref>. In brief, 3T whole-brain fMRI data were acquired with a spatial resolution of 2×2×2 mm and a temporal resolution of 0.72s. The preprocessing pipeline is described in (<xref ref-type="bibr" rid="R48">Smith et al., 2013</xref>), but the primary steps included motion correction, high-pass temporal filtering to remove the linear trends of the data, and artefact removal using ICA+FIX (<xref ref-type="bibr" rid="R20">Griffanti et al., 2014</xref>). A parcellation of 25 independent components (ICs) was obtained by performing group spatial ICA using MELODIC (<xref ref-type="bibr" rid="R12">Beckmann &amp; Smith, 2004</xref>). We chose the 25 IC parcellation as it covers the major functional networks. Additionally, coarser parcellations are more reliable for estimating FC dynamics due to the smaller number of free parameters per state (<xref ref-type="bibr" rid="R2">Ahrends et al., 2022</xref>). For each participant, this resulted in 25 timeseries, composed of 4,800 time points across four scanning sessions (with 1,200 time points in each session) of approximately 15 minutes each. For performance evaluation, we selected all traits related to fluid intelligence, as well as the traits given by the unadjusted cognitive test scores. This resulted in 15 cognitive traits. The list of these 15 traits can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-1</xref>.</p><p id="P13">As in <xref ref-type="bibr" rid="R58">Vidaurre et al. (2017)</xref>, we controlled for sex and motion. Additionally, we accounted for familial relationships when assigning subjects to cross-validation folds, to make sure related subjects were never split across folds.</p></sec><sec id="S4"><label>2.2</label><title>UKB neuroimaging and non-imaging data</title><p id="P14">Our aim was to predict a set of cognitive traits (e.g., task performance) for 16,352 subjects from the UK Biobank (UKB) dataset (<xref ref-type="bibr" rid="R49">Sudlow et al., 2015</xref>) using models of their rfMRI dynamics. The full acquisition details and preprocessing pipeline for the UKB dataset are described in <xref ref-type="bibr" rid="R47">Smith et al. (2022)</xref>. In brief, 3T fMRI data were acquired for each participant, consisting of 490 time points per session at a TR of 0.735s and with 2.4mm spatial resolution. Preprocessing was performed using the standard UKB pipeline, including brain extraction, motion correction, structured artefact removal using ICA+FIX (<xref ref-type="bibr" rid="R20">Griffanti et al., 2014</xref>), high-pass temporal filtering and registration to MNI152 space (<xref ref-type="bibr" rid="R4">Alfaro-Almagro et al., 2018</xref>). We used surface-based node timeseries generated using 25-dimensional group-ICA surface maps from the Human Connectome Project (HCP) S1200 dataset (<xref ref-type="bibr" rid="R53">Van Essen et al., 2013</xref>). This resulted in 25 timeseries, composed of 490 time points across a 6-minute scanning session for each patient, and allowed for a more direct comparison between UKB and HCP.</p><p id="P15">To match the number of traits investigated in HCP, we first excluded cognitive traits with missing recorded values for over half of UKB subjects, before performing preliminary analyses to generate predictions for the remaining 450 cognitive traits in UKB (of the 1331 cognitive traits in total). Subsequently, we narrowed down our focus to the top-performing 15 traits in terms of prediction accuracy. The rationale behind selecting the traits which could be predicted most accurately was to demonstrate the potential of stacking compared to alternative approaches. Since stacking involves combining predictions from base estimators, these underlying estimators must exhibit a certain level of accuracy for stacking to be any meaningful. Given that the primary focus of our investigation was to compare predictions from dynamic FC, this preliminary analysis was performed using static FC to reduce bias towards a particular dynamic FC approach. The prediction accuracies for all 450 cognitive traits are shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-1</xref> and the list of the selected 15 traits can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-2</xref>.</p><p id="P16">Also, we employed a reduced set of confounds extracted from a comprehensive set of 602 UKB imaging confounds provided by <xref ref-type="bibr" rid="R5">Alfaro-Almagro et al. (2021)</xref>. To generate our reduced set of confounds, we first selected a subset of conventional confounds including sex, scanning site, head size, and head motion (which use FSL’s FEAT (<xref ref-type="bibr" rid="R61">Woolrich et al., 2001</xref>) and EDDY (<xref ref-type="bibr" rid="R10">Andersson et al., 2016</xref>, <xref ref-type="bibr" rid="R9">2017</xref>)). Then, we reduced the remaining confounds by using the singular value decomposition and selecting the top principal components which accounted for 85% of the variance. This process resulted in a set of 30 variables that we used for deconfounding by regressing them out from both the predictor and target variables using linear regression. To prevent information leakage from the test data to the training data, we applied deconfounding within cross-validation folds.</p></sec><sec id="S5"><label>2.3</label><title>The hidden Markov model</title><p id="P17">The hidden Markov model is a generative probabilistic model that can be applied to fMRI timeseries to find recurrent patterns of activity and FC (<xref ref-type="bibr" rid="R58">Vidaurre et al., 2017</xref>). The model assumes that the data can be described using a discrete number of probabilistic models, or latent states. While it is possible to parametrise the HMM to focus primarily on FC, here we chose to model both FC and signed amplitude for the majority of our analysis, but also considered models with only FC (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-2</xref>).</p><p id="P18">An adequate choice for fMRI is to model these brain states as Gaussian distributions, where each state <italic>k</italic> (of a total <italic>K</italic> states) is governed by two sets of parameters - a mean vector <italic>μ<sub>k</sub></italic>, which can be interpreted as the mean activation for each of <italic>M</italic> brain regions from a specified parcellation (here <italic>M</italic> group-ICA components), and a covariance matrix Σ<sub>k</sub>, which can be interpreted as FC between the <italic>M</italic> brain regions. The model inference also estimates the initial probabilities that the timeseries start in each of the <italic>K</italic> states, given by <italic>π</italic>, as well as the probabilities of transitioning between states, given by a transition probability matrix <italic>A</italic>.</p><p id="P19">The HMM used to represent the fMRI data is described by this set of parameters, <italic>θ</italic> (see <xref ref-type="fig" rid="F1">Figure 1a</xref>), where <italic>μ</italic> and Σ represents the mean vectors and covariance matrices across all states: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>π</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mtext>Σ</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi>π</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mtext>Σ</mml:mtext><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>M</mml:mi><mml:mi>x</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P20">The optimal parameters are estimated from the data at the group level (i.e., the state probability distributions are the same for all subjects)<sup><xref ref-type="fn" rid="FN1">1</xref></sup>. Following this, subject-specific HMM parameters can be determined using dual estimation (<xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>), as well as subject-specific state-time courses (i.e., the probabilities of each state being active at each time point).</p><p id="P21">In the HMM generative process, we assume that each time step <italic>t</italic> of the observed fMRI timeseries <italic>X<sub>t</sub></italic> involves sampling from a Gaussian distribution with mean <italic>μ<sub>k</sub></italic> and covariance Σ<italic><sub>k</sub></italic> when state <italic>k</italic> is active: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>Σ</mml:mtext><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>q<sub>t</sub></italic> is the active state at time <italic>t</italic>. This currently active state, <italic>q<sub>t</sub></italic>, depends on the previous state <italic>q<sub>t-1</sub></italic> and is determined by the transition probabilities <italic>A</italic>. Consequently, the sequence of states is generated by sampling from a categorical distribution with parameters: <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>~</mml:mo><mml:mi>C</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula> where, <italic>A<sub>k</sub></italic> represents the <italic>k</italic>-th row of the transition probability matrix.</p><p id="P22">When applying the HMM, there are two potential sources of variability. The first we refer to as run-to-run estimation variability, which can arise both from the random initialisation of the HMM estimation process for a given dataset, as well as from the stochastic inference method we apply. Random initialisation refers to the random assignment of parameter values, such as the state parameters or transition probabilities, at the beginning of the training process. Furthermore, we use an efficient stochastic approach for the HMM estimation, which applies the principles of stochastic optimisation to variational inference (<xref ref-type="bibr" rid="R56">Vidaurre et al., 2018</xref>). This approach is computationally cheaper and suitable for large neuroimaging datasets such as UKB and HCP. However, it introduces variability in the estimation of the state-time courses, which are determined by the Baum-Welch algorithm (<xref ref-type="bibr" rid="R11">Baum et al., 1970</xref>). At each iteration, the estimation of the state-time course is updated based on a random subset of subjects, which is a noisy but computationally efficient approach. Since the HMM optimisation can converge to local minima, different initialisations and batches of subjects for the estimation can impact the accuracy of subsequent predictions.</p><p id="P23">The second source of variability comes from the selection of the hyperparameters of the HMM, which are set by the user. We refer to this as hyperparameter selection variability. By varying the hyperparameters, we can discover distinct patterns of FC, for example across different time scales (<xref ref-type="bibr" rid="R2">Ahrends et al., 2022</xref>). In our study, we focused on varying two HMM hyperparameters: the number of states (<italic>K</italic>), and the prior probability of remaining in the same state (δ), which is parametrised by the prior Dirichlet distribution concentration parameter of the corresponding prior distribution<sup><xref ref-type="fn" rid="FN2">2</xref></sup> (<xref ref-type="bibr" rid="R32">Masaracchia et al., 2023</xref>), which effectively influences the time scale of the estimate. This parameter determines the size of the diagonal elements in the prior distribution of the hidden states (i.e., the transition probability matrix).</p><p id="P24">In total, we fit the HMM to the same rfMRI timeseries 100 times; 50 times with fixed hyperparameters<sup><xref ref-type="fn" rid="FN3">3</xref></sup> (i.e., with random initialisations and random batches of subjects for the stochastic inference;), and 50 times with varying hyperparameters<sup><xref ref-type="fn" rid="FN4">4</xref></sup> (i.e., with additional variability coming from the selection of hyperparameters;). When fixing the hyperparameters, we chose <italic>δ</italic> = 10, the default setting in the HMM-MAR toolbox, and <italic>K</italic> = 6, commonly used values in recent literature (<xref ref-type="bibr" rid="R7">Alonso &amp; Vidaurre, 2023</xref>; <xref ref-type="bibr" rid="R40">Quinn et al., 2018</xref>; <xref ref-type="bibr" rid="R56">Vidaurre et al., 2018</xref>). The choice of states represents a compromise between having a higher number of states, which can increase the chances of certain states being present in only a subset of subjects (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>), and a lower number of states, which can increase the chance of assigning entire sessions to a single state (i.e., model stasis) (<xref ref-type="bibr" rid="R2">Ahrends et al., 2022</xref>). Nevertheless, the specific choice of hyperparameters is less critical to our study, as our focus when fixing the hyperparameter is to investigate run-to-run variability which could be done with an alternative configuration.</p><p id="P25">When we varied the hyperparameters, our objective was to investigate both run-to-run variability and hyperparameter selection variability, irrespective of the specific hyperparameter settings employed. Therefore, we used a wide range of hyperparameters and repeated each hyperparameter combination twice with different initialisations and different subject batches for the stochastic inference. As a result, we anticipated a greater degree of variability when adjusting the hyperparameters of the HMM, since we expect to encounter both types of variability.</p><sec id="S6"><label>2.3.1</label><title>Static FC</title><p id="P26">To compare the performance of our dynamic FC predictions with simpler methods that do not consider brain dynamics, we also used time-averaged FC (also referred to as static FC) for prediction. To be comparable to the dynamic FC predictions, we used a one-state HMM as a static FC estimator, which essentially yields a single covariance matrix per subject (modelled as an inverse Wishart distribution). Subsequently, we used the Fisher kernel method and kernel ridge regression in an analogous manner to the dynamic FC predictions.</p><p id="P27">Alternatively, it is also possible to use partial correlations, which is defined as the correlation between the time series of two brain regions after regressing out the time series of all the other regions (i.e., the inverse covariance). We repeated our static FC analysis using this alternative approach, the results of which can be found in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-3</xref>.</p></sec></sec><sec id="S7"><label>2.4</label><title>The Fisher kernel</title><p id="P28">To predict subject traits from the HMM, we used the Fisher kernel method (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>; <xref ref-type="bibr" rid="R23">Jaakkola et al., 2000</xref>; <xref ref-type="bibr" rid="R24">Jaakkola &amp; Haussler, 1998</xref>), which is a mathematically principled approach to predict from generative probabilistic models. To do this, we first estimate subject-specific HMM parameter sets (i.e., initial state probabilities, transition probability matrices, and brain state means and covariance) using dual estimation (<xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>). The Fisher kernel is constructed from these subject-specific HMM parameter sets and can be used with a kernel-based classification or prediction method, here KRR, while respecting the structure of the HMM. Specifically, the Fisher kernel is constructed upon the Riemannian manifold formed by the parameters that define the HMM (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>; <xref ref-type="bibr" rid="R23">Jaakkola et al., 2000</xref>; <xref ref-type="bibr" rid="R24">Jaakkola &amp; Haussler, 1998</xref>). This method therefore provides an approach, given a certain choice of hyperparameters, to predict subject traits from time-varying patterns of activity and FC.</p><p id="P29">The intuition behind the Fisher kernel is in the comparison of the generative processes for the subjects’ timeseries. Specifically, we look at the influence of a particular HMM parameter in the generation for the timeseries of each subject and compare these for every pair of subjects. This is represented by the Fisher scores, which are gradients in the parameter space. Fisher scores are then compared between subjects, such that similar subjects have similar scores for fixed HMM parameters.</p><p id="P30">The Fisher score, <italic>U<sub>x</sub></italic>, is given by the gradient of the log-likelihood with respect to each parameter (<xref ref-type="fig" rid="F1">Figure 1b</xref>): <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>∇</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>x<sub>t</sub></italic> is the fMRI timeseries of a given subject, and <inline-formula><mml:math id="M5"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo>|</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the log-likelihood of timeseries <italic>x<sub>t</sub></italic> given the HMM parameters <italic>θ</italic>.</p><p id="P31">The Riemannian manifold upon which the parameters of the HMM lie is completed by choosing a local Riemannian metric, with the Fisher information matrix being a natural choice for probability distributions (<xref ref-type="bibr" rid="R8">Amari &amp; Nagaoka, 2000</xref>). The Fisher information matrix, <inline-formula><mml:math id="M6"><mml:mi>ℱ</mml:mi></mml:math></inline-formula>, is defined as: <disp-formula id="FD5"><mml:math id="M7"><mml:mrow><mml:mi>ℱ</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>U</mml:mi><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where the expectation is taken with respect to <italic>x</italic> under the distribution <italic>p</italic>(<italic>x</italic>|<italic>θ</italic>).</p><p id="P32">The invariant Fisher kernel, <italic>K<sub>I</sub></italic>, is then defined as the inner product of Fisher scores, <italic>U<sub>x</sub></italic>, scaled by the Fisher information matrix: <disp-formula id="FD6"><mml:math id="M8"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msup><mml:mi>ℱ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula> for subjects <italic>m</italic> and <italic>n</italic>, where the Fisher information matrix accounts for the different scales of the various model parameters.</p><p id="P33">In practice, the Fisher information matrix is often disregarded because its empirical approximation is simply the covariance matrix of the Fisher scores, meaning that using it would be equivalent to whitening the scores. This makes its impact negligible as sample size increases while being computationally expensive (<xref ref-type="bibr" rid="R24">Jaakkola &amp; Haussler, 1998</xref>; <xref ref-type="bibr" rid="R46">Shawe-Taylor et al., 2004</xref>). As a result, the invariant Fisher kernel is frequently reduced to the practical Fisher kernel, <italic>K<sub>F</sub></italic>, for which the linear version is defined as: <disp-formula id="FD7"><mml:math id="M9"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mo>·</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p id="P34">The dot product is computed for all subject pairs to obtain a similarity matrix, or kernel matrix, which can be used with any suitable kernel prediction method or classifier. In our study, we adopted a linear kernel in combination with KRR.</p></sec><sec id="S8"><label>2.5</label><title>Kernel ridge regression (KRR)</title><p id="P35">After developing a kernel matrix, kernel methods can be used to determine the relationship between a subject’s time-varying activity and FC patterns and their cognitive traits.</p><p id="P36">In this study we used KRR due to its prevalence in neuroimaging literature, efficiency, and ability to achieve comparable performance to more complex approaches such as deep learning (<xref ref-type="bibr" rid="R22">He et al., 2020</xref>; <xref ref-type="bibr" rid="R33">Mihalik et al., 2019</xref>).</p><p id="P37">KRR is the kernelised version of ridge regression (<xref ref-type="bibr" rid="R42">Saunders et al., 1998</xref>). Given a subject trait to predict, <italic>y</italic>, regression coefficients, <italic>α</italic>, are determined by solving the optimisation problem: <disp-formula id="FD8"><mml:math id="M10"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>subject</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>to</mml:mtext><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mi>α</mml:mi><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula> where <italic>λ</italic> is the L2-regularisation chosen by cross-validation and <italic>K<sub>F</sub></italic> is the practical Fisher kernel. The optimal regression coefficients, <italic>α*</italic>, can then be estimated as: <disp-formula id="FD9"><mml:math id="M11"><mml:mrow><mml:msup><mml:mi>α</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi>I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>y</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>I</italic> is the identity matrix.</p><p id="P38">We used KRR to generate base-level predictions corresponding to independent runs of the HMM on the same data, for 15 cognitive traits from the UKB and HCP datasets.</p></sec><sec id="S9"><label>2.6</label><title>Stacking</title><p id="P39">Stacking, or stacked generalisations, is a method which can be employed to potentially improve the predictions generated from methods such as KRR. Specifically, stacking is a technique that combines the predictions from multiple base models by training a meta-model that uses the base model predictions as input features.</p><p id="P40">Given multiple base models, the objective is to determine optimal model coefficients, or stacking weights, for combining the base-level predictions to generate the best stacked predictions for a given subject trait. To achieve this, a common approach is to use the base-level predictions as input features in a constrained linear regression, where the model coefficients are forced to be non-negative and sum to 1 (<xref ref-type="bibr" rid="R16">Breiman, 1996</xref>): <disp-formula id="FD10"><mml:math id="M12"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>β</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mtext>β</mml:mtext><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mtext>subject</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>to</mml:mtext><mml:mi>β</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext>Σ</mml:mtext><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M13"><mml:mover accent="true"><mml:mi>Y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> is the feature matrix (formed of base-level predictions), <italic>y</italic> represents the corresponding subject traits, and <italic>β</italic> are the stacking weights to be determined.</p><p id="P41">The base-level predictions are likely to be highly correlated, particularly when generated from HMMs with identical hyperparameters. Therefore, if unconstrained least squares is used to determine the stacking weights, there is no guarantee that the resulting stacked prediction would stay within the range of the base-level predictions and generalisation may be poor (<xref ref-type="bibr" rid="R16">Breiman, 1996</xref>). However, by imposing non-negativity and sum-to-1 constraints, an “interpolating” prediction is developed (i.e., the stacked prediction remains within the range of the base-level predictions).</p><p id="P42">The primary contribution of this paper is the implementation of a stacked generalisation scheme (<xref ref-type="bibr" rid="R60">Wolpert, 1992</xref>) to combine predictions obtained from multiple HMM runs using the Fisher kernel method. After developing multiple base-level predictions, we used a two-layered nested cross-validation scheme to generate out-of-sample stacked predictions (i.e., predictions for unseen subjects).</p><p id="P43">A summary of the stacking and nested cross-validation framework is depicted in <xref ref-type="fig" rid="F2">Figure 2</xref>. Initially, the subjects were divided into 10 folds. Each fold was sequentially chosen as the outer loop test set, while the remaining 9 folds formed the outer loop training set. This outer loop served as the first layer of the nested cross-validation and was used to evaluate the performance of the stacked model. The subjects in the outer loop training set (i.e., 90% of the full sample) were further divided into 10 folds, which served as the 9 training folds and 1 test fold for the inner loops of the cross-validation. This constituted the second layer of the nested cross-validation, involving two inner loops using the same cross-validation folds. These inner loops served two purposes: optimising the regularisation parameter and determining stacking weights.</p><p id="P44">The first inner cross-validation loop (see <italic>inner loop 1</italic> in <xref ref-type="fig" rid="F2">Figure 2</xref>) was used to tune the L2-regularisation parameters of the KRR models. This process was carried out separately for each Fisher kernel matrix obtained from the respective HMM. Following that, the second inner cross-validation loop, (see <italic>inner loop 2</italic> in <xref ref-type="fig" rid="F2">Figure 2</xref>) was used to determine stacking weights. In this step, the optimised KRR regularisation parameters (from inner loop 1) were used to generated 50 base-level predictions for all inner loop subjects (or, equivalently, the outer loop training subjects; see <italic>inner loop 2</italic> in <xref ref-type="fig" rid="F2">Figure 2</xref>). These base-level predictions were subsequently used as input features in a meta-model to generate stacking weights, where the stacking weights were forced to be non-negative and sum to 1.</p><p id="P45">Finally, we developed stacked predictions for the outer loop test set. To do this, we used the optimised KRR regularisation parameters generated in inner loop 1 to generate base-level predictions for the outer loop test set. We then combined these outer loop test set predictions with the stacking weights generated from inner loop 2 (in other words the outer loop training set) to produce out-of-sample test set stacked predictions, used to evaluate model performance. We completed this analysis for 50 HMMs with fixed hyperparameter (to test run-to-run variability of the HMM), and 50 HMMs with varying hyperparameters (to test hyperparameter selection variability). This resulted in two sets of 50 base-level predictions and two corresponding stacked predictions. Through this approach, we sought to create a more robust stacked prediction with superior accuracy.</p><p id="P46">By varying the hyperparameters of the HMM, our objective was to explore different configurations that could reveal unique brain state distributions, accurately capturing information across subjects. However, adjusting these hyperparameters can sometimes lead to inadequate characterisation of brain data. For instance, the choice of the number of states can impact model stasis (<xref ref-type="bibr" rid="R2">Ahrends et al., 2022</xref>). To assess the diversity of the predictions, we examined the correlation between predictions generated by distinct HMMs to indicate if they potentially contained differential information. Additionally, we examined the accuracy of these predictions to discern whether diversity was driven by inaccurate predictions, or whether they were distinct and complementary. By combining diverse yet accurate predictions (e.g., multiple predictions which are accurate but each one explains one aspect of the data better than the others), we aimed to develop accurate subject-specific trait predictions. This approach also offers the advantage of circumventing the challenge of model selection by combining the information from multiple representations of the data.</p></sec><sec id="S10"><label>2.7</label><title>Performance Evaluation</title><p id="P47">We evaluated predictions for 16,352 subjects from UKB, and 1,001 subjects from HCP, or as many subjects as data were available for a given trait. To investigate the effectiveness of stacking, we compared the performance of the stacked prediction with the base-level predictions that were combined to generate it. We also compared our stacked prediction with the common and most simplistic form of combination, which involves taking the average of all base-level predictions so that each one contributes equally to the final prediction (<xref ref-type="bibr" rid="R30">Lincoln &amp; Skrzypekt, 1989</xref>; <xref ref-type="bibr" rid="R36">Perrone &amp; Cooper, 1992</xref>; <xref ref-type="bibr" rid="R50">Tumer &amp; Ghosht, 1996</xref>). In other words, given <italic>k</italic> base-level predictions, <inline-formula><mml:math id="M14"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>,we combine them as follows: <disp-formula id="FD11"><mml:math id="M15"><mml:mrow><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="left"><mml:mrow><mml:mtext>Simple</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>averaging</mml:mtext><mml:mo>:</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mrow><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p id="P48">Our models were assessed based on accuracy and robustness. Accuracy was measured using the coefficient of determination, R<sup>2</sup>. Given an observed subject trait, <italic>y</italic>, and our prediction, <inline-formula><mml:math id="M16"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula>, for <italic>N</italic> subjects, the coefficient of determination is given by <disp-formula id="FD12"><mml:math id="M17"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:msubsup><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M18"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is the mean of the observed subject trait over all subjects.</p><p id="P49">For a model to be robust, the model’s accuracy should be consistent across variations of the training set. Robustness is extremely important for real-world applications, but often a short-coming in neuroimaging-based prediction studies (<xref ref-type="bibr" rid="R55">Varoquaux et al., 2017</xref>).</p><p id="P50">We estimated prediction accuracy using 10-fold cross-validation. For each trait, we repeated the cross-validation process 10 times using different randomised folds, following the approach of <xref ref-type="bibr" rid="R18">Džeroski &amp; Ženko (2004)</xref>. This allowed us to assess the consistency and variance of our models’ accuracy. To assess accuracy, we calculated the mean R<sup>2</sup> scores across the 10 cross-validation iterations. To assess robustness, we examined the variance of the R<sup>2</sup> scores across the 10 iterations.</p><p id="P51">To assess the statistical significance of the differences in accuracy between our different approaches, we used Welch t-tests. Specifically, we compared the mean prediction accuracies across 15 subject traits and 10 cross-validation iterations for each dataset separately. We used Welch’s t-test since the variance between the groups compared was different between the base-level predictions from HMMs with fixed versus varying hyperparameters for certain traits (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-4</xref> for details). To account for multiple comparisons (i.e., the 30 different tests composed of the 15 comparisons between different stacking approaches for the 2 datasets), we applied Benjamini-Hochberg’s False Discovery Rate (FDR) procedure (<xref ref-type="bibr" rid="R13">Benjamini &amp; Hochberg, 1995</xref>) to correct the resulting p-values.</p><p id="P52">To investigate the robustness of stacking, we compared the variance of the distribution of accuracies across cross-validation iterations between base-level predictions and stacked predictions. Since the distributions did not follow a normal distribution for certain traits (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-5</xref> for details), we used Levene’s test to compare the distributions.</p><p id="P53">Similar to our previous analysis, p-values were corrected across using Benjamini-Hochberg’s FDR procedure (across the 60 different tests comprised of 15 from UKB and 15 from HCP tests for the two sets of Levene’s tests).</p></sec></sec><sec id="S11" sec-type="results"><label>3</label><title>Results</title><p id="P54">In this section, we investigate the effectiveness of stacking predictions using HMMs with fixed and HMMs with varying hyperparameters. We find that stacking predictions from HMMs with fixed hyperparameters generally exhibits moderate effectiveness. On the other hand, stacking predictions from HMMs with varying hyperparameters substantially improves accuracies. Although stacking performs similarly to simple averaging in many situations, we find that there are certain scenarios where stacking yields much higher accuracies. Furthermore, we show that stacking produces robust predictions, particularly when combining predictions from HMMs with varying hyperparameters. We then explore the factors contributing to the effectiveness of stacking in certain scenarios. Finally, we showcase the flexibility and power of stacking by combining predictions from static FC with those from dynamic FC.</p><sec id="S12"><label>3.1</label><title>Stacking predictions generated from HMMs with fixed hyperparameters improves prediction accuracy only moderately</title><p id="P55">Our analysis revealed that combining predictions generated from 50 HMMs with fixed hyperparameters (i.e., those with run-to-run variability caused by different initialisations and the stochasticity of the HMM inference) resulted in a higher average prediction accuracy than base-level predictions for UKB but was only effective for certain subject traits in HCP. <xref ref-type="fig" rid="F3">Figure 3</xref> provides a comparison of the distribution of explained variance (coefficients of determination; R<sup>2</sup>) between predictions generated from HMMs with fixed hyperparameters (depicted in blue) and observed subject traits for three approaches: base-level predictions (shown by the boxplots), averaging base-level predictions (shown by the crosses), and stacking them using constrained least squares (shown by the triangles).</p><p id="P56">The results for the UKB are depicted in <xref ref-type="fig" rid="F3">Figure 3a</xref>. Across 15 traits and 10 cross-validation repetitions, stacking exhibited a significantly higher average prediction accuracy compared to base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0299 vs. mean R<sup>2</sup> <sc>BASE</sc>: 0.0268; <italic>p<sub>BH</sub></italic> =0.0091). Additionally, for 10 out of the 15 traits investigated, the stacked prediction outperformed every base-level prediction. Despite this, stacking performed equally to simply averaging across the base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0299 vs. mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.0295; <italic>p<sub>BH</sub></italic> =0.499).</p><p id="P57">Although stacking base-level predictions from HMMs with fixed hyperparameters was relatively ineffective for the majority of traits in the HCP dataset, it was extremely effective for one of the subject traits (including when compared to simple averaging). <xref ref-type="fig" rid="F3">Figure 3b</xref> shows the accuracy values for all approaches for the selected traits in HCP, highlighting the relative ineffectiveness of stacking for most subject traits. Across all subject traits, stacking performed insignificantly better than base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0415 vs. mean R<sup>2</sup> <sc>BASE</sc>: 0.0374; <italic>p<sub>BH</sub></italic>=0.166); however, the improvement was driven by the specific subject trait ‘PicVocab_Unadj’, where stacking was extremely effective compared to both base-level predictions and averaging (mean R<sup>2</sup> <sc>STACKED</sc>: 0.114, compared to R<sup>2</sup> <sc>BASE</sc>: 0.0757, compared to R<sup>2</sup> <sc>AVERAGE</sc>: 0.0903). For this trait, stacking was very effective due to the high accuracy of a few base-level predictions compared to the remaining predictions, as indicated by the outliers of the boxplot in <xref ref-type="fig" rid="F3">Figure 3b</xref>. Although it is visually clear that stacking outperforms simple averaging for this trait, across all traits stacking performed similarly to averaging (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0415 vs. mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.0396; <italic>p<sub>BH</sub></italic>=0.436)</p><p id="P58">These results highlight the relative effectiveness of stacking in cases where specific random initialisations of the HMM result in significantly more accurate base-level predictions compared to other predictions. Stacking has the ability to disregard the less accurate predictions, where simply averaging across base-level predictions cannot. For the remaining traits, the extremely short boxplots highlight the similarity of prediction accuracies for base-level predictions when the HMM hyperparameters are fixed, resulting in ineffective stacking.</p></sec><sec id="S13"><label>3.2</label><title>Stacking predictions generated from HMMs with varying hyperparameters improves prediction accuracy to a larger extent</title><p id="P59">For UKB, stacked predictions obtained from HMMs with varying hyperparameters were generally more accurate than alternative approaches, as shown by <xref ref-type="fig" rid="F3">Figure 3b</xref>. Across the 15 traits and 10 cross-validation repetitions, the stacked predictions exhibited significant improvements over the base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0320 vs. mean R<sup>2</sup> <sc>BASE</sc>: 0.0254; <italic>p<sub>BH</sub></italic>=4.59e-08), but performed comparably to averaging the base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0320 vs. mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.316; <italic>p<sub>BH</sub></italic>=0.499). Notably, the stacked prediction consistently outperformed every individual base-level prediction for each subject trait, highlighting the benefit of stacking over selecting a single configuration of the HMM for predictions. Although the improvement of stacking predictions from HMMs with varying hyperparameters over HMMs with fixed hyperparameters was insignificant (mean R<sup>2</sup> <sc>VARY</sc>: 0.0320 vs. mean R<sup>2</sup> <sc>FIXED</sc>: 0.0299; <italic>p<sub>BH</sub></italic>=0.184), the boxplots in <xref ref-type="fig" rid="F3">Figure 3b</xref> show a consistent albeit small improvement across subject traits. The left panel of <xref ref-type="fig" rid="F3">Figure 3c</xref> shows the difference in these R<sup>2</sup> scores, highlighting this small but consistent improvement.</p><p id="P60">For HCP, varying the hyperparameters was also effective. Across the 15 traits and 10 cross-validation repetitions, we found that the stacked predictions outperformed the base-level predictions significantly (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0438 vs. mean R<sup>2</sup> <sc>BASE</sc>: 0.0360; <italic>p<sub>BH</sub></italic> =0.0104) but performed comparably to averaging the base-level predictions (mean R<sup>2</sup> <sc>STACKED</sc>: 0.0438 vs. mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.0446; <italic>p<sub>BH</sub></italic>=0.633). Moreover, stacking predictions from HMMs with varying hyperparameters was only insignificantly better than stacking predictions from HMMs with fixed hyperparameters, as shown by <xref ref-type="fig" rid="F3">Figure 3b</xref> and the right panel of <xref ref-type="fig" rid="F3">Figure 3c</xref> (mean R<sup>2</sup> <sc>VARY</sc>: 0.0438 vs. mean R<sup>2</sup> <sc>FIXED</sc>: 0.0415; <italic>p<sub>BH</sub></italic>=0.414).</p><p id="P61">While stacking and simple averaging produced similar results in this specific scenario, it is important to note that this may not always be the case. As illustrated in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-2</xref>, we replicated the analysis by parametrising the HMM to primarily focus on FC. In this particular configuration, stacking consistently achieved significantly higher results compared to simple averaging in UKB, and yielded much higher accuracy for certain traits in HCP. The circumstances under which stacking outperforms averaging are difficult to anticipate. Therefore, it is generally recommended to employ stacking over averaging, as it is expected to yield at least comparable if not superior performance with little extra computational expense.</p></sec><sec id="S14"><label>3.3</label><title>Stacking leads to a more robust prediction</title><p id="P62">While diversity among base-level predictions can result in improved stacked predictions (e.g., through varying the HMM hyperparameters), it is important to ensure that diversity is not driven by a lack of robustness, where model accuracies vary depending on the subjects used for training. To assess the robustness of the predictions when subjects were randomised across cross-validation folds, we conducted 10 cross-validation iterations independently for 50 HMMs with fixed hyperparameters and 50 HMMs with varying hyperparameters.</p><p id="P63">For UKB, <xref ref-type="fig" rid="F4">Figure 4a</xref> compares the variance in accuracy values for the base-level predictions and the stacked predictions across cross-validation iterations (i.e., full repeats of the nested cross-validation framework with different randomised fold distribution). Stacking predictions obtained from HMMs with fixed hyperparameters (light and dark blue boxplots) was ineffective at increasing robustness for any of the subject traits. In contrast, when stacking predictions from HMMs with varying hyperparameters (yellow and gold boxplots), our framework significantly improved robustness across cross-validation iterations for 11 of the 15 traits (p<sub>BH</sub>&lt;0.01<sup><xref ref-type="fn" rid="FN5">5</xref></sup>).</p><p id="P64">For HCP, stacking was only moderately effective at producing robust predictions, as shown in <xref ref-type="fig" rid="F4">Figure 4b</xref>. Stacking predictions obtained from HMMs with fixed hyperparameters (light and dark blue boxplots) significantly increased robustness for only 1 out of 15 subject traits (<italic>p<sub>BH</sub></italic>&lt;0.01<sup><xref ref-type="fn" rid="FN5">5</xref></sup>). Moreover, stacking predictions obtained from HMMs with varying hyperparameters (yellow and gold boxplots) increased robustness for 3` out of 15 subject traits (<italic>p<sub>BH</sub></italic>&lt;0.01<sup><xref ref-type="fn" rid="FN5">5</xref></sup>).</p><p id="P65">Overall, stacking predictions demonstrated greater robustness in UKB compared to HCP. This disparity can be attributed to the larger number of subjects in UKB, resulting in stacked predictions which were consistently accurate in UKB. Conversely, the smaller number of subjects in HCP resulted in higher variability across cross-validation iterations.</p><p id="P66">Furthermore, stacking predictions showed greater robustness when combining predictions from HMMs with varying hyperparameters. This was likely due to the limited run-to-run variability, but increased variance driven by hyperparameter selection variability. In other words, by varying the hyperparameters of the HMM, the accuracy of the base-level predictions varied greatly; however, stacking was able to consistently find an optimal combination of them, particularly in UKB.</p></sec><sec id="S15"><label>3.4</label><title>Stacking is effective when base-level predictions are diverse yet accurate</title><p id="P67">But, when is stacking most useful? To effectively stack multiple base-level predictions, the predictions should both be diverse and show a minimum level of accuracy (<xref ref-type="bibr" rid="R21">Hansen &amp; Salamon, 1990</xref>). One way to signify diversity is by seeking predictions that are less correlated with each other, indicating differential information (<xref ref-type="bibr" rid="R50">Tumer &amp; Ghosht, 1996</xref>). We found that while varying hyperparameters of the HMM generally led to more diversity and subsequently a more accurate stacked prediction, this was not true if the diversity was driven by noise.</p><p id="P68">We explored the differences in diversity induced by run-to-run and hyperparameter selection variability across datasets. As an illustrative example, <xref ref-type="fig" rid="F5">Figure 5</xref> provides a summary of our findings on the impact of both types of variability of the HMM when predicting fluid intelligence for both datasets. We observed that base-level predictions from HMMs with fixed hyperparameters (<xref ref-type="fig" rid="F5">Figure 5a</xref> and <xref ref-type="fig" rid="F5">Figure 5b</xref>) exhibited higher correlations with each other compared to predictions from HMMs with varying hyperparameters (<xref ref-type="fig" rid="F5">Figure 5c</xref> and <xref ref-type="fig" rid="F5">Figure 5d</xref>) in both datasets, and that HCP exhibited higher correlations than UKB (UKB: mean <italic>ρ</italic> <sc>FIXED</sc>: 0.973, compared to mean <italic>ρ</italic> <sc>VARYING</sc>: 0.934; HCP: mean <italic>ρ</italic> <sc>FIXED</sc>: 0.993, compared to mean <italic>ρ</italic> <sc>VARYING</sc>: 0.975).</p><p id="P69">Greater diversity (indicted by lower correlation values) generally led to improved stacked prediction. Also, stacking was more effective in UKB than in HCP, particularly when varying HMM hyperparameters, as discussed in <xref ref-type="sec" rid="S12">Sections 0</xref> and <xref ref-type="sec" rid="S13">3.2.</xref> Nonetheless, it is important to ensure that this diversity stems from complementary and meaningful patterns in the data rather than inaccurate predictions (i.e., pure noise).</p><p id="P70">To illustrate this distinction, <xref ref-type="fig" rid="F6">Figure 6</xref> considers predictions obtained from HMMs with varying hyperparameters for two cognitive traits in HCP: ‘PMAT24_A_SI’<sup><xref ref-type="fn" rid="FN6">6</xref></sup> and ‘Language_Task_Acc’<sup><xref ref-type="fn" rid="FN7">7</xref></sup>. <xref ref-type="fig" rid="F6">Figure 6a</xref> and <xref ref-type="fig" rid="F6">Figure 6b</xref> show that the base-level predictions for the PMAT24_A_SI exhibited a lower correlation between each other (mean ρ<sub>VSPLOT_OFF</sub>: 0.896) compared to Language_Task_Acc (mean ρ<sub>Language_Task_Acc</sub>: 0.981). This greater diversity in base-level predictions resulted in a more effective stacked prediction for PMAT24_A_SI (mean R<sup>2</sup> <sc>STACK</sc>: 0.0488; mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.0449; mean R<sup>2</sup> <sc>BASE</sc>: 0.0362) than for Language_Task_Acc (mean R<sup>2</sup> <sc>STACK</sc>: 0.0175; mean R<sup>2</sup> <sc>AVERAGE</sc>: 0.0260; mean R<sup>2</sup> <sc>BASE</sc>: 0.0238), as shown in <xref ref-type="fig" rid="F6">Figure 6c</xref> and <xref ref-type="fig" rid="F6">Figure 6d</xref>.</p><p id="P71">In the left panel, it is evident that the correlations among the base-level predictions for PMAT24_A_SI vary considerably (<xref ref-type="fig" rid="F6">Figure 6a</xref>), exhibiting a range of accuracies (<xref ref-type="fig" rid="F6">Figure 6c</xref>). Conversely, the right panel shows that the chosen base-level predictions (i.e., the most instrumental in forming the stacked prediction) for Language_Task_Acc generally have a higher correlation between predictions. However, predictions 1, 2, and 4 have a comparatively lower correlation with the other predictions, as shown by the light blue columns (or rows) in the heatmap (<xref ref-type="fig" rid="F6">Figure 6b</xref>). However, these three base-level predictions also have the lowest accuracy (R<sup>2</sup>) values (<xref ref-type="fig" rid="F6">Figure 6d</xref>). In other words, the diversity in the base-level predictions for Language_Task_Acc stems from inaccurate predictions. This suggests that the variability among the base-level predictions for Language_Task_Acc is likely due to estimation noise rather than capturing meaningful complementary patterns in the data.</p><p id="P72">Moreover, the inefficacy of stacking for Language_Task_Acc is further hindered by the estimation process of stacking weights, which can also introduce statistical noise. This issue arises due to a lack of robustness in HCP predictions, as described in the previous section. Consequently, the stacking process assigns non-zero stacking weights to less accurate predictions (which it should not), caused by certain models performing well during training on specific subjects, but performing poorly on the test set. This resulted in a stacked prediction for Language_Task_Acc that performs worse than many of the base-level predictions.</p><p id="P73">Overall, these observations emphasise that stacking is ineffective when the variability among base-level predictions is primarily driven by estimation noise rather than capturing distinct and informative aspects of the data.</p></sec><sec id="S16"><label>3.5</label><title>Dynamic predictions outperform static predictions with sufficient data</title><p id="P74">While dynamic models, such as the HMMs above, can potentially capture more information, predictions based on static FC may show an advantage due to their simplicity and reliability. Our approach to generating predictions from static FC was chosen to be comparable to our predictions from dynamic FC, given the current prediction algorithm. Furthermore, our goal for this analysis was to investigate if predictions from static FC can complement, or improve upon, predictions from dynamic FC. Given that the case of varying hyperparameter (exhibiting both run-to-run variability and hyperparameter selection variability) subsumes the case of fixed hyperparameters (run-to-run variability only), we therefore focus solely on predictions from dynamic FC generated from HMMs with varying hyperparameters.</p><p id="P75">For UKB, <xref ref-type="fig" rid="F7">Figure 7a</xref> shows that the predictions from static FC performed comparably to the stacked dynamic predictions, which combined base-level predictions from HMMs with varying hyperparameters (mean R<sup>2</sup> <sc>STATIC</sc>: 0.0323 vs. mean R<sup>2</sup> <sc>DYNAMIC (STACKED)</sc>: 0.0320; <italic>p<sub>BH</sub></italic> =0.889). Combining the static and dynamic base-level predictions using our stacking framework (i.e., the 50 dynamic base-level predictions and one prediction from static FC) resulted in a marginal but insignificant improvement in accuracy to stacking only the predictions from dynamic FC (mean R<sup>2</sup> <sc>STATIC</sc> + <sc>DYNAMIC (STACKED)</sc>: 0.0340 vs. mean R<sup>2</sup> <sc>DYNAMIC (STACKED)</sc>: 0.0320; <italic>p<sub>BH</sub></italic>=0.184) and to averaging across the 51 predictions (mean R<sup>2</sup> <sc>STATIC</sc> + <sc>DYNAMIC (STACKED)</sc>: 0.0340 vs. mean R<sup>2</sup> <sc>STATIC</sc> + <sc>DYNAMIC (AVERAGE)</sc>: 0.0316; <italic>p<sub>BH</sub></italic>=0.140).</p><p id="P76">For HCP, <xref ref-type="fig" rid="F7">Figure 7b</xref> highlights that the predictions from static FC performed poorly for certain subject traits (i.e., the top performing traits; ReadEnd_Unadj and PicVocab_Unadj) compared to predictions from dynamic FC. However, averaged across all subject traits, the improvement of predictions from dynamic FC over static FC was insignificant (mean R<sup>2</sup> <sc>varying (stacked)</sc>: 0.0438 vs. mean R<sup>2</sup> <sc>STATIC</sc>: 0.0394; <italic>p<sub>BH</sub></italic> =0.372). Stacking the dynamic predictions together with the static prediction resulted in comparable accuracies to only stacking the predictions from dynamic FC (mean R<sup>2</sup> <sc>DYNAMIC (STACKED)</sc>: 0.0447 vs. mean R<sup>2</sup> <sc>STATIC</sc> + <sc>DYNAMIC (STACKED)</sc>: 0.0438; <italic>p<sub>BH</sub></italic>=0.499), and to averaging across dynamic and static predictions (mean R<sup>2</sup> <sc>DYNAMIC (STACKED)</sc>: 0.0447 vs. mean R<sup>2</sup> <sc>STATIC</sc> + <sc>DYNAMIC (AVERAGE)</sc>: 0.0446; <italic>p<sub>BH</sub></italic>=0.583).</p><p id="P77">Our findings suggest that dynamic FC tends to generate more accurate predictions than static FC when there is sufficient high-quality data, and for traits that can be predicted to a certain degree of accuracy, as observed in HCP. The HCP dataset, with its 4800 time points compared to UK’s 490, provided ample data for dynamic FC to yield superior prediction accuracy. Conversely, in UKB, where scanning sessions were shorter, the performance of predictions from dynamic FC was comparable to that of static FC for the traits we examined.</p><p id="P78">As previously discussed, the approach we have focused on for developing predictions from static FC, which used full correlations between brain regions, was specifically chosen to be comparable to our predictions from dynamic FC. In addition, we investigated an alternative approach for developing predictions from static FC, which involved constructing partial correlation network matrices directly from the brain timeseries. The results of this analysis, available in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-3</xref>, demonstrate the superiority of partial correlations over full correlations.</p></sec></sec><sec id="S17" sec-type="discussion"><label>4</label><title>Discussion</title><p id="P79">Generating robust and accurate predictions of subject traits (e.g., clinical or psychological) from brain data is crucial for both the interpretation and the clinical translation of findings. Previous research has shown that dynamic (time-varying) representations of the brain can add information above and beyond that of static (time-averaged) functional representations (<xref ref-type="bibr" rid="R28">Liégeois et al., 2019</xref>; <xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>), resulting in superior prediction accuracies for behavioural traits using brain dynamics—particularly when combined with the Fisher kernel (<xref ref-type="bibr" rid="R3">Ahrends et al., 2023</xref>). To complement structural representations of the brain, and potentially improve over static FC representations, we employed generative models of brain network dynamics and extracted features suitable for subject trait prediction. By combining predictions from multiple brain dynamic models that provided distinct and complementary perspectives of the data, we presented two major benefits of stacking. Firstly, we showed that stacked predictions were more accurate and robust across cross-validation iterations than base-level predictions; this is consistent with previous work on model combination for statistical testing (<xref ref-type="bibr" rid="R59">Vidaurre et al., 2019</xref>). Secondly, we highlighted how this approach eliminates the need to arbitrarily select the ‘best’ HMM hyperparameters, and instead incorporates information from many options, ensuring superior predictions across a diverse set of subject traits (albeit at the cost of greater computational expense).</p><p id="P80">We found that variability in the base-level predictions obtained from HMMs with fixed hyperparameters (i.e., those with run-to-run variability) enhanced the stacked prediction accuracy compared to the base-level predictions for UKB but was generally ineffective for HCP. Additionally, for both datasets, when the HMM hyperparameters were fixed, stacking performed similarly to simple averaging. This outcome is not surprising, as the variation among HMMs with identical hyperparameters but different initialisations is likely to be limited, and driven by the difference in the quality of estimations rather than differences in information content.</p><p id="P81">We also found that by exploring a range of HMM hyperparameters, we generated diverse base-level predictions that were combined to produce superior stacked predictions. Previous research has shown that the selection of HMM hyperparameters can influence the sensitivity to different temporal scales in FC changes (<xref ref-type="bibr" rid="R2">Ahrends et al., 2022</xref>), and this information can be optimally combined for prediction using our approach. We found that base-level predictions obtained from HMMs with varying hyperparameters were found to be less correlated with each other, indicating diversity, and were stacked to produce more accurate predictions. Furthermore, when we repeated the analysis using an alternative parametrisation of the HMM that focused primarily on FC, stacking was particularly effective compared to simple averaging. Overall, these findings indicate that by varying the hyperparameters of the HMM, we go beyond merely reducing noise through combining predictions. Instead, we uncover unique and diverse FC patterns by using different configurations of the dynamic brain model.</p><p id="P82">It is important to appreciate the differences between the two considered datasets. In UKB, there is a large number of subjects, but a relatively short scanning time per subject, whereas in HCP we have fewer subjects but they underwent longer scanning sessions. This resulted in two differences. Firstly, our predictions in UKB were robust, whereas the accuracy of predictions in HCP was more susceptible to changes in training examples. This finding is consistent with previous research, which has shown that large sample sizes are necessary for replicable brain-phenotype associations, particularly for intelligence traits (<xref ref-type="bibr" rid="R31">Liu et al., 2023</xref>). While this limited the robustness of predictions in HCP, stacking still outperformed alternative approaches in both datasets on average. Secondly, the longer scanning session in HCP resulted in dynamic FC adding valuable information beyond that of static FC, whereas they were comparable in UKB. This suggests that in order to reveal unique information beyond that of static FC when using existing dynamic FC techniques, a dataset with sufficient quantity and quality of data per subject, as found in HCP, is more appropriate. Therefore, we suggest that our stacking framework would be particularly beneficial when applied to higher quality datasets.</p><p id="P83">It is worth mentioning that there are several measures of static FC. Our approach was chosen to be comparable to our predictions from dynamic FC, given the current prediction algorithm. Our results, that time-varying FC outperformed time-averaged FC for cognitive trait predictions in HCP, supports the conclusions in <xref ref-type="bibr" rid="R57">Vidaurre et al., 2021</xref>. However, this is not necessarily optimal, as shown in <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure SI-2</xref>. The use of partial correlation network matrices in our alternative approach resulted in improved predictions from static FC, which generally outperformed our predictions from dynamic FC (which do not use precision but covariance matrices). This is not surprising given previous evidence demonstrating the benefit of using partial correlations over full correlations (<xref ref-type="bibr" rid="R37">Pervaiz et al., 2020</xref>). Interestingly, the figure also shows the remarkable effectiveness of stacking static FC predictions from partial correlations together with dynamic FC predictions from full correlations. This further highlights the benefits of using stacking when incorporating diverse prediction approaches. A comprehensive investigation of partial and full correlations in the context of static and dynamic FC, as well as their combination through stacking, remains an area for future research. Importantly, in both our approaches, the relationship between static FC (and consequently the dynamic FC, as state covariances include static FC) and non-imaging behavioural measures can be influenced by the cross-subject variations in the spatial arrangement of functional brain regions (<xref ref-type="bibr" rid="R15">Bijsterbosch et al., 2018</xref>, <xref ref-type="bibr" rid="R15">2019</xref>).</p><p id="P84">We have shown stacking to be particularly effective in large-scale datasets such as UKB, where exploring a wide HMM hyperparameter space can be computationally expensive. To ameliorate computation costs, a preliminary exploration of the hyperparameter space could help to identify the key hyperparameters that contribute to an optimal stacked prediction. Given that the HMM has been found to be relatively stable—for example in discovering consistent resting-state topographies across small changes in the number of states (Baker et al., 2014)—using larger differences between HMM hyperparameters could reduce the number of base-level predictions that need to be generated. This drastically reduces the computation time since the fitting of the individual models is the most expensive aspect of the stacking process.</p><p id="P85">Our proposed method offers a consistent and flexible approach to integrating multiple models of brain dynamics. To further optimise its performance, we suggest diversifying the range of base-level predictions even more. For example, by applying the HMM to different timeseries from the same group of subjects, analyses from multiple brain imaging modalities (e.g., M/EEG) and/or different brain parcellations could be combined. An alternative stacking method, such as random forests, could be used to capture nonlinear relationships between the base-level predictions and subject traits, particularly if different HMM hyperparameters are optimal for different subjects. Additionally, our approach can be readily adapted to different kernel-based prediction methods or to address classification problems, such as classifying patients with brain disorders, by substituting ridge regression with kernel-based classifiers like kernel logistic regression or SVMs.</p><p id="P86">Overall, this work demonstrates that combining complementary models of brain dynamics allows us to achieve more accurate and robust predictions. The stacking framework we introduced not only provides flexibility for future exploration, but also reduces the risk of poor predictions due to trivial factors such as poor initialisations, incorrect hyperparameter choice, or cross-validation structure.</p><sec id="S18"><title>Implementation</title><p id="P87">The code used in this study was implemented in MATLAB 2019a. The MATLAB code for the HMM analysis is publicly available in the repository of the HMM-MAR toolbox at <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR">https://github.com/OHBA-analysis/HMM-MAR</ext-link>, and a Python equivalent is publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/vidaurre/glhmm/tree/main">https://github.com/vidaurre/glhmm/tree/main</ext-link>. The code for constructing the Fisher kernel can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/HMM-MAR/blob/master/utils/prediction/hmm_kernel.m">https://github.com/OHBA-analysis/HMM-MAR/blob/master/utils/prediction/hmm_kernel.m</ext-link>.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS190953-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d77aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S19"><title>Acknowledgements</title><p>D. Vidaurre is supported by a Novo Nordisk Foundation Emerging Investigator Fellowship (NNF19OC-0054895), an ERC Starting Grant (ERC-StG-2019-850404), and a DFF Project 1 from the Independent Research Fund of Denmark (2034-00054B)</p><p>This research has been conducted in part using the UK Biobank Resource under Application Number 8107. We are grateful to UK Biobank for making the data available, and to all UK Biobank study participants, who generously donated their time to make this resource possible. Analysis was carried out on the clusters at the Oxford Biomedical Research Computing (BMRC) facility. BMRC is a joint development between the Wellcome Centre for Human Genetics and the Big Data Institute, supported by Health Data Research UK and the NIHR Oxford Biomedical Research Centre.</p><p>Data were provided in part by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.</p><p>This research was funded in part by the Wellcome Trust (215573/Z/19/Z). For the purpose of Open Access, the author has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising from this submission.</p><p>We additionally thank Chetan Gohil and Rukuang Huang for helpful discussions.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P88">The initial state probabilities, <italic>π</italic>, are excluded from the prediction analysis since they contain minimal informative information for subject trait predictions at the expense of extra computation.</p></fn><fn id="FN2"><label>2</label><p id="P89">In the HMM-MAR MATLAB toolbox and the glhmm Python toolbox, this is specified by the ‘DirichletDiag’/‘dirichlet_diag’ option.</p></fn><fn id="FN3"><label>3</label><p id="P90"><italic>K</italic> = 6, <italic>δ</italic> = 10</p></fn><fn id="FN4"><label>4</label><p id="P91"><italic>K</italic> ∈ {3,6,9,12,15}, <italic>δ</italic> ∈ {10,100,1000,10000,100000}. See <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-3</xref> for full details.</p></fn><fn id="FN5"><label>5</label><p id="P92">See <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-5</xref> for p-values for each trait.</p></fn><fn id="FN6"><label>6</label><p id="P93">‘PMAT24_A_SI’ refers to the HCP trait ‘Penn Matrix Test: Total Skipped Items’, which counts the number of items not presented because the maximum number of errors allowed was reached</p></fn><fn id="FN7"><label>7</label><p id="P94">‘Language_Task_Acc’ refers to the HCP trait ‘Language Task overall accuracy’, which is the average accuracy from each condition in the language task</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abrol</surname><given-names>A</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><source>Multimodal Data Fusion of Deep Learning and Dynamic Functional Connectivity Features to Predict Alzheimer’s Disease Progression</source><conf-name>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><conf-sponsor>EMBS</conf-sponsor><year>2019</year><pub-id pub-id-type="doi">10.1109/EMBC.2019.8856500</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrends</surname><given-names>C</given-names></name><name><surname>Stevner</surname><given-names>A</given-names></name><name><surname>Pervaiz</surname><given-names>U</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><article-title>Data and model considerations for estimating time-varying functional connectivity in fMRI</article-title><source>NeuroImage</source><year>2022</year><volume>252</volume><pub-id pub-id-type="pmcid">PMC9361391</pub-id><pub-id pub-id-type="pmid">35217207</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119026</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahrends</surname><given-names>C</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><article-title>Predicting individual traits from models of brain dynamics accurately and reliably using the Fisher kernel</article-title><source>BioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.03.02.530638</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bangerter</surname><given-names>NK</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name><name><surname>Jbabdi</surname><given-names>S</given-names></name><name><surname>Hernandez-Fernandez</surname><given-names>M</given-names></name><name><surname>Vallee</surname><given-names>E</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><etal/></person-group><article-title>Image processing and Quality Control for the first 10,000 brain imaging datasets from UK Biobank</article-title><source>NeuroImage</source><year>2018</year><volume>166</volume><pub-id pub-id-type="pmcid">PMC5770339</pub-id><pub-id pub-id-type="pmid">29079522</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.10.034</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>McCarthy</surname><given-names>P</given-names></name><name><surname>Afyouni</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Bastiani</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Confound modelling in UK Biobank brain imaging</article-title><source>NeuroImage</source><year>2021</year><volume>224</volume><pub-id pub-id-type="pmcid">PMC7610719</pub-id><pub-id pub-id-type="pmid">32502668</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117002</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>EA</given-names></name><name><surname>Damaraju</surname><given-names>E</given-names></name><name><surname>Plis</surname><given-names>SM</given-names></name><name><surname>Erhardt</surname><given-names>EB</given-names></name><name><surname>Eichele</surname><given-names>T</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><article-title>Tracking whole-brain connectivity dynamics in the resting state</article-title><source>Cerebral Cortex</source><year>2014</year><volume>24</volume><issue>3</issue><pub-id pub-id-type="pmcid">PMC3920766</pub-id><pub-id pub-id-type="pmid">23146964</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs352</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>S</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><article-title>Towards stability of dynamic FC estimates in neuroimaging and electrophysiology: solutions and limits</article-title><source>BioRxiv</source><year>2023</year><pub-id pub-id-type="doi">10.1101/2023.01.18.524539</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name><name><surname>Nagaoka</surname><given-names>H</given-names></name></person-group><article-title>Methods of information geometry, volume 191 of Translations of Mathematical Monographs</article-title><source>American Mathematical Society</source><year>2000</year></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Graham</surname><given-names>MS</given-names></name><name><surname>Drobnjak</surname><given-names>I</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Filippini</surname><given-names>N</given-names></name><name><surname>Bastiani</surname><given-names>M</given-names></name></person-group><article-title>Towards a comprehensive framework for movement and distortion correction of diffusion MR images: Within volume movement</article-title><source>NeuroImage</source><year>2017</year><volume>152</volume><pub-id pub-id-type="pmcid">PMC5445723</pub-id><pub-id pub-id-type="pmid">28284799</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.02.085</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersson</surname><given-names>JLR</given-names></name><name><surname>Graham</surname><given-names>MS</given-names></name><name><surname>Zsoldos</surname><given-names>E</given-names></name><name><surname>Sotiropoulos</surname><given-names>SN</given-names></name></person-group><article-title>Incorporating outlier detection and replacement into a non-parametric framework for movement and distortion correction of diffusion MR images</article-title><source>NeuroImage</source><year>2016</year><volume>141</volume><pub-id pub-id-type="pmid">27393418</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baum</surname><given-names>LE</given-names></name><name><surname>Petrie</surname><given-names>T</given-names></name><name><surname>Soules</surname><given-names>G</given-names></name><name><surname>Weiss</surname><given-names>N</given-names></name></person-group><article-title>A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains</article-title><source>The Annals of Mathematical Statistics</source><year>1970</year><volume>41</volume><issue>1</issue><pub-id pub-id-type="doi">10.1214/aoms/1177697196</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Probabilistic Independent Component Analysis for Functional Magnetic Resonance Imaging</article-title><source>IEEE Transactions on Medical Imaging</source><year>2004</year><volume>23</volume><issue>2</issue><pub-id pub-id-type="pmid">14964560</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society: Series B (Methodological)</source><year>1995</year><volume>57</volume><issue>1</issue><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bijsterbosch</surname><given-names>JD</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Harrison</surname><given-names>SJ</given-names></name></person-group><article-title>The relationship between spatial configuration and functional connectivity of brain regions revisited</article-title><source>ELife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6541435</pub-id><pub-id pub-id-type="pmid">31066676</pub-id><pub-id pub-id-type="doi">10.7554/eLife.44890</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bijsterbosch</surname><given-names>JD</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Harrison</surname><given-names>SJ</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>The relationship between spatial configuration and functional connectivity of brain regions</article-title><source>ELife</source><year>2018</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC5860869</pub-id><pub-id pub-id-type="pmid">29451491</pub-id><pub-id pub-id-type="doi">10.7554/eLife.32992</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title>Stacked regressions</article-title><source>Machine Learning</source><year>1996</year><volume>24</volume><issue>1</issue><fpage>49</fpage><lpage>64</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dinsdale</surname><given-names>NK</given-names></name><name><surname>Bluemke</surname><given-names>E</given-names></name><name><surname>Sundaresan</surname><given-names>V</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Namburete</surname><given-names>AIL</given-names></name></person-group><article-title>Challenges for machine learning in clinical translation of big data imaging studies</article-title><source>Neuron</source><year>2022</year><volume>110</volume><issue>23</issue><fpage>3866</fpage><lpage>3881</lpage><pub-id pub-id-type="pmid">36220099</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Džeroski</surname><given-names>S</given-names></name><name><surname>Ženko</surname><given-names>B</given-names></name></person-group><article-title>Is combining classifiers with stacking better than selecting the best one?</article-title><source>Machine Learning</source><year>2004</year><volume>54</volume><issue>3</issue><pub-id pub-id-type="doi">10.1023/B:MACH.0000015881.36452.6e</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Kozynets</surname><given-names>O</given-names></name><name><surname>Sabbagh</surname><given-names>D</given-names></name><name><surname>Lemaitre</surname><given-names>G</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Liem</surname><given-names>F</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><article-title>Combining magnetoencephalography with magnetic resonance imaging enhances learning of surrogate-biomarkers</article-title><source>ELife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7308092</pub-id><pub-id pub-id-type="pmid">32423528</pub-id><pub-id pub-id-type="doi">10.7554/eLife.54055</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Salimi-Khorshidi</surname><given-names>G</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Sexton</surname><given-names>CE</given-names></name><name><surname>Zsoldos</surname><given-names>E</given-names></name><name><surname>Ebmeier</surname><given-names>KP</given-names></name><name><surname>Filippini</surname><given-names>N</given-names></name><name><surname>Mackay</surname><given-names>CE</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><etal/></person-group><article-title>ICA-based artefact removal and accelerated fMRI acquisition for improved resting state network imaging</article-title><source>NeuroImage</source><year>2014</year><volume>95</volume><pub-id pub-id-type="pmcid">PMC4154346</pub-id><pub-id pub-id-type="pmid">24657355</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.03.034</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hansen</surname><given-names>LK</given-names></name><name><surname>Salamon</surname><given-names>P</given-names></name></person-group><article-title>Neural Network Ensembles</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>1990</year><volume>12</volume><issue>10</issue><pub-id pub-id-type="doi">10.1109/34.58871</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>T</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Holmes</surname><given-names>AJ</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Bzdok</surname><given-names>D</given-names></name><name><surname>Feng</surname><given-names>J</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><article-title>Deep neural networks and kernel regression achieve comparable accuracies for functional connectivity prediction of behavior and demographics</article-title><source>NeuroImage</source><year>2020</year><volume>206</volume><pub-id pub-id-type="pmcid">PMC6984975</pub-id><pub-id pub-id-type="pmid">31610298</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116276</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaakkola</surname><given-names>T</given-names></name><name><surname>Diekhans</surname><given-names>M</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><article-title>A discriminative framework for detecting remote protein homologies</article-title><source>Journal of Computational Biology</source><year>2000</year><volume>7</volume><issue>1–2</issue><fpage>95</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">10890390</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaakkola</surname><given-names>T</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name></person-group><article-title>Exploiting generative models in discriminative classifiers</article-title><source>Advances in Neural Information Processing Systems</source><year>1998</year><volume>11</volume></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufmann</surname><given-names>T</given-names></name><name><surname>van der Meer</surname><given-names>D</given-names></name><name><surname>Doan</surname><given-names>NT</given-names></name><name><surname>Schwarz</surname><given-names>E</given-names></name><name><surname>Lund</surname><given-names>MJ</given-names></name><name><surname>Agartz</surname><given-names>I</given-names></name><name><surname>Alnæs</surname><given-names>D</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Baur-Streubel</surname><given-names>R</given-names></name><name><surname>Bertolino</surname><given-names>A</given-names></name><name><surname>Bettella</surname><given-names>F</given-names></name><etal/></person-group><article-title>Common brain disorders are associated with heritable patterns of apparent aging of the brain</article-title><source>Nature Neuroscience</source><year>2019</year><volume>22</volume><issue>10</issue><pub-id pub-id-type="pmcid">PMC6823048</pub-id><pub-id pub-id-type="pmid">31551603</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0471-7</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kavitha</surname><given-names>A</given-names></name><name><surname>Prakash</surname><given-names>SS</given-names></name><name><surname>Sreeja</surname><given-names>P</given-names></name><name><surname>Carshia</surname><given-names>AS</given-names></name></person-group><source>Investigations on the Functional connectivity disruptive patterns of progressive neurodegenerative disorders</source><conf-name>Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society</conf-name><conf-sponsor>EMBS</conf-sponsor><year>2019</year><pub-id pub-id-type="doi">10.1109/EMBC.2019.8856919</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonardi</surname><given-names>N</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name></person-group><article-title>On spurious and real fluctuations of dynamic functional connectivity during rest</article-title><source>NeuroImage</source><year>2015</year><volume>104</volume><pub-id pub-id-type="pmid">25234118</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>C</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Ge</surname><given-names>T</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><article-title>Resting brain dynamics at different timescales capture distinct aspects of human behavior</article-title><source>Nature Communications</source><year>2019</year><volume>10</volume><issue>1</issue><pub-id pub-id-type="pmcid">PMC6534566</pub-id><pub-id pub-id-type="pmid">31127095</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-10317-7</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Ziegler</surname><given-names>E</given-names></name><name><surname>Phillips</surname><given-names>C</given-names></name><name><surname>Geurts</surname><given-names>P</given-names></name><name><surname>Gómez</surname><given-names>F</given-names></name><name><surname>Bahri</surname><given-names>MA</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name><name><surname>Soddu</surname><given-names>A</given-names></name><name><surname>Vanhaudenhuyse</surname><given-names>A</given-names></name><name><surname>Laureys</surname><given-names>S</given-names></name><name><surname>Sepulchre</surname><given-names>R</given-names></name></person-group><article-title>Cerebral functional connectivity periodically (de)synchronizes with anatomical constraints</article-title><source>Brain Structure and Function</source><year>2016</year><volume>221</volume><issue>6</issue><pub-id pub-id-type="pmid">26197763</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lincoln</surname><given-names>WP</given-names></name><name><surname>Skrzypekt</surname><given-names>J</given-names></name></person-group><article-title>Synergy Of Clustering Multiple Back Propagation Networks</article-title><source>Advances in Neural Information Processing Systems</source><year>1989</year><volume>2</volume></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Abdellaoui</surname><given-names>A</given-names></name><name><surname>Verweij</surname><given-names>KJH</given-names></name><name><surname>van Wingen</surname><given-names>GA</given-names></name></person-group><article-title>Replicable brain–phenotype associations require large-scale neuroimaging data</article-title><source>Nature Human Behaviour</source><year>2023</year><pub-id pub-id-type="pmid">37365408</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masaracchia</surname><given-names>L</given-names></name><name><surname>Fredes</surname><given-names>F</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><article-title>Dissecting unsupervised learning through hidden Markov modelling in electrophysiological data</article-title><source>BioRxiv</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10625837</pub-id><pub-id pub-id-type="pmid">37403598</pub-id><pub-id pub-id-type="doi">10.1152/jn.00054.2023</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mihalik</surname><given-names>A</given-names></name><name><surname>Brudfors</surname><given-names>M</given-names></name><name><surname>Robu</surname><given-names>M</given-names></name><name><surname>Ferreira</surname><given-names>FS</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Rau</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>Blumberg</surname><given-names>SB</given-names></name><name><surname>Kanber</surname><given-names>B</given-names></name><name><surname>Tariq</surname><given-names>M</given-names></name><name><surname>Garcia</surname><given-names>ME</given-names></name><etal/></person-group><article-title>ABCD Neurocognitive Prediction Challenge 2019: Predicting Individual Fluid Intelligence Scores from Structural MRI Using Probabilistic Segmentation and Kernel Ridge Regression</article-title><source>Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11791 LNCS</source><year>2019</year><pub-id pub-id-type="doi">10.1007/978-3-030-31901-4_16</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mokhtari</surname><given-names>F</given-names></name><name><surname>Akhlaghi</surname><given-names>MI</given-names></name><name><surname>Simpson</surname><given-names>SL</given-names></name><name><surname>Wu</surname><given-names>G</given-names></name><name><surname>Laurienti</surname><given-names>PJ</given-names></name></person-group><article-title>Sliding window correlation analysis: Modulating window shape for dynamic brain connectivity in resting state</article-title><source>NeuroImage</source><year>2019</year><volume>189</volume><fpage>655</fpage><lpage>666</lpage><pub-id pub-id-type="pmcid">PMC6513676</pub-id><pub-id pub-id-type="pmid">30721750</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.001</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mourao-Miranda</surname><given-names>J</given-names></name><name><surname>Reinders</surname><given-names>AATS</given-names></name><name><surname>Rocha-Rego</surname><given-names>V</given-names></name><name><surname>Lappin</surname><given-names>J</given-names></name><name><surname>Rondina</surname><given-names>J</given-names></name><name><surname>Morgan</surname><given-names>C</given-names></name><name><surname>Morgan</surname><given-names>KD</given-names></name><name><surname>Fearon</surname><given-names>P</given-names></name><name><surname>Jones</surname><given-names>PB</given-names></name><name><surname>Doody</surname><given-names>GA</given-names></name><name><surname>Murray</surname><given-names>RM</given-names></name><name><surname>Kapur</surname><given-names>S</given-names></name><name><surname>Dazzan</surname><given-names>P</given-names></name></person-group><article-title>Individualized prediction of illness course at the first psychotic episode: A support vector machine MRI study</article-title><source>Psychological Medicine</source><year>2012</year><volume>42</volume><issue>5</issue><fpage>1037</fpage><lpage>1047</lpage><pub-id pub-id-type="pmcid">PMC3315786</pub-id><pub-id pub-id-type="pmid">22059690</pub-id><pub-id pub-id-type="doi">10.1017/S0033291711002005</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrone</surname><given-names>MP</given-names></name><name><surname>Cooper</surname><given-names>LN</given-names></name></person-group><article-title>When Networks Disagree: Ensemble Methods for Technical Report Hybrid Neural Networks Unclassified</article-title><source>Neural Networks for Speech and Image Processing</source><year>1992</year></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pervaiz</surname><given-names>U</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Optimising network modelling methods for fMRI</article-title><source>NeuroImage</source><year>2020</year><volume>211</volume><pub-id pub-id-type="pmcid">PMC7086233</pub-id><pub-id pub-id-type="pmid">32062083</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116604</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pievani</surname><given-names>M</given-names></name><name><surname>de Haan</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>Seeley</surname><given-names>WW</given-names></name><name><surname>Frisoni</surname><given-names>GB</given-names></name></person-group><article-title>Functional network disruption in the degenerative dementias</article-title><source>The Lancet Neurology</source><year>2011</year><volume>10</volume><issue>9</issue><pub-id pub-id-type="pmcid">PMC3219874</pub-id><pub-id pub-id-type="pmid">21778116</pub-id><pub-id pub-id-type="doi">10.1016/S1474-4422(11)70158-2</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Preti</surname><given-names>MG</given-names></name><name><surname>Bolton</surname><given-names>TA</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name></person-group><article-title>The dynamic functional connectome: State-of-the-art and perspectives</article-title><source>NeuroImage</source><year>2017</year><volume>160</volume><fpage>41</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">28034766</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Abeysuriya</surname><given-names>R</given-names></name><name><surname>Becker</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Task-evoked dynamic network analysis through Hidden Markov Modeling</article-title><source>Frontiers in Neuroscience</source><year>2018</year><volume>12</volume><month>AUG</month><pub-id pub-id-type="pmcid">PMC6121015</pub-id><pub-id pub-id-type="pmid">30210284</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00603</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakoğlu</surname><given-names>Ü</given-names></name><name><surname>Pearlson</surname><given-names>GD</given-names></name><name><surname>Kiehl</surname><given-names>KA</given-names></name><name><surname>Wang</surname><given-names>YM</given-names></name><name><surname>Michael</surname><given-names>AM</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><article-title>A method for evaluating dynamic functional network connectivity and task-modulation: Application to schizophrenia</article-title><source>Magnetic Resonance Materials in Physics, Biology and Medicine</source><year>2010</year><volume>23</volume><issue>5–6</issue><pub-id pub-id-type="pmcid">PMC2891285</pub-id><pub-id pub-id-type="pmid">20162320</pub-id><pub-id pub-id-type="doi">10.1007/s10334-010-0197-8</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Saunders</surname><given-names>C</given-names></name><name><surname>Gammerman</surname><given-names>A</given-names></name><name><surname>Vovk</surname><given-names>V</given-names></name></person-group><source>Ridge Regression Learning Algorithm in Dual Variables</source><conf-name>Proceedings of the 15th International Conference on Machine Learning</conf-name><year>1998</year></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>AJ</given-names></name></person-group><article-title>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond Adaptive computation and machine learning</article-title><source>Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond Adaptive computation and machine learning</source><year>2001</year></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sen</surname><given-names>B</given-names></name><name><surname>Parhi</surname><given-names>KK</given-names></name></person-group><article-title>Predicting Biological Gender and Intelligence from fMRI via Dynamic Functional Connectivity</article-title><source>IEEE Transactions on Biomedical Engineering</source><year>2021</year><volume>68</volume><issue>3</issue><pub-id pub-id-type="pmid">32746070</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahab</surname><given-names>S</given-names></name><name><surname>Mulsant</surname><given-names>BH</given-names></name><name><surname>Levesque</surname><given-names>ML</given-names></name><name><surname>Calarco</surname><given-names>N</given-names></name><name><surname>Nazeri</surname><given-names>A</given-names></name><name><surname>Wheeler</surname><given-names>AL</given-names></name><name><surname>Foussias</surname><given-names>G</given-names></name><name><surname>Rajji</surname><given-names>TK</given-names></name><name><surname>Voineskos</surname><given-names>AN</given-names></name></person-group><article-title>Brain structure, cognition, and brain age in schizophrenia, bipolar disorder, and healthy controls</article-title><source>Neuropsychopharmacology</source><year>2019</year><volume>44</volume><issue>5</issue><pub-id pub-id-type="pmcid">PMC6461913</pub-id><pub-id pub-id-type="pmid">30635616</pub-id><pub-id pub-id-type="doi">10.1038/s41386-018-0298-z</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shawe-Taylor</surname><given-names>J</given-names></name><name><surname>Cristianini</surname><given-names>N</given-names></name><etal/></person-group><source>Kernel methods for pattern analysis</source><publisher-name>Cambridge university press</publisher-name><year>2004</year></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Smith Win-Fmrib</surname><given-names>S</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Clare</surname><given-names>S</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Duff</surname><given-names>E</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Hernandez Fernandez</surname><given-names>M</given-names></name><name><surname>Flitney</surname><given-names>D</given-names></name><etal/></person-group><article-title>UK Biobank Brain Imaging Documentation UK Biobank Brain Imaging Documentation Contributors to UK Biobank Brain Imaging</article-title><year>2022</year><comment><ext-link ext-link-type="uri" xlink:href="https://www.ukbiobank.ac.uk">http://www.ukbiobank.ac.uk</ext-link></comment></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Auerbach</surname><given-names>EJ</given-names></name><name><surname>Bijsterbosch</surname><given-names>J</given-names></name><name><surname>Douaud</surname><given-names>G</given-names></name><name><surname>Duff</surname><given-names>E</given-names></name><name><surname>Feinberg</surname><given-names>DA</given-names></name><name><surname>Griffanti</surname><given-names>L</given-names></name><name><surname>Harms</surname><given-names>MP</given-names></name><name><surname>Kelly</surname><given-names>M</given-names></name><etal/></person-group><article-title>Resting-state fMRI in the Human Connectome Project</article-title><source>NeuroImage</source><year>2013</year><volume>80</volume><pub-id pub-id-type="pmcid">PMC3720828</pub-id><pub-id pub-id-type="pmid">23702415</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.039</pub-id></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sudlow</surname><given-names>C</given-names></name><name><surname>Gallacher</surname><given-names>J</given-names></name><name><surname>Allen</surname><given-names>N</given-names></name><name><surname>Beral</surname><given-names>V</given-names></name><name><surname>Burton</surname><given-names>P</given-names></name><name><surname>Danesh</surname><given-names>J</given-names></name><name><surname>Downey</surname><given-names>P</given-names></name><name><surname>Elliott</surname><given-names>P</given-names></name><name><surname>Green</surname><given-names>J</given-names></name><name><surname>Landray</surname><given-names>M</given-names></name><etal/></person-group><article-title>UK biobank: an open access resource for identifying the causes of a wide range of complex diseases of middle and old age</article-title><source>PLoS Medicine</source><year>2015</year><volume>12</volume><issue>3</issue><elocation-id>e1001779</elocation-id><pub-id pub-id-type="pmcid">PMC4380465</pub-id><pub-id pub-id-type="pmid">25826379</pub-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1001779</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname><given-names>K</given-names></name><name><surname>Ghosht</surname><given-names>J</given-names></name></person-group><article-title>Error Correlation and Error Reduction in Ensemble Classifiers</article-title><source>Connection Science</source><year>1996</year><volume>8</volume><issue>3–4</issue></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tumer</surname><given-names>K</given-names></name><name><surname>Ghosht</surname><given-names>J</given-names></name></person-group><article-title>Analysis of Decision Boundaries in Linearly Combined Neural Classifiers</article-title><source>Pattern Recognition</source><year>1996</year><volume>29</volume><issue>2</issue><fpage>341</fpage><lpage>348</lpage></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaghari</surname><given-names>D</given-names></name><name><surname>Kabir</surname><given-names>E</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><article-title>Late combination shows that MEG adds to MRI in classifying MCI versus controls</article-title><source>NeuroImage</source><year>2022</year><volume>252</volume><pub-id pub-id-type="pmcid">PMC8987738</pub-id><pub-id pub-id-type="pmid">35247546</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119054</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Barch</surname><given-names>DM</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Consortium</surname><given-names>W-MHCP</given-names></name><etal/></person-group><article-title>The WU-Minn human connectome project: an overview</article-title><source>Neuroimage</source><year>2013</year><volume>80</volume><fpage>62</fpage><lpage>79</lpage><pub-id pub-id-type="pmcid">PMC3724347</pub-id><pub-id pub-id-type="pmid">23684880</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.05.041</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Auerbach</surname><given-names>E</given-names></name><name><surname>Barch</surname><given-names>D</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Bucholz</surname><given-names>R</given-names></name><name><surname>Chang</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Curtiss</surname><given-names>SW</given-names></name><name><surname>Della Penna</surname><given-names>S</given-names></name><etal/></person-group><article-title>The Human Connectome Project: A data acquisition perspective</article-title><source>NeuroImage</source><year>2012</year><volume>62</volume><issue>4</issue><pub-id pub-id-type="pmcid">PMC3606888</pub-id><pub-id pub-id-type="pmid">22366334</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.018</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Raamana</surname><given-names>PR</given-names></name><name><surname>Engemann</surname><given-names>DA</given-names></name><name><surname>Hoyos-Idrobo</surname><given-names>A</given-names></name><name><surname>Schwartz</surname><given-names>Y</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name></person-group><article-title>Assessing and tuning brain decoders: Cross-validation, caveats, and guidelines</article-title><source>NeuroImage</source><year>2017</year><volume>145</volume><pub-id pub-id-type="pmid">27989847</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Abeysuriya</surname><given-names>R</given-names></name><name><surname>Becker</surname><given-names>R</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Discovering dynamic brain networks from big data in rest and task</article-title><source>Neuroimage</source><year>2018</year><volume>180</volume><fpage>646</fpage><lpage>656</lpage><pub-id pub-id-type="pmcid">PMC6138951</pub-id><pub-id pub-id-type="pmid">28669905</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.077</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Behavioural relevance of spontaneous, transient brain network interactions in fMRI</article-title><source>Neuroimage</source><year>2021</year><volume>229</volume><elocation-id>117713</elocation-id><pub-id pub-id-type="pmcid">PMC7994296</pub-id><pub-id pub-id-type="pmid">33421594</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117713</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><article-title>Brain network dynamics are hierarchically organized in time</article-title><source>Proceedings of the National Academy of Sciences</source><year>2017</year><volume>114</volume><issue>48</issue><fpage>12827</fpage><lpage>12832</lpage><pub-id pub-id-type="pmcid">PMC5715736</pub-id><pub-id pub-id-type="pmid">29087305</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Winkler</surname><given-names>AM</given-names></name><name><surname>Karapanagiotidis</surname><given-names>T</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><article-title>Stable between-subject statistical inference from unstable within-subject functional connectivity estimates</article-title><source>Human Brain Mapping</source><year>2019</year><volume>40</volume><issue>4</issue><pub-id pub-id-type="pmcid">PMC6492297</pub-id><pub-id pub-id-type="pmid">30357995</pub-id><pub-id pub-id-type="doi">10.1002/hbm.24442</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DH</given-names></name></person-group><article-title>Stacked generalization</article-title><source>Neural Networks</source><year>1992</year><volume>5</volume><issue>2</issue><fpage>241</fpage><lpage>259</lpage></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Ripley</surname><given-names>BD</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name></person-group><article-title>Temporal autocorrelation in univariate linear modeling of FMRI data</article-title><source>NeuroImage</source><year>2001</year><volume>14</volume><issue>6</issue><pub-id pub-id-type="pmid">11707093</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Procedure for predicting subject traits from resting-state fMRI (rfMRI) timeseries.</title><p><bold>(a) Generative Model.</bold> (i), (ii) rfMRI in groupICA parcellations with 25 ICs are concatenated across all subjects from UKB and HCP, respectively. (iii) The hidden Markov model is trained on the timeseries, where different HMM hyperparameters, such as number of states (<italic>K</italic>) or the prior probability of remaining in the same state (<italic>δ</italic>), lead to different HMM descriptions. <bold>(b) Fisher Kernel Method.</bold> (iv) The HMM consists of state-time courses and the parameters defining the model (<italic>θ</italic>), including the brain state means (<italic>μ</italic>) and covariances (<italic>Σ</italic>), and the transition probability matrix (<italic>A</italic>). (v) Fisher scores are generated for each subject’s parameter set by taking the derivative of the log-likelihood with respect to each parameter to determine those which are instrumental in forming the subject specific state-time courses. (vi) The Fisher scores are vectorised for each subject and combined to form the Fisher score feature matrix. <bold>(c) Predictive Method.</bold> (vii) We then construct the practical Fisher kernel from all subjects’ Fisher scores to form a subject-by-subject similarity matrix—the Fisher kernel matrix. (viii) The Fisher kernel is used as a predictor in a (nested) cross-validated, deconfounded kernel ridge regression model to predict subject traits, where the optimal regularisation parameters are found via 10-fold cross-validation. (ix) This last step is carried out independently to predict multiple cognitive traits.</p></caption><graphic xlink:href="EMS190953-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Cross-validation framework for stacking 50 base-level predictions developed from Fisher kernels generated from the features of 50 distinct hidden Markov models.</title><p><bold>(1)</bold> 50 group-level HMMs were run on the concatenated rfMRI timeseries across subjects, from which subjects-by-subjects similarity matrices were developed by applying the Fisher kernel method to each HMM. <bold>(2) Outer loop:</bold> the subjects were split into 10 cross-validation folds, where each fold was selected in turn as an outer loop test set (used for stacked model performance evaluation), and the remaining 9 folds are used as an outer loop training set. The training subjects were further divided into 10 cross-validation folds, which are used for two purposes. <bold>(3) Inner loop 1:</bold> this loop was used to optimise the L2-regularisation parameter, <italic>λ</italic>. <bold>(4) Inner loop 2:</bold> using the optimised <italic>λ</italic> from inner loop 1, base-level predictions were developed for all inner loop subjects (or equivalently the outer loop training subjects). <bold>(5)</bold> Separately, base-level predictions are generated for the outer loop test subjects using the optimised hyperparameter from <bold>inner loop 1</bold>. <bold>(6)</bold> The 50 base-level estimators from <bold>inner loop 2</bold> were then stacked, with the stacking weights constrained to be non-negative and sum to 1. <bold>(7)</bold> Finally, the stacking weights are combined with the base-level predictions for the outer loop test subjects to produce out-of-sample stacked predictions for a selected cognitive trait. This is repeated for all 10 outer loop cross-validation folds.</p></caption><graphic xlink:href="EMS190953-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Comparison of performance for base-level predictions and stacking predictions from HMMs with varying hyperparameters against HMMs with fixed hyperparameters.</title><p><bold>(a, b)</bold> Performance of stacking across subject traits for UKB and HCP respectively. Boxplots show the R<sup>2</sup> scores between observed subject traits and base-level predictions generated from 50 HMMs. These are compared to the R<sup>2</sup> scores when we combine the base-level predictions by taking the average of them (×) and by stacking (<styled-content style="color:#44546A">△</styled-content>). Blue represents the results of using HMMs with fixed hyperparameters. Yellow represents the results of using HMMs with varying model hyperparameters. <bold>(c)</bold> Distribution of the difference between stacking prediction using varying hyperparameters (<styled-content style="color:#918007">△</styled-content>) and fixed hyperparameters (<styled-content style="color:#44546A">△</styled-content>) across 10 cross-validation iterations and 15 cognitive traits for UKB (left) and HCP (right).</p></caption><graphic xlink:href="EMS190953-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Comparison of variance across 10 cross-validation iterations in performance for base-level predictions and using stacking of HMMs with varying hyperparameters against HMMs with fixed hyperparameters.</title><p><bold>(a)</bold> UK Biobank. <bold>(b)</bold> Human Connectome Project. ‘Base-level Predictions’ boxplots (light blue and light yellow) show the R<sup>2</sup> scores between observed subject traits and 500 base-level predictions (50 HMMs across 10 cross-validation iterations). ‘Stacked Predictions’ boxplots (dark blue and gold) show the R<sup>2</sup> scores between observed subject traits and 10 stacked predictions (for each cross-validation iteration). Blue represents the results of using HMMs with fixed model hyperparameters. Yellow represents the results of using HMMs with varying model hyperparameters. Stacking was more effective at producing a robust prediction in UKB than HCP and when combining predictions from HMMs with varying hyperparameters.</p></caption><graphic xlink:href="EMS190953-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Correlation between base-level predictions for predicting fluid intelligence in UKB and HCP subjects.</title><p>The correlations between the first 25 of the 50 base-level predictions is shown (i.e., a single run per HMM hyperparameter configuration—see <xref ref-type="supplementary-material" rid="SD1">Supplementary Table SI-3</xref> for more details). <bold>(a) – (d)</bold> Heatmaps show the mean correlation between base-level predictions across 10 cross-validation repetitions obtained from 25 independent runs of the HMM using <bold>(a)</bold> fixed hyperparameters (UKB); <bold>(b)</bold> fixed hyperparameters (HCP); <bold>(c)</bold> varying hyperparameters (UKB); <bold>(d)</bold> varying hyperparameters (HCP). <bold>(e) – (f)</bold> Distribution of off-diagonal values in the correlation matrices. The correlations between base-level predictions are comparatively lower in <bold>(e)</bold> UKB than <bold>(f)</bold> HCP, and varying the hyperparameters leads to even lower correlations. The wider range of accuracies in <bold>(f)</bold> reflects the fact that stacking in HCP is more effective for some traits than others.</p></caption><graphic xlink:href="EMS190953-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Diversity and accuracy analysis of 15 base-level predictions corresponding to the largest stacking weights from the pool of 50 models for predicting two cognitive traits from HCP.</title><p>In the top panel, correlation between base-level predictions is shown for <bold>(a)</bold> PMAT24_A_SI and <bold>(b)</bold> Language_Task_Acc. In the bottom panel, the accuracy of base-level predictions (R<sup>2</sup>) is shown for <bold>(c)</bold> PMAT24_A_SI and <bold>(d)</bold> Language_Task_Acc. The predictions were all generated from HMMs with varying hyperparameters. Higher diversity in the base level predictions resulted in more effective stacking for PMAT24_A_SI.</p></caption><graphic xlink:href="EMS190953-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Comparison of performance for static FC, base-level dynamic FC predictions, and stacking the predictions.</title><p><bold>(a)</bold> UKB. <bold>(b)</bold> HCP. Boxplots show the R<sup>2</sup> scores between observed subject traits and dynamic base-level predictions generated from 50 HMMs, and are compared to predictions from static FC (●). These individual predictions are then compared to the R<sup>2</sup> scores when we combine the base-level predictions by stacking the dynamic base-level predictions with varying HMM hyperparameters (<styled-content style="color:#918007">△</styled-content>), as well as stacking the predictions from static FC with dynamic base-level predictions (<styled-content style="color:#228B21">△</styled-content>).</p></caption><graphic xlink:href="EMS190953-f007"/></fig></floats-group></article>