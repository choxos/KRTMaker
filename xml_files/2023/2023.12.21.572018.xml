<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS193101</article-id><article-id pub-id-type="doi">10.1101/2023.12.21.572018</article-id><article-id pub-id-type="archive">PPR778217</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Syntax through rapid synaptic changes</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Sun</surname><given-names>Lin</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Manohar</surname><given-names>Sanjay G.</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib></contrib-group><aff id="A1"><label>1</label>Queen Square Institute of Neurology, University College London; London, WC1N 3BG, United Kingdom</aff><aff id="A2"><label>2</label>Nuffield Department of Clinical Neurosciences, University of Oxford; Oxford, OX3 9DU, United Kingdom</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author. <email>lin.sun1@nhs.net</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>31</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>22</day><month>12</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Syntax is a central organizing component of human language but few models explain how it may be implemented in neurons. We combined two rapid synaptic rules to demonstrate how neurons can implement a simple grammar. Words bind to syntactic roles (e.g. “dog” as subject or object) and the roles obey ordering rules (e.g. subject → verb → object), guided by predefined syntactic knowledge. We find that, like humans, the model recalls sentences better than shuffled word-lists, and it can serialize words to express an idea as a sentence. The model also supports order-free morphemic languages, exhibits syntactic priming and demonstrates typical patterns of aphasia when damaged. Crucially, it achieves these using an intuitive representation where words fill roles, allowing structured cognition.</p></abstract></article-meta></front><body><p id="P2">Why is it easier to remember “colorless green ideas sleep furiously”, than “furiously sleep ideas green colorless”? Although the sequence of words is itself new, something about the order of word <italic>types</italic> is familiar. In a grammatically correct sentence, each word has a role, which makes it more likely to be followed by a word with another particular role. This underlying syntactic representation is key both in language comprehension (<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R2">2</xref>), and production (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>). However, we do not yet have an understanding of how the brain encodes syntax, which may itself be an important mechanism for reasoning and high-level cognition (<xref ref-type="bibr" rid="R5">5</xref>, <xref ref-type="bibr" rid="R6">6</xref>).</p><p id="P3">Currently, no dynamic neural model can capture the distinction between the <italic>structure</italic> and <italic>content</italic> of thoughts. Specifically, how do brain circuits capture the fact that when we think, our sequence of thoughts has a structure that obeys a set of rules, while the contents are highly flexible?</p><p id="P4">Sequences can be understood as forward associations between items (<xref ref-type="bibr" rid="R7">7</xref>), but in the case of syntax, it is the word-<italic>roles</italic>, not words themselves, that have sequential structure. We hypothesize that working memory links each word to its role in the sentence, and those roles follow probabilistic orderings. Although this cannot account for true grammar (<xref ref-type="bibr" rid="R8">8</xref>), since it does not support hierarchical structure (<xref ref-type="bibr" rid="R9">9</xref>), it provides a biologically plausible way to understand some aspects of syntax. In this paper we propose a neural architecture that stores and generates syntactic sequences.</p><p id="P5">Many neural models of working memory can hold sequential information, storing either timing (<xref ref-type="bibr" rid="R10">10</xref>), order (<xref ref-type="bibr" rid="R23">23</xref>), or production rules (<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R13">13</xref>). Word sequences may form forward associations between words (<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R15">15</xref>) with short-term synaptic plasticity (<xref ref-type="bibr" rid="R16">16</xref>) or by associating each word with a “temporal context” (neurons that represent the current time) in a “phonological loop” (<xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R18">18</xref>). But these models ignore the rules that assign words to their structural roles.</p><p id="P6">To account for syntax, sentences can be encoded using neural states (<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R20">20</xref>) or neurons that code for word-and-role combinations (<xref ref-type="bibr" rid="R21">21</xref>). However these approaches suffer from combinatorial explosion, e.g. requiring duplicate neurons to allow encoding “dog” as either subject or object. One option is to interface neural networks with an external symbolic system (<xref ref-type="bibr" rid="R22">22</xref>). Fully neural systems include transformer-based large language models, which rely on operations that are difficult to implement using biological neurons. Recurrent neural networks may be more biologically plausible (<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R23">23</xref>) and can represent some language structures (<xref ref-type="bibr" rid="R24">24</xref>–<xref ref-type="bibr" rid="R26">26</xref>), but still lack an intuitively symbol-like architecture (<xref ref-type="bibr" rid="R27">27</xref>) (<xref ref-type="supplementary-material" rid="SD1">Supplementary background</xref>).</p><p id="P7">We propose that words and their roles are coded by two populations of neurons. Each role within a sequence is associatively bound to a different word – or equivalently, each word is tagged with a different syntactic role. While neurons in superior temporal cortex are selective for the identity of auditory stimuli (<xref ref-type="bibr" rid="R28">28</xref>), neurons in prefrontal cortex encode abstract categorization of input (<xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R32">32</xref>) and might therefore represent word roles (<xref ref-type="bibr" rid="R33">33</xref>). We implement this role tagging with rapid synaptic changes that can store <italic>bindings</italic> between words (“fillers”) and their roles. Since synapses vastly outnumber neurons, the combinatorial binding problem is solved.</p><p id="P8">We harness an idea first proposed for working memory, where contents are rapidly bound to ‘slots’ using a population of competing, flexible neurons (<xref ref-type="bibr" rid="R34">34</xref>). Here in the current model, words are bound to roles in a similar way. We use a highly simplified, hand-wired network to simulate hearing sentences, and also producing them from unordered words.</p><sec id="S1"><title>How our model works</title><p id="P9">Our approach is to tackle the minimal case, using just a handful of neurons, with hand-wired synapses and rapid plasticity. There is no training or training data, since the connections are so simple, and are pre-specified for a given language.</p><p id="P10">Word-selective neurons and role-selective neurons form a recurrent network (<xref ref-type="fig" rid="F1">Fig. 1A</xref>). The two types of neuron are bidirectionally connected, and role neurons also have directed connections to each other (<xref ref-type="supplementary-material" rid="SD1">fig. S1</xref>). To flexibly encode a new sentence, word-selective neurons are activated in sequential order. These words drive role-selective neurons according to long-term knowledge of word classes encoded in synapses between word and role neurons. A role may be filled by one of several words (e.g. a subject noun could be “dog” or “cat”), and a given word could play different roles (e.g. “dog” could be a subject noun or object noun). In addition to this drive from words, role neurons drive each other. Long-term knowledge between role neurons provides a tendency to follow a familiar order (e.g. subject → verb → object) forming a Markov chain. Together these two constraints determine the role associated with each word.</p><p id="P11">Superimposed on the long-term synapses are short-term, rapidly changing weights. These weights between the word and its role, and between a role and its successor, are rapidly strengthened via a Hebbian rule, encoding the sequence of word-role pairs into working memory. The temporarily strengthened connections generate a <italic>plastic attractor</italic> – a partly-stable sequence of states of the network, that allows the subsequent re-activation (i.e. recall) of the same sequence of words and roles. Our model runs in continuous time and is robust to the timing of inputs, maintaining information across delays using these partly-stable states.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Simulating simple grammar in working memory</title><p id="P12">We begin with the simplest possible syntax, and extend it incrementally to evaluate the model’s core capabilities.</p><p id="P13">First we simulate a grammar with only one possible syntax, and where only one word can fulfill each role (<xref ref-type="fig" rid="F1">Fig. 1B</xref>). As expected, the model recalls syntactic sentences with greater accuracy than a shuffled word-list. The pre-existing long-term syntactic knowledge means that role neurons are activated in order. However with a shuffled list, the role neurons receive two conflicting drives: the word neurons activate their corresponding roles, but this does not match the natural tendency of the role-to-role synapses to generate a grammatical sequence. Consequently, errors occur, resembling human data (<xref ref-type="bibr" rid="R35">35</xref>). Furthermore, when humans recall shuffled-order sentences, they make “grammaticalization” errors where adjacent words are more likely to fit with grammar than chance (<xref ref-type="bibr" rid="R1">1</xref>). Our model gives rise to this because role neurons tend to activate in grammatical sequences (<xref ref-type="supplementary-material" rid="SD1">supplementary text</xref>).</p><p id="P14">Next we show that the model can store which word is bound to each role (<xref ref-type="fig" rid="F1">Fig. 1C</xref>). Two word neurons can compete to drive one role neuron, for example either “dogs” or “cats” could be the subject. Similarly, a single word could fill one of two possible roles, e.g. “dogs” may play either the subject or object role in the sentence. To remember which word fills which role, the word-to-role synapses rapidly strengthen to create a new attractor state, potentially overwriting other connections of those units.</p><p id="P15">Finally, we demonstrate that the network can remember which of two syntactic structures was presented, by rapid plasticity between the role neurons (<xref ref-type="fig" rid="F1">Fig. 1D</xref>). For example, the chain of role neurons can branch as adjectives can be skipped. The network remembers which particular sentence structure was used by strengthening the role-to-role synapses. This channels the sequence of role neurons during recall. If motifs from both simulations 2 and 3 are combined, the network simultaneously remembers both the words’ roles and the branches taken (<xref ref-type="supplementary-material" rid="SD1">fig. S2A</xref>).</p><p id="P16">Since the model allows multiple role sequences and multiple roles for a word, ambiguity naturally arises. The long-term strengths of word-to-role synapses determine the probability of a word being used in a particular role, and simultaneously, the long-term strengths of role-to-role synapses determine the transition probabilities to the next role, given the history. The competition results in parsing of sentences with soft constraint satisfaction (<xref ref-type="bibr" rid="R36">36</xref>) (<xref ref-type="supplementary-material" rid="SD1">fig. S2B</xref>). The disambiguation of word class based on role-sequence is also supported by evidence from magneto-encephalography studies (<xref ref-type="bibr" rid="R37">37</xref>) (<xref ref-type="supplementary-material" rid="SD1">fig. S2C</xref>).</p></sec><sec id="S4"><title>Morphemic languages with flexible word order</title><p id="P17">In many languages, word order is not fixed. Instead, morphemes (parts of words) provide cues to a word’s role. It turns out that no extra machinery is needed for our network to model this. The word neurons function as roots (e.g. “lik-” in liking) allowing them to fill a range of roles depending on affixes (e.g. “-ing”, “-able”) (<xref ref-type="bibr" rid="R38">38</xref>). We add morpheme neurons that function like word neurons, but are distinct from words as they need to be concurrently active without mutually inhibiting word neurons. Each morpheme neuron is paired with a role neuron via a long-term, fixed connection (<xref ref-type="fig" rid="F2">Fig. 2A</xref>, bold solid arrows). During encoding, if a word requires a morpheme to determine its role, we activate both the word <italic>and</italic> morpheme neurons simultaneously. The morpheme dictates the syntactic roles attached to each word (overriding word order effects) by directly driving the corresponding role neuron.</p><p id="P18">In simulations 4-6, the network encodes and recalls sentences in a language with no constraint on word order (<xref ref-type="fig" rid="F2">Fig. 2B-D</xref>). Due to rapid changes in the role-to-role synapses, the network recalls words in the same order they were encoded, even without fixed role-ordering. Across languages, this architecture can flexibly accommodate a wide range of possible structures, from morphemic to order-based, including situations where morphemes are paired only to some words.</p></sec><sec id="S5"><title>Sentence generation: syntactic serialization</title><p id="P19">A central feature of human cognition is that we can express ideas. In particular, we arrange words into ordered sentences, to express an idea held in an internal semantic representation. We demonstrate that our model is able to serialize a “bag of words” that are fed simultaneously to the network (<xref ref-type="fig" rid="F3">Fig. 3A,B</xref>). Simultaneously activated words compete with each other, and the order in which they become active during recall is constrained by role-to-role synapses. This leads to the idea being expressed as a grammatical sequence of words. The model even inserts function words that were not present in the input (<xref ref-type="fig" rid="F3">Fig. 3C</xref>). To achieve this, a ‘conceptual’ input must be added to the word neurons (<xref ref-type="supplementary-material" rid="SD1">equation S1</xref>), and role-to-role weights must be stronger to promote automatic sequential word activation.</p><p id="P20">Neurological disorders can disrupt language production. Strikingly, some patients lose grammar without losing content (agrammatic aphasia) whereas others lose contents of speech with preserved grammar (paragrammatic aphasia). A key characteristic of agrammatism, often seen in Broca’s aphasia, is a breakdown in sentence structure despite the correct words being generated (<xref ref-type="bibr" rid="R39">39</xref>). Since we model only one neuron per role, to simulate lesions we added noise, corresponding to weaker representational strength. To model agrammatic aphasia, we introduced noise into rapid synaptic plasticity between role neurons. We further assumed that certain roles can only be filled by a limited set of words (closed-class words, e.g. prepositions). The model produces errors characteristic of agrammatic aphasia (<xref ref-type="fig" rid="F3">Fig. 3D</xref>) (<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R41">41</xref>).</p><p id="P21">In a complementary syndrome of jargon or paragrammatic aphasia, sometimes seen in Wernicke’s aphasia, speech is fluent and often grammatical but is semantically incoherent with word substitution errors. If noise is added to the long-term word-to-role knowledge with reduced rapid synaptic plasticity between word and role neurons, we find that the model produces these kinds of errors (<xref ref-type="fig" rid="F3">Fig. 3E</xref>). These results qualitatively match the archetypal neuropsychological distinctions (<xref ref-type="fig" rid="F3">Fig. 3F</xref>) (<xref ref-type="bibr" rid="R42">42</xref>).</p></sec><sec id="S6"><title>Lexical and syntactic priming</title><p id="P22">Synaptic changes also provide immediate explanations for both lexical and syntactic priming. In the model, lexical priming occurs when exposure to the same item improves retrieval and syntactic priming improves recall accuracy when target and priming sentences have the same syntax (<xref ref-type="fig" rid="F4">Fig. 4A</xref>), similar to human priming (<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R4">4</xref>, <xref ref-type="bibr" rid="R43">43</xref>–<xref ref-type="bibr" rid="R45">45</xref>). The network also takes longer to repeat a sentence when the syntax changes (<xref ref-type="bibr" rid="R46">46</xref>–<xref ref-type="bibr" rid="R49">49</xref>). Intuitively, this is because encoding a sentence creates stable attractor states, which make the network less effective at shifting to new states, yielding priming for both words in their roles, and also sequences of roles.</p><p id="P23">Syntactic priming can also bias language production (<xref ref-type="fig" rid="F4">Fig. 4B</xref>) (<xref ref-type="bibr" rid="R3">3</xref>). The network is first primed with a sentence in either the active or passive voice. Then, the content elements in a scenario are pre-associated to their appropriate semantic roles (i.e. nouns to ’agent’ or ’patient’) via semantic tags. The semantic tags behave just like the morpheme neurons, preferentially driving different role neurons. Finally, the words necessary for forming sentences are simultaneously fed to the model as a “bag of words”. The roles activate sequentially with a preference for the primed order, generating words in the order appropriate for that syntactic construction. This reproduces the empirical finding that a syntactic prime biases language production.</p><p id="P24">Semantic tags can also guide the flow of role neurons, for example allowing the phenomenon of “affix hopping” where semantic tags can control morphemes after the word to which they apply (<xref ref-type="supplementary-material" rid="SD1">fig. S5</xref>).</p></sec><sec id="S7"><title>Learning a language</title><p id="P25">So far, we assumed that long-term word-class and role-to-role knowledge are already pre-learned and that plastic attractors are created <italic>within</italic> the existing knowledge structure. Next we challenge the model to acquire syntactic knowledge using slow plasticity, by simply adding long term plasticity that mirrors short term changes.</p><p id="P26">First we ask whether the model can learn the potential roles of each word. To demonstrate this, we simplify the grammar with just two roles (<xref ref-type="fig" rid="F5">Fig. 5A</xref>). This could correspond to a “pivot grammar” (primitive two-element utterances in early childhood) (<xref ref-type="bibr" rid="R50">50</xref>) or an innate syntactic structure present in all children (<xref ref-type="bibr" rid="R51">51</xref>). Words that tend to come first in this two-word phrase bind preferentially to the first role neuron. A separate set of words tend to follow, and associate with the second role neuron, thus establishing two primitive word classes.</p><p id="P27">Second we challenge the model to learn connections between role neurons. After word-to-role mappings have been established, new orderings of these words drive new role sequences (<xref ref-type="fig" rid="F5">Fig. 5B</xref>). Frequent role orders become embedded in the long-term role-to-role weights. Even branched role-to-role connections can be learned (<xref ref-type="supplementary-material" rid="SD1">fig. S6</xref>). This aligns well with theories that propose both learning and priming might arise from a single plasticity rule (<xref ref-type="bibr" rid="R52">52</xref>).</p><p id="P28">While word classes and role sequences can be learned independently, it remains unclear how they organically emerge together in a developing child, with overlapping time frames during language acquisition.</p></sec><sec id="S8"><title>Simulating evoked potentials</title><p id="P29">The model’s response to incoming words varies dynamically, and could therefore generate physiological predictions. To simulate EEG data from a single trial, we applied a derivative, smoothing and delay to the total neural activity in the model (<xref ref-type="fig" rid="F5">Fig. 5C-D</xref>). Both violations in phrase structure and permissible morphemes produce ERPs resembling human EEG studies (<xref ref-type="bibr" rid="R53">53</xref>).</p></sec></sec><sec id="S9" sec-type="discussion"><title>Discussion</title><p id="P30">We describe a neural model that encodes words in a sentence together with their syntactic roles, separating their structure from content. It binds words to their respective roles via a rapid Hebbian rule. The model predicts sentence superiority, grammaticalization errors, types of aphasia, and priming. It allows sentence generation, and morphemic languages. The model relies on two core principles. First, winner-takes-all dynamics allows symbol-like discrete representations to activate in structured sequences (<xref ref-type="supplementary-material" rid="SD1">fig. S8</xref><bold>)</bold>. Second, Markov-like transitions between role neurons maintain structure over the words. This new architecture solves an old problem: How does the brain encode words as well as their function in a sentence?</p><p id="P31">Unlike large language models, our architecture is simple and interpretable. It may also be more biologically plausible, using only synapse-local rules, and may be realized in cortical synapses (<xref ref-type="bibr" rid="R54">54</xref>). Few neural models can <italic>generate</italic> as well as encode syntax – transformer-based neural networks achieve this using biologically implausible artifices, such as scrolling buffers and positional encoding. Our model also explains why violations of syntax lead to a larger neural response in evoked potentials, a key neural signature of syntax (<xref ref-type="bibr" rid="R55">55</xref>).</p><p id="P32">The approach allows us to model errors and response times. Lesions produce separable agrammatism and paragrammatism, often seen in stroke patients – but which are unlikely to occur in other language models. The explicit symbol-like behavior, where structural rules are separated from content, could allow this architecture to be applied to other sequential cognitive processes, such as logical reasoning or deduction. In contrast with approaches that link neural networks to pure symbolic systems (<xref ref-type="bibr" rid="R56">56</xref>, <xref ref-type="bibr" rid="R57">57</xref>) a sentence is encoded <italic>in synapses</italic>, rather than as a pattern of neural activity. One alternative solution has used oscillatory synchrony to bind words to roles, but no readout or manipulation mechanisms have been proposed yet for such systems (<xref ref-type="bibr" rid="R58">58</xref>).</p><p id="P33">Our directional plasticity requires that the role-to-role facilitation must depend on the postsynaptic role neuron also receiving inputs from word neurons. Is this plausible? We speculate that potentiation only arises if the postsynaptic action potential travels retrogradely up dendrites, but the dendrites must be concurrently activated by word axons that arrive proximally on the dendritic tree. Such 3-way gating of facilitation is a strong prediction of our model.</p><p id="P34">The simple model presented has a number of important limitations. Presently, it does not reflect the hierarchical nature of sentence structures. This means that, to encode sentences like “the dog that chases the cat runs”, the current model needs to duplicate all its syntactic rules for the subordinate clause (<xref ref-type="bibr" rid="R8">8</xref>), and the model’s linear order will not explain rules of agreement (<xref ref-type="bibr" rid="R9">9</xref>). Nor is the model able to simultaneously assign the same word to two different roles, in the example “the little star is beside a big star”. A possible mechanism for this may be to include dynamic splitting of a population of word or role neurons (<xref ref-type="bibr" rid="R59">59</xref>) whereby new subgroups might correspond to clauses which reuse instances of the same syntactic rule. This splitting could create hierarchy, characteristic of human language. We also do not model word recognition, effects of semantic context (<xref ref-type="bibr" rid="R60">60</xref>), and do not address general variable binding that would allow semantic inference (<xref ref-type="bibr" rid="R61">61</xref>).</p><p id="P35">The strengths of our approach are its breadth, accounting for cognitive, psychophysical, neurophysiological and neurological findings, while remaining so simple as to be fully transparent.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS193101-supplement-Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d83aAdKbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S10"><title>Acknowledgments</title><p>We thank Ray Jackendoff and Matthew Husband for invaluable discussions that shaped the manuscript.</p><sec id="S11"><title>Funding</title><p>National Institute for Healthcare Research (NIHR) Oxford Biomedical Research Centre (BRC) (SGM)</p><p>James S McDonnell Foundation grant (SGM)</p><p>ESRC research grant ES/S015477/1 (SGM)</p></sec></ack><sec id="S12" sec-type="data-availability"><title>Data and materials availability</title><p id="P36">All data are available in the main text or the supplementary materials.</p></sec><fn-group><fn id="FN1" fn-type="con"><p id="P37"><bold>Author contributions</bold></p><p id="P38">Conceptualization: LS, SGM</p><p id="P39">Methodology: LS, SGM</p><p id="P40">Investigation: LS, SGM</p><p id="P41">Visualization: LS, SGM</p><p id="P42">Funding acquisition: SGM</p><p id="P43">Project administration: LS, SGM</p><p id="P44">Supervision: SGM</p><p id="P45">Writing – original draft: LS, SGM</p><p id="P46">Writing – review &amp; editing: LS, SGM</p></fn><fn id="FN2" fn-type="conflict"><p id="P47"><bold>Competing interests:</bold> Authors declare that they have no competing interests.</p></fn></fn-group><ref-list><title>References and Notes</title><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>T</given-names></name><name><surname>Farrell</surname><given-names>S</given-names></name></person-group><article-title>Does syntax bias serial order reconstruction of verbal short-term memory?</article-title><source>J Mem Lang</source><year>2018</year><volume>100</volume><fpage>98</fpage><lpage>122</lpage></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perham</surname><given-names>N</given-names></name><name><surname>Marsh</surname><given-names>JE</given-names></name><name><surname>Jones</surname><given-names>DM</given-names></name></person-group><article-title>Syntax and serial recall: How language supports short-term memory for order</article-title><source>Q J Exp Psychol</source><year>2009</year><volume>62</volume><fpage>1285</fpage><lpage>1291</lpage><pub-id pub-id-type="pmid">19142831</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>J Kathryn</given-names></name></person-group><article-title>Syntactic persistence in language production</article-title><source>Cogn Psychol</source><year>1986</year><volume>18</volume><fpage>355</fpage><lpage>387</lpage></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pickering</surname><given-names>MJ</given-names></name><name><surname>Ferreira</surname><given-names>VS</given-names></name></person-group><article-title>Structural priming: a critical review</article-title><source>Psychol Bull</source><year>2008</year><volume>134</volume><fpage>427</fpage><lpage>459</lpage><pub-id pub-id-type="pmcid">PMC2657366</pub-id><pub-id pub-id-type="pmid">18444704</pub-id><pub-id pub-id-type="doi">10.1037/0033-2909.134.3.427</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verguts</surname><given-names>T</given-names></name><name><surname>De Boeck</surname><given-names>P</given-names></name></person-group><article-title>On the correlation between working memory capacity and performance on intelligence tests</article-title><source>Learn Individ Differ</source><year>2002</year><volume>13</volume><fpage>37</fpage><lpage>55</lpage></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Süß</surname><given-names>H-M</given-names></name><name><surname>Oberauer</surname><given-names>K</given-names></name><name><surname>Wittmann</surname><given-names>WW</given-names></name><name><surname>Wilhelm</surname><given-names>O</given-names></name><name><surname>Schulze</surname><given-names>R</given-names></name></person-group><article-title>Working-memory capacity explains reasoning ability—and a little bit more</article-title><source>Intelligence</source><year>2002</year><volume>30</volume><fpage>261</fpage><lpage>288</lpage></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wennekers</surname><given-names>T</given-names></name><name><surname>Garagnani</surname><given-names>M</given-names></name><name><surname>Pulvermüller</surname><given-names>F</given-names></name></person-group><article-title>Language models based on Hebbian cell assemblies</article-title><source>J Physiol Paris</source><year>2006</year><volume>100</volume><fpage>16</fpage><lpage>30</lpage><pub-id pub-id-type="pmid">17081735</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poletiek</surname><given-names>FH</given-names></name><name><surname>Monaghan</surname><given-names>P</given-names></name><name><surname>van de Velde</surname><given-names>M</given-names></name><name><surname>Bocanegra</surname><given-names>BR</given-names></name></person-group><article-title>The semantics-syntax interface: Learning grammatical categories and hierarchical syntactic structure through semantics</article-title><source>J Exp Psychol Learn Mem Cogn</source><year>2021</year><volume>47</volume><fpage>1141</fpage><lpage>1155</lpage><pub-id pub-id-type="pmid">34694843</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everaert</surname><given-names>MBH</given-names></name><name><surname>Huybregts</surname><given-names>MAC</given-names></name><name><surname>Chomsky</surname><given-names>N</given-names></name><name><surname>Berwick</surname><given-names>RC</given-names></name><name><surname>Bolhuis</surname><given-names>JJ</given-names></name></person-group><article-title>Structures, Not Strings: Linguistics as Part of the Cognitive Sciences</article-title><source>Trends Cogn Sci</source><year>2015</year><volume>19</volume><fpage>729</fpage><lpage>743</lpage><pub-id pub-id-type="pmid">26564247</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tokuhara</surname><given-names>H</given-names></name><name><surname>Fujita</surname><given-names>K</given-names></name><name><surname>Kashimori</surname><given-names>Y</given-names></name></person-group><article-title>Neural Mechanisms of Maintenance and Manipulation of Information of Temporal Sequences in Working Memory</article-title><source>Cognit Comput</source><year>2021</year><volume>13</volume><fpage>1085</fpage><lpage>1098</lpage></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>Frank</surname><given-names>MJ</given-names></name></person-group><article-title>Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia</article-title><source>Neural Comput</source><year>2006</year><volume>18</volume><fpage>283</fpage><lpage>328</lpage><pub-id pub-id-type="pmid">16378516</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruijne</surname><given-names>W</given-names></name><name><surname>Bohte</surname><given-names>SM</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Olivers</surname><given-names>CNL</given-names></name></person-group><article-title>Flexible Working Memory Through Selective Gating and Attentional Tagging</article-title><source>Neural Comput</source><year>2021</year><volume>33</volume><fpage>1</fpage><lpage>40</lpage><pub-id pub-id-type="pmid">33080159</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cer</surname><given-names>DM</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><chapter-title>Neural mechanisms of binding in the hippocampus and neocortex: insights from computational models</chapter-title><source>Handbook of Binding and Memory: Perspectives from Cognitive Neuroscience</source><publisher-name>Oxford University Press</publisher-name><year>2006</year><fpage>192</fpage><lpage>220</lpage><comment>in</comment></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zhen</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Long</surname><given-names>T</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Fang</surname><given-names>W</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name></person-group><article-title>Working Memory for Spatial Sequences: Developmental and Evolutionary Factors in Encoding Ordinal and Relational Structures</article-title><source>J Neurosci</source><year>2022</year><volume>42</volume><fpage>850</fpage><lpage>864</lpage><pub-id pub-id-type="pmcid">PMC8808738</pub-id><pub-id pub-id-type="pmid">34862186</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0603-21.2021</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rezende</surname><given-names>D</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Variational learning for recurrent spiking networks</article-title><source>Adv Neural Inf Process Syst</source><year>2011</year><volume>24</volume><comment>in</comment></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kappel</surname><given-names>D</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title>STDP installs in Winner-Take-All circuits an online approximation to hidden Markov model learning</article-title><source>PLoS Comput Biol</source><year>2014</year><volume>10</volume><elocation-id>e1003511</elocation-id><pub-id pub-id-type="pmcid">PMC3967926</pub-id><pub-id pub-id-type="pmid">24675787</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003511</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Reilly</surname><given-names>Soto</given-names></name></person-group><article-title>A model of the phonological loop: Generalization and binding</article-title><source>Adv Neural Inf Process Syst</source><volume>14</volume><comment>in</comment></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MW</given-names></name><name><surname>Kahana</surname><given-names>MJ</given-names></name></person-group><article-title>A Distributed Representation of Temporal Context</article-title><source>J Math Psychol</source><year>2002</year><volume>46</volume><fpage>269</fpage><lpage>299</lpage></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Markert</surname><given-names>H</given-names></name><name><surname>Knoblauch</surname><given-names>A</given-names></name><name><surname>Palm</surname><given-names>G</given-names></name></person-group><chapter-title>Detecting Sequences and Understanding Language with Neural Associative Memories and Cell Assemblies</chapter-title><person-group person-group-type="editor"><name><surname>Wermter</surname><given-names>S</given-names></name><name><surname>Palm</surname><given-names>G</given-names></name><name><surname>Elshaw</surname><given-names>M</given-names></name></person-group><source>Biomimetic Neural Learning for Intelligent Robots: Intelligent Systems, Cognitive Robotics, and Neuroscience</source><publisher-name>Springer Berlin Heidelberg</publisher-name><publisher-loc>Berlin, Heidelberg</publisher-loc><year>2005</year><fpage>107</fpage><lpage>117</lpage><comment>in Eds</comment></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriete</surname><given-names>T</given-names></name><name><surname>Noelle</surname><given-names>DC</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><article-title>Indirection and symbol-like processing in the prefrontal cortex and basal ganglia</article-title><source>Proc Natl Acad Sci U S A</source><year>2013</year><volume>110</volume><fpage>16390</fpage><lpage>16395</lpage><pub-id pub-id-type="pmcid">PMC3799308</pub-id><pub-id pub-id-type="pmid">24062434</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1303547110</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><article-title>Networks for memory, perception, and decision-making, and beyond to how the syntax for language might be implemented in the brain</article-title><source>Brain Res</source><year>2015</year><volume>1621</volume><fpage>316</fpage><lpage>334</lpage><pub-id pub-id-type="pmid">25239476</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hammond</surname><given-names>K</given-names></name><name><surname>Leake</surname><given-names>D</given-names></name></person-group><source>Large Language Models Need Symbolic AI</source><conf-name>17th International Workshop on Neural-Symbolic Learning and Reasoning CEUR Workshop Proceedings</conf-name><year>2023</year><comment>in <ext-link ext-link-type="uri" xlink:href="https://ceur-ws.org/Vol-3432/paper17.pdf">https://ceur-ws.org/Vol-3432/paper17.pdf</ext-link></comment></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>J</given-names></name><name><surname>Lim</surname><given-names>S</given-names></name></person-group><article-title>Unsupervised learning for robust working memory</article-title><source>PLoS Comput Biol</source><year>2022</year><volume>18</volume><elocation-id>e1009083</elocation-id><pub-id pub-id-type="pmcid">PMC9098088</pub-id><pub-id pub-id-type="pmid">35500033</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009083</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Suzgun</surname><given-names>M</given-names></name><name><surname>Gehrmann</surname><given-names>S</given-names></name><name><surname>Belinkov</surname><given-names>Y</given-names></name><name><surname>Shieber</surname><given-names>SM</given-names></name></person-group><source>LSTM Networks Can Perform Dynamic Counting</source><conf-name>Proceedings of the Workshop on Deep Learning and Formal Languages: Building Bridges</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2019</year><fpage>44</fpage><lpage>54</lpage><comment>in</comment></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Marvin</surname><given-names>R</given-names></name><name><surname>Linzen</surname><given-names>T</given-names></name></person-group><source>Targeted Syntactic Evaluation of Language Models</source><conf-name>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2018</year><fpage>1192</fpage><lpage>1202</lpage><comment>in</comment></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Blank</surname><given-names>IA</given-names></name><name><surname>Tuckute</surname><given-names>G</given-names></name><name><surname>Kauf</surname><given-names>C</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><article-title>The neural architecture of language: Integrative modeling converges on predictive processing</article-title><source>Proc Natl Acad Sci U S A</source><year>2021</year><volume>118</volume><pub-id pub-id-type="pmcid">PMC8694052</pub-id><pub-id pub-id-type="pmid">34737231</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2105646118</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Al Roumi</surname><given-names>F</given-names></name><name><surname>Lakretz</surname><given-names>Y</given-names></name><name><surname>Planton</surname><given-names>S</given-names></name><name><surname>Sablé-Meyer</surname><given-names>M</given-names></name></person-group><article-title>Symbols and mental programs: a hypothesis about human singularity</article-title><source>Trends Cogn Sci</source><year>2022</year><volume>26</volume><fpage>751</fpage><lpage>766</lpage><pub-id pub-id-type="pmid">35933289</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><year>2000</year><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="pmid">10659849</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>A comparison of primate prefrontal and inferior temporal cortices during visual categorization</article-title><source>J Neurosci</source><year>2003</year><volume>23</volume><fpage>5235</fpage><lpage>5246</lpage><pub-id pub-id-type="pmcid">PMC6741148</pub-id><pub-id pub-id-type="pmid">12832548</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.23-12-05235.2003</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanji</surname><given-names>J</given-names></name><name><surname>Shima</surname><given-names>K</given-names></name><name><surname>Mushiake</surname><given-names>H</given-names></name></person-group><article-title>Concept-based behavioral planning and the lateral prefrontal cortex</article-title><source>Trends Cogn Sci</source><year>2007</year><volume>11</volume><fpage>528</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">18024183</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shima</surname><given-names>K</given-names></name><name><surname>Isoda</surname><given-names>M</given-names></name><name><surname>Mushiake</surname><given-names>H</given-names></name><name><surname>Tanji</surname><given-names>J</given-names></name></person-group><article-title>Categorization of behavioural sequences in the prefrontal cortex</article-title><source>Nature</source><year>2007</year><volume>445</volume><fpage>315</fpage><lpage>318</lpage><pub-id pub-id-type="pmid">17183266</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antzoulatos</surname><given-names>EG</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name></person-group><article-title>Increases in functional connectivity between prefrontal cortex and striatum during category learning</article-title><source>Neuron</source><year>2014</year><volume>83</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="pmcid">PMC4098789</pub-id><pub-id pub-id-type="pmid">24930701</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.005</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Neural dynamics of phoneme sequences reveal position-invariant code for content and order</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>6606</elocation-id><pub-id pub-id-type="pmcid">PMC9633780</pub-id><pub-id pub-id-type="pmid">36329058</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-34326-1</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manohar</surname><given-names>SG</given-names></name><name><surname>Zokaei</surname><given-names>N</given-names></name><name><surname>Fallon</surname><given-names>SJ</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Husain</surname><given-names>M</given-names></name></person-group><article-title>Neural mechanisms of attending to items in working memory</article-title><source>Neurosci Biobehav Rev</source><year>2019</year><volume>101</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC6525322</pub-id><pub-id pub-id-type="pmid">30922977</pub-id><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.03.017</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>RJ</given-names></name><name><surname>Hitch</surname><given-names>GJ</given-names></name><name><surname>Baddeley</surname><given-names>AD</given-names></name></person-group><article-title>Exploring the sentence advantage in working memory: Insights from serial recall and recognition</article-title><source>Q J Exp Psychol</source><year>2018</year><volume>71</volume><fpage>2571</fpage><lpage>2585</lpage></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cho</surname><given-names>PW</given-names></name><name><surname>Goldrick</surname><given-names>M</given-names></name><name><surname>Smolensky</surname><given-names>P</given-names></name></person-group><article-title>Incremental parsing in a continuous dynamical system: sentence processing in Gradient Symbolic Computation</article-title><source>Linguistics Vanguard</source><year>2017</year><volume>3</volume><pub-id pub-id-type="doi">10.1515/lingvan-2016-0105</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gwilliams</surname><given-names>L</given-names></name><name><surname>Marantz</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>J-R</given-names></name></person-group><article-title>Top-down information shapes lexical processing when listening to continuous speech</article-title><source>Language, Cognition and Neuroscience</source><year>2023</year><pub-id pub-id-type="doi">10.1080/23273798.2023.2171072</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>SR</given-names></name><name><surname>Aronoff</surname><given-names>M</given-names></name><name><surname>Baerman</surname><given-names>M</given-names></name><name><surname>Carstairs-Mccarthy</surname><given-names>A</given-names></name><name><surname>Mugdan</surname><given-names>J</given-names></name></person-group><chapter-title>The Morpheme: Its Nature and Use</chapter-title><source>The Oxford Handbook of Inflection</source><publisher-name>Oxford University Press</publisher-name><year>2016</year><fpage>11</fpage><lpage>34</lpage><comment>in</comment></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname><given-names>EA</given-names></name><name><surname>Friederici</surname><given-names>AD</given-names></name><name><surname>Wulfeck</surname><given-names>BB</given-names></name><name><surname>Juarez</surname><given-names>LA</given-names></name></person-group><article-title>On the preservation of word order in aphasia: cross-linguistic evidence</article-title><source>Brain Lang</source><year>1988</year><volume>33</volume><fpage>323</fpage><lpage>364</lpage><pub-id pub-id-type="pmid">3359173</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martínez-Ferreiro</surname><given-names>S</given-names></name><name><surname>Ishkhanyan</surname><given-names>B</given-names></name><name><surname>Rosell-Clarí</surname><given-names>V</given-names></name><name><surname>Boye</surname><given-names>K</given-names></name></person-group><article-title>Prepositions and pronouns in connected discourse of individuals with aphasia</article-title><source>Clin Linguist Phon</source><year>2019</year><volume>33</volume><fpage>497</fpage><lpage>517</lpage><pub-id pub-id-type="pmid">30526143</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname><given-names>AD</given-names></name></person-group><article-title>Syntactic and semantic processes in aphasic deficits: the availability of prepositions</article-title><source>Brain Lang</source><year>1982</year><volume>15</volume><fpage>249</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">7074344</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caramazza</surname><given-names>A</given-names></name><name><surname>Berndt</surname><given-names>RS</given-names></name><name><surname>Basili</surname><given-names>AG</given-names></name><name><surname>Koller</surname><given-names>JJ</given-names></name></person-group><article-title>Syntactic Processing Deficits in Aphasia</article-title><source>Cortex</source><year>1981</year><volume>17</volume><fpage>333</fpage><lpage>347</lpage><pub-id pub-id-type="pmid">7333108</pub-id></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tulving</surname><given-names>E</given-names></name><name><surname>Schacter</surname><given-names>DL</given-names></name></person-group><article-title>Priming and human memory systems</article-title><source>Science</source><year>1990</year><volume>247</volume><fpage>301</fpage><lpage>306</lpage><pub-id pub-id-type="pmid">2296719</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>K</given-names></name><name><surname>Loebell</surname><given-names>H</given-names></name><name><surname>Morey</surname><given-names>R</given-names></name></person-group><article-title>From conceptual roles to structural relations: bridging the syntactic cleft</article-title><source>Psychol Rev</source><year>1992</year><volume>99</volume><fpage>150</fpage><lpage>171</lpage><pub-id pub-id-type="pmid">1546115</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>K</given-names></name><name><surname>Griffin</surname><given-names>ZM</given-names></name></person-group><article-title>The persistence of structural priming: transient activation or implicit learning?</article-title><source>J Exp Psychol Gen</source><year>2000</year><volume>129</volume><fpage>177</fpage><lpage>192</lpage><pub-id pub-id-type="pmid">10868333</pub-id></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>F</given-names></name><name><surname>Dell</surname><given-names>GS</given-names></name><name><surname>Bock</surname><given-names>K</given-names></name></person-group><article-title>Becoming syntactic</article-title><source>Psychol Rev</source><year>2006</year><volume>113</volume><fpage>234</fpage><lpage>272</lpage><pub-id pub-id-type="pmid">16637761</pub-id></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>G</given-names></name></person-group><source>thesis</source><year>2009</year><publisher-name>The University of Edinburgh</publisher-name></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaeger</surname><given-names>TF</given-names></name><name><surname>Snider</surname><given-names>NE</given-names></name></person-group><article-title>Alignment as a consequence of expectation adaptation: syntactic priming is affected by the prime’s prediction error given both prior and recent experience</article-title><source>Cognition</source><year>2013</year><volume>127</volume><fpage>57</fpage><lpage>83</lpage><pub-id pub-id-type="pmcid">PMC7313543</pub-id><pub-id pub-id-type="pmid">23354056</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2012.10.013</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reitter</surname><given-names>D</given-names></name><name><surname>Keller</surname><given-names>F</given-names></name><name><surname>Moore</surname><given-names>JD</given-names></name></person-group><article-title>A computational cognitive model of syntactic priming</article-title><source>Cogn Sci</source><year>2011</year><volume>35</volume><fpage>587</fpage><lpage>637</lpage><pub-id pub-id-type="pmid">21564266</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braine</surname><given-names>MDS</given-names></name></person-group><article-title>The Ontogeny of English Phrase Structure: The First Phase</article-title><source>Language</source><year>1963</year><volume>39</volume><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Giusti</surname><given-names>G</given-names></name><name><surname>Gozzi</surname><given-names>R</given-names></name></person-group><source>The acquisition of determiners: Evidence for the Full Competence Hypothesis</source><person-group person-group-type="editor"><name><surname>Belletti</surname><given-names>A</given-names></name><name><surname>Bennati</surname><given-names>E</given-names></name><name><surname>Chesi</surname><given-names>C</given-names></name><name><surname>Di Domenico</surname><given-names>E</given-names></name><name><surname>Ferrari</surname><given-names>I</given-names></name></person-group><conf-name>Language Acquisition and Development: Proceedings of GALA2005</conf-name><publisher-name>Cambridge Scholars Publishing</publisher-name><year>2006</year><fpage>232</fpage><lpage>237</lpage><comment>in Eds</comment></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>F</given-names></name><name><surname>Dell</surname><given-names>GS</given-names></name><name><surname>Bock</surname><given-names>K</given-names></name><name><surname>Griffin</surname><given-names>ZM</given-names></name></person-group><article-title>Structural priming as implicit learning: a comparison of models of sentence production</article-title><source>J Psycholinguist Res</source><year>2000</year><volume>29</volume><fpage>217</fpage><lpage>229</lpage><pub-id pub-id-type="pmid">10709186</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>Brown</surname><given-names>C</given-names></name><name><surname>Groothusen</surname><given-names>J</given-names></name></person-group><article-title>The syntactic positive shift (sps) as an erp measure of syntactic processing</article-title><source>Lang Cogn Process</source><year>1993</year><volume>8</volume><fpage>439</fpage><lpage>483</lpage></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiebig</surname><given-names>F</given-names></name><name><surname>Lansner</surname><given-names>A</given-names></name></person-group><article-title>A Spiking Working Memory Model Based on Hebbian Short-Term Potentiation</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="pmcid">PMC5214637</pub-id><pub-id pub-id-type="pmid">28053032</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1989-16.2016</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clardy</surname><given-names>SM</given-names></name><name><surname>Hayden</surname><given-names>ME</given-names></name><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Reilly</surname><given-names>EL</given-names></name></person-group><article-title>Speech evoked potentials in response to grammatical structures</article-title><source>J Acoust Soc Am</source><year>1979</year><volume>65</volume><pub-id pub-id-type="doi">10.1121/1.2016949</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>H</given-names></name><name><surname>Jiajun</surname><given-names>W</given-names></name><name><surname>Levy</surname><given-names>RP</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name></person-group><source>Grammar-Based Grounded Lexicon Learning</source><conf-name>35th Conference on Neural Information Processing Systems</conf-name><year>2021</year><comment>in</comment></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smolensky</surname><given-names>P</given-names></name></person-group><article-title>Tensor product variable binding and the representation of symbolic structures in connectionist systems</article-title><source>Artif Intell</source><year>1990</year><volume>46</volume><fpage>159</fpage><lpage>216</lpage></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segaert</surname><given-names>K</given-names></name><name><surname>Mazaheri</surname><given-names>A</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><article-title>Binding language: structuring sentences through precisely timed oscillatory mechanisms</article-title><source>Eur J Neurosci</source><year>2018</year><volume>48</volume><fpage>2651</fpage><lpage>2662</lpage><pub-id pub-id-type="pmid">29283204</pub-id></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hayworth</surname><given-names>KJ</given-names></name></person-group><article-title>Dynamically partitionable autoassociative networks as a solution to the neural binding problem</article-title><source>Front Comput Neurosci</source><year>2012</year><volume>6</volume><fpage>73</fpage><pub-id pub-id-type="pmcid">PMC3460218</pub-id><pub-id pub-id-type="pmid">23060784</pub-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00073</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ralph</surname><given-names>MAL</given-names></name><name><surname>Jefferies</surname><given-names>E</given-names></name><name><surname>Patterson</surname><given-names>K</given-names></name><name><surname>Rogers</surname><given-names>TT</given-names></name></person-group><article-title>The neural and computational bases of semantic cognition</article-title><source>Nat Rev Neurosci</source><year>2017</year><volume>18</volume><fpage>42</fpage><lpage>55</lpage><pub-id pub-id-type="pmid">27881854</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Feldman</surname><given-names>J</given-names></name></person-group><article-title>The neural binding problem(s)</article-title><source>Cogn Neurodyn</source><year>2013</year><volume>7</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="pmcid">PMC3538094</pub-id><pub-id pub-id-type="pmid">24427186</pub-id><pub-id pub-id-type="doi">10.1007/s11571-012-9219-8</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>N</surname><given-names>MK</given-names></name><name><surname>Rao</surname><given-names>KS Prema</given-names></name></person-group><article-title>Processing syntax: perspectives on language specificity</article-title><source>Int J Neurosci</source><year>2020</year><volume>130</volume><fpage>841</fpage><lpage>851</lpage><pub-id pub-id-type="pmid">31858846</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glushko</surname><given-names>A</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Steinhauer</surname><given-names>K</given-names></name></person-group><article-title>Overt and implicit prosody contribute to neurophysiological responses previously attributed to grammatical processing</article-title><source>Sci Rep</source><year>2022</year><volume>12</volume><elocation-id>14759</elocation-id><pub-id pub-id-type="pmcid">PMC9427746</pub-id><pub-id pub-id-type="pmid">36042220</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-18162-3</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname><given-names>P</given-names></name></person-group><article-title>How the brain solves the binding problem for language: a neurocomputational model of syntactic processing</article-title><source>Neuroimage</source><year>2003</year><volume>20</volume><supplement>Suppl 1</supplement><fpage>S18</fpage><lpage>29</lpage><pub-id pub-id-type="pmid">14597293</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Warstadt</surname><given-names>A</given-names></name><name><surname>Cao</surname><given-names>Y</given-names></name><name><surname>Grosu</surname><given-names>I</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Blix</surname><given-names>H</given-names></name><name><surname>Nie</surname><given-names>Y</given-names></name><name><surname>Alsop</surname><given-names>A</given-names></name><name><surname>Bordia</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Parrish</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>S-F</given-names></name><etal/></person-group><source>Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs</source><conf-name>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</conf-name><conf-sponsor>Association for Computational Lingusitics</conf-sponsor><year>2019</year><fpage>2877</fpage><lpage>2887</lpage><comment>in</comment></element-citation></ref><ref id="R66"><label>66</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Clouatre</surname><given-names>L</given-names></name><name><surname>Parthasarathi</surname><given-names>P</given-names></name><name><surname>Zouaq</surname><given-names>A</given-names></name><name><surname>Chandar</surname><given-names>S</given-names></name></person-group><source>Local Structure Matters Most: Perturbation Study in NLU</source><conf-name>Findings of the Association for Computational Linguistics: ACL 2022</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2022</year><fpage>3712</fpage><lpage>3731</lpage><comment>in</comment></element-citation></ref><ref id="R67"><label>67</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linzen</surname><given-names>T</given-names></name><name><surname>Dupoux</surname><given-names>E</given-names></name><name><surname>Goldberg</surname><given-names>Y</given-names></name></person-group><article-title>Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies</article-title><source>Transactions of the Association for Computational Linguistics</source><year>2016</year><volume>4</volume><fpage>521</fpage><lpage>535</lpage></element-citation></ref><ref id="R68"><label>68</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bhatt</surname><given-names>G</given-names></name><name><surname>Bansal</surname><given-names>H</given-names></name><name><surname>Singh</surname><given-names>R</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name></person-group><source>How much complexity does an RNN architecture need to learn syntax-sensitive dependencies?</source><conf-name>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><conf-loc>Stroudsburg, PA, USA</conf-loc><year>2020</year><comment>in <ext-link ext-link-type="uri" xlink:href="https://www.aclweb.org/anthology/2020.acl-srw.33">https://www.aclweb.org/anthology/2020.acl-srw.33</ext-link></comment></element-citation></ref><ref id="R69"><label>69</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wilcox</surname><given-names>E</given-names></name><name><surname>Qian</surname><given-names>P</given-names></name><name><surname>Futrell</surname><given-names>R</given-names></name><name><surname>Ballesteros</surname><given-names>M</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name></person-group><source>Structural Supervision Improves Learning of Non-Local Grammatical Dependencies</source><conf-name>Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2019</year><fpage>3302</fpage><lpage>3312</lpage><comment>in</comment></element-citation></ref><ref id="R70"><label>70</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>P</given-names></name><name><surname>Naseem</surname><given-names>T</given-names></name><name><surname>Levy</surname><given-names>R</given-names></name><name><surname>Astudillo</surname><given-names>RF</given-names></name></person-group><source>Structural Guidance for Transformer Language Models</source><conf-name>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</conf-name><conf-sponsor>Association for Computational Linguistics</conf-sponsor><year>2021</year><fpage>3735</fpage><lpage>3745</lpage><comment>in</comment></element-citation></ref><ref id="R71"><label>71</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>JR</given-names></name><name><surname>Dyer</surname><given-names>C</given-names></name><name><surname>Kuncoro</surname><given-names>A</given-names></name><name><surname>Hale</surname><given-names>JT</given-names></name></person-group><article-title>Localizing syntactic predictions using recurrent neural network grammars</article-title><source>Neuropsychologia</source><year>2020</year><volume>146</volume><elocation-id>107479</elocation-id><pub-id pub-id-type="pmid">32428530</pub-id></element-citation></ref><ref id="R72"><label>72</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kuncoro</surname><given-names>A</given-names></name><name><surname>Ballesteros</surname><given-names>M</given-names></name><name><surname>Kong</surname><given-names>L</given-names></name><name><surname>Dyer</surname><given-names>C</given-names></name><name><surname>Neubig</surname><given-names>G</given-names></name><name><surname>Smith</surname><given-names>NA</given-names></name></person-group><source>What Do Recurrent Neural Network Grammars Learn About Syntax?</source><conf-name>Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers</conf-name><year>2017</year><fpage>1249</fpage><lpage>1258</lpage><comment>in</comment></element-citation></ref><ref id="R73"><label>73</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russin</surname><given-names>J</given-names></name><name><surname>Jo</surname><given-names>J</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title>Compositional generalization in a deep seq2seq model by separating syntax and semantics</article-title><source>arXiv [cs.LG]</source><year>2019</year><comment>available at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1904.09708">http://arxiv.org/abs/1904.09708</ext-link></comment></element-citation></ref><ref id="R74"><label>74</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vosse</surname><given-names>T</given-names></name><name><surname>Kempen</surname><given-names>G</given-names></name></person-group><article-title>Syntactic structure assembly in human parsing: a computational model based on competitive inhibition and a lexicalist grammar</article-title><source>Cognition</source><year>2000</year><volume>75</volume><fpage>105</fpage><lpage>143</lpage><pub-id pub-id-type="pmid">10771275</pub-id></element-citation></ref><ref id="R75"><label>75</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grodzinsky</surname><given-names>Y</given-names></name></person-group><article-title>The neurology of syntax: Language use without Broca’s area</article-title><source>Behav Brain Sci</source><year>2000</year><volume>23</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="pmid">11303337</pub-id></element-citation></ref><ref id="R76"><label>76</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name></person-group><article-title>The Spectrum of Aphasia Subtypes and Etiology in Subacute Stroke</article-title><source>J Stroke Cerebrovasc Dis</source><year>2013</year><volume>22</volume><fpage>1385</fpage><lpage>1392</lpage><pub-id pub-id-type="pmid">23680689</pub-id></element-citation></ref><ref id="R77"><label>77</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hillis</surname><given-names>AE</given-names></name><name><surname>Boatman</surname><given-names>D</given-names></name><name><surname>Hart</surname><given-names>J</given-names></name><name><surname>Gordon</surname><given-names>B</given-names></name></person-group><article-title>Making sense out of jargon: A neurolinguistic and computational account of jargon aphasia: Neurology: Vol 53, No 8</article-title><source>Neurology</source><volume>53</volume><fpage>1813</fpage><lpage>1824</lpage><pub-id pub-id-type="pmid">10563633</pub-id></element-citation></ref><ref id="R78"><label>78</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roelofs</surname><given-names>A</given-names></name></person-group><article-title>A neurocognitive computational account of word production, comprehension, and repetition in primary progressive aphasia</article-title><source>Brain Lang</source><year>2022</year><volume>227</volume><elocation-id>105094</elocation-id><pub-id pub-id-type="pmid">35202892</pub-id></element-citation></ref><ref id="R79"><label>79</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>Y-N</given-names></name><name><surname>Ralph</surname><given-names>MA Lambon</given-names></name></person-group><article-title>A unified neurocomputational bilateral model of spoken language production in healthy participants and recovery in poststroke aphasia</article-title><source>Proc Natl Acad Sci U S A</source><year>2020</year><volume>117</volume><fpage>32779</fpage><lpage>32790</lpage><pub-id pub-id-type="pmcid">PMC7768768</pub-id><pub-id pub-id-type="pmid">33273118</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2010193117</pub-id></element-citation></ref><ref id="R80"><label>80</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodd</surname><given-names>JM</given-names></name><name><surname>Vitello</surname><given-names>S</given-names></name><name><surname>Woollams</surname><given-names>AM</given-names></name><name><surname>Adank</surname><given-names>P</given-names></name></person-group><article-title>Localising semantic and syntactic processing in spoken and written language comprehension: an Activation Likelihood Estimation meta-analysis</article-title><source>Brain Lang</source><year>2015</year><volume>141</volume><fpage>89</fpage><lpage>102</lpage><pub-id pub-id-type="pmid">25576690</pub-id></element-citation></ref><ref id="R81"><label>81</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vigneau</surname><given-names>M</given-names></name><name><surname>Beaucousin</surname><given-names>V</given-names></name><name><surname>Hervé</surname><given-names>PY</given-names></name><name><surname>Duffau</surname><given-names>H</given-names></name><name><surname>Crivello</surname><given-names>F</given-names></name><name><surname>Houdé</surname><given-names>O</given-names></name><name><surname>Mazoyer</surname><given-names>B</given-names></name><name><surname>Tzourio-Mazoyer</surname><given-names>N</given-names></name></person-group><article-title>Meta-analyzing left hemisphere language areas: phonology, semantics, and sentence processing</article-title><source>Neuroimage</source><year>2006</year><volume>30</volume><fpage>1414</fpage><lpage>1432</lpage><pub-id pub-id-type="pmid">16413796</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Fig. 1</label><caption><title>Simulations of simple grammar with our model.</title><p>(<bold>A</bold>) Architecture of our model comprising word and role-selective neurons in a rapidly reconfigurable neural network. The word neurons compete with each other via fixed interactions (W<sup><italic>ff</italic></sup>), but connect reciprocally to and from the role neurons (W<sup><italic>fr</italic></sup>). The role neurons also compete amongst themselves (W<sup><italic>rr</italic></sup>), with some role sequences being prepotent. (<bold>B</bold>) Simulation 1: the trivial case of non-branching grammar (i). The model receives a recall cue to generate the input - either a (ii) syntactic sentence or a (iii) shuffled word-list. (iv) The model demonstrates better recall accuracy for a sentence compared to a shuffled word-list, comparable to human data from Allen et al (2018). (v) In fact, recall of the shuffled word-list tends to produce syntactic word pairs (“grammaticalization errors”) above chance, comparable to human data from Jones &amp; Farrell (2018). (<bold>C</bold>) Simulation 2: (i) The network can sequentially encode two sentences with the same syntax but different words. Both nouns “dogs” and “cats” are able to bind to either the subject or object role neurons. (ii) Recall of the first sentence input where “dogs” is the subject and “cats” is the object. (iii) Subsequent recall of the second sentence, where “dogs” and “cats” switched positions. (<bold>D</bold>) Simulation 3: (i) The network can encode sentences with different syntactic structures. (ii) The network encodes and recalls a first sentence with an adjective, and then (iii) a sentence without an adjective. *p&lt;0.05; **p&lt;0.01; ***p &lt; 0.001, error bars are standard error of mean (SEM). Black arrows: connections with predefined long-term knowledge. Red rounded arrow: inhibitory connections.</p></caption><graphic xlink:href="EMS193101-f001"/></fig><fig id="F2" position="float"><label>Fig. 2</label><caption><title>Morphemes cue word roles.</title><p>(<bold>A</bold>) In this network, morphemes accompany word input in a language with no long-term knowledge of syntactic structure. Each role neuron has no bias to follow any other role neuron (broken arrows, not all shown). Therefore, the order in which the words are presented during the encoding phase will determine their order during recall. Bold solid arrows: fixed strong connection between pairs of morpheme and role neurons. (<bold>B</bold>) Simulation 4: Encoding and recall of an input sentence with the structure “verb-subject-object”. (<bold>C</bold>) Simulation 5: Same for “subject-object-verb”. (<bold>D</bold>) Simulation 6: Same for “object-subject-verb” but with the nouns in the same serial position as <bold>C</bold>.</p></caption><graphic xlink:href="EMS193101-f002"/></fig><fig id="F3" position="float"><label>Fig. 3</label><caption><title>Sentence generation and aphasia.</title><p>(<bold>A</bold>) Network architecture for simulating syntactic serialization and agrammatic and paragrammatic aphasias. (<bold>B</bold>) During the encoding phase, “them” and “behave” are fed into the network simultaneously without the closed-class word “to”. (<bold>C</bold>) Simulation 7: The network serializes the word inputs, adding the missing closed-class word, to form a syntactic sentence. (<bold>D</bold>) Simulation 8: The agrammatic lesion results in omission of the closed-class word while the message is preserved. Coloured maps of word activation represent the strongest activated word of each timestep. (<bold>E</bold>) Simulation 9: In paragrammatic aphasia, grammatical structure is preserved but there are word substitution errors. (<bold>F</bold>) Frequency of syntax errors versus word errors in agrammatic and paragrammatic aphasias produced by the model (simulations 8 and 9), compared to data from humans. Agrammatic and paragrammatic simulations include noise, therefore more example outputs are presented in <xref ref-type="supplementary-material" rid="SD1">fig. S3</xref>.</p></caption><graphic xlink:href="EMS193101-f003"/></fig><fig id="F4" position="float"><label>Fig. 4</label><caption><title>Synaptic changes predict priming.</title><p>(<bold>A</bold>) Lexical and syntactic priming during comprehension. (i) Simulation 10: Lexical priming - recall accuracy improves on repeated encoding of the same sentence. (ii) Simulation 11: Syntactic priming - recall accuracy is reduced following encoding of a syntactically distinct sentence. Response time is also increased. Different syntactic structures are coded with letters, a or b. Numerical subscripts (a<sub>1</sub>, a<sub>2</sub> etc) represent different sentences with non-overlapping words. More lexical and syntactic priming results are presented in <xref ref-type="supplementary-material" rid="SD1">fig. S4</xref>. ns non-significant; *P-value&lt;0.05; **P-value&lt;0.01; ***P-value &lt; 0.001, error bars are SEM. Box plot shows interquartile range. (<bold>B</bold>) Simulation 12: Syntactic priming during production. (i) To encode active and passive sentences, the network has two branching pathways through the role neurons. There are two object role neurons, each with a semantic tag (one for agent, and the other, patient). The subject can either be an agent or patient. (ii) The network is first presented with an active or passive priming sentence. (iii) Then on encountering a mental idea, both nouns are semantically associated with the relevant agent or patient tag. (iv) “Bag of words” without closed-class words (i.e. “were” and “by”) were fed simultaneously to the network to allow syntactic serialization. (v) The network generates either the active or passive voice. (vi) The syntactic priming effect qualitatively matches human data.</p></caption><graphic xlink:href="EMS193101-f004"/></fig><fig id="F5" position="float"><label>Fig. 5</label><caption><title>Long-term acquisition of word-class and syntactic knowledge.</title><p>(<bold>A</bold>) Simulation 13: Long-term acquisition of word class with pivot grammar. The role neurons parse only two-word sentences. The role neurons are activated sequentially, in tandem with the word input. This results in gradual learning between pairs of word and role neurons, representing long-term knowledge that some words appear in the first role, and others in the second. (<bold>B</bold>) Simulation 14: Long-term acquisition of syntactic structure. The network receives randomly generated sentences with the same syntax over 30 periods (three possible words per role), and long-term knowledge between role neurons is learnt from the resulting order of activation of the role neurons. After each learning period recall accuracy is tested. Gray broken arrows indicate lack of predefined long-term knowledge. (<bold>C</bold>) Simulation 15: Simulated event-related potentials (ERPs) from a syntactic vs non-syntactic sentence. Upper panel shows data from an ERP study, and the lower panel is the total neural activity in the model, passed through a delay, time derivative and smoothing filter. Larger neural responses seen after a syntactic violation resembles positive shifts in human ERPs (black bar). (<bold>D</bold>) Simulation 16: Simulated ERP from correct vs incorrect morpheme conditions. Positive shift in total neural activity in the model matches positive shift observed in ERP study (black bar). The networks are presented in <xref ref-type="supplementary-material" rid="SD1">fig. S7</xref>. Error band is SEM.</p></caption><graphic xlink:href="EMS193101-f005"/></fig></floats-group></article>