<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189806</article-id><article-id pub-id-type="doi">10.1101/2023.10.15.562381</article-id><article-id pub-id-type="archive">PPR744003</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Bayesian nonparametric (non-)renewal processes for analyzing neural spike train variability</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Liu</surname><given-names>David</given-names></name><aff id="A1">Department of Cognitive Science Central European University</aff></contrib><contrib contrib-type="author"><name><surname>Lengyel</surname><given-names>Máté</given-names></name><aff id="A2">Department of Engineering University of Cambridge</aff></contrib></contrib-group><author-notes><corresp id="CR1"><label>*</label><email>dl543@cam.ac.uk</email>; <email>m.lengyel@eng.cam.ac.uk</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>20</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Neural spiking activity is generally variable, non-stationary, and exhibits complex dependencies on covariates, such as sensory input or behavior. These dependencies have been proposed to be signatures of specific computations, and so characterizing them with quantitative rigor is critical for understanding neural computations. Approaches based on point processes provide a principled statistical framework for modeling neural spiking activity. However, currently, they only allow the instantaneous mean, but not the instantaneous variability, of responses to depend on covariates. To resolve this limitation, we propose a scalable Bayesian approach generalizing modulated renewal processes using sparse variational Gaussian processes. We leverage pathwise conditioning for computing nonparametric priors over conditional interspike interval distributions and rely on automatic relevance determination to detect lagging interspike interval dependencies beyond renewal order. After systematically validating our method on synthetic data, we apply it to two foundational datasets of animal navigation: head direction cells in freely moving mice and hippocampal place cells in rats running along a linear track. Our model exhibits competitive or better predictive power compared to state-of-the-art baselines, and outperforms them in terms of capturing interspike interval statistics. These results confirm the importance of modeling <italic>covariate-dependent</italic> spiking variability, and further analyses of our fitted models reveal rich patterns of variability modulation beyond the temporal resolution of flexible count-based approaches.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><label>1</label><title>Introduction</title><p id="P2">Analyses of spike time [<xref ref-type="bibr" rid="R49">49</xref>, <xref ref-type="bibr" rid="R76">76</xref>, <xref ref-type="bibr" rid="R86">86</xref>] and count [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R26">26</xref>, <xref ref-type="bibr" rid="R57">57</xref>] statistics have revealed neural responses <italic>in vivo</italic> to be structured but generally variable or noisy [<xref ref-type="bibr" rid="R24">24</xref>, <xref ref-type="bibr" rid="R44">44</xref>, <xref ref-type="bibr" rid="R70">70</xref>, <xref ref-type="bibr" rid="R78">78</xref>]. To model this stochastic aspect of neural spike trains, probabilistic approaches based on temporal point processes have been widely applied. This in turn has been a major driver of point process theory development [<xref ref-type="bibr" rid="R42">42</xref>] for capturing spiking variability structure with statistical models [<xref ref-type="bibr" rid="R74">74</xref>]. The study of neural computation underlying naturalistic behavior in particular involves non-stationary spike trains, which presents a significant challenge as apparent spiking variability is a result of both irreducible “intrinsic” neural stochasticity as well as dependencies on behavioral covariates that can themselves vary on multiple time scales.</p><p id="P3">Different approaches have been proposed for handling non-stationary spike trains, starting with the classical log Cox Gaussian process [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R55">55</xref>] to allow variations in the local intensity or firing rate while modeling independent spikes. Dependencies on previous spikes can be captured to first order with renewal processes, and these models have been extended to non-stationary cases through modulation of the hazard function with some time-dependent function [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R79">79</xref>] or through rescaling interspike intervals with a covariate-dependent rate function [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>]. Another approach based on Hawkes processes and spike-history filters [<xref ref-type="bibr" rid="R45">45</xref>, <xref ref-type="bibr" rid="R83">83</xref>, <xref ref-type="bibr" rid="R91">91</xref>] introduces conditional point processes that go beyond the first order Markov assumption. Approaches based on recurrent networks [<xref ref-type="bibr" rid="R51">51</xref>, <xref ref-type="bibr" rid="R90">90</xref>] and neural ODEs [<xref ref-type="bibr" rid="R10">10</xref>, <xref ref-type="bibr" rid="R41">41</xref>] can in theory capture arbitrarily long dependencies on past spikes and input covariates, but provide more limited descriptive interpretability.</p><p id="P4">However, not only the rate but also the variability of spiking encodes task-relevant information [<xref ref-type="bibr" rid="R34">34</xref>, <xref ref-type="bibr" rid="R60">60</xref>], and bears signatures of the underlying computations [<xref ref-type="bibr" rid="R11">11</xref>]. Importantly, this variability has stimulus- and state-dependent structure [<xref ref-type="bibr" rid="R12">12</xref>, <xref ref-type="bibr" rid="R21">21</xref>, <xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R67">67</xref>]. In statistical modeling language, this corresponds to heteroscedastic or input-dependent observation noise. Such structure reflects computations performed in the underlying neural circuit, and thus characterizing it from data in a flexible and robust manner is critical for advancing theories of neural computation. The classical approaches reviewed above do not attempt to characterize such covariate-dependent changes in variability. Flexible count models have been introduced to more faithfully capture variability at the count level [<xref ref-type="bibr" rid="R28">28</xref>], and recent work has extended this to the general case of input-dependent variability [<xref ref-type="bibr" rid="R48">48</xref>]. Count approaches however are limited in the resolution of the analysis set by the time bin size. In addition, the resulting count statistics strongly depend on the chosen time bin size [<xref ref-type="bibr" rid="R48">48</xref>, <xref ref-type="bibr" rid="R73">73</xref>].</p><p id="P5">While firing rates are routinely modeled as input-dependent, extending point process models with input-dependent variability has not been widely explored in the literature. Rate-rescaled and modulated renewal processes rely on fixed base renewal densities. Allowing the shape parameters of the renewal density to vary with covariates in the corresponding hazard function is one potential approach, but this still relies on a commitment to a particular parametric family of renewal densities. Spike-history filters in conditional point processes are conventionally fixed and thus do not directly model input-dependent spiking variability, though dependence on observed and unobserved covariates [<xref ref-type="bibr" rid="R91">91</xref>] and switching filters based on discrete states [<xref ref-type="bibr" rid="R23">23</xref>] have been considered. Recent work has moved away from parametric filters to nonparametric Gaussian processes [<xref ref-type="bibr" rid="R18">18</xref>], which can be extended to flexibly model dynamic filters as functions of external covariates with a spatio-temporal Gaussian process. However, any modulation of the filter will no longer permit fast convolutions, and such models will be computationally expensive as the filter needs to be recomputed every time step. The direct nonparametric estimation of conditional intensity functions based on maximum likelihood has been explored [<xref ref-type="bibr" rid="R13">13</xref>, <xref ref-type="bibr" rid="R82">82</xref>], but scalable Bayesian approaches have remained absent.</p><sec id="S2"><title>Contribution</title><p id="P6">To enable flexible modeling as well as modulation of instantaneous point process statistics for analyzing neural spike train variability, we introduce the Bayesian nonparametric non-renewal process (NPNR). NPNR builds on sparse variational Gaussian processes and defines a nonparametric prior over conditional interspike interval distributions, generalizing modulated renewal processes with nonparametric renewal densities and spike-history dependencies beyond renewal order. In particular, our point process can flexibly model modulations of not only spiking intensity but also variability. We validate our model using parametric inhomogeneous renewal processes, recovering conditional interspike interval distributions and identifying renewal order in spike-history dependence. On neural data from mouse thalamus and rat hippocampus, our method has competitive predictive power while being superior in capturing interspike interval statistics from the non-stationary data. In particular, our method provides instantaneous measures of spike train variability that are modulated by covariates, and shows rich variability patterns in both datasets consistent with previous studies at coarser timescales. We provide a <monospace>JAX</monospace> [<xref ref-type="bibr" rid="R4">4</xref>] implementation of our method as well as established baseline models within a scalable general variational inference scheme. <sup><xref ref-type="fn" rid="FN1">1</xref></sup></p></sec></sec><sec id="S3" sec-type="intro"><label>2</label><title>Background</title><p id="P7">We start with a brief overview of the theoretical foundations and related point process models, as well as their combination with Gaussian processes to introduce non-stationarity.</p><sec id="S4"><label>2.1</label><title>Temporal point processes</title><p id="P8">Statistical modeling of events that occur stochastically in time is handled by the general framework of temporal point processes [<xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R72">72</xref>]. Denoting the number of events that occurred until time <italic>t</italic> by <italic>N</italic>(<italic>t</italic>), a temporal point process model is completely characterized by its conditional intensity function (CIF) <disp-formula id="FD1"><label>(1)</label><mml:math id="M1"><mml:mrow><mml:mi>λ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>|</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>lim</mml:mi></mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where <italic>λ</italic>(<italic>t</italic>)<italic>δt</italic> is the probability to emit a spike event in the infinitesimal interval [<italic>t</italic>, <italic>t</italic> + <italic>δt</italic>) conditioned on <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, the spiking history before <italic>t</italic>. We can write the point process likelihood for a single neuron spike train consisting of an ordered sequence of <italic>S</italic> spike events at times <italic>t<sub>i</sub></italic> as [<xref ref-type="bibr" rid="R5">5</xref>] <disp-formula id="FD2"><label>(2)</label><mml:math id="M3"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>S</mml:mi></mml:munderover><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P9">In neuroscience applications, one often wants to describe modulation of the point process statistics with some time-varying covariates <bold><italic>x</italic></bold>(<italic>t</italic>), such as animal head direction or body position, which leads to a generalized CIF <inline-formula><mml:math id="M4"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>≤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. Several classes of models have been proposed that are defined by particular restrictions on the functional form of <inline-formula><mml:math id="M5"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>≤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>.</p><sec id="S5"><label>2.1.1</label><title>Inhomogeneous renewal processes</title><sec id="S6"><title>Renewal assumption</title><p id="P10">The statistical model in <xref ref-type="disp-formula" rid="FD2">Eq. 2</xref> describes dependencies between all spikes. One common simplification is the renewal assumption: interspike intervals (ISIs) Δ<sup>(<italic>i</italic>)</sup> = <italic>t</italic><sub><italic>i</italic>+1</sub> − <italic>t<sub>i</sub></italic> are drawn i.i.d. from an interval distribution called the renewal density <italic>g</italic>(Δ; <italic>θ</italic>) with parameters <italic>θ</italic>. This induces a Markov structure <italic>p</italic>(<italic>t<sub>i</sub></italic>|<italic>t</italic><sub><italic>i</italic>−1</sub>, <italic>t</italic><sub><italic>i</italic>−2</sub>, …) = <italic>p</italic>(<italic>t<sub>i</sub></italic>|<italic>t</italic><sub><italic>i</italic>−1</sub>) in the spike train likelihood <disp-formula id="FD3"><label>(3)</label><mml:math id="M6"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋯</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P11">Common renewal densities used for neural data are the exponential (equivalent to a Poisson process), gamma, and inverse Gaussian distributions [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>].</p></sec><sec id="S7"><title>Hazard function modulation</title><p id="P12">Non-stationary point processes need to model changes in statistics with time, and combined with the renewal assumption one obtains inhomogeneous renewal processes. A classical approach that dates back to Cox [<xref ref-type="bibr" rid="R14">14</xref>] is to modulate the hazard function (<xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>) <disp-formula id="FD4"><label>(4)</label><mml:math id="M7"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> with time since last spike <italic>τ</italic> = <italic>t</italic> − <italic>t<sub>i</sub></italic> and the modulation factor <italic>ρ</italic>(<italic>t</italic>). In our context, this can be replaced by some function of covariates <italic>ρ</italic>(<bold><italic>x</italic></bold><sub><italic>t</italic></sub>) [<xref ref-type="bibr" rid="R43">43</xref>]. A multiplicative interaction between <italic>ρ</italic> and <italic>h</italic> as above is typically considered, though this framework allows general parametric forms [<xref ref-type="bibr" rid="R71">71</xref>, <xref ref-type="bibr" rid="R79">79</xref>].</p></sec><sec id="S8"><title>Rate-rescaling</title><p id="P13">Another approach that has been widely applied in the neuroscience community is rate-rescaling [<xref ref-type="bibr" rid="R2">2</xref>, <xref ref-type="bibr" rid="R6">6</xref>], closely related to time-rescaling (see <xref ref-type="supplementary-material" rid="SD1">Appendix B.3</xref>). Here, modulation is achieved with a rate function <italic>r</italic>(<bold><italic>x</italic></bold><sub><italic>t</italic></sub>) ≥ 0 that transforms time <italic>t</italic> into rescaled time <inline-formula><mml:math id="M8"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo stretchy="true">˜</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> <disp-formula id="FD5"><label>(5)</label><mml:math id="M9"><mml:mrow><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mover><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>t</mml:mi></mml:mover><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P14">This maps all spike times <italic>t<sub>i</sub></italic> to rescaled times <inline-formula><mml:math id="M10"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, and will be one-to-one as long as <italic>r</italic>(<italic>t</italic>) &gt; 0. By modeling the rescaled ISIs <inline-formula><mml:math id="M11"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as drawn from a stationary renewal density <italic>g</italic>(·), we obtain an inhomogeneous renewal process from a homogeneous one. The CIF becomes dependent on the covariate path since last spike <inline-formula><mml:math id="M12"><mml:mrow><mml:msub><mml:mi mathvariant="script">P</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:math></inline-formula>, see <xref ref-type="supplementary-material" rid="SD1">Appendix B.3</xref>.</p></sec></sec><sec id="S9"><label>2.1.2</label><title>Conditional point processes</title><sec id="S10"><title>Conditional Poisson processes</title><p id="P15">The renewal assumption ignores correlations between ISIs, which generally are observed in both the peripheral and central nervous system [<xref ref-type="bibr" rid="R1">1</xref>, <xref ref-type="bibr" rid="R25">25</xref>] and can be computationally relevant for signal detection and encoding [<xref ref-type="bibr" rid="R8">8</xref>, <xref ref-type="bibr" rid="R9">9</xref>, <xref ref-type="bibr" rid="R22">22</xref>, <xref ref-type="bibr" rid="R69">69</xref>]. Going beyond Markovian dependencies, a tractable approach, similar to Hawkes processes [<xref ref-type="bibr" rid="R51">51</xref>], is to introduce a causal linear filter <italic>h</italic>(<italic>t</italic>) that is convolved with spikes and added to the log CIF, giving conditional Poisson processes <disp-formula id="FD6"><label>(6)</label><mml:math id="M13"><mml:mrow><mml:mi>log</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mo>∗</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.2em"/><mml:mtext>with</mml:mtext><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:mi>δ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where * denotes temporal convolution. These models are closely linked to mechanistic integrate-and-fire models [<xref ref-type="bibr" rid="R52">52</xref>, <xref ref-type="bibr" rid="R85">85</xref>] and have a long history in the neuroscience literature [<xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R31">31</xref>, <xref ref-type="bibr" rid="R35">35</xref>, <xref ref-type="bibr" rid="R47">47</xref>, <xref ref-type="bibr" rid="R62">62</xref>, <xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R83">83</xref>], appearing as generalized linear models (GLMs) and spike response models (SRMs).</p></sec><sec id="S11"><title>Conditional renewal processes</title><p id="P16">An even more expressive model can be obtained by replacing the Poisson spiking process with a rate-rescaled renewal process. This results in a conditional renewal process, where the rate function has history dependence <inline-formula><mml:math id="M14"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as in <xref ref-type="disp-formula" rid="FD6">Eq. 6</xref>.</p></sec></sec></sec><sec id="S12"><label>2.2</label><title>Gaussian process modulated point processes</title><p id="P17">Gaussian processes (GPs) represent a data-efficient alternative to neural networks, which have been widely used to model the CIF [<xref ref-type="bibr" rid="R59">59</xref>, <xref ref-type="bibr" rid="R72">72</xref>, <xref ref-type="bibr" rid="R90">90</xref>]. When combining GPs with point process likelihoods, the resulting generative model leads to doubly stochastic processes for event data. Placing a Gaussian process prior [<xref ref-type="bibr" rid="R87">87</xref>] over the log intensity function leads to the classic log Cox Gaussian processes [<xref ref-type="bibr" rid="R14">14</xref>, <xref ref-type="bibr" rid="R55">55</xref>], and in the same spirit one can modulate renewal hazard functions [<xref ref-type="bibr" rid="R79">79</xref>] or perform rate-rescaling [<xref ref-type="bibr" rid="R16">16</xref>] with GPs. Such constructions form the basis of many widely used Bayesian neural encoding models for spike trains, both for modeling single neuron responses [<xref ref-type="bibr" rid="R15">15</xref>, <xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R68">68</xref>] as well as population activity [<xref ref-type="bibr" rid="R19">19</xref>, <xref ref-type="bibr" rid="R93">93</xref>]. Combining the flexibility offered by renewal and conditional point processes with GP rate or modulation functions within a variational framework has been impeded by the fact that the original papers were built on a GLM framework with parametric covariate mappings [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R7">7</xref>, <xref ref-type="bibr" rid="R18">18</xref>, <xref ref-type="bibr" rid="R65">65</xref>]. To provide a fair comparison of NPNR to these baselines, we implement a general scalable variational inference framework for the construction and application of such models (see <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref> for details on the baseline models).</p></sec></sec><sec id="S13" sec-type="methods"><label>3</label><title>Method</title><p id="P18">We now introduce the nonparametric non-renewal (NPNR) process and present the approximate Bayesian inference scheme used for model fitting, noting connections to related works in the literature. Our NPNR model provides a nonparametric generalization of modulated renewal processes beyond renewal order, and adds suitable inductive biases for neural spike train data. It implicitly defines a flexible prior over conditional ISI distributions that can be computed using pathwise conditioning, which enables one to analyze spiking variability modulation with minimal parametric constraints. Furthermore, the Bayesian framework provides an elegant data-driven approach to inferring the lagging ISI order of the spike-history dependence.</p><sec id="S14"><label>3.1</label><title>Generative model</title><sec id="S15"><title>Conditional intensity surface priors</title><p id="P19">To obtain flexible modulated point process models, we directly model the CIF, or more precisely its logarithm, of the form <disp-formula id="FD7"><label>(7)</label><mml:math id="M15"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>≤</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>t<sub>i</sub></italic> is the most recent spike at current time <italic>t</italic>. First considering the renewal case, we note the spatio-temporal structure in the log CIF using time since last spike <italic>τ</italic> = <italic>t</italic> − <italic>t<sub>i</sub></italic> is <disp-formula id="FD8"><label>(8)</label><mml:math id="M16"><mml:mrow><mml:mi>log</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> which suggests placing a spatio-temporal GP prior on the log CIF to describe a log intensity surface <disp-formula id="FD9"><label>(9)</label><mml:math id="M17"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="script">P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P20">This generalizes the parametric forms of modulation considered in previous approaches [<xref ref-type="bibr" rid="R43">43</xref>, <xref ref-type="bibr" rid="R79">79</xref>], in particular allowing modulation of the effective instantaneous renewal density by covariates <bold><italic>x</italic></bold><sub><italic>t</italic></sub>. We can introduce lagging ISIs covariates Δ<sub><italic>k</italic></sub>(<italic>t</italic>) with lag <italic>k</italic> as depicted in <xref ref-type="fig" rid="F1">Fig. 1A</xref> to extend the model to a non-renewal process <disp-formula id="FD10"><label>(10)</label><mml:math id="M18"><mml:mrow><mml:mi>log</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mo mathvariant="bold">Δ</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mo mathvariant="bold">Δ</mml:mo><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula> and for a maximum ISI lag <italic>K</italic> we denote the lagging ISIs <bold>Δ</bold><sub><italic>t</italic></sub> = [Δ<sub>1</sub>(<italic>t</italic>), …, Δ<sub><italic>K</italic></sub>(<italic>t</italic>)] to obtain <disp-formula id="FD11"><label>(11)</label><mml:math id="M19"><mml:mrow><mml:mi>log</mml:mi><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>Δ</mml:mi></mml:mstyle><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><sec id="S16"><title>Inductive biases for neural data</title><p id="P21">For neural spiking data, there are biological properties to consider for building a more realistic prior. Firstly, neurons have refractory periods immediately following a spike, though in practice neural recordings may not respect this due to contamination in spike sorting [<xref ref-type="bibr" rid="R38">38</xref>]. Another potentially useful inductive bias is that changes in the spiking intensity of neurons fluctuate mostly at shorter ISI timescales [<xref ref-type="bibr" rid="R32">32</xref>], whereas at longer delays they tend to be temporally smoother. The latter suggests non-stationary GP kernels to be more suitable for modeling the spike-history dependencies [<xref ref-type="bibr" rid="R18">18</xref>]. However, non-stationary kernels do not allow straight-forward use of random Fourier features for evaluating GP posterior function samples at many locations with pathwise conditioning [<xref ref-type="bibr" rid="R88">88</xref>, <xref ref-type="bibr" rid="R89">89</xref>]. This in particular is needed to compute the conditional ISI distributions <italic>g</italic>(<italic>τ</italic>| …) in <xref ref-type="disp-formula" rid="FD15">Eq. 15</xref>, see <xref ref-type="supplementary-material" rid="SD1">Appendix B.5</xref> for details. To achieve the desired non-stationarity for modeling <xref ref-type="disp-formula" rid="FD11">Eq. 11</xref> while maintaining the ability to draw samples using pathwise conditioning, we apply time warping on <italic>τ</italic> from [0, <italic>∞</italic>) → [0, 1] with some warping timescale <italic>τ<sub>w</sub></italic> <disp-formula id="FD12"><label>(12)</label><mml:math id="M20"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mtd><mml:mtd><mml:mo>⇔</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula> and place a stationary Gaussian process prior over the warped temporal dimension <inline-formula><mml:math id="M21"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi mathvariant="script">G</mml:mi><mml:mi mathvariant="script">P</mml:mi></mml:mrow></mml:math></inline-formula> with a temporal kernel <inline-formula><mml:math id="M22"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. This transformation is monotonic (see <xref ref-type="fig" rid="F1">Fig. 1B</xref>), and hence we can easily compute the transformation on the CIF <disp-formula id="FD13"><label>(13)</label><mml:math id="M23"><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>t</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∣</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P22">Similarly, we apply time warping to the Δ<sub><italic>k</italic></sub> dimensions on which we also place stationary kernels <inline-formula><mml:math id="M24"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:msup><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. We note that unlike spike-history filters in conditional point processes (<xref ref-type="disp-formula" rid="FD6">Eq. 6</xref>) which do not change with inputs, the resulting coupling to past activity in <xref ref-type="disp-formula" rid="FD11">Eq. 11</xref> is dependent on covariates <italic>x<sub>t</sub></italic>. This allows one to capture spiking variability modulation via the conditional ISI distribution perspective discussed below. The refractory nature of real neurons can be addressed by the mean function <disp-formula id="FD14"><label>(14)</label><mml:math id="M25"><mml:mrow><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula> with parameters <italic>a<sub>m</sub></italic>, <italic>τ<sub>m</sub></italic>, <italic>b<sub>m</sub></italic>, where refractory periods can be modeled with large negative <italic>a<sub>m</sub></italic>.</p></sec><sec id="S17"><title>Conditional ISI distributions</title><p id="P23">Instead of looking at the CIF, we can view the model as a prior over conditional ISI distributions as depicted in <xref ref-type="fig" rid="F1">Fig. 1C</xref> using the relation (see <xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>) <disp-formula id="FD15"><label>(15)</label><mml:math id="M26"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mo mathvariant="bold">Δ</mml:mo><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∝</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>∣</mml:mo><mml:msub><mml:mi>ℋ</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>d</mml:mtext><mml:msup><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where one drops the dependence on <inline-formula><mml:math id="M27"><mml:mrow><mml:msub><mml:mi>ℋ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> in <italic>g</italic> for the modulated renewal case. If one fixes the lagging ISIs and picks a constant covariate path <italic>g</italic>(<italic>τ</italic>|<bold>Δ</bold><sub>*</sub>, <bold><italic>x</italic></bold><sub>*</sub>), this can be interpreted as an instantaneous ISI distribution of a neuron at the conditioned inputs <bold>Δ</bold><sub>*</sub> and <bold><italic>x</italic></bold><sub>*</sub>. Moments of the conditional ISI distribution are computed using Gauss-Legendre quadratures in warped time (<xref ref-type="supplementary-material" rid="SD1">Appendix B.5</xref>) <disp-formula id="FD16"><label>(16)</label><mml:math id="M28"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mn>0</mml:mn><mml:mi>∞</mml:mi></mml:munderover><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msup><mml:mtext>d</mml:mtext><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mn>0</mml:mn><mml:mn>1</mml:mn></mml:munderover><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:mfrac><mml:mo>|</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mi>τ</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>m</mml:mi></mml:msup><mml:mtext>d</mml:mtext><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover></mml:mrow></mml:math></disp-formula> and this can be used to compute tuning curves of spike train statistics, such as the mean ISI <inline-formula><mml:math id="M29"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> and coefficient of variation <inline-formula><mml:math id="M30"><mml:mrow><mml:mtext>CV</mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mtext>Var</mml:mtext><mml:mo stretchy="false">[</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msqrt><mml:mo>/</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> [<xref ref-type="bibr" rid="R58">58</xref>], as a function of <italic>x</italic><sub>*</sub>. This approach generalizes the homogeneous case considered in the literature, and in particular allows one to compute instantaneous measures of non-stationary spike train variability that are otherwise non-trivial to estimate [<xref ref-type="bibr" rid="R58">58</xref>, <xref ref-type="bibr" rid="R75">75</xref>].</p></sec></sec></sec><sec id="S18"><label>3.2</label><title>Inference</title><sec id="S19"><title>Temporal discretization</title><p id="P24">The generative model is formulated as a continuous time model. In practice, neural and behavioural data are typically recorded with finite temporal resolution at small regular intervals Δ<italic>t</italic>. The cumulative intensity integral has to be approximated by a sum, though note that directly modeling the cumulative hazard function [<xref ref-type="bibr" rid="R59">59</xref>] elegantly avoids this for purely temporal point processes. Spike times are now discretized as a binary vector <bold><italic>y</italic></bold> = [<italic>y</italic><sub>1</sub>, …, <italic>y<sub>T</sub></italic>] where <italic>y<sub>t</sub></italic> = 1 if <italic>t</italic> has a spike event, zero otherwise. Overall, this discretizes the point process likelihood <xref ref-type="disp-formula" rid="FD2">Eq. 2</xref> as <disp-formula id="FD17"><label>(17)</label><mml:math id="M31"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>→</mml:mo><mml:mspace width="0.2em"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>∣</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∏</mml:mo></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo mathvariant="bold">Δ</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula> where we have <italic>T</italic> time steps in total. Note that the discretization scheme implies we do not have observations at <italic>τ</italic> = 0, since the time step immediately after an observed spike has <italic>τ</italic> = Δ<italic>t</italic>.</p></sec><sec id="S20"><title>Variational lower bound</title><p id="P25">We use stochastic variational inference [<xref ref-type="bibr" rid="R39">39</xref>] with batches obtained from consecutive temporal segments and sparse variational GPs [<xref ref-type="bibr" rid="R37">37</xref>], giving the loss objective <disp-formula id="FD18"><label>(18)</label><mml:math id="M32"><mml:mrow><mml:mi>ℒ</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo mathvariant="bold">Δ</mml:mo><mml:mi>t</mml:mi><mml:munderover><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula> where <italic>n</italic> indexes neurons (of which there are <italic>N</italic>)<sup><xref ref-type="fn" rid="FN2">2</xref></sup>, <bold><italic>u</italic></bold> = [<italic>u</italic><sub>1</sub>, …, <italic>u<sub>M</sub></italic>] denotes the set of <italic>M</italic> inducing points, <italic>p</italic>(<bold><italic>u</italic></bold>) the GP prior at inducing locations, <italic>q</italic>(<bold><italic>u</italic></bold>) the variational posterior, and <italic>q</italic>(<bold><italic>f</italic></bold>|<bold><italic>u</italic></bold>) the conditional posterior (see <xref ref-type="supplementary-material" rid="SD1">Appendix B.1</xref> for details). Combined with temporal mini-batching to fit batch segments of length <italic>T</italic>, we can fit to very long time series given the <inline-formula><mml:math id="M33"><mml:mrow><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mtext/><mml:mi>T</mml:mi><mml:mtext/><mml:msup><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mtext/><mml:msup><mml:mi>M</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> computational complexity. We also no longer rely on computing hazard functions of parametric renewal densities to obtain the CIF, which can be numerically unstable. Modulated renewal processes instead rely on a specialized thinning procedure [<xref ref-type="bibr" rid="R79">79</xref>], but we take a more scalable and general variational approach. Overall, we optimize the kernel hyperparameters, variational posterior mean and covariance, inducing point locations, and mean function parameters <italic>a<sub>m</sub></italic>, <italic>b<sub>m</sub></italic> and <italic>τ<sub>m</sub></italic> using gradient descent with Adam [<xref ref-type="bibr" rid="R46">46</xref>] (see <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref> for details). The time warping parameter <italic>τ<sub>w</sub></italic> is fixed in our experiments to the empirical mean ISI, and the hyperparameter <italic>K</italic> is fixed and chosen in advance (see also subsection on automatic relevance determination below).</p></sec><sec id="S21"><title>Automatic relevance determination</title><p id="P26">The Bayesian framework with GPs enables us to perform automatic relevance determination (ARD) over the input dimensions to automatically select relevant input [<xref ref-type="bibr" rid="R37">37</xref>, <xref ref-type="bibr" rid="R80">80</xref>]. Applied to lagging ISI dimensions in our NPNR model, this provides an elegant approach to making a data-driven renewal assumption and generally determining the spike-history dependence of the CIF. We choose to fix <italic>τ<sub>w</sub></italic> to the empirical mean ISI as shown in <xref ref-type="fig" rid="F1">Fig. 1B</xref> (rather than learning it) to achieve interpretability of kernel timescales for ARD (<xref ref-type="supplementary-material" rid="SD1">Fig. 6A</xref>) at a small cost of performance (<xref ref-type="supplementary-material" rid="SD1">Fig. 12</xref>). For a chosen maximum lag <italic>K</italic>, there is no need for manual selection of the history interaction window as for GLM spike-history filters, though recent work on nonparametric GLM filters provides a related window size selection procedure [<xref ref-type="bibr" rid="R18">18</xref>]. In the spirit of Bayesian models, we choose <italic>K</italic> to give a sufficiently high capacity model [<xref ref-type="bibr" rid="R40">40</xref>, <xref ref-type="bibr" rid="R80">80</xref>] to be able to flexibly capture history dependence, as seen in panel D of <xref ref-type="fig" rid="F3">Fig. 3</xref> and <xref ref-type="fig" rid="F4">Fig. 4</xref>.</p></sec></sec></sec><sec id="S22" sec-type="results"><label>4</label><title>Results</title><p id="P27">All datasets discretize spike trains and input time series at regular intervals of Δ<italic>t</italic> = 1 ms. We use a product kernel for <italic>k</italic>(<bold><italic>x</italic></bold>, <bold><italic>x</italic></bold>′) with periodic kernels for angular dimensions, and squared exponential kernels in other cases. For <inline-formula><mml:math id="M34"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M35"><mml:mrow><mml:mi>k</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msup><mml:mover accent="true"><mml:mo mathvariant="bold">Δ</mml:mo><mml:mo>˜</mml:mo></mml:mover><mml:mo>′</mml:mo></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we pick a product kernel with Matérn-<inline-formula><mml:math id="M36"><mml:mrow><mml:mstyle scriptlevel="+1"><mml:mfrac bevelled="true"><mml:mn>3</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></inline-formula> (see <xref ref-type="supplementary-material" rid="SD1">Fig. 12</xref> for different kernel choices) and set the maximum ISI lag <italic>K</italic> = 3. For illustration, conditional ISI distributions and corresponding tuning curves are computed by fixing <bold>Δ</bold><sub><italic>k</italic></sub> to be the mean ISI per neuron. Firing rates are defined as <inline-formula><mml:math id="M37"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>τ</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, since this corresponds to the number of spikes fired per unit time in infinitely large time bins for a renewal process (<xref ref-type="supplementary-material" rid="SD1">Eq. 26</xref>). GP inducing points were randomly initialized, and for a fair comparison, all models used 8 inducing points for each covariate dimension (including temporal dimensions <italic>τ</italic> and <bold>Δ</bold> in the NPNR process). For each experiment, we repeat model fitting with 3 different random seeds and pick the model with the best training likelihood. Further details on experiments are presented in <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>.</p><sec id="S23"><label>4.1</label><title>Validation on synthetic data</title><p id="P28">For validating our approach, we generate 1000 s of data using rate-rescaling [<xref ref-type="bibr" rid="R65">65</xref>] mimicking a place cell population of 9 neurons for an animal moving in a 2D square arena, each with a unique rate map and renewal density (<xref ref-type="fig" rid="F2">Fig. 2A</xref>, details in <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>). The models applied are baseline Poisson, raised cosine filter conditional Poisson and rate-rescaled gamma processes (<xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>), and our NPNR process. Note that the rescaled gamma process is within-model class for 3 of the synthetic neurons. Inferred conditional ISI distributions and rate maps of our NPNR process in <xref ref-type="fig" rid="F2">Fig. 2B</xref> are close to ground truth, showing the ability of our model to capture modulated spiking statistics drawn from various parametric families. To assess how well ISI statistics are captured, we apply time-rescaling using the GP posterior mean functions (<xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>) which we visualize with quantile-quantile (QQ) plots [<xref ref-type="bibr" rid="R6">6</xref>] in <xref ref-type="fig" rid="F2">Fig. 2C</xref>. Again, we see an excellent fit of our model compared to baseline models, indicating that only the NPNR is capable of satisfactorily capturing the empirical ISI statistics. Learned temporal kernel timescales of the NPNR process in <xref ref-type="fig" rid="F2">Fig. 2B</xref> show a clear separation between the time since last spike <italic>τ</italic> dimension (lag 0) and lagging ISI <bold>Δ</bold> dimensions (lag ≥ 1) with the dotted relevance boundary at <italic>l</italic> = 3 (dimensionless), as expected for renewal processes.</p></sec><sec id="S24"><label>4.2</label><title>Neural data</title><p id="P29">Now we apply our method to head direction cells in freely moving mice [<xref ref-type="bibr" rid="R63">63</xref>, <xref ref-type="bibr" rid="R64">64</xref>] and place cells in rats running along a linear track [<xref ref-type="bibr" rid="R54">54</xref>]. We select 33 units from the mouse and 35 units from the rat data, which leads to around 36 and 68 million data points to fit in the training set, respectively (see <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref> for preprocessing details and <xref ref-type="supplementary-material" rid="SD1">Appendix B.2</xref> on data scaling). Experiments involve fitting to the first half of a dataset (~18 min. for mouse, ~32 min. for rat), and testing on the second half split into 5 consecutive segments. The split into 5 test folds is used to quantify dataset variability. For prediction, we evaluate the expected log likelihood summed over all neurons <disp-formula id="FD19"><label>(19)</label><mml:math id="M38"><mml:mrow><mml:mtext>ELL</mml:mtext><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mi>n</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>∣</mml:mo><mml:msub><mml:mi mathvariant="bold">f</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula> using Gauss-Hermite quadrature with 50 points (Monte Carlo for renewal processes, see <xref ref-type="supplementary-material" rid="SD1">Appendix B</xref>). To assess goodness-of-fit to ISI statistics, we compute QQ plots as before and apply the Kolmogorov-Smirnov (KS) test, giving a <italic>p</italic>-value per neuron that indicates how likely the data came from the model (<xref ref-type="supplementary-material" rid="SD1">Appendix A</xref>). Baselines are the inhomogeneous Poisson (P), rate-rescaled gamma (G) and inverse Gaussian (IG) renewal [<xref ref-type="bibr" rid="R6">6</xref>, <xref ref-type="bibr" rid="R83">83</xref>], raised cosine (RC) filter conditional Poisson [<xref ref-type="bibr" rid="R66">66</xref>, <xref ref-type="bibr" rid="R85">85</xref>] and renewal [<xref ref-type="bibr" rid="R65">65</xref>], and nonparametric (NP) filter conditional Poisson processes [<xref ref-type="bibr" rid="R18">18</xref>] (details in <xref ref-type="supplementary-material" rid="SD1">Appendix C</xref>).</p><sec id="S25"><label>4.2.1</label><title>Mouse head direction cell data</title><p id="P30">We choose the animal head direction as our 1D input covariate <italic>x</italic>. From the KS test <italic>p</italic>-value distribution, we see that our model outperforms all baselines in capturing ISI statistics (<xref ref-type="fig" rid="F3">Fig. 3A</xref> left, higher is better; <xref ref-type="fig" rid="F3">Fig. 3B</xref>, QQ plots closer to diagonal). It performs competitively to conditional Poisson processes in terms of predictive performance (<xref ref-type="fig" rid="F3">Fig. 3A</xref> right), but those models fail to capture ISI statistics. In addition, we note the spiking saturation in some samples of the nonparametric conditional Poisson model (<xref ref-type="fig" rid="F3">Fig. 3C</xref>, purple) due to a known instability [<xref ref-type="bibr" rid="R3">3</xref>, <xref ref-type="bibr" rid="R29">29</xref>]. Samples from our model (<xref ref-type="fig" rid="F3">Fig. 3C</xref>, gray) exhibit visually similar spike patterns to the real spike train segment. Furthermore, kernel timescales in <xref ref-type="fig" rid="F3">Fig. 3D</xref> show a sizable fraction of the population is characterized by a non-renewal spiking process.</p><sec id="S26"><title>Neural dispersion regimes</title><p id="P31">From <xref ref-type="fig" rid="F3">Fig. 3E and F</xref>, we observe both under- and overdispersion (CV smaller and bigger than one) consistent with a previous study based on spike counts [<xref ref-type="bibr" rid="R48">48</xref>]. Estimated instantaneous rates and CVs in <xref ref-type="fig" rid="F3">Fig. 3E</xref> are computed using the conditional ISI distribution evaluated along the time series of covariates in the training data. One can regress instantaneous CV against rate, and the CV-rate <italic>R</italic><sup>2</sup> shows some cells with near linear relations and some with nonlinear trends that can be captured by a GP. Despite that, many cells still show a low overall <italic>R</italic><sup>2</sup>, implying there is no parametric relation. Most neurons increase CV with rate, while some show a slight linear decrease reminiscent of refractory Poisson processes (<xref ref-type="supplementary-material" rid="SD1">Eq. 31</xref>, see <xref ref-type="supplementary-material" rid="SD1">Fig. 7</xref> for CV-rate patterns).</p></sec></sec><sec id="S27"><label>4.2.2</label><title>Rat place cell data</title><p id="P32">In this case, <bold><italic>x</italic></bold> is 3D consisting of the body position along the track, head direction, and local field potential (LFP) <italic>θ</italic>-phase. Our model significantly outperforms all baselines, having both a better KS test <italic>p</italic>-value distribution (<xref ref-type="fig" rid="F4">Fig. 4A</xref> left, higher is better; <xref ref-type="fig" rid="F4">Fig. 4B</xref>, QQ plots closer to diagonal) and predictive performance (<xref ref-type="fig" rid="F4">Fig. 4A</xref>, right). Note that the ELLs for this dataset differ from those shown in <xref ref-type="fig" rid="F3">Fig. 3A</xref> due to the fundamentally different (less predictable) spiking statistics of place cells compared to head direction cells (e.g. due to theta oscillations). As the nonparametric conditional Poisson model introduces nonparametric but <italic>covariate-independent</italic> variability patterns, these results highlights the importance of modeling <italic>covariate-dependent</italic> spiking variability. Note that the rate-rescaled renewal processes struggle to fit this data, with test ELLs of inverse Gaussian models below -200 nats/s (<xref ref-type="fig" rid="F4">Fig. 4A</xref> right). Samples from our model (<xref ref-type="fig" rid="F4">Fig. 4C</xref>, gray) show it captures the characteristic bursting nature of the real spike train segment. Kernel timescales in <xref ref-type="fig" rid="F4">Fig. 4D</xref> show most cells are described well by a renewal process, different to mouse data <xref ref-type="fig" rid="F4">Fig. 4D</xref>.</p><sec id="S28"><title>Capturing overdispersion</title><p id="P33">We see CV values in <xref ref-type="fig" rid="F4">Fig. 4E</xref> higher than the mouse thalamus dataset, consistent with overdispersion of place cell discharge in 2D open field navigation [<xref ref-type="bibr" rid="R26">26</xref>]. Similar to the mouse data, the CV-rate <italic>R</italic><sup>2</sup> again shows there is generally no parametric relation. We also tend to observe larger increases in CV with firing rate compared to mouse data (<xref ref-type="supplementary-material" rid="SD1">Fig. 9</xref>).</p></sec><sec id="S29"><title>θ-modulation and phase precession</title><p id="P34">Spiking activity modulation during <italic>θ</italic>-cycles [<xref ref-type="bibr" rid="R53">53</xref>] is prominent in rat hippocampus, and is visible here in the log CIF (<xref ref-type="fig" rid="F4">Fig. 4C</xref>). We also see phase precession [<xref ref-type="bibr" rid="R76">76</xref>] in <xref ref-type="fig" rid="F4">Fig. 4F</xref> (top), a classical example where spike timing relative to some rhythm has coding significance [<xref ref-type="bibr" rid="R33">33</xref>]. Our model enables one to extract not only spiking intensity but also variability, and shows that variability generally inherits the phase precession pattern (<xref ref-type="fig" rid="F4">Fig. 4F</xref> bottom).</p></sec></sec></sec></sec><sec id="S30" sec-type="discussion"><label>5</label><title>Discussion</title><sec id="S31"><label>5.1</label><title>Limitations and further work</title><sec id="S32"><title>ISI statistics</title><p id="P35">Apart from the coefficient of variation, there are other ISI statistics that characterize spiking dynamics aspects such as bursting or regularity. Of particular interest is the local coefficient of variation [<xref ref-type="bibr" rid="R75">75</xref>], which involves joint statistics of consecutive ISIs (Δ<sup>(<italic>i</italic>)</sup>, Δ<sup>(<italic>i</italic>−1)</sup>) that can be computed from our model (see <xref ref-type="supplementary-material" rid="SD1">Appendix D</xref>). The same applies to serial correlations [<xref ref-type="bibr" rid="R25">25</xref>], which may provide insights into biophysical details [<xref ref-type="bibr" rid="R77">77</xref>]. Furthermore, quantifying the shape of ISI distributions is of interest as it is associated with various properties of the underlying neural circuit dynamics [<xref ref-type="bibr" rid="R61">61</xref>].</p></sec><sec id="S33"><title>Neural correlations</title><p id="P36">To capture correlations in multivariate spike train data, direct spike couplings as in GLMs are less suitable for current neural recordings compared to latent variable models due to the sparse sampling of populations by electrodes [<xref ref-type="bibr" rid="R50">50</xref>]. Combining the latter alongside observed covariates [<xref ref-type="bibr" rid="R48">48</xref>] with our point process provides a powerful framework for capturing correlations [<xref ref-type="bibr" rid="R81">81</xref>], which can have significant impact on neural coding [<xref ref-type="bibr" rid="R56">56</xref>]. To perform goodness-of-fit tests, the Kolmogorov-Smirnov test with time-rescaling can be extended to the multivariate case [<xref ref-type="bibr" rid="R30">30</xref>, <xref ref-type="bibr" rid="R92">92</xref>].</p></sec></sec><sec id="S34"><label>5.2</label><title>Conclusion and impact</title><p id="P37">We introduced the Bayesian nonparametric non-renewal (NPNR) process for flexible modeling of variability in neural spike train data. On synthetic renewal process data, NPNR successfully captures spiking statistics and their modulation by covariates, and finds renewal order in the spike-history dependence. When applied to mouse head direction cells and rat hippocampal place cells, NPNR has competitive or improved predictive performance to established baseline models, and is superior in terms of capturing ISI statistics, establishing the importance of capturing covariate-dependent variability. NPNR-based analyses recover known behavioral tuning, while also revealing novel patterns of spiking variability at millisecond timescales that are compatible with count-based studies.</p><p id="P38">Neural firing rates traditionally characterize most computational functions and information encoded by neurons [<xref ref-type="bibr" rid="R16">16</xref>, <xref ref-type="bibr" rid="R17">17</xref>, <xref ref-type="bibr" rid="R33">33</xref>], but recent work on V1 [<xref ref-type="bibr" rid="R20">20</xref>, <xref ref-type="bibr" rid="R27">27</xref>, <xref ref-type="bibr" rid="R36">36</xref>, <xref ref-type="bibr" rid="R60">60</xref>] and hippocampal place cells [<xref ref-type="bibr" rid="R84">84</xref>] have started to assign computationally well-defined roles to variability in the context of representing uncertainty. Our method introduced in this paper is a principled tool for empirically characterizing neural spiking variability and its modulation at the timescales of individual spikes, and we hope our model will be useful for revealing new aspects of neural coding. Such findings are foundational to advances in computational and theoretical neuroscience, and may have downstream practical applications in designing and improving algorithms for brain-machine interfaces.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Materials</label><media xlink:href="EMS189806-supplement_Supplementary_Materials.pdf" mimetype="application" mime-subtype="pdf" id="d37aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S35"><title>Acknowledgments and Disclosure of Funding</title><p id="P39">This work was supported by the Cambridge European and Wolfson College Scholarship by the Cambridge Trust (D.L.) and by the Wellcome Trust (Investigator Award in Science 212262/Z/18/Z to M.L.). We are grateful to Kristopher Jensen, Marine Schimel and Valentina Njaradi for helpful comments on the manuscript. We would also like to thank Alexander Terenin and Jonathan So for helpful discussions.</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P40">Code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/davindicode/nonparametric-nonrenewal-process">https://github.com/davindicode/nonparametric-nonrenewal-process</ext-link></p></fn><fn id="FN2"><label>2</label><p id="P41">Note the simple summation over <italic>n</italic> in <xref ref-type="disp-formula" rid="FD18">Eq. 18</xref>, as our model does not capture neural correlations without introducing latent covariates [<xref ref-type="bibr" rid="R48">48</xref>]. In other words, in its current form, our model treats neurons as independent.</p></fn></fn-group><ref-list><ref id="R1"><label>[1]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avila-Akerberg</surname><given-names>O</given-names></name><name><surname>Chacron</surname><given-names>MJ</given-names></name></person-group><article-title>Nonrenewal spike train statistics: causes and functional consequences on neural coding</article-title><source>Experimental brain research</source><year>2011</year><volume>210</volume><issue>3</issue><fpage>353</fpage><lpage>371</lpage><pub-id pub-id-type="pmcid">PMC4587930</pub-id><pub-id pub-id-type="pmid">21451983</pub-id><pub-id pub-id-type="doi">10.1007/s00221-011-2639-6</pub-id></element-citation></ref><ref id="R2"><label>[2]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barbieri</surname><given-names>R</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><article-title>Construction and analysis of non-poisson stimulus-response models of neural spiking activity</article-title><source>Journal of neuroscience methods</source><year>2001</year><volume>105</volume><issue>1</issue><fpage>25</fpage><lpage>37</lpage><pub-id pub-id-type="pmid">11166363</pub-id></element-citation></ref><ref id="R3"><label>[3]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellec</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Modirshanechi</surname><given-names>A</given-names></name><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>Fitting summary statistics of neural data with a differentiable spiking network simulator</article-title><source>Advances in Neural Information Processing Systems</source><year>2021</year><volume>34</volume><fpage>18552</fpage><lpage>18563</lpage></element-citation></ref><ref id="R4"><label>[4]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Frostig</surname><given-names>R</given-names></name><name><surname>Hawkins</surname><given-names>P</given-names></name><name><surname>Johnson</surname><given-names>MJ</given-names></name><name><surname>Leary</surname><given-names>C</given-names></name><name><surname>Maclaurin</surname><given-names>D</given-names></name><name><surname>Necula</surname><given-names>G</given-names></name><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Wanderman-Milne</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name></person-group><article-title>JAX: composable transformations of Python+NumPy programs</article-title><year>2018</year></element-citation></ref><ref id="R5"><label>[5]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Barbieri</surname><given-names>R</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>Likelihood methods for neural spike train data analysis</article-title><source>Computational neuroscience: A comprehensive approach</source><year>2003</year><fpage>253</fpage><lpage>286</lpage></element-citation></ref><ref id="R6"><label>[6]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Barbieri</surname><given-names>R</given-names></name><name><surname>Ventura</surname><given-names>V</given-names></name><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name></person-group><article-title>The time-rescaling theorem and its application to neural spike train data analysis</article-title><source>Neural computation</source><year>2002</year><volume>14</volume><issue>2</issue><fpage>325</fpage><lpage>346</lpage><pub-id pub-id-type="pmid">11802915</pub-id></element-citation></ref><ref id="R7"><label>[7]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>EN</given-names></name><name><surname>Frank</surname><given-names>LM</given-names></name><name><surname>Tang</surname><given-names>D</given-names></name><name><surname>Quirk</surname><given-names>MC</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><article-title>A statistical paradigm for neural spike train decoding applied to position prediction from ensemble firing patterns of rat hippocampal place cells</article-title><source>Journal of Neuroscience</source><year>1998</year><volume>18</volume><issue>18</issue><fpage>7411</fpage><lpage>7425</lpage><pub-id pub-id-type="pmcid">PMC6793233</pub-id><pub-id pub-id-type="pmid">9736661</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-18-07411.1998</pub-id></element-citation></ref><ref id="R8"><label>[8]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chacron</surname><given-names>MJ</given-names></name><name><surname>Lindner</surname><given-names>B</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name></person-group><article-title>Noise shaping by interval correlations increases information transfer</article-title><source>Physical review letters</source><year>2004</year><volume>92</volume><issue>8</issue><elocation-id>080601</elocation-id><pub-id pub-id-type="pmid">14995762</pub-id></element-citation></ref><ref id="R9"><label>[9]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chacron</surname><given-names>MJ</given-names></name><name><surname>Longtin</surname><given-names>A</given-names></name><name><surname>Maler</surname><given-names>L</given-names></name></person-group><article-title>Negative interspike interval correlations increase the neuronal capacity for encoding time-dependent stimuli</article-title><source>Journal of Neuroscience</source><year>2001</year><volume>21</volume><issue>14</issue><fpage>5328</fpage><lpage>5343</lpage><pub-id pub-id-type="pmcid">PMC6762847</pub-id><pub-id pub-id-type="pmid">11438609</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-14-05328.2001</pub-id></element-citation></ref><ref id="R10"><label>[10]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>TQ</given-names></name><name><surname>Rubanova</surname><given-names>Y</given-names></name><name><surname>Bettencourt</surname><given-names>J</given-names></name><name><surname>Duvenaud</surname><given-names>DK</given-names></name></person-group><article-title>Neural ordinary differential equations</article-title><source>Advances in neural information processing systems</source><year>2018</year><fpage>6571</fpage><lpage>6583</lpage></element-citation></ref><ref id="R11"><label>[11]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><article-title>Variance as a signature of neural computations during decision making</article-title><source>Neuron</source><year>2011</year><volume>69</volume><issue>4</issue><fpage>818</fpage><lpage>831</lpage><pub-id pub-id-type="pmcid">PMC3066020</pub-id><pub-id pub-id-type="pmid">21338889</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2010.12.037</pub-id></element-citation></ref><ref id="R12"><label>[12]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Byron</surname><given-names>MY</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Sugrue</surname><given-names>LP</given-names></name><name><surname>Cohen</surname><given-names>MR</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Clark</surname><given-names>AM</given-names></name><name><surname>Hosseini</surname><given-names>P</given-names></name><name><surname>Scott</surname><given-names>BB</given-names></name><etal/></person-group><article-title>Stimulus onset quenches neural variability: a widespread cortical phenomenon</article-title><source>Nature neuroscience</source><year>2010</year><volume>13</volume><issue>3</issue><fpage>369</fpage><pub-id pub-id-type="pmcid">PMC2828350</pub-id><pub-id pub-id-type="pmid">20173745</pub-id><pub-id pub-id-type="doi">10.1038/nn.2501</pub-id></element-citation></ref><ref id="R13"><label>[13]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>TP</given-names></name><name><surname>Sarma</surname><given-names>SS</given-names></name></person-group><article-title>A computationally efficient method for nonparametric modeling of neural spiking activity with point processes</article-title><source>Neural Computation</source><year>2010</year><volume>22</volume><issue>8</issue><fpage>2002</fpage><lpage>2030</lpage><pub-id pub-id-type="pmid">20438334</pub-id></element-citation></ref><ref id="R14"><label>[14]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>DR</given-names></name></person-group><article-title>The statistical analysis of dependencies in point processes</article-title><source>Stochastic Point Processes</source><publisher-name>Wiley</publisher-name><publisher-loc>New York</publisher-loc><year>1972</year><fpage>55</fpage><lpage>66</lpage></element-citation></ref><ref id="R15"><label>[15]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><source>Fast gaussian process methods for point process intensity estimation</source><conf-name>Proceedings of the 25th international conference on Machine learning</conf-name><year>2008</year><fpage>192</fpage><lpage>199</lpage></element-citation></ref><ref id="R16"><label>[16]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><article-title>Inferring neural firing rates from spike trains using gaussian processes</article-title><source>Advances in neural information processing systems</source><year>2007</year><volume>20</volume></element-citation></ref><ref id="R17"><label>[17]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DePasquale</surname><given-names>B</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Abbott</surname><given-names>L</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name></person-group><article-title>The centrality of population-level factors to network computation is demonstrated by a versatile approach for training spiking networks</article-title><source>Neuron</source><year>2023</year><pub-id pub-id-type="pmcid">PMC10118067</pub-id><pub-id pub-id-type="pmid">36630961</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2022.12.007</pub-id></element-citation></ref><ref id="R18"><label>[18]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowling</surname><given-names>M</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Park</surname><given-names>IM</given-names></name></person-group><article-title>Non-parametric generalized linear model</article-title><source>arXiv preprint</source><elocation-id>arXiv:2009.01362</elocation-id><year>2020</year></element-citation></ref><ref id="R19"><label>[19]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncker</surname><given-names>L</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><article-title>Temporal alignment and latent gaussian process factor inference in population spike trains</article-title><source>Advances in neural information processing systems</source><year>2018</year><volume>31</volume></element-citation></ref><ref id="R20"><label>[20]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Echeveste</surname><given-names>R</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference</article-title><source>bioRxiv</source><year>2020</year><elocation-id>696088</elocation-id><pub-id pub-id-type="pmcid">PMC7610392</pub-id><pub-id pub-id-type="pmid">32778794</pub-id><pub-id pub-id-type="doi">10.1038/s41593-020-0671-1</pub-id></element-citation></ref><ref id="R21"><label>[21]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Subramaniyan</surname><given-names>M</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Cadwell</surname><given-names>CR</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>State dependence of noise correlations in macaque primary visual cortex</article-title><source>Neuron</source><year>2014</year><volume>82</volume><issue>1</issue><fpage>235</fpage><lpage>248</lpage><pub-id pub-id-type="pmcid">PMC3990250</pub-id><pub-id pub-id-type="pmid">24698278</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2014.02.006</pub-id></element-citation></ref><ref id="R22"><label>[22]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>TA</given-names></name><name><surname>Helbig</surname><given-names>B</given-names></name><name><surname>Russell</surname><given-names>DF</given-names></name><name><surname>Schimansky-Geier</surname><given-names>L</given-names></name><name><surname>Neiman</surname><given-names>AB</given-names></name></person-group><article-title>Coherent stochastic oscillations enhance signal detection in spiking neurons</article-title><source>Physical Review E</source><year>2009</year><volume>80</volume><issue>2</issue><elocation-id>021919</elocation-id><pub-id pub-id-type="pmcid">PMC4942810</pub-id><pub-id pub-id-type="pmid">19792163</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevE.80.021919</pub-id></element-citation></ref><ref id="R23"><label>[23]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Escola</surname><given-names>S</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>Katz</surname><given-names>D</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><article-title>Hidden markov models for the stimulus-response relationships of multistate neural systems</article-title><source>Neural computation</source><year>2011</year><volume>23</volume><issue>5</issue><fpage>1071</fpage><lpage>1132</lpage><pub-id pub-id-type="pmid">21299424</pub-id></element-citation></ref><ref id="R24"><label>[24]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LP</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><article-title>Noise in the nervous system</article-title><source>Nature reviews neuroscience</source><year>2008</year><volume>9</volume><issue>4</issue><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="pmcid">PMC2631351</pub-id><pub-id pub-id-type="pmid">18319728</pub-id><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id></element-citation></ref><ref id="R25"><label>[25]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farkhooi</surname><given-names>F</given-names></name><name><surname>Strube-Bloss</surname><given-names>MF</given-names></name><name><surname>Nawrot</surname><given-names>MP</given-names></name></person-group><article-title>Serial correlation in neural spike trains: Experimental evidence, stochastic modeling, and single neuron variability</article-title><source>Physical Review E</source><year>2009</year><volume>79</volume><issue>2</issue><elocation-id>021905</elocation-id><pub-id pub-id-type="pmid">19391776</pub-id></element-citation></ref><ref id="R26"><label>[26]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fenton</surname><given-names>AA</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name></person-group><article-title>Place cell discharge is extremely variable during individual passes of the rat through the firing field</article-title><source>Proceedings of the National Academy of Sciences</source><year>1998</year><volume>95</volume><issue>6</issue><fpage>3182</fpage><lpage>3187</lpage><pub-id pub-id-type="pmcid">PMC19716</pub-id><pub-id pub-id-type="pmid">9501237</pub-id><pub-id pub-id-type="doi">10.1073/pnas.95.6.3182</pub-id></element-citation></ref><ref id="R27"><label>[27]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Statistically optimal perception and learning: from behavior to neural representations</article-title><source>Trends in cognitive sciences</source><year>2010</year><volume>14</volume><issue>3</issue><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="pmcid">PMC2939867</pub-id><pub-id pub-id-type="pmid">20153683</pub-id><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.003</pub-id></element-citation></ref><ref id="R28"><label>[28]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Busing</surname><given-names>L</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><article-title>High-dimensional neural spike train analysis with generalized count linear dynamical systems</article-title><source>Advances in neural information processing systems</source><year>2015</year><fpage>2044</fpage><lpage>2052</lpage></element-citation></ref><ref id="R29"><label>[29]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerhard</surname><given-names>F</given-names></name><name><surname>Deger</surname><given-names>M</given-names></name><name><surname>Truccolo</surname><given-names>W</given-names></name></person-group><article-title>On the stability and dynamics of stochastic spiking neuron models: Nonlinear hawkes process and point process glms</article-title><source>PLoS computational biology</source><year>2017</year><volume>13</volume><issue>2</issue><elocation-id>e1005390</elocation-id><pub-id pub-id-type="pmcid">PMC5325182</pub-id><pub-id pub-id-type="pmid">28234899</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005390</pub-id></element-citation></ref><ref id="R30"><label>[30]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerhard</surname><given-names>F</given-names></name><name><surname>Haslinger</surname><given-names>R</given-names></name><name><surname>Pipa</surname><given-names>G</given-names></name></person-group><article-title>Applying the multivariate time-rescaling theorem to neural population models</article-title><source>Neural computation</source><year>2011</year><volume>23</volume><issue>6</issue><fpage>1452</fpage><lpage>1483</lpage><pub-id pub-id-type="pmcid">PMC3090500</pub-id><pub-id pub-id-type="pmid">21395436</pub-id><pub-id pub-id-type="doi">10.1162/NECO_a_00126</pub-id></element-citation></ref><ref id="R31"><label>[31]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>A framework for spiking neuron models: The spike response model</article-title><source>Handbook of biological physics</source><publisher-name>Elsevier</publisher-name><year>2001</year><volume>4</volume><fpage>469</fpage><lpage>516</lpage></element-citation></ref><ref id="R32"><label>[32]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Kistler</surname><given-names>WM</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><source>Neuronal dynamics: From single neurons to networks and models of cognition</source><year>2014</year><publisher-name>Cambridge University Press</publisher-name></element-citation></ref><ref id="R33"><label>[33]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname><given-names>W</given-names></name><name><surname>Kreiter</surname><given-names>AK</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name><name><surname>Herz</surname><given-names>AV</given-names></name></person-group><article-title>Neural codes: firing rates and beyond</article-title><source>Proceedings of the National Academy of Sciences</source><year>1997</year><volume>94</volume><issue>24</issue><fpage>12740</fpage><lpage>12741</lpage><pub-id pub-id-type="pmcid">PMC34168</pub-id><pub-id pub-id-type="pmid">9398065</pub-id><pub-id pub-id-type="doi">10.1073/pnas.94.24.12740</pub-id></element-citation></ref><ref id="R34"><label>[34]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghanbari</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>CM</given-names></name><name><surname>Read</surname><given-names>HL</given-names></name><name><surname>Stevenson</surname><given-names>IH</given-names></name></person-group><article-title>Modeling stimulus-dependent variability improves decoding of population neural responses</article-title><source>Journal of Neural Engineering</source><year>2019</year><volume>16</volume><issue>6</issue><elocation-id>066018</elocation-id><pub-id pub-id-type="pmid">31404915</pub-id></element-citation></ref><ref id="R35"><label>[35]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Henze</surname><given-names>DA</given-names></name><name><surname>Hirase</surname><given-names>H</given-names></name><name><surname>Leinekugel</surname><given-names>X</given-names></name><name><surname>Dragoi</surname><given-names>G</given-names></name><name><surname>Czurkó</surname><given-names>A</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Spike train dynamics predicts theta-related phase precession in hippocampal pyramidal cells</article-title><source>Nature</source><year>2002</year><volume>417</volume><issue>6890</issue><fpage>738</fpage><lpage>741</lpage><pub-id pub-id-type="pmid">12066184</pub-id></element-citation></ref><ref id="R36"><label>[36]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><article-title>The dynamical regime of sensory cortex: stable dynamics around a single stimulus-tuned attractor account for patterns of noise variability</article-title><source>Neuron</source><year>2018</year><volume>98</volume><issue>4</issue><fpage>846</fpage><lpage>860</lpage><pub-id pub-id-type="pmcid">PMC5971207</pub-id><pub-id pub-id-type="pmid">29772203</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.017</pub-id></element-citation></ref><ref id="R37"><label>[37]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hensman</surname><given-names>J</given-names></name><name><surname>Fusi</surname><given-names>N</given-names></name><name><surname>Lawrence</surname><given-names>ND</given-names></name></person-group><source>Gaussian processes for big data</source><person-group person-group-type="editor"><name><surname>Nicholson</surname><given-names>AE</given-names></name><name><surname>Smyth</surname><given-names>P</given-names></name></person-group><conf-name>Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI 2013, Bellevue, WA, USA, August 11-15, 2013</conf-name><year>2013</year><publisher-name>AUAI Press</publisher-name></element-citation></ref><ref id="R38"><label>[38]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>DN</given-names></name><name><surname>Mehta</surname><given-names>SB</given-names></name><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><article-title>Quality metrics to accompany spike sorting of extracellular signals</article-title><source>Journal of Neuroscience</source><year>2011</year><volume>31</volume><issue>24</issue><fpage>8699</fpage><lpage>8705</lpage><pub-id pub-id-type="pmcid">PMC3123734</pub-id><pub-id pub-id-type="pmid">21677152</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0971-11.2011</pub-id></element-citation></ref><ref id="R39"><label>[39]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Blei</surname><given-names>DM</given-names></name><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Paisley</surname><given-names>J</given-names></name></person-group><article-title>Stochastic variational inference</article-title><source>The Journal of Machine Learning Research</source><year>2013</year><volume>14</volume><issue>1</issue><fpage>1303</fpage><lpage>1347</lpage></element-citation></ref><ref id="R40"><label>[40]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>K</given-names></name><name><surname>Kao</surname><given-names>T-C</given-names></name><name><surname>Stone</surname><given-names>J</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name></person-group><article-title>Scalable bayesian gpfa with automatic relevance determination and discrete noise models</article-title><source>Advances in Neural Information Processing Systems</source><year>2021</year><volume>34</volume><fpage>10613</fpage><lpage>10626</lpage></element-citation></ref><ref id="R41"><label>[41]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>J</given-names></name><name><surname>Benson</surname><given-names>AR</given-names></name></person-group><article-title>Neural jump stochastic differential equations</article-title><source>Advances in Neural Information Processing Systems</source><year>2019</year><volume>32</volume></element-citation></ref><ref id="R42"><label>[42]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Kelly</surname><given-names>RC</given-names></name><name><surname>Loh</surname><given-names>W-L</given-names></name></person-group><article-title>Assessment of synchrony in multiple neural spike trains using loglinear point process models</article-title><source>The annals of applied statistics</source><year>2011</year><volume>5</volume><issue>2B</issue><fpage>1262</fpage><pub-id pub-id-type="pmcid">PMC3152213</pub-id><pub-id pub-id-type="pmid">21837263</pub-id><pub-id pub-id-type="doi">10.1214/10-AOAS429</pub-id></element-citation></ref><ref id="R43"><label>[43]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kass</surname><given-names>RE</given-names></name><name><surname>Ventura</surname><given-names>V</given-names></name></person-group><article-title>A spike-train probability model</article-title><source>Neural computation</source><year>2001</year><volume>13</volume><issue>8</issue><fpage>1713</fpage><lpage>1720</lpage><pub-id pub-id-type="pmid">11506667</pub-id></element-citation></ref><ref id="R44"><label>[44]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name></person-group><article-title>Millisecond encoding precision of auditory cortex neurons</article-title><source>Proceedings of the National Academy of Sciences</source><year>2010</year><volume>107</volume><issue>39</issue><fpage>16976</fpage><lpage>16981</lpage><pub-id pub-id-type="pmcid">PMC2947890</pub-id><pub-id pub-id-type="pmid">20837521</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1012656107</pub-id></element-citation></ref><ref id="R45"><label>[45]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Putrino</surname><given-names>D</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><article-title>A granger causality measure for point process models of ensemble neural spiking activity</article-title><source>PLoS computational biology</source><year>2011</year><volume>7</volume><issue>3</issue><elocation-id>e1001110</elocation-id><pub-id pub-id-type="pmcid">PMC3063721</pub-id><pub-id pub-id-type="pmid">21455283</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1001110</pub-id></element-citation></ref><ref id="R46"><label>[46]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv preprint</source><elocation-id>arXiv:1412.6980</elocation-id><year>2014</year></element-citation></ref><ref id="R47"><label>[47]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linderman</surname><given-names>SW</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Inferring structured connectivity from spike trains under negative-binomial generalized linear models</article-title><source>Cosyne Abstracts</source><year>2015</year></element-citation></ref><ref id="R48"><label>[48]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>D</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>A universal probabilistic spike count model reveals ongoing modulation of neural variability</article-title><source>Advances in Neural Information Processing Systems</source><year>2021</year><volume>34</volume><fpage>13392</fpage><lpage>13405</lpage></element-citation></ref><ref id="R49"><label>[49]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>London</surname><given-names>M</given-names></name><name><surname>Roth</surname><given-names>A</given-names></name><name><surname>Beeren</surname><given-names>L</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><article-title>Sensitivity to perturbations in vivo implies high noise and suggests rate coding in cortex</article-title><source>Nature</source><year>2010</year><volume>466</volume><issue>7302</issue><fpage>123</fpage><lpage>127</lpage><pub-id pub-id-type="pmcid">PMC2898896</pub-id><pub-id pub-id-type="pmid">20596024</pub-id><pub-id pub-id-type="doi">10.1038/nature09086</pub-id></element-citation></ref><ref id="R50"><label>[50]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macke</surname><given-names>JH</given-names></name><name><surname>Buesing</surname><given-names>L</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><article-title>Empirical models of spiking in neural populations</article-title><source>Advances in neural information processing systems</source><year>2011</year><volume>24</volume></element-citation></ref><ref id="R51"><label>[51]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mei</surname><given-names>H</given-names></name><name><surname>Eisner</surname><given-names>JM</given-names></name></person-group><article-title>The neural hawkes process: A neurally self-modulating multivariate point process</article-title><source>Advances in neural information processing systems</source><year>2017</year><volume>30</volume></element-citation></ref><ref id="R52"><label>[52]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mensi</surname><given-names>S</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><article-title>From stochastic nonlinear integrate-and-fire to generalized linear models</article-title><source>Advances in Neural Information Processing Systems</source><year>2011</year><volume>24</volume></element-citation></ref><ref id="R53"><label>[53]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizuseki</surname><given-names>K</given-names></name><name><surname>Sirota</surname><given-names>A</given-names></name><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Theta oscillations provide temporal windows for local circuit computation in the entorhinal-hippocampal loop</article-title><source>Neuron</source><year>2009</year><volume>64</volume><issue>2</issue><fpage>267</fpage><lpage>280</lpage><pub-id pub-id-type="pmcid">PMC2771122</pub-id><pub-id pub-id-type="pmid">19874793</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2009.08.037</pub-id></element-citation></ref><ref id="R54"><label>[54]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mizuseki</surname><given-names>K</given-names></name><name><surname>Sirota</surname><given-names>A</given-names></name><name><surname>Pastalkova</surname><given-names>E</given-names></name><name><surname>Diba</surname><given-names>K</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Multiple single unit recordings from different rat hippocampal and entorhinal regions while the animals were performing multiple behavioral tasks</article-title><publisher-name>CRCNS org</publisher-name><year>2013</year></element-citation></ref><ref id="R55"><label>[55]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Møller</surname><given-names>J</given-names></name><name><surname>Syversveen</surname><given-names>AR</given-names></name><name><surname>Waagepetersen</surname><given-names>RP</given-names></name></person-group><article-title>Log gaussian cox processes</article-title><source>Scandinavian journal of statistics</source><year>1998</year><volume>25</volume><issue>3</issue><fpage>451</fpage><lpage>482</lpage></element-citation></ref><ref id="R56"><label>[56]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Beck</surname><given-names>J</given-names></name><name><surname>Kanitscheider</surname><given-names>I</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><article-title>Information-limiting correlations</article-title><source>Nature neuroscience</source><year>2014</year><volume>17</volume><issue>10</issue><elocation-id>1410</elocation-id><pub-id pub-id-type="pmcid">PMC4486057</pub-id><pub-id pub-id-type="pmid">25195105</pub-id><pub-id pub-id-type="doi">10.1038/nn.3807</pub-id></element-citation></ref><ref id="R57"><label>[57]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagele</surname><given-names>J</given-names></name><name><surname>Herz</surname><given-names>AV</given-names></name><name><surname>Stemmler</surname><given-names>MB</given-names></name></person-group><article-title>Untethered firing fields and intermittent silences: Why grid-cell discharge is so variable</article-title><source>Hippocampus</source><year>2020</year><pub-id pub-id-type="pmid">32045073</pub-id></element-citation></ref><ref id="R58"><label>[58]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nawrot</surname><given-names>MP</given-names></name><name><surname>Boucsein</surname><given-names>C</given-names></name><name><surname>Molina</surname><given-names>VR</given-names></name><name><surname>Riehle</surname><given-names>A</given-names></name><name><surname>Aertsen</surname><given-names>A</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><article-title>Measurement of variability dynamics in cortical spike trains</article-title><source>Journal of neuroscience methods</source><year>2008</year><volume>169</volume><issue>2</issue><fpage>374</fpage><lpage>390</lpage><pub-id pub-id-type="pmid">18155774</pub-id></element-citation></ref><ref id="R59"><label>[59]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Omi</surname><given-names>T</given-names></name><name><surname>Ueda</surname><given-names>N</given-names></name><name><surname>Aihara</surname><given-names>K</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Beygelzimer</surname><given-names>A</given-names></name><name><surname>d’Alché-Buc</surname><given-names>F</given-names></name><name><surname>Fox</surname><given-names>E</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><article-title>Fully neural network based model for general temporal point processes</article-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2019</year><volume>32</volume></element-citation></ref><ref id="R60"><label>[60]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><article-title>Neural variability and sampling-based probabilistic representations in the visual cortex</article-title><source>Neuron</source><year>2016</year><volume>92</volume><issue>2</issue><fpage>530</fpage><lpage>543</lpage><pub-id pub-id-type="pmcid">PMC5077700</pub-id><pub-id pub-id-type="pmid">27764674</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.038</pub-id></element-citation></ref><ref id="R61"><label>[61]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><article-title>Interspike interval distributions of spiking neurons driven by fluctuating inputs</article-title><source>Journal of neurophysiology</source><year>2011</year><volume>106</volume><issue>1</issue><fpage>361</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">21525364</pub-id></element-citation></ref><ref id="R62"><label>[62]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title><source>Network: Computation in Neural Systems</source><year>2004</year><volume>15</volume><issue>4</issue><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="pmid">15600233</pub-id></element-citation></ref><ref id="R63"><label>[63]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><source>Extracellular recordings from multi-site silicon probes in the anterior thalamus and subicular formation of freely moving mice</source><publisher-name>CRCNS</publisher-name><year>2015</year></element-citation></ref><ref id="R64"><label>[64]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Lacroix</surname><given-names>MM</given-names></name><name><surname>Petersen</surname><given-names>PC</given-names></name><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><article-title>Internally organized mechanisms of the head direction sense</article-title><source>Nature neuroscience</source><year>2015</year><volume>18</volume><issue>4</issue><fpage>569</fpage><lpage>575</lpage><pub-id pub-id-type="pmcid">PMC4376557</pub-id><pub-id pub-id-type="pmid">25730672</pub-id><pub-id pub-id-type="doi">10.1038/nn.3968</pub-id></element-citation></ref><ref id="R65"><label>[65]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Time-rescaling methods for the estimation and assessment of non-poisson neural encoding models</article-title><source>Advances in neural information processing systems</source><year>2009</year><fpage>1473</fpage><lpage>1481</lpage></element-citation></ref><ref id="R66"><label>[66]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name><name><surname>Sher</surname><given-names>A</given-names></name><name><surname>Litke</surname><given-names>AM</given-names></name><name><surname>Chichilnisky</surname><given-names>E</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><year>2008</year><volume>454</volume><issue>7207</issue><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="pmcid">PMC2684455</pub-id><pub-id pub-id-type="pmid">18650810</pub-id><pub-id pub-id-type="doi">10.1038/nature07140</pub-id></element-citation></ref><ref id="R67"><label>[67]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce-Alvarez</surname><given-names>A</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Albright</surname><given-names>TD</given-names></name><name><surname>Stoner</surname><given-names>GR</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><article-title>Stimulus-dependent variability and noise correlations in cortical mt neurons</article-title><source>Proceedings of the National Academy of Sciences</source><year>2013</year><volume>110</volume><issue>32</issue><fpage>13162</fpage><lpage>13167</lpage><pub-id pub-id-type="pmcid">PMC3740826</pub-id><pub-id pub-id-type="pmid">23878209</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1300098110</pub-id></element-citation></ref><ref id="R68"><label>[68]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rad</surname><given-names>KR</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><article-title>Efficient, adaptive estimation of two-dimensional firing rate surfaces via gaussian process methods</article-title><source>Network: Computation in Neural Systems</source><year>2010</year><volume>21</volume><issue>3-4</issue><fpage>142</fpage><lpage>168</lpage><pub-id pub-id-type="pmid">21138363</pub-id></element-citation></ref><ref id="R69"><label>[69]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratnam</surname><given-names>R</given-names></name><name><surname>Nelson</surname><given-names>ME</given-names></name></person-group><article-title>Nonrenewal statistics of electrosensory afferent spike trains: implications for the detection of weak sensory signals</article-title><source>Journal of Neuroscience</source><year>2000</year><volume>20</volume><issue>17</issue><fpage>6672</fpage><lpage>6683</lpage><pub-id pub-id-type="pmcid">PMC6772956</pub-id><pub-id pub-id-type="pmid">10964972</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-17-06672.2000</pub-id></element-citation></ref><ref id="R70"><label>[70]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reich</surname><given-names>DS</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Knight</surname><given-names>BW</given-names></name><name><surname>Ozaki</surname><given-names>T</given-names></name><name><surname>Kaplan</surname><given-names>E</given-names></name></person-group><article-title>Response variability and timing precision of neuronal spike trains in vivo</article-title><source>Journal of neurophysiology</source><year>1997</year><volume>77</volume><issue>5</issue><fpage>2836</fpage><lpage>2841</lpage><pub-id pub-id-type="pmid">9163398</pub-id></element-citation></ref><ref id="R71"><label>[71]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahin</surname><given-names>I</given-names></name></person-group><article-title>A generalization of renewal processes</article-title><source>Operations research letters</source><year>1993</year><volume>13</volume><issue>4</issue><fpage>259</fpage><lpage>263</lpage></element-citation></ref><ref id="R72"><label>[72]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shchur</surname><given-names>O</given-names></name><name><surname>Gao</surname><given-names>N</given-names></name><name><surname>Biloš</surname><given-names>M</given-names></name><name><surname>Günnemann</surname><given-names>S</given-names></name></person-group><article-title>Fast and flexible temporal point processes with triangular maps</article-title><source>Advances in Neural Information Processing Systems</source><year>2020</year><volume>33</volume><fpage>73</fpage><lpage>84</lpage></element-citation></ref><ref id="R73"><label>[73]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimazaki</surname><given-names>H</given-names></name><name><surname>Shinomoto</surname><given-names>S</given-names></name></person-group><article-title>A method for selecting the bin size of a time histogram</article-title><source>Neural computation</source><year>2007</year><volume>19</volume><issue>6</issue><fpage>1503</fpage><lpage>1527</lpage><pub-id pub-id-type="pmid">17444758</pub-id></element-citation></ref><ref id="R74"><label>[74]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shimokawa</surname><given-names>T</given-names></name><name><surname>Koyama</surname><given-names>S</given-names></name><name><surname>Shinomoto</surname><given-names>S</given-names></name></person-group><article-title>A characterization of the time-rescaled gamma process as a model for spike trains</article-title><source>Journal of computational neuroscience</source><year>2010</year><volume>29</volume><issue>1-2</issue><fpage>183</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">19844786</pub-id></element-citation></ref><ref id="R75"><label>[75]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinomoto</surname><given-names>S</given-names></name><name><surname>Miura</surname><given-names>K</given-names></name><name><surname>Koyama</surname><given-names>S</given-names></name></person-group><article-title>A measure of local variation of inter-spike intervals</article-title><source>Biosystems</source><year>2005</year><volume>79</volume><issue>1-3</issue><fpage>67</fpage><lpage>72</lpage><pub-id pub-id-type="pmid">15649590</pub-id></element-citation></ref><ref id="R76"><label>[76]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name></person-group><article-title>Theta phase precession in hippocampal neuronal populations and the compression of temporal sequences</article-title><source>Hippocampus</source><year>1996</year><volume>6</volume><issue>2</issue><fpage>149</fpage><lpage>172</lpage><pub-id pub-id-type="pmid">8797016</pub-id></element-citation></ref><ref id="R77"><label>[77]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>JA</given-names></name><name><surname>Kiselev</surname><given-names>I</given-names></name><name><surname>Iyengar</surname><given-names>V</given-names></name><name><surname>Trapani</surname><given-names>JG</given-names></name><name><surname>Tania</surname><given-names>N</given-names></name></person-group><article-title>Mathematical modeling and analyses of interspike-intervals of spontaneous activity in afferent neurons of the zebrafish lateral line</article-title><source>Scientific Reports</source><year>2018</year><volume>8</volume><issue>1</issue><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="pmcid">PMC6173758</pub-id><pub-id pub-id-type="pmid">30291277</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-33064-z</pub-id></element-citation></ref><ref id="R78"><label>[78]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>RB</given-names></name><name><surname>Gossen</surname><given-names>ER</given-names></name><name><surname>Jones</surname><given-names>KE</given-names></name></person-group><article-title>Neuronal variability: noise or part of the signal?</article-title><source>Nature Reviews Neuroscience</source><year>2005</year><volume>6</volume><issue>5</issue><fpage>389</fpage><lpage>397</lpage><pub-id pub-id-type="pmid">15861181</pub-id></element-citation></ref><ref id="R79"><label>[79]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teh</surname><given-names>YW</given-names></name><name><surname>Rao</surname><given-names>V</given-names></name></person-group><article-title>Gaussian process modulated renewal processes</article-title><source>Advances in Neural Information Processing Systems</source><year>2011</year><fpage>2474</fpage><lpage>2482</lpage></element-citation></ref><ref id="R80"><label>[80]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Titsias</surname><given-names>M</given-names></name><name><surname>Lawrence</surname><given-names>ND</given-names></name></person-group><source>Bayesian gaussian process latent variable model</source><conf-name>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</conf-name><year>2010</year><fpage>844</fpage><lpage>851</lpage></element-citation></ref><ref id="R81"><label>[81]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name></person-group><article-title>From point process observations to collective neural dynamics: Nonlinear hawkes process glms, low-dimensional dynamics and coarse graining</article-title><source>Journal of Physiology-Paris</source><year>2016</year><volume>110</volume><issue>4</issue><fpage>336</fpage><lpage>347</lpage><pub-id pub-id-type="pmcid">PMC5610574</pub-id><pub-id pub-id-type="pmid">28336305</pub-id><pub-id pub-id-type="doi">10.1016/j.jphysparis.2017.02.004</pub-id></element-citation></ref><ref id="R82"><label>[82]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name></person-group><article-title>Nonparametric modeling of neural point processes via stochastic gradient boosting regression</article-title><source>Neural computation</source><year>2007</year><volume>19</volume><issue>3</issue><fpage>672</fpage><lpage>705</lpage><pub-id pub-id-type="pmid">17298229</pub-id></element-citation></ref><ref id="R83"><label>[83]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname><given-names>W</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name><name><surname>Fellows</surname><given-names>MR</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title><source>Journal of neurophysiology</source><year>2005</year><volume>93</volume><issue>2</issue><fpage>1074</fpage><lpage>1089</lpage><pub-id pub-id-type="pmid">15356183</pub-id></element-citation></ref><ref id="R84"><label>[84]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ujfalussy</surname><given-names>BB</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name></person-group><article-title>Sampling motion trajectories during hippocampal theta sequences</article-title><source>Elife</source><year>2022</year><volume>11</volume><elocation-id>e74058</elocation-id><pub-id pub-id-type="pmcid">PMC9643003</pub-id><pub-id pub-id-type="pmid">36346218</pub-id><pub-id pub-id-type="doi">10.7554/eLife.74058</pub-id></element-citation></ref><ref id="R85"><label>[85]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname><given-names>AI</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name></person-group><article-title>Capturing the dynamical repertoire of single neurons with generalized linear models</article-title><source>Neural computation</source><year>2017</year><volume>29</volume><issue>12</issue><fpage>3260</fpage><lpage>3289</lpage><pub-id pub-id-type="pmid">28957020</pub-id></element-citation></ref><ref id="R86"><label>[86]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>AH</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Maheswaranathan</surname><given-names>N</given-names></name><name><surname>Dhawale</surname><given-names>AK</given-names></name><name><surname>Fisher</surname><given-names>T</given-names></name><name><surname>Wilson</surname><given-names>CD</given-names></name><name><surname>Brann</surname><given-names>DH</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shusterman</surname><given-names>R</given-names></name><etal/></person-group><article-title>Discovering precise temporal patterns in large-scale neural recordings through robust and interpretable time warping</article-title><source>Neuron</source><year>2020</year><volume>105</volume><issue>2</issue><fpage>246</fpage><lpage>259</lpage><pub-id pub-id-type="pmcid">PMC7336835</pub-id><pub-id pub-id-type="pmid">31786013</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.020</pub-id></element-citation></ref><ref id="R87"><label>[87]</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>CK</given-names></name><name><surname>Rasmussen</surname><given-names>CE</given-names></name></person-group><source>Gaussian processes for machine learning</source><publisher-name>MIT press Cambridge, MA</publisher-name><year>2006</year><volume>2</volume><pub-id pub-id-type="pmid">15112367</pub-id></element-citation></ref><ref id="R88"><label>[88]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Borovitskiy</surname><given-names>V</given-names></name><name><surname>Terenin</surname><given-names>A</given-names></name><name><surname>Mostowsky</surname><given-names>P</given-names></name><name><surname>Deisenroth</surname><given-names>M</given-names></name></person-group><source>Efficiently sampling functions from gaussian process posteriors</source><conf-name>International Conference on Machine Learning</conf-name><publisher-name>PMLR</publisher-name><year>2020</year><fpage>10292</fpage><lpage>10302</lpage></element-citation></ref><ref id="R89"><label>[89]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>JT</given-names></name><name><surname>Borovitskiy</surname><given-names>V</given-names></name><name><surname>Terenin</surname><given-names>A</given-names></name><name><surname>Mostowsky</surname><given-names>P</given-names></name><name><surname>Deisenroth</surname><given-names>MP</given-names></name></person-group><article-title>Pathwise conditioning of gaussian processes</article-title><source>The Journal of Machine Learning Research</source><year>2021</year><volume>22</volume><issue>1</issue><fpage>4741</fpage><lpage>4787</lpage></element-citation></ref><ref id="R90"><label>[90]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xiao</surname><given-names>S</given-names></name><name><surname>Yan</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Zha</surname><given-names>H</given-names></name><name><surname>Chu</surname><given-names>S</given-names></name></person-group><source>Modeling the intensity function of point process via recurrent neural networks</source><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><year>2017</year><volume>31</volume></element-citation></ref><ref id="R91"><label>[91]</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>R</given-names></name><name><surname>Gupta</surname><given-names>G</given-names></name><name><surname>Bogdan</surname><given-names>P</given-names></name></person-group><source>Data-driven perception of neuron point process with unknown unknowns</source><conf-name>Proceedings of the 10th ACM/IEEE International Conference on Cyber-Physical Systems</conf-name><year>2019</year><fpage>259</fpage><lpage>269</lpage></element-citation></ref><ref id="R92"><label>[92]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yousefi</surname><given-names>A</given-names></name><name><surname>Amidi</surname><given-names>Y</given-names></name><name><surname>Nazari</surname><given-names>B</given-names></name><name><surname>Eden</surname><given-names>UT</given-names></name></person-group><article-title>Assessing goodness-of-fit in marked point process models of neural population coding via time and rate rescaling</article-title><source>Neural Computation</source><year>2020</year><volume>32</volume><issue>11</issue><fpage>2145</fpage><lpage>2186</lpage><pub-id pub-id-type="pmid">32946712</pub-id></element-citation></ref><ref id="R93"><label>[93]</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>BM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Santhanam</surname><given-names>G</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Advances in neural information processing systems</source><year>2008</year><volume>21</volume><fpage>1881</fpage><lpage>1888</lpage><pub-id pub-id-type="pmcid">PMC2712272</pub-id><pub-id pub-id-type="pmid">19357332</pub-id><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Schematic of our proposed model.</title><p><bold>(A)</bold> Time since last spike <italic>τ</italic> and lagging ISIs <bold>Δ</bold> for an observed spike train (top row) alongside covariates <bold><italic>x</italic></bold>. <bold>(B)</bold> Illustration of the time warping procedure. We fix the warping parameter <italic>τ<sub>w</sub></italic> to the empirical mean ISI, which leads to more uniform distributions <inline-formula><mml:math id="M39"><mml:mrow><mml:mover accent="true"><mml:mi>τ</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> suitable for a stationary GP kernel. <bold>(C)</bold> Prior samples from the generative model for two values of <italic>a<sub>m</sub></italic> characterized by the lack and presence of a refractory period. The transformation <xref ref-type="disp-formula" rid="FD15">Eq. 15</xref> links the log CIF (top rows) with conditional ISI distributions (bottom rows).</p></caption><graphic xlink:href="EMS189806-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Validation on synthetic data.</title><p><bold>(A)</bold> True rate maps (brighter is higher) defined over a square environment (top) and base renewal densities (bottom) in each column given by gamma (left), log normal (middle) and inverse Gaussian (right) distributions with various shape parameters. Each color corresponds to a separate neuron. <bold>(B)</bold> Posterior mean rate maps (top left) and conditional ISI distribution samples in gray overlaid on true renewal densities at various locations (bottom) for the NPNR process fit to synthetic data. The relevance boundary (dotted line) for kernel timescales (top right) is placed at <italic>l</italic> = 3 (dimensionless). (C) QQ-plots of fitted models (each curve is a neuron).</p></caption><graphic xlink:href="EMS189806-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Application to mouse head direction cell data.</title><p><bold>(A)</bold> Violin plot of KS <italic>p</italic>-values per neuron (left, lines marking quartiles) and test expected log likelihoods with errorbars showing s.e.m. across test folds (right). Larger values in both metrics indicate better model fit to data. <bold>(B)</bold> QQ-plots for various models (each curve is a neuron) identified by color (panel A left). <bold>(C)</bold> Predicted log CIF (middle) for an observed spike train (top) and posterior spike train samples (bottom) conditioned on the same covariates <bold><italic>x</italic></bold><sub><italic>t</italic></sub> for various models identified by color. <bold>(D)</bold> <italic>Left:</italic> temporal kernel timescales for <italic>τ</italic> (lag 0) and Δ<sub><italic>k</italic></sub> dimensions with the relevance boundary at <italic>l</italic> = 3 (dimensionless). <italic>Right:</italic> histogram of “ISI-order” (1 + largest lag <italic>k</italic> for which <italic>k</italic> is below the boundary) across neurons. <bold>(E)</bold> Time average of estimated instantaneous rates and CVs from the training data (left) and <italic>R</italic><sup>2</sup> values of CV-rate regression with a linear and a GP model (right). <bold>(F)</bold> Posterior median and 95% intervals of tuning curves over head direction for the rate and CV, with posterior ISI distribution samples (right) at dashed locations.</p></caption><graphic xlink:href="EMS189806-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Application to rat hippocampal place cell data.</title><p><bold>(A)-(E)</bold> Similar to <xref ref-type="fig" rid="F3">Fig. 3A-E</xref>. <bold>(F)</bold> Posterior mean tuning maps over <italic>x</italic>-position and <italic>θ</italic>-phase for the rate and CV (left) for left-to-right runs (based on head direction) with posterior ISI distribution samples (right) at marked locations.</p></caption><graphic xlink:href="EMS189806-f004"/></fig></floats-group></article>