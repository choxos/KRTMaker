<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189811</article-id><article-id pub-id-type="doi">10.1101/2023.10.17.562789</article-id><article-id pub-id-type="archive">PPR744040</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Bidirectional Generative Adversarial Representation Learning for Natural Stimulus Synthesis</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name><surname>Reilly</surname><given-names>Johnny</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Goodwin</surname><given-names>John D.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Lu</surname><given-names>Sihao</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Kozlov</surname><given-names>Andriy S.</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Bioengineering, Imperial College London</aff><author-notes><corresp id="CR1">
<label>*</label>For correspondence: <email>a.kozlov@imperial.ac.uk</email> (ASK)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>20</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>17</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Thousands of species use vocal signals to communicate with one another. Vocalisations carry rich information, yet characterising and analysing these complex, high-dimensional signals is difficult and prone to human bias. Moreover, animal vocalisations are ethologically relevant stimuli whose representation by auditory neurons is an important subject of research in sensory neuroscience. A method that can efficiently generate naturalistic vocalisation waveforms would offer an unlimited supply of stimuli with which to probe neuronal computations. While unsupervised learning methods allow for the projection of vocalisations into low-dimensional latent spaces learned from the waveforms themselves, and generative modelling allows for the synthesis of novel vocalisations for use in downstream tasks, there is currently no method that would combine these tasks to produce naturalistic vocalisation waveforms for stimulus playback. In this paper, we demonstrate BiWaveGAN: a bidirectional Generative Adversarial Network (GAN) capable of learning a latent representation of ultrasonic vocalisations (USVs) from mice. We show that BiWaveGAN can be used to generate, and interpolate between, realistic vocalisation waveforms. We then use these synthesised stimuli along with natural USVs to probe the sensory input space of mouse auditory cortical neurons. We show that stimuli generated from our method evoke neuronal responses as effectively as real vocalisations, and produce receptive fields with the same predictive power. BiWaveGAN is not restricted to mouse USVs but can be used to synthesise naturalistic vocalisations of any animal species and interpolate between vocalisations of the same or different species, which could be useful for probing categorical boundaries in representations of ethologically relevant auditory signals.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Artificial intelligence is increasingly used to predict and better understand how neurons respond to natural stimuli. For example, evolutionary algorithms and convolutional neural networks (CNNs) can be used to identify stimuli that maximally drive single neurons in the auditory and visual cortices [<xref ref-type="bibr" rid="R7">Chambers et al., 2014</xref>, <xref ref-type="bibr" rid="R46">Walker et al., 2019</xref>]. Similarly, unsupervised neural networks employing learning rules grounded in biological principles are able to extract meaningful features from animal vocalisations which can provide context for the features discovered from neural recordings [<xref ref-type="bibr" rid="R32">Ngiam et al., 2011</xref>, <xref ref-type="bibr" rid="R26">Kozlov and Gentner, 2016</xref>, <xref ref-type="bibr" rid="R30">Lu et al., 2023</xref>]. Furthermore, CNNs have been shown to be effective in modelling auditory coding of complex, natural sounds, even generalising well to unseen neuronal data [<xref ref-type="bibr" rid="R2">Arneodo et al., 2021</xref>]. Such studies underscore the versatility and potential of deep learning models.</p><p id="P3">Natural stimuli such as animal vocalisations serve as a powerful tool to understand how the brain processes ethologically relevant sensory information, offering a more realistic perspective than simple artificial stimuli commonly used in experimental contexts [<xref ref-type="bibr" rid="R9">David et al., 2004</xref>, <xref ref-type="bibr" rid="R48">Woolley et al., 2005</xref>, <xref ref-type="bibr" rid="R26">Kozlov and Gentner, 2016</xref>, <xref ref-type="bibr" rid="R35">Rose et al., 2021</xref>]. It is well documented that mice and other rodents use a rich system of vocalisations at frequencies above the human hearing range (greater than 20 kHz). These vocalisations are referred to as <italic>ultrasonic vocalisations (USVs)</italic>. USVs are understood to be social signals that are produced in a variety of contexts, for example, adult male mice are known to produce USVs in the presence of female mice, and pups will emit them when separated from their mothers [<xref ref-type="bibr" rid="R18">Gourbal et al., 2004</xref>, <xref ref-type="bibr" rid="R20">Haack et al., 1983</xref>]. Neurons in the auditory cortex respond to these behaviourally salient sounds [<xref ref-type="bibr" rid="R28">Liu et al., 2006</xref>, <xref ref-type="bibr" rid="R29">Liu and Schreiner, 2007</xref>, <xref ref-type="bibr" rid="R5">Carruthers et al., 2013</xref>, <xref ref-type="bibr" rid="R30">Lu et al., 2023</xref>], which are typically emitted as sequences of discrete ‘syllables’, sometimes organised into song-like repeated phrases [<xref ref-type="bibr" rid="R21">Holy and Guo, 2005</xref>]. Much research has gone into analysing the structure of USV syllables and attempting to classify them into different types [<xref ref-type="bibr" rid="R8">Coffey et al., 2019</xref>, <xref ref-type="bibr" rid="R15">Fonseca et al., 2021</xref>]. However, there is no consensus on the number of syllable classes or how to classify them. Indeed, recent deep-learning based approaches to understanding the distribution of USVs seem to favour the idea that USV syllables cannot be clustered into discrete categories based on their discernible features but must be understood as varying continuously. Despite the efforts to analyse the structure of these vocalisations, there are few methods available to computationally and systematically generate these vocalisations [<xref ref-type="bibr" rid="R16">Goffinet et al., 2021</xref>, <xref ref-type="bibr" rid="R38">Sainburg et al., 2019</xref>].</p><p id="P4">By leveraging advances in deep learning, we developed a method capable of efficiently generating novel vocalisations. We show that these vocalisations are suitable naturalistic stimuli to probe the mouse auditory system: they drive neurons in the auditory cortex well and produce receptive-field features that predict neuronal responses to natural USVs just as well as features obtained using natural USVs do. Given that the mouse is a powerful animal model, with its plethora of genetic and behavioural tools, we believe that this method will be a valuable addition to the toolset for the study of sensory processing. In principle, given enough data for model training, our method can be used to produce, and interpolate between, vocalisations of any animal species, offering a rich source of stimuli for sensory neuroscience and other applications.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Bidirectional WaveGAN</title><p id="P5">Our approach is based on WaveGAN [<xref ref-type="bibr" rid="R12">Donahue et al., 2019</xref>], a Generative Adversarial Network (GAN) which has been successfully used to generate realistic waveforms of human speech and birdsong. GANs consist of two neural networks: The generator, <bold><italic>G</italic></bold>, which attempts to learn a mapping from a low dimensional latent space to the data domain, and the discriminator, <bold><italic>D</italic></bold>, a binary classifier which attempts to distinguish between real and synthetic data generated by <bold><italic>G</italic></bold>. The latent space provides a low dimensional encoding of the data that can be used for downstream tasks, or probed to explore to see what features the model has learned to encode. However, standard GANs do not come equipped with an ‘encoder’ that can map a data point to its latent vector representation, which limits its use as a representation learning tool. A class of architectures that do provide such capabilities are Variational Autoencoders (VAEs). VAEs consist of two neural networks, an encoder that maps data into a low-dimensional latent space, and a decoder that generates data from its learned latent representation. The benefit of using VAEs for representation learning is that, unlike GANs, they are bidirectional so not only can we generate data from points in the latent space but we can also map (real or synthetic data) into its latent representation. However, compared to GANs, VAEs are considered less capable of creating realistic synthetic data. For this reason, we opt to use the GAN modelling approach over a VAE architecture.</p><p id="P6">To address these shortcomings, we developed BiWaveGAN, a bidirectional GAN [<xref ref-type="bibr" rid="R13">Donahue et al., 2016</xref>] which includes an additional encoder network, <bold><italic>E</italic></bold>, which learns to encode the data, mapping waveforms onto their latent representations. The output of the encoder network is incorporated into the loss function to ensure it learns an accurate encoding and is optimised jointly alongside the generator and discriminator. To our knowledge, this is the first reported work to apply bidirectional, encoding GANs to the problem of representing and synthesising realistic animal vocalisations.</p><p id="P7">Note that there are various other approaches to generative modelling that may perform well, such as autoregressive models (e.g., WaveNet [<xref ref-type="bibr" rid="R33">Oord et al., 2016</xref>] for audio generation) and diffusion models (state-of-the-art for image modelling) [<xref ref-type="bibr" rid="R11">Dhariwal and Nichol, 2021</xref>]—however, it is not obvious how to adapt such models for bidirectional representation learning.</p><p id="P8">In <xref ref-type="fig" rid="F1">Figures 1</xref> and <xref ref-type="supplementary-material" rid="SD1">S1</xref> we present spectrograms of several samples drawn by the generator, alongside spectrograms of real USVs recorded from mice. The generator is able to produce a wide variety of vocalisations which indicates that no significant mode collapse has occurred. While the model’s output is varied, some syllables are generated better than others, with longer and more complicated syllables being poorly generated. This is likely because they are less common in the dataset so the model cannot learn to generate them as well as the more common shorter syllables. Despite being of high fidelity, the generated samples still possess distinguishable characteristics that differentiate them from real data. These characteristics manifest as artefacts present both audibly and visually in the spectrograms of the generated samples. In particular, when played back (slowed down by a factor of 16 to be audible) the samples do mostly achieve the distinctive whistling timbre of a USV, as well as recognisable shapes in spectrograms, but a repetitive stuttering background noise can be heard. These artefacts are likely the consequence of the generator’s upsampling procedure, and it remains unclear to what extent they would disappear upon further training.</p><p id="P9">Moreover, to evaluate the fidelity of the encoder and generator in their mutual inversion, we compare actual USVs from the dataset with their corresponding reconstructions, represented as <bold><italic>G</italic></bold>(<bold><italic>E</italic></bold>(<italic>x</italic>)) where <italic>x</italic> denotes the test syllable. Spectrograms of these reconstructions are presented in <xref ref-type="fig" rid="F1">Figure 1</xref>. The model is capable of reconstructing a wide array of vocalisations well, and even in instances where the real USV is not accurately reconstructed, the reconstruction looks realistic and displays similar temporal and spectral content when compared to the original.</p><p id="P10">Additionally, we explored the latent space further by examining 10-point linear interpolations between two points in the latent space. These interpolations allow us to assess the smoothness of the representation. The results of these interpolations are shown in <xref ref-type="fig" rid="F2">Figure 2</xref>. While the reconstructions themselves are imperfect, it is clear that the model has learned to smoothly interpolate between USVs, even if they are very different from each other. Further, the intermediate waveforms look like reasonable USVs. This shows that the model has successfully learned a continuous latent representation of the data. The fact that the model is able to show us believable interpolations between USVs that would be considered categorically different shows that the syllables are not clustered. If the syllables were tightly clustered, we would not observe gradual change in interpolations but sharp changes as the representation ‘jumps’ from one class to the other.</p></sec><sec id="S4"><title><italic>In Vivo</italic> Neural Responses to Reconstructed Stimuli</title><p id="P11">Spiking activity from a total of 46 single units was recorded from the right auditory cortex of 6 anaesthetised female C57BL/6J mice, during presentation of natural and BiWaveGAN-generated USVs. An example of one unit’s spiking activity in response to three syllables is shown in <xref ref-type="fig" rid="F3">Figure 3</xref>. The power spectra of natural and reconstructed USVs were compared to ensure that any potential differences in neural response were not simply due to differences in the spectral profile of the stimuli (<xref ref-type="fig" rid="F4">Fig. 4A</xref>).</p><p id="P12">The inherent variability of neural responses sets an upper limit on the correlation coefficient between responses to repeated presentations of a stimulus. This limit can be estimated by calculating the expected correlation coefficient indicating how consistently a stimulus evokes a neural response [<xref ref-type="bibr" rid="R23">Hsu et al., 2004</xref>, <xref ref-type="bibr" rid="R43">Touryan et al., 2005</xref>]. The expected correlation coefficients in response to both types of stimuli are approximately equal (<xref ref-type="fig" rid="F4">Fig. 4B</xref>).</p><p id="P13">Maximum Noise Entropy (MNE) models were then trained for each unit, for natural USVs and reconstructed stimuli. An example of a unit’s excitatory and inhibitory features learned by the MNE model in response to reconstructed stimuli is shown in <xref ref-type="fig" rid="F4">Figure 4E</xref>. The performance of each model, as measured by the correlation coefficient between the predicted response and the real response in the unseen test set, is shown in <xref ref-type="fig" rid="F4">Figure 4C</xref>. These correlation coefficients are plotted against their upper bounds. One can see that the models’ prediction accuracies with the two classes of stimuli, natural and reconstructed USVs, are very similar.</p><p id="P14">The MNE models were then tested on the unseen test set of the other stimulus class, such that a model trained on natural USV stimulus activity was tested on a section of reconstructed USV stimulus activity, and vice versa. Model performance was compared to the performance using the test set of the same stimulus type (<xref ref-type="fig" rid="F4">Fig.4D</xref>). Model performance is approximately equal between the two test data sets, further indicating that the natural and reconstructed USVs are equally suitable stimuli for estimating neurons’ receptive fields.</p><p id="P15">The similarity between MNE model features learned with the two stimuli was further shown (<xref ref-type="fig" rid="F4">Fig. 4F</xref>) by projecting the features onto a low-dimensional manifold using the UMAP algorithm [<xref ref-type="bibr" rid="R31">McInnes et al., 2020</xref>].</p><p id="P16">Two measures of sparseness were then calculated to characterise the selectivity with which the neural population responds to the two types of stimuli [<xref ref-type="bibr" rid="R47">Willmore and Tolhurst, 2001</xref>]. The lifetime sparseness was calculated for each unit to indicate the degree of selectivity each unit has for different syllables within the two stimulus ensembles (<xref ref-type="fig" rid="F5">Fig. 5A</xref>). Lifetime sparseness values near 100% indicate sparse coding, in which neurons respond strongly to only a few syllables, whereas values close to 0% indicate non-selective activity in response to all syllables in the stimulus ensemble. A Wilcoxon matched-pairs signed-rank test revealed a significant but small difference in the lifetime sparseness between the two stimuli (N=46 units; median and interquartile range (IQR) for natural USVs: 0.43 (0.32); for reconstructed: 0.40 (0.30); P=3.71e-6), with units showing a median decrease in sparseness of approximately 4% for the reconstructed stimulus.</p><p id="P17">Population sparseness was then calculated to assess whether the two types of stimuli evoke more or less sparse codes: unlike lifetime sparseness, population sparseness characterises the responses of a set of neurons to each individual syllable rather than responses of each individual neuron to a set of syllables. Population sparseness values close to 0% indicate a dense code in which stimuli are encoded by a large proportion of the neural population, whereas population sparseness values close to 100% indicate a sparse code in which each stimulus is encoded by only a small fraction of the neural population. Note that as our units were recorded from multiple animals and only stimulus-responsive units are included in this analysis, this does not represent a true description of the full population response [<xref ref-type="bibr" rid="R22">Hromádka et al., 2008</xref>]. Rather, we use this metric only to compare the responses to the two types of stimuli. The population sparseness across all 519 syllables of both types of stimuli are shown in <xref ref-type="fig" rid="F5">Fig. 5C</xref>. A Wilcoxon matched-pairs signed-rank test revealed no significant difference in population sparseness between them (median (IQR) for natural USVs: 0.63 (0.07); for reconstructed: 0.62 (0.08); P=0.27).</p><p id="P18">Lastly, to explain the apparent difference in findings between the two sparseness metrics, each unit’s response to each syllable was classified as either ‘strong’ or ‘weak’ by thresholding the PSTH. The maximum value of the PSTH (i.e., the maximum average firing rate) in response to either stimulus was taken as the maximal response. A response was classified as ‘strong’ if it exceeded 50% of this value, and as ‘weak’ if it exceeded 25% of this value. <xref ref-type="fig" rid="F5">Figure 5E</xref> shows the number of syllables evoking ‘strong’ responses for each unit, and <xref ref-type="fig" rid="F5">Figure 5G</xref> shows the sum of ‘strong’ and ‘weak’ responses. Although a significantly greater number of natural USV syllables evoke a strong response compared to reconstructed syllables (median (IQR) for natural USVs: 17 (20); for reconstructed: 10 (16); Wilcoxon matched-pairs signed-rank test, P=1.50e-4), both stimulus types evoke the same number of responses when including both strong and weak responses (median (IQR) for natural USVs: 79 (140); for reconstructed: 81 (143); Wilcoxon matched-pairs signed-rank test, P=0.64). This result indicates that the small difference in lifetime sparseness between the two stimuli is driven by a small number of very strong responses, which occur more often with the natural USV stimulus, resulting in increased lifetime sparseness. As population sparseness measures the distribution of responses over the population and is invariant to the magnitude of the response, this is shown to be the same between the two stimuli.</p></sec></sec><sec id="S5" sec-type="discussion"><title>Discussion</title><p id="P19">The use of natural stimuli to probe sensory systems has been shown to improve our ability to model and predict responses of single neurons [<xref ref-type="bibr" rid="R41">Talebi and Baker, 2012</xref>, <xref ref-type="bibr" rid="R27">Laudanski et al., 2012</xref>]. The improvement in model performance is associated with a distinct difference in the estimated receptive field structure [<xref ref-type="bibr" rid="R42">Theunissen et al., 2000</xref>, <xref ref-type="bibr" rid="R9">David et al., 2004</xref>]. At the population level as well there appears to be a marked difference in the responses to artificial and natural stimuli [<xref ref-type="bibr" rid="R24">Hénaff et al., 2021</xref>]. One can attribute this discrepancy to the inability of artificial stimuli to sample the full input space and thus biasing computational models [<xref ref-type="bibr" rid="R40">Sharpee, 2013</xref>]. In the case of auditory stimuli, artificial sounds such as pure tones are unable to capture the complex correlation structure that is present in natural vocalisations or recordings of the natural environment. Whilst efforts have been made to devise artificial stimuli that can overcome this limitation [<xref ref-type="bibr" rid="R10">Depireux et al., 2001</xref>, <xref ref-type="bibr" rid="R3">Atencio and Schreiner, 2012</xref>], they remain inferior to using natural vocalisations [<xref ref-type="bibr" rid="R27">Laudanski et al., 2012</xref>].</p><p id="P20">We aim to bridge the gap between artificial and natural stimuli by providing a method to systematically generate naturalistic vocalisations with near-identical properties to vocalisations produced naturally. We show that these stimuli evoke comparable neural responses, that the receptive fields obtained using synthesised stimuli have the same predictive power as those obtained with natural stimuli, and that these receptive-field features form a single cluster when projected in the low-dimensional UMAP space.</p><p id="P21">By exposing the latent space used to generate these vocalisations, it is also possible to sample the sensory input space in a more principled manner by drawing homogeneously spaced samples from the latent space. This could improve the performance of existing models of sensory processing by reducing the bias introduced by <italic>ad hoc</italic> stimulus selection. Additionally, it is possible to smoothly interpolate between different vocalisations (<xref ref-type="fig" rid="F2">Fig. 2</xref>) which can be used to study decision making in a more natural context [<xref ref-type="bibr" rid="R37">Sainburg et al., 2022</xref>]. Similar methods exist for the generation of vocalisations [<xref ref-type="bibr" rid="R39">Sainburg et al., 2020</xref>, <xref ref-type="bibr" rid="R2">Arneodo et al., 2021</xref>], however, these models generate spectrograms whose phase must be reconstructed to produce waveforms, while our model produces waveforms directly.</p><p id="P22">Generative models like BiWaveGAN can be used to discern the structure of animal communications in an unsupervised, unbiased manner. While valuable in their own right, these models could be particularly useful for researchers relying on the playback of vocalisations as stimuli to study the neurological and behavioural responses of animals. Using such a model in this manner eliminates the need to record large volumes of vocalisations from animals, which can be time consuming in the laboratory and difficult in the wild [<xref ref-type="bibr" rid="R6">Chabout et al., 2017</xref>]. Once trained, the model can efficiently generate large volumes of diverse samples which can be continuously varied via interpolation in the latent space, and clusters of highly similar but not identical samples can be produced by sampling nearby points in latent space. In addition, such models are not restricted to any one type of animal but could include multiple species. This would allow for interpolation between vocalisations of different species to study the importance of ethological relevance in auditory encoding.</p></sec><sec id="S6" sec-type="methods | materials"><title>Methods and Materials</title><sec id="S7"><title>Dataset</title><p id="P23">Samples of ultrasonic vocalisations were acquired from data recorded by <bold><italic>Van Segbroeck et al.</italic></bold> [<bold><italic>2017</italic></bold>]. Briefly, the data consist of spontaneous vocalisations from two inbred strains (C57Bl/6J and DBA/2J) recorded at a sample rate of 250 kHz. These recordings were then processed and segmented by segmentation software (MUPET [<xref ref-type="bibr" rid="R44">Van Segbroeck et al., 2017</xref>], using default parameters). This produced a total of 31475 individual syllables which were randomly divided into a training set of 28329 syllables and a held-out test set of 3164 syllables (90% and 10% of the total data, respectively). <xref ref-type="supplementary-material" rid="SD1">Figure S1</xref> shows a sample of the data visualised as spectrograms. Before training the model, the length of individual audio samples was adjusted to accommodate the WaveGAN architecture which requires equal sample length. The length was chosen to be 2<sup>15</sup> = 32768 samples, which equates to approximately 130 ms when sampled at 250 kHz. Samples were either zero-padded or truncated to achieve the desired length.</p></sec><sec id="S8"><title>Model Architecture</title><p id="P24">Given a dataset of samples <bold><italic>X</italic></bold> = {<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, …, <italic>x<sub>N</sub></italic>} where <italic>x<sub>i</sub></italic> is the waveform of an individual USV syllable, we assume these data have been drawn from an unknown probability distribution <italic>p<sub>X</sub></italic>(<italic>x</italic>). The task of the generative model is to learn a distribution <inline-formula><mml:math id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> that approximates the true underlying distribution <italic>p<sub>X</sub></italic>(<italic>x</italic>) of the data. Here <italic>θ</italic> represents the parameterising variables of the model, and training the generative model means iteratively optimising <italic>θ</italic> such that <inline-formula><mml:math id="M2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> approximates <italic>p<sub>X</sub></italic>(<italic>x</italic>). Generating new data then simply requires sampling data points <inline-formula><mml:math id="M3"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> from the model distribution.</p><p id="P25">We construct the generative model following the WaveGAN architecture [<xref ref-type="bibr" rid="R12">Donahue et al., 2019</xref>]. The generator consists mainly of 1D transpose convolutional layers. In these layers the input is upsampled by a factor of 4 with new elements set to zero; this is followed by a convolution with a kernel of length 25 whose weights are to be optimised during training. Each layer is followed by a <bold><italic>ReLU</italic></bold> (Rectified Linear Unit) activation function, except the last layer which is followed by a <bold><italic>tanh</italic></bold> activation function (<xref ref-type="table" rid="T1">Table 1</xref>, left). The output of these layers is twice as long as the input, therefore by concatenating several layers the generator can expand the latent vector into a full waveform.</p><p id="P26">The discriminator (or ‘critic’) consists of three sub-networks. First, a waveform <italic>x</italic> is processed by <bold><italic>D</italic></bold><sub><italic>x</italic></sub>, and a latent vector <italic>z</italic> is processed by <bold><italic>D</italic></bold><sub><italic>z</italic></sub>. <bold><italic>D</italic></bold><sub><italic>x</italic></sub> consists of 1D convolutional layers with kernels of length 25 and a stride of 4 (except for the first convolutional layer, which has a stride of 2). Each layer is followed by a <bold><italic>LeakyReLU</italic></bold> activation function with a negative slope of 0.2 and then a phase shuffle operation (<xref ref-type="table" rid="T2">Table 2</xref>). A phase shuffle operation randomly perturbs the phase of its input by a random amount. This is necessary because the up-sampling performed by the generator causes artefacts in the generated audio that occur with a particular phase which the discriminator can exploit. By permuting the phase, the discriminator is made to be roughly invariant to the phase of the input waveform [<xref ref-type="bibr" rid="R12">Donahue et al., 2019</xref>]. <bold><italic>D</italic></bold><sub><italic>z</italic></sub> consists of three convolutional layers with a kernel length of 1 and a stride of 1, followed by a <bold><italic>LeakyRELU</italic></bold> activation function.</p><p id="P27">Afterwards, the two feature maps produced by <bold><italic>D</italic></bold><sub><italic>x</italic></sub> and <bold><italic>D</italic></bold><sub><italic>z</italic></sub> are concatenated and fed into the final network <bold><italic>D</italic></bold><sub>joint</sub> which produces the final output. <bold><italic>D</italic></bold><sub>joint</sub> also consists of three convolutional layers with a kernel length of 1 and a stride of 1, followed by a <bold><italic>LeakyRELU</italic></bold> activation function, except for the final layer which has no activation function. Therefore <bold><italic>D</italic></bold>(<italic>x</italic>, <italic>z</italic>) = <bold><italic>D</italic></bold><sub><italic>joint</italic></sub>(Concat(<bold><italic>D</italic></bold><sub><italic>x</italic></sub>(<italic>x</italic>), <bold><italic>D</italic></bold><sub><italic>z</italic></sub>(<italic>z</italic>)).</p><p id="P28">Similarly, the encoder network used to sample the model distribution, <inline-formula><mml:math id="M4"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, consists of one-dimensional convolutions followed by <bold><italic>LeakyRELU</italic></bold> activation functions. The kernel lengths were chosen to be the inverse of the generator’s. The stride of each convolution layer is 4, except for the first convolution layer where it is 2. Unlike the discriminator network, there are no phase shuffle operations (<xref ref-type="table" rid="T1">Table 1</xref>, right).</p><p id="P29"><xref ref-type="table" rid="T1">Table 1</xref> shows the architectures of <bold><italic>G</italic></bold> and <bold><italic>E</italic></bold>, placed side by side for comparison. Notice how <bold><italic>E</italic></bold> has a roughly inverse architecture to <bold><italic>G</italic></bold>, each TransConv1D in <bold><italic>G</italic></bold> having a corresponding Conv1D in <bold><italic>E</italic></bold>, since we want <bold><italic>G</italic></bold> and <bold><italic>E</italic></bold> to be approximate inverses. <xref ref-type="fig" rid="F6">Figure 6</xref> displays the full architecture of all three models.</p><p id="P30">The dataset, model training script and model weights can be accessed via GitLab <sup><xref ref-type="fn" rid="FN1">1</xref></sup>.</p></sec><sec id="S9"><title>Model Training</title><p id="P31">As stated previously, GANs consist of a generator network, <bold><italic>G</italic></bold>, and a discriminator network, <bold><italic>D</italic></bold>, that work against each other during the training process. <bold><italic>G</italic></bold> maps latent vectors <italic>z</italic> to artificial data points <bold><italic>G</italic></bold>(<italic>z</italic>), while <bold><italic>D</italic></bold> takes inputs from the data domain and outputs a value in [0, 1]. <bold><italic>D</italic></bold>(<italic>x</italic>) can be interpreted as the probability according to <bold><italic>D</italic></bold> that <italic>x</italic> comes from the real data distribution, so a perfect discriminator satisfies <bold><italic>D</italic></bold>(<italic>x</italic>) = 1 for all real data <italic>x</italic> and <bold><italic>D</italic></bold>(<bold><italic>G</italic></bold>(<italic>z</italic>)) = 0 for all fake data <bold><italic>G</italic></bold>(<italic>z</italic>). Let <italic>p<sub>X</sub></italic> be the true underlying distribution of our data and <italic>p<sub>z</sub></italic> be the distribution of our latent variable (defined as either a Gaussian or uniform distribution). In the original GAN formulation, the training objective is as follows [<xref ref-type="bibr" rid="R17">Goodfellow et al., 2014</xref>]: <disp-formula id="FD1"><label>(1)</label><mml:math id="M5"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:munder><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:munder><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P32">This means that <bold><italic>D</italic></bold> attempts to maximise its output for real data and minimise its output for fake data generated by <bold><italic>G</italic></bold>, while <bold><italic>G</italic></bold> attempts to maximise <bold><italic>D</italic></bold>’s output for the fake data that it generates. The weights of <bold><italic>G</italic></bold> and <bold><italic>D</italic></bold> can be optimised via iterative methods, e.g., stochastic gradient descent.</p><p id="P33">Due to the adversarial nature of their training, GANs can be unstable and suffer from a phenomenon known as mode collapse in which the generator only learns to generate a subset of the data distribution, resulting in less variety in the generated data than in the real data. To alleviate these shortcomings, <bold><italic>Arjovsky et al.</italic></bold> [<bold><italic>2017</italic></bold>] developed a variant known as Wasserstein GAN or WGAN.</p><p id="P34">It can be shown that the original GAN optimisation problem in <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> is equivalent to minimising the Jensen-Shannon divergence between the distributions <italic>p<sub>X</sub></italic> and <italic>p</italic><sub><italic>G</italic>(<italic>Z</italic>)</sub> (i.e., the real and fake data). However, the authors argue that it is more beneficial to optimise the Wasserstein-1 distance instead, which can be expressed as follows: <disp-formula id="FD2"><label>(2)</label><mml:math id="M6"><mml:mrow><mml:mi>W</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi>sup</mml:mi></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:mi>f</mml:mi><mml:msub><mml:mo>∥</mml:mo><mml:mi>L</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P35">Note that <italic>f</italic> maps data points (real or fake) to any real number and ‖<italic>f</italic>‖<sub><italic>L</italic></sub> &lt; 1 means that <italic>f</italic> is 1-Lipschitz continuous, i.e., its derivative (assuming <italic>f</italic> is differentiable) is bounded by 1: |∇<sub><italic>x</italic></sub><italic>f</italic>(<italic>x</italic>)| ≤ 1 for all <italic>x</italic>. Thus the Wasserstein-1 distance is the <italic>supremum</italic> of <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> over the set of 1-Lipschitz continuous functions.</p><p id="P36">Therefore in a WGAN we replace <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> with the following optimisation objective: <disp-formula id="FD3"><label>(3)</label><mml:math id="M8"><mml:mrow><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mi>G</mml:mi></mml:munder><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:munder><mml:msub><mml:mi>V</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P37">Note that here <bold><italic>D</italic></bold> is playing the role of <italic>f</italic> in <xref ref-type="disp-formula" rid="FD2">Eq. (2)</xref>, and its job is no longer to distinguish between real and fake inputs but to assist in computing the Wasserstein-1 distance by maximising <bold><italic>V<sub>W</sub></italic></bold> (<bold><italic>D</italic></bold>, <bold><italic>G</italic></bold>)—hence it is often referred to as a ‘critic’ rather than a ‘discriminator’. Therefore <italic>D</italic>’s outputs are not restricted to the range [0, 1] but can be any real number.</p><p id="P38">Recall that the definition of the Wasserstein-1 distance requires <italic>f</italic> to be 1-Lipschitz continuous, but <bold><italic>D</italic></bold> is in practice a neural network which does not necessarily satisfy this property. To enforce the 1-Lipschitz continuity of <bold><italic>D</italic></bold>, <bold><italic>Gulrajani et al.</italic></bold> [<bold><italic>2017</italic></bold>] propose Wasserstein GAN with Gradient Penalty (WGAN-GP). This means adding a term onto the loss function of <bold><italic>D</italic></bold> that punishes <bold><italic>D</italic></bold> when its gradients are larger than one.</p><p id="P39">The encoder, <bold><italic>E</italic></bold>, is trained adversarially alongside <bold><italic>G</italic></bold> and <bold><italic>D</italic></bold>. Furthermore, we must modify our discriminator so that it accepts as input both a data point and a latent vector. During training, instead of showing the discriminator <italic>x</italic> or <bold><italic>G</italic></bold>(<italic>z</italic>), we show it the tuples (<italic>x</italic>, <bold><italic>E</italic></bold>(<italic>x</italic>)) or (<bold><italic>G</italic></bold>(<italic>z</italic>), <italic>z</italic>).</p><p id="P40">The adversarial objective of a BiGAN is as follows: <disp-formula id="FD4"><label>(4)</label><mml:math id="M9"><mml:mrow><mml:munder><mml:mrow><mml:mi>min </mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi> max</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:munder><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>Bi</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>log</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P41">Note the similarities between this and <xref ref-type="disp-formula" rid="FD1">Eq. (1)</xref> for regular GANs, and that <bold><italic>E</italic></bold> and <bold><italic>G</italic></bold> have the same objective. It can be shown that in order for <bold><italic>G</italic></bold> and <bold><italic>E</italic></bold> to ‘fool’ a perfect discriminator at a point in the joint space (<italic>x</italic>, <italic>z</italic>) they must satisfy <italic>x</italic> = <bold><italic>G</italic></bold>(<bold><italic>E</italic></bold>(<italic>x</italic>)) and <italic>z</italic> = <bold><italic>E</italic></bold>(<bold><italic>G</italic></bold>(<italic>z</italic>)), i.e., <bold><italic>E</italic></bold> and <bold><italic>G</italic></bold> must learn to be inverse to each other, as we desire.</p><p id="P42">Our aim is to adapt WaveGAN into a BiGAN. Since WaveGAN is a WGAN-GP model, we must first synthesise the optimisation objectives and training procedures of WGAN-GP and BiGAN. By doing this we get the stable training and improved convergence properties of WGAN-GP as well as the latent-representation enabled by BiGAN.</p><p id="P43">Looking at <xref ref-type="disp-formula" rid="FD3">Equations (3)</xref> and <xref ref-type="disp-formula" rid="FD4">(4)</xref> we see that they can be combined into: <disp-formula id="FD5"><label>(5)</label><mml:math id="M10"><mml:mrow><mml:munder><mml:mrow><mml:mi>min  </mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mi>D</mml:mi></mml:munder><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mtext>Bi</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>X</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="double-struck">E</mml:mi><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>Z</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P44">Training a model around this optimisation objective can then be done by accommodating the encoder into the WGAN-GP training procedure as can be seen in Alg. 1.</p><p id="P45">The model was trained using an Adam optimiser [<xref ref-type="bibr" rid="R25">Kingma and Ba, 2017</xref>] with a learning rate of 0.0001 for 150,000 iterations with a batch size of 64. Inputs to the generator are sampled from a Uniform[-1,1] distribution. The model was trained on an NVIDIA Titan Xp GPU.</p></sec><sec id="S10"><title>Surgical preparation</title><p id="P46">All procedures were carried out under the terms and conditions of licences issued by the UK Home Office under the Animals (Scientific Procedures) Act 1986. Extracellular recordings were made in the auditory cortex of adult female C57BL/6J mice (N=6, aged 6 to 11 weeks). Animals were anaesthetised using a mixture of fentanyl, midazolam and medetomidine (0.05, 5 and 0.5 mg/kg, respectively). A midline incision was made over the dorsal surface of the cranium and the right temporalis muscle was partially resected. Stereotaxic coordinates were used to locate the right auditory cortex, using a rostro-caudal coordinate of 70% bregma-lambda and a dorso-ventral coordinate of bregma -2.2 mm (the lateral coordinate being determined by the surface of the skull). A steel head-plate comprising a bent piece of flat bar (approximately 5 mm x 30 mm) was attached to the dorsal surface of the skull using a combination of tissue adhesive (Histoacryl) and dental cement (Kemdent works, Swindon, UK). This was subsequently used in combination with a magnetic stand to secure the animal in place for the remainder of the surgery and recordings.</p><boxed-text id="BX1" position="anchor" content-type="below"><caption><title>Algorithm 1</title></caption><p id="P47">Training a Bidirectional WGAN-GP via minibatch gradient descent. Parameters are:</p><p id="P48"><italic>n<sub>iter</sub></italic> - the number of iterations to train for</p><p id="P49"><italic>m</italic> - the batch size</p><p id="P50"><italic>k</italic> - the number of optimisation steps <bold><italic>D</italic></bold> takes per iteration (<bold><italic>G</italic></bold> takes only one per iteration)</p><p id="P51"><italic>θ<sub>D</sub></italic> - the parameters of <bold><italic>D</italic></bold> to be optimised</p><p id="P52"><italic>θ<sub>E</sub></italic> - the parameters of <bold><italic>E</italic></bold> to be optimised</p><p id="P53"><italic>θ<sub>G</sub></italic> - the parameters of <bold><italic>G</italic></bold> to be optimised</p><p id="P54"><italic>η</italic> - the learning rate used in stochastic gradient descent</p><p id="P55">  1: <bold>for</bold> <italic>n</italic> = 1, …, <italic>n<sub>iter</sub></italic> <bold>do</bold></p><p id="P56">  2:   <bold>for</bold> <italic>i</italic> = 1, …, <italic>k</italic> <bold>do</bold></p><p id="P57">  3:   Sample <italic>z</italic><sup>(1)</sup>, …, <italic>z</italic><sup>(<italic>m</italic>)</sup> ~ <italic>p<sub>Z</sub></italic></p><p id="P58">  4:   Sample <italic>x</italic><sup>(1)</sup>, …, <italic>x</italic><sup>(<italic>m</italic>)</sup> ~ <italic>p<sub>X</sub></italic></p><p id="P59">  5:   <inline-formula><mml:math id="M11"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P60">  6:   <inline-formula><mml:math id="M12"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></p><p id="P61">  7:   <bold>end for</bold></p><p id="P62">  8:   Sample <italic>z</italic><sup>(1)</sup>, …, <italic>z</italic><sup>(<italic>m</italic>)</sup> ~ <italic>p<sub>Z</sub></italic></p><p id="P63">  9:   Sample <italic>x</italic><sup>(1)</sup>, …, <italic>x</italic><sup>(<italic>m</italic>)</sup> ~ <italic>p<sub>X</sub></italic></p><p id="P64">10:   <inline-formula><mml:math id="M13"><mml:mrow><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>m</mml:mi></mml:mfrac><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:mo stretchy="false">[</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></p><p id="P65">11:   <inline-formula><mml:math id="M14"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p><p id="P66">12:   <inline-formula><mml:math id="M15"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:msub><mml:mo>∇</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>ℒ</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula></p><p id="P67">13: <bold>end for</bold></p></boxed-text><p id="P68">A small (ø=2mm) craniotomy was made over the auditory cortex using a dental drill and burr, and a small hole was made in the dura. The surface of the brain was protected from desiccation by regular application of silicone oil. A machine screw (M1 x 2 mm) was inserted into the skull approximately over the contralateral motor cortex to act as a ground for the probe. The animal was then transferred to an acoustic chamber, and once again secured by fixation of the headpost in a magnetic stand. Body temperature was maintained throughout the duration of the experiment using activated Deltaphase isothermal pads (Braintree Scientific, Braintree, USA).</p></sec><sec id="S11"><title>Syllable preprocessing and Stimulus Presentation</title><p id="P69">Auditory stimuli consisting of 519 syllables of natural and BiWaveGAN-reconstructed USVs were generated. Syllables were first filtered by 6th-order Chebyshev Type II filters (40 kHz highpass and 100 kHz lowpass), and noise was reduced using the Noise reduction tool in Audacity <sup><xref ref-type="fn" rid="FN2">2</xref></sup>. Syllables were then concatenated, separated by 100ms of silence. Natural USV and reconstructed USV stimuli were played alternately for a total of at least 20 repetitions of each stimulus type. Stimuli were presented using an Avisoft UltraSoundGate Player 116 in combination with an Avisoft Vifa ultrasonic speaker (Avisoft Bioacoustics, Glienicke, Germany). Signal intensity was set such that the peak intensity did not exceed 85 dB SPL. For clarity, all spectrograms are displayed thresholded at 2 standard deviations above the mean power.</p></sec><sec id="S12"><title>Electrophysiological recording</title><p id="P70">The experimental setup consisted of a silicon multi-electrode probe (single-shank, 32-channel polytrode, Neuronexus) connected to a Neuronexus Smartbox Pro data acquisition system which amplified the signals and digitised them at a sampling rate of 30 kHz. The conductive area of the recording sites measured 177<italic>μm</italic><sup>2</sup>. Spike sorting was done offline using an automated algorithm (Kilosort3 [<xref ref-type="bibr" rid="R34">Pachitariu et al., 2023</xref>]). Units detected by Kilosort3 were subsequently manually inspected and curated using Phy [<xref ref-type="bibr" rid="R36">Rossant and Harris, 2013</xref>]. Units were considered to be well-isolated single units if their spike waveforms displayed consistent shapes and their autocorrelograms exhibited clear refractory periods. Units were then classified as being auditory by manual inspection of raster plots to identify stimulus-locked spikes. Units that were lost (e.g., due to electrode drift) resulting in recordings of fewer than 10 trials of stimulus presentation were excluded.</p></sec><sec id="S13"><title>Receptive field characterisation</title><p id="P71">The receptive fields of single units were characterised using the Maximum Noise Entropy (MNE) method [<xref ref-type="bibr" rid="R14">Fitzgerald et al., 2011</xref>]. Briefly, this method comprises fitting a model that describes the probability of a spike as a function of a stimulus, <bold>s</bold>, as <bold><italic>P</italic></bold>(<italic>spike</italic>|<bold>s</bold>) = (1 + <italic>exp</italic>(<italic>a</italic> + <bold>s</bold><italic>h</italic> + <bold>s</bold><sup><italic>T</italic></sup><bold><italic>J</italic>s</bold>)<sup>−1</sup>, where <italic>a</italic>, <italic>h</italic> &amp; <bold><italic>J</italic></bold> are parameters optimised via gradient descent. These parameters are determined such that the predicted firing rate, spike-triggered average and spike-triggered covariance match those in the observed data. The specific parameters used for the spectro-temporal representation of the stimulus data were chosen through bioacoustical analysis, as detailed in our previous work [<xref ref-type="bibr" rid="R30">Lu et al., 2023</xref>].</p><p id="P72">The MNE model for each unit was trained with 80% of the stimulus-response data. The remaining 20% of the data were used to test the model. The training data were randomly extracted from the full recording without replacement to minimise the effects of any non-stationarity of the data. After training the MNE model for each unit, the model was used to predict a response to the test stimulus. The correlation coefficient between the model predictions and the recorded neural activity was calculated to assess the performance of the model.</p><p id="P73">Neural activity of a single unit is not perfectly correlated between repeated presentations of the stimulus; this affects how the performance of a model based on its correlation coefficient with recorded data is interpreted. To account for this inherent variability in the neural activity, we estimate the expected correlation coefficient between the responses to repeated presentations of the same stimulus in the test set [<xref ref-type="bibr" rid="R23">Hsu et al., 2004</xref>, <xref ref-type="bibr" rid="R43">Touryan et al., 2005</xref>]. The expected correlation coefficient <inline-formula><mml:math id="M16"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is defined as the correlation between the true or actual firing rate, <bold><italic>A</italic></bold>, and the firing rate <inline-formula><mml:math id="M17"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> as measured by averaging over <bold><italic>M</italic></bold> repeats. As detailed in [<xref ref-type="bibr" rid="R23">Hsu et al., 2004</xref>], we can calculate <inline-formula><mml:math id="M18"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> using the following equation: <disp-formula id="FD6"><mml:math id="M19"><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>M</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula> where <inline-formula><mml:math id="M20"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the correlation between <inline-formula><mml:math id="M21"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="M22"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. <inline-formula><mml:math id="M23"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the firing rate estimated by averaging over <bold><italic>M</italic></bold>/2 repetitions and <inline-formula><mml:math id="M24"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the firing rate estimated by averaging over the other <bold><italic>M</italic></bold>/2 repetitions of the neural response.</p><p id="P74">The correlation coefficients between the model prediction and the test set were then scaled by the expected correlation of the test set.</p></sec><sec id="S14"><title>Feature comparison</title><p id="P75">To compare the similarity between MNE features estimated from neurons’ responses to real or reconstructed USVs, we projected the modulation power spectra of the features into a 2D latent space using the UMAP algorithm [<xref ref-type="bibr" rid="R31">McInnes et al., 2020</xref>] with parameters <italic>n</italic>_<italic>neighbors</italic> = 256, <italic>min</italic>_<italic>dist</italic> = 0, <italic>n</italic>_<italic>dim</italic> = 2 and <italic>metric</italic> = ‘<italic>euclidean</italic>’. Modulation power spectra were obtained by computing the two-dimensional Fourier transform of each spectrogram as described in <bold><italic>Atencio and Sharpee</italic></bold> [<bold><italic>2017</italic></bold>].</p></sec><sec id="S15"><title>Sparseness Measurement</title><p id="P76">Lifetime and population sparseness were calculated using the definition for sparseness given in [<xref ref-type="bibr" rid="R45">Vinje and Gallant, 2000</xref>]: <disp-formula id="FD7"><mml:math id="M25"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:msup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mi>n</mml:mi></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P77">In the case of lifetime sparseness, <bold><italic>r</italic></bold><sub><italic>i</italic></sub> represents the average firing rate of a single unit in response to the <italic>i<sup>th</sup></italic> stimulus in the ensemble of n individual stimuli; in the case of population sparseness, <italic>r<sub>i</sub></italic> represents the average firing rate of the <italic>i<sup>th</sup></italic> unit in the population of n units, during the presentation of a single stimulus. Lifetime sparseness thereby characterises the response of an individual unit to a set of stimuli, whereas population sparseness characterises the response of a whole population of units to a single stimulus. Lifetime sparseness values near 0 indicate a dense code, in which the neuron responds with equal firing rate to all stimuli in the ensemble, whereas values close to 1 indicate a sparse code in which the neuron responds highly selectively to only a few stimuli. Similarly, population sparseness values close to 0 indicate a dense code in which a large proportion of the population responds to a stimulus, whereas values close to 1 indicate that only a small fraction of the population responds to the stimulus.</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary</label><media xlink:href="EMS189811-Supplement-Supplementary.pdf" mimetype="application" mime-subtype="pdf" id="d9aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S16"><title>Funding</title><p id="P78">This work was funded by Biotechnology and Biological Sciences Research Council grant “How do auditory cortical neurons represent ethologically relevant natural stimuli? Characterizing stimulus feature selectivity and invariance” (BB/N008731/1).</p></ack><fn-group><fn id="FN1"><label>1</label><p id="P79"><ext-link ext-link-type="uri" xlink:href="https://gitlab.com/kozlovlabcode/biwavegan">https://gitlab.com/kozlovlabcode/biwavegan</ext-link></p></fn><fn id="FN2"><label>2</label><p id="P80"><ext-link ext-link-type="uri" xlink:href="https://audacity.com">https://audacity.com</ext-link></p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Wasserstein</surname><given-names>GAN</given-names></name></person-group><source>arXiv</source><year>2017</year><elocation-id>arXiv:1701.07875 [cs, stat]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.1701.07875</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arneodo</surname><given-names>EM</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Brown</surname><given-names>DE</given-names></name><name><surname>Gilja</surname><given-names>V</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><article-title>Neurally driven synthesis of learned, complex vocalizations</article-title><source>Current Biology</source><year>2021</year><volume>31</volume><issue>15</issue><fpage>3419</fpage><lpage>3425.e5</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0960982221007338">https://www.sciencedirect.com/science/article/pii/S0960982221007338</ext-link></comment><pub-id pub-id-type="pmcid">PMC8375361</pub-id><pub-id pub-id-type="pmid">34139192</pub-id><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.035</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Spectrotemporal processing in spectral tuning modules of cat primary auditory cortex</article-title><source>PloS One</source><year>2012</year><volume>7</volume><issue>2</issue><elocation-id>e31537</elocation-id><pub-id pub-id-type="pmcid">PMC3288040</pub-id><pub-id pub-id-type="pmid">22384036</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0031537</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atencio</surname><given-names>CA</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><article-title>Multidimensional receptive field processing by cat primary auditory cortical neurons</article-title><source>Neuroscience</source><year>2017</year><month>Sep</month><volume>359</volume><fpage>130</fpage><lpage>141</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0306452217304736">https://www.sciencedirect.com/science/article/pii/S0306452217304736</ext-link></comment><pub-id pub-id-type="pmcid">PMC5600511</pub-id><pub-id pub-id-type="pmid">28694174</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroscience.2017.07.003</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Carruthers</surname><given-names>IM</given-names></name><name><surname>Natan</surname><given-names>RG</given-names></name><name><surname>Geffen</surname><given-names>MN</given-names></name></person-group><article-title>Encoding of ultrasonic vocalizations in the auditory cortex</article-title><source>Journal of Neurophysiology</source><year>2013</year><month>Apr</month><volume>109</volume><issue>7</issue><fpage>1912</fpage><lpage>1927</lpage><publisher-name>publisher: American Physiological Society</publisher-name><pub-id pub-id-type="pmcid">PMC4073926</pub-id><pub-id pub-id-type="pmid">23324323</pub-id><pub-id pub-id-type="doi">10.1152/jn.00483.2012</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Jones-Macopson</surname><given-names>J</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><article-title>Eliciting and Analyzing Male Mouse Ultrasonic Vocalization (USV) Songs</article-title><source>JoVE (Journal of Visualized Experiments)</source><year>2017</year><month>May</month><issue>123</issue><comment><ext-link ext-link-type="uri" xlink:href="https://www.jove.com/v/54137/eliciting-and-analyzing-male-mouse-ultrasonic-vocalization-usv-songs">https://www.jove.com/v/54137/eliciting-and-analyzing-male-mouse-ultrasonic-vocalization-usv-songs</ext-link></comment><elocation-id>e54137</elocation-id><pub-id pub-id-type="pmcid">PMC5607930</pub-id><pub-id pub-id-type="pmid">28518074</pub-id><pub-id pub-id-type="doi">10.3791/54137</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chambers</surname><given-names>AR</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Sen</surname><given-names>K</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><article-title>Online Stimulus Optimization Rapidly Reveals Multidimensional Selectivity in Auditory Cortical Neurons</article-title><source>Journal of Neuroscience</source><year>2014</year><month>Jul</month><volume>34</volume><issue>27</issue><fpage>8963</fpage><lpage>8975</lpage><publisher-name>publisher: Society for Neuroscience Section: Articles</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/34/27/8963">https://www.jneurosci.org/content/34/27/8963</ext-link></comment><pub-id pub-id-type="pmcid">PMC4078078</pub-id><pub-id pub-id-type="pmid">24990917</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0260-14.2014</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coffey</surname><given-names>KR</given-names></name><name><surname>Marx</surname><given-names>RE</given-names></name><name><surname>Neumaier</surname><given-names>JF</given-names></name></person-group><article-title>DeepSqueak: a deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title><source>Neuropsychopharmacology</source><year>2019</year><month>Apr</month><volume>44</volume><issue>5</issue><fpage>859</fpage><lpage>868</lpage><publisher-name>Publisher: Nature Publishing Group</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41386-018-0303-6">https://www.nature.com/articles/s41386-018-0303-6</ext-link></comment><pub-id pub-id-type="pmcid">PMC6461910</pub-id><pub-id pub-id-type="pmid">30610191</pub-id><pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Natural Stimulus Statistics Alter the Receptive Field Structure of V1 Neurons</article-title><source>Journal of Neuroscience</source><year>2004</year><month>Aug</month><volume>24</volume><issue>31</issue><fpage>6991</fpage><lpage>7006</lpage><publisher-name>publisher: Society for Neuroscience Section: Behavioral/Systems/Cognitive</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/24/31/6991">https://www.jneurosci.org/content/24/31/6991</ext-link></comment><pub-id pub-id-type="pmcid">PMC6729594</pub-id><pub-id pub-id-type="pmid">15295035</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1422-04.2004</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Depireux</surname><given-names>DA</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name><name><surname>Klein</surname><given-names>DJ</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>Spectro-temporal response field characterization with dynamic ripples in ferret primary auditory cortex</article-title><source>Journal of Neurophysiology</source><year>2001</year><month>Mar</month><volume>85</volume><issue>3</issue><fpage>1220</fpage><lpage>1234</lpage><pub-id pub-id-type="pmid">11247991</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dhariwal</surname><given-names>P</given-names></name><name><surname>Nichol</surname><given-names>A</given-names></name></person-group><article-title>Diffusion Models Beat GANs on Image Synthesis</article-title><source>arXiv</source><year>2021</year><elocation-id>arXiv:2105.05233 [cs, stat]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2105.05233">http://arxiv.org/abs/2105.05233</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.2105.05233</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>C</given-names></name><name><surname>McAuley</surname><given-names>J</given-names></name><name><surname>Puckette</surname><given-names>M</given-names></name></person-group><article-title>Adversarial Audio Synthesis</article-title><source>arXiv</source><year>2019</year><elocation-id>arXiv:1802.04208 [cs]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.04208">http://arxiv.org/abs/1802.04208</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.1802.04208</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Krähenbühl</surname><given-names>P</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name></person-group><article-title>Adversarial Feature Learning</article-title><source>CoRR</source><year>2016</year><elocation-id>arXiv: 1605.09782</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.09782">http://arxiv.org/abs/1605.09782</ext-link></comment></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname><given-names>JD</given-names></name><name><surname>Rowekamp</surname><given-names>RJ</given-names></name><name><surname>Sincich</surname><given-names>LC</given-names></name><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><article-title>Second order dimensionality reduction using minimum and maximum mutual information models</article-title><source>PLoS Computational Biology</source><year>2011</year><volume>7</volume><issue>10</issue><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmcid">PMC3203063</pub-id><pub-id pub-id-type="pmid">22046122</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002249</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Fonseca</surname><given-names>AH</given-names></name><name><surname>Santana</surname><given-names>GM</given-names></name><name><surname>Ortiz</surname><given-names>GMB</given-names></name><name><surname>Bampi</surname><given-names>S</given-names></name><name><surname>Dietrich</surname><given-names>MO</given-names></name></person-group><article-title>Analysis of ultrasonic vocalizations from mice using computer vision and machine learning</article-title><source>eLife</source><year>2021</year><pub-id pub-id-type="doi">10.7554/eLife.59161</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goffinet</surname><given-names>J</given-names></name><name><surname>Brudner</surname><given-names>S</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Pearson</surname><given-names>J</given-names></name></person-group><article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title><source>eLife</source><year>2021</year><month>May</month><volume>10</volume><publisher-name>publisher: eLife Sciences Publications, Ltd</publisher-name><elocation-id>e67855</elocation-id><pub-id pub-id-type="pmcid">PMC8213406</pub-id><pub-id pub-id-type="pmid">33988503</pub-id><pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Warde-Farley</surname><given-names>D</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><chapter-title>Generative Adversarial Nets</chapter-title><person-group person-group-type="editor"><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name><name><surname>Cortes</surname><given-names>C</given-names></name><etal/></person-group><source>Advances in Neural Information Processing Systems</source><year>2014</year><volume>27</volume><publisher-name>Curran Associates, Inc.</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</ext-link></comment></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gourbal</surname><given-names>BEF</given-names></name><name><surname>Barthelemy</surname><given-names>M</given-names></name><name><surname>Petit</surname><given-names>G</given-names></name><name><surname>Gabrion</surname><given-names>C</given-names></name></person-group><article-title>Spectrographic analysis of the ultrasonic vocalisations of adult male and female BALB/c mice</article-title><source>Naturwissenschaften</source><year>2004</year><month>Aug</month><volume>91</volume><issue>8</issue><fpage>381</fpage><lpage>385</lpage><pub-id pub-id-type="pmid">15278217</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gulrajani</surname><given-names>I</given-names></name><name><surname>Ahmed</surname><given-names>F</given-names></name><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Dumoulin</surname><given-names>V</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><article-title>Improved Training of Wasserstein GANs</article-title><source>arXiv</source><year>2017</year><elocation-id>arXiv:1704.00028 [cs, stat]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.00028">http://arxiv.org/abs/1704.00028</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.1704.00028</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Haack</surname><given-names>B</given-names></name><name><surname>Markl</surname><given-names>H</given-names></name><name><surname>Ehret</surname><given-names>G</given-names></name></person-group><article-title>Sound communication between parents and offspring</article-title><source>The auditory psychobiology of the mouse</source><year>1983</year><fpage>57</fpage><lpage>97</lpage><publisher-name>Open Access Repositorium der Universität Ulm und Technischen Hochschule Ulm</publisher-name></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name></person-group><article-title>Ultrasonic Songs of Male Mice</article-title><source>PLoS Biology</source><year>2005</year><volume>3</volume><issue>12</issue><fpage>e386</fpage><pub-id pub-id-type="pmcid">PMC1275525</pub-id><pub-id pub-id-type="pmid">16248680</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hromádka</surname><given-names>T</given-names></name><name><surname>DeWeese</surname><given-names>MR</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><article-title>Sparse Representation of Sounds in the Unanesthetized Auditory Cortex</article-title><source>PLOS Biology</source><year>2008</year><month>Jan</month><volume>6</volume><issue>1</issue><fpage>e16</fpage><publisher-name>publisher: Public Library of Science</publisher-name><pub-id pub-id-type="pmcid">PMC2214813</pub-id><pub-id pub-id-type="pmid">18232737</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0060016</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Quantifying variability in neural responses and its application for the validation of model predictions</article-title><source>Network: Computation in Neural Systems</source><year>2004</year><month>Jan</month><volume>15</volume><issue>2</issue><fpage>91</fpage><lpage>109</lpage><pub-id pub-id-type="pmid">15214701</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hénaff</surname><given-names>OJ</given-names></name><name><surname>Bai</surname><given-names>Y</given-names></name><name><surname>Charlton</surname><given-names>JA</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Goris</surname><given-names>RLT</given-names></name></person-group><article-title>Primary visual cortex straightens natural video trajectories</article-title><source>Nature Communications</source><year>2021</year><month>Oct</month><volume>12</volume><issue>1</issue><fpage>5982</fpage><publisher-name>Publisher: Nature Publishing Group</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-021-25939-z">https://www.nature.com/articles/s41467-021-25939-z</ext-link></comment><pub-id pub-id-type="pmcid">PMC8514453</pub-id><pub-id pub-id-type="pmid">34645787</pub-id><pub-id pub-id-type="doi">10.1038/s41467-021-25939-z</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><year>2017</year><elocation-id>arXiv:1412.6980 [cs]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">http://arxiv.org/abs/1412.6980</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kozlov</surname><given-names>AS</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><article-title>Central auditory neurons have composite receptive fields</article-title><source>Proceedings of the National Academy of Sciences</source><year>2016</year><volume>113</volume><issue>5</issue><fpage>1441</fpage><lpage>1446</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.pnas.org/content/113/5/1441">https://www.pnas.org/content/113/5/1441</ext-link></comment><pub-id pub-id-type="pmcid">PMC4747712</pub-id><pub-id pub-id-type="pmid">26787894</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1506903113</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Laudanski</surname><given-names>J</given-names></name><name><surname>Edeline</surname><given-names>JM</given-names></name><name><surname>Huetz</surname><given-names>C</given-names></name></person-group><article-title>Differences between Spectro-Temporal Receptive Fields Derived from Artificial and Natural Stimuli in the Auditory Cortex</article-title><source>PLOS ONE</source><year>2012</year><month>Nov</month><volume>7</volume><issue>11</issue><publisher-name>publisher: Public Library of Science</publisher-name><elocation-id>e50539</elocation-id><pub-id pub-id-type="pmcid">PMC3507792</pub-id><pub-id pub-id-type="pmid">23209771</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0050539</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>RC</given-names></name><name><surname>Linden</surname><given-names>JF</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Improved cortical entrainment to infant communication calls in mothers compared with virgin mice</article-title><source>European Journal of Neuroscience</source><year>2006</year><volume>23</volume><issue>11</issue><fpage>3087</fpage><lpage>3097</lpage><pub-id pub-id-type="pmid">16819999</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>RC</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><article-title>Auditory Cortical Detection and Discrimination Correlates with Communicative Significance</article-title><source>PLOS Biology</source><year>2007</year><month>Jun</month><volume>5</volume><issue>7</issue><fpage>e173</fpage><publisher-name>publisher: Public Library of Science</publisher-name><pub-id pub-id-type="pmcid">PMC1891324</pub-id><pub-id pub-id-type="pmid">17564499</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0050173</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>S</given-names></name><name><surname>Ang</surname><given-names>GWY</given-names></name><name><surname>Steadman</surname><given-names>M</given-names></name><name><surname>Kozlov</surname><given-names>AS</given-names></name></person-group><article-title>Composite receptive fields in the mouse auditory cortex</article-title><source>The Journal of Physiology</source><year>2023</year><volume>601</volume><issue>18</issue><fpage>4091</fpage><lpage>4104</lpage><pub-id pub-id-type="pmid">37578817</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><source>arXiv</source><year>2020</year><elocation-id>arXiv:1802.03426 [cs, stat]</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.03426">http://arxiv.org/abs/1802.03426</ext-link></comment><pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Ngiam</surname><given-names>J</given-names></name><name><surname>Koh</surname><given-names>PW</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Bhaskar</surname><given-names>S</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><article-title>Sparse Filtering</article-title><source>Nips</source><year>2011</year><fpage>1</fpage><lpage>9</lpage><comment>ISBN: 9781618395993</comment></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oord</surname><given-names>AVD</given-names></name><name><surname>Dieleman</surname><given-names>S</given-names></name><name><surname>Zen</surname><given-names>H</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Kalchbrenner</surname><given-names>N</given-names></name><name><surname>Senior</surname><given-names>A</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name></person-group><article-title>WaveNet: A Generative Model for Raw Audio</article-title><year>2016</year><fpage>1</fpage><lpage>15</lpage><elocation-id>arXiv: 1609.03499 ISBN: 9783901882760</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03499">http://arxiv.org/abs/1609.03499</ext-link></comment></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Sridhar</surname><given-names>S</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name></person-group><article-title>Solving the spike sorting problem with Kilosort</article-title><source>bioRxiv</source><year>2023</year><comment>Section: New Results</comment><elocation-id>2023.01.07.523036</elocation-id><pub-id pub-id-type="doi">10.1101/2023.01.07.523036</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rose</surname><given-names>MC</given-names></name><name><surname>Styr</surname><given-names>B</given-names></name><name><surname>Schmid</surname><given-names>TA</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name><name><surname>Yartsev</surname><given-names>MM</given-names></name></person-group><article-title>Cortical representation of group social communication in bats</article-title><source>Science</source><year>2021</year><month>Oct</month><volume>374</volume><issue>6566</issue><publisher-name>publisher: American Association for the Advancement of Science</publisher-name><elocation-id>eaba9584</elocation-id><pub-id pub-id-type="pmcid">PMC8775406</pub-id><pub-id pub-id-type="pmid">34672724</pub-id><pub-id pub-id-type="doi">10.1126/science.aba9584</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossant</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>K</given-names></name></person-group><article-title>Hardware-accelerated interactive data visualization for neuroscience in Python</article-title><source>Frontiers in Neuroinformatics</source><year>2013</year><volume>7</volume><pub-id pub-id-type="pmcid">PMC3867689</pub-id><pub-id pub-id-type="pmid">24391582</pub-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00036</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>McPherson</surname><given-names>TS</given-names></name><name><surname>Arneodo</surname><given-names>EM</given-names></name><name><surname>Rudraraju</surname><given-names>S</given-names></name><name><surname>Turvey</surname><given-names>M</given-names></name><name><surname>Thielman</surname><given-names>B</given-names></name><name><surname>Marcos</surname><given-names>PT</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><article-title>Context-dependent sensory modulation underlies Bayesian vocal sequence perception</article-title><source>bioRxiv</source><year>2022</year><comment>Section: New Results</comment><elocation-id>2022.04.14.488412</elocation-id><pub-id pub-id-type="doi">10.1101/2022.04.14.488412</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Theilman</surname><given-names>B</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><article-title>Parallels in the sequential organization of birdsong and human speech</article-title><source>Nature Communications</source><year>2019</year><month>10</month><day>1</day><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41467-019-11605-y">http://www.nature.com/articles/s41467-019-11605-y</ext-link></comment><pub-id pub-id-type="pmcid">PMC6690877</pub-id><pub-id pub-id-type="pmid">31406118</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-11605-y</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sainburg</surname><given-names>T</given-names></name><name><surname>Thielk</surname><given-names>M</given-names></name><name><surname>Gentner</surname><given-names>TQ</given-names></name></person-group><article-title>Latent space visualization, characterization, and generation of diverse vocal communication signals</article-title><source>bioRxiv</source><year>2020</year><comment>Section: New Results</comment><elocation-id>870311</elocation-id><pub-id pub-id-type="doi">10.1101/870311</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharpee</surname><given-names>TO</given-names></name></person-group><article-title>Computational Identification of Receptive Fields</article-title><source>Annual Review of Neuroscience</source><year>2013</year><volume>36</volume><issue>1</issue><fpage>103</fpage><lpage>120</lpage><comment>arXiv: NIHMS150003 ISBN: 2122633255</comment><pub-id pub-id-type="pmcid">PMC3760488</pub-id><pub-id pub-id-type="pmid">23841838</pub-id><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170253</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talebi</surname><given-names>V</given-names></name><name><surname>Baker</surname><given-names>CL</given-names></name></person-group><article-title>Natural versus Synthetic Stimuli for Estimating Receptive Field Models: A Comparison of Predictive Robustness</article-title><source>Journal of Neuroscience</source><year>2012</year><volume>32</volume><issue>5</issue><pub-id pub-id-type="pmcid">PMC6703361</pub-id><pub-id pub-id-type="pmid">22302799</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4661-12.2012</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Sen</surname><given-names>K</given-names></name><name><surname>Doupe</surname><given-names>AJ</given-names></name></person-group><article-title>Spectral-Temporal Receptive Fields of Nonlinear Auditory Neurons Obtained Using Natural Sounds</article-title><source>Journal of Neuroscience</source><year>2000</year><month>Mar</month><volume>20</volume><issue>6</issue><fpage>2315</fpage><lpage>2331</lpage><publisher-name>publisher: Society for Neuroscience Section: ARTICLE</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.jneurosci.org/content/20/6/2315">https://www.jneurosci.org/content/20/6/2315</ext-link></comment><pub-id pub-id-type="pmcid">PMC6772498</pub-id><pub-id pub-id-type="pmid">10704507</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.20-06-02315.2000</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touryan</surname><given-names>J</given-names></name><name><surname>Felsen</surname><given-names>G</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name></person-group><article-title>Spatial structure of complex cell receptive fields measured with natural images</article-title><source>Neuron</source><year>2005</year><volume>45</volume><issue>5</issue><fpage>781</fpage><lpage>791</lpage><pub-id pub-id-type="pmid">15748852</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Segbroeck</surname><given-names>M</given-names></name><name><surname>Knoll</surname><given-names>AT</given-names></name><name><surname>Levitt</surname><given-names>P</given-names></name><name><surname>Narayanan</surname><given-names>S</given-names></name></person-group><article-title>MUPET—Mouse Ultrasonic Profile ExTraction: A Signal Processing Tool for Rapid and Unsupervised Analysis of Ultrasonic Vocalizations</article-title><source>Neuron</source><year>2017</year><month>May</month><volume>94</volume><issue>3</issue><fpage>465</fpage><lpage>485.e5</lpage><comment><ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0896627317302982">https://www.sciencedirect.com/science/article/pii/S0896627317302982</ext-link></comment><pub-id pub-id-type="pmcid">PMC5939957</pub-id><pub-id pub-id-type="pmid">28472651</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2017.04.005</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vinje</surname><given-names>WE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><article-title>Sparse Coding and Decorrelation in Primary Visual Cortex During Natural Vision</article-title><source>Science</source><year>2000</year><month>Feb</month><volume>287</volume><issue>5456</issue><fpage>1273</fpage><lpage>1276</lpage><publisher-name>publisher: American Association for the Advancement of Science</publisher-name><pub-id pub-id-type="pmid">10678835</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Cobos</surname><given-names>E</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Fahey</surname><given-names>PG</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Pitkow</surname><given-names>X</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nature Neuroscience</source><year>2019</year><month>Dec</month><volume>22</volume><issue>12</issue><fpage>2060</fpage><lpage>2065</lpage><publisher-name>Publisher: Nature Publishing Group</publisher-name><comment><ext-link ext-link-type="uri" xlink:href="https://www.nature.com/articles/s41593-019-0517-x">https://www.nature.com/articles/s41593-019-0517-x</ext-link></comment><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willmore</surname><given-names>B</given-names></name><name><surname>Tolhurst</surname><given-names>DJ</given-names></name></person-group><article-title>Characterizing the sparseness of neural codes</article-title><source>Network (Bristol, England)</source><year>2001</year><volume>12</volume><issue>3</issue><fpage>255</fpage><lpage>70</lpage><elocation-id>iSBN: 0954-898X (Print)</elocation-id><comment><ext-link ext-link-type="uri" xlink:href="https://www.ncbi.nlm.nih.gov/pubmed/11563529">http://www.ncbi.nlm.nih.gov/pubmed/11563529</ext-link></comment><pub-id pub-id-type="pmid">11563529</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woolley</surname><given-names>SMN</given-names></name><name><surname>Fremouw</surname><given-names>TE</given-names></name><name><surname>Hsu</surname><given-names>A</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><article-title>Tuning for spectro-temporal modulations as a mechanism for auditory discrimination of natural sounds</article-title><source>Nature Neuroscience</source><year>2005</year><volume>8</volume><issue>10</issue><fpage>1371</fpage><lpage>1379</lpage><pub-id pub-id-type="pmid">16136039</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Example spectrograms of natural USVs and their BiWaveGAN-generated reconstructions.</title><p>Spectrograms of natural USVs are shown in columns 1 and 3 with their corresponding reconstructions shown in columns 2 and 4. Reconstructions are produced by extracting the latent space representation of the corresponding natural USV with the encoder network, and then passing this vector into the decoder network to produce a synthetic USV.</p></caption><graphic xlink:href="EMS189811-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Example interpolations between pairs of USVs in latent space.</title><p>Interpolations are produced by randomly sampling start and endpoints from the latent space, and then linearly interpolating eight intermediate values. These latent vectors are decoded to produce a sequence of ten synthetic USVs (one for each row in the image). As can be seen in the figure, linear interpolation in the latent space produces smooth variation in the USVs, even though the start and endpoint USVs look very different. This suggests that dissimilar USVs do not belong to discrete clusters.</p></caption><graphic xlink:href="EMS189811-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Example responses of a single unit to natural and BiWaveGAN-generated USVs.</title><p>Spectrograms of natural and reconstructed USVs are shown in rows 1 and 2, respectively. Row 3 shows the spike responses of the unit during 22 repeated presentations of the natural USV stimulus shown in black, and the reconstructed stimulus shown in red. Corresponding peri-stimulus time histograms are shown in row 4. The first two pairs of syllables evoke similar responses within each pair, whereas the third syllable’s reconstruction evokes no response in this unit.</p></caption><graphic xlink:href="EMS189811-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>MNE model analysis characterising the response to BiWaveGAN-generated USVs.</title><p><bold>(A)</bold> Power spectra of natural and BiWaveGAN-reconstructed USVs. <bold>(B)</bold> Correlation coefficients between trials, indicating the upper bound on the correlation coefficients in response to the two stimulus types. Dashed line indicates a gradient of 1. <bold>(C)</bold> MNE model prediction correlation coefficients plotted as a function of the upper bound on the correlation coefficient achievable by the model. Dashed line indicates unity. <bold>(D)</bold> MNE model prediction correlation coefficients, where training and test data are swapped between natural and reconstructed USVs, plotted as a function of model performance using the within-class training and test data. <bold>(E)</bold> Example of one unit’s excitatory (top row) and inhibitory (bottom row) features learned by the MNE model using reconstructed stimuli. <bold>(F)</bold> UMAP projection onto two dimensions of the MNE model features learned with natural and reconstructed USVs.</p></caption><graphic xlink:href="EMS189811-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Sparseness of responses to natural and BiWaveGAN-generated USVs.</title><p><bold>(A)</bold> Lifetime sparseness calculated for each unit, for both natural USVs and BiWaveGAN-reconstructed stimuli. Data are presented as median and interquartile range (IQR). N=46 units; natural USV: 0.43 (0.32); reconstructed: 0.40 (0.30); Wilcoxon matched-pairs signed-rank test, P=3.71e-6. <bold>(B)</bold> A histogram of the difference in lifetime sparseness between the two sets of stimuli for each unit. <bold>(C)</bold> Population sparseness calculated for natural USVs and reconstructed stimuli. Data are presented as a box and whisker plot with median, first and third quartiles, and minimum and maximum values that are not outliers. Outliers are shown as circles (computed using the interquartile range). N=519 syllables; natural USVs: 0.63 (0.08); reconstructed: 0.62 (0.08); Wilcoxon matched-pairs signed-rank test, P=0.27. <bold>(D)</bold> A histogram shows the difference in population sparseness between the two stimuli. <bold>(E)</bold> Number of syllables evoking a high firing rate for each unit, defined as the response exceeding 50% of the maximum value of the PSTH to either stimulus type. N=46 units; natural USVs: 17 (20); reconstructed: 10 (16); Wilcoxon matched-pairs signed-rank test, P=1.50e-4. <bold>(F)</bold> A histogram shows the difference in the number of high firing rate syllables. <bold>(G)</bold> Number of syllables evoking any response for each unit, defined as the response exceeding 25% of the maximum value of the PSTH to either stimulus type. N=46 units; natural USVs: 79 (140); reconstructed: 81 (143); Wilcoxon matched-pairs signed-rank test, P=0.64. <bold>(H)</bold> A histogram shows the difference in the number of syllables evoking any response.</p></caption><graphic xlink:href="EMS189811-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>BiWaveGAN model architecture.</title><p>BiWaveGAN consists of three networks: the generator (<bold><italic>G</italic></bold>), the encoder (<bold><italic>E</italic></bold>), and the discriminator or critic (<bold><italic>D</italic></bold>) which consists of the sub-networks <bold><italic>D</italic></bold><sub><italic>x</italic></sub>, <bold><italic>D</italic></bold><sub><italic>z</italic></sub> and <bold><italic>D</italic></bold><sub>joint</sub>. Conv and TransConv layers represent 1D Convolutional and Transpose Convolutional layers, where <italic>k</italic> is the length of the kernel, and <italic>s</italic> is the stride of the convolution or transpose convolution. During training <bold><italic>D</italic></bold> learns to approximate the Wasserstein-1 distance between the distributions of real data-vector pairs, (<italic>x</italic>, <bold><italic>E</italic></bold>(<italic>x</italic>)), and synthetic pairs, (<bold><italic>G</italic></bold>(<italic>z</italic>), <italic>z</italic>) while <bold><italic>G</italic></bold> and <bold><italic>E</italic></bold> learn to produce data-vector pairs which minimise this distance. After training, <bold><italic>G</italic></bold> can be fed latent vectors to produce synthetic waveforms, and <bold><italic>E</italic></bold> can be fed waveforms to infer their latent representations.</p></caption><graphic xlink:href="EMS189811-f006"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><p>Architecture tables for BiWaveGAN generator (left) and encoder (right). All Conv1D and TransConv1D layers have a kernel of length 25. All LeakyReLU activations have a negative slope of 0.2. <bold><italic>m</italic></bold> is the batch size, <bold><italic>l</italic></bold> is the dimension of latent space and <bold><italic>d</italic></bold> is a parameter controlling model size.</p></caption><table frame="void" rules="cols"><thead><tr style="border-bottom: solid thin;"><th align="left" valign="top"><bold><italic>G</italic></bold></th><th align="left" valign="top">Operation</th><th align="left" valign="top">Output Shape</th><th align="left" valign="top">Activation</th><th align="left" valign="top"><bold><italic>E</italic></bold></th><th align="left" valign="top">Operation</th><th align="left" valign="top">Output Shape</th><th align="left" valign="top">Activation</th></tr></thead><tbody><tr><td align="left" valign="top">0</td><td align="left" valign="top">Input <italic>z</italic> batch</td><td align="left" valign="top">(<italic>m</italic>, <italic>l</italic>)</td><td align="left" valign="top"/><td align="left" valign="top">0</td><td align="left" valign="top">Input <italic>x</italic> batch</td><td align="left" valign="top">(<italic>m</italic>, 1, 32768)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">1</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, 1, <italic>l</italic>)</td><td align="left" valign="top"/><td align="left" valign="top">1</td><td align="left" valign="top">Conv1D(stride=2)</td><td align="left" valign="top">(<italic>m</italic>, 2<italic>d</italic>, 16384)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">2</td><td align="left" valign="top">Fully Connected</td><td align="left" valign="top">(<italic>m</italic>, 1, 512<italic>d</italic>)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">2</td><td align="left" valign="top">Conv1D(stride=4)</td><td align="left" valign="top">(<italic>m</italic>, 4<italic>d</italic>, 4096)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">3</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 16)</td><td align="left" valign="top"/><td align="left" valign="top">3</td><td align="left" valign="top">Conv1D(stride=4)</td><td align="left" valign="top">(<italic>m</italic>, 8<italic>d</italic>, 1024)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">4</td><td align="left" valign="top">TransConv1D(stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 64)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">4</td><td align="left" valign="top">Conv1D(stride=4)</td><td align="left" valign="top">(<italic>m</italic>, 16<italic>d</italic>, 256)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">5</td><td align="left" valign="top">TransConv1D(stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 16<italic>d</italic>, 256)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">5</td><td align="left" valign="top">Conv1D(stride=4)</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 64)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">6</td><td align="left" valign="top">TransConv1D(stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 8<italic>d</italic>, 1024)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">6</td><td align="left" valign="top">Conv1D(stride=4)</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 16)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">7</td><td align="left" valign="top">TransConv1D(stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 4<italic>d</italic>, 4096)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">7</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, 1, 512<italic>d</italic>)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">8</td><td align="left" valign="top">TransConv1D(stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 2<italic>d</italic>, 16384)</td><td align="left" valign="top">ReLU</td><td align="left" valign="top">8</td><td align="left" valign="top">Fully Connected</td><td align="left" valign="top">(<italic>m</italic>, 1,<italic>l</italic>)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">9</td><td align="left" valign="top">TransConv1D(stride = 2)</td><td align="left" valign="top">(<italic>m</italic>, 1, 32768)</td><td align="left" valign="top">Tanh</td><td align="left" valign="top">9</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, <italic>l</italic>)</td><td align="left" valign="top"/></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><p>Architectures of <bold><italic>D</italic></bold><sub><italic>x</italic></sub>, <bold><italic>D</italic></bold><sub><italic>z</italic></sub> and <bold><italic>D</italic></bold><sub><italic>joint</italic></sub> in a BiWaveGAN discriminator, where <bold><italic>D</italic></bold><sub><italic>z</italic></sub> has a depth of 3 and <bold><italic>D</italic></bold><sub>joint</sub> has a depth of 3. Stride = 1 for all convolutions unless otherwise stated. <bold><italic>m</italic></bold> is the batch size, <bold><italic>l</italic></bold> is the dimension of the latent space, <italic>d</italic> is a parameter controlling model size, <bold><italic>n</italic></bold> controls the amount of PhaseShuffle that is applied, and <italic>f</italic> controls the number of filters used in the convolutions of <bold><italic>D</italic></bold><sub><italic>z</italic></sub> and <bold><italic>D</italic></bold><sub>joint</sub>. In our model, the values <italic>d</italic> = 32, <italic>n</italic> = 2 and <italic>f</italic> = 512 were used.</p></caption><table frame="void" rules="cols"><thead><tr style="border-bottom: solid thin;"><th align="left" valign="top"><italic>D<sub>x</sub></italic></th><th align="left" valign="top">Operation</th><th align="left" valign="top">Output Shape</th><th align="left" valign="top">Activation</th></tr></thead><tbody><tr><td align="left" valign="top">0</td><td align="left" valign="top">Input <italic>x</italic></td><td align="left" valign="top">(<italic>m</italic>, 1, 32768)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">1</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 2)</td><td align="left" valign="top">(<italic>m</italic>, <italic>d</italic>, 8192)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">2</td><td align="left" valign="top">PhaseShuffle(<italic>n</italic>)</td><td align="left" valign="top">(<italic>m</italic>, <italic>d</italic>, 8192)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">3</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 2<italic>d</italic>, 2048)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">4</td><td align="left" valign="top">PhaseShuffle(<italic>n</italic>)</td><td align="left" valign="top">(<italic>m</italic>, 2<italic>d</italic>, 2048)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">5</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 4<italic>d</italic>, 512)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">6</td><td align="left" valign="top">PhaseShuffle(<italic>n</italic>)</td><td align="left" valign="top">(<italic>m</italic>, 4<italic>d</italic>, 512)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">7</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 8<italic>d</italic>, 128)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">8</td><td align="left" valign="top">PhaseShuffle(<italic>n</italic>)</td><td align="left" valign="top">(<italic>m</italic>, 8<italic>d</italic>, 128)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">9</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 16<italic>d</italic>, 32)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">10</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 15)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">11</td><td align="left" valign="top">Conv1D(kernel length = 25, stride = 4)</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr></tbody></table><table frame="void" rules="cols"><thead><tr style="border-bottom: solid thin;"><th align="left" valign="top"><italic>D<sub>z</sub></italic></th><th align="left" valign="top">Operation</th><th align="left" valign="top">Output Shape</th><th align="left" valign="top">Activation</th></tr></thead><tbody><tr><td align="left" valign="top">0</td><td align="left" valign="top">Input z batch</td><td align="left" valign="top">(<italic>m</italic>, <italic>l</italic>)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">1</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, <italic>l</italic>, 1)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">2</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, <italic>f</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">3</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, <italic>f</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">4</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, <italic>f</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr></tbody></table><table frame="void" rules="cols"><thead><tr style="border-bottom: solid thin;"><th align="left" valign="top"><italic>D</italic><sub>joint</sub></th><th align="left" valign="top">Operation</th><th align="left" valign="top">Output Size</th><th align="left" valign="top">Activation</th></tr></thead><tbody><tr><td align="left" valign="top">0</td><td align="left" valign="top">Concat(<italic>D<sub>x</sub></italic>(<italic>x</italic>), <italic>D<sub>z</sub></italic>(<italic>Z</italic>))</td><td align="left" valign="top">(<italic>m</italic>, 32<italic>d</italic> + <italic>f</italic>, 1)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">1</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, <italic>f</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">2</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, <italic>f</italic>, 1)</td><td align="left" valign="top">LeakyReLU</td></tr><tr><td align="left" valign="top">3</td><td align="left" valign="top">Conv1D(kernel length = 1)</td><td align="left" valign="top">(<italic>m</italic>, 1, 1)</td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">4</td><td align="left" valign="top">Reshape</td><td align="left" valign="top">(<italic>m</italic>, 1)</td><td align="left" valign="top"/></tr></tbody></table></table-wrap></floats-group></article>