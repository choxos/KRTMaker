<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS190621</article-id><article-id pub-id-type="doi">10.1101/2023.11.03.565500</article-id><article-id pub-id-type="archive">PPR753331</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>The feature landscape of visual cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Tong</surname><given-names>Rudi</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>da Silva</surname><given-names>Ronan</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Lin</surname><given-names>Dongyan</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Ghosh</surname><given-names>Arna</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Wilsenach</surname><given-names>James</given-names></name><xref ref-type="aff" rid="A4">4</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Cianfarano</surname><given-names>Erica</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Bashivan</surname><given-names>Pouya</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A6">6</xref></contrib><contrib contrib-type="author"><name><surname>Richards</surname><given-names>Blake</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Trenholm</surname><given-names>Stuart</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib></contrib-group><aff id="A1"><label>1</label>Montreal Neurological Institute, McGill University, Montreal, Canada</aff><aff id="A2"><label>2</label>Mila, Montreal, Canada</aff><aff id="A3"><label>3</label>School of Computer Science, McGill University, Montreal, Canada</aff><aff id="A4"><label>4</label>The Wellcome Trust Centre for Neuroimaging, University College London, London, UK</aff><aff id="A5"><label>5</label>The Alan Turing Institute, British Library, London, UK</aff><aff id="A6"><label>6</label>Department of Physiology, McGill University, Montreal, Canada</aff><aff id="A7"><label>7</label>CIFAR Learning in Machines and Brains, Toronto, Canada</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: <email>stuart.trenholm@mcgill.ca</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>06</day><month>11</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>05</day><month>11</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Understanding computations in the visual system requires a characterization of the distinct feature preferences of neurons in different visual cortical areas. However, we know little about how feature preferences of neurons within a given area relate to that area’s role within the global organization of visual cortex. To address this, we recorded from thousands of neurons across six visual cortical areas in mouse and leveraged generative AI methods combined with closed-loop neuronal recordings to identify each neuron’s visual feature preference. First, we discovered that the mouse’s visual system is globally organized to encode features in a manner invariant to the types of image transformations induced by self-motion. Second, we found differences in the visual feature preferences of each area and that these differences generalized across animals. Finally, we observed that a given area’s collection of preferred stimuli (‘own-stimuli’) drive neurons from the same area more effectively through their dynamic range compared to preferred stimuli from other areas (‘other-stimuli’). As a result, feature preferences of neurons within an area are organized to maximally encode differences among own-stimuli while remaining insensitive to differences among other-stimuli. These results reveal how visual areas work together to efficiently encode information about the external world.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Our visual system makes sense of the world via its myriad neurons, each tasked with extracting specific visual features from the environment. Decades of work in primates have shown that after visual inputs arrive in primary visual cortex (V1), they proceed to numerous higher visual areas (HVAs)<sup><xref ref-type="bibr" rid="R1">1</xref></sup>. At an inter-area level, each HVA is thought to prefer a specific portion of the visual world (e.g. there is a preference for orientated edges in V1<sup><xref ref-type="bibr" rid="R2">2</xref></sup>; there is a preference for faces in fusiform face area<sup><xref ref-type="bibr" rid="R3">3</xref></sup>). At an intra-area level, individual neurons exhibit diverse tuning preferences that enable encoding of the portion of the visual world that a given HVA is concerned with (e.g. V1 possesses neurons with preferences for edges of different orientations<sup><xref ref-type="bibr" rid="R4">4</xref></sup>; face-selective areas possess neurons preferring different aspects of faces<sup><xref ref-type="bibr" rid="R5">5</xref></sup>). However, due in part to the large size of visual areas in primates and the resulting difficulty in broadly recording from neurons within and across different HVAs, a clear understanding of how across-area differences in feature preferences relate to within-area organization of feature preferences is lacking. This limits our ability to gain a holistic view of the visual system and to understand how its computational goals guide its functional organization.</p><p id="P3">The mouse visual system provides a tractable model for addressing these issues. Anatomical tracing<sup><xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup> and functional mapping<sup><xref ref-type="bibr" rid="R10">10</xref>–<xref ref-type="bibr" rid="R13">13</xref></sup> of mouse cortex have revealed ~10 distinct HVAs, most of which receive significant direct input from V1. Circuit tracing and light-evoked spike time analyses have found some evidence that mouse HVAs are organized in a hierarchal manner<sup><xref ref-type="bibr" rid="R6">6</xref>,<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R14">14</xref></sup>, although these studies have also found that mouse HVAs appear to be more strongly interconnected than those in primates<sup><xref ref-type="bibr" rid="R6">6</xref>–<xref ref-type="bibr" rid="R9">9</xref></sup>. In one study<sup><xref ref-type="bibr" rid="R14">14</xref></sup>, a single hierarchy was described: V1<bold>→</bold>LM<bold>→</bold>RL<bold>→</bold>LP<bold>→</bold>AL<bold>→</bold>PM<bold>→</bold>AM (for full nomenclature of mouse HVAs, see Methods). In contrast, other studies attempting to parallel primate visual streams have sorted the mouse hierarchy into putative ventral (V1<bold>→</bold>LM<bold>→</bold>P<bold>→</bold>LI<bold>→</bold>POR) and dorsal (V1<bold>→</bold>RL<bold>→</bold>AL<bold>→</bold>A<bold>→</bold>PM<bold>→</bold>AM) processing streams<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup>. Additional studies have examined differences in visual feature preferences between mouse HVAs, but to date it has been difficult to draw firm conclusions. For instance, by presenting full field drifting grating stimuli, several studies have found differences in spatial and temporal frequency tuning properties between HVAs<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R15">15</xref>–<xref ref-type="bibr" rid="R17">17</xref></sup>, though some of the differences noted appear to arise from the specific inclusion criteria used<sup><xref ref-type="bibr" rid="R18">18</xref></sup> or whether experiments were performed under awake or anesthetized conditions<sup><xref ref-type="bibr" rid="R19">19</xref></sup>. Another study<sup><xref ref-type="bibr" rid="R20">20</xref></sup>, which used 2-photon calcium imaging to record from different genetically-defined cell types, across different cortical layers and HVAs, is notable for revealing broad tuning curves and largely overlapping preferences within and across genetically-defined cell types, across cortical layers, and across HVAs. Thus, what remains missing is a) an inter-area level understanding of which portions of the visual world each mouse HVA is concerned with; b) an intra-area level understanding of how specific portions of the visual world are specifically encoded by neurons within each HVA; c) a cross-scale understanding of how inter-area feature preferences arise from and relate to intra-area feature preferences.</p><p id="P4">To address this, we used <italic>in vivo</italic> 2-photon calcium imaging to record from thousands of neurons across mouse V1 and five HVAs. We leveraged advances in modelling neuronal responses using artificial neural networks (ANNs)<sup><xref ref-type="bibr" rid="R21">21</xref>–<xref ref-type="bibr" rid="R23">23</xref></sup> to build predictive models of the neurons, which we used to generate preferred stimuli for individual neurons. This enabled us to outline how mouse HVAs are functionally organized, ask why such an organization arises, and elucidate what the roles for such an organization are in encoding visual stimuli.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Modelling multiple mouse visual cortical areas with artificial neural networks</title><p id="P5">To gain an understanding of the visual features encoded by neurons in various visual cortical areas in the mouse, we modelled neuron responses to visual stimuli in different areas with artificial neural networks (ANNs). We used this approach to determine the preferred stimulus (i.e. a visual stimulus that will strongly activate the neuron) for each neuron recorded <italic>in vivo</italic>, and to run <italic>in silico</italic> experiments that would be otherwise infeasible experimentally. To generate these models, we recorded from neurons in six visual cortical areas in mouse, including primary visual cortex (V1) and five HVAs spanning different anatomical hierarchical levels of both putative dorsal and ventral visual streams (LM, LI, POR, AL, and RL; <xref ref-type="fig" rid="F1">Figure 1a</xref>). Each area was identified using widefield calcium imaging, with area segmentation based on the phase of retinotopic maps<sup><xref ref-type="bibr" rid="R12">12</xref>,<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup>, using transgenic mice in which the calcium indicator jRGECO1a was expressed throughout cortex<sup><xref ref-type="bibr" rid="R25">25</xref></sup> (<xref ref-type="fig" rid="F1">Figure 1a</xref>). After identifying a given cortical area, we used 2-photon calcium imaging to record light-evoked responses from neurons in layer 2/3 during the presentation of 2,500 different static natural images.</p><p id="P6">Next, we used ANNs to model the responses from neurons in each cortical area (<xref ref-type="fig" rid="F1">Figure 1b</xref>). We used a shallow convolutional neural network with a factorized readout layer<sup><xref ref-type="bibr" rid="R26">26</xref></sup> that separates the visual features (i.e. ‘what’) and the spatial locations in the images that drive neurons (i.e. ‘where’), referred to as the ‘spatial mask’. For each cortical area, we trained a unique ANN to predict the responses of individual neurons to the natural images. For model training, we selected neurons that reliably responded to natural image presentation (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>; see Methods). Additionally, only model units (i.e. digital twins of our real neurons) with &gt; 30% explainable variance explained were used for subsequent analyses. This resulted in &gt; 7,250 model units included for subsequent analyses (V1 = 1,418 units; LM = 1,469 units; LI = 1,112 units; POR = 1,070 units; AL = 1,298 units; RL = 899 units). Importantly, each ANN model had a significantly higher fraction of explainable variance explained compared to a simple linear model (<xref ref-type="fig" rid="F1">Figure 1c</xref>; see Methods).</p><p id="P7">Finally, to validate our modelling strategy, for a subset of recordings (n = 2-3 animals per cortical region), we re-recorded from the same neurons on a second day while presenting the animal with a subset (n = 300-500) of the natural images shown the first day, as well as their preferred stimuli (<xref ref-type="fig" rid="F1">Figure 1d</xref>). Each preferred stimulus, generated to maximally activate the twinned model unit, also strongly and selectively activated its corresponding <italic>in vivo</italic> neuron, with much smaller ‘off-diagonal’ activation compared to ‘on-diagonal’ activation (<xref ref-type="fig" rid="F1">Figure 1e-g</xref>). The median response generated for all preferred stimuli was greater than 90% of the responses evoked by natural images (<xref ref-type="fig" rid="F1">Figure 1h</xref>). Our results indicate that these ANNs can serve as tools for predicting neuronal activity and exploring visual feature preferences in areas V1, LM, LI, POR, AL, and RL.</p></sec><sec id="S4"><title>The functional organization of mouse visual cortex</title><p id="P8">We first examined the overall organization of visual cortex. To do so, we examined the functional similarity between HVAs by using our ANN models to test how features of visual stimuli were differentially represented across brain areas. We took advantage of the fact that we could align the spatial masks (the ‘Where-layer’) of the factorized readout layer for each model unit (<xref ref-type="fig" rid="F2">Figure 2a</xref>), which allowed us to focus exclusively on differences arising from the ‘What-layer’. We presented 10,000 natural images taken from ImageNet<sup><xref ref-type="bibr" rid="R27">27</xref></sup> to the spatial mask centered units. For each image, we measured the evoked population response (i.e. how strongly each model unit was activated by each image) and repeated this process for ANN models of V1 and the five HVAs. We then compared the similarity of population responses to the same set of natural images across visual areas by calculating the distance correlation (dCor), a non-linear method for measuring statistical dependence between two multivariate random variables (<xref ref-type="fig" rid="F2">Figure 2a</xref>). We found that the distance correlations between pairs of HVAs showed clear differences in their strength (<xref ref-type="fig" rid="F2">Figure 2b</xref>). However, we also observed that distance correlation values were relatively high between all areas, suggesting the presence of some widely shared correlations. To specifically focus on unique correlations shared between each pair of regions, we calculated the partial distance correlation (pdCor) between each pair of regions after conditioning out correlations with all other regions (<xref ref-type="fig" rid="F2">Figure 2a</xref>). Focusing on pdCor, we found that activity in V1 was most similar to LM and LI. Activity in LM was most similar to RL and AL. LI and POR were most similar to each other, RL and AL were most similar to each other, and these latter two groups (LI+POR vs. RL+AL) were particularly dissimilar from one another (<xref ref-type="fig" rid="F2">Figure 2c</xref>). We used multidimensional scaling (MDS) to visualize the distance correlations and partial distance correlations between the different areas as undirected graphs. This visualization emphasized the existence of two distinct functional streams emerging from V1 and passing through LM: one being V1<bold>→</bold>LM<bold>→</bold>LI<bold>→</bold>POR, the other being V1<bold>→</bold>LM<bold>→</bold>RL<bold>→</bold>AL (<xref ref-type="fig" rid="F2">Figure 2b,c</xref>; note that since we know <italic>a priori</italic> that V1 is the predominant input region receiving signals from the visual thalamus, we orientated the MDS graph to place V1 at the bottom). Remarkably, a proposed dorsal/ventral two-stream hierarchy based on anatomical connectivity<sup><xref ref-type="bibr" rid="R9">9</xref></sup> is in strong agreement with our results (<xref ref-type="fig" rid="F2">Figure 2d</xref>), revealing a tight relationship between function and anatomy.</p><p id="P9">Finally, previous work has shown that receptive field size increases as one moves up the visual hierarchy<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. We examined whether the spatial mask size in the factorized readout layer, which is loosely analogous to a receptive field, showed differences across visual cortical areas. We normalized mask size to V1 and found that spatial mask size increased along the visual hierarchy (<xref ref-type="fig" rid="F2">Figure 2e</xref>). Overall, these analyses provide strong evidence that, on a functional level, mouse HVAs are representationally organized along two distinct hierarchical processing streams. Moreover, representations of natural images increasingly differ between HVAs as a function of hierarchical distance.</p></sec><sec id="S5"><title>Visual feature similarity with invariance to self-motion related image transformations guides the organization of feature preferences across HVAs</title><p id="P10">We next asked why HVAs are organized in this manner. Unlike in primates – where it is well-established that IT cortex is dedicated to processing visual objects<sup><xref ref-type="bibr" rid="R28">28</xref></sup>, and thus specific hypotheses can be formulated around how preferences for specific visual categories are organized in different HVAs<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup> – we did not have a clear <italic>a priori</italic> assumption for the type of model that might guide the organization of visual feature preferences in mouse cortex. We therefore decided to compare three simple image-based organizing models: a) a template-matching based model; b) a spatial frequency based model; c) a model based on visual feature similarity with invariance to image transformations that could arise as an animal moves around the world (we focused on invariance to affine transformations, including translation, scaling and in-plane rotation, as these are simple to apply to 2-D images).</p><p id="P11">To compare between these three models, we pooled the preferred stimuli from all areas. Next, for each model we constructed a 256-dimensional embedding space. For the template-matching based model, we performed a principal component analysis (PCA) on the preferred stimuli and defined the resulting principal component space as the embedding. For the spatial frequency based model, we performed a fast Fourier transform (FFT) on each preferred stimulus, followed by PCA. For the third model, we embedded the preferred stimuli using SimCLR, a contrastive deep learning tool specifically designed to group together images that are similar to one another<sup><xref ref-type="bibr" rid="R32">32</xref></sup>, and which has previously been used to measure the similarity of AI-generated preferred stimuli from primate V4<sup><xref ref-type="bibr" rid="R33">33</xref></sup>. SimCLR maximizes invariances towards arbitrary transformations of input images, which we achieved by applying a set of affine transformations to the preferred stimuli – translation (± 10% X/Y shift), rotation (± 90°), and resizing – then projecting these images into SimCLR’s latent space, and minimizing the distance between different transformations of the same preferred stimuli (<xref ref-type="fig" rid="F3">Figure 3a</xref>). Notably, the transformations we applied are similar to those that would be expected from self-motion.</p><p id="P12">First, we tested which of these models was best able to group together preferred stimuli according to the area that they were generated from. For each embedding model of preferred stimuli, we performed a k-nearest neighbours analysis (kNN; k = 20) and examined its classification accuracy (<xref ref-type="fig" rid="F3">Figure 3b</xref>). We found that the SimCLR method significantly outperformed both template-matching (pixel-level PCA) and spatial frequency (FFT) based models (<xref ref-type="fig" rid="F3">Figure 3b</xref>), meaning that SimCLR was better at grouping together preferred stimuli from the same HVA. We thus further explored details of the SimCLR embedding of preferred stimuli.</p><p id="P13">We visualized SimCLR’s 256-dimensional space by projecting it down to two dimensions using UMAP<sup><xref ref-type="bibr" rid="R34">34</xref></sup>. In this visualization (<xref ref-type="fig" rid="F3">Figure 3c</xref>), each point represents a preferred stimulus, and the distance between any two points indicates the relative similarity between those two preferred stimuli in SimCLR’s latent space: specifically, distance in this space relates to image similarity with invariance to affine transformations. While the UMAP plot did not reveal distinct clusters – as predicted by the strong pairwise functional correlation between areas (<xref ref-type="fig" rid="F2">Figure 2b</xref>) – colour-coding the embedding according to the visual area that each preferred stimulus came from revealed strong biases in the extent to which preferred stimulus space was represented by each visual area (<xref ref-type="fig" rid="F3">Figure 3c</xref>).</p><p id="P14">How do we know if SimCLR’s embedding of preferred stimuli is related to how the brain organizes visual feature preferences? To ask this question, we examined whether SimCLR’s embedding of preferred stimuli could recapitulate the functional organization of HVAs (<xref ref-type="fig" rid="F2">Figure 2b,c</xref>). We reasoned that the functional relationship between HVAs could be reflected in the overlap of their respective manifolds within the SimCLR embedding space. To calculate this, for each preferred stimulus from each HVA, we measured the likelihood of finding preferred stimuli from other HVAs within its 20 nearest neighbours (<xref ref-type="fig" rid="F3">Figure 3d</xref>). As visualized with an MDS plot (<xref ref-type="fig" rid="F3">Figure 3d</xref>), SimCLR’s embedding of preferred stimuli resulted in an organization of HVAs that was remarkably similar to that generated from neuron population responses (<xref ref-type="fig" rid="F2">Figure 2b,c</xref>). To quantify this, we measured the topological similarity between SimCLR’s embedding (<xref ref-type="fig" rid="F3">Figure 3d</xref>) and the results from the partial distance correlation analysis (<xref ref-type="fig" rid="F2">Figure 2c</xref>) via the Spearman correlation coefficient of the respective similarity matrices. This indicated a significantly higher topological similarity between the SimCLR embedding and the functional organization of HVAs compared to when the same analysis was run on the template-matching or spatial frequency based models (<xref ref-type="fig" rid="F3">Figure 3e</xref>). These results suggest that, at a cortex-wide scale, HVAs encode visual features agnostic to their orientation, scale, and position, in line with a general invariance to types of image transformations that can be induced by self-motion.</p></sec><sec id="S6"><title>Each HVA prefers distinct visual features</title><p id="P15">What do the preferred stimuli for different HVAs look like, and how do they differ between areas? To address these questions, we visualized the two-dimensional UMAP projection of the SimCLR embedding. We split the embedding into a 40 x 40 grid, and for each tile in the grid we randomly visualized an image contained within that tile (<xref ref-type="fig" rid="F4">Figure 4</xref>). We refer to this visualization as the ‘Feature landscape of visual cortex’. It can be seen from this visualization that SimCLR effectively sorted preferred stimuli based on image similarity, as neighbouring preferred stimuli appear alike. To examine visual feature preference differences across HVAs, we selected the 200 most representative preferred stimuli for each visual area, on which we performed additional analyses. This was done within the SimCLR embedding by using a kNN analysis (k = 100) to select the images with the most preferred stimuli from the same visual area within their neighbourhood.</p><p id="P16">We observed several differences between the representative stimuli from different HVAs (<xref ref-type="fig" rid="F5">Figure 5a</xref>). First, we noted differences in mean luminance, with some areas preferring darker or brighter stimuli. We found that preferred stimuli from LI and POR were particularly dark, whereas RL preferred stimuli were relatively brighter (<xref ref-type="fig" rid="F5">Figure 5b</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). Second, we noted differences in how many distinct segments made up preferred stimuli from each area. Calculating individual segments (measured via thresholding preferred stimuli into black and white segments), we found that preferred stimuli from most areas were best described as having white segments on top of a black background (i.e. significantly more white than black segments (except for AL)), and having smaller white segments than black segments (<xref ref-type="fig" rid="F5">Figure 5c,d</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>; with LI having significantly more white segments than the other areas). Third, we noted that representative stimuli from some areas were dominated by lower spatial frequencies, whereas others possessed higher spatial frequency content. Performing an FFT and averaging radially over all orientations, we found that POR was dominated by the lowest spatial frequencies (though with increased power again at very high spatial frequencies), LM, AL, and RL were dominated by medium-to-low spatial frequencies, V1 was dominated by medium-to-high spatial frequencies, and LI was dominated by high spatial frequencies (<xref ref-type="fig" rid="F5">Figure 5e</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). Lastly, we noted that whereas areas V1, AL, and RL had representative stimuli that tended to feature long edges orientated along a single axis, areas LI and POR contained dotted segments that appeared to be arranged in grid-like patterns. Calculating folio symmetry (i.e. folding the image once) vs. quarto symmetry (i.e. folding the image twice) revealed that areas V1, LM, AL, and RL exhibited higher folio symmetry (<xref ref-type="fig" rid="F5">Figure 5f</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>), whereas areas LI and POR exhibited significantly higher quarto symmetry (<xref ref-type="fig" rid="F5">Figure 5g</xref>; <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>). Importantly, these differences in image statistics for preferred stimuli from different HVAs persisted when we performed the same analyses on all preferred stimuli from each HVA, not just on the 200 most representative stimuli (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 4</xref>). These results show that each HVA exhibits distinct preferences in visual features.</p></sec><sec id="S7"><title>The set of preferred stimuli in a visual area represents a spanning set that effectively drive neurons through their dynamic range</title><p id="P17">The results above indicate that the preferred stimuli for each visual area possess distinct image statistics. As such, we wondered whether at a population level the set of preferred stimuli generated from a given area (which we refer to as ‘own-stimuli’) would drive stronger activity in the neurons from the same area compared to the set of preferred stimuli generated from other visual areas (‘other-stimuli’). To test this, we performed <italic>in vivo</italic> widefield calcium imaging with a new cohort of mice that were not included in the models from which we generated the preferred stimuli (<xref ref-type="fig" rid="F6">Figure 6a</xref>). We presented a mix of preferred stimuli from all areas and found that, for all areas other than LM, own-stimuli drove significantly stronger activity in the area that they were generated from compared to other areas (<xref ref-type="fig" rid="F6">Figure 6b</xref>; for <italic>in silico</italic> results, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5</xref>). As such, the biases we found in feature preferences for each area are consistent and generalize across animals.</p><p id="P18">Do own-stimuli drive stronger average activity simply because they contain more of the overall visual statistics preferred by neurons in a given area? To test this, we leveraged our <italic>in silico</italic> models, where we could center both own- and other-stimuli on individual models units by aligning their spatial masks, and we examined the range of responses evoked by sets of own- vs. other-stimuli. We found that instead of increasing the average response amplitude to all stimuli, own-stimuli drove neurons through a wider dynamic range (<xref ref-type="fig" rid="F6">Figure 6c</xref>). To quantify this, we compared response amplitudes at the 10<sup>th</sup> and 90<sup>th</sup> percentiles and found that, compared to other-stimuli, own-stimuli extended the dynamic range over which neurons responded, on both upper and lower bounds (<xref ref-type="fig" rid="F6">Figure 6d</xref>). However, the effect size at the 90<sup>th</sup> percentile was significantly stronger than at the 10<sup>th</sup> percentile (<xref ref-type="fig" rid="F6">Figure 6d</xref>), which explains why own-stimuli drive stronger average activity in the area they were generated from compared to other-stimuli (<xref ref-type="fig" rid="F6">Figure 6b</xref>). This also indicates that each own-stimulus, though generated to maximize the response of a specific neuron, actually also drives weak activity in many other neurons from the same area. Similarly, responses to own stimuli are often weaker than responses to other-stimuli. Consistent with this finding, we found that population variance (the extent to which a given stimulus drives diverse responses in a population of neurons, defined as the inter-quartile range of the distribution of responses to a given stimulus) was significantly higher for own- vs. other-stimuli (<xref ref-type="fig" rid="F6">Figure 6e</xref>). Thus, the different features preferred by neurons within a given area can be viewed as a ‘spanning set’ that maximally drives neurons from that area through their dynamic range.</p></sec></sec><sec id="S8" sec-type="discussion"><title>Discussion</title><p id="P19">Here we described the feature landscape of visual cortex. By combining widefield calcium imaging, 2-photon calcium imaging, and modelling with ANNs, we generated digital twins of thousands of neurons in six different cortical areas in mouse, including primary visual cortex (V1) and five higher visual areas (LM, LI, POR, AL, and RL). Our results outline the functional organization of mouse visual cortex and provide a detailed understanding of how visual feature preferences are arranged across HVAs, why they are arranged in this manner, and what the outcomes of this organization are on encoding of the visual world.</p><p id="P20">How is the mouse visual system organized? Using a data driven approach, based on similarities of population activity to natural image presentations, assessed with distance correlation, we identified two hierarchically organized processing streams: V1<bold>→</bold>LM<bold>→</bold>LI<bold>→</bold>POR and V1<bold>→</bold>LM<bold>→</bold>RL<bold>→</bold>AL. These two processing streams are corroborated by previous studies that split mouse HVAs into dorsal and ventral streams based on anatomical tracing<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R9">9</xref></sup> and certain functional response properties<sup><xref ref-type="bibr" rid="R35">35</xref></sup>, and are consistent with studies in mouse which found that receptive fields become larger at successive stages in the visual hierarchy<sup><xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R14">14</xref>,<xref ref-type="bibr" rid="R20">20</xref></sup>. Our finding that most areas’ preferred stimuli are biased towards lower luminance values build upon work from other mammalian species which have indicated that more neuronal resources are dedicated to processing OFF vs ON visual stimuli<sup><xref ref-type="bibr" rid="R36">36</xref></sup>. Furthermore, our findings of differences in spatial frequency preferences for preferred stimuli from different HVAs are consistent with some previous studies, which for instance found that RL and AL tended to prefer lower spatial frequencies, and V1 and LI tended to prefer relatively higher spatial frequencies<sup><xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R11">11</xref>,<xref ref-type="bibr" rid="R16">16</xref>,<xref ref-type="bibr" rid="R35">35</xref></sup>. Nonetheless, similar to previous studies of mouse V1<sup><xref ref-type="bibr" rid="R23">23</xref>,<xref ref-type="bibr" rid="R37">37</xref></sup>, using an ANN-based methodology to generate preferred stimuli allowed us to reveal how the various feature preferences (e.g. luminance, spatial frequency, receptive field size, etc.) interact to generate each neuron’s specific preferred stimulus, in various HVAs. For example, this revealed that many neurons in LI and POR appear to have preferences for dotted, grid-like patterns. However, to what purpose the mouse visual system developed these specific visual feature preferences across its HVAs remains an open question. To facilitate further exploration of the feature landscape of mouse visual cortex, we generated a graphical user interface that can be accessed here.</p><p id="P21">Why is the mouse visual system organized in this manner? We took inspiration from primate models, which have tried to explain why the primate ventral visual stream is organized into functional patches preferring specific aspects of the visual world (e.g. faces, bodies, colours, etc.)<sup><xref ref-type="bibr" rid="R29">29</xref>–<xref ref-type="bibr" rid="R31">31</xref></sup>. Unlike in primates, where the hierarchy of HVAs is well matched by ANNs trained on object categorization, this does not appear to be the case for mouse cortex<sup><xref ref-type="bibr" rid="R38">38</xref></sup>, so we did not focus on testing models based on the computation of object categorization. However, much like primates, mice wander around the world and thus need to visually recognize things in a manner invariant to the types of transformations their visual system experiences<sup><xref ref-type="bibr" rid="R29">29</xref></sup>. We leveraged an ANN model that allowed us to sort preferred stimuli based on image similarity<sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup> that was trained to be invariant to specific image transformations. We focused on the types of transformations that could be induced by self-motion and that are applicable to 2-D images: translation, scale, and in-plane rotation. We compared this to the most naïve model possible, a template-matching model that compared pixel-level similarity between preferred stimuli, and another model based solely on spatial frequency content, which was the focus of many early studies of mouse HVAs<sup><xref ref-type="bibr" rid="R19">19</xref></sup>. Our results indicate that, of the models tested, the model based on image similarity with invariance to self-motion related transformations best explained how visual feature preferences are organized in the mouse’s visual cortex. This suggests that at a cortex-wide level, while HVAs have arisen to extract distinct visual features from the world, they have done so in a way to minimize the effect of the types of transformations that could arise from self-motion. Nonetheless, it should be noted that whereas our SimCLR model only contained affine transformations, future work that studies exactly how a mouse’s body and eyes move in tandem could generate a more ecologically relevant set of image transformations, and we predict that such a model would outperform the one we present here.</p><p id="P22">What are the implications of this functional organization on how the visual world is encoded by the mouse? We found that the answer depends on the scale at which the question is asked. At the brain-wide level, the visual system is organized to encode features in a manner that is invariant to self-motion related image transformations (<xref ref-type="fig" rid="F3">Figure 3</xref>). In turn, neurons in a given area possess an overall set of visual feature preferences that distinguish one area from another. The differences in area-wide feature preferences are delineated in the feature landscape of visual cortex (<xref ref-type="fig" rid="F4">Figure 4</xref>). Though the preferred stimuli from each HVA do not clearly cluster, their image statistics are significantly different from one another (<xref ref-type="fig" rid="F5">Figure 5</xref>). Finally, at the intra-area level, we find that individual neurons are driven through their dynamic range to a greater extent when presented with the set of own-stimuli compared to other-stimuli (<xref ref-type="fig" rid="F6">Figure 6</xref>). This occurs because even though each preferred stimulus was designed to strongly activate a specific neuron, own-stimuli drive weak activity in many other neurons from the same area.</p><p id="P23">Since own-stimuli maximally activate their target neurons, responses to own-stimuli can be thought of as lying at the boundary of the neuron population manifold (the space of all possible patterns of activity for a population of neurons). Trajectories through the neuron population manifold that drive neurons from low to high activity can then be thought of as tuning curves. Now consider trajectories along the boundary of the manifold (<xref ref-type="fig" rid="F6">Figure 6f</xref>, red arrow). Our results show that these trajectories effectively drive individual neurons through their dynamic range. Since movement along the boundary corresponds to rotations of the population vector, neuron population activity is also strongly decorrelated along these directions through the manifold. As such, trajectories along the boundary represent an efficient way to encode differences between the tuning preferences of neurons in the same area (which we refer to as ‘intra-area coding axes’). In contrast, consider trajectories that move from the center of the manifold outwards towards the boundary – these correspond to scaling of the population vector (<xref ref-type="fig" rid="F6">Figure 6f</xref>, black arrow). These trajectories encode information that is shared across the neuron population, such as transitioning from the overall features preferred by one visual area to those of another (which we refer to as ‘inter-area coding axes’). Notably, the strong correlations between neurons that arise when moving from manifold center towards the boundary mean that such trajectories fail to encode differences in preferred features of neurons within an area. Therefore, since preferred stimuli lie at the boundary of the manifold, traversal through the set of preferred stimuli provides a principled and data-driven way of constructing tuning curves for a population of neurons. This is especially useful for brain areas whose neurons exhibit highly mixed tuning preferences or in which tuning properties are otherwise not easily parameterized. Finally, future work is needed to develop methods for identifying which specific trajectories through the set of preferred stimuli represent ecologically meaningful coding directions.</p></sec><sec id="S9" sec-type="methods" specific-use="web-only"><title>Methods</title><sec id="S10"><title>Animals</title><p id="P24">All procedures were performed in accordance with the Canadian Council on Animal Care and approved by the Montreal Neurological Institute’s Animal Care Committee. Mice used were Thy1-jRGECO1a-WPRE, line GP8.31 (The Jackson Laboratory #030526). All mice were adults (2-6 months old) and mice of both sexes were included. Mice were maintained in a temperature and humidity controlled facility on a 12 hr light/dark cycle.</p></sec><sec id="S11"><title>Head-bar and cranial window implantation</title><p id="P25">Mice were anesthetized with a cocktail containing fentanyl (0.05mg/kg), medetomidine (0.5mg/kg), and midazolam (5mg/kg)<sup><xref ref-type="bibr" rid="R39">39</xref></sup>. Skin was cut away over the skull, and a custom head-bar (adapted from a design from the Polley lab (Harvard University)) was attached to the skull over the right hemisphere using dental cement (C&amp;B Metabond). Next, a 5 mm circular cranial window was made on the left hemisphere over visual cortex and sealed with a 5 mm glass coverslip (Warner) that was held in place with super glue. The exact position of the cranial window varied from mouse to mouse to enable easier optical access to the specific higher visual areas we were interested in.</p></sec><sec id="S12"><title>Widefield calcium imaging</title><p id="P26">Widefield imaging was performed similarly to previous studies<sup><xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R13">13</xref>,<xref ref-type="bibr" rid="R24">24</xref></sup>. In brief, an awake head-fixed mouse was placed under a 2-photon microscope (Neurolabware) with an independent epifluorescent imaging pathway. A 5X objective (Mitutoyo, M Plan Apo) was used to pass excitation light and collect emitted light. Excitation light was generated by a white LED (Thorlabs, MCWHL5), passed through a 559 nm excitation filter (Thorlabs, MF559-34), and a 588 nm dichroic (Thorlabs, MD588). Emission light passed back through the dichroic and a 630 nm emission filter (Thorlabs, MF630-39) and was captured at 10 Hz by a digital camera (PCO edge 3.1 M). We recorded light-evoked calcium responses through the cranial window on the left hemisphere, while visual stimuli were presented to the right eye. The mouse was presented with an inverting checkerboard stimulus that passed in both directions along both azimuth and elevation (10 repetitions for each direction) on a 24-inch computer monitor (BenQ RL2455) positioned 13 cm from the mouse’s eye. The moving stimulus was a 20° wide bar that was periodically swept across the monitor at a velocity of 4 °/s. The bar was filled with a checkerboard pattern (25° spatial frequency) reversing at 6 Hz. Spherical stimulus correction was applied to compensate for the flatness of the monitor<sup><xref ref-type="bibr" rid="R11">11</xref></sup>. The video data was first high pass filtered at half the stimulus frequency. A discrete Fourier transform (DFT) at the stimulus frequency was then applied: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>t</mml:mi></mml:munder><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p id="P27">where <italic>ν</italic> is the stimulus frequency, and the phase difference between directions offset by 180 degrees was calculated to correct for the response delay due to slow dynamics of the calcium dye: <disp-formula id="FD2"><mml:math id="M2"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>v</mml:mi><mml:mi>θ</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mi>v</mml:mi><mml:mrow><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P28">where <italic>θ</italic> is the direction of the stimulus. Phase and amplitude were then extracted. To generate a sign map, the difference between the gradients of the phases for both azimuth and elevation stimuli was calculated: <disp-formula id="FD3"><mml:math id="M3"><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>Δ</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>∇</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>φ</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p id="P29">To identify HVA boundaries, the sign map was standardized, thresholded at 1.5 times the standard deviation, and denoised via binary opening. Lastly, a few (1-4) iterations of binary erosion were applied to refine the boundaries of each region.</p><p id="P30">For widefield experiments in <xref ref-type="fig" rid="F6">Figure 6</xref>, static images were presented for 0.5 s with an inter-stimulus interval uniformly distributed between 1.3-1.7 s. Calcium activity was averaged for each visual area, identified by retinotopic mapping. The data was then high pass filtered at half the stimulus frequency, normalized to baseline, and denoised using singular value decomposition (SVD). Specifically, we found that the first right singular vector of the data matrix (stimulus x time matrix) corresponded well to the stimulus-evoked response kernel, and we therefore defined the response amplitude as the projection of the data matrix onto this vector. In order to compare whether a given stimulus more strongly activated neurons in the area it was generated from, we compared its response amplitude against the average amplitude in all other areas.</p></sec><sec id="S13"><title>2-photon calcium imaging</title><p id="P31">2-photon imaging was performed similarly to previously described<sup><xref ref-type="bibr" rid="R40">40</xref></sup>. In brief, the laser (Insight X3, Spectra-Physics) was set to 1080 nm, and head-fixed, awake animals were placed under a resonant-galvo scanning 2P microscope (Neurolabware). Recordings were acquired at 10 Hz, from neurons in layer 2/3 (depth between 120 and 300 µm measured at the center of the FOV). Animals were presented with 2500 natural images from ImageNet<sup><xref ref-type="bibr" rid="R27">27</xref></sup>. Images were scaled to a size of 135 x 135 pixels, converted to grayscale, and shown for 0.5 s with a vertical height of 98 degrees. The inter-stimulus interval was uniformly distributed between 1.3-1.7 s so that responses were not entrained by a fixed stimulation frequency. A subset of the images (100/2500) were repeated 10 times and used for calculating response reliability and evaluating model performance (see below). Following data acquisition, recordings were processed using Suite2P<sup><xref ref-type="bibr" rid="R41">41</xref></sup> to identify neurons and extract their responses (deconvolved spiking responses). For ANN modelling, data was denoised via singular value decomposition (SVD) to extract stimulus-dependent signals. Specifically, SVD was performed on the ‘trial x time’ matrix for each neuron and the data was projected onto the first singular vector, as we found this closely corresponded to the stimulus-evoked response kernel for the majority of neurons. This denoising was only performed for ANN modelling; validation experiments were analyzed using raw deconvolved spiking responses. To extract stimulus-evoked responses, we averaged the activity in a 700 ms window following stimulus onset and normalized the response for each neuron. In total, we recorded from &gt; 17,500 neurons from 21 animals (V1 = 2,213 cells; LM = 2,315 cells; LI = 3,341 cells; POR = 2,524 cells; AL = 3,214 cells; RL = 4,232 cells), and after taking into account response reliability and explainable variance explained into account (see below), we used &gt; 7,250 model units for our subsequent analyses.</p></sec><sec id="S14"><title>Response reliability</title><p id="P32">The reliability of each neuron in response to the presentation of natural images was calculated as the Spearman-Brown corrected correlation coefficient for random half-splits: <disp-formula id="FD4"><mml:math id="M4"><mml:mrow><mml:mover accent="true"><mml:mi>ρ</mml:mi><mml:mo>^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P33">where ρ is the Pearson correlation coefficient, averaged across 100 random samples. For many areas, we found a bimodal distribution of response reliability across all experiments (<xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 1</xref>) and therefore only included neurons with response reliability &gt; 0.5 for subsequent modeling. Reliability could be affected by various factors, including ‘innate’ trial-by-trial variance, non-visual induced activity (e.g. motor movements), or representational drift occurring over the course of a relatively long recording session. Note that RL appeared to be less reliably overall than the other areas, similar to what has been found previously<sup><xref ref-type="bibr" rid="R20">20</xref></sup>.</p></sec><sec id="S15"><title>Calculation of explainable variance</title><p id="P34">For each neuron, the amount of explainable variance was estimated for the 100 repeated stimuli as <disp-formula id="FD5"><mml:math id="M5"><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mrow/><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mspace width="0.2em"/></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P35">where Var(r) is the variance of the response of a given neuron to all stimuli and ⟨<italic>σ</italic><sup><xref ref-type="bibr" rid="R2">2</xref></sup>⟩<sub><italic>trials</italic></sub> is the average variance of the responses across repeated trials.</p></sec><sec id="S16"><title>Linear model</title><p id="P36">The performance of the ANN model was compared to a simple linear model. Neuron responses were fitted using Partial Least Squares regression with 5-fold cross-validation. Each model was run with a range of bottleneck dimensions (10-20) and the best performing model was chosen.</p></sec><sec id="S17"><title>Artificial neural network modelling</title><p id="P37">Deep convolutional neural networks were trained to predict neural responses to natural images. The networks consisted of four blocks, each block composed of 2D convolutional (kernel size = 3, stride = 1), batch normalization, rectified linear unit (ReLU), and max pooling (kernel size = 2, stride = 2) layers, followed by a factorized readout layer<sup><xref ref-type="bibr" rid="R26">26</xref></sup>. The factorized readout decomposes into independent spatial and feature layers, which consist of tensors of shape (1 x height x width) and (channel x 1 x 1), respectively. The number of channels in the feature layer was set to 512; the height and width of the spatial layer amounted to 8. The spatial layer was further constrained to have non-negative entries and unit Frobenius norm to facilitate its interpretation as a spatial mask. Additionally, L1 regularization was applied to both layers to encourage sparsity.</p><p id="P38">Networks were trained to predict neural responses to natural images by maximizing the normalized dot product: <disp-formula id="FD6"><mml:math id="M6"><mml:mrow><mml:mi>max</mml:mi><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>N</mml:mi><mml:mi>n</mml:mi></mml:munderover><mml:mfrac><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P39">where <italic>y</italic><sub><italic>n</italic></sub>, <inline-formula><mml:math id="M7"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo>˜</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the response and predicted response of neuron n, respectively, averaged over all neurons N. The networks were trained using Adam optimizer for 30 epochs, with L2 regularization, and early stopping. A hyperparameter search was performed with Oríon (<ext-link ext-link-type="uri" xlink:href="https://orion.readthedocs.io/en/stable/">https://orion.readthedocs.io/en/stable/</ext-link>) to find optimal training parameters (<xref ref-type="supplementary-material" rid="SD1">Supplementary Table 1</xref>).</p><p id="P40">The performance of the model was assessed by calculating the squared Pearson correlation coefficient with a set of held-out data (100 repeated stimuli). A linear regression without offset between the explainable variance and model performance was then performed to estimate the fraction explainable variance explained<sup><xref ref-type="bibr" rid="R23">23</xref></sup>.</p></sec><sec id="S18"><title>Spatial mask analysis and alignment</title><p id="P41">We upscaled the spatial layer of the factorized readout (8 x 8) to the stimulus size (135 x 135). To calculate the area of the spatial mask, we fit a 2D Gaussian <disp-formula id="FD7"><mml:math id="M8"><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>A</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:msup><mml:mo>Σ</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mo>Σ</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:mi>ρ</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi>ρ</mml:mi><mml:msub><mml:mi>σ</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mi>y</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p id="P42">where x are the pixel coordinates, µ the center, σ<sub>x</sub> and σ<sub>y</sub> the standard deviations, ρ the correlation coefficient, A the amplitude, and b the offset. The area of the spatial mask was defined as σ<sub>x</sub> σ<sub>y</sub> of the ellipse at one standard deviation. Only spatial masks that were fit well by the 2D Gaussian (r<sup>2</sup> &gt; 0.8) were used to estimate the area. For analyses of image statistics, we masked preferred stimuli to include only those regions within two standard deviations of the median spatial mask size of each model. The edges of the mask were further smoothed by applying a Gaussian filter (σ = 5 pixels). We set the background of the preferred stimuli to a pixel value of 0.5.</p><p id="P43">For <italic>in silico</italic> analyses, the centers of the spatial masks were aligned to the center of the input stimuli. The spatial masks were shifted towards the center with boundaries being wrapped around and values interpolated with 3<sup>rd</sup> order splines. This was repeated 10 times for more consistent alignment.</p></sec><sec id="S19"><title>Generation of preferred stimuli</title><p id="P44">Building on early work that sought to understand the features represented by model units in ANNs<sup><xref ref-type="bibr" rid="R42">42</xref></sup>, here, preferred stimuli were generated using the Lucent library (<ext-link ext-link-type="uri" xlink:href="https://github.com/greentfrapp/lucent">https://github.com/greentfrapp/lucent</ext-link>). In brief, starting from random white noise, images were updated to maximize the response of ANN model units by backpropagating the error through the ANN to the input images. To avoid high frequency noise, which is known to result in image artefacts that interfere with interpretability, small random transformations were applied to the images, including padding (0-4 pixels), jitter (0-8 pixels), and rotations (± 10 degrees). The optimization was run for 512 epochs with a learning rate of 3e-3.</p></sec><sec id="S20"><title>In vivo validation experiments</title><p id="P45"><italic>In vivo</italic> validation experiments followed the same protocol as described in “2-photon calcium imaging”. After the first recording session, data was analyzed and preferred stimuli were generated for all neurons. For the second recording session, the same field of view was found by aligning the blood vessel patterns. On average, we were able to match ~80% of neurons across days. Preferred stimuli and up to 500 natural images (randomly selected from ImageNet) were presented three times each. For the quantification, the responses to the three repeats were averaged and normalized to the distribution of responses to natural images for each neuron.</p></sec><sec id="S21"><title>Distance and partial distance correlation</title><p id="P46">Distance correlation was used to compare the functional similarity between areas. First, the spatial mask of model units in the ANN models were aligned to the center of the input. Next, 10,000 natural images were presented to the models and the responses were collected and normalized for each neuron. We subsampled the resulting response matrix by randomly choosing 200 neurons and 1,000 stimuli and computed the distance correlation<sup><xref ref-type="bibr" rid="R43">43</xref></sup> between all pairs of models, repeated 100 times. The distance correlation was then averaged across repeats. To isolate unique correlations between areas, we computed the partial distance correlation<sup><xref ref-type="bibr" rid="R44">44</xref></sup>, which approximates the conditional distance correlation<sup><xref ref-type="bibr" rid="R45">45</xref></sup>: <disp-formula id="FD8"><mml:math id="M9"><mml:mrow><mml:mi>p</mml:mi><mml:mi>d</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>;</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p id="P47">where C is the concatenated matrix over all areas C ≠ A, B.</p><p id="P48">To visualize the resulting network structure, we converted the pairwise correlation matrix to a dissimilarity matrix d = 1-dCor or d = 1-pdCor and performed multidimensional scaling (MDS) using the scikit-learn library.</p></sec><sec id="S22"><title>Embeddings of the collection of preferred stimuli</title><p id="P49">Three image embeddings were used based on (1) perceptual distance, (2) pixel distance, and (3) spatial frequency distance. The distances were defined as the Euclidean distance in the respective embedding spaces. For perceptual distance, we trained a SimCLR model which learns an embedding that is invariant to a custom set of transformations<sup><xref ref-type="bibr" rid="R32">32</xref>,<xref ref-type="bibr" rid="R33">33</xref></sup>. Specifically, we used the backbone of the ResNet18 network and appended a 256-dimensional linear readout layer. The network was then trained to minimize the following loss function: <disp-formula id="FD9"><mml:math id="M10"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>C</mml:mi><mml:mi>N</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext>log</mml:mtext><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:msubsup><mml:mi>∑</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P50">where b=3955 is the batch size and τ = 0.1 a temperature parameter. Setting τ &lt; 1 prevented dimensionality collapse in which the rank of the embedding space would be much smaller than 256 dimensions. d denotes the Euclidean distance calculated for pairs i, j of two random transformations applied to the same preferred image. For invariance to affine transformations, the random transformations were cropping and scaling (area before resizing: 0.08-1 times the total image size of 64 x 64 pixels, resized to 32 x 32 pixels), random rotation (± 90 degree range), random translation (± 0.1 times the total image size), random horizontal and vertical flips, and a Gaussian blur. SimCLR was trained for 500 epochs with Adam optimizer (learning rate = 1e-4) and a cosine annealing schedule for the learning rate. To visualize the SimCLR embedding, UMAP<sup><xref ref-type="bibr" rid="R34">34</xref></sup> was used to project the embedding into two-dimensional space (metric = Euclidean, min_dist = 0.1, n_neighbours = 30).</p><p id="P51">For pixel similarity, the dimensionality of the images was reduced using PCA to match the 256 dimensions of the SimCLR embedding. For the spatial frequency distance, we performed fast Fourier Transform (FFT), calculated the power spectra, removed the DC component, and reduced the dimensionality to 256 using PCA.</p><p id="P52">A k-nearest neighbours classifier (k=20) was used to assess classification accuracy of the image embeddings (note that the results were qualitatively indistinguishable for k = 10, 20, or 50; data not shown). The classifier was trained on 75% of the data and tested on the remaining 25%. This was repeated 100 times. We separately sampled each area’s preferred stimuli to account for unequal numbers of preferred stimuli generated for each area.</p><p id="P53">To estimate the overlap between embeddings of preferred stimuli from two areas, we calculated the average distribution of area labels of the 20 nearest neighbours of each image. For example, to calculate the overlap of V1 preferred stimuli with the HVAs, for the 20 nearest neighbours of each preferred stimulus in V1, we counted the number of neighbours that belonged to each HVA. This procedure resulted in a 6 x 6 overlap matrix M for each pair of visual areas. This matrix was not symmetric, so we next averaged across the diagonal, M<sub>symmetric</sub> = (M+M<sup>T</sup>)/2, i.e. we averaged the overlap of area A with B and the overlap of area B with A. We used this symmetric matrix as a measure for local overlap.</p><p id="P54">The topological similarity between the local overlap with the functional similarity was computed as the Spearman correlation between the symmetric overlap matrix (<xref ref-type="fig" rid="F3">Figure 3d</xref>) and the pairwise partial distance correlation matrix (<xref ref-type="fig" rid="F2">Figure 2c</xref>).</p></sec><sec id="S23"><title>Generating the ‘Feature landscape of mouse visual cortex’</title><p id="P55">We visualized the two-dimensional UMAP projection of the SimCLR embedding as an image atlas. First, the embedding was scaled to lie within the unit interval [0,1]. Next, we tiled the embedding into an NxN grid (for <xref ref-type="fig" rid="F4">Figure 4</xref>, a 40x40 grid is shown). Then, within each tile, we randomly chose an image to display. For visualization purposes, the small number of empty tiles in the 40x40 grid that were fully enclosed within the UMAP projection (i.e. ‘holes’ in the grid) were filled in using the closest image from a neighbouring tile.</p></sec><sec id="S24"><title>Computing the most representative preferred stimuli</title><p id="P56">For each preferred stimulus, we defined the degree of representative-ness as the fraction of nearest neighbours (k = 100) in SimCLR’s embedding space that were from the same visual area. We then chose the top 200 most representative stimuli for each region for subsequent analysis in <xref ref-type="fig" rid="F5">Figure 5</xref>.</p></sec><sec id="S25"><title>Preferred stimulus analyses</title><p id="P57">We computed various low-level image statistics for the preferred stimuli. Pixel intensities ranged from 0 to 1. <list list-type="simple" id="L1"><list-item><label>(1)</label><p id="P58">Luminance was calculated as the average pixel intensity within the spatial mask.</p></list-item><list-item><label>(2)</label><p id="P59">To identify dark and light segments, we thresholded stimuli around the background intensity (for dark segments: pixels &lt; 0.49; for light segments: pixels &gt; 0.51). Next, disconnected segments with area &gt; 2 pixels were identified. We analyzed the number of segments and the area, which was normalized by the full area of the stimulus.</p></list-item><list-item><label>(3)</label><p id="P60">To calculate the spatial frequency content, we first performed a FFT and averaged the power spectrum radially with bin size = 1 pixel. To calculate the radial average, we converted pixel Cartesian coordinates to polar coordinates, defining the center of the image as the origin (0,0). Pixels were then binned according to their radius, rounded down. Spatial frequency power was normalized for each frequency across all images from all areas.</p></list-item><list-item><label>(4)</label><p id="P61">To calculate folio (1-fold) and quarto (2-fold) symmetries, we first performed FFT and averaged the power spectrum axially with bin size = 1/16 π radians. To calculate the axial average, we converted pixel Cartesian coordinates to polar coordinates, defining the center of the image as the origin (0,0). Pixels were then binned according to their angle, rounded down. Next, we defined an n-fold symmetry index (SI) as <disp-formula id="FD10"><mml:math id="M11"><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mrow><mml:msup><mml:mstyle mathsize="140%" displaystyle="true"><mml:mi>∑</mml:mi></mml:mstyle><mml:mspace width="0.2em"/></mml:msup></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:msub><mml:mi>P</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula> where P<sub>θ</sub> is the average power at angle θ and the scalar multiplier 2 is due to the inherent point symmetry of the FFT power spectrum. SI is defined on the unit interval with 0 denoting a lack of symmetry and 1 being a fully n-fold symmetric image.</p></list-item></list></p></sec><sec id="S26"><title>Data analysis</title><p id="P62">Statistical significance was assessed using Mann-Whitney U test for unpaired data, Wilcoxon rank sum test for paired data, and Kruskal-Wallis followed by <italic>post hoc</italic> Dunn’s test with Bonferroni correction for multiple comparisons. Data values are reported as mean ± SEM, unless mentioned otherwise. Box plot elements are defined as follows: center line = median, box limits = upper and lower quartiles, whiskers = 1.5 times interquartile range.</p></sec><sec id="S27"><title>Nomenclature of mouse HVAs</title><p id="P63">The list<sup><xref ref-type="bibr" rid="R19">19</xref></sup> of widely agreed upon mouse HVAs include: LM (lateromedial), AL (anterolateral), RL (rostrolateral), A (anterior), AM (anteromedial), PM (posterior medial), LI (laterointermediate), P (posterior), and POR (postrhinal).</p></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Figures and Tables</label><media xlink:href="EMS190621-supplement-Supplementary_Figures_and_Tables.pdf" mimetype="application" mime-subtype="pdf" id="d88aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S28"><title>Acknowledgements</title><p>We thank E. Macé for assistance setting up HVA identification using widefield calcium imaging. We thank M. Krause, K. Kuchibhotla and C. Pack for helpful feedback on the manuscript. We acknowledge our funding sources: Jeanne Timmins Costello Fellowship and VHRN Recruitment Award Scholarship to R.T.; HBHL Fellowship to RdS; Vanier Scholarship to A.G.; NSERC CGS-D to D.L; HBP Special Grant Agreement 3 and BBSRC (BB/Y003020/1) to J.W.; NSERC CGS-M and FRQNT Scholarship to E.C.; NSERC Discovery Grant (RGPIN-2020-05105), NSERC Discovery Accelerator Supplement (RGPAS-2020-00031), Arthur B. McDonald Fellowship (566355-2022), and CIFAR Canada AI Chair (Learning in Machine and Brains Fellowship) to B.A.R.; HFSP Career Development Award, Sloan Research Fellowship, ONR-Global Research Grant, Canada Research Chair, and CIHR Project Grant to S.T. This research was also enabled in part by support provided by Calcul Québec (<ext-link ext-link-type="uri" xlink:href="https://www.calculquebec.ca/en/">https://www.calculquebec.ca/en/</ext-link>) and Compute Canada (<ext-link ext-link-type="uri" xlink:href="https://www.computecanada.ca">www.computecanada.ca</ext-link>). The authors acknowledge the material support of NVIDIA in the form of computational resources.</p></ack><sec id="S29" sec-type="data-availability"><title>Data availability</title><p id="P64">All processed data included in the manuscript will be made available upon publication.</p></sec><sec id="S30" sec-type="data-availability"><title>Code availability</title><p id="P65">The code for ANN modelling and analysis can be accessed from <ext-link ext-link-type="uri" xlink:href="https://github.com/Trenholm-Lab/MouseFeatureLandscape">https://github.com/Trenholm-Lab/MouseFeatureLandscape</ext-link>.</p></sec><fn-group><fn fn-type="con" id="FN1"><p id="P66"><bold>Author contributions:</bold></p><p id="P67">Experiments were designed by R.T., R.d.S., D.L., A.G., B.A.R., and S.T. <italic>In vivo</italic> data was collected by R.T., R.d.S., and E.C. Data analysis and statistics were performed by R.T., R.d.S., and J.W. ANN design and modelling was performed by R.T., D.L., A.G., and B.A.R. P.B. advised on issues related to ANN design and implementation. Figures were generated by R.T. The paper was written by R.T. and S.T.</p></fn></fn-group><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trenholm</surname><given-names>S</given-names></name><name><surname>Krishnaswamy</surname><given-names>A</given-names></name></person-group><article-title>An Annotated Journey through Modern Visual Neuroscience</article-title><source>J Neurosci Off J Soc Neurosci</source><year>2020</year><volume>40</volume><fpage>44</fpage><lpage>53</lpage><pub-id pub-id-type="pmcid">PMC6939491</pub-id><pub-id pub-id-type="pmid">31896562</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1061-19.2019</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Grinvald</surname><given-names>A</given-names></name></person-group><article-title>Iso-orientation domains in cat visual cortex are arranged in pinwheel-like patterns</article-title><source>Nature</source><year>1991</year><volume>353</volume><fpage>429</fpage><lpage>431</lpage><pub-id pub-id-type="pmid">1896085</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>J Neurosci Off J Soc Neurosci</source><year>1997</year><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmcid">PMC6573547</pub-id><pub-id pub-id-type="pmid">9151747</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04302.1997</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><article-title>Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex</article-title><source>J Physiol</source><year>1962</year><volume>160</volume><fpage>106</fpage><lpage>154</lpage><pub-id pub-id-type="pmcid">PMC1359523</pub-id><pub-id pub-id-type="pmid">14449617</pub-id><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>The Code for Facial Identity in the Primate Brain</article-title><source>Cell</source><year>2017</year><volume>169</volume><fpage>1013</fpage><lpage>1028</lpage><elocation-id>e14</elocation-id><pub-id pub-id-type="pmcid">PMC8088389</pub-id><pub-id pub-id-type="pmid">28575666</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2017.05.011</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Area map of mouse visual cortex</article-title><source>J Comp Neurol</source><year>2007</year><volume>502</volume><fpage>339</fpage><lpage>357</lpage><pub-id pub-id-type="pmid">17366604</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Burkhalter</surname><given-names>A</given-names></name></person-group><article-title>Network Analysis of Corticocortical Connections Reveals Ventral and Dorsal Processing Streams in Mouse Visual Cortex</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><fpage>4386</fpage><lpage>4399</lpage><pub-id pub-id-type="pmcid">PMC3328193</pub-id><pub-id pub-id-type="pmid">22457489</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.6063-11.2012</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>SW</given-names></name><etal/></person-group><article-title>A mesoscale connectome of the mouse brain</article-title><source>Nature</source><year>2014</year><volume>508</volume><fpage>207</fpage><lpage>214</lpage><pub-id pub-id-type="pmcid">PMC5102064</pub-id><pub-id pub-id-type="pmid">24695228</pub-id><pub-id pub-id-type="doi">10.1038/nature13186</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Souza</surname><given-names>RD</given-names></name><etal/></person-group><article-title>Hierarchical and nonhierarchical features of the mouse visual cortical network</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><fpage>503</fpage><pub-id pub-id-type="pmcid">PMC8791996</pub-id><pub-id pub-id-type="pmid">35082302</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-28035-y</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andermann</surname><given-names>ML</given-names></name><name><surname>Kerlin</surname><given-names>AM</given-names></name><name><surname>Roumis</surname><given-names>DK</given-names></name><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><article-title>Functional specialization of mouse higher visual cortical areas</article-title><source>Neuron</source><year>2011</year><volume>72</volume><fpage>1025</fpage><lpage>1039</lpage><pub-id pub-id-type="pmcid">PMC3876958</pub-id><pub-id pub-id-type="pmid">22196337</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.11.013</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><article-title>Functional specialization of seven mouse visual cortical areas</article-title><source>Neuron</source><year>2011</year><volume>72</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="pmcid">PMC3248795</pub-id><pub-id pub-id-type="pmid">22196338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.004</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Zhuang</surname><given-names>J</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><article-title>Automated identification of mouse visual areas with intrinsic signal imaging</article-title><source>Nat Protoc</source><year>2017</year><volume>12</volume><fpage>32</fpage><lpage>43</lpage><pub-id pub-id-type="pmcid">PMC5381647</pub-id><pub-id pub-id-type="pmid">27906169</pub-id><pub-id pub-id-type="doi">10.1038/nprot.2016.158</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>J</given-names></name><etal/></person-group><article-title>An extended retinotopic map of mouse cortex</article-title><source>eLife</source><year>2017</year><volume>6</volume><pub-id pub-id-type="pmcid">PMC5218535</pub-id><pub-id pub-id-type="pmid">28059700</pub-id><pub-id pub-id-type="doi">10.7554/eLife.18372</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname><given-names>JH</given-names></name><etal/></person-group><article-title>Survey of spiking in the mouse visual system reveals functional hierarchy</article-title><source>Nature</source><year>2021</year><volume>592</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="pmcid">PMC10399640</pub-id><pub-id pub-id-type="pmid">33473216</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-03171-x</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roth</surname><given-names>MM</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name><name><surname>Kampa</surname><given-names>BM</given-names></name></person-group><article-title>Distinct Functional Properties of Primary and Posteromedial Visual Area of Mouse Neocortex</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><fpage>9716</fpage><lpage>9726</lpage><pub-id pub-id-type="pmcid">PMC6622284</pub-id><pub-id pub-id-type="pmid">22787057</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0110-12.2012</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tohmi</surname><given-names>M</given-names></name><name><surname>Meguro</surname><given-names>R</given-names></name><name><surname>Tsukano</surname><given-names>H</given-names></name><name><surname>Hishida</surname><given-names>R</given-names></name><name><surname>Shibuki</surname><given-names>K</given-names></name></person-group><article-title>The Extrageniculate Visual Pathway Generates Distinct Response Properties in the Higher Visual Areas of Mice</article-title><source>Curr Biol</source><year>2014</year><volume>24</volume><fpage>587</fpage><lpage>597</lpage><pub-id pub-id-type="pmid">24583013</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>IT</given-names></name><name><surname>Townsend</surname><given-names>LB</given-names></name><name><surname>Huh</surname><given-names>R</given-names></name><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Smith</surname><given-names>SL</given-names></name></person-group><article-title>Stream-dependent development of higher visual cortical areas</article-title><source>Nat Neurosci</source><year>2017</year><volume>20</volume><fpage>200</fpage><lpage>208</lpage><pub-id pub-id-type="pmcid">PMC5272868</pub-id><pub-id pub-id-type="pmid">28067905</pub-id><pub-id pub-id-type="doi">10.1038/nn.4469</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesa</surname><given-names>N</given-names></name><name><surname>Waters</surname><given-names>J</given-names></name><name><surname>de Vries</surname><given-names>SEJ</given-names></name></person-group><article-title>The Effect of Inclusion Criteria on the Functional Properties Reported in Mouse Visual Cortex</article-title><source>eNeuro</source><year>2021</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC8114876</pub-id><pub-id pub-id-type="pmid">33509948</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0188-20.2021</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickfeld</surname><given-names>LL</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name></person-group><article-title>Higher-Order Areas of the Mouse Visual Cortex</article-title><source>Annu Rev Vis Sci</source><year>2017</year><volume>3</volume><fpage>251</fpage><lpage>273</lpage><pub-id pub-id-type="pmid">28746815</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Vries</surname><given-names>SEJ</given-names></name><etal/></person-group><article-title>A large-scale standardized physiological survey reveals functional organization of the mouse visual cortex</article-title><source>Nat Neurosci</source><year>2020</year><volume>23</volume><fpage>138</fpage><lpage>151</lpage><pub-id pub-id-type="pmcid">PMC6948932</pub-id><pub-id pub-id-type="pmid">31844315</pub-id><pub-id pub-id-type="doi">10.1038/s41593-019-0550-9</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ponce</surname><given-names>CR</given-names></name><etal/></person-group><article-title>Evolving Images for Visual Neurons Using a Deep Generative Network Reveals Coding Principles and Neuronal Preferences</article-title><source>Cell</source><year>2019</year><volume>177</volume><fpage>999</fpage><lpage>1009</lpage><elocation-id>e10</elocation-id><pub-id pub-id-type="pmcid">PMC6718199</pub-id><pub-id pub-id-type="pmid">31051108</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2019.04.005</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><year>2019</year><volume>364</volume><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>EY</given-names></name><etal/></person-group><article-title>Inception loops discover what excites neurons most using deep predictive models</article-title><source>Nat Neurosci</source><year>2019</year><volume>22</volume><fpage>2060</fpage><lpage>2065</lpage><pub-id pub-id-type="pmid">31686023</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalatsky</surname><given-names>VA</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><article-title>New paradigm for optical imaging: temporally encoded maps of intrinsic signal</article-title><source>Neuron</source><year>2003</year><volume>38</volume><fpage>529</fpage><lpage>545</lpage><pub-id pub-id-type="pmid">12765606</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dana</surname><given-names>H</given-names></name><etal/></person-group><article-title>Thy1 transgenic mice expressing the red fluorescent calcium indicator jRGECO1a for neuronal population imaging in vivo</article-title><source>PloS One</source><year>2018</year><volume>13</volume><elocation-id>e0205444</elocation-id><pub-id pub-id-type="pmcid">PMC6181368</pub-id><pub-id pub-id-type="pmid">30308007</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0205444</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Klindt</surname><given-names>D</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><chapter-title>Neural system identification for large populations separating “ what” and “ where”</chapter-title><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><year>2017</year><volume>30</volume></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="web"><source>ImageNet: A large-scale hierarchical image database</source><comment><ext-link ext-link-type="uri" xlink:href="https://ieeexplore.ieee.org/abstract/document/5206848/">https://ieeexplore.ieee.org/abstract/document/5206848/</ext-link></comment></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><year>2012</year><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="pmcid">PMC3306444</pub-id><pub-id pub-id-type="pmid">22325196</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tacchetti</surname><given-names>A</given-names></name><name><surname>Isik</surname><given-names>L</given-names></name><name><surname>Poggio</surname><given-names>TA</given-names></name></person-group><article-title>Invariant Recognition Shapes Neural Representations of Visual Input</article-title><source>Annu Rev Vis Sci</source><year>2018</year><volume>4</volume><fpage>403</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">30052494</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><year>2020</year><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC8088388</pub-id><pub-id pub-id-type="pmid">32494012</pub-id><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><article-title>On the relationship between maps and domains in inferotemporal cortex</article-title><source>Nat Rev Neurosci</source><year>2021</year><volume>22</volume><fpage>573</fpage><lpage>583</lpage><pub-id pub-id-type="pmcid">PMC8865285</pub-id><pub-id pub-id-type="pmid">34345018</pub-id><pub-id pub-id-type="doi">10.1038/s41583-021-00490-4</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title>A Simple Framework for Contrastive Learning of Visual Representations</article-title></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willeke</surname><given-names>KF</given-names></name><etal/></person-group><article-title>Deep learning-driven characterization of single cell tuning in primate visual area V4 unveils topological organization</article-title><year>2023</year><elocation-id>2023.05.12.540591</elocation-id><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2023.05.12.540591</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McInnes</surname><given-names>L</given-names></name><name><surname>Healy</surname><given-names>J</given-names></name><name><surname>Melville</surname><given-names>J</given-names></name></person-group><article-title>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</article-title><year>2020</year><comment>Preprint at</comment><pub-id pub-id-type="doi">10.48550/arXiv.1802.03426</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Vermaercke</surname><given-names>B</given-names></name><name><surname>Bonin</surname><given-names>V</given-names></name></person-group><article-title>Diversity of spatiotemporal coding reveals specialized visual processing streams in the mouse cortex</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>3249</elocation-id><pub-id pub-id-type="pmcid">PMC9170684</pub-id><pub-id pub-id-type="pmid">35668056</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-29656-z</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahimi-Nasrabadi</surname><given-names>H</given-names></name><etal/></person-group><article-title>Luminance Contrast Shifts Dominance Balance between ON and OFF Pathways in Human Vision</article-title><source>J Neurosci</source><year>2023</year><volume>43</volume><fpage>993</fpage><lpage>1007</lpage><pub-id pub-id-type="pmcid">PMC9908321</pub-id><pub-id pub-id-type="pmid">36535768</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1672-22.2022</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ustyuzhaninov</surname><given-names>I</given-names></name><etal/></person-group><article-title>Digital twin reveals combinatorial code of non-linear computations in the mouse primary visual cortex</article-title><year>2022</year><elocation-id>2022.02.10.479884</elocation-id><comment>Preprint at</comment><pub-id pub-id-type="doi">10.1101/2022.02.10.479884</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadena</surname><given-names>SA</given-names></name><etal/></person-group><article-title>How well do deep neural networks trained on object recognition characterize the mouse visual system?</article-title><year>2019</year></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asumbisa</surname><given-names>K</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Trenholm</surname><given-names>S</given-names></name></person-group><article-title>Flexible cue anchoring strategies enable stable head direction coding in both sighted and blind animals</article-title><source>Nat Commun</source><year>2022</year><volume>13</volume><elocation-id>5483</elocation-id><pub-id pub-id-type="pmcid">PMC9485117</pub-id><pub-id pub-id-type="pmid">36123333</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-33204-0</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wertz</surname><given-names>A</given-names></name><etal/></person-group><article-title>PRESYNAPTIC NETWORKS. Single-cell-initiated monosynaptic tracing reveals layer-specific cortical network modules</article-title><source>Science</source><year>2015</year><volume>349</volume><fpage>70</fpage><lpage>74</lpage><pub-id pub-id-type="pmid">26138975</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><etal/></person-group><article-title>Suite2p: beyond 10,000 neurons with standard two-photon microscopy</article-title><source>bioRxiv</source><year>2017</year><elocation-id>061507</elocation-id><pub-id pub-id-type="doi">10.1101/061507</pub-id></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erhan</surname><given-names>D</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name><name><surname>Box</surname><given-names>PO</given-names></name></person-group><article-title>Visualizing Higher-Layer Features of a Deep Network</article-title></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Székely</surname><given-names>GJ</given-names></name><name><surname>Rizzo</surname><given-names>ML</given-names></name><name><surname>Bakirov</surname><given-names>NK</given-names></name></person-group><article-title>Measuring and testing dependence by correlation of distances</article-title><source>Ann Stat</source><year>2007</year><volume>35</volume></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Szekely</surname><given-names>GJ</given-names></name><name><surname>Rizzo</surname><given-names>ML</given-names></name></person-group><source>Partial Distance Correlation with Methods for Dissimilarities</source><year>2014</year><comment>Preprint at <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/1310.2926">http://arxiv.org/abs/1310.2926</ext-link></comment></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Edelmann</surname><given-names>D</given-names></name><name><surname>Fokianos</surname><given-names>K</given-names></name><name><surname>Pitsillou</surname><given-names>M</given-names></name></person-group><article-title>An Updated Literature Review of Distance Correlation and Its Applications to Time Series</article-title><source>Int Stat Rev</source><year>2019</year><volume>87</volume><fpage>237</fpage><lpage>262</lpage></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Modelling mouse visual cortex with artificial neural networks.</title><p><bold>a</bold>, Visual cortical areas were identified via retinotopic mapping using widefield calcium imaging. Six visual areas were targeted for subsequent 2-photon calcium imaging: V1, LM, LI, POR, AL, and RL. (A: Anterior, L: Lateral, M: Medial, P: Posterior). <bold>b</bold>, Workflow for building ANN models of mouse visual areas. Neuronal responses to 2,500 natural images were recorded in each area using 2-photon calcium imaging. For each area, an ANN model was trained to predict the neuronal responses to the same image set. We used the models to generate preferred stimuli. For validation experiments, preferred stimuli were presented back to the animal on a second day. <bold>c</bold>, Model performance measure as the fraction explainable variance explained for ANN models (<italic>coloured</italic>, left bars) compared to a linear model (<italic>grey</italic>, right bars). <bold>d</bold>, <italic>Top</italic>, Example 2-photon imaging field of view from a validation experiment. <italic>Bottom</italic>, Zoomed-in view of the region highlighted above. Scale bars: <italic>Top</italic>, 50 μm; <italic>Bottom</italic>, 30 μm. <bold>e</bold>, Responses of ten example neurons (rows) to their respective preferred stimuli (columns) from the experiment shown in (<bold>d</bold>). <bold>f</bold>, Same as (<bold>e</bold>), except for all 231 neurons simultaneously recorded in a single V1 session, showing a strong selective preference for preferred stimuli, with little ‘off-diagonal’ activity. <bold>g</bold>, For the recording session above, the distribution of response amplitudes of each neuron to its preferred stimulus, as a percentile of its response amplitude to all preferred and natural stimuli. <italic>Inset</italic> – Normalized neuronal response amplitudes to preferred (P) and natural (N) images. <bold>h</bold>, Same as (<bold>g</bold>), except pooling all validation experiments across all six visual cortical areas (V1: n = 364 neurons/2 mice, LM: n = 351 neurons/2 mice, LI: n = 129 neurons/3 mice, POR: n = 80 neurons/2 mice, AL: n = 126 neurons/2 mice, RL: n = 105 neurons/2 mice). All asterisks, p-value &lt; 1e-11.</p></caption><graphic xlink:href="EMS190621-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>The functional organization of mouse visual cortex.</title><p><bold>a</bold>, <italic>Top</italic>, The spatial mask layer of each model unit was aligned, and each ANN was presented with the same 10,000 natural images. <italic>Bottom</italic>, Distance correlation (dCor) was used to compute the functional similarity between responses of pairs of areas. Partial distance correlation (pdCor) was used to condition out correlations shared globally across all areas. <bold>b</bold>, A matrix showing the pairwise distance correlation (dCor) between the stimulus manifolds of pairs of areas. <italic>Inset</italic>, Visualization of the resulting network structure using multidimensional scaling (MDS). <bold>c</bold>, Same as (<bold>b</bold>) but for pdCor. <bold>d</bold>, Two-stream hierarchy proposed by D’Souza et al. based on anatomical connectivity<sup><xref ref-type="bibr" rid="R9">9</xref></sup>. <bold>e</bold>, The functional distance from V1, defined as 1 – pdCor, is shown against the median spatial mask area of each visual area. <italic>Inset</italic> – Distribution of spatial mask area for each ANN model. For full statistical comparisons, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 2</xref>.</p></caption><graphic xlink:href="EMS190621-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>A preferred stimulus embedding based on image similarities with invariance to self-motion related transformations best explains the functional organization of mouse visual cortex.</title><p><bold>a</bold>, SimCLR was used to generate an embedding space that is invariant to affine transformations. The SimCLR model was trained to minimize the distance in its 256-dimensional embedding space between pairs of randomly transformed versions of a given preferred stimulus. <bold>b</bold>, Classification accuracy of stimulus labels using a kNN (k=20) trained either on a template-matching embedding (Pixel), spatial frequency embedding (FFT), or SimCLR embedding. <bold>c</bold>, Visualization of SimCLR’s embedding of all preferred stimuli, reduced to two dimensions using UMAP. Preferred stimuli from each area are colour-coded according to the visual area that each stimulus originated from. <bold>d</bold>, A matrix indicating the extent of local overlap between the stimulus manifolds in the SimCLR embedding space between each area, and the resulting network structure visualized using MDS. <bold>e</bold>, Spearman correlation of the pairwise overlap matrix (<bold>d</bold>) with the partial distance correlation matrix (<xref ref-type="fig" rid="F2">Figure 2c</xref>) for Pixel, FFT, and SimCLR embeddings.</p></caption><graphic xlink:href="EMS190621-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>The feature landscape of visual cortex.</title><p>Preferred stimuli were first projected into SimCLR’s 256-dimensional embedding space, then further projected onto a two-dimensional plane using UMAP. The UMAP projection was tiled into a 40 x 40 grid, and for each tile a random preferred stimulus contained within that tile is shown. <italic>Inset</italic> – Distribution of preferred stimuli from different cortical areas across the UMAP embedding (related to <xref ref-type="fig" rid="F3">Figure 3c</xref>).</p></caption><graphic xlink:href="EMS190621-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Mouse visual areas have distinct visual feature preferences.</title><p><bold>a</bold>, The most representative stimuli for each visual area. <bold>b</bold>, Mean luminance of representative stimuli across regions. <bold>c</bold>, Number of light and dark segments. <bold>d</bold>, Area of light and dark segments normalized to the area of the full stimulus. <bold>e</bold>, Radially averaged spatial frequency power spectrum. <bold>f</bold>, <italic>Folio</italic> (one-fold), and <bold>g</bold>, <italic>quarto</italic> (two-fold) symmetry index. For the full set of statistical comparisons, see <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 3</xref>.</p></caption><graphic xlink:href="EMS190621-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Intra-vs. inter-area feature coding axes arise from distinct trajectories through the neuron population manifold.</title><p><bold>a</bold>, Schematic outlining that preferred stimuli from various areas were presented to mice while activity across visual areas was measured using widefield calcium imaging. <bold>b</bold>, For experiments outlined in (a), average response amplitude within an area to its own preferred stimuli was compared to the activity that the same preferred stimuli drove in other areas. Wilcoxon rank sum test: for V1, LI, POR and AL, p &lt; 0.001; for RL, p = 0.0118; for LM, p = 0.1007. See also <xref ref-type="supplementary-material" rid="SD1">Supplementary Figure 5. C</xref>, Example <italic>in silico</italic> data of response amplitudes of individual neurons to own-and other-stimuli, pooled across model units and all visual areas. <italic>Left</italic>, Probability density plots (histograms) of response amplitudes for all model units, pooled across all areas, evoked by the sets of ‘own-stimuli’ or ‘other-stimuli. <italic>Right</italic>, Cumulative density plots (line plots) of the same data. <bold>d</bold>, <italic>Left</italic>, The response amplitude (from <italic>in silico</italic> models, averaged across all areas) at the 10<sup>th</sup> and 90<sup>th</sup> percentiles evoked by own-stimuli and other-stimuli. Wilcoxon rank rum test: p &lt; 0.001. <italic>Right</italic>, The <italic>Left</italic> data replotted to show that the 90<sup>th</sup> percentile is significantly more different between own- and other-stimuli than the 10<sup>th</sup> percentile. Mann-Whitney U test: p &lt; 0.001. <bold>e</bold>, For <italic>in silico</italic> experiments, for all areas averaged together, the population variance in an area was greater when it was shown own-compared to other-stimuli. Mann-Whitney U Test: p &lt; 0.001. <bold>f</bold>, Schematic of trajectories through the neuron population manifold. X’s represent responses to 3 different own-(<italic>red</italic>) and other-stimuli (<italic>black</italic>). Own-stimuli are located on the boundary of the population response manifold (the space of all possible population responses). Trajectories along the boundary of the manifold, i.e. rotations of the population vector, encode intra-area feature differences (intra-area coding axes, <italic>red</italic>), whereas trajectories from the center outwards, i.e. scaling of the population vector, correspond to axes encoding the overall feature differences across areas (inter-area coding axes, <italic>black</italic>).</p></caption><graphic xlink:href="EMS190621-f006"/></fig></floats-group></article>