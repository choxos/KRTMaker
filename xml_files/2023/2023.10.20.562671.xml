<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189944</article-id><article-id pub-id-type="doi">10.1101/2023.10.20.562671</article-id><article-id pub-id-type="archive">PPR745472</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Priming using Human and Chimpanzee Expressions of Emotion Biases Attention toward Positive Emotions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Matsulevits</surname><given-names>Anna</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Kret</surname><given-names>Mariska E.</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib></contrib-group><aff id="A1"><label>1</label>Groupe d’lmagerie Neurofonctionnelle, Institut des Maladies Neurodégénératives-UMR 5293, CNRS, CEA, University of Bordeaux, Bordeaux, France</aff><aff id="A2"><label>2</label>Brain Connectivity and Behaviour Laboratory, Sorbonne Universities, Paris, France</aff><aff id="A3"><label>3</label>Comparative Psychology and Affective Neuroscience Lab, Cognitive Psychology Unit, Leiden University, Netherlands. <ext-link ext-link-type="uri" xlink:href="https://www.mariskakret.com">www.mariskakret.com</ext-link></aff><aff id="A4"><label>4</label>Leiden Institute for Brain and Cognition (LIBC), Leiden University, Netherlands</aff><author-notes><corresp id="CR1">Corresponding Author: Anna Matsulevits – <email>anna.matsulevits@u-bordeaux.fr</email></corresp></author-notes><pub-date pub-type="nihms-submitted"><day>22</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>20</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Perceiving and correctly interpreting emotional expressions is one of the most important abilities for social animals’ communication. It determines the majority of social interactions, group dynamics, and cooperation, being highly relevant for an individual’s survival. Core mechanisms of this ability have been hypothesized to be shared across closely related species with phylogenetic similarities. Here, we explored homologies in human processing of different species’ facial expressions using eye-tracking. Introducing a prime-target paradigm, we tested the influences on human attention elicited by priming with differently valenced emotional stimuli depicting human and chimpanzee faces. We demonstrated an attention shift towards the conspecific (human) target picture that was congruent with the valence depicted in the primer picture. We did not find this effect with heterospecific (chimpanzee) primers and ruled out that this was due to participants interpreting them incorrectly. Implications about the involvement of related emotion-processing mechanisms for human and chimpanzee facial expressions, are discussed. Systematic cross-species-investigations of emotional expressions are needed to unravel how emotion representation mechanisms can extend to process other species’ faces. Through such studies, we address the gap of a shared evolutionary ancestry between humans and other animals to ultimately answer the question of <italic>“Where do emotions come from?”</italic>.</p></abstract><kwd-group><kwd>Emotional Priming</kwd><kwd>Chimpanzee</kwd><kwd>Eye-Tracking</kwd><kwd>Cognition</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">In social animal groups, the ability to integrate different cues from facial expressions largely determines inter-individual interactions and influences group dynamics and cooperation, conceivably having an impact on survival (<xref ref-type="bibr" rid="R4">Bourjade, 2016</xref>). Nevertheless, the mechanisms by which our brains process these sensory cues from species other than humans remain relatively unexplored. In the current study, we aspired to investigate the underpinnings of the discernment of complex facial muscle movements – manifestations of emotional expressions. We tested whether humans possess the capacity to interpret and comprehend the facial expressions of one of our genetically closest kin, the chimpanzee (<italic>Pan troglodytes</italic>), and to what extent their priming influence endures, as compared to emotional expressions within our own species.</p><p id="P3">While the importance of emotions in regulating and navigating the lives of social species is well established (van Hoof, 1967), the universality of emotional expressions, their contagion, and how we and other animals process them remain controversial. Referring to Darwin’s theory of emotion universality (1872), humans and other animals show emotional states through remarkably similar face-and-body actions. Similar adaptive properties that apply across species further support cognitive underpinnings of emotion processing being well conserved among humans and other great apes (<xref ref-type="bibr" rid="R2">Anderson &amp; Adolphs, 2014</xref>; <xref ref-type="bibr" rid="R33">Nieuwburg, Ploeger &amp; Kret, 2021</xref>). To effectively process such signals, it is required to form an internal representation of the other’s emotion based on the information from different facial muscle movements and their complex combinations (<xref ref-type="bibr" rid="R33">Nieuwburg et al., 2021</xref>). As a result of their nearly identical-to-human facial musculature (Burrows et al., 2006; <xref ref-type="bibr" rid="R55">Waller et al. 2006</xref>), chimpanzees are capable of activating a high variety of distinct combinations of so-called action units (AUs) that can resemble even more complex and equivalent-to-human facial expressions (<xref ref-type="bibr" rid="R50">Vick et al., 2007</xref>). Supporting the ability of great apes to produce comparable facial expressions, <xref ref-type="bibr" rid="R54">Waller, Cray, and Burrows (2008)</xref> found facial muscles which are involved in the production of the basic emotional expressions to show the least individual variation across species.</p><p id="P4">Whether the internal representation of human emotion can be translated to other species and to what extent we are able to assess emotions in heterospecifics is not entirely clear. Based on the theory of emotion universality and facial expressions having an innate basis leading to a stereotypical appearance, evolutionary closely related species should be able to process each other’s facial expressions (<xref ref-type="bibr" rid="R42">Pollick &amp; de Waal, 2007</xref>). If faces of different species that roughly share the spatial arrangement of face elements (eyes, nose, mouth, etc.) are associated with one and the same face prototype, the formation of emotion representations and the correct interpretation of these may be facilitated. Hence, a particular expression in two different species would, in this case, be classified as homologous and assigned to the same category, presumably eliciting similar reactions in the observer. Using the first objective and standardized instrument (ChimpFACS) salient expressions communicating agonistic and affiliative affective states have been validated with a fair degree of certainty in the chimpanzee (<xref ref-type="bibr" rid="R41">Parr et al, 2007</xref>; <xref ref-type="bibr" rid="R38">Parr &amp; Waller, 2006</xref>). However, comparative studies on visual perception and attention toward affective stimuli yield mixed results. Since it is highly likely that primates’ emotional expressions share a common ancestry, it is not surprising that humans can differentiate between facial expressions of their own, and expressions of other primates (<xref ref-type="bibr" rid="R27">Kret, Muramatsu &amp; Matsuzawa, 2018</xref>; <xref ref-type="bibr" rid="R37">Parr, 2003</xref>). For example, affective stimuli depicting heterospecifics has shown to induce similar interpretation and attention bias, evoking comparable physiological reactions as equivalent stimuli depicting conspecifics (<xref ref-type="bibr" rid="R12">Dufour, Pascalis &amp; Petit, 2006</xref>; <xref ref-type="bibr" rid="R40">Parr, Hopkins &amp; de Waal, 1998</xref>; <xref ref-type="bibr" rid="R36">Parr, 2001</xref>; <xref ref-type="bibr" rid="R3">Berlo, 2022</xref>). On the other hand, multiple studies in human and non-human primates have confirmed better processing and understanding of information observed in a conspecific (<xref ref-type="bibr" rid="R16">Hattori, Kano &amp; Tomonaga 2010</xref>). Thus, it remains unclear to what extent affective states in other, evolutionary closely related but not domesticated species can be processed, and in which way those exert influence on one’s attention, perception, and potentially one’s emotional state.</p><p id="P5">Being generally more attentive towards affectively significant stimuli of evolutionary importance is highly beneficial since quickly reorientating visual attention and correctly evaluating such signals increases the chances of survival (<xref ref-type="bibr" rid="R51">Vuilleumier, 2005</xref>). Regardless of age, species, and the ability to consciously process and understand an affective expression, attentional biases towards emotional stimuli are overall a well-studied concept (<xref ref-type="bibr" rid="R49">van Rooijen, Ploeger &amp; Kret, 2017</xref>). In natural environments, especially in potentially threatening situations, spotting negative cues is more important to make a quick decision about the subsequent action, namely to <italic>fight</italic> or <italic>flight</italic> (<xref ref-type="bibr" rid="R14">Frijda, 2010</xref>). Therefore, attentional biases are often observed towards cues with a negative association (<xref ref-type="bibr" rid="R47">Vaish, Grossmann &amp; Woodward, 2008</xref>). Given their evolutionary role, threatening faces in visual search studies are detected faster with an increasing perception of “threat” (<xref ref-type="bibr" rid="R35">Ohman, Lundqvist &amp; Esteves, 2001</xref>; <xref ref-type="bibr" rid="R56">Wilson &amp; Tomonaga; 2021</xref>). Contradictory to this finding, <xref ref-type="bibr" rid="R46">Smith et al. (2006)</xref> argued that in environments where the benefits from positive events are outweighing the consequences from negative events, individuals might adapt to be more prone toward positive rather than negative information. This adaptation seems to be bidirectional (<xref ref-type="bibr" rid="R1">Aas er al., 2017</xref>; <xref ref-type="bibr" rid="R9">Dai et al., 2016</xref>). Put more generally, such shifts in attention towards particular valenced affective stimuli could potentially be partly traced back to the accumulated influences which are prevalent in an agent’s environment.</p><p id="P6">By applying semantic prime-target paradigms, the effects of prior context on the perception of a subsequent stimulus can be used to study the recognition and processing of emotional expressions (<xref ref-type="bibr" rid="R6">Carroll &amp; Young, 2005</xref>). Presenting a cue that is related to a subsequently shown target has been found to change the attention, reaction, and classification accuracy toward that target (<xref ref-type="bibr" rid="R18">Higgins, Bargh &amp; Lombardi, 1958</xref>). Once a priming stimulus is perceived, it is thought to trigger a cascade of associations in the receiver, facilitating and fastening the processing of related stimuli (<xref ref-type="bibr" rid="R31">Molden, 2014</xref>). Demonstrating this effect, <xref ref-type="bibr" rid="R46">Smith et al. (2006)</xref> found attentional bias in participants who were primed with positive and negative information to shift toward a stimulus that was congruent with the primer. Semantic priming has consistently been demonstrated to increase reaction times for non-matching targets and decrease reaction times for matching targets (<xref ref-type="bibr" rid="R30">McNamara, 2005</xref>). This further indicates a tendency of adaptation and modulation of bias based on the demands of an individual’s situation or environment (<xref ref-type="bibr" rid="R46">Smith et al., 2006</xref>), supporting the idea of <xref ref-type="bibr" rid="R13">Fredrickson’s (1998)</xref> broaden-and-build model. In his work, Fredrickson suggests that positive emotions would temporarily broaden an individual’s thought-action repertoire, promoting attention to related (here: positively valenced) cues. This theory was confirmed by <xref ref-type="bibr" rid="R52">Wadlinger and Isaacowitz (2006)</xref>, who found positive mood (induced by sugary food) to broaden visual attention to positive stimuli. <xref ref-type="bibr" rid="R6">Carroll and Young (2005)</xref> conducted several studies on affective priming with facial expressions, showing that facial expressions can likewise act as direct elicitors of affect, which is in line with facial displays being seen as tools for social influence (<xref ref-type="bibr" rid="R8">Crivelli &amp; Fridlund, 2018</xref>). Interestingly, <xref ref-type="bibr" rid="R6">Carroll and Young (2005)</xref> demonstrated that this priming effect on recognition can cross from nonverbal representations (pictures of emotions) to verbal representations (words) and vice versa. Given the discussed parallels between chimpanzee and human faces, along with the assumption of the ability to correctly classify facial expressions in heterospecifics, this bidirectional priming effect could apply to primers depicting different species’ facial expressions.</p><p id="P7">Comparing how differently valenced emotional primers of different species influence the attention toward emotional target stimuli will further improve our understanding of the mechanisms underlying the internal representations of affective processing. If primers of different species are understood correctly, then they would identically facilitate the accessibility of positive and negative constructs in memory, depending on the valence category of the primer, irrespective of the species. By triggering associated perceptual representations, an affective primer could potentially shift the observer’s mood, introducing biased perception towards stimuli related to the induced mood. Therefore, the perceived primer valence might influence the agent’s own internal state to the extent that it shifts the attentional biases towards a matching state. Supporting this assumption, previous work has confirmed mood to modulate attentional biases (<xref ref-type="bibr" rid="R43">Rowe, Hirsch &amp; Anderson, 2007</xref>). We aim to explore the functional consequences of facial expressions and broaden our knowledge of how such affective stimuli of different species are understood and how they impact attention-related processes in the viewer.</p></sec><sec id="S2"><title>Hypotheses</title><p id="P8"><bold>H<sub>1</sub>:</bold> Priming with emotionally valenced faces (positive and negative), compared to neutral faces of both species (i.e., humans and chimpanzees) introduces an attentional bias towards the emotion representations on the target screen.</p><p id="P9"><bold>H<sub>2</sub>:</bold> The direction of the attention shift depends on the congruency with the primer valence. Attention is shifted towards the stimulus on the target screen that is congruent with the valence previously presented in the primer picture.</p><p id="P10"><bold>H<sub>3</sub>:</bold> The elicited attentional bias is larger upon seeing pictures of conspecifics compared to seeing heterospecifics.</p></sec><sec id="S3" sec-type="methods"><title>Methods</title><sec id="S4" sec-type="subjects"><title>Participants</title><p id="P11">A total of 50 participants recruited at Leiden University took part in the eye-tracking experiment after filling in the informed consent. The participants were reimbursed with course credit (1 credit for 30 minutes of participation). The sample consisted of 30 women and 20 men with an average age of 26.5 years old (<italic>SD</italic> = 6.59). All participants had normal or corrected-to-normal vision and no history of clinically diagnosed psychiatric or neurological conditions. Data were collected in June 2022. The procedure and methods were approved by the Leiden University Ethics Committee (CEP: 2022-02-20-M.E. Kret-V1-3988).</p></sec><sec id="S5"><title>Stimulus material</title><p id="P12">The human face stimuli were taken from the validated Chicago Face Database (CFD), while the chimpanzee face stimuli were collected from different resources such as researcher’s archives, animal photographers’ work, and the iNaturalist webpage for uploading high quality pictures of different species, suitable for research purposes. The stimuli set that was used for the primer pictures contained 18 unique primer pictures for each of the six conditions, resulting in 108 primer pictures in total. No primer pictures were re-used as target stimuli.</p><p id="P13">Since each trial required an affiliative and an agonistic picture of a human for the target screen, our stimulus set for the targets consisted of (108 trials * 2 valences) 216 target pictures in total. We could have had 216 unique target pictures depicting human emotional expressions, however, due to the limited resources of emotional stimuli of chimpanzees, we were not able to entirely avoid repetition of the emotionally valenced target pictures in this group. The additional material was also taken from the Chicago Face Database. Thus, we added 27 extra pictures of positively and 27 extra pictures of negatively valenced human emotional expressions to our stimulus set which were repeated 4 times during the trials (27 pictures * 2 valences * 4 repetitions = 216). To account for this limitation, we made sure that 1) there is no overlap between the primer and one of the target pictures within the same trial 2) the target does not contain any picture from the previous and the next trial, and 3) the position of the target (left/right) regarding the emotional valence depicted, is pseudo-randomized and counterbalanced across trials and sessions. The colored pictures had a dimension of 420 x 320 pixels on a 1280 x 1024 computer display.</p><p id="P14">We selected stimuli of emotional expressions in humans and chimpanzees that appear to be well represented across these species. For the affiliative (positive) pictures of humans, we selected images that were labelled as “happy” in the CFD and for the agonistic (negative) pictures, we selected images that were labelled as “angry”. These emotional categories are found in other primates and equivalent facial expressions communicating these internal states have been observed in chimpanzees. Expressing an angry face for humans includes the activation of AU4, AU7, AU10, AU16, AU25, AU5, AU20, AU9 and AU26, whereas expressing a happy face includes the activation of AU12, AU7, AU26, AU6, AU10, AU1 and AU25 (Kohler et al., 2004). Hence, there are only three overlapping AUs among these human expressions, making them good models for studying emotional function. Nevertheless, relying purely on the AU activation for deducing similarities in expressions between species can be misleading. Entangling the activation of facial muscles in chimpanzees for expression production has shown that some identical AUs (i.e., AU10, AU16) were indeed active for chimpanzee agonistic faces as for human angry and fearful (agonistic) faces. However, finding homologous expressions in the prototypical chimpanzee facial expression repertoire is more challenging, because a related expression can communicate different affective signals. For instance, the AUs active in a human smile are overlapping with AUs in fear-grin and bared-teeth displays in chimpanzees (i.e., AU12, AU25) which occur predominantly in stressful situations (<xref ref-type="bibr" rid="R38">Parr &amp; Waller, 2006</xref>). This makes two facial expressions communicating contrasting signals in humans and chimpanzees, related to each other.</p><p id="P15">For the equivalent of a “happy” face in chimpanzees we chose to base our affiliative stimuli selection following the proposed parallels between human laughter and a non-human “play-face” (<xref ref-type="bibr" rid="R48">van Hooff, 1972</xref>). All AUs present in a chimpanzee play-face (AU12, AU25, AU26) are within the subset of a human happy-face (<xref ref-type="bibr" rid="R38">Parr &amp; Waller, 2006</xref>). For the selection of agonistic stimuli, we matched the “angry” face in humans with the bared-teeth and screaming displays in chimpanzees. Knowing that this could potentially be a source for interpretation mistakes in our participants and consequently lead to wrong priming effects, we compared their classification ratings (in valence and arousal of the seen stimuli) with the ratings of seven non-human primate experts and found no significant difference between the non-experts and experts (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Material I</xref>. Validation of the Stimulus Material). In addition, we confirmed that valence and arousal ratings of human and chimpanzee stimuli were in line with our expectations (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Material Figure 2</xref>).</p></sec><sec id="S6"><title>Calibration</title><p id="P16">Participants were calibrated using the 5-point automated calibration procedure in Tobii Pro Lab. Calibrations were accepted when the error displayed after finishing the calibration was minimal (less than one degree) and the data loss was less than 1%.</p></sec><sec id="S7"><title>Design and Procedure</title><p id="P17">Participants were actively recruited by the experimenter in the facilities of Leiden University. After reading the information letter for the study and signing the consent form, participants were individually tested in an eye-tracking lab at Leiden University. By signing the consent form they allowed to use their data for further analyses and publications.</p><p id="P18">We developed a within-subject design with three predictor conditions (affiliative, agonistic, neutral) of the priming factor and two valence levels (affiliative, agonistic) shown on the target screen. In the prime-target paradigm, positively and negatively valenced picture targets of emotional expressions were presented adjacently (4 seconds), preceded by a positive, negative, or neutral facial expression prime of either a human or chimpanzee face (2 seconds). Importantly, the target screen always depicted affiliative and agonistic human facial expressions. The order of the trials and the position of the positive and negative target images (left/right) were pseudo-randomized and counterbalanced across trials and sessions. Two trial sequences of different <italic>primer species</italic> conditions can be found in <xref ref-type="fig" rid="F1">Figure 1</xref>.</p><p id="P19">The experiment was run via Tobi Pro Lab (version 1.181.37603) on a Windows computer. After the participant sat down behind the computer, the experimenter started the 5-point calibration procedure (duration ~ 1 minute). Once the calibration was finished, the program proceeded to the eye-tracking trials. During the eye-tracking procedure, the participants had no active task to perform, but were asked to freely view the images on the computer screen. Their eye movements were measured via a Tobii Pro-Fusion eye-tracker attached to the monitor. A session started with a fixation cross that was shown for 1000ms. This was followed by a primer depicting the facial expression of a human or a chimpanzee (classified as either affiliative, agonistic, or neutral) that flashed up for 2000ms in the middle of the screen, then directly followed by a 4000ms target screen showing two emotionally valenced pictures depicting humans. The primer picture was spatially not overlapping with the position of the target pictures to prevent inaccuracies in the gaze fixation assessment. A trial ended with a blank screen that was shown for 3000ms. After the 9<sup>th</sup> session, a short break was programmed into the experiment, so that the participants could rest their eyes and look away for a couple of seconds. Once the second part was finished (108 trials in total), a Qualtrics questionnaire was opened remotely on the participant’s screen that first assessed the participants’ demographic information and then proceeded to the rating task. Participants were asked to rate all the primer pictures (108 in total) on two separate sliders, both on valence (negative to positive) and arousal (low to high). Since the primer pictures were only shown once, keeping the familiarization effects at its minimum compared to the target pictures, we decided to limit the rating to the sub-set of the primer pictures. The answers were coded on a 100-point scale with 50 indicating the neutral “zero-point” of the slider. Numbers smaller than 50 represented the rating in the negative/low spectrum and numbers larger than 50 represented the rating in the positive/high spectrum. Participants were given a debrief form explaining the background information and the goal of the study after they had finished the task, as well as the opportunity to ask remaining questions. The experiment took about 30-40 minutes to finish.</p></sec><sec id="S8"><title>Data Preparation</title><p id="P20">Before the analyses, we plotted the gaze data with the locations of the stimuli on the screen to check whether the raw fixation data matched with the areas of the stimuli on the screen. We drew a 430x320 square around each of the primer pictures and around each of the two simultaneously presented target pictures. Through Tobii Pro Lab’s <italic>Metrics</italic> option, we extracted the data on <italic>Total Fixation Duration</italic> per ROI using the Tobii Pro Lab Fixation Filter.</p></sec><sec id="S9"><title>Statistical Analyses</title><p id="P21">All data was analyzed in RStudio (v. 4.1.2). To answer our research questions and test whether emotional primers elicit attentional bias, we performed a multi-level analysis using Bayesian mixed modeling to analyse the total fixation duration. Our key question was whether fixations on the emotionally valenced targets were influenced by the previously seen primer emotion and species. Since the target screen depicted two facial expressions simultaneously, the looking durations toward the targets were highly correlated. Thus, we calculated our dependent variable from the proportional looking duration towards the positive target picture (based on Tobi Pro Lab’s <italic>Total Fixation Duration</italic> (TFD), from here on: bias score) using the following formula: <disp-formula id="FD1"><mml:math id="M1"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>D</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>D</mml:mi><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>F</mml:mi><mml:mi>D</mml:mi><mml:mtext> </mml:mtext><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mtext> </mml:mtext><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p id="P22">Since the bias score reflects the probability of looking at the positive picture, the “remaining” probability is the attentional bias towards the negative picture. Thus, there is no need of computing an extra negative bias score. The measure of the bias score higher than 0.5 indicates a longer fixation duration towards the positive emotional expression in the target screen. Hence, a bias score lower than 0.5 indicates a longer fixation duration towards the negative emotional expression.</p><p id="P23">To analyze the eye-tracking data, we used a zero-one inflated Bayesian beta regression model, which is suitable to analyze continuous proportions containing zeros and ones. These models consist of two components, namely a beta component to describe the values between 0 and 1, and a binary component to predict the occurrences of zeros and ones (Ospina &amp; Ferrari, 2021). For our measure of interest, proportional looking duration to emotional target stimuli (positivity bias) across trials, we ran a multilevel model analyzing the main effects, as well as the interaction between variables <italic>primer species (2)</italic> and <italic>primer emotion valence (3)</italic>. Since the dependency between scores from the same participant violates a key assumption of the linear regression, using a multilevel model was best suited to account for these correlations in the data. All variables of interest were dummy-coded and modeled as fixed effects. We allowed the intercept to vary by subject. Then, we computed the positivity bias and assessed whether these differed depending on emotion (agonistic, affiliative, or neutral primer) and the species (human or chimpanzee primer), or their interaction.</p><p id="P24">For the independent variables, we specified regularizing Gaussian priors with M = 0 and SD = 0.5. This also applied to the independent variables in the formulas for phi, coi, and zoi. For all variance parameters, we kept the default Student’s t priors with 3 degrees of freedom. Furthermore, we kept the default logistic priors for the intercepts of zoi and coi, and default Student’s t prior with 3 degrees of freedom for the Intercept of phi. After running the model, we used the emmeans-package (Lenth et al., 2018) to integrate the different model components and provide estimates based on the posterior predictive distribution. Using these values, we calculated multiple quantitative measures to describe the effects. The model notation including prior distributions can be found in the <xref ref-type="supplementary-material" rid="SD1">supplementary information</xref>.</p><p id="P25">For the interaction model, we report the medial estimate coefficients, the logit transformed regression coefficients, and the odds ratio coefficients together with the 95% credible interval (CI). In addition, we also report the probability of direction (pd), which indicates the certainty that an effect goes in a specific direction. All analyses were conducted using RStudio (v. 4.1.2) and the packages <italic>brms, emmeans</italic>, and <italic>ez</italic>.</p></sec></sec><sec id="S10" sec-type="results"><title>Results</title><p id="P26">The results show that firstly, looking duration (positivity bias) on the emotional target pictures is higher than 0.5. This effect was robust for primers depicting humans (<italic>Mdn</italic> = .510, 95% <italic>CI</italic> [0.501 - 0.518], <italic>pd</italic> = 99% ), as well as primers depicting chimpanzees (<italic>Mdn</italic> = .515, 95% <italic>CI</italic> [0.506 - 0.523], <italic>pd</italic> = 98%), meaning that all priming effects combined (positive, negative, neutral) led to human participants reliably looking longer at the affiliative compared to agonistic target pictures (see <xref ref-type="table" rid="T1">Table 1</xref>, Model 1).</p><p id="P27">We then separately investigated the specific emotion categories (<italic>primer valence</italic>), as well as their interaction effect with the <italic>primer species</italic>. In the model where we included <italic>primer valence</italic> as a factor (H1), we found robust evidence for increased positivity bias and hence, a decreased negativity bias when the participants were primed with an affiliative primer (<italic>Mdn</italic> = .545, 95% <italic>CI</italic> [0.535 - 0.555], <italic>pd</italic> = 100% ) compared to when the primer was either neutral or negative. Summarized across both species, there was no robust effect from seeing agonistic or neutral primers. Hence, we could confirm that averaged over species, only affiliative emotional primers introduce an attentional bias compared to neutral and agonistic primers (H1) (see <xref ref-type="table" rid="T1">Table 1</xref>, Model 2).</p><p id="P28">Examining the interaction between <italic>primer valence</italic> and <italic>primer species</italic> (H<sub>2</sub>, H<sub>3</sub>), we observed that attention shift towards the affiliative stimulus on the target screen was linked to presenting chimpanzee primers of affiliative nature (<italic>Mdn</italic> = .471, 95% <italic>CI</italic> [0.457- 0.484], <italic>pd</italic>= 1.00), as well as presenting human primers of affiliative nature (<italic>Mdn</italic> = .485, 95% <italic>CI</italic> [0.426 - 0.453], <italic>pd</italic>= 1.00. Furthermore, we found robust evidence for human participants looking longer at the agonistic stimulus in the target screen compared to the affiliative stimuli, given an agonistic human face primer (<italic>Mdn</italic> = .533, 95% <italic>CI</italic> [0.519- 0.548], <italic>pd</italic>= 1.00). The opposite effect was found for primers depicting agonistic chimpanzee faces (<italic>Mdn</italic> = .485, 95% <italic>CI</italic> [0.471- 0.499], <italic>pd</italic>= 0.98) (H<sub>3</sub>). This disparity also drives the main effect of agonistic valenced primers to being not robust. Neutral primers of both species did not introduce any reliable effect, thus, seeing a neutral primer did not cause a shift in the attention toward a positively or negatively valenced target picture. Entangling this interaction effect confirms that priming with emotionally valenced faces (positive and negative), compared to neutral faces of both species (i.e., humans and chimpanzees) introduces an attentional bias toward the emotion representations in the target screen (H<sub>1</sub>). Inspecting the main effect of valence, this result is not present due to agonistic primers of both species presumably having contrary effects and cancelling each other out.</p><p id="P29">Zooming in on the interaction effect, we compared the amount of positivity bias introduced by differently valenced emotional primers showing different species (Table 2). In addition we specified the model to estimate precision of the beta distribution, the zero-one inflation probability and the conditional one-inflation probability as a function of the positivity bias. We found the main effect of <italic>primer species</italic> to be significant, with seeing human primers leading to a decreased positivity bias compared to seeing chimpanzee primers (<italic>β</italic><sub>species_human</sub> = - .20, <italic>CI</italic> [-0.12 - (-0.28)], <italic>OR</italic> = 1.22). For the chimpanzee primers (reference category), there was no significant difference found in positivty bias regardless of the primer valence. This is confirmed by the overlapping credible intervals for the interaction effect between chimpanzee primer and the three valence levels (see <xref ref-type="table" rid="T1">Table 1</xref>, model 3 and <xref ref-type="fig" rid="F2">Figure 2</xref>).</p><p id="P30">Investigating the interactions in the model (Table 2) reveals whether the strength of the positivity bias introduced by the valence effect differs for seeing primers of different species. For chimpanzee primers, no significant mean simple effect of valence was found. However, the tendency of positivity biases shifts shows that upon seeing affiliative stimuli, the bias increased (<italic>β</italic><sub>valence_aff</sub> = .06), meaning that positively valenced stimuli influence the participant to look more prevalently to the affiliative target picture. Contrary to our expectations, neutral chimpanzee primers compared to agonistic chimpanzee primers are followed by an decreased positivity biases towards the target pictures (<italic>β</italic><sub>valence_neu</sub>= -.06). These simple effects are only reflecting tendencies and are not robust.</p><p id="P31">For the interactions comparing the simple effects of different valences between species, we found both effects to be significant. Comparing the neutrally valenced primers across species reveals that human neutral primers increased positivity bias more reliably than chimpanzee neutral primers (<italic>β</italic><sub>species_human:valence_neu</sub> = .19, <italic>CI</italic>[0.09 - 0.30], <italic>OR</italic> = 0.83). The main effect of neutrally human valenced primers is therefore <italic>β</italic><sub>H_valence_neu</sub> = .13 (-.06 + .19 = .13). Similarly, comparing positively valenced primers across species yielded a significant interaction effect (<italic>β</italic><sub>species_human:valence_aff</sub> = .31, <italic>CI</italic>[0.20- 0.41], <italic>OR</italic> = 0.74 ), showing that human affiliative primers reliably introduce positivity bias in the participants (compared to agonistic primers). The main effect on positivity bias upon seeing a positively valenced human primer is <italic>β</italic><sub>H_valence_aff</sub> = .37 (.06 + .31 = .37). An overview of the results is presented in <xref ref-type="table" rid="T2">Table 2</xref> and <xref ref-type="fig" rid="F2">Figure 2</xref>. The parameters of the beta distribution’s precision, the zero-one inflation and the conditional-one inflation are available in the <xref ref-type="supplementary-material" rid="SD1">Supplementary Material Table 1</xref>.</p></sec><sec id="S11" sec-type="discussion"><title>Discussion</title><p id="P32">Understanding the emotions of others is a crucially valuable skill for social animals to successfully master group interactions and to navigate in their environment (<xref ref-type="bibr" rid="R11">de Waal, 2011</xref>). The ability to recognize and distinguish facial expressions that communicate affective states helps to predict the subsequent action of an encountering individual and can be highly impactful on an individual’s survival (<xref ref-type="bibr" rid="R14">Frijda, 2010</xref>). Since also the perception of other species’ emotional displays can influence the chances of survival (<xref ref-type="bibr" rid="R20">Jacobs &amp; Vaske, 2019</xref>), we investigated to what extent emotion processing mechanisms in humans are transferable to accurately perceive and interpret the emotional expressions of our closest living relative, the chimpanzee. Introducing a prime-target paradigm, we compared how our participants’ attentional bias is influenced by emotional expressions of their own, as well as by the other species. As hypothesized, the attentional shifts occurred towards the targets that were congruent with the previously seen primer. Importantly, this effect was robust for humans viewing human emotional expressions, but partly contradictory and weaker for viewing chimpanzee emotional expressions. In our study, we confirmed that priming with emotional facial expressions introduces attentional biases toward emotional stimuli of conspecifics. Participants looked reliably longer at an emotional target stimulus that was congruent with the valence of the conspecific primer they saw before. Hence, a priorly presented affective stimulus depicting a human changed the amount of attention that participants allocated to afterward presented positive and negative visual information. Eventually, most likely due to their less pronounced relevance, neutral primers did not introduce this effect. The demonstrated priming effect elicited by positively and negatively connotated emotional displays confirms previous findings about the moderating role of valenced primers (<xref ref-type="bibr" rid="R46">Smith et al., 2006</xref>).</p><p id="P33">Contrary to our predictions, we did not find heterospecific (chimpanzee) primers to influence participants’ attention in a comparable way to conspecific (human) primers. This result is somewhat surprising, as previous research has demonstrated humans to be equally sensitive toward social cues from both species (<xref ref-type="bibr" rid="R16">Hattori, Kano, Tomonaga, 2010</xref>; <xref ref-type="bibr" rid="R27">Kret, Muramatsu &amp; Matsuzawa, 2018</xref>). However, testing functional implications, in the present study we did not find the attentional bias of chimpanzees to have a robust priming effect. In regard to the negatively and neutrally valenced chimpanzee primers, the tendency of attentional bias shifts was somewhat comparable to the shifts upon viewing primers depicting humans. Positively valenced primers increased positivity bias, whereas neutral primers did barely change the fixation duration towards the two emotional targets. The negatively valenced primers did not have a robust impact on attention.</p><p id="P34">To verify that the participants’ correctly encoded chimpanzees’ facial expressions, we analyzed their valence and arousal ratings for the priming stimuli of both species. We found that negative emotional expressions were rated accordingly with more negative valence scores, as well as positive emotional expressions were rated accordingly with more positive valence scores. Ratings in valence mirrored the depicted emotional expressions’ categories, irrespective of the species. Similarly, we found emotional stimuli (positive and negative) to induce higher arousal in the participants, compared to neutral stimuli. These findings are in line with previous studies on the emotional perception of different species (<xref ref-type="bibr" rid="R3">Berlo, 2022</xref>; <xref ref-type="bibr" rid="R27">Kret &amp; Matsuzawa, 2018</xref>) and confirm participants’ understanding and correct classification of chimpanzee emotional expressions. Furthermore, comparisons with ratings that were assigned to human primers show that the perceived valence and arousal of the emotional stimuli depicting different species are fairly similar. These data support a developed sensitivity for the perception and successful discrimination of emotional expressions in our close living relatives and possibly other related animals. This sensitivity might be contingent upon the extent of shared characteristics (<xref ref-type="bibr" rid="R39">Parr, Cohen &amp; de Waal, 2005</xref>; <xref ref-type="bibr" rid="R40">Parr, Hopkins &amp; de Waal, 1998</xref>). Arguably, close evolutionary relatedness is advantageous to demonstrate this skill, but not necessarily the only critical component. Humans being able to process and recognize e.g., dogs’ emotional states (<xref ref-type="bibr" rid="R24">Konok, Nagy &amp; Miklosi, 2015</xref>) shows that the individual accumulated experience with another species may act as an additional covariate for understanding emotions in heterospecifics. We were able to validate the rating results of the participants with the rating results by experienced primate social cognition experts.</p><p id="P35">Given the participants’ validated understanding of chimpanzees’ emotional expressions, the induction of comparable effects on i.e., attention bias should conceivably be feasible. However, our results yielded a tendency of a positivity bias increase (negativity bias decrease) upon viewing a negatively valenced chimpanzee primer. Zooming in on potential explanations for this contradictory observation, the selected chimpanzee stimuli need to be closer examined. For the stimulus sub-set communicating negative affective states in chimpanzees, we chose one of the most frequently observed facial expressions across non-human primates: the bared-teeth display (<xref ref-type="bibr" rid="R22">Kim &amp; Kret, 2022</xref>). Although in chimpanzees the expression predominantly occurs in agonistic interactions (<xref ref-type="bibr" rid="R53">Waller &amp; Dunbar, 2005</xref>), it can signal different affective states in other species. Moreover, it is far from being one-dimensional, since morphological variants of the bared-teeth display have been found to signal different information and yield different social interaction outcomes (<xref ref-type="bibr" rid="R7">Clark et al., 2020</xref>, <xref ref-type="bibr" rid="R29">Martin-Malivel et al., 2007</xref>). Important in this context is that the activation of AUs that highly overlap with AUs forming a bared-teeth display, can resemble a smiling face in humans (<xref ref-type="bibr" rid="R48">van Hooff, 1972</xref>; <xref ref-type="bibr" rid="R38">Parr &amp; Waller, 2006</xref>). Hence, some participants of the present study might have misinterpreted the negative affective state in chimpanzees for a smile (perhaps due to canine visibility) and evaluated it as a positive expression, which might have averaged out the expected effect. <xref ref-type="bibr" rid="R26">Kret and van Berlo (2021)</xref> found an identical leakage effect from one emotional category to another in children who perceived pictures of distressed bonobos rather positively than negatively. The study argued that children, as opposed to adults, have not yet learned to take contextual information into account and incorporate this for their interpretation of an emotional expression. In the current eye-tracking study, this misclassification of the bared-teeth display occurred in the prime-target paradigm where the stimulus was presented on the screen for 2 seconds, but not in the post-hoc rating questionnaire where participants had no time restriction for indicating the perceived valence and arousal of the viewed primers. From this mismatch in the participant’s emotion classification abilities, we can conclude that the exposure time to an affective stimulus might play a consequential role. Supporting this relation, <xref ref-type="bibr" rid="R28">Lohse and Overgaard (2019)</xref> found perceptual awareness of faces to increase gradually with duration of the presented stimuli. Thus, future studies investigating humans’ understanding of other animals’ emotions should take the time exposure aspect into account and eventually increase the presentation duration of heterospecific primer pictures.</p><p id="P36">One caveat of the current study that should be explored further is the categorization of different emotional expressions. Establishing salient categories and classifying human expressions has been a great challenge, with disagreement in the field. Since expressions in non-human primates have been studied far less, the disagreement in their categorization is even more pronounced, introducing increased variability in experimental designs. Quantifying the neural, physiological, and phenomenological organization of emotions in humans has produced a categorical structure of various emotions across different sources for arousal and brain activity (<xref ref-type="bibr" rid="R34">Nummenmaa &amp; Saarimaki, 2019</xref>). Similar neuroimaging experiments and multivariate pattern recognition analyses could be applied using chimpanzee emotional stimuli to incorporate the different domains of arousal for (proposed) distinct emotion categories. Comparing these activations for heterospecific and conspecific stimuli would potentially reveal homologies in emotion expression, perception, and processing in different species. In the present study, we chose two opposing valences to test the effects of priming on attention, however, even with ecologically validated stimuli, we presumably encountered partial leaking from one category to another. Studying a larger variety of emotional expressions in non-human primates might arguably first increase such overlaps of categories but in the long-term help to distinguish salient cues and benefit the more precise identification of facial expressions. Possibly, instead of the distinct categorization into exclusive categories, future research could follow the approach of assigning animal emotions on a continuum, exploring dynamic facial movements and their communicative signals (<xref ref-type="bibr" rid="R19">Jack, Garrod &amp; Schyns, 2014</xref>). Further regarding stimuli selection, follow-up studies could investigate affective state induction using dynamic stimuli. In the current study, we used static pictures of facial expressions in humans and chimpanzees which might have limited the participants’ ability to recognize an affective state correctly. Previous investigations have shown that moving faces are generally better recognized compared to still faces (<xref ref-type="bibr" rid="R23">Knight &amp; Johnson, 1997</xref>) since a dynamic scene provides more communicative signals and includes more contextual cues. Overall, an increased amount of information facilitates the perception of an emotional expression (<xref ref-type="bibr" rid="R37">Parr, 2003</xref>; <xref ref-type="bibr" rid="R44">Seyfarth, Cheney &amp; Marler, 1980</xref>) and hence might induce a stronger priming effect (<xref ref-type="bibr" rid="R15">Garrido-Vasquez et al. (2018)</xref>.</p><p id="P37">In addition to the methodological concern about selecting stimuli depicting equivalent facial expressions in different species, our study faces another constraint regarding the match between chimpanzee and human primers. While all chimpanzee photographs were taken opportunistically in a naturalistic setting, the human photographs as a subset of the Chicago Face Database were showing acted emotions in a highly standardized environment. This introduces an additional disbalance to the comparison of the presented stimuli and limits the experimental control, which might have influenced the perception of emotions, as previous studies have demonstrated (<xref ref-type="bibr" rid="R17">Hideg &amp; van Kleef, 2017</xref>). While it would have been feasible to incorporate human stimuli from naturalistic contexts, we deliberately abstained from doing so due to its limited exploration within the current body of literature. Our intention was to extend upon prior research efforts. Tackling the standardization problem from another side, recent studies have started creating morphologically altered, hyper-realistic facial expressions in non-human primates for studies on visual processing and social cognition (<xref ref-type="bibr" rid="R32">Murphy &amp; Leopold, 2019</xref>; <xref ref-type="bibr" rid="R57">Wilson et al., 2019</xref>). Using advanced high-tech software programs, some teams have been able to produce highly realistic scenes depicting various facial and body movements in monkeys and great apes, overcoming the uncanny valley (<xref ref-type="bibr" rid="R45">Siebert et al., 2020</xref>; <xref ref-type="bibr" rid="R21">Kawaguchi, 2021</xref>).</p><p id="P38">Based on the parallels between humans and non-human primates that since Darwin’s pioneering work have only unfolded further, it can be assumed that there is an evolutionary continuity in the emotional behaviors and their processing in humans and non-human primates (<xref ref-type="bibr" rid="R22">Kim &amp; Kret, 2019</xref>; <xref ref-type="bibr" rid="R25">Kret et al., 2020</xref>). Aiming to find homologies in human processing of facial expressions in their own species, and in an evolutionary closely related species, we tested the influences on attention introduced by priming with differently valenced emotional stimuli depicting humans and chimpanzees. Attention was shifted toward the emotional target picture that was congruent with the valence of the conspecific emotional expression depicted in the primer picture. Contrary to our expectations, we did not find this effect to occur with chimpanzee primers. Additional cross-species systematic investigations with slight adjustments are needed to fully address the gap of a shared evolutionary ancestry, and ultimately rule out the idea of emotions being unique to humans.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS189944-supplement-Supplementary_Material.docx" mimetype="application" mime-subtype="vnd.openxmlformats-officedocument.wordprocessingml.document" id="d103aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S12"><title>Acknowledgments</title><p>We would like to thank Roy de Kleijn, Tom Wilderjans, Elio Sjak-Shie, Lukas Kunz, Alessandro Palumbo, Juan Olvido Perea-Garcia, Evert Dekker, Yena Kim, and Tristan Matsulevits for their valuable feedback and discussions on the methods used in this work. This research is supported by the European Research Council (ERC) (Starting Grant #804582) awarded to M.E.K.</p></ack><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aas</surname><given-names>M</given-names></name><name><surname>Kauppi</surname><given-names>K</given-names></name><name><surname>Brandt</surname><given-names>CL</given-names></name><name><surname>Tesli</surname><given-names>M</given-names></name><name><surname>Kaufmann</surname><given-names>T</given-names></name><name><surname>Steen</surname><given-names>NE</given-names></name><etal/><name><surname>Melle</surname><given-names>I</given-names></name></person-group><article-title>Childhood trauma is associated with increased brain responses to emotionally negative as compared with positive faces in patients with psychotic disorders</article-title><source>Psychological medicine</source><year>2017</year><volume>47</volume><issue>4</issue><fpage>669</fpage><lpage>679</lpage><pub-id pub-id-type="pmid">27834153</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>DJ</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name></person-group><article-title>A framework for studying emotions across species</article-title><source>Cell</source><year>2014</year><volume>157</volume><issue>1</issue><fpage>187</fpage><lpage>200</lpage><pub-id pub-id-type="pmcid">PMC4098837</pub-id><pub-id pub-id-type="pmid">24679535</pub-id><pub-id pub-id-type="doi">10.1016/j.cell.2014.03.003</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berlo</surname><given-names>EV</given-names></name></person-group><source>Emotions through the eyes of our closest living relatives: exploring attentional and behavioral mechanisms</source><publisher-name>Doctoral dissertation, Leiden University</publisher-name><year>2022</year></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bourjade</surname><given-names>M</given-names></name></person-group><article-title>Social attention</article-title><source>The international encyclopedia of primatology</source><year>2016</year><fpage>1</fpage><lpage>2</lpage></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burrows</surname><given-names>AM</given-names></name><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Durham</surname><given-names>EL</given-names></name><name><surname>Matthews</surname><given-names>LC</given-names></name><name><surname>Smith</surname><given-names>TD</given-names></name></person-group><article-title>Human faces are slower than chimpanzee faces</article-title><source>PLoS One</source><year>2014</year><volume>9</volume><issue>10</issue><elocation-id>e110523</elocation-id><pub-id pub-id-type="pmcid">PMC4206419</pub-id><pub-id pub-id-type="pmid">25338058</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0110523</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carroll</surname><given-names>NC</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><article-title>Priming of emotion recognition</article-title><source>The Quarterly Journal of Experimental Psychology Section A</source><year>2005</year><volume>58</volume><issue>7</issue><fpage>1173</fpage><lpage>1197</lpage><pub-id pub-id-type="pmid">16194954</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>PR</given-names></name><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Burrows</surname><given-names>AM</given-names></name><name><surname>Julle Danière</surname><given-names>E</given-names></name><name><surname>Agil</surname><given-names>M</given-names></name><name><surname>Engelhardt</surname><given-names>A</given-names></name><name><surname>Micheletta</surname><given-names>J</given-names></name></person-group><article-title>Morphological variants of silent bared teeth displays have different social interaction outcomes in crested macaques (Macaca nigra)</article-title><source>American Journal of Physical Anthropology</source><year>2020</year><volume>173</volume><issue>3</issue><fpage>411</fpage><lpage>422</lpage><pub-id pub-id-type="pmid">32820559</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crivelli</surname><given-names>C</given-names></name><name><surname>Fridlund</surname><given-names>AJ</given-names></name></person-group><article-title>Facial displays are tools for social influence</article-title><source>Trends in Cognitive Sciences</source><year>2018</year><volume>22</volume><issue>5</issue><fpage>388</fpage><lpage>399</lpage><pub-id pub-id-type="pmid">29544997</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>Q</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Shu</surname><given-names>X</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name></person-group><article-title>Negativity bias for sad faces in depression: An event-related potential study</article-title><source>Clinical Neurophysiology</source><year>2016</year><volume>127</volume><issue>12</issue><fpage>3552</fpage><lpage>3560</lpage><pub-id pub-id-type="pmid">27833064</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C</given-names></name><name><surname>Prodger</surname><given-names>P</given-names></name></person-group><source>The expression of the emotions in man and animals</source><publisher-name>Oxford University Press</publisher-name><publisher-loc>USA</publisher-loc><year>1998</year></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Waal</surname><given-names>FB</given-names></name></person-group><article-title>What is an animal emotion?</article-title><source>Annals of the New York Academy of Sciences</source><year>2011</year><volume>1224</volume><issue>1</issue><fpage>191</fpage><lpage>206</lpage><pub-id pub-id-type="pmid">21486301</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dufour</surname><given-names>V</given-names></name><name><surname>Pascalis</surname><given-names>O</given-names></name><name><surname>Petit</surname><given-names>O</given-names></name></person-group><article-title>Face processing limitation to own species in primates: a comparative study in brown capuchins, Tonkean macaques and humans</article-title><source>Behavioural Processes</source><year>2006</year><volume>73</volume><issue>1</issue><fpage>107</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">16690230</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fredrickson</surname><given-names>BL</given-names></name></person-group><article-title>What good are positive emotions?</article-title><source>Review of general psychology</source><year>1998</year><volume>2</volume><issue>3</issue><fpage>300</fpage><lpage>319</lpage><pub-id pub-id-type="pmcid">PMC3156001</pub-id><pub-id pub-id-type="pmid">21850154</pub-id><pub-id pub-id-type="doi">10.1037/1089-2680.2.3.300</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frijda</surname><given-names>NH</given-names></name></person-group><article-title>Impulsive action and motivation</article-title><source>Biological psychology</source><year>2010</year><volume>84</volume><issue>3</issue><fpage>570</fpage><lpage>579</lpage><pub-id pub-id-type="pmid">20064583</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido-Vásquez</surname><given-names>P</given-names></name><name><surname>Pell</surname><given-names>MD</given-names></name><name><surname>Paulmann</surname><given-names>S</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>Dynamic facial expressions prime the processing of emotional prosody</article-title><source>Frontiers in human neuroscience</source><year>2018</year><volume>12</volume><fpage>244</fpage><pub-id pub-id-type="pmcid">PMC6007283</pub-id><pub-id pub-id-type="pmid">29946247</pub-id><pub-id pub-id-type="doi">10.3389/fnhum.2018.00244</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hattori</surname><given-names>Y</given-names></name><name><surname>Kano</surname><given-names>F</given-names></name><name><surname>Tomonaga</surname><given-names>M</given-names></name></person-group><article-title>Differential sensitivity to conspecific and allospecific cues in chimpanzees and humans: a comparative eye-tracking study</article-title><source>Biology Letters</source><year>2010</year><volume>6</volume><issue>5</issue><fpage>610</fpage><lpage>613</lpage><pub-id pub-id-type="pmcid">PMC2936142</pub-id><pub-id pub-id-type="pmid">20335197</pub-id><pub-id pub-id-type="doi">10.1098/rsbl.2010.0120</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hideg</surname><given-names>I</given-names></name><name><surname>van Kleef</surname><given-names>GA</given-names></name></person-group><article-title>When expressions of fake emotions elicit negative reactions: The role of observers’ dialectical thinking</article-title><source>Journal of Organizational Behavior</source><year>2017</year><volume>38</volume><issue>8</issue><fpage>1196</fpage><lpage>1212</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>ET</given-names></name><name><surname>Bargh</surname><given-names>JA</given-names></name><name><surname>Lombardi</surname><given-names>WJ</given-names></name></person-group><article-title>Nature of priming effects on categorization</article-title><source>Journal of experimental psychology: Learning, Memory, and Cognition</source><year>1985</year><volume>11</volume><issue>1</issue><fpage>59</fpage></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jack</surname><given-names>RE</given-names></name><name><surname>Garrod</surname><given-names>OG</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><article-title>Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time</article-title><source>Current biology</source><year>2014</year><volume>24</volume><issue>2</issue><fpage>187</fpage><lpage>192</lpage><pub-id pub-id-type="pmid">24388852</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>MAARTEN</given-names></name><name><surname>Vaske</surname><given-names>JJ</given-names></name></person-group><article-title>Understanding emotions as opportunities for and barriers to coexistence with wildlife</article-title><source>Human–wildlife interactions: Turning conflict into coexistence</source><year>2019</year><fpage>65</fpage><lpage>84</lpage></element-citation></ref><ref id="R21"><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Kawaguchi</surname><given-names>Y</given-names></name></person-group><source>Recognition of infant faces in great apes</source><year>2021</year></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Kret</surname><given-names>M</given-names></name></person-group><article-title>The emotional expressions and emotion perception in nonhuman primates</article-title><source>The Oxford Handbook of Emotional Development</source><year>2022</year><fpage>129</fpage></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knight</surname><given-names>B</given-names></name><name><surname>Johnston</surname><given-names>A</given-names></name></person-group><article-title>The role of movement in face recognition</article-title><source>Visual cognition</source><year>1997</year><volume>4</volume><issue>3</issue><fpage>265</fpage><lpage>273</lpage></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konok</surname><given-names>V</given-names></name><name><surname>Nagy</surname><given-names>K</given-names></name><name><surname>Miklósi</surname><given-names>Á</given-names></name></person-group><article-title>How do humans represent the emotions of dogs? The resemblance between the human representation of the canine and the human affective space</article-title><source>Applied Animal Behaviour Science</source><year>2015</year><volume>162</volume><fpage>37</fpage><lpage>46</lpage></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Prochazkova</surname><given-names>E</given-names></name><name><surname>Sterck</surname><given-names>EH</given-names></name><name><surname>Clay</surname><given-names>Z</given-names></name></person-group><article-title>Emotional expressions in human and non-human great apes</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2020</year><volume>115</volume><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="pmid">31991191</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>van Berlo</surname><given-names>E</given-names></name></person-group><article-title>Attentional bias in humans toward human and bonobo expressions of emotion</article-title><source>Evolutionary Psychology</source><year>2021</year><volume>19</volume><issue>3</issue><elocation-id>14747049211032816</elocation-id><pub-id pub-id-type="pmcid">PMC10358346</pub-id><pub-id pub-id-type="pmid">34318723</pub-id><pub-id pub-id-type="doi">10.1177/14747049211032816</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kret</surname><given-names>ME</given-names></name><name><surname>Muramatsu</surname><given-names>A</given-names></name><name><surname>Matsuzawa</surname><given-names>T</given-names></name></person-group><article-title>Emotion processing across and within species: A comparison between humans (Homo sapiens) and chimpanzees (Pan troglodytes)</article-title><source>Journal of Comparative Psychology</source><year>2018</year><volume>132</volume><issue>4</issue><fpage>395</fpage><pub-id pub-id-type="pmid">30024235</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lohse</surname><given-names>M</given-names></name><name><surname>Overgaard</surname><given-names>M</given-names></name></person-group><article-title>Emotional priming depends on the degree of conscious experience</article-title><source>Neuropsychologia</source><year>2019</year><volume>128</volume><fpage>96</fpage><lpage>102</lpage><pub-id pub-id-type="pmcid">PMC6562235</pub-id><pub-id pub-id-type="pmid">29129593</pub-id><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2017.10.028</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin-Malivel</surname><given-names>J</given-names></name><name><surname>Okada</surname><given-names>K</given-names></name></person-group><article-title>Human and chimpanzee face recognition in chimpanzees (Pan troglodytes): role of exposure and impact on categorical perception</article-title><source>Behavioral Neuroscience</source><year>2007</year><volume>121</volume><issue>6</issue><fpage>1145</fpage><pub-id pub-id-type="pmid">18085867</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>TP</given-names></name></person-group><source>Semantic priming: Perspectives from memory and word recognition</source><publisher-name>Psychology Press</publisher-name><year>2005</year></element-citation></ref><ref id="R31"><element-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Molden</surname><given-names>DC</given-names></name></person-group><source>Understanding priming effects in social psychology</source><publisher-name>Guilford Publications</publisher-name><year>2014</year></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>AP</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name></person-group><article-title>A parameterized digital 3D model of the Rhesus macaque face for investigating the visual processing of social cues</article-title><source>Journal of neuroscience methods</source><year>2019</year><volume>324</volume><elocation-id>108309</elocation-id><pub-id pub-id-type="pmcid">PMC7446874</pub-id><pub-id pub-id-type="pmid">31229584</pub-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.06.001</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwburg</surname><given-names>EG</given-names></name><name><surname>Ploeger</surname><given-names>A</given-names></name><name><surname>Kret</surname><given-names>ME</given-names></name></person-group><article-title>Emotion recognition in nonhuman primates: How experimental research can contribute to a better understanding of underlying mechanisms</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2021</year><volume>123</volume><fpage>24</fpage><lpage>47</lpage><pub-id pub-id-type="pmid">33453306</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nummenmaa</surname><given-names>L</given-names></name><name><surname>Saarimäki</surname><given-names>H</given-names></name></person-group><article-title>Emotions as discrete patterns of systemic activity</article-title><source>Neuroscience letters</source><year>2019</year><volume>693</volume><fpage>3</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">28705730</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ohman</surname><given-names>A</given-names></name><name><surname>Lundqvist</surname><given-names>D</given-names></name><name><surname>Esteves</surname><given-names>F</given-names></name></person-group><article-title>The face in the crowd revisited: a threat advantage with schematic stimuli</article-title><source>Journal of personality and social psychology</source><year>2001</year><volume>80</volume><issue>3</issue><fpage>381</fpage><pub-id pub-id-type="pmid">11300573</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name></person-group><article-title>Cognitive and physiological markers of emotional awareness in chimpanzees (Pan troglodytes)</article-title><source>Animal Cognition</source><year>2001</year><volume>4</volume><issue>3</issue><fpage>223</fpage><lpage>229</lpage><pub-id pub-id-type="pmid">24777512</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name></person-group><article-title>The discrimination of faces and their emotional content by chimpanzees (Pan troglodytes)</article-title><source>Annals of the New York Academy of Sciences</source><year>2003</year><volume>1000</volume><issue>1</issue><fpage>56</fpage><lpage>78</lpage><pub-id pub-id-type="pmid">14766620</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Waller</surname><given-names>BM</given-names></name></person-group><article-title>Understanding chimpanzee facial expression: insights into the evolution of communication</article-title><source>Social cognitive and affective neuroscience</source><year>2006</year><volume>1</volume><issue>3</issue><fpage>221</fpage><lpage>228</lpage><pub-id pub-id-type="pmcid">PMC2555422</pub-id><pub-id pub-id-type="pmid">18985109</pub-id><pub-id pub-id-type="doi">10.1093/scan/nsl031</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Cohen</surname><given-names>M</given-names></name><name><surname>Waal</surname><given-names>FD</given-names></name></person-group><article-title>Influence of social context on the use of blended and graded facial displays in chimpanzees</article-title><source>International Journal of Primatology</source><year>2005</year><volume>26</volume><issue>1</issue><fpage>73</fpage><lpage>103</lpage></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Hopkins</surname><given-names>WD</given-names></name><name><surname>de Waal</surname><given-names>FB</given-names></name></person-group><article-title>The perception of facial expressions by chimpanzees, Pan troglodytes</article-title><source>Evolution of Communication</source><year>1998</year><volume>2</volume><issue>1</issue><fpage>1</fpage><lpage>23</lpage></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Vick</surname><given-names>SJ</given-names></name><name><surname>Bard</surname><given-names>KA</given-names></name></person-group><article-title>Classifying chimpanzee facial expressions using muscle action</article-title><source>Emotion</source><year>2007</year><volume>7</volume><issue>1</issue><fpage>172</fpage><pub-id pub-id-type="pmcid">PMC2826116</pub-id><pub-id pub-id-type="pmid">17352572</pub-id><pub-id pub-id-type="doi">10.1037/1528-3542.7.1.172</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollick</surname><given-names>AS</given-names></name><name><surname>De Waal</surname><given-names>FB</given-names></name></person-group><article-title>Ape gestures and language evolution</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><issue>19</issue><fpage>8184</fpage><lpage>8189</lpage><pub-id pub-id-type="pmcid">PMC1876592</pub-id><pub-id pub-id-type="pmid">17470779</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0702624104</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rowe</surname><given-names>G</given-names></name><name><surname>Hirsh</surname><given-names>JB</given-names></name><name><surname>Anderson</surname><given-names>AK</given-names></name></person-group><article-title>Positive affect increases the breadth of attentional selection</article-title><source>Proceedings of the National Academy of Sciences</source><year>2007</year><volume>104</volume><issue>1</issue><fpage>383</fpage><lpage>388</lpage><pub-id pub-id-type="pmcid">PMC1765470</pub-id><pub-id pub-id-type="pmid">17182749</pub-id><pub-id pub-id-type="doi">10.1073/pnas.0605198104</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seyfarth</surname><given-names>RM</given-names></name><name><surname>Cheney</surname><given-names>DL</given-names></name><name><surname>Marler</surname><given-names>P</given-names></name></person-group><article-title>Monkey responses to three different alarm calls: evidence of predator classification and semantic communication</article-title><source>Science</source><year>1980</year><volume>210</volume><issue>4471</issue><fpage>801</fpage><lpage>803</lpage><pub-id pub-id-type="pmid">7433999</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siebert</surname><given-names>R</given-names></name><name><surname>Taubert</surname><given-names>N</given-names></name><name><surname>Spadacenta</surname><given-names>S</given-names></name><name><surname>Dicke</surname><given-names>PW</given-names></name><name><surname>Giese</surname><given-names>MA</given-names></name><name><surname>Thier</surname><given-names>P</given-names></name></person-group><article-title>A naturalistic dynamic monkey head avatar elicits species-typical reactions and overcomes the uncanny valley</article-title><source>Eneuro</source><year>2020</year><volume>7</volume><issue>4</issue><pub-id pub-id-type="pmcid">PMC7340843</pub-id><pub-id pub-id-type="pmid">32513660</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0524-19.2020</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>NK</given-names></name><name><surname>Larsen</surname><given-names>JT</given-names></name><name><surname>Chartrand</surname><given-names>TL</given-names></name><name><surname>Cacioppo</surname><given-names>JT</given-names></name><name><surname>Katafiasz</surname><given-names>HA</given-names></name><name><surname>Moran</surname><given-names>KE</given-names></name></person-group><article-title>Being bad isn’t always good: affective context moderates the attention bias toward negative information</article-title><source>Journal of personality and social psychology</source><year>2006</year><volume>90</volume><issue>2</issue><fpage>210</fpage><pub-id pub-id-type="pmid">16536647</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaish</surname><given-names>A</given-names></name><name><surname>Grossmann</surname><given-names>T</given-names></name><name><surname>Woodward</surname><given-names>A</given-names></name></person-group><article-title>Not all emotions are created equal: the negativity bias in social-emotional development</article-title><source>Psychological bulletin</source><year>2008</year><volume>134</volume><issue>3</issue><fpage>383</fpage><pub-id pub-id-type="pmcid">PMC3652533</pub-id><pub-id pub-id-type="pmid">18444702</pub-id><pub-id pub-id-type="doi">10.1037/0033-2909.134.3.383</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Hooff</surname><given-names>JARAM</given-names></name></person-group><article-title>A comparative approach to the phylogeny of laughter and smiling</article-title><source>Nonverbal communication</source><year>1972</year><fpage>209</fpage><lpage>241</lpage></element-citation></ref><ref id="R49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Rooijen</surname><given-names>R</given-names></name><name><surname>Ploeger</surname><given-names>A</given-names></name><name><surname>Kret</surname><given-names>ME</given-names></name></person-group><article-title>The dot-probe task to measure emotional attention: A suitable measure in comparative studies?</article-title><source>Psychonomic bulletin &amp; review</source><year>2017</year><volume>24</volume><issue>6</issue><fpage>1686</fpage><lpage>1717</lpage><pub-id pub-id-type="pmid">28092078</pub-id></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vick</surname><given-names>SJ</given-names></name><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Smith Pasqualini</surname><given-names>MC</given-names></name><name><surname>Bard</surname><given-names>KA</given-names></name></person-group><article-title>A cross-species comparison of facial morphology and movement in humans and chimpanzees using the facial action coding system (FACS)</article-title><source>Journal of nonverbal behavior</source><year>2007</year><volume>31</volume><issue>1</issue><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="pmcid">PMC3008553</pub-id><pub-id pub-id-type="pmid">21188285</pub-id><pub-id pub-id-type="doi">10.1007/s10919-006-0017-z</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><article-title>How brains beware: neural mechanisms of emotional attention</article-title><source>Trends in cognitive sciences</source><year>2005</year><volume>9</volume><issue>12</issue><fpage>585</fpage><lpage>594</lpage><pub-id pub-id-type="pmid">16289871</pub-id></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wadlinger</surname><given-names>HA</given-names></name><name><surname>Isaacowitz</surname><given-names>DM</given-names></name></person-group><article-title>Positive mood broadens visual attention to positive stimuli</article-title><source>Motivation and emotion</source><year>2006</year><volume>30</volume><issue>1</issue><fpage>87</fpage><lpage>99</lpage><pub-id pub-id-type="pmcid">PMC2860869</pub-id><pub-id pub-id-type="pmid">20431711</pub-id><pub-id pub-id-type="doi">10.1007/s11031-006-9021-1</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Dunbar</surname><given-names>RI</given-names></name></person-group><article-title>Differential behavioural effects of silent bared teeth display and relaxed open mouth display in chimpanzees (Pan troglodytes)</article-title><source>Ethology</source><year>2005</year><volume>111</volume><issue>2</issue><fpage>129</fpage><lpage>142</lpage></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Cray</surname><given-names>JJ</given-names><suffix>Jr</suffix></name><name><surname>Burrows</surname><given-names>AM</given-names></name></person-group><article-title>Selection for universal facial emotion</article-title><source>Emotion</source><year>2008</year><volume>8</volume><issue>3</issue><fpage>435</fpage><pub-id pub-id-type="pmid">18540761</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waller</surname><given-names>BM</given-names></name><name><surname>Vick</surname><given-names>SJ</given-names></name><name><surname>Parr</surname><given-names>LA</given-names></name><name><surname>Bard</surname><given-names>KA</given-names></name><name><surname>Pasqualini</surname><given-names>MCS</given-names></name><name><surname>Gothard</surname><given-names>KM</given-names></name><name><surname>Fuglevand</surname><given-names>AJ</given-names></name></person-group><article-title>Intramuscular electrical stimulation of facial muscles in humans and chimpanzees: Duchenne revisited and extended</article-title><source>Emotion</source><year>2006</year><volume>6</volume><issue>3</issue><fpage>367</fpage><pub-id pub-id-type="pmcid">PMC2826128</pub-id><pub-id pub-id-type="pmid">16938079</pub-id><pub-id pub-id-type="doi">10.1037/1528-3542.6.3.367</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Tomonaga</surname><given-names>M</given-names></name></person-group><article-title>Search asymmetries for threatening faces in chimpanzees (Pan troglodytes)</article-title><source>Journal of Comparative Psychology</source><year>2021</year><pub-id pub-id-type="pmid">34843313</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>VA</given-names></name><name><surname>Kade</surname><given-names>C</given-names></name><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Treue</surname><given-names>S</given-names></name><name><surname>Kagan</surname><given-names>I</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><article-title>Development of a monkey avatar to study social perception in macaques</article-title><source>bioRxiv</source><year>2019</year><elocation-id>758458</elocation-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p><italic>Note</italic>. Trial sequence. Each trial started with a fixation cross (1s), followed by the first screen with the stimulus of a primer (2s) and subsequently by the second screen with the two target stimuli presented next to each other (4s). The trial ended with a blank screen (3s).</p><p>*** The real stimuli photographs of human faces are replaced with emoji faces only for the present preprint version in order to meet the policies of <italic>biorxiv</italic>. ***</p></caption><graphic xlink:href="EMS189944-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p><italic>Note</italic>: Graphs displaying the proportional fixation duration (predicted model data) to emotional stimuli (positivity bias) of conspecifics and heterospecifics by human participants. Error bars reflect the 95% credible interval, dots represent the median. The measure of the positivity bias score higher than 0.5 indicates a longer fixation duration towards the positive emotional expression in the target screen. Hence, a positivity bias score lower than 0.5 indicates a longer fixation duration towards the negative emotional expression.</p></caption><graphic xlink:href="EMS189944-f002"/></fig><table-wrap id="T1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Overview of results per factor level of interest for the three models. Robust effects are in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top">Model</th><th align="center" valign="top">Primer Species</th><th align="center" valign="top">Primer Valence</th><th align="center" valign="top"><italic>Median</italic></th><th align="center" valign="top"><italic>CI 95%</italic></th><th align="center" valign="top"><italic>pd</italic></th></tr></thead><tbody><tr><td align="center" valign="middle" rowspan="2">1 (Primer Species)</td><td align="center" valign="top">Chimpanzee</td><td align="center" valign="top"><bold>All</bold></td><td align="center" valign="top"><bold>0.515</bold></td><td align="center" valign="top"><bold>[0.506 - 0.523]</bold></td><td align="center" valign="top"><bold>0.99</bold></td></tr><tr><td align="center" valign="top">Human</td><td align="center" valign="top"><bold>All</bold></td><td align="center" valign="top"><bold>0.510</bold></td><td align="center" valign="top"><bold>[0.501 - 0.518]</bold></td><td align="center" valign="top"><bold>0.98</bold></td></tr><tr style="border-top: solid thin"><td align="center" valign="middle" rowspan="3">2 (Valence)</td><td align="center" valign="top">All</td><td align="center" valign="top">Agonistic</td><td align="center" valign="top">0.491</td><td align="center" valign="top">[0.481 - 0.502]</td><td align="center" valign="top">0.95</td></tr><tr><td align="center" valign="top">All</td><td align="center" valign="top"><bold>Affiliative</bold></td><td align="center" valign="top"><bold>0.545</bold></td><td align="center" valign="top"><bold>[0.535 - 0.555]</bold></td><td align="center" valign="top"><bold>1.00</bold></td></tr><tr><td align="center" valign="top">All</td><td align="center" valign="top">Neutral</td><td align="center" valign="top">0.501</td><td align="center" valign="top">[0.490 - 0.511]</td><td align="center" valign="top">0.54</td></tr><tr style="border-top: solid thin"><td align="center" valign="middle" rowspan="6">3 (Primer Species*Primer Valence)</td><td align="center" valign="top" rowspan="3">Chimpanzee</td><td align="center" valign="top"><bold>Agonistic</bold></td><td align="center" valign="top"><bold>0.515</bold></td><td align="center" valign="top"><bold>[0.501- 0.529]</bold></td><td align="center" valign="top"><bold>0.98</bold></td></tr><tr><td align="center" valign="top"><bold>Affiliative</bold></td><td align="center" valign="top"><bold>0.529</bold></td><td align="center" valign="top"><bold>[0.516- 0.543]</bold></td><td align="center" valign="top"><bold>1.00</bold></td></tr><tr><td align="center" valign="top">Neutral</td><td align="center" valign="top">0.499</td><td align="center" valign="top">[0.486- 0.513]</td><td align="center" valign="top">0.54</td></tr><tr><td align="center" valign="top" rowspan="3">Human</td><td align="center" valign="top"><bold>Agonistic</bold></td><td align="center" valign="top"><bold>0.467</bold></td><td align="center" valign="top"><bold>[0.452- 0.481]</bold></td><td align="center" valign="top"><bold>1.00</bold></td></tr><tr><td align="center" valign="top"><bold>Affiliative</bold></td><td align="center" valign="top"><bold>0.560</bold></td><td align="center" valign="top"><bold>[0.547 - 0.574]</bold></td><td align="center" valign="top"><bold>1.00</bold></td></tr><tr><td align="center" valign="top">Neutral</td><td align="center" valign="top">0.502</td><td align="center" valign="top">[0.488- 0.516]</td><td align="center" valign="top">0.60</td></tr></tbody></table></table-wrap><table-wrap id="T2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Logit transformed regression coefficients (posterior mean, standard error, 95% credible intervals and Bayes factor) and odds ratios (OR) of the beta distribution (positivity bias as a function of the two AOIs in the target screen). Reference category: Chimpanzee/agonistic.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="top"/><th align="center" valign="top">Estimate</th><th align="center" valign="top">Estimate Error</th><th align="center" valign="top">OR</th><th align="center" valign="top">Q2.5</th><th align="center" valign="top">Q97.5</th></tr></thead><tbody><tr><td align="left" valign="top"><italic>Group-level effects</italic></td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Participant (sd)</td><td align="center" valign="top">0.13</td><td align="center" valign="top">0.02</td><td align="center" valign="top">1.13</td><td align="center" valign="top">0.17</td><td align="center" valign="top">0.09</td></tr><tr><td align="left" valign="top"><italic>Population-level effects</italic></td><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/><td align="center" valign="top"/></tr><tr><td align="left" valign="top">Intercept</td><td align="center" valign="top">0.06</td><td align="center" valign="top">0.03</td><td align="center" valign="top">0.94</td><td align="center" valign="top">0.00</td><td align="center" valign="top">0.13</td></tr><tr><td align="left" valign="top"><italic>β</italic><sub>species_human</sub></td><td align="center" valign="top"><bold>-0.20</bold></td><td align="center" valign="top">0.04</td><td align="center" valign="top">1.22</td><td align="center" valign="top">-0.28</td><td align="center" valign="top">-0.12</td></tr><tr><td align="left" valign="top"><italic>β</italic><sub>valence_neu</sub></td><td align="center" valign="top">-0.06</td><td align="center" valign="top">0.04</td><td align="center" valign="top">1.06</td><td align="center" valign="top">-0.13</td><td align="center" valign="top">0.02</td></tr><tr><td align="left" valign="top"><italic>β</italic><sub>valence_aff</sub></td><td align="center" valign="top">0.06</td><td align="center" valign="top">0.04</td><td align="center" valign="top">0.94</td><td align="center" valign="top">-0.01</td><td align="center" valign="top">0.14</td></tr><tr><td align="left" valign="top"><italic>β</italic><sub>species_human:valence_neu</sub></td><td align="center" valign="top"><bold>0.19</bold></td><td align="center" valign="top">0.05</td><td align="center" valign="top">0.83</td><td align="center" valign="top">0.09</td><td align="center" valign="top">0.30</td></tr><tr><td align="left" valign="top"><italic>β</italic><sub>species_human:valence_aff</sub></td><td align="center" valign="top"><bold>0.31</bold></td><td align="center" valign="top">0.05</td><td align="center" valign="top">0.74</td><td align="center" valign="top">0.20</td><td align="center" valign="top">0.41</td></tr></tbody></table></table-wrap></floats-group></article>