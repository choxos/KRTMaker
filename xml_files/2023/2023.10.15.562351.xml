<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="epub">2692-8205</issn></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189710</article-id><article-id pub-id-type="doi">10.1101/2023.10.15.562351</article-id><article-id pub-id-type="archive">PPR742470</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Cortical and behavioural tracking of rhythm in music: Effects of pitch predictability, enjoyment, and expertise</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Keitel</surname><given-names>Anne</given-names></name><xref ref-type="corresp" rid="CR1">*</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Pelofi</surname><given-names>Claire</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Guan</surname><given-names>Xinyi</given-names></name><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Watson</surname><given-names>Emily</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Wight</surname><given-names>Lucy</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A5">5</xref></contrib><contrib contrib-type="author"><name><surname>Allen</surname><given-names>Sarah</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Mencke</surname><given-names>Iris</given-names></name><xref ref-type="aff" rid="A6">6</xref><xref ref-type="aff" rid="A7">7</xref></contrib><contrib contrib-type="author"><name><surname>Keitel</surname><given-names>Christian</given-names></name><xref ref-type="fn" rid="FN1">+</xref><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>Rimmele</surname><given-names>Johanna</given-names></name><xref ref-type="fn" rid="FN1">+</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="aff" rid="A8">8</xref></contrib></contrib-group><aff id="A1"><label>1</label>Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/03h2bxq36</institution-id><institution>University of Dundee</institution></institution-wrap>, <country country="GB">UK</country></aff><aff id="A2"><label>2</label>Department of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap>, <country country="US">US</country></aff><aff id="A3"><label>3</label>Max Planck NYU Center for Language, Music, and Emotion, Frankfurt am Main, Germany, New York, NY, US</aff><aff id="A4"><label>4</label>Digital and Cognitive Musicology Lab, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap>, <city>Lausanne</city>, <country country="CH">Switzerland</country></aff><aff id="A5"><label>5</label>School of Psychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/05j0ve876</institution-id><institution>Aston University</institution></institution-wrap>, <city>Birmingham</city>, <country country="GB">UK</country></aff><aff id="A6"><label>6</label>Department of Medical Physics and Acoustics, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/033n9gh91</institution-id><institution>University of Oldenburg</institution></institution-wrap>, <city>Oldenburg</city>, <country country="DE">Germany</country></aff><aff id="A7"><label>7</label>Department of Music, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/000rdbk18</institution-id><institution>Max-Planck-Institute for Empirical Aesthetics</institution></institution-wrap>, <city>Frankfurt</city>, <country country="DE">Germany</country></aff><aff id="A8"><label>8</label>Department of Cognitive Neuropsychology, <institution-wrap><institution-id institution-id-type="ror">https://ror.org/000rdbk18</institution-id><institution>Max-Planck-Institute for Empirical Aesthetics</institution></institution-wrap>, <city>Frankfurt</city>, <country country="DE">Germany</country></aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author: Dr Anne Keitel, Psychology Division; School of Humanities, Social Sciences, and Law, University of Dundee, DD1 4HN, Dundee, UK</corresp><fn id="FN1"><label>+</label><p id="P1">indicates equal contributions from the two senior authors.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>18</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">The cortical tracking of stimulus features (such as the sound envelope) is a crucial neural requisite of how we process continuous music. We here tested whether cortical tracking of the beat, typically related to rhythm processing, is modulated by pitch predictability and other top-down factors. Participants listened to tonal (high pitch predictability) and atonal (low pitch predictability) music while undergoing EEG, and we analysed their cortical tracking of the acoustic envelope. Interestingly, cortical envelope tracking was stronger while listening to atonal than tonal music, likely reflecting listeners’ violated pitch expectations. Envelope tracking was also stronger with more expertise and enjoyment. Furthermore, we analysed cortical tracking of pitch surprisal (using IDyOM) and show that listeners’ expectations for tonal and atonal music match those computed by the IDyOM model, with higher surprisal (prediction errors) for atonal than tonal music. Behaviourally, we measured participants’ ability to tap along to the beat of tonal and atonal sequences in two experiments. In both experiments, finger-tapping performance was better in the tonal than the atonal condition, indicating a positive effect of pitch predictability on behavioural rhythm processing. Cortical envelope tracking predicted tapping performance for tonal music, as did pitch surprisal tracking for atonal music, indicating that conditions of high and low predictability might impose different processing regimes. We show that cortical envelope tracking, beyond reflecting musical rhythm processing, is modulated by pitch predictability, as well as musical expertise and enjoyment. Taken together, our results show various ways in which those top-down factors impact musical rhythm processing.</p></abstract><kwd-group><kwd>music perception</kwd><kwd>EEG</kwd><kwd>naturalistic music</kwd><kwd>pitch surprisal</kwd><kwd>musical expertise</kwd><kwd>top-down influences</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">The cortical tracking of continuous auditory stimuli, such as music and speech, has been the topic of intense investigation in the past years (<xref ref-type="bibr" rid="R39">Keitel <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="R71">Peelle <italic>et al</italic>., 2013</xref>; <xref ref-type="bibr" rid="R85">Tierney &amp; Kraus, 2015</xref>). Cortical tracking usually refers to the neural signal matching slow amplitude fluctuations in the acoustic signal and is quantified by neural alignment to the stimulus envelope, thought to reflect processing of the rhythmic structure (<xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>; <xref ref-type="bibr" rid="R29">Gross <italic>et al</italic>., 2013</xref>; <xref ref-type="bibr" rid="R52">Luo &amp; Poeppel, 2007</xref>). Although mostly investigated in speech, recent findings suggest that processing of naturalistic music might rely on comparable mechanisms (<xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>; <xref ref-type="bibr" rid="R79">Sammler, 2020</xref>; <xref ref-type="bibr" rid="R93">Zuk <italic>et al</italic>., 2021</xref>). Cortical tracking is influenced by numerous top-down factors, but their interaction and relative importance are poorly understood. For example, increased attention and listening effort generally leads to stronger speech tracking (<xref ref-type="bibr" rid="R19">Ding &amp; Simon, 2012</xref>; <xref ref-type="bibr" rid="R50">Lesenfants &amp; Francart, 2020</xref>; <xref ref-type="bibr" rid="R77">Rimmele <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="R84">Song &amp; Iverson, 2018</xref>; <xref ref-type="bibr" rid="R92">Zion Golumbic <italic>et al</italic>., 2013</xref>). Conversely, both speech and music tracking are enhanced with language and music proficiency and prior knowledge (<xref ref-type="bibr" rid="R8">Blanco-Elorrieta <italic>et al</italic>., 2020</xref>; <xref ref-type="bibr" rid="R12">Cervantes Constantino &amp; Simon, 2018</xref>; <xref ref-type="bibr" rid="R14">Di Liberto <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>; <xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>). Other factors, such as the influence of enjoyment on the cortical tracking of music, have also recently elicited researchers’ interest (<xref ref-type="bibr" rid="R87">Weineck <italic>et al</italic>., 2022</xref>). Overall, any study of cortical tracking of rhythmic stimuli, needs to take into account stimuli and listener characteristics, which is one driver of the present study.</p><p id="P4">Recent studies on the cortical tracking of music have shown that the auditory cortex tracks not only the acoustic envelope but also melodic expectations, modelled as surprisal values (<xref ref-type="bibr" rid="R2">Abrams <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R15">Di Liberto <italic>et al</italic>., 2020a</xref>; <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R53">Marion <italic>et al</italic>., 2021</xref>). These studies suggest that humans automatically process melodic expectations while listening to naturalistic, continuous stimuli (<xref ref-type="bibr" rid="R68">Pearce <italic>et al</italic>., 2010</xref>). Here, we examine the cortical tracking of pitch surprisal, using music stimuli with different levels of pitch predictability, namely tonal and atonal music excerpts. Music that is composed according to (Western) tonal principles, has an intrinsic hierarchical pitch organisation (<xref ref-type="bibr" rid="R49">Lerdahl, 2019</xref>). Therefore, this compositional style results in far more predictable pitch sequences than atonal music (<xref ref-type="bibr" rid="R55">Mencke <italic>et al</italic>., 2018</xref>), which is based on the compositional principle that all twelve tones within an octave are equiprobable. The few studies that have been conducted using atonal music show that the resulting lack of a hierarchical pitch organisation negatively affects memorisation (<xref ref-type="bibr" rid="R82">Schulze <italic>et al</italic>., 2011</xref>), recognition (<xref ref-type="bibr" rid="R13">Cuddy <italic>et al</italic>., 1981</xref>; <xref ref-type="bibr" rid="R17">Dibben, 1994</xref>; <xref ref-type="bibr" rid="R21">Dowling <italic>et al</italic>., 1995</xref>) and the strength of melodic expectations (<xref ref-type="bibr" rid="R62">Ockelford &amp; Sergeant, 2013</xref>) (for review, <xref ref-type="bibr" rid="R54">Mencke <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R55">Mencke <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="R86">Vuvan <italic>et al</italic>., 2014</xref>). Electrophysiological research suggests that weaker expectancies in atonal music particularly affect later attention-related processing stages (<xref ref-type="bibr" rid="R56">Mencke <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="R59">Neuloh &amp; Curio, 2004</xref>). Taken together, atonal music seems to present specific perceptual challenges to listeners, in particular related to melodic expectations.</p><p id="P5">In the context of musical rhythm perception, finger-tapping is often used as ‘behavioural tracking’ measure (<xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>; <xref ref-type="bibr" rid="R79">Sammler, 2020</xref>) to assess rhythm skills (<xref ref-type="bibr" rid="R24">Fiveash <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R37">Iversen <italic>et al</italic>., 2015</xref>; <xref ref-type="bibr" rid="R75">Repp, 2005</xref>). The present study addresses the little-known relationship between behavioural tracking (measured by finger-tapping) and cortical envelope tracking (measured by electroencephalogram [EEG] recordings of listening participants) of musical rhythm in the context of varying pitch predictability. While temporal predictability has been shown to increase pitch discrimination performance (<xref ref-type="bibr" rid="R33">Herbst &amp; Obleser, 2019</xref>; <xref ref-type="bibr" rid="R38">Jones <italic>et al</italic>., 2002</xref>), it is currently unclear whether pitch predictability affects the ability to behaviourally track naturalistic musical rhythms.</p><p id="P6">In sum, we here investigated whether cortical tracking of the music envelope, beyond rhythm processing, is modulated by pitch surprisal in two continuous, naturalistic stimulus conditions: tonal (high pitch predictability) and atonal (low pitch predictability) music. In the main experiment, we analysed participants’ EEG during passive listening, focusing on cortical envelope and surprisal tracking. We also investigated the role of enjoyment and musical expertise for cortical envelope tracking. In both the main and follow-up replication experiment, we used a behavioural measure of rhythm perception (finger-tapping) to analyse whether cortical tracking is behaviourally relevant and whether pitch predictability influences behavioural tracking. We expected that high pitch predictability in the tonal condition would be associated with better behavioural rhythm tracking than low pitch predictability in the atonal condition (see pre-registration: <ext-link ext-link-type="uri" xlink:href="https://osf.io/qctpj">https://osf.io/qctpj</ext-link>). Due to complex and opposing effects of attention and previous experience (e.g., <xref ref-type="bibr" rid="R74">Reetzke <italic>et al</italic>., 2021</xref>) we did not hypothesise a priori on whether cortical envelope tracking would be stronger in the tonal or atonal condition.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and methods</title><sec id="S3" sec-type="subjects"><title>Participants</title><p id="P7">Twenty volunteers participated in the main study (14 female, 6 male; 18 to 26 years old; <italic>M</italic> = 20.95, <italic>SD</italic> = 1.88). It was initially planned to test 24 participants (preregistration: <ext-link ext-link-type="uri" xlink:href="https://osf.io/qctpj">https://osf.io/qctpj</ext-link>), but data collection had to be halted due to the COVID-19 pandemic. However, the sample size analysis was based on a previous study (<xref ref-type="bibr" rid="R82">Schulze <italic>et al</italic>., 2011</xref>); <italic>d</italic> = .64, <italic>α</italic> = .05, <italic>β</italic> = .80; see preregistration) and yielded a desired sample size of <italic>N</italic> = 21, which was close to being achieved. In addition, we tested further 52 participants in a behavioural follow up experiment (see below). Participants in the main study were right-handed (<italic>N</italic> = 19) or ambidextrous (N = 1; <xref ref-type="bibr" rid="R63">Oldfield, 1971</xref>). Self-reports (Quick Hearing Check, <xref ref-type="bibr" rid="R46">Koike <italic>et al</italic>., 1994</xref>) indicated that 19 participants had normal hearing, while one reported a score that might suggest slightly diminished hearing (score of 27/60, hearing test recommended from score 20). All participants reported never having received a diagnosis of neurological/psychological disorders or dyslexia. Participants rated their musical expertise on a scale from 1 to 3 (‘none’, ‘some’, ‘a great deal’; <italic>M</italic> = 1.95, <italic>SD</italic> = 0.76). Six participants reported no musical expertise. Most participants (<italic>N</italic> = 18) were unfamiliar with the musical stimuli, and although two reported familiarity with the music, they could not name the piece nor composer.</p><p id="P8">The study was approved by the School of Social Sciences Research Ethics Committee at the University of Dundee (approval number: UoD-SoSS-PSY-UG-2019-84) and adhered to the guidelines for the treatment of human participants in the Declaration of Helsinki. Participants were reimbursed for their time with £15.</p></sec><sec id="S4"><title>Musical stimuli</title><p id="P9">Tonal and atonal polyphonic piano stimuli were used (see <xref ref-type="fig" rid="F1">Figure 1</xref> top). For the tonal condition, we used an excerpt from W. A. Mozart’s <italic>‘Sonata No. 5 in G Major, K. 283’</italic>. The excerpt was taken from the second movement (<italic>II. Andante</italic>). The atonal piece was a manipulated version of this excerpt, created by randomly shifting the pitch of each note from 1 to 9 semitones up or down the musical scale (using GuitarPro v7.5) corresponding to 100 – 900 cents. Therefore, notes no longer formed harmonic relationships, while the timing of each note remained the same. Overall, the music in both conditions contained identical timbre, velocity, and rhythm. Each excerpt was approximately five-minutes long (292 seconds) and had a standard 4/4 time signature. The tempo of the pieces was 46 beats per minute (bpm), but because eighth note measures were consistently used, the dominant beat was 92 bpm (<xref ref-type="fig" rid="F1">Figure 1</xref>). This equalled a rate of 1.52 Hz (see modulation spectrum in <xref ref-type="fig" rid="F2">Figure 2D</xref>), and the beats were 652 ms apart. For the finger-tapping task, unique two-bar segments from the same pieces were extracted per condition (18 segments, each 10.4 seconds long). All music pieces were presented at a sampling rate of 44,100 Hz. All stimuli are available on the OSF server (<ext-link ext-link-type="uri" xlink:href="https://osf.io/3gf6k/">https://osf.io/3gf6k/</ext-link>).</p></sec><sec id="S5"><title>Procedure and task</title><p id="P10">Participants performed the EEG experiment in a quiet room. They sat comfortably, approximately 110 cm from a ‘Benq’ computer screen (22.65 × 13.39 inches; 1920 × 1080 pixels resolution). On-screen instructions were presented in black, size 30 Consolas font, and displayed against a grey background. Participants could adjust the volume of the sound to a comfortable level before the start of the experimental blocks. Musical stimuli were presented using E-Prime 3.0 software (Psychology Software Tools Inc., 2016), and listened to through high-quality wired headphones (Creative, Draco HS880). Participants first passively listened to the 5-min tonal and atonal music excerpts (randomised order). Participants started the music self-paced. A five-second countdown was shown, before an ‘X’ appeared at music onset, on which participants fixated throughout the music listening. After each music piece, participants rated how pleasant they had found the music. We used a Visual Analog Scale, on which participants could rate their enjoyment by drawing a vertical line between ‘<italic>Not pleasant</italic>’ and ‘<italic>Very pleasant</italic>’.</p><p id="P11">After the passive listening blocks, participants performed a finger-tapping task to measure behavioural rhythm tracking in the tonal and atonal conditions. 36 unique trials (18 per condition) were presented in four blocks (two tonal and two atonal) of nine trials each. The order of blocks and of trials within each block was randomised across participants. Each trial was started self-paced and began with a visual presentation of the dominant beat (i.e., eighth notes). For this, an ‘X’ flashed four times at the beat frequency before the music started (see <xref ref-type="fig" rid="F2">Figure 2A</xref>). The dominant beat was presented visually and not acoustically, to not interfere with music processing. Participants then tapped their right index finger on the outer ‘Enter’ key of a silent keyboard to the dominant beat of the music. The length of the music segments (2 bars each) required 16 finger taps per trial, resulting in approximately 288 taps per condition.</p></sec><sec id="S6"><title>Replication of behavioural results</title><p id="P12">To make sure that the behavioural effect found in the main experiment (more consistent finger-tapping to tonal than atonal excerpts) was robust, we carried out a follow-up replication study. All experimental procedures were ethically approved by the Ethics Council of the Max Planck Society (Nr. 2017_12). The number of participants was <italic>N</italic> = 52 (33 female, 19 male), and their age ranged between 20 and 41 years (<italic>M</italic> = 26.6, <italic>SD</italic> = 5.3 years). Most participants were right-handed (<italic>N</italic> = 43), some were left-handed (<italic>N</italic> = 6) or ambidextrous (<italic>N</italic> = 3). The procedure was identical to the main experiment, with the exception that 4 bars were used for each trial, thus doubling the time for finger-tapping per trial. This led to 10 unique tonal and 10 unique atonal trials, each 20.9 seconds long. Each trial required 32 finger taps, resulting in 320 taps per condition. Furthermore, all tonal and atonal trials were presented in random order (in contrast to tonal and atonal blocks as in the main experiment).</p></sec><sec id="S7"><title>Analysis of behavioural data</title><p id="P13">The inter-tap-intervals of participants’ keyboard taps for each trial were pre-processed in several ways to clean the data. First, the first two finger taps (i.e., before 981 ms) at the beginning of the trial were excluded from further analysis, to allow participants to hear two eighth notes to inform their tapping. Trials with fewer than 50% of expected remaining inter-tap-intervals were excluded (i.e., 6 necessary inter-tap-intervals in the original experiment, and 15 in the replication experiment). Inter-tap-intervals of faster than 50 ms (indicating involuntary movements) and slower than 3000 ms (indicating idling) were removed. Within each participant and condition, trials with intervals of larger than 3 standard deviations from the mean were also excluded (<xref ref-type="bibr" rid="R1">Abel <italic>et al</italic>., 2009</xref>; <xref ref-type="bibr" rid="R78">Rovetti <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R88">Zarate <italic>et al</italic>., 2015</xref>). At the participant level, our criterion was to exclude outlier data of larger than 3 standard deviations from the mean per condition (<italic>N</italic> = 0 in the original experiment, <italic>N</italic> = 0 in the replication). During the replication experiment, three participants misunderstood instructions and consistently tapped to fast 16th notes. These three participants were excluded, resulting in 49 participants that were included in the final analyses. Finger-tapping performance per trial, condition, and participant was quantified as the Median Absolute Deviation (MAD), a robust measure of dispersion (<xref ref-type="bibr" rid="R51">Leys <italic>et al</italic>., 2013</xref>) that precisely captures the variability in tapping timing. As the MAD is based on median values, it is less affected by outliers than measures based on the mean, such as variance or the coefficient of variation (<xref ref-type="bibr" rid="R4">Arachchige <italic>et al</italic>., 2022</xref>). Enjoyment ratings on the visual analogue scales to both the tonal and atonal excerpts were analysed on a scale between 0 and 100, in increments of 1 (a.u., see <xref ref-type="fig" rid="F2">Figure 2B</xref>). We also preregistered analysing participants’ tapping accuracy in addition to their variability (<ext-link ext-link-type="uri" xlink:href="https://osf.io/qctpj">https://osf.io/qctpj</ext-link>) but were unable to carry this out after the experiment had concluded due to restrictions on access to the laboratory and to the equipment for necessary latency measurements, as imposed by the start of the COVID-19 pandemic.</p></sec><sec id="S8"><title>EEG data acquisition and preprocessing</title><p id="P14">The electroencephalogram (EEG) was recorded from 32 scalp electrodes, using a BioSemi ActiveTwo system (sampling rate 512 Hz). Electrodes were placed according to the International 10-20 system. Electrodes with an offset of greater/less than ±20mV were adjusted. Ultimately, electrode offset was always below an absolute value of 30mV before the experiment began. Horizontal eye-movements were captured by two electro-oculographic (EOG) electrodes, placed at the outer canthus of each eye. To capture vertical eye-movements and blinks, a further two electrodes were positioned above and below the participants’ left eye.</p><p id="P15">Pre-processing of the EEG data was conducted using FieldTrip (<xref ref-type="bibr" rid="R64">Oostenveld <italic>et al</italic>., 2011</xref>) functions in MATLAB 2021a (MathWorks Inc.). For both 5-minute excerpts used during passive listening, we cut out epochs of 304 seconds (300 s stimulation time from music onset, plus additional 2-s leading and trailing windows). Data were initially re-referenced to <italic>Cz</italic> and bandpass filtered between 0.1 and 100 Hz (3<sup>rd</sup> order Butterworth filter, forward and reverse). Data were then visually inspected using summary metrics (maximum value and z-value in each channel), and noisy channels were removed and interpolated using triangulation. A maximum of 4 channels was removed per participant (M = 2.47, <italic>SD</italic> = 0.91). Before ICA was conducted to identify blinks and artifacts, data were re-referenced to average reference (Bertrand et al. 1985). On average, <italic>M</italic> = 2.16 (<italic>SD</italic> = 0.69) components per participant were removed from the data.</p></sec><sec id="S9"><title>Music envelope pre-processing</title><p id="P16">To analyse the tracking of the music signal, we extracted the wideband music envelope. We first down-sampled each music excerpt to a sampling rate of 150 Hz (<xref ref-type="bibr" rid="R39">Keitel <italic>et al</italic>., 2018</xref>). Acoustic waveforms were then filtered into eight frequency bands (between 100 and 8,000 Hz, 3<sup>rd</sup> order Butterworth filter, forward and reverse) that are equidistant on the cochlear frequency map (<xref ref-type="bibr" rid="R83">Smith <italic>et al</italic>., 2002</xref>). The signal in each of these eight frequency bands was <italic>Hilbert</italic>-transformed, and the magnitude extracted, before they were averaged for the wideband music envelope, which was used for further analyses.</p></sec><sec id="S10"><title>Pitch surprisal modelling</title><p id="P17">Surprisal during music listening refers to how expected a certain musical event is. Some note sequences are extremely prevalent across Western classical music, thus creating high expectations and low surprisal for an audience listening to them. To provide a computational account of music surprisal in the stimuli used, we rely on a model that learns statistical regularities of music (<xref ref-type="bibr" rid="R67">Pearce, 2018</xref>). Based on a variable-order Markov model, IDyOM (Information Dynamics Of Music; <xref ref-type="bibr" rid="R66">Pearce, 2005</xref>; <xref ref-type="bibr" rid="R69">Pearce &amp; Wiggins, 2006</xref>) simulates listeners’ expectations while listening to music by collecting statistical structures of note sequences over <italic>n</italic>-orders on a training corpus set. Here, the training corpus was composed of a collection of Western folk songs (a subset of the Essen Folksong Collection containing 953 melodies), so as to accurately model surprisal for typical Western listeners (<xref ref-type="bibr" rid="R30">Guan <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>., 2022</xref>). Specifically, the long-term component (LTM) of the model collects the sequence statistics over <italic>n</italic>-orders of the training set while the short-term component (STM) dynamically collects the local context over n-orders for each testing melody. For each note of the testing melodies, the model outputs a probability distribution of pitch obtained from merging distributions obtained by the STM and the LTM (for more details, see <xref ref-type="bibr" rid="R66">Pearce, 2005</xref>). By comparing the pitch ‘ground truth’ to the probability predicted by the model, a surprisal value is obtained. Formally, the surprisal (or information content) is the log-negative to the base 2 of the probability of the note. It essentially represents the expectedness of each note given the STM (e.g., the local context) and LTM (e.g., the long-term exposure to a musical style or culture). If surprisal is high, prediction error is also high, and vice versa (<xref ref-type="bibr" rid="R67">Pearce, 2018</xref>). The choice of IDyOM was motivated by numerous empirical evidence that it can accurately model a listener’s internal representation of musical regularities, both using neural and behavioural data (<xref ref-type="bibr" rid="R15">Di Liberto <italic>et al</italic>., 2020a</xref>; <xref ref-type="bibr" rid="R27">Gold <italic>et al</italic>., 2019</xref>; <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R68">Pearce <italic>et al</italic>., 2010</xref>; <xref ref-type="bibr" rid="R70">Pearce &amp; Wiggins, 2012</xref>). Since IDyOM in its current development only takes monophonic MIDI inputs, we reduced the complete score of each excerpt into a monophonic version that contained the melody and the bass line. The pitch surprisal values for each note were then used to build a continuous signal, with surprisal values making up the amplitude for the duration of the respective note. This initial step function was smoothed by convoluting it with a Gaussian filter (<italic>sigma</italic> = 50). The continuous surprisal signal was created to have the same sampling rate as the EEG signal (150 Hz).</p></sec><sec id="S11"><title>Mutual information analysis</title><p id="P18">The correspondence between the continuous EEG signal and envelope and surprisal signals (i.e., cortical envelope tracking and cortical surprisal tracking) was analysed using a Gaussian Copula mutual information (MI) framework (<xref ref-type="bibr" rid="R36">Ince <italic>et al</italic>., 2017</xref>; <xref ref-type="bibr" rid="R41">Keitel <italic>et al</italic>., 2017</xref>). In this approach, which is optimised for neurophysiological data, Gaussian copulas are used to normalise the continuous, analytical signals (<xref ref-type="bibr" rid="R36">Ince <italic>et al</italic>., 2017</xref>). The first 500 ms of the signals were removed from analysis, to avoid contamination with strong transient evoked responses at the start of the music. Mutual information (in bits) between the EEG signal and the music envelope was computed with both signals filtered at the dominant beat frequency range (0.5 – 2 Hz). A participant-specific stimulus-brain lag was included for the envelope tracking analyses, which was based on the individual phase coherence (<xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>) peak at auditory electrode Cz, which was averaged for slow frequencies between 1 and 12 Hz (before the narrow band-pass filtering described above). Initial coherence values were computed for lags between 40 and 200 ms in steps of 20 ms.</p><p id="P19">Mutual information between the EEG signal and the pitch surprisal signals was computed with signals bandpass filtered between 0.1 and 50 Hz. As the analysis sampling rate was 150 Hz, filtering up to 50 Hz allowed for this frequency to be robustly reflected in the data (1/3 of the sampling frequency). This wide range was chosen as no clear assumptions about a specific, narrow-band frequency range could be made and prediction processes have been shown across multiple frequency bands (<xref ref-type="bibr" rid="R23">Engel <italic>et al</italic>., 2001</xref>; <xref ref-type="bibr" rid="R57">Morillon <italic>et al</italic>., 2019</xref>). The surprisal-tracking analysis included surprisal values for both the melody and bass lines jointly, capitalising on the multivariate capabilities of the used MI approach (<xref ref-type="bibr" rid="R36">Ince <italic>et al</italic>., 2017</xref>). Apart from the wider frequency band, the analysis of surprisal tracking was equivalent to that of envelope tracking, including the same individual stimulus-brain lags.</p><p id="P20">Each mutual information value was computed per participant, condition, and electrode. The results of these analyses will be referred to as cortical (envelope or surprisal) ‘tracking’, and we do not make assumptions about the underlying mechanisms (e.g., cortical entrainment) as these are still debated (<xref ref-type="bibr" rid="R3">Alexandrou <italic>et al</italic>., 2018</xref>; <xref ref-type="bibr" rid="R42">Keitel <italic>et al</italic>., 2021</xref>; <xref ref-type="bibr" rid="R61">Obleser &amp; Kayser, 2019</xref>).</p></sec><sec id="S12"><title>Statistical analyses</title><p id="P21">To test the statistical significance of mutual information values for envelope and surprisal tracking against chance, 3000 permutations were computed per participant, condition, and electrode. Specifically, to create permuted data, we segmented the continuous envelope/surprisal signals into 1-s segments and shuffled the segments randomly. This kept the statistical properties of the signal but destroyed the temporal relationship between the music and brain signals. MI was then computed between the brain signal and the 3000 shuffled envelope/surprise signals. The group level mean was then tested against the 95th percentile of the random group mean distribution, essentially implementing a one-sided randomisation test at <italic>p</italic> &lt; .05 (<xref ref-type="bibr" rid="R10">Brohl <italic>et al</italic>., 2022</xref>).</p><p id="P22">For the comparison between the two conditions, <italic>t</italic>-values were computed using the real MI values, as well as the 3000 MI values from the shuffled data. These real and permuted data were then compared, again using a cluster-based permutation test, with a critical <italic>t-</italic>value of 2.1, which represents the critical value of the <italic>Student’s</italic> <italic>t</italic> distribution for 20 participants and a two-tailed probability of <italic>p</italic> = .05 (<xref ref-type="bibr" rid="R39">Keitel <italic>et al</italic>., 2018</xref>).</p><p id="P23">Pearson’s correlations between cortical tracking and behavioural measures (tapping variability, musical competency, and enjoyment) were computed between the behavioural measures and the true MI values, as well as between the behavioural measures and the 3000 permuted MI values. Before comparing the true <italic>r</italic>-values with the permutation distribution using cluster-based permutation (as above), Pearson’s <italic>r</italic>-values were transformed to be normally distributed using Fisher’s z-transformation (e.g., <xref ref-type="bibr" rid="R28">Gorsuch &amp; Lehmann, 2010</xref>). For all cluster-based permutation analyses, initial clusters were chosen at an alpha level of <italic>p</italic> &lt; .05. As an indicator of effect sizes, we either report <italic>Cohen’s d</italic> for peak electrodes in the case of <italic>t</italic> or <italic>r</italic> values (<xref ref-type="bibr" rid="R9">Brohl &amp; Kayser, 2021</xref>; <xref ref-type="bibr" rid="R47">Lakens, 2013</xref>), or summed MI values within each significant cluster (<italic>MI</italic><sub>sum</sub>) (<xref ref-type="bibr" rid="R39">Keitel <italic>et al</italic>., 2018</xref>).</p><p id="P24">To be able to draw evidence-led conclusions about the laterality of our results, we explicitly tested for hemispheric lateralisation (<xref ref-type="bibr" rid="R40">Keitel <italic>et al</italic>., 2020</xref>; <xref ref-type="bibr" rid="R65">Park &amp; Kayser, 2019</xref>). Hemispheric differences in cortical tracking have been theoretically assumed and occasionally been found experimentally for the music envelope (<xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>; <xref ref-type="bibr" rid="R89">Zatorre, 2001</xref>). The participant-specific results (e.g., MI values) were extracted for significant electrodes in one hemisphere and in corresponding contralateral electrodes. We then averaged these values within each hemisphere and calculated the between-hemispheres differences with a group-level, <italic>Student's</italic> <italic>t</italic>-test (two-sided). <italic>P</italic>-values were corrected for multiple comparisons using FDR correction at the level of 5% (<xref ref-type="bibr" rid="R7">Benjamini &amp; Hochberg, 1995</xref>).</p><p id="P25">All tests are <italic>two-tailed</italic>, except for the comparison of finger-tapping variability between the tonal and atonal condition. These comparisons are one-tailed, as we had an <italic>a priori</italic> directed hypothesis that finger-tapping in the atonal condition would be more variable than in the tonal condition (see preregistration, <ext-link ext-link-type="uri" xlink:href="https://osf.io/qctpj">https://osf.io/qctpj</ext-link>).</p></sec></sec><sec id="S13" sec-type="results"><title>Results</title><sec id="S14"><title>Differences between tonal and atonal music stimuli</title><p id="P26">As intended, the pitch surprisal was overall higher for the atonal than the tonal stimuli (see <xref ref-type="fig" rid="F5">Figure 5B</xref>). Pitch surprisal was computed for all notes, separately for melody and bass lines using IDyOM (<xref ref-type="bibr" rid="R66">Pearce, 2005</xref>). The surprisal is estimated by comparing the pitch ground truth of a note to its predicted value in the model’s output distribution. For the melody line, pitch surprisal was on average <italic>M</italic> = 3.02 (<italic>SD</italic> = 3.28) in the tonal condition, and <italic>M</italic> = 6.96 (<italic>SD</italic> = 3.78) in the atonal condition (see <xref ref-type="fig" rid="F5">Figure 5B</xref>). A <italic>Student's t</italic>-test confirmed that surprisal values were statistically larger in the atonal than the tonal condition, <italic>t</italic>(597) = -25.21, <italic>p</italic> &lt; .001, <italic>Cohen’s d</italic> = -1.03. Likewise, pitch surprisal in the bass line was higher in the atonal than in the tonal condition (tonal: <italic>M</italic> = 3.26, <italic>SD</italic> = 2.72; atonal: <italic>M</italic> = 7.38, <italic>SD</italic> = 3.54; <italic>t</italic>(722) = -28.41, <italic>p</italic> &lt; .001, <italic>Cohen’s d</italic> = -1.06).</p><p id="P27">Furthermore, participants were asked to rate how pleasant they found listening to the tonal and atonal music stimuli (on a scale effectively analysed from 0 to 100 a.u., <xref ref-type="fig" rid="F2">Figure 2B</xref>). The enjoyment ratings indicated that participants found the tonal excerpt more pleasant than the atonal excerpt (tonal: <italic>M</italic> = 76.18, <italic>SD</italic> = 19.31; atonal: <italic>M</italic> = 42.53, <italic>SD</italic> = 25.99; <italic>t</italic>(19) = 6.47, <italic>p</italic> &lt; .001, <italic>Cohen’s d</italic> = 1.45).</p><p id="P28">We also computed the modulation spectrum (<xref ref-type="bibr" rid="R18">Ding <italic>et al</italic>., 2017</xref>) for both conditions between 0 and 12 Hz (<xref ref-type="fig" rid="F2">Figure 2C</xref>). This showed several peaks at beat-related frequencies (i.e., subharmonic and harmonics of 1.52 Hz). A comparison of average beat-related frequencies (c.f., <xref ref-type="bibr" rid="R11">Celma-Miralles &amp; Toro, 2019</xref>; <xref ref-type="bibr" rid="R60">Nozaradan <italic>et al</italic>., 2012</xref>) for excerpts across both conditions showed no statistical difference (tonal: <italic>M</italic> = 0.66, <italic>SD</italic> = 0.10; atonal: <italic>M</italic> = 0.63, <italic>SD</italic> = 0.06; <italic>t</italic>(17) = 1.02, <italic>p</italic> = .322, <italic>Cohen’s d</italic> = 0.24; <xref ref-type="fig" rid="F2">Figure 2D</xref>). This indicates that the amplitudes of beat-related peaks in the modulation spectrum were comparable in both conditions.</p><p id="P29">To assess whether the atonal condition shows any drastic perceptive pitch differences compared with the tonal condition, we computed roughness as measure of sensory dissonance (using the MIR toolbox; <xref ref-type="bibr" rid="R48">Lartillot <italic>et al</italic>., 2008</xref>). The default frame length of 50 ms (25 ms overlap) was used. The roughness in the tonal condition was slightly higher than in the atonal condition (tonal: <italic>M</italic> = 82.85, <italic>SD</italic> = 155.73; atonal: <italic>M</italic> = 60.18, <italic>SD</italic> = 122.02; <italic>t</italic>(11674) = 15.25, <italic>p</italic> &lt; .001, <italic>Cohen’s d</italic> = 0.14; <xref ref-type="fig" rid="F2">Figure 2E</xref>). The effect was very small (according to <italic>Cohen’s d</italic> conventions) but implies at least that the atonal condition did not show more sensory dissonance than the tonal condition.</p></sec><sec id="S15"><title>Behavioural tracking: Finger-tapping is more variable in the atonal than the tonal condition</title><p id="P30">To test how pitch predictability influences behavioural rhythm tracking, we first analysed differences in inter-tap-intervals between the tonal and atonal conditions. To mitigate the effect of potential outliers on the group level (see <xref ref-type="fig" rid="F3">Figure 3</xref>), we used a non-parametric approach. In the Experiment 1, the median absolute deviation (MAD) of inter-tap-intervals in the tonal condition was on average <italic>M</italic> = 29.72 ms (<italic>SD</italic> = 14.99 ms). In the atonal condition, the MAD was slightly higher, on average <italic>M</italic> = 40.30 ms (<italic>SD</italic> = 35.66 ms). A direct comparison using a Wilcoxon Signed Rank test indicated that tapping performance was significantly more variable in the atonal than the tonal condition (difference = 10.6 ms, <italic>Z</italic> = -1.94, <italic>p</italic> = 0.026, <italic>one-tailed</italic>). Out of the 20 participants, 15 (75%) had more variable inter-tap-intervals when tapping in the atonal than in the tonal condition.</p><p id="P31">We performed the same analysis for the behavioural follow-up study, which had more than twice as many participants, and in which individual trials were twice as long as in the original experiment. The MAD of inter-tap-intervals in the tonal condition was on average <italic>M</italic> = 42.56 ms (<italic>SD</italic> = 46.24 ms). In the atonal condition, the MAD was again slightly higher, on average <italic>M</italic> = 52.42 ms (<italic>SD</italic> = 67.33 ms). A Wilcoxon Signed Rank test confirmed that tapping performance was significantly more variable in the atonal than the tonal condition (difference = 9.9 ms, <italic>Z</italic> = -1.78, <italic>p</italic> = 0.037, <italic>one-tailed</italic>). Out of the 49 participants, 30 (61.2%) had more variable inter-tap-intervals when tapping in the atonal than in the tonal condition. Together, the results of the original and replication experiments indicate that there is a small but replicable effect of tonality on finger-tapping variability: When listening to tonal music, participants tap to the beat more consistently than when listening to atonal music.</p></sec><sec id="S16"><title>Cortical tracking of the music envelope</title><p id="P32">We first analysed whether participants tracked the acoustic music envelope, band-pass filtered around the dominant beat frequency, in the tonal and atonal condition compared with chance level, using cluster-based permutation (<xref ref-type="fig" rid="F2">Figure 2A</xref>). In the tonal condition, we found a large positive cluster of 31 electrodes that tracked amplitude fluctuations significantly (<italic>p</italic> &lt; .001, <italic>MI</italic><sub>sum</sub> = 0.332). Equivalently, in the atonal condition, there was a positive cluster of 32 electrodes that showed significant envelope tracking (<italic>p</italic> &lt; .001, <italic>MI</italic><sub>sum</sub> = 0.438). There was no evidence for a hemispheric lateralisation, neither in the tonal nor the atonal condition (both <italic>p</italic><sub>FDR</sub> &gt; 0.82). We then directly compared envelope tracking in both conditions. This resulted in two negative clusters, one over frontal electrodes (<italic>p</italic> &lt; .001, <italic>Cohen’s d</italic><sub>peak</sub> = -1.17, 7 electrodes) and one over left occipital electrodes (<italic>p</italic> = .003, <italic>Cohen’s d</italic><sub>peak</sub> = -2.09, 3 electrodes). These negative clusters indicated that the acoustic music envelope was tracked stronger in the atonal than the tonal condition.</p></sec><sec id="S17"><title>Envelope tracking for tonal music during passive listening predicts finger-tapping performance</title><p id="P33">To test whether envelope tracking during passive listening predicted participants’ behavioural tracking of the beat (i.e., finger-tapping performance), we correlated the MI values per electrode with participants’ average tapping variation (MAD, <xref ref-type="fig" rid="F4">Figure 4A</xref>). We found one negative cluster over left-frontal electrodes that predicted tapping variance in the tonal condition (<italic>p</italic> = .026, <italic>Cohen’s</italic> <italic>d</italic><sub>peak</sub> = -1.02, 2 electrodes). These results indicate that participants who showed stronger envelope tracking to tonal music had smaller variance (i.e., better performance) when tapping, than participants with weaker envelope tracking. Envelope tracking when listening to atonal music did not significantly predict tapping performance in the atonal condition. To compare this relationship directly between the tonal and atonal condition, we entered the average MI values of electrodes in the negative cluster as predictor into a regression model, with <italic>tonality</italic> as additional predictor, an <italic>envelope tracking × tonality</italic> interaction term, and <italic>finger-tapping variability</italic> (MAD) as outcome variable. This overall model was not significant (<italic>F</italic>(3,26) = 1.02, <italic>p</italic> = .395) and only explained 7.84% of the variance. The model yielded no main effects (both <italic>p</italic> &gt; .32) and no interaction (<italic>p</italic> &gt; .21), which suggests that the effect of envelope tracking on finger-tapping is small in the tonal condition, and not statistically different between the tonal and atonal condition.</p></sec><sec id="S18"><title>Expertise predicts envelope tracking for tonal and atonal music</title><p id="P34">Several previous studies have found that musical expertise is associated with stronger neural synchronisation to music (<xref ref-type="bibr" rid="R16">Di Liberto <italic>et al</italic>., 2020b</xref>; <xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>; <xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>). We therefore tested the relationship between participants’ musical expertise and acoustic envelope tracking. A large fronto-temporal cluster showed a significant positive correlation between self-assessed musical competency and music tracking in the tonal condition (<italic>p</italic> &lt; .001, <italic>Cohen’s d<sub>peak</sub></italic> = 2.76, 7 electrodes). In the atonal condition, envelope tracking was also positively predicted by a fronto-temporal cluster (<italic>p</italic> = .004, <italic>Cohen’s d<sub>peak</sub></italic> = 2.85, 4 electrodes). A regression model predicting <italic>envelope tracking</italic> (averaged across the electrodes included in the significant clusters reported above) from <italic>tonality,</italic> <italic>musical expertise</italic>, and their interaction, (<italic>F</italic>(3,26) = 12.83, <italic>p</italic> &lt; .001) explained 51.7% of the variance. Only the main effect of <italic>expertise</italic> was significant (<italic>t</italic> = 4.15, <italic>p</italic> &lt; .001; main effect of <italic>tonality</italic> and the interaction both <italic>p</italic> &gt; .59). These results indicate that musical expertise is associated with enhanced tracking of the music envelope for both high predictable (tonal) and low predictable (atonal) music but is unlikely to explain differences in tracking between conditions.</p></sec><sec id="S19"><title>Enjoyment predicts envelope tracking for tonal music</title><p id="P35">Participants also indicated how pleasant they found listening to the music after each condition using visual analogue scales. These enjoyment ratings were significantly higher for tonal than atonal music (<italic>Cohen’s d</italic> = 1.45; see above and <xref ref-type="fig" rid="F2">Figure 2C</xref>). Ratings were correlated with cortical envelope tracking across participants. In the tonal condition, two significant clusters showed a positive correlation between enjoyment and envelope tracking, one frontocentral cluster (<italic>p</italic> = .026, <italic>Cohen’s d<sub>peak</sub></italic> = 1.40, 3 electrodes [F3, FC1, Fz]) and one occipital cluster (<italic>p</italic> = .004, <italic>Cohen’s d</italic> = 1.61; 4 electrodes [Oz, O2, PO4, P8]). No significant clusters emerged in the atonal condition. A regression model predicting <italic>envelope tracking</italic> (averaged across the electrodes included in the significant clusters reported above) from <italic>enjoyment</italic> ratings, <italic>tonality</italic> and their interaction (<italic>F</italic>(3,26) = 2.92, <italic>p</italic> = .047) explained 19.6% of the variance. More <italic>enjoyment</italic> increased cortical tracking (<italic>t</italic> = 2.28, <italic>p</italic> = .029), whereas neither <italic>tonality</italic> (<italic>t</italic> = 1.92, <italic>p</italic> = .062), nor an interaction of <italic>tonality</italic> with <italic>enjoyment</italic> affected tracking (<italic>t</italic> = -1.03, p = .310).</p></sec><sec id="S20"><title>Cortical tracking of pitch surprisal</title><p id="P36">Pitch surprisal was analysed using the IDyOM model (<xref ref-type="bibr" rid="R66">Pearce, 2005</xref>) in both conditions. As expected, surprisal was higher for notes in the atonal than the tonal condition for both the melody and bass lines (<italic>Cohen’s d<sub>peak</sub></italic> = -1.03 and <italic>Cohen’s d<sub>peak</sub></italic> = -1.06, respectively; see above and <xref ref-type="fig" rid="F5">Figure 5B</xref>). We first analysed whether pitch surprisal was tracked in both conditions above chance level, in a multivariate analysis including surprisal in melody and bass lines. In the tonal condition, pitch surprisal was tracked in a large bilateral cluster (<xref ref-type="fig" rid="F5">Figure 5A</xref>; <italic>p</italic> &lt; .001, <italic>MI</italic><sub>sum</sub> = 0.073; 16 electrodes). Likewise, in the atonal condition, pitch surprisal was tracked in a bilateral electrode cluster (<italic>p</italic> &lt; .001, <italic>MI</italic><sub>sum</sub> = 0.051; 14 electrodes). Although pitch tracking in the tonal condition appeared to be larger than in the atonal condition, directly comparing the tracking of pitch surprisal between both conditions yielded no statistically significant clusters. Likewise, although pitch surprisal tracking appeared larger in the right hemisphere, contrasting left- and right-hemispheric cluster electrodes yielded no systematic lateralisation of cortical tracking (tonal: <italic>p</italic><sub>FDR</sub> =.27 and atonal: <italic>p</italic><sub>FDR</sub> =.13, respectively). These results suggest that listeners form pitch expectations (i.e., prediction errors) comparable with the IDYOM model, and that pitch surprisal is represented in the brain to a similar extent in tonal and atonal conditions.</p></sec><sec id="S21"><title>Surprisal tracking predicts finger-tapping performance in the atonal condition</title><p id="P37">We also analysed whether the extent to which participants tracked pitch surprisal predicted their finger-tapping performance. This correlation analysis yielded no significant clusters in the tonal condition. However, in the atonal condition, the tracking of pitch surprisal was positively correlated with finger-tapping performance in one frontocentral cluster (<xref ref-type="fig" rid="F5">Figure 5B</xref>; <italic>p</italic> = .027, <italic>Cohen’s d<sub>peak</sub></italic> = 1.11, 2 electrodes). Again, to be able to draw conclusions about differences between the tonal and atonal condition, we entered the average MI values of the positive cluster as predictor into a regression model, with <italic>tonality</italic> as additional predictor, a <italic>surprisal tracking × tonality</italic> interaction term, and <italic>finger-tapping variability</italic> (MAD) as outcome variable. The overall regression model (<italic>F</italic>(3,36) = 7.26, <italic>p</italic> &lt; .001) explained 37.6% of variance in finger-tapping variability. Neither the main effect of <italic>surprisal tracking</italic> (<italic>p</italic> &gt; .49) nor the main effect of <italic>condition</italic> (<italic>p</italic> &gt; .16) reached significance. However, the interaction <italic>surprisal tracking × condition</italic> was statistically significant (<italic>t</italic> = 3.20, <italic>p</italic> &lt; .003). This interaction stemmed from an effect, exclusive to the atonal condition, where participants who tracked the pitch surprisal well, finger-tapped with higher variability than participants with relatively poor cortical tracking.</p></sec><sec id="S22"><title>Relationship between envelope tracking and surprisal tracking</title><p id="P38">Last, we were interested in the relationship between acoustic envelope tracking and pitch surprisal tracking, because these measures have not previously been looked at together. We used the average MI value per cluster (as seen in <xref ref-type="fig" rid="F4">Figure 4A</xref> and <xref ref-type="fig" rid="F5">Figure 5A&amp;B</xref>) and participant in a regression model with <italic>acoustic envelope tracking</italic> as outcome variable and <italic>surprisal tracking</italic>, <italic>tonality</italic> and a <italic>surprisal tracking × tonality</italic> interaction term as predictors. The overall regression model was not significant (<italic>F</italic>(3,36) = 1.67, <italic>p</italic> = .190) and explained 12.2% of the variance. No main or interaction effect reached significance (all <italic>p</italic>-values &gt; .295; <xref ref-type="fig" rid="F5">Figure 5D</xref>). Thus, there seems to be no systematic relationship between acoustic envelope tracking and pitch surprisal tracking.</p></sec></sec><sec id="S23" sec-type="discussion"><title>Discussion</title><p id="P39">In the present study we show that the cortical representation of naturalistic continuous music, as measured through envelope tracking, reflects not only rhythm processing, but is also modulated by pitch predictability, musical expertise, and enjoyment (<xref ref-type="fig" rid="F6">Figure 6</xref>).</p><sec id="S24"><title>Pitch predictability affects rhythm processing as reflected in behavioural tracking</title><p id="P40">Atonal music can be used to study predictive processing under high uncertainty contexts (<xref ref-type="bibr" rid="R54">Mencke <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R55">Mencke <italic>et al</italic>., 2018</xref>). While temporal predictability has been shown to increase pitch discrimination performance (<xref ref-type="bibr" rid="R33">Herbst &amp; Obleser, 2019</xref>; <xref ref-type="bibr" rid="R38">Jones <italic>et al</italic>., 2002</xref>), it is unclear whether long-term pitch predictability affects the ability to behaviourally follow the beat particularly in naturalistic musical stimuli. Crucially, in our main experiment and a replication study, we show that when listening to naturalistic music, pitch predictability (modelled on long-term statistics, which reflect exposure to a musical culture, and short-term melodic context) modulates the variability of finger-tapping to the beat. In the tonal condition with higher pitch predictability the finger-tapping performance was more consistent (less variable inter-tap-interval) compared with the atonal low predictability condition. The atonal music in our study contained a rhythmic structure that was identical to the tonal condition, but had generally lower pitch predictability, suggesting that this finding reflects a modulation of behavioural rhythm processing (i.e., finger-tapping to the beat) by pitch predictability. This is in line with and extends previous studies showing expectation effects on musical perception (<xref ref-type="bibr" rid="R33">Herbst &amp; Obleser, 2019</xref>; <xref ref-type="bibr" rid="R35">Huron, 2008</xref>; <xref ref-type="bibr" rid="R38">Jones <italic>et al</italic>., 2002</xref>; <xref ref-type="bibr" rid="R69">Pearce &amp; Wiggins, 2006</xref>)</p></sec><sec id="S25"><title>Cortical tracking of the music envelope in tonal and atonal music</title><p id="P41">At the neural level, the music envelope was tracked in our study for both tonal and atonal music (<xref ref-type="fig" rid="F4">Figure 4A</xref>). The tracking was observed in both conditions with a centro-temporal topography, in accordance with previous reports suggesting auditory cortex generators of the envelope tracking in speech (<xref ref-type="bibr" rid="R29">Gross et al., 2013</xref>; <xref ref-type="bibr" rid="R52">Luo &amp; Poeppel, 2007</xref>) and music (<xref ref-type="bibr" rid="R16">Di Liberto et al., 2020b</xref>; <xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>). Some studies reported a right lateralisation for music envelope tracking (<xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>) in line with the asymmetric sampling in time theory (<xref ref-type="bibr" rid="R73">Poeppel, 2003</xref>; <xref ref-type="bibr" rid="R91">Zatorre et al., 2002</xref>). The heterogeneous findings in the literature regarding whether a hemispheric lateralisation is observed have been related to various top-down influences (<xref ref-type="bibr" rid="R5">Assaneo et al., 2019</xref>; <xref ref-type="bibr" rid="R25">Flinker et al., 2019</xref>; <xref ref-type="bibr" rid="R90">Zatorre, 2022</xref>).</p><p id="P42">Atonal music was more strongly tracked at frontal and left parietal electrodes compared with tonal music. Importantly, this was the case, although both conditions had an identical rhythmic structure and there were no significant acoustic differences in the modulation spectrum (<xref ref-type="fig" rid="F2">Figure 2D</xref>). We interpret this as evidence that pitch predictability influences neural rhythm tracking. Our control analysis showed that even when the tracking of pitch intervals was partialled out, atonal music was still tracked stronger than tonal music (<xref ref-type="supplementary-material" rid="SD1">Figure S1</xref>). Further, the atonal condition did not carry higher sensory dissonance. These control analyses support the interpretation that it is pitch predictability, and not low-level pitch differences, that affect rhythm tracking. Our results from the neural data are also in line with our behavioural findings in that they suggest an effect of pitch predictability on rhythm processing. Interestingly, <xref ref-type="bibr" rid="R87">Weineck <italic>et al</italic>. (2022)</xref> speculated that more predictable music produces stronger neural synchronisation, which our results contradict. However, their paradigm did not manipulate pitch predictability and results are therefore not directly comparable. A predictive coding approach (<xref ref-type="bibr" rid="R26">Friston, 2010</xref>; <xref ref-type="bibr" rid="R32">Heilbron &amp; Chait, 2018</xref>) could provide a potential explanation for the observed effect: In the atonal condition, notes were generally less predictable than in the tonal condition. In line with the assumption of expectation suppression (<xref ref-type="bibr" rid="R6">Auksztulewicz &amp; Friston, 2016</xref>), this likely led to stronger neural prediction errors, which in turn might have resulted in stronger neural responses to the acoustic envelope (not unlike a mismatch-negativity response, e.g., <xref ref-type="bibr" rid="R45">Koelsch <italic>et al</italic>., 2019</xref>; <xref ref-type="bibr" rid="R58">Naatanen <italic>et al</italic>., 2007</xref>). Accordingly, <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>. (2022)</xref> showed that surprising notes elicit stronger neural responses compared to predictable ones (see also <xref ref-type="bibr" rid="R15">Di Liberto <italic>et al</italic>., 2020a</xref>).</p></sec><sec id="S26"><title>Increased envelope tracking in tonal music related to better tapping performance</title><p id="P43">In the tonal condition, the cortical tracking of the musical envelope correlated with the behavioural tracking, with higher cortical tracking being associated with an increased ability to behaviourally follow the beat. The findings are in line with previous research showing a positive correlation between behavioural performance and cortical tracking of speech (<xref ref-type="bibr" rid="R80">Schmitt <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R81">Schubert <italic>et al</italic>., 2023</xref>) and music (<xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>). The relationship between cortical tracking and behavioural performance, however, might be more complex than this, as suggested for speech (<xref ref-type="bibr" rid="R34">Howard &amp; Poeppel, 2010</xref>; <xref ref-type="bibr" rid="R72">Pefkou <italic>et al</italic>., 2017</xref>). No correlation was observed in the atonal condition and furthermore when the correlation effects were tested in a regression model that included both conditions and selected electrodes, no significant interaction effect was observed. This makes any interpretation regarding differences between tonal and atonal music difficult. A potential explanation is that under conditions of low pitch predictability (as in the atonal condition) the positive relationship between envelope tracking and behavioural tapping performance is confounded, perhaps due to the increased difficulty of trying to (unsuccessfully) make predictions about the upcoming notes. In summary, increased behavioural tracking was related to increased cortical tracking only in the tonal condition, albeit the effect was small.</p></sec><sec id="S27"><title>Pitch surprisal is cortically tracked in tonal and atonal music</title><p id="P44">We designed our stimuli so that the pitch predictability of the musical pieces was increased in the atonal compared to the tonal condition (<xref ref-type="fig" rid="F5">Figure 5B</xref>). Importantly, our human listeners significantly tracked pitch surprisal in both conditions, which indicates that their pitch expectations (or their prediction errors) were comparable with the IDyOM model. Further, there were no significant condition differences in the cortical tracking of pitch surprisal (<xref ref-type="fig" rid="F5">Figure 5A</xref>). This suggests that participants’ neural models of pitch surprisal matches the IDyOM computations, and the notes in the atonal condition elicited not only more surprisal in the IDyOM model, but also in listeners' neural response. Cortical tracking of pitch surprisal in natural music has been rarely investigated. Two recent studies report melodic surprisal tracking in tonal music that was localised to bilateral superior temporal and Heschl’s gyri (amongst others), and additionally showed either a central topography (using EEG/EcoG, <xref ref-type="bibr" rid="R15">Di Liberto <italic>et al</italic>., 2020a</xref>) or a broad fronto-temporal (and central) topography (using MEG, <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>., 2022</xref>). Overall, we found relatively widespread fronto-temporal tracking of pitch surprisal across conditions, which is in line with the above results. We here show that listeners consistently track pitch surprisal not only for high-predictable music, as previously shown, but also for low-predictable music.</p></sec><sec id="S28"><title>In atonal music lower pitch surprisal tracking is related to better tapping performance</title><p id="P45">Interestingly, the surprisal tracking strength was only correlated with the behavioural rhythm tracking performance in the atonal but not the tonal condition (as shown by the significant interaction between condition and surprisal tracking). Participants who tracked the pitch surprisal stronger, meaning they matched the high surprisal values from the IDyOM model, also showed more variation (worse performance) in their tapping in the atonal condition. Prediction tendencies have been suggested to vary across participants (<xref ref-type="bibr" rid="R81">Schubert <italic>et al</italic>., 2023</xref>). Our measure of surprisal tracking might reflect such a tendency, with some individuals being more or less prone (or able) to make predictions. In the atonal condition with its high uncertainty, pitch predictions might be less informative for rhythm processing, and listeners who tend to make (stronger) predictions, which lead to high prediction errors, could have fewer resources to track the envelope, and to perform well in the tapping task. The few behavioural studies that looked at long-term pitch surprisal tracking have not related it to the rhythm processing performance (e.g., <xref ref-type="bibr" rid="R44">Kern <italic>et al</italic>., 2022</xref>). Our results indicate that the negative effect of pitch surprisal tracking on behavioural rhythm processing might only be expected when pitch predictability is low, and prediction errors are high, as in the case of atonal music. The cortical tracking of pitch surprisal was not systematically related to the cortical tracking of the acoustic envelope, at least not in our sample of 20 participants that underwent the EEG part of the study.</p></sec><sec id="S29"><title>Enjoyment and musical expertise are related to cortical envelope tracking</title><p id="P46">As expected based on the literature (<xref ref-type="bibr" rid="R54">Mencke <italic>et al</italic>., 2022</xref>; <xref ref-type="bibr" rid="R55">Mencke <italic>et al</italic>., 2018</xref>), the atonal music condition was rated as less pleasant than the tonal music condition. The individually perceived pleasure or enjoyment has a strong influence on our everyday music listening behaviour (c.f., <xref ref-type="bibr" rid="R27">Gold <italic>et al</italic>., 2019</xref>). In the tonal condition, the enjoyment ratings correlated with the strength of cortical music envelope tracking, and this pattern was not statistically different in the atonal condition. The causal nature of this relationship remains unclear: Do listeners show stronger acoustic envelope tracking because they find the music more enjoyable, or do they find it more enjoyable because they have better acoustic envelope tracking? Interestingly, a previous study that investigated whether enjoyment influences neural synchronisation to music did not find a significant effect (<xref ref-type="bibr" rid="R87">Weineck <italic>et al</italic>., 2022</xref>). The discrepancy with our findings might be due to differences in experimental paradigms, music choices, quantification of music tracking and analytical methods. If future studies replicate our finding, this suggests that the individual preference of listeners should be taken into account when measuring envelope tracking.</p><p id="P47">Additionally, musical expertise was correlated positively with the cortical envelope tracking of the music pieces at a cluster of frontal and right temporal electrodes for both the tonal and atonal condition, with more expertise being related to stronger tracking. Our finding is in line with previous reports of effects of musical expertise on cortical envelope tracking (<xref ref-type="bibr" rid="R16">Di Liberto <italic>et al</italic>., 2020b</xref>; <xref ref-type="bibr" rid="R20">Doelling &amp; Poeppel, 2015</xref>; <xref ref-type="bibr" rid="R31">Harding <italic>et al</italic>., 2019</xref>; but see <xref ref-type="bibr" rid="R87">Weineck <italic>et al</italic>., 2022</xref> for a null effect). We here extend these previous results by showing that stronger envelope tracking by individuals with more musical expertise also extends to atonal music with low pitch predictability. The effect of musical expertise on auditory processing of music has been related to increased auditory-motor coupling after musical training (<xref ref-type="bibr" rid="R22">Du &amp; Zatorre, 2017</xref>; <xref ref-type="bibr" rid="R76">Rimmele <italic>et al</italic>., 2021</xref>).</p><p id="P48">Another, not mutually exclusive, interpretation for the effects of both enjoyment and expertise, could be that both are associated with generally more attention to the musical stimuli. That is, more musical expertise, as well as more enjoyment of the music, could result in unspecific increases of attention, which in turn could increase the signal-to-noise ratio of acoustic envelope tracking (e.g., <xref ref-type="bibr" rid="R19">Ding &amp; Simon, 2012</xref>; <xref ref-type="bibr" rid="R43">Keitel <italic>et al</italic>., 2011</xref>; <xref ref-type="bibr" rid="R77">Rimmele <italic>et al</italic>., 2015</xref>). To disentangle contributions of attention and other variables, future studies could implement measures of attention or listening effort into their paradigms.</p></sec></sec><sec id="S30" sec-type="conclusions"><title>Conclusion</title><p id="P49">Our findings suggest that tracking of the envelope of naturalistic music, beyond rhythmic processing, is modulated by top-down factors such as pitch predictability, musical expertise, and enjoyment. In addition to the rhythm, musical pitch surprisal is tracked for both low and high predictable music. This supports the view that long-term musical pitch predictability is processed in the brain and used to facilitate rhythm processing. For tonal, more predictable music, the ability to make valid pitch predictions seems to facilitate the ability to behaviourally follow the rhythm. For atonal music, the reduced pitch predictability results in stronger acoustic envelope tracking than for tonal music, possibly related to higher prediction errors. These higher prediction errors also seem to come at the cost of finger-tapping performance, as individuals with stronger pitch surprisal tracking show worse behavioural rhythm tracking. Overall, our findings indicate that rhythm processing interacts with non-rhythmic stimulus properties, in our case pitch surprisal, and listeners’ characteristics such as music expertise and enjoyment.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Material</label><media xlink:href="EMS189710-supplement-Supplementary_Material.pdf" mimetype="application" mime-subtype="pdf" id="d17aAcFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S31"><title>Acknowledgments</title><p>AK is supported by the Medical Research Council [grant number MR/W02912X/1]. AK, CK and JR are members of the Scottish-EU Critical Oscillations Network (SCONe), funded by the Royal Society of Edinburgh (RSE Saltire Facilitation Network Award to CK and AK, Reference Number 1963). JR is supported by the Max Planck Institute for Empirical Aesthetics. IM is funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; project number 510788453). JR, CP and XG are supported by the Max Planck NYU Center for Language, Music, and Emotion (CLaME). We thank Roddy Easson for providing the ‘tapping person’ drawing.</p></ack><fn-group><fn id="FN2"><p id="P50">This is a revised manuscript. An overview of the changes can be found here: <ext-link ext-link-type="uri" xlink:href="https://osf.io/wvsjr">https://osf.io/wvsjr</ext-link>.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abel</surname><given-names>S</given-names></name><name><surname>Dressel</surname><given-names>K</given-names></name><name><surname>Bitzer</surname><given-names>R</given-names></name><name><surname>Kummerer</surname><given-names>D</given-names></name><name><surname>Mader</surname><given-names>I</given-names></name><name><surname>Weiller</surname><given-names>C</given-names></name><name><surname>Huber</surname><given-names>W</given-names></name></person-group><article-title>The separation of processing stages in a lexical interference fMRI-paradigm</article-title><source>Neuroimage</source><year>2009</year><volume>44</volume><issue>3</issue><fpage>1113</fpage><lpage>1124</lpage><pub-id pub-id-type="pmid">19015036</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Abrams</surname><given-names>EB</given-names></name><name><surname>Vidal</surname><given-names>EM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Ripollés</surname><given-names>P</given-names></name></person-group><source>Retrieving musical information from neural data: how cognitive features enrich acoustic ones</source><conf-name>Ismir 2022 Hybrid Conference</conf-name><year>2022</year></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrou</surname><given-names>A</given-names></name><name><surname>Saarinen</surname><given-names>T</given-names></name><name><surname>Kujala</surname><given-names>J</given-names></name><name><surname>Salmelin</surname><given-names>R</given-names></name></person-group><article-title>Cortical entrainment: what we can learn from studying naturalistic speech perception</article-title><source>Language, Cognition and Neuroscience</source><year>2018</year><fpage>1</fpage><lpage>13</lpage></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arachchige</surname><given-names>C</given-names></name><name><surname>Prendergast</surname><given-names>LA</given-names></name><name><surname>Staudte</surname><given-names>RG</given-names></name></person-group><article-title>Robust analogs to the coefficient of variation</article-title><source>J Appl Stat</source><year>2022</year><volume>49</volume><issue>2</issue><fpage>268</fpage><lpage>290</lpage><pub-id pub-id-type="pmcid">PMC9196089</pub-id><pub-id pub-id-type="pmid">35707217</pub-id><pub-id pub-id-type="doi">10.1080/02664763.2020.1808599</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assaneo</surname><given-names>MF</given-names></name><name><surname>Rimmele</surname><given-names>JM</given-names></name><name><surname>Orpella</surname><given-names>J</given-names></name><name><surname>Ripolles</surname><given-names>P</given-names></name><name><surname>de Diego-Balaguer</surname><given-names>R</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The Lateralization of Speech-Brain Coupling Is Differentially Modulated by Intrinsic Auditory and Top-Down Mechanisms</article-title><source>Front Integr Neurosci</source><year>2019</year><volume>13</volume><fpage>28</fpage><pub-id pub-id-type="pmcid">PMC6650591</pub-id><pub-id pub-id-type="pmid">31379527</pub-id><pub-id pub-id-type="doi">10.3389/fnint.2019.00028</pub-id></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auksztulewicz</surname><given-names>R</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>Repetition suppression and its contextual determinants in predictive coding</article-title><source>cortex</source><year>2016</year><volume>80</volume><fpage>125</fpage><lpage>140</lpage><pub-id pub-id-type="pmcid">PMC5405056</pub-id><pub-id pub-id-type="pmid">26861557</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2015.11.024</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><article-title>Controlling the False Discovery Rate - a Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society Series B-Methodological</source><year>1995</year><volume>57</volume><issue>1</issue><fpage>289</fpage><lpage>300</lpage><comment>&lt;Go to ISI&gt;://WOS:A1995QE45300017</comment></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanco-Elorrieta</surname><given-names>E</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Pylkkänen</surname><given-names>L</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Understanding requires tracking: noise and knowledge interact in bilingual comprehension</article-title><source>Journal of cognitive neuroscience</source><year>2020</year><volume>32</volume><issue>10</issue><fpage>1975</fpage><lpage>1983</lpage><pub-id pub-id-type="pmid">32662732</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brohl</surname><given-names>F</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Delta/theta band EEG differentially tracks low and high frequency speech-derived envelopes</article-title><source>Neuroimage</source><year>2021</year><volume>233</volume><elocation-id>117958</elocation-id><pub-id pub-id-type="pmcid">PMC8204264</pub-id><pub-id pub-id-type="pmid">33744458</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2021.117958</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brohl</surname><given-names>F</given-names></name><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>MEG Activity in Visual and Auditory Cortices Represents Acoustic Speech-Related Information during Silent Lip Reading</article-title><source>eNeuro</source><year>2022</year><volume>9</volume><issue>3</issue><pub-id pub-id-type="pmcid">PMC9239847</pub-id><pub-id pub-id-type="pmid">35728955</pub-id><pub-id pub-id-type="doi">10.1523/ENEURO.0209-22.2022</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celma-Miralles</surname><given-names>A</given-names></name><name><surname>Toro</surname><given-names>JM</given-names></name></person-group><article-title>Ternary meter from spatial sounds: Differences in neural entrainment between musicians and non-musicians</article-title><source>Brain Cogn</source><year>2019</year><volume>136</volume><elocation-id>103594</elocation-id><pub-id pub-id-type="pmid">31415948</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cervantes Constantino</surname><given-names>F</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Restoration and efficiency of the neural processing of continuous speech are promoted by prior knowledge</article-title><source>Frontiers in systems neuroscience</source><year>2018</year><volume>12</volume><fpage>56</fpage><pub-id pub-id-type="pmcid">PMC6220042</pub-id><pub-id pub-id-type="pmid">30429778</pub-id><pub-id pub-id-type="doi">10.3389/fnsys.2018.00056</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuddy</surname><given-names>LL</given-names></name><name><surname>Cohen</surname><given-names>AJ</given-names></name><name><surname>Mewhort</surname><given-names>DJ</given-names></name></person-group><article-title>Perception of structure in short melodic sequences</article-title><source>J Exp Psychol Hum Percept Perform</source><year>1981</year><volume>7</volume><issue>4</issue><fpage>869</fpage><lpage>883</lpage><pub-id pub-id-type="pmid">6457099</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name><name><surname>Millman</surname><given-names>RE</given-names></name></person-group><article-title>Causal cortical dynamics of a predictive enhancement of speech intelligibility</article-title><source>Neuroimage</source><year>2018</year><volume>166</volume><fpage>247</fpage><lpage>258</lpage><pub-id pub-id-type="pmid">29102808</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Bianco</surname><given-names>R</given-names></name><name><surname>Patel</surname><given-names>P</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Herrero</surname><given-names>JL</given-names></name><etal/><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title><source>Elife</source><year>2020a</year><volume>9</volume><elocation-id>e51784</elocation-id><pub-id pub-id-type="pmcid">PMC7053998</pub-id><pub-id pub-id-type="pmid">32122465</pub-id><pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name></person-group><article-title>Musical expertise enhances the cortical tracking of the acoustic envelope during naturalistic music listening</article-title><source>Acoustical Science and Technology</source><year>2020b</year><volume>41</volume><issue>1</issue><fpage>361</fpage><lpage>364</lpage></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dibben</surname><given-names>N</given-names></name></person-group><article-title>The cognitive reality of hierarchic structure in tonal and atonal music</article-title><source>Music Perception</source><year>1994</year><volume>12</volume><issue>1</issue><fpage>1</fpage><lpage>25</lpage></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Temporal modulations in speech and music</article-title><source>Neurosci Biobehav Rev</source><year>2017</year><volume>81</volume><issue>Pt B</issue><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><article-title>Emergence of neural encoding of auditory objects while listening to competing speakers</article-title><source>Proc Natl Acad Sci U S A</source><year>2012</year><volume>109</volume><issue>29</issue><fpage>11854</fpage><lpage>11859</lpage><pub-id pub-id-type="pmcid">PMC3406818</pub-id><pub-id pub-id-type="pmid">22753470</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1205381109</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Cortical entrainment to music and its modulation by expertise</article-title><source>Proceedings of the National Academy of Sciences</source><year>2015</year><volume>112</volume><issue>45</issue><fpage>E6233</fpage><lpage>E6242</lpage><pub-id pub-id-type="pmcid">PMC4653203</pub-id><pub-id pub-id-type="pmid">26504238</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1508431112</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dowling</surname><given-names>WJ</given-names></name><name><surname>Kwak</surname><given-names>S</given-names></name><name><surname>Andrews</surname><given-names>MW</given-names></name></person-group><article-title>The time course of recognition of novel melodies</article-title><source>Perception &amp; Psychophysics</source><year>1995</year><volume>57</volume><fpage>136</fpage><lpage>149</lpage><pub-id pub-id-type="pmid">7885812</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Musical training sharpens and bonds ears and tongue to hear speech better</article-title><source>Proc Natl Acad Sci U S A</source><year>2017</year><volume>114</volume><issue>51</issue><fpage>13579</fpage><lpage>13584</lpage><pub-id pub-id-type="pmcid">PMC5754781</pub-id><pub-id pub-id-type="pmid">29203648</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1712223114</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Singer</surname><given-names>W</given-names></name></person-group><article-title>Dynamic predictions: oscillations and synchrony in top-down processing</article-title><source>Nat Rev Neurosci</source><year>2001</year><volume>2</volume><issue>10</issue><fpage>704</fpage><lpage>716</lpage><pub-id pub-id-type="pmid">11584308</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiveash</surname><given-names>A</given-names></name><name><surname>Bella</surname><given-names>SD</given-names></name><name><surname>Bigand</surname><given-names>E</given-names></name><name><surname>Gordon</surname><given-names>RL</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><article-title>You got rhythm, or more: The multidimensionality of rhythmic abilities</article-title><source>Atten Percept Psychophys</source><year>2022</year><volume>84</volume><issue>4</issue><fpage>1370</fpage><lpage>1392</lpage><pub-id pub-id-type="pmcid">PMC9614186</pub-id><pub-id pub-id-type="pmid">35437703</pub-id><pub-id pub-id-type="doi">10.3758/s13414-022-02487-2</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Mehta</surname><given-names>AD</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Spectrotemporal modulation provides a unifying framework for auditory cortical asymmetries</article-title><source>Nat Hum Behav</source><year>2019</year><volume>3</volume><issue>4</issue><fpage>393</fpage><lpage>405</lpage><pub-id pub-id-type="pmcid">PMC6650286</pub-id><pub-id pub-id-type="pmid">30971792</pub-id><pub-id pub-id-type="doi">10.1038/s41562-019-0548-z</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>The free-energy principle: a unified brain theory?</article-title><source>Nat Rev Neurosci</source><year>2010</year><volume>11</volume><issue>2</issue><fpage>127</fpage><lpage>138</lpage><pub-id pub-id-type="pmid">20068583</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>BP</given-names></name><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Mas-Herrero</surname><given-names>E</given-names></name><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Predictability and uncertainty in the pleasure of music: a reward for learning?</article-title><source>Journal of Neuroscience</source><year>2019</year><volume>39</volume><issue>47</issue><fpage>9397</fpage><lpage>9409</lpage><pub-id pub-id-type="pmcid">PMC6867811</pub-id><pub-id pub-id-type="pmid">31636112</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0428-19.2019</pub-id></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorsuch</surname><given-names>RL</given-names></name><name><surname>Lehmann</surname><given-names>CS</given-names></name></person-group><article-title>Correlation coefficients: Mean bias and confidence interval distortions</article-title><source>Journal of Methods and Measurement in the Social Sciences</source><year>2010</year><volume>1</volume><issue>2</issue><fpage>52</fpage><lpage>65</lpage></element-citation></ref><ref id="R29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Hoogenboom</surname><given-names>N</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Schyns</surname><given-names>P</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Garrod</surname><given-names>S</given-names></name></person-group><article-title>Speech rhythms and multiplexed oscillatory sensory coding in the human brain</article-title><source>PLoS Biol</source><year>2013</year><volume>11</volume><issue>12</issue><elocation-id>e1001752</elocation-id><pub-id pub-id-type="pmcid">PMC3876971</pub-id><pub-id pub-id-type="pmid">24391472</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001752</pub-id></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guan</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>Z</given-names></name><name><surname>Pelofi</surname><given-names>C</given-names></name></person-group><article-title>py2lispIDyOM: A Python package for the information dynamics of music (IDyOM) model</article-title><source>Journal of Open Source Software</source><year>2022</year><volume>7</volume><issue>79</issue><fpage>4738</fpage></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harding</surname><given-names>EE</given-names></name><name><surname>Sammler</surname><given-names>D</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name><name><surname>Large</surname><given-names>EW</given-names></name><name><surname>Kotz</surname><given-names>SA</given-names></name></person-group><article-title>Cortical tracking of rhythm in music and speech</article-title><source>Neuroimage</source><year>2019</year><volume>185</volume><fpage>96</fpage><lpage>101</lpage><pub-id pub-id-type="pmid">30336253</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>Chait</surname><given-names>M</given-names></name></person-group><article-title>Great Expectations: Is there Evidence for Predictive Coding in Auditory Cortex?</article-title><source>Neuroscience</source><year>2018</year><volume>389</volume><fpage>54</fpage><lpage>73</lpage><pub-id pub-id-type="pmid">28782642</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herbst</surname><given-names>SK</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name></person-group><article-title>Implicit temporal predictability enhances pitch discrimination sensitivity and biases the phase of delta oscillations in auditory cortex</article-title><source>Neuroimage</source><year>2019</year><volume>203</volume><elocation-id>116198</elocation-id><pub-id pub-id-type="pmid">31539590</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname><given-names>MF</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Discrimination of speech stimuli based on neuronal response phase patterns depends on acoustics but not comprehension</article-title><source>J Neurophysiol</source><year>2010</year><volume>104</volume><issue>5</issue><fpage>2500</fpage><lpage>2511</lpage><pub-id pub-id-type="pmcid">PMC2997028</pub-id><pub-id pub-id-type="pmid">20484530</pub-id><pub-id pub-id-type="doi">10.1152/jn.00251.2010</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huron</surname><given-names>D</given-names></name></person-group><source>Sweet anticipation: Music and the psychology of expectation</source><publisher-name>MIT press</publisher-name><year>2008</year></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title><source>Hum Brain Mapp</source><year>2017</year><volume>38</volume><issue>3</issue><fpage>1541</fpage><lpage>1573</lpage><pub-id pub-id-type="pmcid">PMC5324576</pub-id><pub-id pub-id-type="pmid">27860095</pub-id><pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iversen</surname><given-names>JR</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Nicodemus</surname><given-names>B</given-names></name><name><surname>Emmorey</surname><given-names>K</given-names></name></person-group><article-title>Synchronization to auditory and visual rhythms in hearing and deaf individuals</article-title><source>Cognition</source><year>2015</year><volume>134</volume><fpage>232</fpage><lpage>244</lpage><pub-id pub-id-type="pmcid">PMC4255154</pub-id><pub-id pub-id-type="pmid">25460395</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2014.10.018</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>MR</given-names></name><name><surname>Moynihan</surname><given-names>H</given-names></name><name><surname>MacKenzie</surname><given-names>N</given-names></name><name><surname>Puente</surname><given-names>J</given-names></name></person-group><article-title>Temporal aspects of stimulus-driven attending in dynamic arrays</article-title><source>Psychol Sci</source><year>2002</year><volume>13</volume><issue>4</issue><fpage>313</fpage><lpage>319</lpage><pub-id pub-id-type="pmid">12137133</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLoS Biol</source><year>2018</year><volume>16</volume><issue>3</issue><elocation-id>e2004473</elocation-id><pub-id pub-id-type="pmcid">PMC5864086</pub-id><pub-id pub-id-type="pmid">29529019</pub-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Shared and modality-specific brain regions that mediate auditory and visual word comprehension</article-title><source>Elife</source><year>2020</year><volume>9</volume><pub-id pub-id-type="pmcid">PMC7470824</pub-id><pub-id pub-id-type="pmid">32831168</pub-id><pub-id pub-id-type="doi">10.7554/eLife.56972</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>A</given-names></name><name><surname>Ince</surname><given-names>RA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks</article-title><source>Neuroimage</source><year>2017</year><volume>147</volume><fpage>32</fpage><lpage>42</lpage><pub-id pub-id-type="pmcid">PMC5315055</pub-id><pub-id pub-id-type="pmid">27903440</pub-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.11.062</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>C</given-names></name><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Jessen</surname><given-names>S</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><article-title>Frequency-Specific Effects in Infant Electroencephalograms Do Not Require Entrained Neural Oscillations: A Commentary on Koster et al. (2019)</article-title><source>Psychol Sci</source><year>2021</year><volume>32</volume><issue>6</issue><fpage>966</fpage><lpage>971</lpage><pub-id pub-id-type="pmid">33979246</pub-id></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname><given-names>C</given-names></name><name><surname>Schroger</surname><given-names>E</given-names></name><name><surname>Saupe</surname><given-names>K</given-names></name><name><surname>Muller</surname><given-names>MM</given-names></name></person-group><article-title>Sustained selective intermodal attention modulates processing of language-like stimuli</article-title><source>Exp Brain Res</source><year>2011</year><volume>213</volume><issue>2-3</issue><fpage>321</fpage><lpage>327</lpage><pub-id pub-id-type="pmid">21503650</pub-id></element-citation></ref><ref id="R44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kern</surname><given-names>P</given-names></name><name><surname>Heilbron</surname><given-names>M</given-names></name><name><surname>de Lange</surname><given-names>FP</given-names></name><name><surname>Spaak</surname><given-names>E</given-names></name></person-group><article-title>Cortical activity during naturalistic music listening reflects short-range predictions based on long-term experience</article-title><source>eLife</source><year>2022</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC9836393</pub-id><pub-id pub-id-type="pmid">36562532</pub-id><pub-id pub-id-type="doi">10.7554/eLife.80935</pub-id></element-citation></ref><ref id="R45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koelsch</surname><given-names>S</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><article-title>Predictive Processes and the Peculiar Case of Music</article-title><source>Trends Cogn Sci</source><year>2019</year><volume>23</volume><issue>1</issue><fpage>63</fpage><lpage>77</lpage><pub-id pub-id-type="pmid">30471869</pub-id></element-citation></ref><ref id="R46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koike</surname><given-names>KJ</given-names></name><name><surname>Hurst</surname><given-names>MK</given-names></name><name><surname>Wetmore</surname><given-names>SJ</given-names></name></person-group><article-title>Correlation between the American-Academy-of-Otolaryngology-Head-and-Neck-Surgery 5-minute hearing test and standard audiological data</article-title><source>Otolaryngology-Head and Neck Surgery</source><year>1994</year><volume>111</volume><issue>5</issue><fpage>625</fpage><lpage>632</lpage><comment>&lt;Go to ISI&gt;://A1994PR79400014</comment><pub-id pub-id-type="pmid">7970802</pub-id></element-citation></ref><ref id="R47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><article-title>Calculating and reporting effect sizes to facilitate cumulative science: a practical primer for t-tests and ANOVAs</article-title><source>Front Psychol</source><year>2013</year><volume>4</volume><fpage>863</fpage><pub-id pub-id-type="pmcid">PMC3840331</pub-id><pub-id pub-id-type="pmid">24324449</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00863</pub-id></element-citation></ref><ref id="R48"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lartillot</surname><given-names>O</given-names></name><name><surname>Toiviainen</surname><given-names>P</given-names></name><name><surname>Eerola</surname><given-names>T</given-names></name></person-group><chapter-title>A Matlab Toolbox for Music Information Retrieval</chapter-title><person-group person-group-type="editor"><name><surname>Preisach</surname><given-names>C</given-names></name><name><surname>Burkhardt</surname><given-names>H</given-names></name><name><surname>Schmidt-Thieme</surname><given-names>L</given-names></name><name><surname>Decker</surname><given-names>R</given-names></name></person-group><source>Data Analysis, Machine Learning and Applications</source><publisher-name>Springer-Verlag</publisher-name><year>2008</year></element-citation></ref><ref id="R49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lerdahl</surname><given-names>F</given-names></name></person-group><source>Composition and cognition: Reflections on contemporary music and the musical mind</source><publisher-name>University of California Press</publisher-name><year>2019</year></element-citation></ref><ref id="R50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lesenfants</surname><given-names>D</given-names></name><name><surname>Francart</surname><given-names>T</given-names></name></person-group><article-title>The interplay of top-down focal attention and the cortical tracking of speech</article-title><source>Scientific reports</source><year>2020</year><volume>10</volume><issue>1</issue><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="pmcid">PMC7181730</pub-id><pub-id pub-id-type="pmid">32332791</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-63587-3</pub-id></element-citation></ref><ref id="R51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leys</surname><given-names>C</given-names></name><name><surname>Ley</surname><given-names>C</given-names></name><name><surname>Klein</surname><given-names>O</given-names></name><name><surname>Bernard</surname><given-names>P</given-names></name><name><surname>Licata</surname><given-names>L</given-names></name></person-group><article-title>Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median</article-title><source>Journal of experimental social psychology</source><year>2013</year><volume>49</volume><issue>4</issue><fpage>764</fpage><lpage>766</lpage></element-citation></ref><ref id="R52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><year>2007</year><volume>54</volume><issue>6</issue><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="pmcid">PMC2703451</pub-id><pub-id pub-id-type="pmid">17582338</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id></element-citation></ref><ref id="R53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marion</surname><given-names>G</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><article-title>The Music of Silence: Part I: Responses to Musical Imagery Encode Melodic Expectations and Acoustics</article-title><source>Journal of Neuroscience</source><year>2021</year><volume>41</volume><issue>35</issue><fpage>7435</fpage><lpage>7448</lpage><pub-id pub-id-type="pmcid">PMC8412990</pub-id><pub-id pub-id-type="pmid">34341155</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0183-21.2021</pub-id></element-citation></ref><ref id="R54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mencke</surname><given-names>I</given-names></name><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name></person-group><article-title>Atonal Music as a Model for Investigating Exploratory Behavior</article-title><source>Frontiers in Neuroscience</source><year>2022</year><volume>16</volume><elocation-id>793163</elocation-id><pub-id pub-id-type="pmcid">PMC9256982</pub-id><pub-id pub-id-type="pmid">35812236</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.793163</pub-id></element-citation></ref><ref id="R55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mencke</surname><given-names>I</given-names></name><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Wald-Fuhrmann</surname><given-names>M</given-names></name><name><surname>Brattico</surname><given-names>E</given-names></name></person-group><article-title>Atonal Music: Can Uncertainty Lead to Pleasure?</article-title><source>Front Neurosci</source><year>2018</year><volume>12</volume><fpage>979</fpage><pub-id pub-id-type="pmcid">PMC6331456</pub-id><pub-id pub-id-type="pmid">30670941</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00979</pub-id></element-citation></ref><ref id="R56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mencke</surname><given-names>I</given-names></name><name><surname>Quiroga-Martinez</surname><given-names>DR</given-names></name><name><surname>Omigie</surname><given-names>D</given-names></name><name><surname>Michalareas</surname><given-names>G</given-names></name><name><surname>Schwarzacher</surname><given-names>F</given-names></name><name><surname>Haumann</surname><given-names>NT</given-names></name><etal/><name><surname>Brattico</surname><given-names>E</given-names></name></person-group><article-title>Prediction under uncertainty: Dissociating sensory from cognitive expectations in highly uncertain musical contexts</article-title><source>Brain Res</source><year>2021</year><volume>1773</volume><elocation-id>147664</elocation-id><pub-id pub-id-type="pmid">34560052</pub-id></element-citation></ref><ref id="R57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morillon</surname><given-names>B</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Keitel</surname><given-names>A</given-names></name></person-group><article-title>Prominence of delta oscillatory rhythms in the motor cortex and their relevance for auditory and speech perception</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><year>2019</year><pub-id pub-id-type="pmid">31518638</pub-id></element-citation></ref><ref id="R58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naatanen</surname><given-names>R</given-names></name><name><surname>Paavilainen</surname><given-names>P</given-names></name><name><surname>Rinne</surname><given-names>T</given-names></name><name><surname>Alho</surname><given-names>K</given-names></name></person-group><article-title>The mismatch negativity (MMN) in basic research of central auditory processing: a review</article-title><source>Clin Neurophysiol</source><year>2007</year><volume>118</volume><issue>12</issue><fpage>2544</fpage><lpage>2590</lpage><pub-id pub-id-type="pmid">17931964</pub-id></element-citation></ref><ref id="R59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neuloh</surname><given-names>G</given-names></name><name><surname>Curio</surname><given-names>G</given-names></name></person-group><article-title>Does familiarity facilitate the cortical processing of music sounds?</article-title><source>Neuroreport</source><year>2004</year><volume>15</volume><issue>16</issue><fpage>2471</fpage><lpage>2475</lpage><pub-id pub-id-type="pmid">15538177</pub-id></element-citation></ref><ref id="R60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nozaradan</surname><given-names>S</given-names></name><name><surname>Peretz</surname><given-names>I</given-names></name><name><surname>Mouraux</surname><given-names>A</given-names></name></person-group><article-title>Selective neuronal entrainment to the beat and meter embedded in a musical rhythm</article-title><source>J Neurosci</source><year>2012</year><volume>32</volume><issue>49</issue><fpage>17572</fpage><lpage>17581</lpage><pub-id pub-id-type="pmcid">PMC6621650</pub-id><pub-id pub-id-type="pmid">23223281</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3203-12.2012</pub-id></element-citation></ref><ref id="R61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname><given-names>J</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Neural Entrainment and Attentional Selection in the Listening Brain</article-title><source>Trends in cognitive sciences</source><year>2019</year><volume>23</volume><issue>11</issue><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="pmid">31606386</pub-id></element-citation></ref><ref id="R62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ockelford</surname><given-names>A</given-names></name><name><surname>Sergeant</surname><given-names>D</given-names></name></person-group><article-title>Musical expectancy in atonal contexts: Musicians’ perception of “antistructure”</article-title><source>Psychology of Music</source><year>2013</year><volume>41</volume><issue>2</issue><fpage>139</fpage><lpage>174</lpage></element-citation></ref><ref id="R63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oldfield</surname><given-names>RC</given-names></name></person-group><article-title>The assessment and analysis of handedness: the Edinburgh inventory</article-title><source>Neuropsychologia</source><year>1971</year><volume>9</volume><issue>1</issue><fpage>97</fpage><lpage>113</lpage><pub-id pub-id-type="pmid">5146491</pub-id></element-citation></ref><ref id="R64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name><name><surname>Maris</surname><given-names>E</given-names></name><name><surname>Schoffelen</surname><given-names>JM</given-names></name></person-group><article-title>FieldTrip: Open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data</article-title><source>Comput Intell Neurosci</source><year>2011</year><volume>2011</volume><elocation-id>156869</elocation-id><pub-id pub-id-type="pmcid">PMC3021840</pub-id><pub-id pub-id-type="pmid">21253357</pub-id><pub-id pub-id-type="doi">10.1155/2011/156869</pub-id></element-citation></ref><ref id="R65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name></person-group><article-title>Shared neural underpinnings of multisensory integration and trial-by-trial perceptual recalibration in humans</article-title><source>Elife</source><year>2019</year><volume>8</volume><pub-id pub-id-type="pmcid">PMC6660215</pub-id><pub-id pub-id-type="pmid">31246172</pub-id><pub-id pub-id-type="doi">10.7554/eLife.47001</pub-id></element-citation></ref><ref id="R66"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><source>The construction and evaluation of statistical models of melodic structure in music perception and composition</source><year>2005</year><comment><ext-link ext-link-type="uri" xlink:href="https://openaccess.city.ac.uk/id/eprint/8459">https://openaccess.city.ac.uk/id/eprint/8459</ext-link></comment></element-citation></ref><ref id="R67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name></person-group><article-title>Statistical learning and probabilistic prediction in music cognition: mechanisms of stylistic enculturation</article-title><source>Ann N Y Acad Sci</source><year>2018</year><volume>1423</volume><issue>1</issue><fpage>378</fpage><lpage>395</lpage><pub-id pub-id-type="pmcid">PMC6849749</pub-id><pub-id pub-id-type="pmid">29749625</pub-id><pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id></element-citation></ref><ref id="R68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Ruiz</surname><given-names>MH</given-names></name><name><surname>Kapasi</surname><given-names>S</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name><name><surname>Bhattacharya</surname><given-names>J</given-names></name></person-group><article-title>Unsupervised statistical learning underpins computational, behavioural, and neural manifestations of musical expectation</article-title><source>Neuroimage</source><year>2010</year><volume>50</volume><issue>1</issue><fpage>302</fpage><lpage>313</lpage><pub-id pub-id-type="pmid">20005297</pub-id></element-citation></ref><ref id="R69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name></person-group><article-title>Expectation in Melody: The Influence of Context and Learning</article-title><source>Music Perception</source><year>2006</year><volume>23</volume><issue>5</issue><fpage>377</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1525/mp.2006.23.5.377</pub-id></element-citation></ref><ref id="R70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname><given-names>MT</given-names></name><name><surname>Wiggins</surname><given-names>GA</given-names></name></person-group><article-title>Auditory expectation: the information dynamics of music perception and cognition</article-title><source>Top Cogn Sci</source><year>2012</year><volume>4</volume><issue>4</issue><fpage>625</fpage><lpage>652</lpage><pub-id pub-id-type="pmid">22847872</pub-id></element-citation></ref><ref id="R71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><article-title>Phase-locked responses to speech in human auditory cortex are enhanced during comprehension</article-title><source>Cereb Cortex</source><year>2013</year><volume>23</volume><issue>6</issue><fpage>1378</fpage><lpage>1387</lpage><pub-id pub-id-type="pmcid">PMC3643716</pub-id><pub-id pub-id-type="pmid">22610394</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhs118</pub-id></element-citation></ref><ref id="R72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pefkou</surname><given-names>M</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Fontolan</surname><given-names>L</given-names></name><name><surname>Giraud</surname><given-names>AL</given-names></name></person-group><article-title>theta-band and beta-band neural activity reflects independent syllable tracking and comprehension of time-compressed speech</article-title><source>J Neurosci</source><year>2017</year><volume>37</volume><issue>33</issue><fpage>7930</fpage><lpage>7938</lpage><pub-id pub-id-type="pmcid">PMC6596908</pub-id><pub-id pub-id-type="pmid">28729443</pub-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2882-16.2017</pub-id></element-citation></ref><ref id="R73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as 'asymmetric sampling in time'</article-title><source>Speech Communication</source><year>2003</year><volume>41</volume><issue>1</issue><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="R74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reetzke</surname><given-names>R</given-names></name><name><surname>Gnanateja</surname><given-names>GN</given-names></name><name><surname>Chandrasekaran</surname><given-names>B</given-names></name></person-group><article-title>Neural tracking of the speech envelope is differentially modulated by attention and language experience</article-title><source>Brain Lang</source><year>2021</year><volume>213</volume><elocation-id>104891</elocation-id><pub-id pub-id-type="pmcid">PMC7856208</pub-id><pub-id pub-id-type="pmid">33290877</pub-id><pub-id pub-id-type="doi">10.1016/j.bandl.2020.104891</pub-id></element-citation></ref><ref id="R75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Repp</surname><given-names>BH</given-names></name></person-group><article-title>Sensorimotor synchronization: a review of the tapping literature</article-title><source>Psychon Bull Rev</source><year>2005</year><volume>12</volume><issue>6</issue><fpage>969</fpage><lpage>992</lpage><pub-id pub-id-type="pmid">16615317</pub-id></element-citation></ref><ref id="R76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname><given-names>JM</given-names></name><name><surname>Kern</surname><given-names>P</given-names></name><name><surname>Lubinus</surname><given-names>C</given-names></name><name><surname>Frieler</surname><given-names>K</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Assaneo</surname><given-names>MF</given-names></name></person-group><article-title>Musical Sophistication and Speech Auditory-Motor Coupling: Easy Tests for Quick Answers</article-title><source>Front Neurosci</source><year>2021</year><volume>15</volume><elocation-id>764342</elocation-id><pub-id pub-id-type="pmcid">PMC8763673</pub-id><pub-id pub-id-type="pmid">35058741</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2021.764342</pub-id></element-citation></ref><ref id="R77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname><given-names>JM</given-names></name><name><surname>Zion Golumbic</surname><given-names>E</given-names></name><name><surname>Schroger</surname><given-names>E</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>The effects of selective attention and speech acoustics on neural speech-tracking in a multi-talker scene</article-title><source>Cortex</source><year>2015</year><volume>68</volume><fpage>144</fpage><lpage>154</lpage><pub-id pub-id-type="pmcid">PMC4475476</pub-id><pub-id pub-id-type="pmid">25650107</pub-id><pub-id pub-id-type="doi">10.1016/j.cortex.2014.12.014</pub-id></element-citation></ref><ref id="R78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rovetti</surname><given-names>J</given-names></name><name><surname>Copelli</surname><given-names>F</given-names></name><name><surname>Russo</surname><given-names>FA</given-names></name></person-group><article-title>Audio and visual speech emotion activate the left pre-supplementary motor area</article-title><source>Cogn Affect Behav Neurosci</source><year>2022</year><volume>22</volume><issue>2</issue><fpage>291</fpage><lpage>303</lpage><pub-id pub-id-type="pmid">34811708</pub-id></element-citation></ref><ref id="R79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sammler</surname><given-names>D</given-names></name></person-group><article-title>Splitting speech and music</article-title><source>science</source><year>2020</year><volume>367</volume><issue>6481</issue><fpage>974</fpage><lpage>976</lpage><pub-id pub-id-type="pmid">32108099</pub-id></element-citation></ref><ref id="R80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitt</surname><given-names>R</given-names></name><name><surname>Meyer</surname><given-names>M</given-names></name><name><surname>Giroud</surname><given-names>N</given-names></name></person-group><article-title>Better speech-in-noise comprehension is associated with enhanced neural speech tracking in older adults with hearing impairment</article-title><source>cortex</source><year>2022</year><volume>151</volume><fpage>133</fpage><lpage>146</lpage><pub-id pub-id-type="pmid">35405539</pub-id></element-citation></ref><ref id="R81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schubert</surname><given-names>J</given-names></name><name><surname>Schmidt</surname><given-names>F</given-names></name><name><surname>Gehmacher</surname><given-names>Q</given-names></name><name><surname>Bresgen</surname><given-names>A</given-names></name><name><surname>Weisz</surname><given-names>N</given-names></name></person-group><article-title>Cortical speech tracking is related to individual prediction tendencies</article-title><source>Cereb Cortex</source><year>2023</year><volume>33</volume><issue>11</issue><fpage>6608</fpage><lpage>6619</lpage><pub-id pub-id-type="pmcid">PMC10233232</pub-id><pub-id pub-id-type="pmid">36617790</pub-id><pub-id pub-id-type="doi">10.1093/cercor/bhac528</pub-id></element-citation></ref><ref id="R82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulze</surname><given-names>K</given-names></name><name><surname>Jay Dowling</surname><given-names>W</given-names></name><name><surname>Tillmann</surname><given-names>B</given-names></name></person-group><article-title>Working memory for tonal and atonal sequences during a forward and a backward recognition task</article-title><source>Music Perception</source><year>2011</year><volume>29</volume><issue>3</issue><fpage>255</fpage><lpage>267</lpage></element-citation></ref><ref id="R83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>ZM</given-names></name><name><surname>Delgutte</surname><given-names>B</given-names></name><name><surname>Oxenham</surname><given-names>AJ</given-names></name></person-group><article-title>Chimaeric sounds reveal dichotomies in auditory perception</article-title><source>Nature</source><year>2002</year><volume>416</volume><issue>6876</issue><fpage>87</fpage><lpage>90</lpage><pub-id pub-id-type="pmcid">PMC2268248</pub-id><pub-id pub-id-type="pmid">11882898</pub-id><pub-id pub-id-type="doi">10.1038/416087a</pub-id></element-citation></ref><ref id="R84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>J</given-names></name><name><surname>Iverson</surname><given-names>P</given-names></name></person-group><article-title>Listening effort during speech perception enhances auditory and lexical processing for non-native listeners and accents</article-title><source>Cognition</source><year>2018</year><volume>179</volume><fpage>163</fpage><lpage>170</lpage><pub-id pub-id-type="pmid">29957515</pub-id></element-citation></ref><ref id="R85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tierney</surname><given-names>A</given-names></name><name><surname>Kraus</surname><given-names>N</given-names></name></person-group><article-title>Neural entrainment to the rhythmic structure of music</article-title><source>J Cogn Neurosci</source><year>2015</year><volume>27</volume><issue>2</issue><fpage>400</fpage><lpage>408</lpage><pub-id pub-id-type="pmid">25170794</pub-id></element-citation></ref><ref id="R86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vuvan</surname><given-names>DT</given-names></name><name><surname>Podolak</surname><given-names>OM</given-names></name><name><surname>Schmuckler</surname><given-names>MA</given-names></name></person-group><article-title>Memory for musical tones: the impact of tonality and the creation of false memories</article-title><source>Front Psychol</source><year>2014</year><volume>5</volume><fpage>582</fpage><pub-id pub-id-type="pmcid">PMC4054327</pub-id><pub-id pub-id-type="pmid">24971071</pub-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.00582</pub-id></element-citation></ref><ref id="R87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weineck</surname><given-names>K</given-names></name><name><surname>Wen</surname><given-names>OX</given-names></name><name><surname>Henry</surname><given-names>MJ</given-names></name></person-group><article-title>Neural synchronization is strongest to the spectral flux of slow music and depends on familiarity and beat salience</article-title><source>eLife</source><year>2022</year><volume>11</volume><pub-id pub-id-type="pmcid">PMC9467512</pub-id><pub-id pub-id-type="pmid">36094165</pub-id><pub-id pub-id-type="doi">10.7554/eLife.75515</pub-id></element-citation></ref><ref id="R88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zarate</surname><given-names>JM</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Woods</surname><given-names>KJ</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><article-title>Multiple levels of linguistic and paralinguistic features contribute to voice recognition</article-title><source>Sci Rep</source><year>2015</year><volume>5</volume><elocation-id>11475</elocation-id><pub-id pub-id-type="pmcid">PMC4473599</pub-id><pub-id pub-id-type="pmid">26088739</pub-id><pub-id pub-id-type="doi">10.1038/srep11475</pub-id></element-citation></ref><ref id="R89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Neural specializations for tonal processing</article-title><source>Ann N Y Acad Sci</source><year>2001</year><volume>930</volume><fpage>193</fpage><lpage>210</lpage><pub-id pub-id-type="pmid">11458830</pub-id></element-citation></ref><ref id="R90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name></person-group><article-title>Hemispheric asymmetries for music and speech: Spectrotemporal modulations and top-down influences</article-title><source>Front Neurosci</source><year>2022</year><volume>16</volume><elocation-id>1075511</elocation-id><pub-id pub-id-type="pmcid">PMC9809288</pub-id><pub-id pub-id-type="pmid">36605556</pub-id><pub-id pub-id-type="doi">10.3389/fnins.2022.1075511</pub-id></element-citation></ref><ref id="R91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name></person-group><article-title>Structure and function of auditory cortex: music and speech</article-title><source>Trends Cogn Sci</source><year>2002</year><volume>6</volume><issue>1</issue><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="pmid">11849614</pub-id></element-citation></ref><ref id="R92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zion Golumbic</surname><given-names>EM</given-names></name><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Bickel</surname><given-names>S</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><etal/><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><article-title>Mechanisms underlying selective neuronal tracking of attended speech at a “cocktail party”</article-title><source>Neuron</source><year>2013</year><volume>77</volume><issue>5</issue><fpage>980</fpage><lpage>991</lpage><pub-id pub-id-type="pmcid">PMC3891478</pub-id><pub-id pub-id-type="pmid">23473326</pub-id><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.037</pub-id></element-citation></ref><ref id="R93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuk</surname><given-names>NJ</given-names></name><name><surname>Murphy</surname><given-names>JW</given-names></name><name><surname>Reilly</surname><given-names>RB</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><article-title>Envelope reconstruction of speech and music highlights stronger tracking of speech at low frequencies</article-title><source>PLoS Comput Biol</source><year>2021</year><volume>17</volume><issue>9</issue><elocation-id>e1009358</elocation-id><pub-id pub-id-type="pmcid">PMC8480853</pub-id><pub-id pub-id-type="pmid">34534211</pub-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009358</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><title>Examples of sheet music and waveforms.</title><p><italic>Top:</italic> Sheet music for two bars of the tonal condition (original music: <italic>Mozart’s Sonata No. 5 in G Major, K. 283: II. Andante</italic>) and the same bars in the atonal condition. <italic>Bottom:</italic> Waveforms of the same bars in the tonal (green) and atonal (purple) condition, including the music envelope. Grey bars represent the positions of the dominant beat with an inter-beat-interval of 652 ms (1.52 Hz).</p></caption><graphic xlink:href="EMS189710-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Behavioural paradigm and stimulus properties.</title><p><bold>A)</bold> Depiction of the trial time-course for the behavioural tracking task. Before the music started for each 2-bar trial, the dominant beat frequency (1.52 Hz) was indicated visually by flashing a fixation cross four times at that frequency. Participants tapped their finger to the dominant beat of the music once the music started. <bold>B)</bold> Enjoyment ratings for both 5-min tonal and atonal excerpts by all participants in the main experiment (N = 20). Overall, participants rated the tonal condition as more pleasant/enjoyable than the atonal condition. <bold>C)</bold> Modulation spectrum of both tonal and atonal excerpts. Thick lines indicate average values across 2-bar segments, with shaded areas representing standard error of the mean. Beat/meter related frequencies are indicated by arrows and dotted lines. <bold>D)</bold> Averaged amplitude values of beat-related frequencies (as shown in C) for both tonal and atonal excerpts.<bold>E)</bold> Sensory dissonance, assessed via roughness. There was a small difference in that the tonal condition showed larger roughness values than the atonal condition.</p><p><italic>Notes:</italic> Distribution plots show individual data points, box plots (including median, interquartile ranges and minimum/maximum), and kernel density estimates.</p></caption><graphic xlink:href="EMS189710-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>Behavioural tracking results of the main experiment (<italic>N</italic> = 20) and the replication experiment (<italic>N</italic> = 49).</title><p>Shown is the Median Absolute Deviation (MAD) in ms for inter-tap-intervals in both the tonal and atonal condition. Points indicate individual data for all participants, violin plots show kernel density estimates, and boxplots show median interquartile ranges and minimum/maximum.</p></caption><graphic xlink:href="EMS189710-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Cortical tracking of acoustic music envelope and its relationship with behavioural measures.</title><p><bold>A)</bold> Topography of cortical envelope tracking assessed through mutual information (in bits) for both conditions. The right topography shows <italic>t</italic>-values from a direct comparison between tonal and atonal music. <bold>B)</bold> Correlation between cortical envelope tracking and participants’ finger-tapping performance. Envelope tracking predicted finger-tapping performance only in the tonal condition, in a left-frontal cluster. Here, stronger envelope tracking was associated with better performance (i.e., less tapping variability). <bold>C)</bold> Correlation between cortical envelope tracking and participants’ self-reported musical expertise. Expertise predicted envelope tracking in both conditions, in a frontal (tonal) and fronto-right-lateral (atonal) cluster. Stronger envelope tracking was associated with more musical expertise. <bold>D)</bold> Correlation between cortical envelope tracking and enjoyment. Enjoyment predicted envelope tracking in the tonal condition in fronto-central and posterior electrodes. A regression model showed a significant main effect of enjoyment with no significant interaction (<italic>p</italic> = .06). Perceiving the music as more pleasant was associated with stronger envelope tracking.</p><p>Note: Significant electrodes are highlighted with white circles.</p></caption><graphic xlink:href="EMS189710-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Cortical tracking of pitch surprisal performance.</title><p><bold>A)</bold> Topography of cortical surprisal tracking assessed through mutual information (in bits) for both conditions. The right topography shows t-values from a direct comparison between tonal and atonal music. Significant electrodes are highlighted with white circles. <bold>B)</bold> Correlation between pitch surprisal tracking and participants’ finger-tapping performance. Surprisal tracking predicted finger-tapping performance only in the atonal condition, in a right-frontal cluster. Here, stronger surprisal tracking was associated with worse performance (i.e., higher tapping variability). C<bold>)</bold> Pitch surprisal values for each note in the melody and bass lines of both tonal and atonal 5-min excerpts. Points indicate data for all notes, violin plots show kernel density estimates, and boxplots show median interquartile ranges and minimum/maximum. Pitch surprisal was higher for the atonal than the tonal condition. <bold>C)</bold> Relationship between cortical tracking of acoustic envelope and pitch surprisal.</p></caption><graphic xlink:href="EMS189710-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><title>Overview of results.</title></caption><graphic xlink:href="EMS189710-f006"/></fig></floats-group></article>