<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189775</article-id><article-id pub-id-type="doi">10.1101/2023.10.12.562046</article-id><article-id pub-id-type="archive">PPR743022</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">1</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>New Interactive Machine Learning Tool For Marine Image Analysis</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Clark</surname><given-names>H.Poppy</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Smith</surname><given-names>Abraham George</given-names></name><xref ref-type="aff" rid="A2">2</xref></contrib><contrib contrib-type="author"><name><surname>Fletcher</surname><given-names>Daniel Mckay</given-names></name><xref ref-type="aff" rid="A3">3</xref></contrib><contrib contrib-type="author"><name><surname>Larsson</surname><given-names>Ann I.</given-names></name><xref ref-type="aff" rid="A4">4</xref></contrib><contrib contrib-type="author"><name><surname>Jaspars</surname><given-names>Marcel</given-names></name><xref ref-type="aff" rid="A1">1</xref></contrib><contrib contrib-type="author"><name><surname>De Clippele</surname><given-names>Laurence H.</given-names></name><xref ref-type="aff" rid="A5">5</xref></contrib></contrib-group><aff id="A1"><label>1</label>Marine Biodiscovery Centre, Department of Chemistry, University of Aberdeen, Old Aberdeen, AB24 3DT, Scotland, United Kingdom</aff><aff id="A2"><label>2</label>Department of Computer Science, University of Copenhagen, Universitetsparken 1, 2100 Copenhagen, Denmark</aff><aff id="A3"><label>3</label>Rural Economy, Environment and Society, Scotland’s Rural College, West Mains Road, Edinburgh, EH9 3JG, Scotland, United Kingdom</aff><aff id="A4"><label>4</label>Tjarno Marine Laboratory, Department of Marine Sciences, University of Gothenburg, Stromstad, Sweden</aff><aff id="A5"><label>5</label>School of Biodiversity, One Health &amp; Veterinary Medicine, University of Glasgow, Bearsden Road, G61 1QH, Scotland, United Kingdom</aff><author-notes><corresp id="CR1"><label>*</label>Corresponding author: H. Poppy Clark (<email>h.clark.21@abdn.ac.uk</email>)</corresp></author-notes><pub-date pub-type="nihms-submitted"><day>19</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>16</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P1">Due to advances in imaging technologies, the rate of marine video and image data collection is drastically increasing. Often these datasets are not analysed to their full potential as extracting information for multiple species, such as their presence and surface area, is incredibly time-consuming. This study demonstrates the potential of a new open-source interactive machine learning tool, RootPainter, to analyse large marine image datasets quickly and accurately. The tool was initially developed to measure plant roots, but here was tested on its ability to measure the presence and surface area of the cold-water coral reef associate sponge species, <italic>Mycale lingua</italic>, in two types of underwater image data: 18,346 timelapse images and 1,420 remotely operated vehicle video frames. New corrective annotation metrics integrated with RootPainter, such as dice score and species area error, allow for the objective assessment of when to stop model training and reduce the need for manual model validation. Three highly accurate <italic>Mycale lingua</italic> models were created using RootPainter, as indicated by their average dice score of 0.94 ± 0.06. Model transfer and optimisation aided in the production of two of these models, increasing analysis efficiency from 6 to 16 times faster than manual annotation in Photoshop, for underwater observatory images. Sponge and surface area measurements were extracted from both datasets allowing future investigation of sponge behaviours and distributions. This study demonstrates that interactive machine learning tools and model sharing have the potential to dramatically increase image analysis speeds, collaborative research, and our collective knowledge on spatiotemporal patterns in biodiversity.</p></abstract></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P2">Large image datasets enable detailed and long-term studies of underwater species, providing a vital tool for the ecological investigation of deep-water species [<xref ref-type="bibr" rid="R1">1</xref>–<xref ref-type="bibr" rid="R3">3</xref>]. However, extracting information of interest from image datasets, such as species presence or size, can be prohibitively time-consuming [<xref ref-type="bibr" rid="R4">4</xref>–<xref ref-type="bibr" rid="R6">6</xref>]. This problem is exacerbated for more complex images, such as those captured by mobile cameras on remotely operated vehicles (ROVs), or automated underwater vehicles (AUVs), where lighting and focus may vary compared to stationary underwater cameras or fixed observatories [<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R8">8</xref>]. There has therefore been a trend to develop bespoke machine learning algorithms to extract information from a given image dataset; the most successful of these result from collaborations between marine and computer scientists [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R10">10</xref>]. As machine learning algorithms are non-trivial to construct and apply, their accessibility to individuals without experience in scientific programming languages is limited, creating a barrier to model sharing that is increasing the redundancy of work being completed by image analysis groups.</p><p id="P3">Pre-developed, user-friendly, and widely applicable machine learning tools may present a solution to some of these issues. They allow individuals with no machine learning or coding skills to develop models through training a pre-existing and adaptable ‘base’ neural network. The process to train models can vary depending on the tool employed, but their complexity is often masked through user interfaces. Improved accessibility of machine learning for image analysis increases the rate and range of measurements that can be extracted from image data, facilitating a variety of ecological investigations, as has previously been demonstrated through manual analyses. Extracting the length, perimeter, or area of species from images allows estimation of their size, and/or growth rates [<xref ref-type="bibr" rid="R11">11</xref>–<xref ref-type="bibr" rid="R15">15</xref>] and extracting count data can be used in species abundance [<xref ref-type="bibr" rid="R16">16</xref>], or biodiversity estimates [<xref ref-type="bibr" rid="R17">17</xref>]. Previously unknown species behaviour traits can be revealed by tracking individuals through extraction of their x,y coordinates [<xref ref-type="bibr" rid="R2">2</xref>], and extraction of ‘global shape measures’ such as the eccentricity, or curvature, of subjects of interest may allow investigation of morphological diversity [<xref ref-type="bibr" rid="R18">18</xref>]. Combining analyses can increase the value of information obtained further, for example extracting both species abundance and individual areas allows estimation of biomass [<xref ref-type="bibr" rid="R15">15</xref>,<xref ref-type="bibr" rid="R19">19</xref>,<xref ref-type="bibr" rid="R20">20</xref>], and investigating sessile species size alongside local biotic or abiotic factors may inform on their behaviour [<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R22">22</xref>].</p><p id="P4">These measurements have therefore been key targets in marine machine learning studies. Whilst algorithms capable of automated species detection have been developed [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R23">23</xref>–<xref ref-type="bibr" rid="R25">25</xref>], they are yet to be utilised to produce biodiversity estimates. This may be the result of inherent difficulties associated with automated species detection, such as the need for each species to be annotated a sufficient number of times in the training data to be detected when the model is subsequently applied [<xref ref-type="bibr" rid="R6">6</xref>]. Conversely, development of machine learning algorithms capable of predicting the area of sessile organisms from marine images has led to successful investigation of behaviour such as cold-water coral feeding [<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R27">27</xref>], and sponge contractions [<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref>]. Some algorithms have taken a step further, attempting to simultaneously investigate the areas of multiple species to study community structure and ecosystem functions [<xref ref-type="bibr" rid="R10">10</xref>]. However, as Purser <italic>et al</italic>. found, a major barrier to this is the differing visual complexity of species; their model performed well when estimating live cold-water coral density but struggled to accurately assess sponge coverage. Future investigations were thus completed through manual annotations [<xref ref-type="bibr" rid="R30">30</xref>]. Given these challenges, an approach where simultaneous extraction of multiple measurements for one species from image data may provide an alternative solution to increase the speed and complexity of ecological conclusions possible in these studies. There is therefore a significant need for an accessible and multi-functional machine learning tool to analyse marine image data.</p><p id="P5">RootPainter is an open-source and user-friendly graphical user interface-based software tool enabling the rapid training of convolutional neural networks via corrective annotation [<xref ref-type="bibr" rid="R31">31</xref>]. It is classified as an interactive machine learning tool as the user is part of the training feedback loop involved in model development. RootPainter was developed to investigate root length and the presence of soil voids (bipores) in soil images, with the production of a successful model being achievable within one working day. Users are not required to possess a high-powered Graphics Processing Unit (GPU), or any coding competencies and models can be transferred between projects and users. Internally, RootPainter uses a variant of the general purpose U-Net convolutional neural network [<xref ref-type="bibr" rid="R32">32</xref>], that has been found to be effective for many tasks. As RootPainter introduces no requirements on the type of structures that a model can be trained to detect, its applications are not limited to plant images. However, the innate complexity associated with marine images (due to suspended matter affecting image clarity and the uneven illumination of scenes with artificial lighting in the depths) may increase model training time compared to images from controlled (laboratory) conditions. Previous studies employing automated marine image analysis have relied heavily on image pre-processing to diminish the complexity of their image datasets [<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R26">26</xref>]. This can involve denoising or brightness/contrast/colour normalisation and is one of the most time-consuming stages of the analysis pipeline [<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R33">33</xref>]. Reliance on non-trivial image pre-processing not only reduces the accessibility of machine learning algorithms and the transferability of models, but also limits the processing pipeline to application on that specific dataset. In order to avoid these issues image pre-processing is not required to produce successful models with RootPainter [<xref ref-type="bibr" rid="R34">34</xref>–<xref ref-type="bibr" rid="R37">37</xref>].</p><p id="P6">This study, therefore, investigates the suitability of RootPainter to marine image analysis. The potential of RootPainter to combat key challenges in the field was explored through testing its ability to identify and predict the surface area of a known difficult target for machine learning algorithms, the deep-sea sponge <italic>Mycale lingua</italic> [<xref ref-type="bibr" rid="R10">10</xref>]. Additionally, RootPainter’s capabilities with images of varying complexity were assessed through comparison of model performance for static timelapse images from an underwater observatory, and frames extracted from ROV videos.</p></sec><sec id="S2" sec-type="materials | methods"><title>Materials and Methods</title><sec id="S3"><title>Study Sites, Underwater Imagery and Data Availability</title><p id="P7">Data was used from two separate locations in Norway (<xref ref-type="fig" rid="F1">Figure 1a</xref>). Time-lapse imagery was captured at the cabled Lofoten Vesterålen (LoVe) Ocean Observatory at 240 m depth [<xref ref-type="bibr" rid="R38">38</xref>], and ROV videos were recorded at the Tisler reef, between 70-160 m depth [<xref ref-type="bibr" rid="R39">39</xref>].</p><p id="P8">The LoVe Observatory (N 68°54.474′, E 15°23.14) is in the Hola trough, a continental slope 20 km from the Lofoten Islands [<xref ref-type="bibr" rid="R38">38</xref>]. Sub-station satellite 1 (<xref ref-type="fig" rid="F1">Figure 1b</xref>) was installed in 2017, supporting a Canon EOS 550 camera with E-TTL flash mode that captured 9,173 hourly images throughout 2017, 2018 and 2019. Data was transferred from the satellite through 450 m of subsea cable, via the X-frame, to the observatory main cable at the Subsea Distribution Unit, (<xref ref-type="fig" rid="F1">Figure 1b</xref>, [<xref ref-type="bibr" rid="R38">38</xref>]); observatory structure maintenance resulted in data gaps during this time (<xref ref-type="fig" rid="F1">Figure 1c</xref>).</p><p id="P9">The Tisler reef is found north of the Tisler Island in a 48-km-long ocean channel in the Hvaler area [<xref ref-type="bibr" rid="R7">7</xref>]. The research vessel Nereus, stationed at the Tjärnö Marine Laboratory was used to deploy the Ocean Modules ROV (V8 Sii, P/N: 02/00100-01, S/N: 011) to record the eastern section of the reef in 2021 (<xref ref-type="fig" rid="F1">Figure 1d</xref>). A full-colour HD Hama lens camera with two Bowtech LED-K-2400 lights (2400 lumens each) was used to collect the video footage. Video signals were transmitted over an optical fibre as the ROV moved. Two laser beams,separated by 5 cm, were used as a reference to scale video frames. An Applied Acoustics Nexus Lite USBL system, running the Applied Acoustics 1329A Micro beacon provided ROV navigation data. Every 130<sup>th</sup> frame was extracted from a total of 1 hour and 55 minutes of video; this minimised content overlap between frames but maximised reef coverage (ROV speed varied during the survey). A total of 1,420 images of 1920 x 1088 pixels were extracted as a result.</p></sec><sec id="S4"><title>Target Species</title><p id="P10">The Lofoten Vesterålen region and Tisler reef both host abundant <italic>Desmophyllum pertusum</italic> colonies (alternately known as <italic>Lophelia pertusa</italic>, Linnaeus 1758, [<xref ref-type="bibr" rid="R40">40</xref>]) and sponges, including <italic>Mycale lingua</italic> (Bowerbank, 1866). <italic>M. lingua</italic> was chosen as the target species to explore the capabilities of RootPainter as the complex range of colours, textures and morphologies that sponges display within a given species makes them difficult subjects for machine learning algorithms [<xref ref-type="bibr" rid="R10">10</xref>].</p><p id="P11"><italic>Mycale lingua</italic> (Bowerbank, 1866) is a Demospongiae found widely distributed across the northern hemisphere at depths of 30-2,500 m, with particularly high concentrations in the North Atlantic Ocean [<xref ref-type="bibr" rid="R41">41</xref>]. <italic>M. lingua</italic> non-selectively consumes small (&lt;10 μm) plankton [<xref ref-type="bibr" rid="R42">42</xref>] and is one of the only sponge species known to successfully colonise reef areas that have high <italic>L. pertusa</italic> densities [<xref ref-type="bibr" rid="R10">10</xref>,<xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R43">43</xref>]. Other than its association with cold-water coral reefs, little is known about the behaviour of <italic>M. lingua</italic>; thus far there has been limited success maintaining the sponge in aquaria for extended periods of time [<xref ref-type="bibr" rid="R44">44</xref>]. As sponges are important components of benthic ecosystems, both in the presence and absence of <italic>L. pertusa</italic> reefs [<xref ref-type="bibr" rid="R30">30</xref>,<xref ref-type="bibr" rid="R45">45</xref>], understanding their distribution, biomass and behaviour could allow evaluation of factors such as their contribution to carbon-cycling in benthic environments [<xref ref-type="bibr" rid="R19">19</xref>].</p><p id="P12">Utilising both the LoVe Ocean Observatory and Tisler reef datasets allows exploration of the ability of RootPainter to identify <italic>M. lingua</italic> from a more uniform dataset (i.e. one sponge in one location) and a more diverse dataset (i.e. different <italic>M. lingua</italic> individuals in different locations). In this study adjoining sponge lobes were treated as one individual. <italic>M. lingua</italic> are known to exhibit lobed body structures [<xref ref-type="bibr" rid="R41">41</xref>], and without sampling it was not possible to confirm whether lobes were genetically distinct.</p></sec><sec id="S5"><title>RootPainter</title><p id="P13">The software program RootPainter works through three stages:<list list-type="simple" id="L1"><list-item><p id="P14"><bold>Stage 1:</bold> Users annotate images with clear examples until non-random model predictions are seen (requires 6-10 images).</p></list-item><list-item><p id="P15"><bold>Stage 2:</bold> Users switch to corrective annotation, continuing to work through the training images which have been pre-segmented (images displaying predictions) by the current model. These corrections are included in the training data, continuously improving the model until users are satisfied with its performance, which is also indicated by multiple corrective annotation metrics.</p></list-item><list-item><p id="P16"><bold>Stage 3:</bold> The trained model is then used to automatically process (segment) the full dataset.</p></list-item></list></p><p id="P17">The continuous feedback loop in stage 2 allows issues and anomalies to be addressed by the user that may have not been encountered in stage 1. This corrective annotation continually supplies measures of true and false, positives and negatives to the algorithm for each image.</p><p id="P18">Multiple corrective annotation metrics (i.e. precision, recall, dice score and accuracy) can therefore be calculated during training without the need for separate manual annotations to validate the performance of the model. RootPainter (version 0.2.23 onwards) can also estimate the error in the area predicted by its models during training, allowing assessment of model success.</p><p id="P19">Once trained, models classify images into foreground and background, where foreground is the object of interest. These predictions are called segmentations; a visual output is provided for each image where segmentations are shown as blue highlighted regions. From these segmentations six measurements can be simultaneously extracted by RootPainter, these include count and area of regions of interest, as well as the diameter, perimeter and x,y coordinates of each discrete area. Users may also choose to extract the eccentricity (a proxy for curvature) of each discrete area. When several subjects are present within one image, separate values for their finite areas, as predicted by RootPainter, are reported.</p><sec id="S6"><title>RootPainter Model Training</title><p id="P20">RootPainter installation and model development were completed as per the GoogleColab notebook instructions [<xref ref-type="bibr" rid="R46">46</xref>]. A detailed manual describing model training, specifically for marine images, is also available [<xref ref-type="bibr" rid="R47">47</xref>].</p><p id="P21">The LoVe Observatory images were cropped using the “magick” package in R, to form two datasets containing 2200 x 2550 and 1000 x 1964 pixel images, each containing one <italic>Mycale lingua</italic> individual hereafter referred to as Magnus and Mini, respectively (<xref ref-type="fig" rid="F2">Figure 2</xref>). This allowed evaluation of the efficiency of RootPainter on images of different sizes, as well as the model transfer function of RootPainter within a dataset. The ROV frames were cropped to 1400 x 888 pixels using the “magick” package in R, such that the lasers were centralised, the ROV display text was removed, and the far background of each was image limited (<xref ref-type="fig" rid="F2">Figure 2</xref>).</p><p id="P22">RootPainter was run through the free version of GoogleColab, with GoogleDrive used to sync image directories. To comply with free storage limits, one year of images from the LoVe Observatory were uploaded to GoogleDrive as the training datasets for Magnus and Mini; images of Magnus from 2019 and Mini from 2018 were used as a shift in coral rubble obscures one of the lobes of Mini in 2019. The total ROV dataset of 1,420 images was uploaded to GoogleDrive for use in training.</p><p id="P23">Once running, RootPainter presents random successive images to the user from the selected training dataset. Eight of these images were annotated with examples of foreground (species/substrate of interest, here <italic>M. lingua</italic>) and background (everything else in the image) before models provided non-random predictions. Subsequent segmentations were then corrected, highlighting false positives (overpredictions that should be background) in green and false negatives (underpredictions by the current model) in red (<xref ref-type="fig" rid="F2">Figure 2</xref>). These corrections were incorporated into successive new and improved models.</p><p id="P24">In total five RootPainter models were produced. Model 1 was developed on images of Magnus. Additional fine-tuning of Model 1 was required on images of Magnus from April/May of 2018/19 due to a change in sponge colour/texture and turbid conditions; Model 1.1 was then applied on images from this time and Model 1 to the remaining images of Magnus. Model 1 also transferred (i.e. served as a training starting point) on images of Mini and <italic>M. lingua</italic> within ROV video frames, producing Models 2 and 3 respectively (<xref ref-type="fig" rid="F2">Figure 2</xref>). Model 2 was then applied to a GoogleDrive folder that contained all 9,173 images of Mini and Model 3 to a folder containing all 1,420 ROV video frames.</p><p id="P25">Model 4 was trained to identify the lasers in the same 1,420, 1400 x 888 pixel cropped ROV frames from the Tisler reef, independently of all other models. Inter-observer variation was removed as the same individual completed the training of all models.</p></sec><sec id="S7"><title>Stopping Criteria</title><p id="P26">Two distinct approaches were used to determine the endpoint of model training. Training cessation was guided by qualitative criteria for Models 1-3, but quantitative criteria only for Model 4. Training was deemed complete for Models 1 and 2 (LoVe Observatory) when predictions for at least two images from each month of the training dataset had required no corrective annotation. For Model 3 (ROV Tisler) segmentations that did not require any corrective annotation had to be seen for three frames from each video section; this more stringent criterion reflects the higher variability of image content and quality in the Tisler dataset. For Model 4 the simplicity of the subject of interest and its stark contrast to any background objects permitted the decision to stop training to be solely determined through RootPainter’s metrics calculations; specifically, when the rolling average (n=10) for the dice score reached 0.95.</p><p id="P27">The success of all models was also quantitatively assessed in real-time using the metrics calculated continually by RootPainter. Precision, recall, dice score, accuracy and estimated area error were exported to evaluate the suitability of this metrics calculator as stopping criteria in future work.</p></sec><sec id="S8"><title>Post-processing</title><p id="P28">Application of trained models to their respective datasets, via the ‘segment folder’ function in RootPainter, produced foreground predictions for every image. The discrete area values of each foreground prediction were extracted using the RootPainter ‘extract region properties’ function and exported as one .csv file.</p><p id="P29">The results of Model 1 and 2 (LoVe Observatory dataset) and Model 4 (Tisler reef dataset) were visually checked for anomalies. This involved scanning through the segmentation output file thumbnails for obvious errors, such as camera malfunctions, missing sponge/laser areas or obstructions by fish. This facilitated faster and more comprehensive datapoint exclusion than attempting to identify anomalies through pre-processing; post-processing required approximately 30 minutes of active work per 744 images analysed. Significant variation in sponge area and distribution in Tisler reef video frames prevented identification of Model 3 errors through segmentation observation alone. Given that additional approaches would suffer diminishing returns for an increase in result accuracy with user time, no post-processing was conducted for results from Model 3.</p></sec></sec><sec id="S9"><title>Image Scaling</title><sec id="S10"><title>Images from the LoVe Observatory</title><p id="P30">In the absence of laser scales, the average width of a <italic>Lophelia pertusa</italic> branch from the Bømla reef in Hardangerfjord (0.43 cm ± 0.09, [<xref ref-type="bibr" rid="R15">15</xref>, Greiffenhagen et al. in prep]), and branches adjacent to Magnus and Mini were used to scale the pixel dimensions of images in ImageJ. This allowed conversion of the foreground areas predicted by RootPainter from pixels to cm<sup>2</sup>. Relative sponge areas were calculated through division of each surface area value by the maximum sponge area value for that dataset.</p></sec><sec id="S11"><title>ROV Frames from the Tisler Reef</title><p id="P31">The x,y coordinates of areas segmented by Model 4 allowed calculation of the distance between laser points in each ROV video frame in pixels (<xref ref-type="disp-formula" rid="FD1">Equation 1</xref>). Images were then independently scaled based on the true distance between the lasers, which is 5 cm. The area errors for each image, as calculated by RootPainter during training, were scaled in the same manner.
<disp-formula id="FD1"><label>(Equation 1)</label><mml:math id="M1"><mml:mrow><mml:mtext>Distance</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>between</mml:mtext><mml:mspace width="0.2em"/><mml:mtext>lasers</mml:mtext><mml:mspace width="0.2em"/><mml:mo stretchy="false">(</mml:mo><mml:mtext>pixels</mml:mtext><mml:mo stretchy="false">)</mml:mo><mml:mspace width="0.2em"/><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mtext>x</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mtext>y</mml:mtext><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p id="P32">Equation 1: Formula used to calculate the Euclidean distance between laser points given their x,y coordinates, where (x<sub>1</sub>,y<sub>1</sub>) and (x<sub>2</sub>,y<sub>2</sub>) correspond to each laser point respectively.</p></sec></sec><sec id="S12"><title>Model Validation and Statistical Analysis</title><p id="P33">Model 1 was validated by comparing surface area measurements made manually in Photoshop [<xref ref-type="bibr" rid="R48">48</xref>] and predicted by RootPainter for 452 randomly selected images (5% of the total dataset), 28 of which were seen during training. Sponge areas were extracted from the Photoshop annotations using open-source R scripts [<xref ref-type="bibr" rid="R48">48</xref>], and scaled using ImageJ as previously described. The precision, recall, dice score and accuracy of Model 1 were then calculated in Python by assuming the manually annotated images were accurate.</p><p id="P34">Precision is quantified as the ratio of true positives to all positive instances (the sum of true and false positives) and describes the probability that a pixel is truly foreground, given that the RootPainter model predicts it as foreground. Recall is calculated as the ratio of true positives to all true positive instances (the sum of true positives and false negatives) giving a measure of the proportion of foreground pixels the RootPainter model is expected to identify [<xref ref-type="bibr" rid="R49">49</xref>,<xref ref-type="bibr" rid="R50">50</xref>]. Dice score is calculated using precision and recall, giving an overall indication of model performance. Accuracy evaluates how close to the true result is to the model’s predictions based on the degree of overlap between predicted segmentations and the true regions [<xref ref-type="bibr" rid="R50">50</xref>]. In previous studies models have been defined as successful with a precision ≥ 0.71, recall ≥ 0.75, dice score ≥ 0.74 and accuracy ≥ 0.76 [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R51">51</xref>].</p><p id="P35">RootPainter also continually calculates these metrics during training but through assumption that corrected segmentations are accurate. Comparison of the corrective annotation metrics for Model 1 to the externally calculated validation metrics allowed evaluation of the necessity of separate manual validation for future RootPainter studies. Additionally, RootPainter provides estimates of area error during training allowing assessment of model success. Error is calculated through subtraction of the ‘corrected area’ from the ‘predicted area’ for each training image, where the corrected area is the post-annotation result, and is taken to be the true area. For Model 3 the agreement between the corrected area and RootPainter’s training prediction was also investigated through calculation of a Pearson correlation coefficient and linear regression, to ensure lack of bias across multiple sponge individuals of varying size.</p></sec></sec><sec id="S13" sec-type="results"><title>Results</title><sec id="S14"><title>RootPainter Model Development</title><p id="P36">In total, three models were produced and used to evaluate the surface area of <italic>Mycale lingua</italic>; a fourth model was produced to identify red lasers in ROV video frames. <xref ref-type="table" rid="T1">Table 1</xref> displays the number of images and time used in both training and application of the models.</p></sec><sec id="S15"><title>Model 1</title><p id="P37">Model 1 was trained on 640 images of Magnus from the LoVe Observatory, requiring 17 hours. The training times for Model 1.1 (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Table 1</xref>), necessitated by the colour/texture change in Magnus during April and May of 2018/19, are incorporated into Model 1 in <xref ref-type="table" rid="T1">Table 1</xref>. The decision to stop training Model 1 was guided by qualitative criteria, but concurrent increases in the corrective annotation metrics of precision, recall, dice score and accuracy can be seen with improved segmentations in <xref ref-type="fig" rid="F3">Figure 3</xref>.</p><p id="P38">Model 1 was applied to 9,173 images of Magnus. Post-processing to exclude anomalies was completed, and highlighted that segmentations were impacted during March 2019, when sea-stars (suspected <italic>Henricia</italic> spp.) took prolonged residence on the base of Magnus. The area of sponge covered by the sea-stars varied, preventing reliable data point exclusion. Thus, segmentations from this period should be interpreted with caution. In total 548 data points were excluded, with 352 of these corresponding to corrupted images.</p><p id="P39"><xref ref-type="fig" rid="F4">Figure 4</xref> visualises the agreement between the areas of Magnus extracted using Model 1 and those manually measured in Photoshop. The average difference in area values between the methods is 2.26 cm<sup>2</sup> ± 1.69 cm<sup>2</sup> or 5.3% ± 3% of Magnus.</p></sec><sec id="S16"><title>Model 2</title><p id="P40">Fine-tuning of Model 1, through further training, was needed to produce Model 2 due to differential lighting of Magnus and Mini at the LoVe Observatory. This required an additional 142 images and 3.5 hours of corrective annotation on images of Mini, with the decision to stop training guided by qualitative criteria. The corrective annotation metrics from Model 2 training can be seen in <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 3</xref>. Model 2 was applied to 9,173 images of Mini, and post-processing completed to identify anomalies. In total, 601 data points were excluded; 352 of these corresponded to corrupted images.</p></sec><sec id="S17"><title>Model 3</title><p id="P41">Fine-tuning of Model 1 to produce Model 3 was necessary due to the more complex and changing nature of ROV video frames compared to underwater observatory images. This required 10.5 hours of corrective annotation on 556 video frames from the East of the Tisler reef, captured in 2021. The decision to stop training was guided by qualitative criteria, but the agreement between visual observations and the RootPainter corrective annotation metrics for Model 3 is demonstrated in <xref ref-type="fig" rid="F5">Figure 5</xref>. Model 3 can distinguish <italic>M. lingua</italic> from <italic>L. pertusa</italic> (<xref ref-type="fig" rid="F5">Figure 5B-C</xref>) and the sponge <italic>Geodia</italic> spp. (<xref ref-type="fig" rid="F5">Figure 5A and C</xref>).</p><p id="P42">Model 3 was applied to all 1,420 ROV video frames from the East of the Tisler reef, captured in 2021. No post-processing was completed on the results from Model 3.</p></sec><sec id="S18"><title>Model 4</title><p id="P43">Model 4 was developed to segment red ROV lasers. It was trained on 100 video frames from the East of the Tisler reef, captured in 2021, requiring 45 minutes. The termination of training was solely determined by RootPainter’s metrics calculations (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 4</xref>). Model 4 was applied to all 1,420 ROV video frames from the East of the Tisler reef, captured in 2021. Post-processing resulted in exclusion of 124 data points where only one laser was present.</p></sec><sec id="S19"><title>RootPainter Model Performance</title><sec id="S20"><title>Efficiency</title><p id="P44">RootPainter was 5-16 times more efficient compared to manual annotations (<xref ref-type="table" rid="T2">Table 2</xref>). Using RootPainter to analyse an ROV dataset requiring multiple annotations per image was more efficient than manual annotation of an underwater observatory dataset containing one individual per image (Magnus).</p></sec><sec id="S21"><title>Accuracy</title><p id="P45">The precision, recall, dice score and accuracy for Model 1 is displayed in <xref ref-type="table" rid="T3">Table 3</xref>; agreement between the metrics as calculated by external manual validation and internal training calculations in RootPainter can be seen. Average corrective annotation metrics from the end-point of training Models 2-4 can be seen in <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Table 3</xref>.</p></sec><sec id="S22"><title>Assessment of Model Success</title><p id="P46">Precision, recall, dice score and accuracy can reflect disproportionately harshly on model performance when foreground pixels are low (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 5 and 6</xref>). These corrective annotation metrics were therefore used in combination with the area errors as calculated by RootPainter to assess the success of Models 1-3 (<xref ref-type="fig" rid="F6">Figure 6</xref>). The agreement between the corrected/true area in each training image and RootPainter’s training prediction was also assessed for Model 3, as individuals of varying sizes are present in the ROV video frames (<xref ref-type="fig" rid="F6">Figure 6d</xref>). For the final 400 images used in training the Pearson correlation coefficient between the corrected area and predicted RootPainter area is 0.95 (p-value &lt; 2.2 x10<sup>-6</sup>); Model 3 consistently over-predicts the area of <italic>M. lingua</italic> by 4.89 cm<sup>2</sup> as calculated by linear regression, with an R<sup>2</sup> of 0.91.</p><p id="P47">The average area errors for each model, as calculated by RootPainter, towards the end of training can be seen in <xref ref-type="table" rid="T4">Table 4</xref>. The value for Model 1 is in agreement with the average area error calculated from manual validation (2.26 cm<sup>2</sup> ± 1.69 cm<sup>2</sup>, <xref ref-type="fig" rid="F4">Figure 4</xref>).</p></sec></sec><sec id="S23"><title>Model Outputs and Observations</title><p id="P48">In total, 4 measurements were simultaneously extracted by RootPainter from the output segmentations of Models 1-3, including the area of individuals, as well as the diameter, perimeter and x,y coordinates of each discrete area. For the purposes of this study, we focused on the surface area outputs only.</p><p id="P49">In the LoVe Observatory dataset, 100% of the images contained the target species, <italic>Mycale lingua</italic>. The average 2D surface area for Magnus and Mini in the monitored months of 2018/19 are displayed in <xref ref-type="table" rid="T5">Table 5</xref>. In the Tisler reef ROV dataset only 40% of the extracted video frames contained <italic>M. lingua</italic> individuals, with an average size of 19.4 cm<sup>2</sup> ± 51.8 cm<sup>2</sup>.</p><p id="P50">Magnus and Mini both exhibited frequent contractions in each month of recorded data, without any displayed seasonality in this behaviour. A clear decrease in sponge surface area (~50%) in the results from Model 1 compared to Model 2 was seen during February-March of 2019. Returning to the raw data revealed that this resulted from prolonged sea-star (<italic>Henricia</italic> spp.) residence and presumed predation at the base of Magnus. Mini’s area was unaffected during this time (<xref ref-type="fig" rid="F7">Figure 7</xref>).</p></sec></sec><sec id="S24" sec-type="discussion"><title>Discussion</title><p id="P51">This study showed the suitability of the user-friendly machine learning tool, RootPainter, to analyse large datasets of marine images. RootPainter was capable of accurately processing images of varying size, colour, and complexity, five to sixteen times faster than manual annotation, without the need for image pre-processing. As the efficiency of RootPainter is dependent on dataset size this may be even faster for larger datasets. Manual validation demonstrated the reliability of our qualitative stopping criteria and RootPainter’s in-built metrics calculator as a means to assess model success. Therefore, external stopping criteria and model validation may not be required in future studies, allowing ecological conclusions to be drawn, with the appropriate caveats in place, with significantly improved efficiency.</p><sec id="S25"><title>Machine Learning Tools for Marine Image Analysis</title><p id="P52">This work demonstrates that RootPainter is an accessible and affordable tool capable of processing large and complex datasets, with the potential to ease the analysis bottleneck created by the continually increasing volume of video/image data collected by marine researchers. The intuitive interface and instruction notebook accompanying the software allow marine experts to concentrate on dataset content, instead of the intricacies of running a machine learning algorithm. This includes removing the need for image ‘pre-processing’ steps, such as reduction of background complexity, as seen for other machine learning methods. The ability to run RootPainter through GoogleColab prevents users needing to acquire an expensive GPU or to possess significant computing power. In order to comply with the free GoogleColab GPU usage and GoogleDrive space limits, annotations in this study were completed in 3-5 hour sessions and images were uploaded in batches. Consequently, powerful results were produced with no previous user experience in machine learning and at no additional cost.</p><p id="P53">This study focused on RootPainter’s application to <italic>M. lingua</italic> individuals only. However, the success demonstrated with this notoriously complex species of interest, paired with previous terrestrial examples of model success [<xref ref-type="bibr" rid="R31">31</xref>,<xref ref-type="bibr" rid="R52">52</xref>–<xref ref-type="bibr" rid="R54">54</xref>], gives confidence that RootPainter will be capable of segmenting other marine species. The applicability of RootPainter to species identification and biodiversity investigations may be increased by introduction of multi-annotation capabilities. Within the current version of RootPainter simultaneous investigation of multiple species requires the development of several different models for each target species (i.e. class) and extraction of results separately. Alternatively, a multi-staged approach can be used where the general foreground is segmented first and then this is used to remove all background from the data. The extracted foreground could then be further categorised into different classes. Whilst this model-cascade approach may have training and efficiency benefits, akin to localisation [<xref ref-type="bibr" rid="R55">55</xref>], its utility decreases with increasing class number. Therefore, depending on user needs, a conscious decision regarding choice of machine learning tool for a desired investigation needs to be made.</p><p id="P54">The web-based annotation software BIIGLE is widely used by marine ecologists for manual annotations and is capable of automated novelty detection [<xref ref-type="bibr" rid="R56">56</xref>]. The user friendly and open-source ‘machine learning assisted image annotation’ (MAIA) function in BIIGLE has proven suited to biodiversity studies due to its multiclass annotation capabilities [<xref ref-type="bibr" rid="R25">25</xref>]. However, compared to RootPainter, and at the time of writing (October 2023), extraction of information such as species area or perimeter cannot be automated in BIIGLE, no training metrics are provided to aid assessment of model success, and models developed through the MAIA function cannot currently be transferred between users through the existing interface.</p><p id="P55">ImageJ is also widely used for manual analysis of marine images. Whilst it does not possess its own machine learning tool as such, the deepImageJ plugin enables users to apply pre-trained neural networks (models) in ImageJ, that are downloadable from an ‘online zoo’ [<xref ref-type="bibr" rid="R57">57</xref>]. Application of downloaded models is user-friendly, but their <italic>de novo</italic> development or continued training must be completed outside the ImageJ application [<xref ref-type="bibr" rid="R58">58</xref>]. This requires machine learning expertise, creating dependence of non-experienced users on others to develop models they require. However, as deepImageJ users are downloading ‘bespoke’ algorithms, the range of functionalities models can possess and the information that can be extracted from images is expansive. Thus far deepImageJ has been targeted at microscopy work and biomedical imaging, such as virtual tissue staining [<xref ref-type="bibr" rid="R59">59</xref>] and instance segmentation of neurons [<xref ref-type="bibr" rid="R60">60</xref>]. When analysing marine images, the inability to optimise models without computational expertise would act as a significant barrier to the use of deepImageJ, as the quality and background of underwater images varies significantly. The accessibility of model sharing and application within deepImageJ has undoubtedly made significant progress to unifying the field of microscopy image analysis [<xref ref-type="bibr" rid="R57">57</xref>], with some models being downloaded 20,000 times [<xref ref-type="bibr" rid="R58">58</xref>]. The additional flexibility provided by RootPainter’s accessible training process may enable a similar achievement to be made in the marine imaging field.</p></sec><sec id="S26"><title>RootPainter Model Sharing</title><p id="P56">Sharing RootPainter models may allow researchers to dramatically increase their marine image analysis capacity and analyse datasets to their full potential. However, the time-saving capabilities of transferred models within RootPainter likely depends on the specific task and datasets utilised [<xref ref-type="bibr" rid="R61">61</xref>]. Starting training with a suitable pre-established model may reduce the time and number of images required to produce a satisfactory model for a given dataset; only 3.5 hours and 142 images, and 10.5 hours and 556 images were required to optimise Model 1 to produce Models 2 and 3, respectively (<xref ref-type="table" rid="T1">Table 1</xref>). The initial increased accuracy of RootPainter predictions, as seen for the first 20 images of Model 2 compared to Model 1 (<xref ref-type="fig" rid="F3">Figure 3</xref> and <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 4</xref>), significantly reduces the corrective annotation time required per image (<xref ref-type="table" rid="T2">Table 2</xref>). Pre-developed models can also reduce the threshold number of ‘application images’ at which using RootPainter becomes more efficient than manual annotation. For <italic>de novo</italic> model development on static observatory images a minimum of 468 images are needed in the application dataset to ‘justify’ use of RootPainter. Utilising a pre-developed model reduces this to 96 static observatory images or 289 ROV video frames (<xref ref-type="table" rid="T1">Table 1</xref> and <xref ref-type="table" rid="T2">2</xref>). However, greater image numbers may be required depending on the dataset (<xref ref-type="table" rid="T1">Table 1</xref>).</p><p id="P57">The accuracy of transferred models will always be limited by object variation between datasets. Researchers are therefore advised to utilise the adaptability of the RootPainter algorithm to fine-tune a pre-developed model to their dataset before its application. This will also produce corrective annotation metrics, allowing users to assess the success of segmentations themselves.</p></sec><sec id="S27"><title>Analysing Static vs Mobile Image Datasets with RootPainter</title><sec id="S28"><title>Dataset effect on speed of training</title><p id="P58">Image analysis with RootPainter is highly efficient for both static images and frames from moving videos (<xref ref-type="table" rid="T2">Table 2</xref>). However, the number of images and training time required for model development on a given species does increase when moving from underwater observatory images (Models 1 and 2) to ROV video frames (Model 3). The more dynamic background, reduced image clarity and varied lighting within the ROV video frames, as well as the need to identify and distinguish many different <italic>M. lingua</italic> individuals from apparently similar <italic>Geodia</italic> spp., increased the extent of model optimisation required to produce Model 3 compared to Model 2 (<xref ref-type="table" rid="T1">Table 1</xref>).</p><p id="P59">Interestingly, the rate of corrective annotation in RootPainter did not increase with increasing image complexity; optimisation of Models 2 and 3 did not involve significant background annotations, with both requiring 1.2 minutes of annotation per image (<xref ref-type="table" rid="T1">Table 1</xref>). Conversely, the rate of manual annotations does decrease with increasing image complexity (i.e. more individuals per image require more time to manually annotate). Therefore, comparison of the development speed of Model 3 to underwater observatory manual annotations likely underestimates the efficiency of RootPainter for ROV video frame analysis.</p><p id="P60">It is important to note that the nature of the subject of interest also impacts RootPainter model training time. As the red lasers were uniform in each image and visually distinct from all other background objects development of Model 4 required the least time and number of images, despite being trained ROV video frames.</p></sec><sec id="S29"><title>Dataset effect on accuracy of models</title><p id="P61">All RootPainter models in this study exhibited high levels of accuracy (<xref ref-type="table" rid="T3">Table 3</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Table 3</xref>). Manual validation confirmed that Model 1 consistently and accurately predicted the area of Magnus (<xref ref-type="table" rid="T3">Table 3</xref>, <xref ref-type="fig" rid="F4">Figure 4</xref>). Poor agreement between the Photoshop and RootPainter results was often a result of external factors, such as sea-star presence or turbidity (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 7</xref>). The average area error of Model 1, as calculated by manual validation, is larger (5.3% ± 3%) than the estimate provided during training by RootPainter (0.14% ± 6.67%), but not significantly so. Therefore, we may accept the average area error estimates for Models 2 and 3 calculated by RootPainter during training (0.45 cm<sup>2</sup> ± 0.86 cm<sup>2</sup> and 7.09 cm<sup>2</sup> ± 52.97 cm<sup>2</sup>, respectively), to be representative of the true accuracy of area predictions for these models (<xref ref-type="table" rid="T4">Table 4</xref>).</p><p id="P62">The accuracy of Models 1-3 can be seen to decrease with increasing dataset complexity. Model 1 was trained on, and used to segment, images of the same individual. Conversely, each image segmented by Model 3 contained different sponge individuals, including many which the model was not exposed to during training. The effect of this is apparent in the larger average area error for Model 3 than for Model 1 (<xref ref-type="table" rid="T4">Table 4</xref>). However, it should be considered that the accuracy of manual annotations may also decrease across these two datasets, and the overall area error for Model 3 is still acceptably small.</p></sec></sec><sec id="S30"><title>Increasing the Efficiency of RootPainter</title><p id="P63">The efficiency of image analysis with RootPainter depends on the images and computing set-up used. Without non-trivial pre-processing to reduce image complexity, the main methods to increase analysis efficiency involve using smaller images, paying for upgraded GoogleColab access, or investing in a purpose built deep-learning workstation. As the aim of this tool is to be accessible and cost-effective, use of equipment designed for deep-learning will not be discussed further here.</p><p id="P64">Smaller images increase the efficiency of image analysis with RootPainter through increased training speeds and reduced application times [<xref ref-type="bibr" rid="R31">31</xref>]. When constrained to larger images, using the ‘create dataset’ function in RootPainter to randomly crop training images can produce a more efficient training dataset [<xref ref-type="bibr" rid="R31">31</xref>]. Smaller images require less time to segment during model application; Model 2 was applied to images 2.8 times smaller than Model 1 (<xref ref-type="fig" rid="F2">Figure 2</xref>), and they were segmented 3.6 times faster (<xref ref-type="table" rid="T1">Table 1</xref>). The same application segmentation speeds seen for Models 3 and 4 (<xref ref-type="table" rid="T1">Table 1</xref>), demonstrate that subject complexity does not affect RootPainter application time.</p><p id="P65">Upgrading GoogleColab can significantly reduce both the ‘active’ and ‘inactive’ user time required for RootPainter studies, through increased access to higher memory GPUs. Chance assignment to a higher memory GPU resulted in reduced segmentation times during application of Models 3 and 4 compared to Model 2 (<xref ref-type="table" rid="T1">Table 1</xref>), despite their application to images of similar sizes (<xref ref-type="fig" rid="F2">Figure 2</xref>). The application efficiency of RootPainter may therefore be tripled if improved GPU assignments can be consistently secured through a paid upgrade in GoogleColab (~£8 a month in the year 2023).</p><p id="P66">Finally, excluding the optional post-processing stage in this study would have reduced the total ‘active user time’ by 6.2 hours each for Models 1 and 2, increasing the efficiency of RootPainter to 6 and 25 times faster than manual annotation for these models, respectively (<xref ref-type="table" rid="T2">Table 2</xref>).</p></sec><sec id="S31"><title>Improving the Accuracy of RootPainter</title><p id="P67">Using smaller images may increase segmentation accuracy due to mitigation of class balance issues; large background to foreground ratios are known challenges for convolutional neural network model training [<xref ref-type="bibr" rid="R62">62</xref>]. This may be reflected in the smaller standard deviation for average area error of Mini as predicted by Model 2 within smaller images, than for Magnus (−0.06 cm<sup>2</sup> ± 2.87 cm<sup>2</sup>) as predicted by Model 1 (0.45 cm<sup>2</sup> ± 0.86 cm<sup>2</sup>).</p><p id="P68">The post-processing (i.e. exclusion of obvious segmentation anomalies) completed in this study aimed to improve the accuracy of results from Models 1 and 2. Of the 452 images used in manual validation, 5 segmentations (including ‘16/04/2019 22:09’, <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 7</xref>) had been removed during post-processing of Model 1. This caused no improvement in the precision, recall, dice score and accuracy of Model 1, to two decimal places. Therefore, the post-processing stage may not be necessary in future studies.</p><p id="P69">Ultimately, the accuracy of a RootPainter model depends on the quality of user corrective annotations, and whether the training images are sufficiently representative of the subject of interest. Even manual annotations incur some error due to ambiguity on the boundary of subjects, the difficulty of perfect annotation and partial volume issues. This also results in diminishing returns in accuracy from continued annotation towards the end of model training (<xref ref-type="fig" rid="F6">Figure 6</xref>). Therefore, accepting small inherent error in segmentations is essential to maintaining efficiency in RootPainter studies.</p></sec><sec id="S32"><title>Reliability of Metric Calculations in RootPainter</title><p id="P70">The corrective annotation metrics calculated within RootPainter during training overestimated precision, recall, dice score and accuracy by 0.02-0.04 compared to values from external manual validation for Model 1 (<xref ref-type="table" rid="T3">Table 3</xref>). Accounting for this overestimation the corrective metrics values for Models 2-4 still fall within the classification of successful models [<xref ref-type="bibr" rid="R4">4</xref>,<xref ref-type="bibr" rid="R9">9</xref>,<xref ref-type="bibr" rid="R26">26</xref>,<xref ref-type="bibr" rid="R29">29</xref>,<xref ref-type="bibr" rid="R51">51</xref>]. The discrepancy between calculations may result from images with regions of high uncertainty as during corrective annotation users can leave ambiguous errors as unclassified, conversely during manual annotation the user was forced to classify with certainty each pixel of an image. If this potential error is considered, utilising the corrective annotation metrics within RootPainter may negate the need for time-consuming manual validation in future studies. However, this decision should be left to users’ discretion, and it may be advised to complete manual validation when developing a model for a new species.</p><p id="P71">The overall reliability of corrective metrics calculations within RootPainter (<xref ref-type="table" rid="T3">Table 3</xref>) allows identification of when the user can stop training and accurate model performance is achieved. This was trialled to success with Model 4, thus providing a possible mechanism to reduce subjectivity in training cessation across RootPainter users. However, RootPainter’s metric calculations can be skewed by imperfect user corrections. For example, in the early stages of Model 1 training the extensive background pixels were not fully correctively annotated, to avoid overwhelming the algorithm, resulting in incredibly high metrics at a time where segmentations are poor (<xref ref-type="fig" rid="F3">Figure 3</xref>). As Model 1 then improved its corrective metrics initially decreased as corrections became more thorough, before increasing again with the true accuracy of the model. Metrics may also be misleading for subjects of interest more complex than lasers (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figures 5 and 6</xref>). Interestingly, the area error estimate by RootPainter continuously agrees with the visual assessments and can differentiate between good (<xref ref-type="fig" rid="F5">Figure 5C</xref>) and excellent (<xref ref-type="fig" rid="F5">Figure 5B</xref>) segmentations. RootPainter’s calculation of area errors would thus make an excellent contribution to stopping criteria in future studies investigating species area. Therefore, it is recommended that when complex subjects of interest are targeted, a combined qualitative and quantitative stopping criteria approach is used.</p></sec><sec id="S33"><title>RootPainter Applications</title><p id="P72">Machine learning tools for image analysis have the potential to rapidly increase our understanding of marine species and their functions within ecosystems. In this study, RootPainter has demonstrated an aptitude to identifying and predicting the surface area of <italic>M. lingua</italic>, both in underwater observatory images and ROV video frames. Due to the high ratio of background to foreground pixels in images used Models 1-3 slightly overestimated sponge area (<xref ref-type="fig" rid="F4">Figure 4</xref>, <xref ref-type="table" rid="T4">Table 4</xref>). This error is very small and insignificant to the intended purposes of Models 1 and 2, investigating relative changes in the predicted sponge area for Magnus and Mini over time. Conversely to estimate sponge cover or biomass, as is the purpose of Model 3, a small and consistent error in predicted area is important. Whilst this is difficult to achieve with mobile ROV video frames, the average area error of 7.09 ± 52.97 cm<sup>2</sup> for Model 3 is sufficiently small that ecological conclusions can be drawn if it is taken into consideration. The large standard deviation of this area error does not represent model bias disproportionately affecting larger or smaller sponges as there is a strong correlation between the user corrected and RootPainter predicted areas for Model 3 (<xref ref-type="fig" rid="F6">Figure 6d</xref>). Finally, the ability of Model 3 to distinguish between apparently similar sponges, <italic>M. lingua</italic> and <italic>Geodiia</italic> spp. (<xref ref-type="fig" rid="F5">Figure 5C</xref>), confirms that the model is reliable for investigation of ecological questions pertaining to a given species.</p><p id="P73">Results from Models 1 and 2 demonstrated the utility of RootPainter to temporal changes in species behaviour. Sponge contractions have previously been studied in both shallow and deep-water using manual and bespoke machine learning methods [<xref ref-type="bibr" rid="R2">2</xref>,<xref ref-type="bibr" rid="R21">21</xref>,<xref ref-type="bibr" rid="R28">28</xref>,<xref ref-type="bibr" rid="R29">29</xref>]. ‘Intrinsic’ contractions observed in shallow-water sponges likely serve to clear the aquiferous system, where blocked canals may disrupt filter-feeding [<xref ref-type="bibr" rid="R63">63</xref>,<xref ref-type="bibr" rid="R64">64</xref>]. In abyssal sponges the contracted state can be maintained for up to weeks at a time and is believed to represent an energy saving mechanism through reduction of sponge filter-feeding [<xref ref-type="bibr" rid="R2">2</xref>]. In this study, <italic>M. lingua</italic> exhibited short and frequent contractions consistently throughout the seasons, suggesting an alternate purpose for some sponge contractions to energy conservation and aquiferous system clearing is likely. Contractions were consistently ‘larger’ for Mini than for Magnus relative to their overall size, but contraction rate is similar between the sponges. In April 2019 prolonged sea-star residency on Magnus coincides with a ~50% reduction in sponge size and a significant reduction in sponge contractions. This energy conservation may represent a viable survival strategy for during predation; contractions in Mini were unaffected during this time. Previous investigation of <italic>M. lingua</italic> contractions found them to be rare and asynchronous at 30 m depth [<xref ref-type="bibr" rid="R42">42</xref>], but frequent and correlated with salinity in one individual at 260 m depth [<xref ref-type="bibr" rid="R29">29</xref>]. The possibility that environmental drivers are contributing to the observed behaviour of Magnus and Mini at the LoVe Observatory requires further study. This may elucidate the purpose of the non-energy conservation contractions seen through identification of any environmental stimuli. Variation in contractions with abiotic factors will have implications for the ecosystem services provided by deep-sea sponges, especially if frequent contractions are concluded to effect filtration capacity.</p><p id="P74">The suitability of RootPainter to spatial analyses, such as investigations into species distributions, has been shown through successful development of Models 3 and 4. Quantifying deep-sea sponge presence and surface area allows estimation of their percentage cover and/or biomass, and therefore contribution to carbon-cycling in benthic environments [<xref ref-type="bibr" rid="R44">44</xref>,<xref ref-type="bibr" rid="R65">65</xref>]. The distribution of <italic>M. lingua</italic> has previously been investigated at the Tisler reef through small datasets [<xref ref-type="bibr" rid="R7">7</xref>,<xref ref-type="bibr" rid="R10">10</xref>], but determination of its variation in time and space across the reef has been prohibited by methodological limitations. Requiring just two working days, RootPainter produced results for the distribution, abundance, and size of <italic>M. lingua</italic> across the East of the Tisler reef. Thus, the use of machine learning tools, such as RootPainter, will be essential in the future study of spatiotemporal patterns within large image datasets.</p></sec></sec><sec id="S34" sec-type="conclusions"><title>Conclusion</title><p id="P75">RootPainter provides a viable solution to the increasing data processing needs of marine ecologists, both on time-lapse data from static underwater observatories and frames from ROV/AUV video data. Through proper training the algorithm can efficiently produce highly accurate models, and its built-in methods to assess stopping criteria and model success reduce the need for manual validation. Additionally, regular improvements to the software continually enhance its suitability to marine image analysis; completion of the multi-annotation capabilities of RootPainter currently under development would increase the range of ecological questions that can be tackled using RootPainter. Resource limitation is not prohibitive to accessing this user-friendly software and the adaptability of models has the capability to productively link marine image analysis researchers. Moving forward the creation of a RootPainter repository to facilitate model sharing between users has the potential to exponentially increase the rate of information extraction from marine images, and therefore our understanding of marine organisms.</p></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary Information</label><media xlink:href="EMS189775-supplement-Supplementary_Information.pdf" mimetype="application" mime-subtype="pdf" id="d32aAdFbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S35"><title>Acknowledgments</title><p>We would like to thank the Lofoten Vesterålen Ocean Observatory, and specifically Geir Pedersen, for supplying much of the data utilised in this study.</p><p>H.P.C is supported by the Biotechnology and Biological Sciences Research Council EASTBIO Doctoral Training Programme (BB/M010996/1). A.G.S is supported by Novo Nordisk Foundation grant NNF22OC0080177. D.M.F is supported by the Rural and Environment Science and Analytical Services Division (SRUC-C5-1). L.D.C received funding from the European Union’s Horizon 2020 iAtlantic project (Grant Agreement No. 818123). This manuscript reflects the authors’ views alone and the European Union cannot be held responsible for any use that may be made of the information contained herein.</p></ack><sec id="S36" sec-type="data-availability"><title>Availability of data and materials</title><p id="P76">All data used and models produced in this work are actively being uploaded to Pangaea.</p></sec><ref-list><ref id="R1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connell</surname><given-names>JH</given-names></name><name><surname>Hughes</surname><given-names>TP</given-names></name><name><surname>Wallace</surname><given-names>CC</given-names></name><name><surname>Tanner</surname><given-names>JE</given-names></name><name><surname>Harms</surname><given-names>KE</given-names></name><name><surname>Kerr</surname><given-names>AM</given-names></name></person-group><article-title>A long-term study of competition and diversity of corals</article-title><source>Ecol Monogr</source><year>2004</year><volume>74</volume><fpage>179</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1890/02-4043</pub-id></element-citation></ref><ref id="R2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kahn</surname><given-names>AS</given-names></name><name><surname>Pennelly</surname><given-names>CW</given-names></name><name><surname>McGill</surname><given-names>PR</given-names></name><name><surname>Leys</surname><given-names>SP</given-names></name></person-group><article-title>Behaviors of sessile benthic animals in the abyssal northeast Pacific Ocean</article-title><source>Deep Sea Research Part II: Topical Studies in Oceanography</source><year>2020</year><volume>173</volume><elocation-id>104729</elocation-id><pub-id pub-id-type="doi">10.1016/J.DSR2.2019.104729</pub-id></element-citation></ref><ref id="R3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Audenhaege</surname><given-names>L</given-names></name><name><surname>Matabos</surname><given-names>M</given-names></name><name><surname>Brind’Amour</surname><given-names>A</given-names></name><name><surname>Drugmand</surname><given-names>J</given-names></name><name><surname>Laes-Huon</surname><given-names>A</given-names></name><name><surname>Sarradin</surname><given-names>P</given-names></name><name><surname>Sarrazin</surname><given-names>J</given-names></name></person-group><article-title>Long-term monitoring reveals unprecedented stability of a vent mussel assemblage on the Mid-Atlantic Ridge</article-title><source>Prog Oceanogr</source><year>2022</year><volume>204</volume><elocation-id>102791</elocation-id><pub-id pub-id-type="doi">10.1016/J.POCEAN.2022.102791</pub-id></element-citation></ref><ref id="R4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez-vazquez</surname><given-names>V</given-names></name><name><surname>Lopez-guede</surname><given-names>JM</given-names></name><name><surname>Marini</surname><given-names>S</given-names></name><name><surname>Fanelli</surname><given-names>E</given-names></name><name><surname>Johnsen</surname><given-names>E</given-names></name><name><surname>Aguzzi</surname><given-names>J</given-names></name></person-group><article-title>Video Image Enhancement and Machine Learning Pipeline for Underwater Animal Detection and Classification at Cabled Observatories</article-title><source>Sensors</source><year>2020</year><volume>20</volume><fpage>726</fpage><pub-id pub-id-type="pmcid">PMC7038495</pub-id><pub-id pub-id-type="pmid">32012976</pub-id><pub-id pub-id-type="doi">10.3390/s20030726</pub-id></element-citation></ref><ref id="R5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>ID</given-names></name><name><surname>Couch</surname><given-names>C</given-names></name><name><surname>Beijbom</surname><given-names>O</given-names></name><name><surname>Oliver</surname><given-names>T</given-names></name><name><surname>Vargas-Angel</surname><given-names>B</given-names></name><name><surname>Schumacher</surname><given-names>B</given-names></name><name><surname>Brainard</surname><given-names>R</given-names></name></person-group><article-title>Leveraging automated image analysis tools to transform our capacity to assess status and trends on coral reefs</article-title><source>Front Mar Sci</source><year>2019</year><volume>6</volume><fpage>222</fpage><pub-id pub-id-type="doi">10.3389/fmars.2019.00222</pub-id></element-citation></ref><ref id="R6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villon</surname><given-names>S</given-names></name><name><surname>Iovan</surname><given-names>C</given-names></name><name><surname>Mangeas</surname><given-names>M</given-names></name><name><surname>Vigliola</surname><given-names>L</given-names></name></person-group><article-title>Confronting Deep-Learning and Biodiversity Challenges for Automatic Video-Monitoring of Marine Ecosystems</article-title><source>Sensors</source><year>2022</year><volume>22</volume><fpage>497</fpage><pub-id pub-id-type="pmcid">PMC8781840</pub-id><pub-id pub-id-type="pmid">35062457</pub-id><pub-id pub-id-type="doi">10.3390/s22020497</pub-id></element-citation></ref><ref id="R7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Clippele</surname><given-names>LH</given-names></name><name><surname>Huvenne</surname><given-names>VAI</given-names></name><name><surname>Orejas</surname><given-names>C</given-names></name><name><surname>Lundalv</surname><given-names>T</given-names></name><name><surname>Fox</surname><given-names>A</given-names></name><name><surname>Hennige</surname><given-names>SJ</given-names></name><name><surname>Roberts</surname><given-names>JM</given-names></name></person-group><article-title>The effect of local hydrodynamics on the spatial extent and morphology of cold-water coral habitats at Tisler Reef, Norway</article-title><source>Coral Reefs</source><year>2018</year><volume>37</volume><fpage>253</fpage><lpage>266</lpage><pub-id pub-id-type="pmcid">PMC6566294</pub-id><pub-id pub-id-type="pmid">31258386</pub-id><pub-id pub-id-type="doi">10.1007/s00338-017-1653-y</pub-id></element-citation></ref><ref id="R8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>HK</given-names></name><name><surname>Roberts</surname><given-names>EM</given-names></name><name><surname>Rapp</surname><given-names>HT</given-names></name><name><surname>Davies</surname><given-names>AJ</given-names></name></person-group><article-title>Spatial patterns of arctic sponge ground fauna and demersal fish are detectable in autonomous underwater vehicle (AUV) imagery</article-title><source>Deep Sea Research Part I: Oceanographic Research Papers</source><year>2019</year><volume>153</volume><elocation-id>103137</elocation-id><pub-id pub-id-type="doi">10.1016/J.DSR.2019.103137</pub-id></element-citation></ref><ref id="R9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrison</surname><given-names>D</given-names></name><name><surname>De Leo</surname><given-names>FC</given-names></name><name><surname>Gallin</surname><given-names>WJ</given-names></name><name><surname>Mir</surname><given-names>F</given-names></name><name><surname>Marini</surname><given-names>S</given-names></name><name><surname>Leys</surname><given-names>SP</given-names></name></person-group><article-title>Machine Learning Applications of Convolutional Neural Networks and Unet Architecture to Predict and Classify Demosponge Behavior</article-title><source>Water</source><year>2021</year><volume>13</volume><elocation-id>2512</elocation-id><comment>2512</comment><pub-id pub-id-type="doi">10.3390/W13182512</pub-id></element-citation></ref><ref id="R10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purser</surname><given-names>A</given-names></name><name><surname>Bergmann</surname><given-names>M</given-names></name><name><surname>Lundalv</surname><given-names>T</given-names></name><name><surname>Ontrup</surname><given-names>J</given-names></name><name><surname>Nattkemper</surname><given-names>TW</given-names></name></person-group><article-title>Use of machine-learning algorithms for the automated detection of cold-water coral habitats: a pilot study</article-title><source>Mar Ecol Prog Ser</source><year>2009</year><volume>397</volume><fpage>241</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.3354/MEPS08154</pub-id></element-citation></ref><ref id="R11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McMurray</surname><given-names>SE</given-names></name><name><surname>Blum</surname><given-names>JE</given-names></name><name><surname>Pawlik</surname><given-names>JR</given-names></name></person-group><article-title>Redwood of the reef: Growth and age of the giant barrel sponge Xestospongia muta in the Florida Keys</article-title><source>Mar Biol</source><year>2008</year><volume>155</volume><fpage>159</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1007/s00227-008-1014-z</pub-id></element-citation></ref><ref id="R12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vad</surname><given-names>J</given-names></name><name><surname>Orejas</surname><given-names>C</given-names></name><name><surname>Moreno-Navas</surname><given-names>J</given-names></name><name><surname>Findlay</surname><given-names>HS</given-names></name><name><surname>Roberts</surname><given-names>JM</given-names></name></person-group><article-title>Assessing the living and dead proportions of cold-water coral colonies: Implications for deep-water Marine Protected Area monitoring in a changing ocean</article-title><source>PeerJ</source><year>2017</year><volume>2017</volume><elocation-id>e3705</elocation-id><pub-id pub-id-type="pmcid">PMC5632539</pub-id><pub-id pub-id-type="pmid">29018595</pub-id><pub-id pub-id-type="doi">10.7717/peerj.3705</pub-id></element-citation></ref><ref id="R13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>G</given-names></name><name><surname>Ortiz</surname><given-names>J</given-names></name><name><surname>Kaniewska</surname><given-names>P</given-names></name><name><surname>Johnstone</surname><given-names>R</given-names></name></person-group><article-title>Using three-dimensional surface area to compare the growth of two Pocilloporid coral species</article-title><source>Mar Biol</source><year>2008</year><volume>155</volume><fpage>421</fpage><lpage>427</lpage><pub-id pub-id-type="doi">10.1007/s00227-008-1040-x</pub-id></element-citation></ref><ref id="R14"><label>14</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>SR</given-names></name><etal/></person-group><article-title>Reef communities associated with ‘dead’ cold-water coral framework drive resource retention and recycling in the deep sea</article-title><source>Deep Sea Research Part I: Oceanographic Research Papers</source><year>2021</year><volume>175</volume><elocation-id>103574</elocation-id><pub-id pub-id-type="doi">10.1016/J.DSR.2021.103574</pub-id></element-citation></ref><ref id="R15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Clippele</surname><given-names>LH</given-names></name><name><surname>van der Kaaden</surname><given-names>AS</given-names></name><name><surname>Maier</surname><given-names>SR</given-names></name><name><surname>de Froe</surname><given-names>E</given-names></name><name><surname>Roberts</surname><given-names>JM</given-names></name></person-group><article-title>Biomass Mapping for an Improved Understanding of the Contribution of Cold-Water Coral Carbonate Mounds to C and N Cycling</article-title><source>Front Mar Sci</source><year>2021</year><volume>8</volume><elocation-id>1608</elocation-id><pub-id pub-id-type="doi">10.3389/fmars.2021.721062</pub-id></element-citation></ref><ref id="R16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimball</surname><given-names>ME</given-names></name><name><surname>Able</surname><given-names>KW</given-names></name></person-group><article-title>Tidal Migrations of Intertidal Salt Marsh Creek Nekton Examined with Underwater Video</article-title><source>Northeast Nat (Steuben)</source><year>2012</year><volume>19</volume><fpage>475</fpage><lpage>486</lpage><pub-id pub-id-type="doi">10.1656/045.019.0309</pub-id></element-citation></ref><ref id="R17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kipson</surname><given-names>S</given-names></name><name><surname>Fourt</surname><given-names>M</given-names></name><name><surname>Teixido</surname><given-names>N</given-names></name><name><surname>Cebrian</surname><given-names>E</given-names></name><name><surname>Casas</surname><given-names>E</given-names></name><name><surname>Ballesteros</surname><given-names>E</given-names></name><name><surname>Zabala</surname><given-names>M</given-names></name><name><surname>Garrabou</surname><given-names>J</given-names></name></person-group><article-title>Rapid Biodiversity Assessment and Monitoring Method for Highly Diverse Benthic Communities: A Case Study of Mediterranean Coralligenous Outcrops</article-title><source>PLoS One</source><year>2011</year><volume>6</volume><elocation-id>e27103</elocation-id><pub-id pub-id-type="pmcid">PMC3206946</pub-id><pub-id pub-id-type="pmid">22073264</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0027103</pub-id></element-citation></ref><ref id="R18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siders</surname><given-names>ZA</given-names></name><name><surname>Caltabellotta</surname><given-names>FP</given-names></name><name><surname>Loesser</surname><given-names>KB</given-names></name><name><surname>Trotta</surname><given-names>LB</given-names></name><name><surname>Baiser</surname><given-names>B</given-names></name></person-group><article-title>Using pictographs as traits to explore morphological diversity in sharks</article-title><source>Ecol Evol</source><year>2023</year><volume>13</volume><elocation-id>e9761</elocation-id><pub-id pub-id-type="pmcid">PMC9873591</pub-id><pub-id pub-id-type="pmid">36713493</pub-id><pub-id pub-id-type="doi">10.1002/ece3.9761</pub-id></element-citation></ref><ref id="R19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kutti</surname><given-names>T</given-names></name><name><surname>Bannister</surname><given-names>RJ</given-names></name><name><surname>Fossâ</surname><given-names>JH</given-names></name></person-group><article-title>Community structure and ecological function of deep-water sponge grounds in the Traenadypet MPA-Northern Norwegian continental shelf</article-title><source>Cont Shelf Res</source><year>2013</year><volume>69</volume><fpage>21</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1016/j.csr.2013.09.011</pub-id></element-citation></ref><ref id="R20"><label>20</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Clippele</surname><given-names>LH</given-names></name><name><surname>Rovelli</surname><given-names>L</given-names></name><name><surname>Ramiro-Sánchez</surname><given-names>B</given-names></name><name><surname>Kazanidis</surname><given-names>G</given-names></name><name><surname>Vad</surname><given-names>J</given-names></name><name><surname>Turner</surname><given-names>S</given-names></name><name><surname>Glud</surname><given-names>RN</given-names></name><name><surname>Roberts</surname><given-names>JM</given-names></name></person-group><article-title>Mapping cold-water coral biomass: an approach to derive ecosystem functions</article-title><source>Coral Reefs</source><year>2021</year><volume>40</volume><fpage>215</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1007/s00338-020-02030-5</pub-id></element-citation></ref><ref id="R21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reiswig</surname><given-names>HM</given-names></name></person-group><article-title>In situ pumping activities of tropical Demospongiae</article-title><source>Mar Biol</source><year>1971</year><volume>9</volume><fpage>38</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/BF00348816</pub-id></element-citation></ref><ref id="R22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holbrook</surname><given-names>SJ</given-names></name><name><surname>Brooks</surname><given-names>AJ</given-names></name><name><surname>Schmitt</surname><given-names>RJ</given-names></name><name><surname>Stewart</surname><given-names>HL</given-names></name></person-group><article-title>Effects of sheltering fish on growth of their host corals</article-title><source>Mar Biol</source><year>2008</year><volume>155</volume><fpage>521</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1007/s00227-008-1051-7</pub-id></element-citation></ref><ref id="R23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Benthic Organism Detection, Quantification and Seamount Biology Detection Based on Deep Learning</article-title><source>Artificial Intelligence Oceanography</source><year>2023</year><fpage>323</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1007/978-981-19-6375-9_16</pub-id></element-citation></ref><ref id="R24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villon</surname><given-names>S</given-names></name><name><surname>Mouillot</surname><given-names>D</given-names></name><name><surname>Chaumont</surname><given-names>M</given-names></name><name><surname>Darling</surname><given-names>ES</given-names></name><name><surname>Subsol</surname><given-names>G</given-names></name><name><surname>Claverie</surname><given-names>T</given-names></name><name><surname>Villéger</surname><given-names>S</given-names></name></person-group><article-title>A Deep learning method for accurate and fast identification of coral reef fishes in underwater images</article-title><source>Ecol Inform</source><year>2018</year><volume>48</volume><fpage>238</fpage><lpage>244</lpage><pub-id pub-id-type="doi">10.1016/J.EC0INF.2018.09.007</pub-id></element-citation></ref><ref id="R25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zurowietz</surname><given-names>M</given-names></name><name><surname>Langenkämper</surname><given-names>D</given-names></name><name><surname>Hosking</surname><given-names>B</given-names></name><name><surname>Ruhl</surname><given-names>HA</given-names></name><name><surname>Nattkemper</surname><given-names>TW</given-names></name></person-group><article-title>MAIA-A machine learning assisted image annotation method for environmental monitoring and exploration</article-title><source>PLoS One</source><year>2018</year><volume>13</volume><elocation-id>e0207498</elocation-id><pub-id pub-id-type="pmcid">PMC6239313</pub-id><pub-id pub-id-type="pmid">30444917</pub-id><pub-id pub-id-type="doi">10.1371/journal.pone.0207498</pub-id></element-citation></ref><ref id="R26"><label>26</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zuazo</surname><given-names>A</given-names></name><etal/></person-group><article-title>An Automated Pipeline for Image Processing and Data Treatment to Track Activity Rhythms of Paragorgia arborea in Relation to Hydrographic Conditions</article-title><source>Sensors</source><year>2020</year><volume>20</volume><elocation-id>6281</elocation-id><pub-id pub-id-type="pmcid">PMC7662914</pub-id><pub-id pub-id-type="pmid">33158174</pub-id><pub-id pub-id-type="doi">10.3390/s20216281</pub-id></element-citation></ref><ref id="R27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Osterloff</surname><given-names>J</given-names></name><name><surname>Nilssen</surname><given-names>I</given-names></name><name><surname>Järnegren</surname><given-names>J</given-names></name><name><surname>Van Engeland</surname><given-names>T</given-names></name><name><surname>Buhl-Mortensen</surname><given-names>P</given-names></name><name><surname>Nattkemper</surname><given-names>TW</given-names></name></person-group><article-title>Computer vision enables short-and long-term analysis of Lophelia pertusa polyp behaviour and colour from an underwater observatory</article-title><source>Scientific Reports</source><year>2019</year><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmcid">PMC6488648</pub-id><pub-id pub-id-type="pmid">31036904</pub-id><pub-id pub-id-type="doi">10.1038/s41598-019-41275-1</pub-id></element-citation></ref><ref id="R28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leys</surname><given-names>SP</given-names></name><name><surname>Mah</surname><given-names>JL</given-names></name><name><surname>McGill</surname><given-names>PR</given-names></name><name><surname>Hamonic</surname><given-names>L</given-names></name><name><surname>De Leo</surname><given-names>FC</given-names></name><name><surname>Kahn</surname><given-names>AS</given-names></name></person-group><article-title>Sponge Behavior and the Chemical Basis of Responses: A Post-Genomic View</article-title><source>Integr Comp Biol</source><year>2019</year><volume>59</volume><fpage>751</fpage><lpage>764</lpage><pub-id pub-id-type="pmid">31268144</pub-id></element-citation></ref><ref id="R29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Möller</surname><given-names>T</given-names></name><name><surname>Nilssen</surname><given-names>I</given-names></name><name><surname>Nattkemper</surname><given-names>TW</given-names></name></person-group><article-title>Tracking Sponge Size and Behaviour with Fixed Underwater Observatories</article-title><source>Lecture Notes in Computer Science</source><year>2018</year><volume>11188</volume><fpage>45</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-05792-3_5</pub-id></element-citation></ref><ref id="R30"><label>30</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purser</surname><given-names>A</given-names></name><name><surname>Orejas</surname><given-names>C</given-names></name><name><surname>Gori</surname><given-names>A</given-names></name><name><surname>Tong</surname><given-names>R</given-names></name><name><surname>Unnithan</surname><given-names>V</given-names></name><name><surname>Thomsen</surname><given-names>L</given-names></name></person-group><article-title>Local variation in the distribution of benthic megafauna species associated with cold-water coral reefs on the Norwegian margin</article-title><source>Cont Shelf Res</source><year>2013</year><volume>54</volume><fpage>37</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/J.CSR.2012.12.013</pub-id></element-citation></ref><ref id="R31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AG</given-names></name><name><surname>Han</surname><given-names>E</given-names></name><name><surname>Petersen</surname><given-names>J</given-names></name><name><surname>Olsen</surname><given-names>NAF</given-names></name><name><surname>Giese</surname><given-names>C</given-names></name><name><surname>Athmann</surname><given-names>M</given-names></name><name><surname>Dresboll</surname><given-names>DB</given-names></name><name><surname>Thorup-Kristensen</surname><given-names>K</given-names></name></person-group><article-title>RootPainter: deep learning segmentation of biological images with corrective annotation</article-title><source>New Phytologist</source><year>2022</year><volume>236</volume><fpage>774</fpage><lpage>791</lpage><pub-id pub-id-type="pmcid">PMC9804377</pub-id><pub-id pub-id-type="pmid">35851958</pub-id><pub-id pub-id-type="doi">10.1111/nph.18387</pub-id></element-citation></ref><ref id="R32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Fischer</surname><given-names>P</given-names></name><name><surname>Brox</surname><given-names>T</given-names></name></person-group><article-title>U-net: Convolutional networks for biomedical image segmentation</article-title><source>Lecture Notes in Computer Science</source><year>2015</year><volume>9351</volume><fpage>234</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.48550/arXiv.1505.04597</pub-id></element-citation></ref><ref id="R33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maharana</surname><given-names>K</given-names></name><name><surname>Mondal</surname><given-names>S</given-names></name><name><surname>Nemade</surname><given-names>B</given-names></name></person-group><article-title>A review: Data pre-processing and data augmentation techniques</article-title><source>Global Transitions Proceedings</source><year>2022</year><volume>3</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/J.GLTP.2022.04.020</pub-id></element-citation></ref><ref id="R34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso-Crespo</surname><given-names>IM</given-names></name><name><surname>Weidlich</surname><given-names>EWA</given-names></name><name><surname>Temperton</surname><given-names>VM</given-names></name><name><surname>Delory</surname><given-names>BM</given-names></name></person-group><article-title>Assembly history modulates vertical root distribution in a grassland experiment</article-title><source>Oikos</source><year>2023</year><volume>2023</volume><pub-id pub-id-type="doi">10.1111/OIK.08886</pub-id></element-citation></ref><ref id="R35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamov</surname><given-names>R</given-names></name><name><surname>Breite</surname><given-names>C</given-names></name><name><surname>Lomov</surname><given-names>SV</given-names></name><name><surname>Sergeichev</surname><given-names>I</given-names></name><name><surname>Swolfs</surname><given-names>Y</given-names></name></person-group><article-title>Super-Resolution Processing of Synchrotron CT Images for Automated Fibre Break Analysis of Unidirectional Composites</article-title><source>Polymers (Basel)</source><year>2023</year><volume>15</volume><elocation-id>2206</elocation-id><pub-id pub-id-type="pmcid">PMC10180951</pub-id><pub-id pub-id-type="pmid">37177352</pub-id><pub-id pub-id-type="doi">10.3390/polym15092206</pub-id></element-citation></ref><ref id="R36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sell</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>AG</given-names></name><name><surname>Burdun</surname><given-names>I</given-names></name><name><surname>Rohula-Okunev</surname><given-names>G</given-names></name><name><surname>Kupper</surname><given-names>P</given-names></name><name><surname>Ostonen</surname><given-names>I</given-names></name></person-group><article-title>Assessing the fine root growth dynamics of Norway spruce manipulated by air humidity and soil nitrogen with deep learning segmentation of smartphone images</article-title><source>Plant Soil</source><year>2022</year><volume>480</volume><fpage>135</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1007/s11104-022-05565-4</pub-id></element-citation></ref><ref id="R37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monnens</surname><given-names>D</given-names></name><name><surname>Denison</surname><given-names>RF</given-names></name><name><surname>Sadok</surname><given-names>W</given-names></name></person-group><article-title>Rising vapor-pressure deficit increases nitrogen fixation in a legume crop</article-title><source>New Phytologist</source><year>2023</year><volume>239</volume><fpage>54</fpage><lpage>65</lpage><pub-id pub-id-type="pmid">37097254</pub-id></element-citation></ref><ref id="R38"><label>38</label><element-citation publication-type="web"><collab>LoVeOcean</collab><source>Lofoten Vesterälen Ocean Observatory See</source><year>2019</year><date-in-citation>accessed on 15 June 2022</date-in-citation><comment><ext-link ext-link-type="uri" xlink:href="https://loveocean.no/">https://loveocean.no/</ext-link></comment></element-citation></ref><ref id="R39"><label>39</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lavaleye</surname><given-names>M</given-names></name><name><surname>Duineveld</surname><given-names>G</given-names></name><name><surname>Lundälv</surname><given-names>T</given-names></name><name><surname>White</surname><given-names>M</given-names></name><name><surname>Guihen</surname><given-names>D</given-names></name><name><surname>Kiriakoulakis</surname><given-names>K</given-names></name><name><surname>Wolff</surname><given-names>GA</given-names></name></person-group><article-title>Coldwater corals on the tisler reef: preliminary observations on the dynamic reef environment</article-title><source>Oceanography</source><year>2009</year><volume>22</volume><fpage>76</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.5670/oceanog.2009.08</pub-id></element-citation></ref><ref id="R40"><label>40</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Addamo</surname><given-names>AM</given-names></name><name><surname>Vertino</surname><given-names>A</given-names></name><name><surname>Stolarski</surname><given-names>J</given-names></name><name><surname>Garcia-Jimenez</surname><given-names>R</given-names></name><name><surname>Taviani</surname><given-names>M</given-names></name><name><surname>Machordom</surname><given-names>A</given-names></name></person-group><article-title>Merging scleractinian genera: The overwhelming genetic similarity between solitary Desmophyllum and colonial Lophelia</article-title><source>BMC Evol Biol</source><year>2016</year><volume>16</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="pmcid">PMC4870751</pub-id><pub-id pub-id-type="pmid">27193263</pub-id><pub-id pub-id-type="doi">10.1186/s12862-016-0654-8</pub-id></element-citation></ref><ref id="R41"><label>41</label><element-citation publication-type="web"><collab>WoRMS Editorial Board</collab><source>World Register of Marine Species</source><year>2023</year><date-in-citation>accessed on 15 June 2022</date-in-citation><comment>See <ext-link ext-link-type="uri" xlink:href="https://www.marinespecies.org">https://www.marinespecies.org</ext-link></comment></element-citation></ref><ref id="R42"><label>42</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pile</surname><given-names>AJ</given-names></name><name><surname>Patterson</surname><given-names>MR</given-names></name><name><surname>Witman</surname><given-names>JD</given-names></name></person-group><article-title>In situ grazing on plankton &lt;10 um by the boreal sponge Mycale lingua</article-title><source>Mar Ecol Prog Ser</source><year>1996</year><volume>141</volume><fpage>95</fpage><lpage>102</lpage></element-citation></ref><ref id="R43"><label>43</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>CW</given-names></name><name><surname>Grehan</surname><given-names>AJ</given-names></name><name><surname>Kahui</surname><given-names>V</given-names></name><name><surname>Mikkelsen</surname><given-names>E</given-names></name><name><surname>Reithe</surname><given-names>S</given-names></name><name><surname>Van Den Hove</surname><given-names>S</given-names></name></person-group><article-title>Bioeconomic Modeling and the Management of Cold-Water Coral Resources</article-title><source>Oceanography</source><year>2009</year><volume>22</volume><fpage>86</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.2307/24860927</pub-id></element-citation></ref><ref id="R44"><label>44</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maier</surname><given-names>SR</given-names></name><name><surname>Kutti</surname><given-names>T</given-names></name><name><surname>Bannister</surname><given-names>RJ</given-names></name><name><surname>Fang</surname><given-names>JKH</given-names></name><name><surname>van Breugel</surname><given-names>P</given-names></name><name><surname>van Rijswijk</surname><given-names>P</given-names></name><name><surname>van Oevelen</surname><given-names>D</given-names></name></person-group><article-title>Recycling pathways in cold-water coral reefs: Use of dissolved organic matter and bacteria by key suspension feeding taxa</article-title><source>Sci Rep</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmcid">PMC7303112</pub-id><pub-id pub-id-type="pmid">32555406</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-66463-2</pub-id></element-citation></ref><ref id="R45"><label>45</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Witman</surname><given-names>J</given-names></name><name><surname>Kenneth</surname><given-names>S</given-names></name></person-group><article-title>Distribution and Ecology of Sponges at Subtidal Rock Ledge in the Gulf of Maine</article-title><source>New Perspectives in Sponge Biology</source><year>1990</year></element-citation></ref><ref id="R46"><label>46</label><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AG</given-names></name></person-group><source>RootPainterSetup - Colaboratory</source><year>2023</year><date-in-citation>accessed on 17 August 2022</date-in-citation><comment>See <ext-link ext-link-type="uri" xlink:href="https://colab.research.google.com/drive/104narYAvTBt-X4QEDrBSOZm_DRaAKHtA#scrollTo=eWrf_imcXnGi">https://colab.research.google.com/drive/104narYAvTBt-X4QEDrBSOZm_DRaAKHtA#scrollTo=eWrf_imcXnGi</ext-link></comment></element-citation></ref><ref id="R47"><label>47</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>HP</given-names></name><name><surname>Smith</surname><given-names>AG</given-names></name><name><surname>De Clippele</surname><given-names>LH</given-names></name></person-group><source>Marine Image Analysis Handbook for RootPainter</source><year>2023</year><pub-id pub-id-type="doi">10.5281/ZENODO.7984565</pub-id></element-citation></ref><ref id="R48"><label>48</label><element-citation publication-type="other"><person-group person-group-type="author"><name><surname>van der Kaaden</surname><given-names>A</given-names></name><name><surname>De Clippele</surname><given-names>LH</given-names></name></person-group><source>Image/Video Annotation and Analysis</source><year>2021</year><pub-id pub-id-type="doi">10.5281/ZENODO.4575809</pub-id></element-citation></ref><ref id="R49"><label>49</label><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Alpaydin</surname><given-names>E</given-names></name></person-group><source>Machine Learning</source><publisher-name>The MIT Press</publisher-name><year>2021</year><pub-id pub-id-type="doi">10.7551/MITPRESS/13811.001.0001</pub-id></element-citation></ref><ref id="R50"><label>50</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Handelman</surname><given-names>GS</given-names></name><name><surname>Kok</surname><given-names>HK</given-names></name><name><surname>Chandra</surname><given-names>RV</given-names></name><name><surname>Razavi</surname><given-names>AH</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Brooks</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>MJ</given-names></name><name><surname>Asadi</surname><given-names>H</given-names></name></person-group><article-title>Peering into the black box of artificial intelligence: Evaluation metrics of machine learning methods</article-title><source>American Journal of Roentgenology</source><year>2019</year><volume>212</volume><fpage>38</fpage><lpage>43</lpage><pub-id pub-id-type="pmid">30332290</pub-id></element-citation></ref><ref id="R51"><label>51</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>X</given-names></name></person-group><article-title>Benthic Organism Detection, Quantification and Seamount Biology Detection Based on Deep Learning</article-title><source>Artificial Intelligence Oceanography</source><year>2023</year><fpage>323</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1007/978-981-19-6375-9_16</pub-id></element-citation></ref><ref id="R52"><label>52</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>E</given-names></name><name><surname>Kirkegaard</surname><given-names>JA</given-names></name><name><surname>White</surname><given-names>R</given-names></name><name><surname>Smith</surname><given-names>AG</given-names></name><name><surname>Thorup-Kristensen</surname><given-names>K</given-names></name><name><surname>Kautz</surname><given-names>T</given-names></name><name><surname>Athmann</surname><given-names>M</given-names></name></person-group><article-title>Deep learning with multisite data reveals the lasting effects of soil type, tillage and vegetation history on biopore genesis</article-title><source>Geoderma</source><year>2022</year><volume>425</volume><elocation-id>116072</elocation-id><pub-id pub-id-type="doi">10.1016/J.GEODERMA.2022.116072</pub-id></element-citation></ref><ref id="R53"><label>53</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malinowska</surname><given-names>M</given-names></name><etal/></person-group><article-title>Relative importance of genotype, gene expression, and DNA methylation on complex traits in perennial ryegrass</article-title><source>Plant Genome</source><year>2022</year><volume>15</volume><pub-id pub-id-type="pmid">35975565</pub-id></element-citation></ref><ref id="R54"><label>54</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bauer</surname><given-names>FM</given-names></name><name><surname>Lärm</surname><given-names>L</given-names></name><name><surname>Morandage</surname><given-names>S</given-names></name><name><surname>Lobet</surname><given-names>G</given-names></name><name><surname>Vanderborght</surname><given-names>J</given-names></name><name><surname>Vereecken</surname><given-names>H</given-names></name><name><surname>Schnepf</surname><given-names>A</given-names></name></person-group><article-title>Development and Validation of a Deep Learning Based Automated Minirhizotron Image Analysis Pipeline</article-title><source>Plant Phenomics</source><year>2022</year><volume>28</volume><pub-id pub-id-type="pmcid">PMC9168891</pub-id><pub-id pub-id-type="pmid">35693120</pub-id><pub-id pub-id-type="doi">10.34133/2022/9758532</pub-id></element-citation></ref><ref id="R55"><label>55</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>AG</given-names></name><name><surname>Kutnar</surname><given-names>D</given-names></name><name><surname>Vogelius</surname><given-names>IR</given-names></name><name><surname>Darkner</surname><given-names>S</given-names></name><name><surname>Petersen</surname><given-names>J</given-names></name></person-group><article-title>Localise to segment: crop to improve organ at risk segmentation accuracy</article-title><source>ArXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2304.04606</pub-id></element-citation></ref><ref id="R56"><label>56</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langenkämper</surname><given-names>D</given-names></name><name><surname>Zurowietz</surname><given-names>M</given-names></name><name><surname>Schoening</surname><given-names>T</given-names></name><name><surname>Nattkemper</surname><given-names>TW</given-names></name></person-group><article-title>BIIGLE 2.0-browsing and annotating large marine image collections</article-title><source>Front Mar Sci</source><year>2017</year><volume>4</volume><fpage>83</fpage><pub-id pub-id-type="doi">10.3389/fmars.2017.00083</pub-id></element-citation></ref><ref id="R57"><label>57</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-de-Mariscal</surname><given-names>E</given-names></name><name><surname>Garcia-Lopez-de-Haro</surname><given-names>C</given-names></name><name><surname>Ouyang</surname><given-names>W</given-names></name><name><surname>Donati</surname><given-names>L</given-names></name><name><surname>Lundberg</surname><given-names>E</given-names></name><name><surname>Unser</surname><given-names>M</given-names></name><name><surname>Munoz-Barrutia</surname><given-names>A</given-names></name><name><surname>Sage</surname><given-names>D</given-names></name></person-group><article-title>DeepImageJ: A user-friendly environment to run deep learning models in ImageJ</article-title><source>Nature Methods</source><year>2021</year><volume>18</volume><fpage>1192</fpage><lpage>1195</lpage><comment>18:10</comment><pub-id pub-id-type="pmid">34594030</pub-id></element-citation></ref><ref id="R58"><label>58</label><element-citation publication-type="web"><source>deepImageJ</source><year>2023</year><date-in-citation>accessed on 28 February 2023</date-in-citation><comment>See <ext-link ext-link-type="uri" xlink:href="https://deepimagej.github.io/">https://deepimagej.github.io/</ext-link></comment></element-citation></ref><ref id="R59"><label>59</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivenson</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Virtual histological staining of unlabelled tissue-autofluorescence images via deep learning</article-title><source>Nat Biomed Eng</source><year>2019</year><volume>3</volume><fpage>466</fpage><lpage>477</lpage><pub-id pub-id-type="pmid">31142829</pub-id></element-citation></ref><ref id="R60"><label>60</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beier</surname><given-names>T</given-names></name><etal/></person-group><article-title>Multicut brings automated neurite segmentation closer to human performance</article-title><source>Nat Methods</source><year>2017</year><volume>14</volume><fpage>101</fpage><lpage>102</lpage><pub-id pub-id-type="pmid">28139671</pub-id></element-citation></ref><ref id="R61"><label>61</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kataria</surname><given-names>T</given-names></name><name><surname>Knudsen</surname><given-names>B</given-names></name><name><surname>Elhabian</surname><given-names>S</given-names></name></person-group><article-title>To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology</article-title><source>ArXiv</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2307.03275</pub-id></element-citation></ref><ref id="R62"><label>62</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>JM</given-names></name><name><surname>Khoshgoftaar</surname><given-names>TM</given-names></name></person-group><article-title>Survey on deep learning with class imbalance</article-title><source>J Big Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1186/s40537-019-0192-5</pub-id></element-citation></ref><ref id="R63"><label>63</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reiswig</surname><given-names>HM</given-names></name></person-group><article-title>Bacteria as food for temperate-water marine sponges</article-title><source>Can J Zool</source><year>1975</year><volume>53</volume><fpage>582</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1139/Z75-072</pub-id></element-citation></ref><ref id="R64"><label>64</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>GRD</given-names></name><name><surname>Leys</surname><given-names>SP</given-names></name></person-group><article-title>Coordinated contractions effectively expel water from the aquiferous system of a freshwater sponge</article-title><source>Journal of Experimental Biology</source><year>2007</year><volume>210</volume><fpage>3736</fpage><lpage>3748</lpage><pub-id pub-id-type="pmid">17951414</pub-id></element-citation></ref><ref id="R65"><label>65</label><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bart</surname><given-names>MC</given-names></name><name><surname>Hudspith</surname><given-names>M</given-names></name><name><surname>Rapp</surname><given-names>HT</given-names></name><name><surname>Verdonschot</surname><given-names>PFM</given-names></name><name><surname>de Goeij</surname><given-names>JM</given-names></name></person-group><article-title>A Deep-Sea Sponge Loop? Sponges Transfer Dissolved and Particulate Organic Carbon and Nitrogen to Associated Fauna</article-title><source>Front Mar Sci</source><year>2021</year><volume>8</volume><fpage>229</fpage><pub-id pub-id-type="doi">10.3389/FMARS.2021.604879</pub-id></element-citation></ref></ref-list></back><floats-group><fig id="F1" position="float"><label>Figure 1</label><caption><p>(a) Map of Norway highlighting the locations of the LoVe Ocean Observatory and Tisler reef, (b) Subsea layout of the LoVe Ocean Observatory including the satellite 1 structure responsible for collecting data used in this study and an example of the raw 5202 x 3464 pixel image output from satellite 1 during 2017-19 (adapted from [38]), (c) Image data availability from the LoVe observatory between 2017 and 2019, (d) Bathymetry map of the Tisler reef, with the 2021 ROV survey area pin-pointed.</p></caption><graphic xlink:href="EMS189775-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><title>Model development workflow.</title><p>Magnus and Mini from the LoVe Observatory were cropped into separate images, forming datasets of 9,173 images each. The images of Magnus from 2019 were uploaded to GoogleDrive forming a training dataset. During RootPainter model training, the algorithm presented successive random images from the training dataset, along with its prediction for that image. The user then corrected this prediction, highlighting in green pixels that should be included background and in red pixels that should be included in the foreground. The continuous visual feedback loop and accompanying metrics allowed determination of the endpoint of training, producing Model 1. This model was then applied to the Magnus, Mini and ROV frame datasets (the ROV images are shown at 1.5 times their true size, relative to the LoVe observatory, for improved visualisation). After checking the segmentation outputs, further model training was clearly required on April/May 2018/19 for Magnus, and on images from 2018 for Mini; significant further training of Model 1 was required on the ROV frames. This additional training produced Models 1.1, 2 and 3. All four models were then applied to their total respective datasets.</p></caption><graphic xlink:href="EMS189775-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><title>RootPainter predictions for images that appeared 1<sup>st</sup>, 200<sup>th</sup> and 530<sup>th</sup> during training of Model 1, where segmentation by the model is shown in light blue overlaying the input image.</title><p>Accompanying corrective annotation metrics graphs displaying changes in precision, recall, dice score and accuracy of Model 1, as calculated by RootPainter during training. Values are displayed until image 530; the additional 120 images used in training RootPainter to recognise Magnus (<xref ref-type="table" rid="T1">Table 1</xref>), developed Model 1.1 (<xref ref-type="supplementary-material" rid="SD1">Supplementary Information Figure 1 and 2</xref>).</p></caption><graphic xlink:href="EMS189775-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><title>Comparison of Magnus’ area values as predicted by RootPainter and measured manually in Photoshop.</title><p>Area highlighted in grey represents period during which no image data was available from the LoVe Observatory.</p></caption><graphic xlink:href="EMS189775-f004"/></fig><fig id="F5" position="float"><label>Figure 5</label><caption><title>Examples of successful segmentations by RootPainter Model 3 and their accompanying metrics.</title><p>Where P = precision, R = recall, D = dice score, A = accuracy and AE = area error / cm<sup>2</sup>. The images show; (A) <italic>Geodia</italic> spp. that is not mis-identified as <italic>M. lingua</italic>, (B) <italic>M. lingua</italic> individuals accurately segmented within <italic>L. pertusa</italic> and (C) <italic>M. lingua</italic> segmented accurately with nearby <italic>Geodia</italic> spp..</p></caption><graphic xlink:href="EMS189775-f005"/></fig><fig id="F6" position="float"><label>Figure 6</label><caption><p>Graphs displaying changes in scaled area errors during training of: (a) Model 1, (b) Model 2 and (c) Model 3. (d) Graph demonstrating the correlation between <italic>M. lingua</italic> surface area as predicted by RootPainter and corrected during training for images 730-1130. The predicted area consists of all pixels RootPainter classified as <italic>M. lingua</italic> for each training image. The corrected area consists of all the pixels RootPainter classified as <italic>M. lingua</italic>, minus those the user highlights in green and plus additional pixels the user highlights in red.</p></caption><graphic xlink:href="EMS189775-f006"/></fig><fig id="F7" position="float"><label>Figure 7</label><caption><title>Relative areas of Magnus and Mini whilst sea-stars reside on the base of Magnus.</title></caption><graphic xlink:href="EMS189775-f007"/></fig><table-wrap id="T1" orientation="portrait" position="float"><label>Table 1</label><caption><title>Training and application data for RootPainter Models 1, 2, 3 and 4.</title><p>Additional learning time refers to time connected to GPU where no annotations were performed but training was left running to enable the model to better fit the existing annotations. The total images for Magnus include 120 additional images used to optimise Model 1 to turbid images during a colour texture change in April/May (<xref ref-type="fig" rid="F2">Figure 2</xref>, <xref ref-type="supplementary-material" rid="SD1">Supplementary Information Table 1</xref>).</p></caption><table frame="hsides" rules="none"><thead><tr><th align="center" valign="middle">RootPainter Model</th><th align="center" valign="middle">1</th><th align="center" valign="middle">2</th><th align="center" valign="middle">3</th><th align="center" valign="middle">4</th></tr><tr><th align="center" valign="middle">Subject of Interest</th><th align="center" valign="middle">Magnus</th><th align="center" valign="middle">Mini</th><th align="center" valign="middle"><italic>Mycale lingua</italic></th><th align="center" valign="middle">Lasers</th></tr><tr><th align="center" valign="middle">Image Source</th><th align="center" valign="middle">LoVe Observatory</th><th align="center" valign="middle">LoVe Observatory</th><th align="center" valign="middle">Tisler Reef ROV</th><th align="center" valign="middle">Tisler Reef ROV</th></tr></thead><tbody><tr style="background-color:#D6D7D6"><td align="center" valign="middle" colspan="5"><bold>Training</bold></td></tr><tr><td align="center" valign="middle">Dataset</td><td align="center" valign="middle">2019 and April/May 2018</td><td align="center" valign="middle">2018</td><td align="center" valign="middle">2021 Tisler East</td><td align="center" valign="middle">2021 Tisler East</td></tr><tr><td align="center" valign="middle">Total images correctively annotated</td><td align="center" valign="middle">640</td><td align="center" valign="middle">142</td><td align="center" valign="middle">556</td><td align="center" valign="middle">100</td></tr><tr><td align="center" valign="middle">Corrective annotation time (hours)</td><td align="center" valign="middle">17</td><td align="center" valign="middle">3.5</td><td align="center" valign="middle">10.5</td><td align="center" valign="middle">0.75</td></tr><tr><td align="center" valign="middle">Additional learning time (hours)</td><td align="center" valign="middle">8</td><td align="center" valign="middle">3</td><td align="center" valign="middle">0</td><td align="center" valign="middle">0</td></tr><tr style="background-color:#D6D7D6"><td align="center" valign="middle" colspan="5"><bold>Application</bold></td></tr><tr><td align="center" valign="middle">Dataset</td><td align="center" valign="middle">2017/18/19</td><td align="center" valign="middle">2017/18/19</td><td align="center" valign="middle">2021 Tisler East</td><td align="center" valign="middle">2021 Tisler East</td></tr><tr><td align="center" valign="middle">Total images segmented</td><td align="center" valign="middle">9,173</td><td align="center" valign="middle">9,173</td><td align="center" valign="middle">1,420</td><td align="center" valign="middle">1,420</td></tr><tr><td align="center" valign="middle">Segmentation time per image (seconds)</td><td align="center" valign="middle">10.7</td><td align="center" valign="middle">3.0</td><td align="center" valign="middle">1.1</td><td align="center" valign="middle">1.1</td></tr></tbody></table></table-wrap><table-wrap id="T2" orientation="portrait" position="float"><label>Table 2</label><caption><title>Analysis time in seconds per image for Manual and RootPainter methods.</title><p>Active user time for the manual method only includes the annotation times; for RootPainter it is the corrective annotation times and post-processing times combined. Inactive time for the manual method only includes the image area extraction time in R; for RootPainter this is the additional learning time and segmentation times (area extraction times were negligible for RootPainter). Magnus and Mini are both <italic>M. lingua</italic> individuals.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle">Subject of Interest</th><th align="center" valign="middle">Annotation Method</th><th align="center" valign="middle">Image Source</th><th align="center" valign="middle">Active user time (s per image analysed)</th><th align="center" valign="middle">Inactive user time (s per image analysed)</th><th align="center" valign="middle">Total time required (s per image analysed)</th></tr></thead><tbody><tr><td align="center" valign="middle">Magnus</td><td align="center" valign="middle">Manual</td><td align="center" valign="middle">Underwater Observatory</td><td align="center" valign="middle">105</td><td align="center" valign="middle">26.0</td><td align="center" valign="middle">131</td></tr><tr><td align="center" valign="middle">Magnus</td><td align="center" valign="middle">RootPainter</td><td align="center" valign="middle">Underwater Observatory</td><td align="center" valign="middle">9.09</td><td align="center" valign="middle">13.8</td><td align="center" valign="middle">22.9</td></tr><tr><td align="center" valign="middle">Mini</td><td align="center" valign="middle">RootPainter</td><td align="center" valign="middle">Underwater Observatory</td><td align="center" valign="middle">3.80</td><td align="center" valign="middle">4.18</td><td align="center" valign="middle">7.98</td></tr><tr><td align="center" valign="middle"><italic>M. lingua</italic></td><td align="center" valign="middle">RootPainter</td><td align="center" valign="middle">ROV</td><td align="center" valign="middle">26.7</td><td align="center" valign="middle">1.1</td><td align="center" valign="middle">27.7</td></tr></tbody></table></table-wrap><table-wrap id="T3" orientation="portrait" position="float"><label>Table 3</label><caption><title>Performance metrics for Model 1.</title><p>Values from manual validation were calculated as a total result of overlaying all 452 manual annotations and their corresponding RootPainter predictions, meaning calculation of a standard deviation is not possible.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle">Model</th><th align="center" valign="middle">Calculation source</th><th align="center" valign="middle">Precision</th><th align="center" valign="middle">Recall</th><th align="center" valign="middle">Dice Score</th><th align="center" valign="middle">Accuracy</th><th align="center" valign="middle">Training images used to calculate average</th></tr></thead><tbody><tr><td align="center" valign="middle">1</td><td align="center" valign="middle">Manual validation</td><td align="center" valign="middle">0.95</td><td align="center" valign="middle">0.92</td><td align="center" valign="middle">0.94</td><td align="center" valign="middle">1.00</td><td align="center" valign="middle">NA</td></tr><tr><td align="center" valign="middle">1</td><td align="center" valign="middle">RootPainter corrective metrics</td><td align="center" valign="middle">0.97 ± 0.04</td><td align="center" valign="middle">0.96 ± 0.06</td><td align="center" valign="middle">0.96 ± 0.03</td><td align="center" valign="middle">1.00 ± 0.00</td><td align="center" valign="middle">430-530</td></tr></tbody></table></table-wrap><table-wrap id="T4" orientation="portrait" position="float"><label>Table 4</label><caption><title>Average area errors for Models 1, 2, and 3 as calculated by RootPainter during training.</title><p>Average area error as a percentage was calculated using the average size of Magnus in 2019 for Model 1, Mini in 2018 for Model 2 as this was the data used in training the models. The percentage area error cannot be accurately estimated for Model 3 due to the wide range of sponge sizes within the data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle" rowspan="2">Model</th><th align="center" valign="middle" colspan="2">Average area error</th><th align="center" valign="middle" rowspan="2">Training images used to calculate average</th></tr><tr style="border-top: solid thin"><th align="center" valign="middle">cm<sup>2</sup></th><th align="center" valign="middle">%</th></tr></thead><tbody><tr><td align="center" valign="middle">1</td><td align="center" valign="middle">−0.06 ± 2.87</td><td align="center" valign="middle">0.14 ± 6.67</td><td align="center" valign="middle">430-530</td></tr><tr><td align="center" valign="middle">2</td><td align="center" valign="middle">0.45 ± 0.86</td><td align="center" valign="middle">4.05 ± 7.75</td><td align="center" valign="middle">100-150</td></tr><tr><td align="center" valign="middle">3</td><td align="center" valign="middle">7.09 ± 52.97</td><td align="center" valign="middle">NA</td><td align="center" valign="middle">730-1130</td></tr></tbody></table></table-wrap><table-wrap id="T5" orientation="portrait" position="float"><label>Table 5</label><caption><title>Average 2D size of Magnus and Mini at the LoVe Observatory in the monitored months of 2018/19.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="center" valign="middle">Year</th><th align="center" valign="middle" colspan="2">Average 2D sponge surface area (cm<sup>2</sup>)</th></tr><tr><th align="center" valign="middle"/><th align="center" valign="middle">Magnus</th><th align="center" valign="middle">Mini</th></tr></thead><tbody><tr><td align="center" valign="middle">2018</td><td align="center" valign="middle">59.9 ± 3.8</td><td align="center" valign="middle">11.1 ± 2.2</td></tr><tr><td align="center" valign="middle">2019</td><td align="center" valign="middle">43.0 ± 9.8</td><td align="center" valign="middle">6.1 ± 0.8</td></tr></tbody></table></table-wrap></floats-group></article>