<!DOCTYPE article
 PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="preprint"><?all-math-mml yes?><?use-mml?><?origin ukpmcpa?><front><journal-meta><journal-id journal-id-type="nlm-ta">bioRxiv</journal-id><journal-title-group><journal-title>bioRxiv : the preprint server for biology</journal-title></journal-title-group><issn pub-type="ppub"/></journal-meta><article-meta><article-id pub-id-type="manuscript">EMS189008</article-id><article-id pub-id-type="doi">10.1101/2023.10.02.560466</article-id><article-id pub-id-type="archive">PPR734812</article-id><article-version-alternatives><article-version article-version-type="status">preprint</article-version><article-version article-version-type="number">2</article-version></article-version-alternatives><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A simple optical flow model explains why certain object viewpoints are special</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Stewart</surname><given-names>Emma E.M.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A3">3</xref><xref ref-type="corresp" rid="CR1">*</xref></contrib><contrib contrib-type="author"><name><surname>Fleming</surname><given-names>Roland W.</given-names></name><xref ref-type="aff" rid="A1">1</xref><xref ref-type="aff" rid="A2">2</xref><xref ref-type="fn" rid="FN1">**</xref></contrib><contrib contrib-type="author"><name><surname>Schütz</surname><given-names>Alexander C.</given-names></name><xref ref-type="aff" rid="A2">2</xref><xref ref-type="aff" rid="A4">4</xref><xref ref-type="fn" rid="FN1">**</xref></contrib></contrib-group><aff id="A1"><label>1</label>Department of Experimental Psychology, Justus Liebig University Giessen. Germany. 35394</aff><aff id="A2"><label>2</label>Centre for Mind, Brain, and Behaviour (CMBB), University of Marburg and Justus Liebig University Giessen. Germany. 35032</aff><aff id="A3"><label>3</label>School of Biological and Behavioural Sciences, Queen Mary University London. UK. E14NS</aff><aff id="A4"><label>4</label>General and Experimental Psychology, University of Marburg. Germany. 35032</aff><author-notes><corresp id="CR1">
<label>*</label>Corresponding author: Emma E.M. Stewart School of Biological and Behavioural Sciences Queen Mary University of London G.E. Fogg Building, Room 2.23 Mile End Road, London E1 4NS United Kingdom <email>emma.e.m.stewart@gmail.com</email></corresp><fn id="FN1"><label>**</label><p id="P1">Shared senior authorship.</p></fn></author-notes><pub-date pub-type="nihms-submitted"><day>04</day><month>10</month><year>2023</year></pub-date><pub-date pub-type="preprint"><day>02</day><month>10</month><year>2023</year></pub-date><permissions><ali:free_to_read/><license><ali:license_ref>https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p>This work is licensed under a <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">CC BY-NC-ND 4.0 International license</ext-link>.</license-p></license></permissions><abstract><p id="P2">A core challenge in perception is recognizing objects across the highly variable retinal input that occurs when objects are viewed from different directions (e.g., front <italic>vs</italic> side views). It has long been known that certain views are of particular importance, but it remains unclear why. We reasoned that characterising the computations underlying visual comparisons between objects could explain the privileged status of certain qualitatively special views. We measured pose discrimination for a wide range of objects, finding large variations in performance depending on the object and the view angle, with front and back views yielding particularly good discrimination. Strikingly, a simple and biologically plausible computational model based on measuring the projected 3D optical flow between views of objects accurately predicted both successes and failures of discrimination performance. This provides a computational account of why certain views have a privileged status.</p></abstract><kwd-group><title>Classification</title><kwd>Social Sciences</kwd><kwd>Psychological and Cognitive Sciences</kwd></kwd-group><kwd-group><kwd>viewpoint perception</kwd><kwd>perspectival appearance</kwd><kwd>3D object perception</kwd></kwd-group></article-meta></front><body><sec id="S1" sec-type="intro"><title>Introduction</title><p id="P3">When asked to imagine a familiar object, most people find themselves picturing the object from particular viewpoints that are especially informative or qualitatively distinct from other views (<xref ref-type="bibr" rid="R27">Palmer et al., 1981</xref>). Yet, despite decades of research on visual object perception, fundamental questions remain about why certain views of objects are special. There is a confusing array of terms and ideas related to different kinds of views, including ‘canonical’ (<xref ref-type="bibr" rid="R4">Blanz et al., 1996</xref>; <xref ref-type="bibr" rid="R8">Center et al., 2022</xref>; <xref ref-type="bibr" rid="R10">Cutzu &amp; Edelman, 1994</xref>; <xref ref-type="bibr" rid="R15">Gomez et al., 2008</xref>; <xref ref-type="bibr" rid="R41">Woods et al., 2008</xref>), ‘accidental’ or ‘non-accidental’ (<xref ref-type="bibr" rid="R19">Koning &amp; Lier, 2006</xref>; <xref ref-type="bibr" rid="R24">Niimi &amp; Yokosawa, 2006</xref>; <xref ref-type="bibr" rid="R30">Poggio &amp; Vetter, 1994</xref>), ‘generic’ (<xref ref-type="bibr" rid="R13">Freeman, 1994</xref>; <xref ref-type="bibr" rid="R42">Yuille et al., 2000</xref>, <xref ref-type="bibr" rid="R43">2003</xref>), or ‘cardinal’ (or being aligned along a cardinal axis (<xref ref-type="bibr" rid="R1">Aldegheri et al., 2023</xref>; <xref ref-type="bibr" rid="R2">Appelle, 1972</xref>; <xref ref-type="bibr" rid="R3">Balas &amp; Valente, 2012</xref>; <xref ref-type="bibr" rid="R26">Oomes &amp; Dijkstra, 2002</xref>; <xref ref-type="bibr" rid="R31">Shiffrar &amp; Shepard, 1991</xref>)). Here, we sought a computational framework for understanding why some views are privileged in object perception. We show that a simple computational model, based on optical flow (<xref ref-type="bibr" rid="R33">Stewart, Hartmann, et al., 2022</xref>) can accurately predict the costs and benefits of viewing both familiar and novel objects from certain perspectives. The model provides a straightforward quantitative account of how the visual system determines which views of objects are particularly important, based on regularities in object geometry, and the 2D visual information that is projected onto the retina.</p><p id="P4">Viewpoints play an important role in object perception, and can influence how we perceive, remember, and recognise an object. Here we will consider three specific, geometrically and qualitatively distinct viewpoints: (1) canonical viewpoints (<xref ref-type="bibr" rid="R22">Marr &amp; Nishihara, 1978</xref>), which tend to be the oblique views where the most surface of the object is visible (<xref ref-type="bibr" rid="R4">Blanz et al., 1996</xref>; <xref ref-type="bibr" rid="R6">Bülthoff &amp; Edelman, 1992</xref>); (2) end-on cardinal viewpoints, when an object is viewed so that the viewpoint with the smallest width to length ratio is aligned along the viewing axis (for example viewing a pig front-on); and (3) conversely the flat sides of objects, where the largest width to length ratio is aligned along the viewing axis (like the side of a pig) (<xref ref-type="bibr" rid="R25">Niimi &amp; Yokosawa, 2008</xref>; <xref ref-type="bibr" rid="R37">Tarr &amp; Kriegman, 2001</xref>). Viewing an object from one of these special viewpoints can benefit object recognition and recall (<xref ref-type="bibr" rid="R8">Center et al., 2022</xref>; <xref ref-type="bibr" rid="R15">Gomez et al., 2008</xref>; <xref ref-type="bibr" rid="R27">Palmer et al., 1981</xref>; <xref ref-type="bibr" rid="R41">Woods et al., 2008</xref>), and can result in longer inspection times of the object (<xref ref-type="bibr" rid="R28">Perrett &amp; Harries, 1987</xref>). For example, the canonical viewpoints of an object might convey the most information about its overall identity and are the best views for object recognition (<xref ref-type="bibr" rid="R22">Marr &amp; Nishihara, 1978</xref>), and conversely the flat viewpoints are the most perceptually stable, or provide the least amount of visual change if the object were to rotate a little (<xref ref-type="bibr" rid="R25">Niimi &amp; Yokosawa, 2008</xref>; <xref ref-type="bibr" rid="R37">Tarr &amp; Kriegman, 2001</xref>). People are also better at performing viewpoint discrimination tasks from some viewpoints. Previous work has attempted to quantitively define the transition between qualitatively distinct views (<xref ref-type="bibr" rid="R37">Tarr &amp; Kriegman, 2001</xref>) as “visual events,” and found that when an object is rotated across one of these visual events, discrimination performance is higher. Such discrimination benefits were found to be particularly strong for the front and back of familiar objects, particularly when the objects were symmetrical, and/or had orientation-specific features such as linear contours (<xref ref-type="bibr" rid="R25">Niimi &amp; Yokosawa, 2008</xref>).</p><p id="P5">We reasoned that we could use these geometric regularities in object viewpoint discrimination to provide a quantitative prediction of qualitatively distinct viewpoints, derived from the extent to which points on the object shift in the image when viewpoint (or equivalently, object pose) changes. To do this, we used a simple model inspired by optical flow computations, which was recently shown to capture the non-uniformities and geometric regularities in object viewpoint perception (<xref ref-type="bibr" rid="R33">Stewart, Hartmann, et al., 2022</xref>). Specifically, the model assumes that given a pair of views of an object, the visual system: (1) identifies corresponding points on the objects’ surface across the two poses, (2) estimates the vectors in 3D between these corresponding points; (3) projects the vectors into the image plane from the current perspective and (4) takes the average length of these vectors as a measure of the difference between the two poses. We find that this approach provides a quantitative account of the differences between views and thus a quantitative framework for understanding what makes certain object viewpoints qualitatively special.</p></sec><sec id="S2" sec-type="results"><title>Results</title><sec id="S3"><title>Experiment 1: viewpoint discrimination judgements</title><p id="P6">We reasoned that if the proposed optical flow model can be used to unify previous qualitative findings on cardinal viewpoints, it should first be able to predict discrimination benefits at cardinal (front and back) versus non-cardinal (oblique) viewpoints (<xref ref-type="bibr" rid="R25">Niimi &amp; Yokosawa, 2008</xref>; <xref ref-type="bibr" rid="R37">Tarr &amp; Kriegman, 2001</xref>). In two online experiments, we collected human discrimination judgements for cardinal and non-cardinal viewpoints for twenty-one photographs of real objects from the Amsterdam Library of Object Images (<xref ref-type="bibr" rid="R14">Geusebroek et al., 2005</xref>), and thirteen rendered mesh objects (see <bold>Materials and Methods</bold>). In an initial object priming block, participants were shown a video of each of the objects rotating for four seconds, and were asked to report the direction of rotation. This task aimed to prime participants to think about the 3D rotational nature of the objects in the subsequent task. Results from this block were only used for participant exclusion (one participant for this criterion), and were not analysed further. In the subsequent object discrimination block, two viewpoints of the same object were displayed either side of a central cross for 500ms: the base viewpoint was either a cardinal or non-cardinal viewpoint, and the rotated viewpoint was offset from the base view by seven possible rotation levels (0, ±5, ±10, ±15 degrees rotation around the vertical axis). Participants indicated whether the two viewpoints were the same or different by clicking an onscreen button (<xref ref-type="fig" rid="F1">Figure 1A</xref>).</p><p id="P7">To analyse the responses, for each object and rotation level, we calculated the proportion of responses where participants indicated the two views were the same (<xref ref-type="fig" rid="F1">Figure 1B</xref>). Results showed a striking difference in performance between cardinal versus non-cardinal axes for both the photographs of real objects, and the rendered mesh objects. A generalized least squares (GLS) regression model (performance ~ rotation_level (0, 5, 10, 15) * axis_type (cardinal, non-cardinal) * image_set (real, rendered)) demonstrated a significant effect of rotation level: F(1,536) = 734.76, p&lt;0.0001; axis type: F(1,536) = 127.88, p&lt;0.0001; and the interaction between rotation level and axis type: F(1,536) = 28.92, p&lt;0.0001; but not image set: F(1,536) = 0.24, p = 0.63; and no other interactions were significant. This demonstrates that humans are better at discriminating objects rotated around cardinal versus noncardinal axes for both real and rendered objects. These cardinal axes would therefore seem to be analogous to the “visual events” postulated by Tarr and Kriegman (<xref ref-type="bibr" rid="R37">Tarr &amp; Kriegman, 2001</xref>).</p><p id="P8">The results also showed that there was some variability in performance between objects. To quantify the discrimination benefit for cardinal vs non-cardinal axes for a particular object, we calculated the “cardinal axis effect” as the difference in the slope between “response different” judgements for 0- and 5-degree rotation levels for cardinal vs non-cardinal axes, as this was the difference at which the most variability was observed between objects. The higher the slope for an object/axis, the more discriminable it is. <xref ref-type="fig" rid="F2">Figure 2C</xref> shows that the cardinality effect was stronger for some objects than others, for both real and mesh objects, with 32/34 objects showing a perceptual discrimination benefit for cardinal versus non-cardinal viewpoints. This variability in the cardinal axis effect allowed us to use a model to investigate to what extent we could predict the cardinal axis effect for each object, and whether features of our model predictions might predict the magnitude of this effect.</p><sec id="S4"><title>Optical flow model predicts cardinal viewpoints</title><p id="P9">We used an optical flow model to compute viewpoint dissimilarity for every viewpoint around each object. In brief, the model measures how much points on objects shift in the image as the viewpoint changes. Specifically, given a pair of poses of an object, we compute the 3D vectors between corresponding surface points, and then estimate the mean length of these vectors when projected into the 2D image plane. This model has been shown to capture viewpoint-related variations in mental rotation (<xref ref-type="bibr" rid="R33">Stewart et al 2022</xref>), and we reasoned that it may be able to predict both cardinal and non-cardinal axes within objects, as well as the relative degree of the cardinal axis effect across objects. For each of the 72 rendered viewpoints, we calculated the ground-truth optical flow vectors produced as the object rotated towards the next viewpoint. The model prediction for this viewpoint was taken as the mean of the absolute length of these vectors (see Stewart et al for further details). We thus obtained an “optical flow curve” for each object (<xref ref-type="fig" rid="F2">Figure 2</xref>). We calculated the gradient of this flow curve at the tested viewpoints, and the slope of the curve between the tested base viewpoint (front, back, non-cardinal) and each offset viewpoint. Different objects had different optical flow curve profiles, and in particular varied in the range of the curve. We therefore also calculated the range (max-min) of the curve for each object.</p><p id="P10">The gradient of the optical flow curve predicted cardinal versus non-cardinal axes, with cardinal axes having a lower gradient than non-cardinal axes (t(12) = -4.58, p=0.0006, Cohen’s D = 1.27 (large effect size), <xref ref-type="fig" rid="F2">Figure 2AB</xref>). The optical flow model could also predict human discrimination performance in two ways. We first looked at whether the cardinal axis effect could be predicted by the gradient of the optical flow curve (<xref ref-type="fig" rid="F2">Figure 2C</xref>). A simple linear regression model revealed that the model gradient accurately predicted the human discrimination performance (F(1,50) = 14.09, p = 0.00045). Second, we tested whether the cardinal axis effect could be predicted by the difference in gradient (<xref ref-type="fig" rid="F2">Figure 2D</xref>), and by the magnitude of optical flow change across the entire optical flow curve (range of the curve). A linear regression model (cardinal_axis_effect ~ gradient_difference + curve_range) again showed a significant effect of gradient difference (F(1,49) = 9.62, p = 0.0032) and curve range (F(1,49) = 56.2, p&lt;0.0001). These results indicate that the gradient of the optical flow curve is predictive of whether a viewpoint is cardinal or not, providing for the first time a straightforward quantitative predictor for these qualitatively special views of familiar objects.</p></sec></sec><sec id="S5"><title>Experiment 2: the front of novel objects</title><p id="P11">The objects tested in Experiment 1 were all easily recognizable objects, and factors such as familiarity and the geometric properties of the shapes themselves (e.g., symmetry, elongation), might have influenced performance. Thus, while model predictions correlated with the cardinal axis effect for symmetrical, elongated, real objects, the findings may not generalise to novel objects with less regular elongation and symmetry (<xref ref-type="bibr" rid="R30">Poggio &amp; Vetter, 1994</xref>). We therefore created ten non-meaningful mesh objects with both regular and irregular optical flow curves, and in an online experiment verified that these objects were on average rated as non-familiar (see <bold>Methods and Materials</bold> for details of object-familiarity ratings). Objects were created to have varying levels of symmetry and elongation, and to have a more heterogenous pattern of optical flow predictions than the familiar objects in Experiment 1. We then conducted a separate online experiment, and asked a new sample of fifty online participants to rotate each of the ten novel objects, plus four of the familiar mesh objects from the previous experiment (pig, duck, small car, figure), so that the front of the object was facing toward the participant. For each object, we examined where the responded “front” angles lay on the optical flow prediction curve. In general, responses tended to cluster around the peaks and troughs of the optical flow curve (<xref ref-type="fig" rid="F3">Figure 3</xref>), and on average the viewpoints that were indicated as being a “front” had lower gradients than the viewpoints that were never indicated as being the front (Wilcoxon test <italic>z</italic> = 2, p = 0.00037, <italic>r</italic> = 0.54; strong effect size). This demonstrates that, while there is naturally more variability in where the actual front of the object is considered to be, even for novel, non-symmetrical objects, the optical flow model was predictive of which viewpoints may be considered to be candidate “front” views of these objects.</p></sec></sec><sec id="S6"><title>General discussion</title><p id="P12">Our results suggest that the optical flow model can explain variability in viewpoint discrimination and identify qualitatively distinct viewpoints. Participants were better at discriminating between two viewpoints separated by 5 degrees rotation, when one of those viewpoints was a so-called cardinal axis, compared to when it was an oblique view of the object. The magnitude and variability in this perceptual discrimination advantage could be predicted by the gradient of an optical-flow model that computes the magnitude of the 2D displacement vectors that would be produced if the object were to rotate from one viewpoint to the next. Remarkably, this model could also predict viewpoints that were more likely to be labelled as “front” for novel, unfamiliar objects. This model can therefore capture quantitative geometrical relationships between viewpoints and predict which viewpoints stand out as particularly significant for the observer. Our findings suggest the method works for familiar, unfamiliar, regular, and irregular objects. As <xref ref-type="fig" rid="F4">Figure 4</xref> shows, viewpoints that may be considered the most discriminable (front, back), most stable (sides), and “typical”, “generic” or “representative” (oblique; see <xref ref-type="supplementary-material" rid="SD1">Supplementary Materials</xref>) can be constrained using the model output curve (optical flow value/predicted perceived dissimilarity), and the gradient of this curve. Thus, a simple, quantitative account based on the projected spatial shifts of visible surface points provides a computational framework for object viewpoint perception.</p><p id="P13">It is likely that geometrical regularities across natural objects contribute to a form of statistical learning about object geometries. For example, in most quadrupedal animals, the front and back of the animal is narrower and more symmetrical than the side-on view. While such statistical learning about object categories and identity may account for familiarity effects in object recognition (<xref ref-type="bibr" rid="R4">Blanz et al., 1996</xref>), learning about the geometry of objects may also aid in identifying important viewpoints for unfamiliar objects. The results of Experiment 2 suggest that participants may extrapolate learned geometrical regularities of cardinal viewpoints experienced in the real world, and use these statistical regularities to determine the cardinal viewpoints of novel objects. As with many other object features (<xref ref-type="bibr" rid="R17">Kanade, 1981</xref>; <xref ref-type="bibr" rid="R20">Langer &amp; Bülthoff, 2000</xref>; <xref ref-type="bibr" rid="R21">Mamassian &amp; Landy, 2001</xref>; <xref ref-type="bibr" rid="R32">Sprote &amp; Fleming, 2013</xref>; <xref ref-type="bibr" rid="R40">Wilder et al., 2011</xref>), the visual system seems to use knowledge of objects in the world to form priors about (<xref ref-type="bibr" rid="R18">Kersten et al., 2004</xref>), or estimate latent variables underlying (<xref ref-type="bibr" rid="R12">Fleming &amp; Storrs, 2019</xref>), the proximal information about the geometry of distinctive object viewpoints.</p><p id="P14">These results also allow us to reflect on the nature of object viewpoint representations in relation to their true distal form as opposed to our 2D proximal experience of them. We recently suggested, based on the model also used in the current study, that the computations underlying the 3D mental rotation of objects rely on a ‘mental rendering’ of the imagined object, as if predicting its 2D proximal appearance (<xref ref-type="bibr" rid="R33">Stewart, Hartmann, et al., 2022</xref>). We suggest here that the comparison and categorisation of 3D object viewpoints might then rely on similar 3D to 2D computations. <sup>10</sup>Interestingly, in this experiment, even though participants were primed on the 3D nature of the stimulus by being shown the object rotating in space, the model based on distances in 2D is still predictive of human performance, and reflects previous findings that a 2D representation may underpin 3D viewpoint discrimination (<xref ref-type="bibr" rid="R33">Stewart, Hartmann, et al., 2022</xref>). In Experiment 2, the selection of the “front” viewpoint compared to all other viewpoints arguably requires a 3D representation of the object as a whole, yet even in this case responses corresponded to geometric regularities predicted by the 2D model. As a result, the model also provides a route into understanding the origin of putative effects of the ‘perspectival’ appearance of objects (<xref ref-type="bibr" rid="R23">Morales et al., 2020</xref>; <xref ref-type="bibr" rid="R35">Storrs &amp; Arnold, 2013</xref>; <xref ref-type="bibr" rid="R38">Thouless, 1931</xref>)—such as the perceived ‘elliptical nature’ of a coin seen slanted in depth—in the context of theories of vision that assume perceptual constancies (<xref ref-type="bibr" rid="R7">Burge, 2010</xref>; <xref ref-type="bibr" rid="R9">Cohen, 2014</xref>; <xref ref-type="bibr" rid="R16">Green, 2023</xref>; <xref ref-type="bibr" rid="R39">Walsh &amp; Kulikowski, 1998</xref>). Specifically, we suggest that even when 3D object structure is estimated perfectly, comparisons between objects are made in terms of the estimated or predicted changes in the proximal stimulus involved in transforming one view to another. We speculate that representations that express changes in terms of proximal stimulus quantities are particularly useful in learning to see a 3D object in the absence of ground-truth data about its physical state. Specifically, we suggest that learning to accurately predict changes in the proximal stimulus teaches the visual system deep knowledge about the distal forms that produce such changes (<xref ref-type="bibr" rid="R12">Fleming &amp; Storrs, 2019</xref>; <xref ref-type="bibr" rid="R36">Storrs &amp; Fleming, 2021</xref>). Thus paradoxically, retaining a representation of ‘perspectival aspects’ of objects may be a key step in learning to see them as 3D in the first place. Our findings suggest that humans use proximal information about an object’s geometry to make judgements about an object’s pose, and more importantly, use this information to learn regularities about qualitatively special viewpoints.</p></sec><sec id="S7" sec-type="materials | methods"><title>Materials and methods</title><sec id="S8" sec-type="subjects"><title>Participants</title><p id="P15">300 participants completed the study in total. Experiment 1 real objects: 50 participants (24 female; age range 19-59, mean age 29.9 (SD 1.3)), Experiment 1 rendered mesh objects (N = 50, female = 27, undisclosed sex = 1, mean age = 31.4 (SD 10.4)). Experiment 2 familiarity ratings 1 (N = 50, 18 female, mean age = 27.8 (SD 5.9)); familiarity ratings 2 (N = 50, 16 female, mean age = 27.9 (SD 8.5)). Experiment 2 front judgements: 50 participants (N = 50, 19 female, mean age = 30 (SD 8.7)). Experiment 2 representative judgements: 50 participants (N = 50, 28 female, mean age = 41.9 (SD 12.7)). Participants participated online, and were recruited through Prolific. Experiments were approved by the University of Marburg local ethics committee (approval number 2015-35k), and University of Giessen local ethics committee (approval number 2020-0033), and were conducted in accordance of the Declaration of Helsinki (1964).</p></sec><sec id="S9"><title>Stimuli</title><sec id="S10"><title>Experiment 1 – photographed real objects</title><p id="P16">Photographed real objects were taken from the Amsterdam Library of Object Images (ALOI) (<xref ref-type="bibr" rid="R14">Geusebroek et al., 2005</xref>). This library contains 1000 photographs of real-world images, photographed from 72 viewpoints separated by 5 degrees of horizontal rotation. In a previous study, we collected human judgements about which viewpoints of these objects corresponded to a number of axis labels (front, back, left, right, prototypical; see (<xref ref-type="bibr" rid="R33">Stewart, Ludwig, et al., 2022</xref>) for details on data collection). This data gave us a distribution of angular responses for each axis label for each object. For this study, we chose 21 objects that had clearly defined cardinal viewpoints: for each object and viewpoint label, we calculated the mean resultant length of participant responses, and used this to select objects where there was high agreement between participants for all cardinal viewpoints. The cardinal viewpoint for a specific label was then taken as the circular mode of the responses for that label. Non-cardinal viewpoints were taken as those that were midway between two cardinal viewpoints (i.e., front and left), but were not reported to be the prototypical or any other viewpoint.</p></sec><sec id="S11"><title>Experiment 1 – rendered mesh objects</title><p id="P17">We repeated the experiment with mesh objects so that we could apply the mesh-based optical flow model. We selected 13 mesh objects that were as similar as possible to the original ALOI objects in both shape and semantic meaning. Meshes were either freely available online or selected from Evermotion (<ext-link ext-link-type="uri" xlink:href="https://evermotion.org">https://evermotion.org</ext-link>). These meshes were rendered in Blender (<xref ref-type="bibr" rid="R5">BlenderFoundation, 2018</xref>) from 72 viewing angles separated by 5 degrees horizontal rotation, using analogous lighting and camera distance as the ALOI objects (<xref ref-type="bibr" rid="R14">Geusebroek et al., 2005</xref>). For these objects, the front was defined by the researchers (and was analogous to the front of the photographed objects), and the same angular rotations relative to the front as in the photographed objects were used to define non-cardinal viewpoints. Objects were rendered in pretty colours chosen by the researcher.</p></sec><sec id="S12"><title>Experiment 2 – unfamiliar objects</title><p id="P18">We created thirty semantically non-meaningful mesh objects using Mathematica and Blender (<xref ref-type="bibr" rid="R5">BlenderFoundation, 2018</xref>), and chose the ten most unfamiliar (see procedure for details) These objects were rendered from the same viewing angles, with the same lighting conditions as in Experiment 1. The four familiar objects from Experiment 1 were re-rendered in the same colour as the unfamiliar objects.</p></sec></sec><sec id="S13" sec-type="methods"><title>Procedure</title><sec id="S14"><title>Experiment 1</title><p id="P19">Experiment 1 (photographed real objects) was pre-registered at as.predicted.org (#67124), and the experiment using the rendered mesh objects followed the same protocol. The experiments were conducted online and programmed with custom-written software using JsPsych (<xref ref-type="bibr" rid="R11">deLeeuw, 2015</xref>). Each participant completed both an object priming and a perceptual discrimination task. The object priming task aimed to familiarise participants with the 3D nature of the objects, and to prime them to think about the objects rotating through viewpoints. Participants viewed a video of each object rotating through 360 degrees for a duration of four seconds, either clockwise or counter-clockwise. Participants had to indicate the direction of rotation via button-press. Participant responses to the priming task were only used for excluding participants and were not analysed further. In the perceptual discrimination task, participants were presented with two views of an object on either side of a central fixation cross, for 500ms. They then indicated via button press whether the views were the same or different. These views were defined by four conditions, tested in two separate experiments. Experiments 1 and 2 (part 1): front ± 0, 5, 10, 15 degrees; non-cardinal angle 1 ± 0, 5, 10, 15 degrees. Experiments 1 and 2 (part 2): back ± 0, 5, 10, 15 degrees; non-cardinal angle 2 ± 0, 5, 10, 15 degrees. Every participant saw all objects at both cardinal and non-cardinal viewpoints, with randomized viewpoint offset difference levels.</p></sec><sec id="S15"><title>Experiment 2</title><sec id="S16"><title>Familiarity ratings</title><p id="P20">To choose Participants saw a video of each object rotating, and indicated how familiar they found the object to be, using a 6-point slider ranging from “very unfamiliar” to “very familiar”. The video looped to give the appearance of a continuously rotating object until a response was given. Four of the objects from experiment 2 were included as catch and comparison trials (pig, car, figure, duck). Mean familiarity ratings were calculated, and objects were rank ordered by familiarity. We chose 10 objects for the cardinal axis rating task, accounting for low familiarity ratings, varying levels of symmetry, and varying optical flow curve profiles. To confirm that these chosen novel objects were perceived as being unfamiliar, we ran a second online experiment with the same procedure, where participants only saw these ten unfamiliar objects and the four familiar objects (see <xref ref-type="supplementary-material" rid="SD1">Supplementary Materials</xref>).</p></sec><sec id="S17"><title>Cardinal axis ratings</title><p id="P21">In an online experiment, participants were instructed to rotate each object using the left and right arrows on the keyboard until the object was facing toward the “front”.</p></sec></sec><sec id="S18"><title>Exclusions</title><p id="P22">In Experiment 1, trials were excluded if the reaction time was less than 300ms or more than 5000ms or if performance was less than 75% correct on either the training or main task. In Experiment 2, for the familiarity ratings, trials were excluded if the reaction time was less than 300ms or more than 5000ms, or if they rated any of the real objects (pig, figure, car) as having a familiarity rating of less than three out of six. For the front ratings, trials were excluded based on the same reaction time criteria, and additionally if the “front” response of familiar objects was not within 45 degrees of the veridical front. 1.2% of trials were excluded for Experiment 1 (photographed real objects) and 1.19% for the rendered mesh objects. For Experiment 2 (familiarity ratings) no trials were excluded; for Experiment 2 (front ratings), 3 participants were excluded, and no trials were excluded from the remaining participants.</p></sec><sec id="S19"><title>Analyses</title><sec id="S20"><title>Model</title><p id="P23">We used a ground-truth optical flow computation (<xref ref-type="bibr" rid="R33">Stewart et al, 2022</xref>), which has previously been found to predict human performance for viewpoint dissimilarity judgements for block-sequence 3D rendered objects. The model predicted the amount of 2D optical flow information that would be produced as the object rotated by 5 degrees from one viewpoint to the next, by calculating the mean of the absolute horizontal and vertical displacements of every visible vertex in the underlying object mesh. Displacements for vertices visible from one viewpoint but not the rotated viewpoint (unmatched points) are not included in model calculations.</p></sec><sec id="S21"><title>Cardinal axis effect</title><p id="P24">To calculate the cardinal axis effect, we only took the slope of performance difference between 0- and 5-degree displacements for the following reasons: 1) this is the most difficult discrimination level we tested; 2) larger discrimination levels (especially 15-degree become trivial); 3) there’s not necessarily a linear increase in performance between 0,5-,10-, and 15-degree displacements. For some objects there is a very small performance difference between 0- and 5-degree displacements; and for others there is a larger difference in performance. Such between-object differences in sensitivity to small displacements are lost if larger displacements are included in the cardinal axis effect calculations.</p></sec><sec id="S22"><title>Statistical analyses</title><p id="P25">All statistical analyses were conducted in R. Linear regression models were conducted using base R. Generalized least squares (GLS) models were conducted using the package nlme (<xref ref-type="bibr" rid="R29">Pinheiro et al., 2020</xref>), and were implemented where a linear model would otherwise have a heterogonous variance across different levels of a factor. Model comparisons were used to determine the best-fitting variance structure (different variance allowed for different factor levels).</p></sec></sec></sec></sec><sec sec-type="supplementary-material" id="SM"><title>Supplementary Material</title><supplementary-material content-type="local-data" id="SD1"><label>Supplementary materials</label><media xlink:href="EMS189008-supplement-Supplementary_materials.pdf" mimetype="application" mime-subtype="pdf" id="d34aAdEbB" position="anchor"/></supplementary-material></sec></body><back><ack id="S23"><title>Acknowledgements</title><p>This project was supported by the Deutsche Forschungsgemeinschaft, through project numbers 460533638 (EEMS) and 222641018–SFB/TRR-135 TP C1 (RWF) and TP B2 (AS), by the Research Cluster “The Adaptive Mind”, funded by the Hessian Ministry for Higher Education, Research, Science and the Arts and by the European Research Council through projects ERC-2022-AdG “STUFF” (project number 101098225 to RWF) and ERC-2020-CoG “SENCES” (project number 101001250 to AS). Data and stimuli will be made available upon publication at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.8398391">http://doi.org/10.5281/zenodo.8398391</ext-link>.</p></ack><fn-group><fn id="FN2" fn-type="con"><p id="P26"><bold>Author Contributions:</bold></p><p id="P27">Conceptualization and methodology, E.E.M.S., R.W.F., and A.C.S; Formal Analysis, E.E.M.S.; Investigation, E.E.M.S.; Writing – Original Draft, E.E.M.S.; Writing – Review &amp; Editing, E.E.M.S., R.W.F., and A.C.S.; Visualization: E.E.M.S.; Funding Acquisition, E.E.M.S., R.W.F., and A.C.S.</p></fn><fn id="FN3" fn-type="conflict"><p id="P28"><bold>Competing Interest Statement:</bold></p><p id="P29">There are no competing interests.</p></fn></fn-group><ref-list><ref id="R1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aldegheri</surname><given-names>G</given-names></name><name><surname>Gayet</surname><given-names>S</given-names></name><name><surname>Peelen</surname><given-names>MV</given-names></name></person-group><article-title>Scene context automatically drives predictions of object transformations</article-title><source>Cognition</source><year>2023</year><volume>238</volume><elocation-id>105521</elocation-id><pub-id pub-id-type="pmid">37354785</pub-id></element-citation></ref><ref id="R2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appelle</surname><given-names>S</given-names></name></person-group><article-title>Perception and discrimination as a function of stimulus orientation: The “oblique effect” in man and animals</article-title><source>Psychological Bulletin</source><year>1972</year><volume>78</volume><issue>4</issue><fpage>266</fpage><lpage>278</lpage><pub-id pub-id-type="pmid">4562947</pub-id></element-citation></ref><ref id="R3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balas</surname><given-names>B</given-names></name><name><surname>Valente</surname><given-names>N</given-names></name></person-group><article-title>View-adaptation reveals coding of face pose along image, not object, axes</article-title><source>Vision Research</source><year>2012</year><volume>67</volume><fpage>22</fpage><lpage>27</lpage><pub-id pub-id-type="pmcid">PMC3444152</pub-id><pub-id pub-id-type="pmid">22796427</pub-id><pub-id pub-id-type="doi">10.1016/j.visres.2012.07.002</pub-id></element-citation></ref><ref id="R4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanz</surname><given-names>V</given-names></name><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><article-title>What Object Attributes Determine Canonical Views?</article-title><source>Perception</source><year>1996</year><volume>28</volume><issue>5</issue><fpage>575</fpage><lpage>599</lpage><pub-id pub-id-type="pmid">10664755</pub-id></element-citation></ref><ref id="R5"><element-citation publication-type="web"><collab>Blender Foundation</collab><source>Blender-a 3D modelling and rendering package</source><year>2018</year><comment><ext-link ext-link-type="uri" xlink:href="http://www.blender.org">http://www.blender.org</ext-link></comment></element-citation></ref><ref id="R6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bülthoff</surname><given-names>HH</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><article-title>Psychophysical support for a two-dimensional view interpolation theory of object recognition</article-title><source>Proceedings of the National Academy of Sciences</source><year>1992</year><volume>89</volume><issue>1</issue><fpage>60</fpage><lpage>64</lpage><pub-id pub-id-type="pmcid">PMC48175</pub-id><pub-id pub-id-type="pmid">1729718</pub-id><pub-id pub-id-type="doi">10.1073/pnas.89.1.60</pub-id></element-citation></ref><ref id="R7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>T</given-names></name></person-group><source>Origins of Objectivity</source><year>2010</year><fpage>137</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199581405.003.0005</pub-id></element-citation></ref><ref id="R8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Center</surname><given-names>EG</given-names></name><name><surname>Gephart</surname><given-names>AM</given-names></name><name><surname>Yang</surname><given-names>P-L</given-names></name><name><surname>Beck</surname><given-names>DM</given-names></name></person-group><article-title>Typical viewpoints of objects are better detected than atypical ones</article-title><source>Journal of Vision</source><year>2022</year><volume>22</volume><issue>12</issue><fpage>1</fpage><pub-id pub-id-type="pmcid">PMC9639674</pub-id><pub-id pub-id-type="pmid">36318192</pub-id><pub-id pub-id-type="doi">10.1167/jov.22.12.1</pub-id></element-citation></ref><ref id="R9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>J</given-names></name></person-group><chapter-title>Perceptual Constancy</chapter-title><person-group person-group-type="editor"><name><surname>Matthen</surname><given-names>M</given-names></name></person-group><source>The Oxford Handbook of Philosophy of Perception</source><year>2014</year><fpage>621</fpage><lpage>639</lpage><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199600472.001.0001</pub-id></element-citation></ref><ref id="R10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cutzu</surname><given-names>F</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name></person-group><article-title>Canonical views in object representation and recognition</article-title><source>Vision Research</source><year>1994</year><volume>34</volume><issue>22</issue><fpage>3037</fpage><lpage>3056</lpage><pub-id pub-id-type="pmid">7975339</pub-id></element-citation></ref><ref id="R11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deLeeuw</surname><given-names>JR</given-names></name></person-group><article-title>jsPsych: A JavaScript library for creating behavioral experiments in a Web browser</article-title><source>Behavior Research Methods</source><year>2015</year><volume>47</volume><issue>1</issue><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="pmid">24683129</pub-id></element-citation></ref><ref id="R12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>RW</given-names></name><name><surname>Storrs</surname><given-names>KR</given-names></name></person-group><article-title>Learning to see stuff</article-title><source>Current Opinion in Behavioral Sciences</source><year>2019</year><volume>30</volume><fpage>100</fpage><lpage>108</lpage><pub-id pub-id-type="pmcid">PMC6919301</pub-id><pub-id pub-id-type="pmid">31886321</pub-id><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.07.004</pub-id></element-citation></ref><ref id="R13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>WT</given-names></name></person-group><article-title>The generic viewpoint assumption in a framework for visual perception</article-title><source>Nature</source><year>1994</year><volume>368</volume><issue>6471</issue><fpage>542</fpage><lpage>545</lpage><pub-id pub-id-type="pmid">8139687</pub-id></element-citation></ref><ref id="R14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geusebroek</surname><given-names>J-M</given-names></name><name><surname>Burghouts</surname><given-names>GJ</given-names></name><name><surname>Smeulders</surname><given-names>AWM</given-names></name></person-group><article-title>The Amsterdam Library of Object Images</article-title><source>International Journal of Computer Vision</source><year>2005</year><volume>61</volume><issue>1</issue><fpage>103</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1023/b:visi.0000042993.50813.60</pub-id></element-citation></ref><ref id="R15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname><given-names>P</given-names></name><name><surname>Shutter</surname><given-names>J</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><article-title>Memory for objects in canonical and noncanonical viewpoints</article-title><source>Psychonomic Bulletin Review</source><year>2008</year><volume>15</volume><issue>5</issue><fpage>940</fpage><lpage>944</lpage><pub-id pub-id-type="pmid">18926985</pub-id></element-citation></ref><ref id="R16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>EJ</given-names></name></person-group><article-title>Perceptual constancy and perceptual representation</article-title><source>Analytic Philosophy</source><year>2023</year><pub-id pub-id-type="doi">10.1111/phib.12293</pub-id></element-citation></ref><ref id="R17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><article-title>Recovery of the three-dimensional shape of an object from a single view</article-title><source>Artificial Intelligence</source><year>1981</year><volume>17</volume><issue>1-3</issue><fpage>409</fpage><lpage>460</lpage><pub-id pub-id-type="doi">10.1016/0004-3702(81)90031-x</pub-id></element-citation></ref><ref id="R18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kersten</surname><given-names>D</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name><name><surname>Yuille</surname><given-names>A</given-names></name></person-group><article-title>Object Perception as Bayesian Inference</article-title><source>Annual Review of Psychology</source><year>2004</year><volume>55</volume><issue>1</issue><fpage>271</fpage><lpage>304</lpage><pub-id pub-id-type="pmid">14744217</pub-id></element-citation></ref><ref id="R19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koning</surname><given-names>A</given-names></name><name><surname>van Lier</surname><given-names>R</given-names></name></person-group><article-title>No symmetry advantage when object matching involves accidental viewpoints</article-title><source>Psychological Research</source><year>2006</year><volume>70</volume><issue>1</issue><fpage>52</fpage><lpage>58</lpage><pub-id pub-id-type="pmid">15480756</pub-id></element-citation></ref><ref id="R20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langer</surname><given-names>MS</given-names></name><name><surname>Bülthoff</surname><given-names>HH</given-names></name></person-group><article-title>A Prior for Global Convexity in Local Shape-from-Shading</article-title><source>Perception</source><year>2000</year><volume>30</volume><issue>4</issue><fpage>403</fpage><lpage>410</lpage><pub-id pub-id-type="pmid">11383189</pub-id></element-citation></ref><ref id="R21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mamassian</surname><given-names>P</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name></person-group><article-title>Interaction of visual prior constraints</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>20</issue><fpage>2653</fpage><lpage>2668</lpage><pub-id pub-id-type="pmid">11520511</pub-id></element-citation></ref><ref id="R22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Nishihara</surname><given-names>HK</given-names></name></person-group><article-title>Representation and recognition of the spatial organization of three-dimensional shapes</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><year>1978</year><volume>200</volume><issue>1140</issue><fpage>269</fpage><lpage>294</lpage><pub-id pub-id-type="pmid">24223</pub-id></element-citation></ref><ref id="R23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morales</surname><given-names>J</given-names></name><name><surname>Bax</surname><given-names>A</given-names></name><name><surname>Firestone</surname><given-names>C</given-names></name></person-group><article-title>Sustained representation of perspectival shape</article-title><source>Proceedings of the National Academy of Sciences</source><year>2020</year><elocation-id>202000715</elocation-id><pub-id pub-id-type="pmcid">PMC7334526</pub-id><pub-id pub-id-type="pmid">32532920</pub-id><pub-id pub-id-type="doi">10.1073/pnas.2000715117</pub-id></element-citation></ref><ref id="R24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niimi</surname><given-names>R</given-names></name><name><surname>Yokosawa</surname><given-names>K</given-names></name></person-group><article-title>Viewpoint Dependence in the Recognition of Non-Elongated Familiar Objects: Testing the Effects of Symmetry, Front-Back Axis, and Familiarity</article-title><source>Perception</source><year>2006</year><volume>38</volume><issue>4</issue><fpage>533</fpage><lpage>551</lpage><pub-id pub-id-type="pmid">19522322</pub-id></element-citation></ref><ref id="R25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niimi</surname><given-names>R</given-names></name><name><surname>Yokosawa</surname><given-names>K</given-names></name></person-group><article-title>Determining the orientation of depth-rotated familiar objects</article-title><source>Psychonomic Bulletin Review</source><year>2008</year><volume>15</volume><issue>1</issue><fpage>208</fpage><lpage>214</lpage><pub-id pub-id-type="pmid">18605505</pub-id></element-citation></ref><ref id="R26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oomes</surname><given-names>AHJ</given-names></name><name><surname>Dijkstra</surname><given-names>TMH</given-names></name></person-group><article-title>Object pose: Perceiving 3-D shape as sticks and slabs</article-title><source>Perception Psychophysics</source><year>2002</year><volume>64</volume><issue>4</issue><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="pmid">12132754</pub-id></element-citation></ref><ref id="R27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Palmer</surname><given-names>S</given-names></name><name><surname>Rosch</surname><given-names>E</given-names></name><name><surname>chase</surname><given-names>P</given-names></name></person-group><chapter-title>Canonical Perspective and the Perception of Objects</chapter-title><person-group person-group-type="editor"><name><surname>Long</surname><given-names>J</given-names></name><name><surname>Baddeley</surname><given-names>A</given-names></name></person-group><source>Attention and Performance IX</source><publisher-name>Lawrence Erlbaum Associates</publisher-name><year>1981</year><fpage>135</fpage><lpage>151</lpage></element-citation></ref><ref id="R28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Harries</surname><given-names>MH</given-names></name></person-group><article-title>Characteristic Views and the Visual Inspection of Simple Faceted and Smooth Objects: ‘Tetrahedra and Potatoes.’</article-title><source>Perception</source><year>1987</year><volume>17</volume><issue>6</issue><fpage>703</fpage><lpage>720</lpage><pub-id pub-id-type="pmid">3253674</pub-id></element-citation></ref><ref id="R29"><element-citation publication-type="web"><person-group person-group-type="author"><name><surname>Pinheiro</surname><given-names>J</given-names></name><name><surname>Bates</surname><given-names>D</given-names></name><name><surname>DebRoy</surname><given-names>S</given-names></name><name><surname>Sarkar</surname><given-names>D</given-names></name><name><surname>Team</surname><given-names>RC</given-names></name></person-group><source>nlme: Linear and Nonlinear Mixed Effects Models</source><year>2020</year><comment><ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=nlme">https://CRAN.R-project.org/package=nlme</ext-link></comment></element-citation></ref><ref id="R30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Vetter</surname><given-names>T</given-names></name></person-group><article-title>Symmetric 3D objects are an easy case for 2D object recognition</article-title><source>Spatial Vision</source><year>1994</year><volume>8</volume><issue>4</issue><fpage>443</fpage><lpage>453</lpage><pub-id pub-id-type="pmid">7772550</pub-id></element-citation></ref><ref id="R31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shiffrar</surname><given-names>MM</given-names></name><name><surname>Shepard</surname><given-names>RN</given-names></name></person-group><article-title>Comparison of Cube Rotations Around Axes Inclined Relative to the Environment or to the Cube</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>1991</year><volume>17</volume><issue>1</issue><fpage>44</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">1826321</pub-id></element-citation></ref><ref id="R32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sprote</surname><given-names>P</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Concavities, negative parts, and the perception that shapes are complete</article-title><source>Journal of Vision</source><year>2013</year><volume>13</volume><issue>14</issue><fpage>3</fpage><pub-id pub-id-type="pmid">24306852</pub-id></element-citation></ref><ref id="R33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>EEM</given-names></name><name><surname>Hartmann</surname><given-names>FT</given-names></name><name><surname>Morgenstern</surname><given-names>Y</given-names></name><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Maiello</surname><given-names>G</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Mental object rotation based on two-dimensional visual representations</article-title><source>Current Biology</source><year>2022</year><volume>32</volume><issue>21</issue><fpage>R1224</fpage><lpage>R1225</lpage><pub-id pub-id-type="pmid">36347228</pub-id></element-citation></ref><ref id="R34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stewart</surname><given-names>EEM</given-names></name><name><surname>Ludwig</surname><given-names>CJH</given-names></name><name><surname>Schütz</surname><given-names>AC</given-names></name></person-group><article-title>Humans represent the precision and utility of information acquired across fixations</article-title><source>Scientific Reports</source><year>2022</year><volume>12</volume><issue>1</issue><elocation-id>2411</elocation-id><pub-id pub-id-type="pmcid">PMC8844410</pub-id><pub-id pub-id-type="pmid">35165336</pub-id><pub-id pub-id-type="doi">10.1038/s41598-022-06357-7</pub-id></element-citation></ref><ref id="R35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><article-title>Shape Aftereffects Reflect Shape Constancy Operations: Appearance Matters</article-title><source>Journal of Experimental Psychology: Human Perception and Performance</source><year>2013</year><volume>39</volume><issue>3</issue><fpage>616</fpage><lpage>622</lpage><pub-id pub-id-type="pmid">23528000</pub-id></element-citation></ref><ref id="R36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storrs</surname><given-names>KR</given-names></name><name><surname>Fleming</surname><given-names>RW</given-names></name></person-group><article-title>Learning About the World by Learning About Images</article-title><source>Current Directions in Psychological Science</source><year>2021</year><volume>30</volume><issue>2</issue><fpage>120</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1177/0963721421990334</pub-id></element-citation></ref><ref id="R37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tarr</surname><given-names>MJ</given-names></name><name><surname>Kriegman</surname><given-names>DJ</given-names></name></person-group><article-title>What defines a view?</article-title><source>Vision Research</source><year>2001</year><volume>41</volume><issue>15</issue><fpage>1981</fpage><lpage>2004</lpage><pub-id pub-id-type="pmid">11412888</pub-id></element-citation></ref><ref id="R38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thouless</surname><given-names>RH</given-names></name></person-group><article-title>Phenomenal Regression To The ‘Real’ Object. II</article-title><source>British Journal of Psychology General Section</source><year>1931</year><volume>22</volume><issue>1</issue><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1931.tb00609.x</pub-id></element-citation></ref><ref id="R39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Walsh</surname><given-names>V</given-names></name><name><surname>Kulikowski</surname><given-names>J</given-names></name></person-group><source>Perceptual Constancy: Why Things Look as They Do</source><publisher-name>Cambridge University Press</publisher-name><year>1998</year><comment><ext-link ext-link-type="uri" xlink:href="https://books.google.de/books?id=LAtJHcBRAkEC">https://books.google.de/books?id=LAtJHcBRAkEC</ext-link></comment></element-citation></ref><ref id="R40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilder</surname><given-names>J</given-names></name><name><surname>Feldman</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>M</given-names></name></person-group><article-title>Superordinate shape classification using natural shape statistics</article-title><source>Cognition</source><year>2011</year><volume>119</volume><issue>3</issue><fpage>325</fpage><lpage>340</lpage><pub-id pub-id-type="pmcid">PMC3094567</pub-id><pub-id pub-id-type="pmid">21440250</pub-id><pub-id pub-id-type="doi">10.1016/j.cognition.2011.01.009</pub-id></element-citation></ref><ref id="R41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname><given-names>AT</given-names></name><name><surname>Moore</surname><given-names>A</given-names></name><name><surname>Newell</surname><given-names>FN</given-names></name></person-group><article-title>Canonical Views in Haptic Object Perception</article-title><source>Perception</source><year>2008</year><volume>37</volume><issue>12</issue><fpage>1867</fpage><lpage>1878</lpage><pub-id pub-id-type="pmid">19227377</pub-id></element-citation></ref><ref id="R42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yuille</surname><given-names>AL</given-names></name><name><surname>Coughlan</surname><given-names>JM</given-names></name><name><surname>Konishi</surname><given-names>S</given-names></name></person-group><source>The generic viewpoint constraint resolves the generalized bas relief ambiguity</source><conf-name>Proc Conf Information Sciences and Systems</conf-name><year>2000</year></element-citation></ref><ref id="R43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuille</surname><given-names>AL</given-names></name><name><surname>Coughlan</surname><given-names>JM</given-names></name><name><surname>Konishi</surname><given-names>S</given-names></name></person-group><article-title>The generic viewpoint assumption and planar bias</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><year>2003</year><volume>25</volume><issue>6</issue><fpage>775</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/tpami.2003.1201826</pub-id></element-citation></ref></ref-list></back><floats-group><boxed-text id="BX1" position="float" orientation="portrait"><caption><title>Significance statement</title></caption><p>Some viewpoints of objects are qualitatively and perceptually special, making them easier to recognize and remember. We show that qualitatively special viewpoints of familiar and novel 3D objects can be predicted by an optical-flow model that measures how points on the surface shift in the image as viewpoint changes. This provides a quantitative account for why some viewpoints of objects are perceptually special.</p></boxed-text><fig id="F1" position="float"><label>Figure 1</label><caption><p>A) Participants first completed a priming task: each object rotated for 4 seconds; participants indicated rotation direction via button press. Participants then completed a pose discrimination task. Two viewpoints of an object were shown: the cardinal/non-cardinal viewpoint, and a viewpoint offset by 0, +- 5, 10, 15 degrees rotation. Participants had to indicate whether the two viewpoints were the same or different. Photographed real objects and rendered mesh objects were tested in two separate experiments. B) Human discrimination performance for photographed (left) and rendered (right) objects. Pale lines: individual participants; bold lines: mean; error bars indicate standard errors. C) The cardinal axis effect for each object, rank ordered. This was calculated as the slope between performance at 0- and 5-degrees difference. Real objects are depicted in light purple, mesh objects in dark purple.</p></caption><graphic xlink:href="EMS189008-f001"/></fig><fig id="F2" position="float"><label>Figure 2</label><caption><p>A) Optical flow model. For each view of each mesh object, the optical flow was calculated between that view and the next view, resulting in an optical flow curve (left, middle). Left top and bottom show example optical flow output for the Front and Non-cardinal axes of one object (pink indicates rightwards motion, green, leftwards). Right panel shows optical flow curves for each object, with front, back, and non-cardinal viewpoints of each object. B) Comparison of the gradient of cardinal versus non-cardinal axes on the optical flow curve. Dots represent individual objects. The pink shaded area represents objects where the optical flow gradient was lower for cardinal than non-cardinal axes. C) The cardinal axis effect (difference in slope between 0 and 5 degrees offset, as in <xref ref-type="fig" rid="F1">Figure 1C</xref>) was predicted by the gradient difference between cardinal and non-cardinal axes on the optical flow curve. D) The cardinal axis was predicted by the range of the optical flow curve.</p></caption><graphic xlink:href="EMS189008-f002"/></fig><fig id="F3" position="float"><label>Figure 3</label><caption><p>A) Tested novel objects (top) with the corresponding optical flow curve (bottom). Novel objects are shown from the viewpoint with the highest number of “front” responses. Individual points on the curve represent viewpoints that were marked as the “front” of the object. B) Scatter plot representing for each object the mean gradient across participants at viewpoints selected as “front” compared to viewpoints that were not selected as “front”. Novel objects are represented in purple, and the four familiar objects tested are shown in blue for comparison. The shaded pink area represents where the viewpoints selected as “front” have a lower gradient than non-front viewpoints.</p></caption><graphic xlink:href="EMS189008-f003"/></fig><fig id="F4" position="float"><label>Figure 4</label><caption><p>This model can quantitatively predict and constrain which viewpoints of a 3D object are qualitatively meaningful using the predicted dissimilarity curve, and the gradient of this curve.</p></caption><graphic xlink:href="EMS189008-f004"/></fig></floats-group></article>